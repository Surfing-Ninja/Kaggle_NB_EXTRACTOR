{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":59156,"databundleVersionId":13874099,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"0db2f88d-87ff-48a1-b082-5c0ecbf3561a","cell_type":"markdown","source":"#  MABe Challenge - Social Action Recognition in Mice\n\n### Overview\n> In this competition, you’ll develop machine learning models to recognize behaviors in mice based on their movements, providing new insights into animal social structures and advancing behavioral science research.\n\n### Description\n> Animal social behavior is complex. Species from ants to wolves to mice form social groups where they build nests, raise their young, care for their groupmates, and defend their territory. Studying these behaviors teaches us about the brain and the evolution of behavior, but the work has usually required subjective, time-consuming documentation of animals' actions. ML advancements now let us automate this process, supporting large-scale behavioral studies in the wild and in the lab.\n\n> But even automated systems suffer from limited training data and poor generalizability. In current methods, an experimenter must hand-label hundreds of new training examples to automate recognition of a new behavior, which makes studying rare behaviors a challenge. And models trained within one research group usually fail when applied to data from other studies, meaning there is no guarantee that two labs are really studying the same behavior.\n\n> This competition challenges you to build models to identify over 30 different social and non-social behaviors in pairs and groups of co-housed mice, based on markerless motion capture of their movements in top-down video recordings. The dataset includes over 400 hours of footage from 20+ behavioral recording systems, all carefully labeled frame-by-frame by experts. Your goal is to recognize these behaviors as accurately as a trained human observer while overcoming the inherent variability arising from the use of different data collection equipment and motion capture pipelines.\n\nYour work will help scientists automate behavior analysis and better understand animal social structures. These models may be deployed across numerous labs, in neuroscience, computational biology, ethology, and ecology, to create a foundation for future ML and behavior research.","metadata":{}},{"id":"notebook-title","cell_type":"markdown","source":"This notebook tackles the MABe challenge by building a separate GBDT ensemble model for each unique `body_parts_tracked` configuration. \n\n**Core Strategy:**\n1.  **Grouped by Tracker:** Loop through each `body_parts_tracked` string.\n2.  **FPS-Aware Features:** Generate advanced temporal and spatial features (`transform_single`, `transform_pair`). All window sizes, lags, and spans are *scaled* by the video's FPS to ensure features are time-invariant (`_scale` function).\n3.  **Stratified Subsampling:** Use a `StratifiedSubsetClassifier` wrapper to train the GBDTs on a large, stratified subsample of the full dataset to manage memory and time.\n4.  **Ensemble Model:** For each behavior, train an ensemble of LGBM, XGBoost, and CatBoost models.\n5.  **Adaptive Prediction:** Use temporal smoothing, adaptive per-action probability thresholds, and minimum duration filtering to generate final event segments (`predict_multiclass_adaptive`).","metadata":{}},{"id":"9c0f76fb-9290-4e0e-b1e0-780b326243e5","cell_type":"markdown","source":"## 1. IMPORTS & CONFIGURATION","metadata":{}},{"id":"imports-config","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport polars as pl\nfrom tqdm import tqdm\nimport itertools\nimport warnings\nimport json\nimport os, random\nimport gc\nfrom collections import defaultdict\nfrom scipy import signal, stats\n\n# --- Models ---\nimport lightgbm\nfrom sklearn.base import ClassifierMixin, BaseEstimator, clone\nfrom sklearn.pipeline import make_pipeline\n\n# --- Model Selection & Metrics ---\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import f1_score\n\n# --- Optional Imports ---\ntry:\n    from xgboost import XGBClassifier\n    XGBOOST_AVAILABLE = True\nexcept:\n    XGBOOST_AVAILABLE = False\n    \ntry:\n    from catboost import CatBoostClassifier\n    CATBOOST_AVAILABLE = True\nexcept:\n    CATBOOST_AVAILABLE = False\n\n# --- Configuration ---\nvalidate_or_submit = 'submit'\nverbose = True\nSEED = 1234\n\n# Body parts to drop from high-dimensional trackers (e.g., headpieces)\nDROP_BODY_PARTS = [\n    'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n    'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n    'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n]\n\nwarnings.filterwarnings('ignore')\n\n# --- Seeding ---\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:13.059961Z","iopub.execute_input":"2025-11-08T03:39:13.060161Z","iopub.status.idle":"2025-11-08T03:39:19.798833Z","shell.execute_reply.started":"2025-11-08T03:39:13.060143Z","shell.execute_reply":"2025-11-08T03:39:19.798198Z"}},"outputs":[],"execution_count":null},{"id":"da324e91-21f9-4be3-b469-7009772ba05b","cell_type":"markdown","source":"## 2. UTILITY: STRATIFIED SUBSET CLASSIFIER","metadata":{}},{"id":"subset-classifier","cell_type":"code","source":"class StratifiedSubsetClassifier(ClassifierMixin, BaseEstimator):\n    \"\"\"A wrapper to train an estimator on a stratified subsample of data.\"\"\"\n    def __init__(self, estimator, n_samples=None):\n        self.estimator = estimator\n        self.n_samples = n_samples  # if None → no subsampling\n\n    def _to_numpy(self, X):\n        try:\n            return X.to_numpy(np.float32, copy=False)\n        except AttributeError:\n            return np.asarray(X, dtype=np.float32)\n\n    def fit(self, X, y):\n        Xn = self._to_numpy(X)\n        y = np.asarray(y).ravel()\n\n        # Handle rare cases where labels might be [0, 2]\n        uniq = np.unique(y[~pd.isna(y)])\n        if set(uniq.tolist()) == {0, 2}:\n            y = (y > 0).astype(np.int8)\n\n        # If n_samples is None or data is small, fit on full data\n        if self.n_samples is None or len(Xn) <= int(self.n_samples):\n            self.estimator.fit(Xn, y)\n        else:\n            # Fit on a stratified subset\n            sss = StratifiedShuffleSplit(n_splits=1, train_size=int(self.n_samples), random_state=42)\n            try:\n                idx, _ = next(sss.split(np.zeros_like(y), y))\n                self.estimator.fit(Xn[idx], y[idx])\n            except Exception:\n                # Fallback for cases where stratification fails (e.g., too few samples of one class)\n                step = max(len(Xn) // int(self.n_samples), 1)\n                self.estimator.fit(Xn[::step], y[::step])\n\n        try:\n            self.classes_ = np.asarray(self.estimator.classes_)\n        except Exception:\n            self.classes_ = np.unique(y)\n        return self\n\n    def predict_proba(self, X):\n        Xn = self._to_numpy(X)\n        try:\n            P = self.estimator.predict_proba(Xn)\n        except Exception:\n            # Handle models that failed training or have only one class\n            if len(self.classes_) == 1:\n                n = len(Xn)\n                c = int(self.classes_[0])\n                if c == 1:\n                    return np.column_stack([np.zeros(n, dtype=np.float32), np.ones(n, dtype=np.float32)])\n                else:\n                    return np.column_stack([np.ones(n, dtype=np.float32), np.zeros(n, dtype=np.float32)])\n            return np.full((len(Xn), 2), 0.5, dtype=np.float32)\n\n        P = np.asarray(P)\n        # Standardize output shape to (n_samples, 2)\n        if P.ndim == 1:\n            P1 = P.astype(np.float32)\n            return np.column_stack([1.0 - P1, P1])\n        if P.shape[1] == 1 and len(self.classes_) == 2:\n            P1 = P[:, 0].astype(np.float32)\n            return np.column_stack([1.0 - P1, P1])\n        return P\n\n    def predict(self, X):\n        Xn = self._to_numpy(X)\n        try:\n            return self.estimator.predict(Xn)\n        except Exception:\n            return np.argmax(self.predict_proba(Xn), axis=1)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:19.799513Z","iopub.execute_input":"2025-11-08T03:39:19.799954Z","iopub.status.idle":"2025-11-08T03:39:19.812404Z","shell.execute_reply.started":"2025-11-08T03:39:19.799937Z","shell.execute_reply":"2025-11-08T03:39:19.811543Z"}},"outputs":[],"execution_count":null},{"id":"aeaae79c-3538-4453-920c-e59a8d7dec87","cell_type":"markdown","source":"## 3. OFFICIAL SCORING FUNCTION (HIDDEN)","metadata":{}},{"id":"scoring-function","cell_type":"code","source":"class HostVisibleError(Exception):\n    pass\n\ndef single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n    label_frames: defaultdict[str, set[int]] = defaultdict(set)\n    prediction_frames: defaultdict[str, set[int]] = defaultdict(set)\n\n    for row in lab_solution.to_dicts():\n        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n\n    for video in lab_solution['video_id'].unique():\n        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()\n        active_labels: set[str] = set(json.loads(active_labels))\n        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set)\n\n        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts():\n            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n                continue\n           \n            new_frames = set(range(row['start_frame'], row['stop_frame']))\n            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n            prediction_frames[row['prediction_key']].update(new_frames)\n            predicted_mouse_pairs[prediction_pair].update(new_frames)\n\n    tps = defaultdict(int)\n    fns = defaultdict(int)\n    fps = defaultdict(int)\n    for key, pred_frames in prediction_frames.items():\n        action = key.split('_')[-1]\n        matched_label_frames = label_frames[key]\n        tps[action] += len(pred_frames.intersection(matched_label_frames))\n        fns[action] += len(matched_label_frames.difference(pred_frames))\n        fps[action] += len(pred_frames.difference(matched_label_frames))\n\n    distinct_actions = set()\n    for key, frames in label_frames.items():\n        action = key.split('_')[-1]\n        distinct_actions.add(action)\n        if key not in prediction_frames:\n            fns[action] += len(frames)\n\n    action_f1s = []\n    for action in distinct_actions:\n        if tps[action] + fns[action] + fps[action] == 0:\n            action_f1s.append(0)\n        else:\n            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n    return sum(action_f1s) / len(action_f1s)\n\ndef mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n    if len(solution) == 0 or len(submission) == 0:\n        raise ValueError('Missing solution or submission data')\n\n    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n\n    for col in expected_cols:\n        if col not in solution.columns:\n            raise ValueError(f'Solution is missing column {col}')\n        if col not in submission.columns:\n            raise ValueError(f'Submission is missing column {col}')\n\n    solution: pl.DataFrame = pl.DataFrame(solution)\n    submission: pl.DataFrame = pl.DataFrame(submission)\n    assert (solution['start_frame'] <= solution['stop_frame']).all()\n    assert (submission['start_frame'] <= submission['stop_frame']).all()\n    solution_videos = set(solution['video_id'].unique())\n    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n\n    solution = solution.with_columns(\n        pl.concat_str(\n            [\n                pl.col('video_id').cast(pl.Utf8),\n                pl.col('agent_id').cast(pl.Utf8),\n                pl.col('target_id').cast(pl.Utf8),\n                pl.col('action'),\n            ],\n            separator='_',\n        ).alias('label_key'),\n    )\n    submission = submission.with_columns(\n        pl.concat_str(\n            [\n                pl.col('video_id').cast(pl.Utf8),\n                pl.col('agent_id').cast(pl.Utf8),\n                pl.col('target_id').cast(pl.Utf8),\n                pl.col('action'),\n            ],\n            separator='_',\n        ).alias('prediction_key'),\n    )\n\n    lab_scores = []\n    for lab in solution['lab_id'].unique():\n        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n        lab_videos = set(lab_solution['video_id'].unique())\n        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n\n    return sum(lab_scores) / len(lab_scores)\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n    return mouse_fbeta(solution, submission, beta=beta)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:19.814001Z","iopub.execute_input":"2025-11-08T03:39:19.814351Z","iopub.status.idle":"2025-11-08T03:39:19.838164Z","shell.execute_reply.started":"2025-11-08T03:39:19.814324Z","shell.execute_reply":"2025-11-08T03:39:19.837465Z"}},"outputs":[],"execution_count":null},{"id":"b0c08911-e8b2-483f-96fd-b6231ca38be6","cell_type":"markdown","source":"## 4. DATA LOADING & PREPARATION","metadata":{}},{"id":"data-loading","cell_type":"code","source":"BASE_PATH = '/kaggle/input/MABe-mouse-behavior-detection'\n\ntrain = pd.read_csv(f'{BASE_PATH}/train.csv')\ntrain['n_mice'] = 4 - train[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].isna().sum(axis=1)\n\ntest = pd.read_csv(f'{BASE_PATH}/test.csv')\n\n# Get a unique list of all body part tracking configurations\nbody_parts_tracked_list = list(np.unique(train.body_parts_tracked))\n\ndef generate_mouse_data(dataset, traintest, traintest_directory=None, generate_single=True, generate_pair=True):\n    \"\"\"\n    Generator function to load and process video data one by one.\n    Yields data for single mice (agent=target) and mouse pairs (agent!=target).\n    \"\"\"\n    assert traintest in ['train', 'test']\n    if traintest_directory is None:\n        traintest_directory = f\"{BASE_PATH}/{traintest}_tracking\"\n    \n    for _, row in dataset.iterrows():\n        lab_id = row.lab_id\n        video_id = row.video_id\n\n        if not isinstance(row.behaviors_labeled, str):\n            if verbose: print(f'No labeled behaviors: {lab_id} {video_id}')\n            continue\n\n        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n        try:\n            vid = pd.read_parquet(path)\n        except FileNotFoundError:\n            if verbose: print(f\"File not found: {path}\")\n            continue\n\n        # Filter out optional, high-dim body parts\n        if len(np.unique(vid.bodypart)) > 5:\n            vid = vid.query(\"~ bodypart.isin(@DROP_BODY_PARTS)\")\n        \n        # Pivot to a (frame, mouse, bodypart, coord) structure\n        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n        del vid; gc.collect()\n\n        if pvid.isna().any().any():\n            if verbose and traintest == 'test': print(f'video with missing values: {video_id} ({len(pvid)} frames)')\n        else:\n            if verbose and traintest == 'test': print(f'video with all values: {video_id} ({len(pvid)} frames)')\n        \n        # Reorder levels to (mouse, bodypart, coord) and normalize by pixel density\n        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n        pvid /= row.pix_per_cm_approx  # Convert pixels to cm\n\n        # Load behaviors for this video\n        vid_behaviors = json.loads(row.behaviors_labeled)\n        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n        vid_behaviors = [b.split(',') for b in vid_behaviors]\n        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n        \n        annot = None\n        if traintest == 'train':\n            try:\n                annot_path = path.replace('train_tracking', 'train_annotation')\n                annot = pd.read_parquet(annot_path)\n            except FileNotFoundError:\n                if verbose: print(f\"Annotation not found, skipping: {annot_path}\")\n                continue\n\n        # --- Process Single Mouse (self-behaviors) ---\n        if generate_single:\n            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n            for mouse_id_str in np.unique(vid_behaviors_subset.agent):\n                try:\n                    mouse_id = int(mouse_id_str[-1])\n                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n                    \n                    # Select this mouse's pose data\n                    single_mouse = pvid.loc[:, mouse_id]\n                    assert len(single_mouse) == len(pvid)\n                    \n                    single_mouse_meta = pd.DataFrame({\n                        'video_id': video_id,\n                        'agent_id': mouse_id_str,\n                        'target_id': 'self',\n                        'video_frame': single_mouse.index,\n                        'frames_per_second': row.frames_per_second\n                    })\n                    \n                    if traintest == 'train':\n                        # Create frame-by-frame label matrix\n                        single_mouse_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=single_mouse.index)\n                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n                        for _, annot_row in annot_subset.iterrows():\n                            single_mouse_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n                        yield 'single', single_mouse, single_mouse_meta, single_mouse_label\n                    else:\n                        if verbose: print(f'- test single: {video_id} {mouse_id}')\n                        yield 'single', single_mouse, single_mouse_meta, vid_agent_actions\n                except KeyError: # Mouse ID not in pose data\n                    pass\n\n        # --- Process Mouse Pairs (social behaviors) ---\n        if generate_pair:\n            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n            if len(vid_behaviors_subset) > 0:\n                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values('mouse_id')), 2):\n                    agent_str = f\"mouse{agent}\"\n                    target_str = f\"mouse{target}\"\n                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n                    \n                    # Select pose data for agent 'A' and target 'B'\n                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n                    assert len(mouse_pair) == len(pvid)\n                    \n                    mouse_pair_meta = pd.DataFrame({\n                        'video_id': video_id,\n                        'agent_id': agent_str,\n                        'target_id': target_str,\n                        'video_frame': mouse_pair.index,\n                        'frames_per_second': row.frames_per_second\n                    })\n                    \n                    if traintest == 'train':\n                        # Create frame-by-frame label matrix\n                        mouse_pair_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=mouse_pair.index)\n                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n                        for _, annot_row in annot_subset.iterrows():\n                            mouse_pair_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n                        yield 'pair', mouse_pair, mouse_pair_meta, mouse_pair_label\n                    else:\n                        if verbose: print(f'- test pair: {video_id} {agent} -> {target}')\n                        yield 'pair', mouse_pair, mouse_pair_meta, vid_agent_actions","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:19.838907Z","iopub.execute_input":"2025-11-08T03:39:19.839107Z","iopub.status.idle":"2025-11-08T03:39:19.981748Z","shell.execute_reply.started":"2025-11-08T03:39:19.83909Z","shell.execute_reply":"2025-11-08T03:39:19.981159Z"}},"outputs":[],"execution_count":null},{"id":"c1e45c29-c719-4896-8ffe-13a4f8972e1e","cell_type":"markdown","source":"## 5. FPS-AWARE FEATURE ENGINEERING","metadata":{}},{"id":"fps-scaling","cell_type":"code","source":"# --- FPS Scaling Utilities ---\n# These functions are CRITICAL. They convert a window/lag size defined \n# at a reference (30 FPS) to the correct size for the current video's FPS.\n# This makes features robust to variable frame rates.\n\ndef _scale(n_frames_at_30fps, fps, ref=30.0):\n    \"\"\"Scale a frame count defined at 30 fps to the current video's fps.\"\"\"\n    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n\ndef _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n    \"\"\"Signed version of _scale for forward/backward shifts.\"\"\"\n    if n_frames_at_30fps == 0:\n        return 0\n    s = 1 if n_frames_at_30fps > 0 else -1\n    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n    return s * mag\n\ndef _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n    \"\"\"Safely get the FPS for the current video.\"\"\"\n    if 'frames_per_second' in meta_df.columns and pd.notnull(meta_df['frames_per_second']).any():\n        return float(meta_df['frames_per_second'].iloc[0])\n    # Fallback for test set where FPS might be missing in sample\n    vid = meta_df['video_id'].iloc[0]\n    return float(fallback_lookup.get(vid, default_fps))\n\n# --- Advanced Feature Sub-routines (FPS-Aware) ---\n\ndef add_curvature_features(X, center_x, center_y, fps):\n    \"\"\"Trajectory curvature (window lengths scaled by fps).\"\"\"\n    vel_x = center_x.diff()\n    vel_y = center_y.diff()\n    acc_x = vel_x.diff()\n    acc_y = vel_y.diff()\n\n    cross_prod = vel_x * acc_y - vel_y * acc_x\n    vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n    curvature = np.abs(cross_prod) / (vel_mag**3 + 1e-6)\n\n    for w in [30, 60]:\n        ws = _scale(w, fps)\n        X[f'curv_mean_{w}'] = curvature.rolling(ws, min_periods=max(1, ws // 6)).mean()\n\n    angle = np.arctan2(vel_y, vel_x)\n    angle_change = np.abs(angle.diff())\n    w = 30\n    ws = _scale(w, fps)\n    X[f'turn_rate_{w}'] = angle_change.rolling(ws, min_periods=max(1, ws // 6)).sum()\n    return X\n\ndef add_multiscale_features(X, center_x, center_y, fps):\n    \"\"\"Multi-scale temporal features (speed in cm/s; windows scaled by fps).\"\"\"\n    # displacement (cm) * fps = speed (cm/s)\n    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n\n    scales = [10, 40, 160] # ~0.3s, ~1.3s, ~5.3s at 30fps\n    for scale in scales:\n        ws = _scale(scale, fps)\n        if len(speed) >= ws:\n            X[f'sp_m{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).mean()\n            X[f'sp_s{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).std()\n\n    if len(scales) >= 2 and f'sp_m{scales[0]}' in X.columns and f'sp_m{scales[-1]}' in X.columns:\n        X['sp_ratio'] = X[f'sp_m{scales[0]}'] / (X[f'sp_m{scales[-1]}'] + 1e-6)\n    return X\n\ndef add_state_features(X, center_x, center_y, fps):\n    \"\"\"Behavioral state transitions; bins adjusted so semantics are fps-invariant.\"\"\"\n    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)  # cm/s\n    w_ma = _scale(15, fps) # 0.5s moving average\n    speed_ma = speed.rolling(w_ma, min_periods=max(1, w_ma // 3)).mean()\n\n    try:\n        # Original bins (cm/frame) at 30fps: [-inf, 0.5, 2.0, 5.0, inf]\n        # Convert to cm/s by multiplying by fps.\n        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n\n        for window in [60, 120]: # 2s, 4s\n            ws = _scale(window, fps)\n            if len(speed_states) >= ws:\n                for state in [0, 1, 2, 3]:\n                    X[f's{state}_{window}'] = ((speed_states == state).astype(float)\n                                                  .rolling(ws, min_periods=max(1, ws // 6)).mean())\n                state_changes = (speed_states != speed_states.shift(1)).astype(float)\n                X[f'trans_{window}'] = state_changes.rolling(ws, min_periods=max(1, ws // 6)).sum()\n    except Exception: # pd.cut can fail if all values are identical\n        pass\n    return X\n\ndef add_longrange_features(X, center_x, center_y, fps):\n    \"\"\"Long-range temporal features (windows & spans scaled by fps).\"\"\"\n    for window in [120, 240]: # 4s, 8s\n        ws = _scale(window, fps)\n        if len(center_x) >= ws:\n            X[f'x_ml{window}'] = center_x.rolling(ws, min_periods=max(5, ws // 6)).mean()\n            X[f'y_ml{window}'] = center_y.rolling(ws, min_periods=max(5, ws // 6)).mean()\n\n    for span in [60, 120]: # 2s, 4s EWM spans\n        s = _scale(span, fps)\n        X[f'x_e{span}'] = center_x.ewm(span=s, min_periods=1).mean()\n        X[f'y_e{span}'] = center_y.ewm(span=s, min_periods=1).mean()\n\n    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)  # cm/s\n    for window in [60, 120]:\n        ws = _scale(window, fps)\n        if len(speed) >= ws:\n            X[f'sp_pct{window}'] = speed.rolling(ws, min_periods=max(5, ws // 6)).rank(pct=True)\n    return X\n\ndef add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n    \"\"\"Social interaction features (windows scaled by fps).\"\"\"\n    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n        return X\n\n    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n    rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n\n    A_vx = mouse_pair['A']['body_center']['x'].diff()\n    A_vy = mouse_pair['A']['body_center']['y'].diff()\n    B_vx = mouse_pair['B']['body_center']['x'].diff()\n    B_vy = mouse_pair['B']['body_center']['y'].diff()\n\n    # Cosine similarity between agent's velocity and vector to target\n    A_lead = (A_vx * rel_x + A_vy * rel_y) / (np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n    # Cosine similarity between target's velocity and vector to agent\n    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (np.sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n\n    for window in [30, 60]: # 1s, 2s\n        ws = _scale(window, fps)\n        X[f'A_ld{window}'] = A_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n        X[f'B_ld{window}'] = B_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n\n    approach = -rel_dist.diff()  # positive = decreasing distance\n    chase = approach * B_lead # Agent approaches & Target leads (runs away from agent)\n    w = 30 # 1s\n    ws = _scale(w, fps)\n    X[f'chase_{w}'] = chase.rolling(ws, min_periods=max(1, ws // 6)).mean()\n\n    for window in [60, 120]: # 2s, 4s\n        ws = _scale(window, fps)\n        A_sp = np.sqrt(A_vx**2 + A_vy**2)\n        B_sp = np.sqrt(B_vx**2 + B_vy**2)\n        X[f'sp_cor{window}'] = A_sp.rolling(ws, min_periods=max(1, ws // 6)).corr(B_sp)\n    return X\n","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:19.982475Z","iopub.execute_input":"2025-11-08T03:39:19.982697Z","iopub.status.idle":"2025-11-08T03:39:20.129953Z","shell.execute_reply.started":"2025-11-08T03:39:19.98267Z","shell.execute_reply":"2025-11-08T03:39:20.129207Z"}},"outputs":[],"execution_count":null},{"id":"feature-transformers","cell_type":"code","source":"# --- Main Feature Transformation Functions ---\n\ndef transform_single(single_mouse, body_parts_tracked, fps):\n    \"\"\"Master feature engineering pipeline for single mouse (self-behaviors).\"\"\"\n    available_body_parts = single_mouse.columns.get_level_values(0)\n\n    # Base distance features (squared distances, cm^2)\n    X = pd.DataFrame({\n        f\"{p1}+{p2}\": np.square(single_mouse[p1] - single_mouse[p2]).sum(axis=1, skipna=False)\n        for p1, p2 in itertools.combinations(body_parts_tracked, 2)\n        if p1 in available_body_parts and p2 in available_body_parts\n    })\n    X = X.reindex(columns=[f\"{p1}+{p2}\" for p1, p2 in itertools.combinations(body_parts_tracked, 2)], copy=False)\n\n    # Speed-like features (lagged displacement, cm^2)\n    if all(p in single_mouse.columns for p in ['ear_left', 'ear_right', 'tail_base']):\n        lag = _scale(10, fps) # 0.33s lag\n        shifted = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(lag)\n        speeds = pd.DataFrame({\n            'sp_lf': np.square(single_mouse['ear_left'] - shifted['ear_left']).sum(axis=1, skipna=False),\n            'sp_rt': np.square(single_mouse['ear_right'] - shifted['ear_right']).sum(axis=1, skipna=False),\n            'sp_lf2': np.square(single_mouse['ear_left'] - shifted['tail_base']).sum(axis=1, skipna=False),\n            'sp_rt2': np.square(single_mouse['ear_right'] - shifted['tail_base']).sum(axis=1, skipna=False),\n        })\n        X = pd.concat([X, speeds], axis=1)\n\n    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n\n    # Body angle (orientation)\n    if all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n        v1 = single_mouse['nose'] - single_mouse['body_center']\n        v2 = single_mouse['tail_base'] - single_mouse['body_center']\n        X['body_ang'] = (v1['x'] * v2['x'] + v1['y'] * v2['y']) / (\n            np.sqrt(v1['x']**2 + v1['y']**2) * np.sqrt(v2['x']**2 + v2['y']**2) + 1e-6)\n\n    # Core temporal features\n    if 'body_center' in available_body_parts:\n        cx = single_mouse['body_center']['x']\n        cy = single_mouse['body_center']['y']\n\n        for w in [5, 15, 30, 60]: # ~0.16s, 0.5s, 1s, 2s\n            ws = _scale(w, fps)\n            roll = dict(min_periods=1, center=True)\n            X[f'cx_m{w}'] = cx.rolling(ws, **roll).mean()\n            X[f'cy_m{w}'] = cy.rolling(ws, **roll).mean()\n            X[f'cx_s{w}'] = cx.rolling(ws, **roll).std()\n            X[f'cy_s{w}'] = cy.rolling(ws, **roll).std()\n            X[f'x_rng{w}'] = cx.rolling(ws, **roll).max() - cx.rolling(ws, **roll).min()\n            X[f'y_rng{w}'] = cy.rolling(ws, **roll).max() - cy.rolling(ws, **roll).min()\n            X[f'disp{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).sum()**2 +\n                                     cy.diff().rolling(ws, min_periods=1).sum()**2)\n            X[f'act{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).var() +\n                                   cy.diff().rolling(ws, min_periods=1).var())\n\n        # Advanced features (all FPS-scaled internally)\n        X = add_curvature_features(X, cx, cy, fps)\n        X = add_multiscale_features(X, cx, cy, fps)\n        X = add_state_features(X, cx, cy, fps)\n        X = add_longrange_features(X, cx, cy, fps)\n\n    # Nose-tail features (cm)\n    if all(p in available_body_parts for p in ['nose', 'tail_base']):\n        nt_dist = np.sqrt((single_mouse['nose']['x'] - single_mouse['tail_base']['x'])**2 +\n                          (single_mouse['nose']['y'] - single_mouse['tail_base']['y'])**2)\n        for lag in [10, 20, 40]: # ~0.33s, 0.66s, 1.33s\n            l = _scale(lag, fps)\n            X[f'nt_lg{lag}'] = nt_dist.shift(l)\n            X[f'nt_df{lag}'] = nt_dist - nt_dist.shift(l)\n\n    # Ear features (cm)\n    if all(p in available_body_parts for p in ['ear_left', 'ear_right']):\n        ear_d = np.sqrt((single_mouse['ear_left']['x'] - single_mouse['ear_right']['x'])**2 +\n                        (single_mouse['ear_left']['y'] - single_mouse['ear_right']['y'])**2)\n        for off in [-20, -10, 10, 20]: # ~ +/- 0.33s, 0.66s\n            o = _scale_signed(off, fps)\n            X[f'ear_o{off}'] = ear_d.shift(-o)  \n        w = _scale(30, fps) # 1s window\n        X['ear_con'] = ear_d.rolling(w, min_periods=1, center=True).std() / \\\n                       (ear_d.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n\n    return X.astype(np.float32, copy=False)\n\ndef transform_pair(mouse_pair, body_parts_tracked, fps):\n    \"\"\"Master feature engineering pipeline for mouse pairs (social behaviors).\"\"\"\n    avail_A = mouse_pair['A'].columns.get_level_values(0)\n    avail_B = mouse_pair['B'].columns.get_level_values(0)\n\n    # Inter-mouse distances (squared distances, cm^2)\n    X = pd.DataFrame({\n        f\"12+{p1}+{p2}\": np.square(mouse_pair['A'][p1] - mouse_pair['B'][p2]).sum(axis=1, skipna=False)\n        for p1, p2 in itertools.product(body_parts_tracked, repeat=2)\n        if p1 in avail_A and p2 in avail_B\n    })\n    X = X.reindex(columns=[f\"12+{p1}+{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)], copy=False)\n\n    # Speed-like features (lagged displacement, cm^2)\n    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n        lag = _scale(10, fps) # 0.33s lag\n        shA = mouse_pair['A']['ear_left'].shift(lag)\n        shB = mouse_pair['B']['ear_left'].shift(lag)\n        speeds = pd.DataFrame({\n            'sp_A': np.square(mouse_pair['A']['ear_left'] - shA).sum(axis=1, skipna=False),\n            'sp_AB': np.square(mouse_pair['A']['ear_left'] - shB).sum(axis=1, skipna=False),\n            'sp_B': np.square(mouse_pair['B']['ear_left'] - shB).sum(axis=1, skipna=False),\n        })\n        X = pd.concat([X, speeds], axis=1)\n\n    # Elongation (placeholder, as it's a single-mouse feature)\n    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n\n    # Relative orientation\n    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n        dir_A = mouse_pair['A']['nose'] - mouse_pair['A']['tail_base']\n        dir_B = mouse_pair['B']['nose'] - mouse_pair['B']['tail_base']\n        X['rel_ori'] = (dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']) / (\n            np.sqrt(dir_A['x']**2 + dir_A['y']**2) * np.sqrt(dir_B['x']**2 + dir_B['y']**2) + 1e-6)\n\n    # Approach rate (cm^2 / 0.33s)\n    if all(p in avail_A for p in ['nose']) and all(p in avail_B for p in ['nose']):\n        cur = np.square(mouse_pair['A']['nose'] - mouse_pair['B']['nose']).sum(axis=1, skipna=False)\n        lag = _scale(10, fps)\n        shA_n = mouse_pair['A']['nose'].shift(lag)\n        shB_n = mouse_pair['B']['nose'].shift(lag)\n        past = np.square(shA_n - shB_n).sum(axis=1, skipna=False)\n        X['appr'] = cur - past # negative = approach\n\n    # Distance bins (cm)\n    if 'body_center' in avail_A and 'body_center' in avail_B:\n        cd = np.sqrt((mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n                     (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2)\n        X['v_cls'] = (cd < 5.0).astype(float)\n        X['cls']   = ((cd >= 5.0) & (cd < 15.0)).astype(float)\n        X['med']   = ((cd >= 15.0) & (cd < 30.0)).astype(float)\n        X['far']   = (cd >= 30.0).astype(float)\n\n    # Temporal interaction features\n    if 'body_center' in avail_A and 'body_center' in avail_B:\n        cd_full = np.square(mouse_pair['A']['body_center'] - mouse_pair['B']['body_center']).sum(axis=1, skipna=False)\n\n        for w in [5, 15, 30, 60]: # ~0.16s, 0.5s, 1s, 2s\n            ws = _scale(w, fps)\n            roll = dict(min_periods=1, center=True)\n            X[f'd_m{w}']  = cd_full.rolling(ws, **roll).mean()\n            X[f'd_s{w}']  = cd_full.rolling(ws, **roll).std()\n            X[f'd_mn{w}'] = cd_full.rolling(ws, **roll).min()\n            X[f'd_mx{w}'] = cd_full.rolling(ws, **roll).max()\n\n            d_var = cd_full.rolling(ws, **roll).var()\n            X[f'int{w}'] = 1 / (1 + d_var) # Interaction metric (low variance in distance)\n\n            Axd = mouse_pair['A']['body_center']['x'].diff()\n            Ayd = mouse_pair['A']['body_center']['y'].diff()\n            Bxd = mouse_pair['B']['body_center']['x'].diff()\n            Byd = mouse_pair['B']['body_center']['y'].diff()\n            coord = Axd * Bxd + Ayd * Byd # Velocity dot product\n            X[f'co_m{w}'] = coord.rolling(ws, **roll).mean()\n            X[f'co_s{w}'] = coord.rolling(ws, **roll).std()\n\n    # Nose-nose dynamics (cm)\n    if 'nose' in avail_A and 'nose' in avail_B:\n        nn_dist = np.sqrt((mouse_pair['A']['nose']['x'] - mouse_pair['B']['nose']['x'])**2 +\n                          (mouse_pair['A']['nose']['y'] - mouse_pair['B']['nose']['y'])**2)\n        for lag in [10, 20, 40]: # ~0.33s, 0.66s, 1.33s\n            l = _scale(lag, fps)\n            X[f'nn_lg{lag}']  = nn_dist.shift(l)\n            X[f'nn_ch{lag}']  = nn_dist - nn_dist.shift(l)\n            is_cl = (nn_dist < 10.0).astype(float)\n            X[f'cl_ps{lag}']  = is_cl.rolling(l, min_periods=1).mean()\n\n    # Velocity alignment\n    if 'body_center' in avail_A and 'body_center' in avail_B:\n        Avx = mouse_pair['A']['body_center']['x'].diff()\n        Avy = mouse_pair['A']['body_center']['y'].diff()\n        Bvx = mouse_pair['B']['body_center']['x'].diff()\n        Bvy = mouse_pair['B']['body_center']['y'].diff()\n        val = (Avx * Bvx + Avy * Bvy) / (np.sqrt(Avx**2 + Avy**2) * np.sqrt(Bvx**2 + Bvy**2) + 1e-6)\n\n        for off in [-20, -10, 0, 10, 20]: # ~ +/- 0.33s, 0.66s\n            o = _scale_signed(off, fps)\n            X[f'va_{off}'] = val.shift(-o)\n\n        w = _scale(30, fps) # 1s window\n        X['int_con'] = cd_full.rolling(w, min_periods=1, center=True).std() / \\\n                       (cd_full.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n\n        # Advanced interaction (all FPS-scaled internally)\n        X = add_interaction_features(X, mouse_pair, avail_A, avail_B, fps)\n\n    return X.astype(np.float32, copy=False)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:20.130888Z","iopub.execute_input":"2025-11-08T03:39:20.131193Z","iopub.status.idle":"2025-11-08T03:39:20.161311Z","shell.execute_reply.started":"2025-11-08T03:39:20.131174Z","shell.execute_reply":"2025-11-08T03:39:20.160565Z"}},"outputs":[],"execution_count":null},{"id":"567aaf39-0475-49fa-bc4e-3b4a0de57db7","cell_type":"markdown","source":"## 6. POST-PROCESSING & SUBMISSION","metadata":{}},{"id":"post-processing","cell_type":"code","source":"# Default threshold for actions, can be tuned per-action\naction_thresholds = defaultdict(lambda: 0.27)\n# e.g., action_thresholds['attack'] = 0.4\n# e.g., action_thresholds['sniff'] = 0.2\n\ndef predict_multiclass_adaptive(pred, meta, action_thresholds):\n    \"\"\"Converts frame-wise probabilities into event segments.\"\"\"\n    if pred.empty:\n        return pd.DataFrame(columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n\n    # 1. Temporal smoothing\n    pred_smoothed = pred.rolling(window=5, min_periods=1, center=True).mean()\n    \n    # 2. Get most likely action per frame\n    ama = np.argmax(pred_smoothed.values, axis=1)\n    max_probs = pred_smoothed.max(axis=1).values\n    \n    # 3. Apply adaptive per-action thresholds\n    threshold_mask = np.zeros(len(pred_smoothed), dtype=bool)\n    for i, action in enumerate(pred_smoothed.columns):\n        action_mask = (ama == i)\n        threshold = action_thresholds.get(action, 0.27)\n        threshold_mask |= (action_mask & (max_probs >= threshold))\n    \n    # 4. Set frames below threshold to -1 (no action)\n    ama = np.where(threshold_mask, ama, -1)\n    ama = pd.Series(ama, index=meta.video_frame)\n    \n    # 5. Find change-points (start/end of events)\n    changes_mask = (ama != ama.shift(1)).values\n    ama_changes = ama[changes_mask]\n    meta_changes = meta[changes_mask]\n    \n    # 6. Build submission dataframe\n    mask = ama_changes.values >= 0\n    if mask.any():\n        mask[-1] = False # Last event is incomplete\n    else:\n        # No events found\n        return pd.DataFrame(columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n\n    start_indices = np.where(mask)[0]\n    if len(start_indices) == 0:\n        return pd.DataFrame(columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n        \n    stop_indices = start_indices + 1\n    \n    submission_part = pd.DataFrame({\n        'video_id': meta_changes['video_id'].iloc[start_indices].values,\n        'agent_id': meta_changes['agent_id'].iloc[start_indices].values,\n        'target_id': meta_changes['target_id'].iloc[start_indices].values,\n        'action': pred.columns[ama_changes.iloc[start_indices].values],\n        'start_frame': ama_changes.index[start_indices],\n        'stop_frame': ama_changes.index[stop_indices]\n    })\n    \n    # --- Fix stop_frames that cross video boundaries (should be rare) ---\n    stop_video_id = meta_changes['video_id'].iloc[stop_indices].values\n    stop_agent_id = meta_changes['agent_id'].iloc[stop_indices].values\n    stop_target_id = meta_changes['target_id'].iloc[stop_indices].values\n    \n    for i in range(len(submission_part)):\n        video_id = submission_part.video_id.iloc[i]\n        agent_id = submission_part.agent_id.iloc[i]\n        target_id = submission_part.target_id.iloc[i]\n        \n        is_last_event = (i == len(submission_part) - 1)\n        is_boundary = False\n        if i < len(stop_video_id):\n            is_boundary = (stop_video_id[i] != video_id or \n                           stop_agent_id[i] != agent_id or \n                           stop_target_id[i] != target_id)\n        else:\n            is_last_event = True # Handle if stop_indices was shorter\n\n        if is_last_event or is_boundary:\n            # Query for the max frame of this specific agent/target/video\n            q_str = \"(video_id == @video_id) & (agent_id == @agent_id) & (target_id == @target_id)\"\n            new_stop_frame = meta.query(q_str).video_frame.max() + 1\n            submission_part.iat[i, submission_part.columns.get_loc('stop_frame')] = new_stop_frame\n    \n    # 7. Filter out very short events (likely noise)\n    duration = submission_part.stop_frame - submission_part.start_frame\n    submission_part = submission_part[duration >= 3].reset_index(drop=True)\n    \n    if len(submission_part) > 0:\n        assert (submission_part.stop_frame > submission_part.start_frame).all(), 'stop <= start'\n    \n    if verbose: print(f'  actions found: {len(submission_part)}')\n    return submission_part","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:20.162051Z","iopub.execute_input":"2025-11-08T03:39:20.162442Z","iopub.status.idle":"2025-11-08T03:39:20.182981Z","shell.execute_reply.started":"2025-11-08T03:39:20.162395Z","shell.execute_reply":"2025-11-08T03:39:20.182316Z"}},"outputs":[],"execution_count":null},{"id":"0b8f8ae2-5b5c-42cd-a550-2f0547b30a50","cell_type":"markdown","source":"## 7. MODEL DEFINITIONS","metadata":{}},{"id":"model-definitions","cell_type":"code","source":"def get_model_ensemble():\n    \"\"\"Defines the list of models to be trained.\"\"\"\n    models = []\n\n    # --- Model 1: LGBM (Balanced) ---\n    models.append(make_pipeline(\n        StratifiedSubsetClassifier(\n            lightgbm.LGBMClassifier(\n                n_estimators=225, learning_rate=0.07, min_child_samples=40,\n                num_leaves=31, subsample=0.8, colsample_bytree=0.8, verbose=-1,\n                random_state=SEED, bagging_seed=SEED, feature_fraction_seed=SEED, data_random_seed=SEED\n            ), 500_000, # Subsample to 500k rows\n        )\n    ))\n    \n    # --- Model 2: LGBM (Deeper) ---\n    models.append(make_pipeline(\n        StratifiedSubsetClassifier(\n            lightgbm.LGBMClassifier(\n                n_estimators=150, learning_rate=0.1, min_child_samples=20,\n                num_leaves=63, max_depth=8, subsample=0.7, colsample_bytree=0.9,\n                reg_alpha=0.1, reg_lambda=0.1, verbose=-1,\n                random_state=SEED, bagging_seed=SEED, feature_fraction_seed=SEED, data_random_seed=SEED\n            ), 450_000, # Subsample to 450k rows\n        )\n    ))\n\n    # --- Model 3: LGBM (Larger Leaves) ---\n    models.append(make_pipeline(\n        StratifiedSubsetClassifier(\n            lightgbm.LGBMClassifier(\n                n_estimators=100, learning_rate=0.05, min_child_samples=30,\n                num_leaves=127, max_depth=10, subsample=0.75, verbose=-1,\n                random_state=SEED, bagging_seed=SEED, feature_fraction_seed=SEED, data_random_seed=SEED\n            ), 400_000, # Subsample to 400k rows\n        )\n    ))\n    \n    # --- Model 4: XGBoost (if available) ---\n    if XGBOOST_AVAILABLE:\n        models.append(make_pipeline(\n            StratifiedSubsetClassifier(\n                XGBClassifier(\n                    n_estimators=180, learning_rate=0.08, max_depth=6,\n                    min_child_weight=5, subsample=0.8, colsample_bytree=0.8,\n                    tree_method='hist', verbosity=0,\n                    random_state=SEED\n                ), 500_000,\n            )\n        ))\n        \n    # --- Model 5: CatBoost (if available) ---\n    if CATBOOST_AVAILABLE:\n        models.append(make_pipeline(\n            StratifiedSubsetClassifier(\n                CatBoostClassifier(\n                    iterations=120, learning_rate=0.1, depth=6,\n                    verbose=False, allow_writing_files=False,\n                    random_seed=SEED\n                ), 500_000,\n            )\n        ))\n        \n    return models","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:20.183755Z","iopub.execute_input":"2025-11-08T03:39:20.184038Z","iopub.status.idle":"2025-11-08T03:39:20.202206Z","shell.execute_reply.started":"2025-11-08T03:39:20.184012Z","shell.execute_reply":"2025-11-08T03:39:20.201713Z"}},"outputs":[],"execution_count":null},{"id":"900e7edb-79c0-4f78-ae7f-7f8f15cb5f40","cell_type":"markdown","source":"## 8. TRAINING & INFERENCE PIPELINE","metadata":{}},{"id":"train-predict-loop","cell_type":"code","source":"def train_and_predict_ensemble(body_parts_tracked_str, switch_tr, X_tr, label, meta):\n    \"\"\"\n    Trains a full ensemble for one (tracker, type) combo and predicts on the test set.\n    - `body_parts_tracked_str`: The JSON string for the tracker config.\n    - `switch_tr`: 'single' or 'pair'.\n    - `X_tr`, `label`: Training features and labels.\n    - `meta`: Training metadata.\n    \"\"\"\n    \n    models = get_model_ensemble()\n    \n    # Convert to numpy and free memory\n    X_tr_np = X_tr.to_numpy(np.float32, copy=False)\n    del X_tr; gc.collect()\n\n    # --- Train one ensemble for each action ---\n    model_list = [] # List to store (action, [trained_model_1, ...])\n    for action in label.columns:\n        y_raw = label[action].to_numpy()\n        mask = ~pd.isna(y_raw)\n        y_action = y_raw[mask].astype(int)\n        \n        # Only train if we have positive examples\n        if not (y_action == 0).all() and np.sum(y_action) >= 5:\n            trained = []\n            idx = np.flatnonzero(mask)\n            for m in models:\n                m_clone = clone(m)\n                m_clone.fit(X_tr_np[idx], y_action)\n                trained.append(m_clone)\n            model_list.append((action, trained))\n\n    del X_tr_np; gc.collect()\n\n    # --- Prepare for Inference ---\n    body_parts_tracked = json.loads(body_parts_tracked_str)\n    if len(body_parts_tracked) > 5:\n        body_parts_tracked = [b for b in body_parts_tracked if b not in DROP_BODY_PARTS]\n\n    # Get test videos matching this tracker config\n    test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n    if test_subset.empty:\n        if verbose: print(\"  No test videos for this tracker.\")\n        return\n    \n    # Create a lookup for FPS in case test metadata is missing it\n    fps_lookup = (test_subset[['video_id', 'frames_per_second']]\n                  .drop_duplicates('video_id')\n                  .set_index('video_id')['frames_per_second']\n                  .to_dict())\n\n    generator = generate_mouse_data(\n        test_subset, 'test',\n        generate_single=(switch_tr == 'single'),\n        generate_pair=(switch_tr == 'pair')\n    )\n\n    if verbose:\n        print(f\"  Inferring on {len(test_subset)} test video(s) with {len(models)} models...\")\n\n    # --- Inference Loop ---\n    for switch_te, data_te, meta_te, actions_te in generator:\n        assert switch_te == switch_tr\n        try:\n            fps_i = _fps_from_meta(meta_te, fps_lookup, default_fps=30.0)\n\n            # Generate features for this test video\n            if switch_te == 'single':\n                X_te = transform_single(data_te, body_parts_tracked, fps_i).astype(np.float32)\n            else:\n                X_te = transform_pair(data_te, body_parts_tracked, fps_i).astype(np.float32)\n\n            X_te_np = X_te.to_numpy(np.float32, copy=False)\n            del X_te, data_te; gc.collect()\n\n            # Predict probabilities for all relevant actions\n            pred = pd.DataFrame(index=meta_te.video_frame)\n            for action, trained in model_list:\n                if action in actions_te:\n                    # Average probabilities across the ensemble\n                    probs = [m.predict_proba(X_te_np)[:, 1] for m in trained]\n                    pred[action] = np.mean(probs, axis=0)\n\n            del X_te_np; gc.collect()\n\n            # Convert probabilities to submission segments\n            if pred.shape[1] > 0:\n                sub_part = predict_multiclass_adaptive(pred, meta_te, action_thresholds)\n                submission_list.append(sub_part)\n            elif verbose:\n                print(\"  No models trained for required test actions.\")\n\n        except Exception as e:\n            if verbose:\n                print(f\"  ERROR during inference: {str(e)[:100]}\")\n            try: del data_te \n            except Exception: pass\n            gc.collect()\n\ndef robustify_submission(submission, dataset, traintest, traintest_directory=None):\n    \"\"\"Ensures all videos have at least one prediction and cleans overlaps.\"\"\"\n    if traintest_directory is None:\n        traintest_directory = f\"{BASE_PATH}/{traintest}_tracking\"\n\n    # 1. Filter invalid segments\n    submission = submission[submission.start_frame < submission.stop_frame]\n\n    # 2. Remove overlapping predictions for the same (video, agent, target)\n    group_list = []\n    for _, group in submission.groupby(['video_id', 'agent_id', 'target_id']):\n        group = group.sort_values('start_frame')\n        mask = np.ones(len(group), dtype=bool)\n        last_stop = -1\n        for i, (_, row) in enumerate(group.iterrows()):\n            if row['start_frame'] < last_stop:\n                mask[i] = False # This event overlaps with the previous one\n            else:\n                last_stop = row['stop_frame']\n        group_list.append(group[mask])\n    submission = pd.concat(group_list, ignore_index=True) if group_list else pd.DataFrame(columns=submission.columns)\n\n    # 3. Add placeholder predictions for any videos with no predictions\n    s_list = []\n    predicted_videos = set(submission.video_id.unique())\n    \n    for _, row in dataset.iterrows():\n        video_id = row['video_id']\n        if video_id in predicted_videos:\n            continue\n\n        if verbose:\n            print(f\"Video {video_id} has no predictions, adding placeholder.\")\n        \n        try:\n            path = f\"{traintest_directory}/{row['lab_id']}/{video_id}.parquet\"\n            vid = pd.read_parquet(path)\n            vid_behaviors = eval(row['behaviors_labeled'])\n            vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n            vid_behaviors = [b.split(',') for b in vid_behaviors]\n            vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n\n            start_frame = vid.video_frame.min()\n            stop_frame = vid.video_frame.max() + 1\n\n            # Add one tiny placeholder for the first labeled action\n            if not vid_behaviors.empty:\n                vb_row = vid_behaviors.iloc[0]\n                s_list.append((video_id, vb_row['agent'], vb_row['target'], vb_row['action'], start_frame, start_frame + 1))\n        except Exception as e:\n             if verbose: print(f\"Could not create placeholder for {video_id}: {e}\")\n\n    if len(s_list) > 0:\n        placeholder_df = pd.DataFrame(s_list, columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n        submission = pd.concat([submission, placeholder_df], ignore_index=True)\n\n    submission = submission.reset_index(drop=True)\n    return submission\n","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:20.203942Z","iopub.execute_input":"2025-11-08T03:39:20.204146Z","iopub.status.idle":"2025-11-08T03:39:20.232309Z","shell.execute_reply.started":"2025-11-08T03:39:20.20413Z","shell.execute_reply":"2025-11-08T03:39:20.231628Z"}},"outputs":[],"execution_count":null},{"id":"1c686e6b-0843-49ec-a602-81de0d09950c","cell_type":"markdown","source":"## 9. MAIN EXECUTION LOOP","metadata":{}},{"id":"main-loop","cell_type":"code","source":"submission_list = []\nprint(f\"XGBoost: {XGBOOST_AVAILABLE}, CatBoost: {CATBOOST_AVAILABLE}\\n\")\n\n# We skip list[0] as it's often '[]' (no parts tracked)\nfor section, body_parts_tracked_str in enumerate(body_parts_tracked_list[1:], 1):\n    try:\n        body_parts_tracked_names = json.loads(body_parts_tracked_str)\n        print(f\"--- Processing Tracker Config {section}/{len(body_parts_tracked_list)-1} ({len(body_parts_tracked_names)} body parts) ---\")\n        \n        # Filter to only the body parts we use\n        if len(body_parts_tracked_names) > 5:\n            body_parts_tracked = [b for b in body_parts_tracked_names if b not in DROP_BODY_PARTS]\n        else:\n            body_parts_tracked = body_parts_tracked_names\n\n        train_subset = train[train.body_parts_tracked == body_parts_tracked_str]\n        if train_subset.empty:\n            print(\"  No training videos for this tracker. Skipping.\")\n            print()\n            continue\n\n        # Create FPS lookup for this training subset\n        _fps_lookup = (train_subset[['video_id', 'frames_per_second']]\n                       .drop_duplicates('video_id')\n                       .set_index('video_id')['frames_per_second']\n                       .to_dict())\n\n        # --- Load all data for this tracker config ---\n        single_list, single_label_list, single_meta_list = [], [], []\n        pair_list, pair_label_list, pair_meta_list = [], [], []\n\n        data_generator = generate_mouse_data(train_subset, 'train')\n        for switch, data, meta, label in data_generator:\n            if switch == 'single':\n                single_list.append(data)\n                single_meta_list.append(meta)\n                single_label_list.append(label)\n            else:\n                pair_list.append(data)\n                pair_meta_list.append(meta)\n                pair_label_list.append(label)\n\n        # --- Process 'single' mouse data (self-behaviors) ---\n        if len(single_list) > 0:\n            single_feats_parts = []\n            for data_i, meta_i in zip(single_list, single_meta_list):\n                fps_i = _fps_from_meta(meta_i, _fps_lookup, default_fps=30.0)\n                Xi = transform_single(data_i, body_parts_tracked, fps_i).astype(np.float32)\n                single_feats_parts.append(Xi)\n\n            X_tr = pd.concat(single_feats_parts, axis=0, ignore_index=True)\n            single_label = pd.concat(single_label_list, axis=0, ignore_index=True)\n            single_meta  = pd.concat(single_meta_list,  axis=0, ignore_index=True)\n\n            del single_list, single_label_list, single_meta_list, single_feats_parts\n            gc.collect()\n\n            print(f\"  Single: Loaded {X_tr.shape[0]} frames, {X_tr.shape[1]} features\")\n            train_and_predict_ensemble(body_parts_tracked_str, 'single', X_tr, single_label, single_meta)\n\n            del X_tr, single_label, single_meta\n            gc.collect()\n        else:\n             print(\"  Single: No data.\")\n\n        # --- Process 'pair' mouse data (social behaviors) ---\n        if len(pair_list) > 0:\n            pair_feats_parts = []\n            for data_i, meta_i in zip(pair_list, pair_meta_list):\n                fps_i = _fps_from_meta(meta_i, _fps_lookup, default_fps=30.0)\n                Xi = transform_pair(data_i, body_parts_tracked, fps_i).astype(np.float32)\n                pair_feats_parts.append(Xi)\n\n            X_tr = pd.concat(pair_feats_parts, axis=0, ignore_index=True)\n            pair_label = pd.concat(pair_label_list, axis=0, ignore_index=True)\n            pair_meta  = pd.concat(pair_meta_list,  axis=0, ignore_index=True)\n\n            del pair_list, pair_label_list, pair_meta_list, pair_feats_parts\n            gc.collect()\n\n            print(f\"  Pair: Loaded {X_tr.shape[0]} frames, {X_tr.shape[1]} features\")\n            train_and_predict_ensemble(body_parts_tracked_str, 'pair', X_tr, pair_label, pair_meta)\n\n            del X_tr, pair_label, pair_meta\n            gc.collect()\n        else:\n            print(\"  Pair: No data.\")\n\n    except Exception as e:\n        print(f'*** CRITICAL ERROR in main loop: {str(e)[:100]} ***')\n\n    gc.collect()\n    print()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:39:20.233158Z","iopub.execute_input":"2025-11-08T03:39:20.233887Z"}},"outputs":[],"execution_count":null},{"id":"f0879fc4-639d-4fbf-9d86-dfb26fd89c89","cell_type":"markdown","source":"## 10. FINALIZE SUBMISSION","metadata":{}},{"id":"940a865d-55e4-4417-84b9-0f6522f3cf05","cell_type":"code","source":"if len(submission_list) > 0:\n    submission = pd.concat(submission_list, ignore_index=True)\nelse:\n    # Create a dummy submission if all models failed\n    print(\"No predictions generated, creating dummy submission.\")\n    submission = pd.DataFrame({\n        'video_id': [438887472],\n        'agent_id': ['mouse1'],\n        'target_id': ['self'],\n        'action': ['rear'],\n        'start_frame': [278],\n        'stop_frame': [500]\n    })\n\nsubmission_robust = robustify_submission(submission, test, 'test')\nsubmission_robust.index.name = 'row_id'\nsubmission_robust.to_csv('submission.csv')\n\nprint(f\"\\nSubmission created: {len(submission_robust)} total predictions\")\nprint(\"Done.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5e84b0ca-0948-4171-b52b-bf8cb6cc1d17","cell_type":"markdown","source":"### Connect with Me  \n\nFeel free to follow me on these platforms:  \n\n[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/AdilShamim8)  \n[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/adilshamim8)  \n[![Twitter](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/adil_shamim8)  ","metadata":{}}]}