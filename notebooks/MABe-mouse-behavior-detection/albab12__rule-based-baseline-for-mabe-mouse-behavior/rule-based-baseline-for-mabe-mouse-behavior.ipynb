{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":59156,"databundleVersionId":13874099,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31091,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"desperately trying to make a functional solution","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport itertools\nimport warnings\nimport gc\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Masking\nfrom tensorflow.keras.utils import Sequence\nfrom sklearn.preprocessing import StandardScaler\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Configuration\nclass CFG:\n    SEQ_LENGTH = 64\n    BATCH_SIZE = 256\n    EPOCHS = 15\n    VERBOSE = 1\n    USE_AMP = True\n    \nif CFG.USE_AMP:\n    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n\nprint(\"TensorFlow Version:\", tf.__version__)\nprint(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n\n# --- Data Loading ---\nprint(\"Loading training and test metadata...\")\ntrain = pd.read_csv('/kaggle/input/MABe-mouse-behavior-detection/train.csv')\ntest = pd.read_csv('/kaggle/input/MABe-mouse-behavior-detection/test.csv')\nbody_parts_tracked_list = list(np.unique(train.body_parts_tracked))\nprint(f\"Found {len(body_parts_tracked_list)} unique body part configurations.\")\n\n# --- Feature Engineering ---\ndef feature_engineering_single_mouse(mouse_data, body_parts):\n    features = []\n    core_parts = ['nose', 'ear_left', 'ear_right', 'tail_base']\n    parts_to_use = [p for p in core_parts if p in body_parts]\n    for part in parts_to_use:\n        features.append(mouse_data[part]['x'].rename(f'{part}_x'))\n        features.append(mouse_data[part]['y'].rename(f'{part}_y'))\n        features.append(mouse_data[part]['x'].diff().fillna(0).rename(f'{part}_vx'))\n        features.append(mouse_data[part]['y'].diff().fillna(0).rename(f'{part}_vy'))\n    df = pd.concat(features, axis=1)\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df)\n    return pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n\ndef feature_engineering_pair_mouse(mouse_pair_data, body_parts):\n    features = []\n    core_parts = ['nose', 'ear_left', 'ear_right', 'tail_base']\n    parts_to_use = [p for p in core_parts if p in body_parts]\n    for mouse_id in ['A', 'B']:\n        for part in parts_to_use:\n            features.append(mouse_pair_data[mouse_id][part]['x'].rename(f'{mouse_id}_{part}_x'))\n            features.append(mouse_pair_data[mouse_id][part]['y'].rename(f'{mouse_id}_{part}_y'))\n            features.append(mouse_pair_data[mouse_id][part]['x'].diff().fillna(0).rename(f'{mouse_id}_{part}_vx'))\n            features.append(mouse_pair_data[mouse_id][part]['y'].diff().fillna(0).rename(f'{mouse_id}_{part}_vy'))\n    if 'nose' in parts_to_use and 'tail_base' in parts_to_use:\n        dist_n2n = np.linalg.norm(mouse_pair_data['A']['nose'].values - mouse_pair_data['B']['nose'].values, axis=1)\n        features.append(pd.Series(dist_n2n, index=mouse_pair_data.index, name='dist_nose2nose'))\n        dist_n2t_A = np.linalg.norm(mouse_pair_data['A']['nose'].values - mouse_pair_data['B']['tail_base'].values, axis=1)\n        features.append(pd.Series(dist_n2t_A, index=mouse_pair_data.index, name='dist_A_nose_B_tail'))\n        dist_n2t_B = np.linalg.norm(mouse_pair_data['B']['nose'].values - mouse_pair_data['A']['tail_base'].values, axis=1)\n        features.append(pd.Series(dist_n2t_B, index=mouse_pair_data.index, name='dist_B_nose_A_tail'))\n    df = pd.concat(features, axis=1)\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df)\n    return pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n\n# --- Keras Data Generator ---\nclass DataGenerator(Sequence):\n    def __init__(self, features, labels=None, batch_size=CFG.BATCH_SIZE, seq_length=CFG.SEQ_LENGTH, is_test=False):\n        self.features = features\n        self.labels = labels\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.is_test = is_test\n        self.num_features = features.shape[1]\n        if self.is_test:\n            self.indices = np.arange(len(self.features))\n        else:\n            self.indices = np.arange(self.seq_length - 1, len(self.features))\n\n    def __len__(self):\n        return int(np.ceil(len(self.indices) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_pos_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n        X = np.zeros((len(batch_pos_indices), self.seq_length, self.num_features), dtype=np.float32)\n        for i, pos_idx in enumerate(batch_pos_indices):\n            start_pos = pos_idx - self.seq_length + 1\n            end_pos = pos_idx + 1\n            X[i,] = self.features.iloc[start_pos:end_pos].values\n        if self.is_test:\n            return X\n        else:\n            y = self.labels.iloc[batch_pos_indices].values.astype(np.float32)\n            return X, y\n    \n    def on_epoch_end(self):\n        if not self.is_test:\n            np.random.shuffle(self.indices)\n\n# --- Model Building ---\ndef build_model(num_features):\n    model = Sequential([\n        Masking(mask_value=0., input_shape=(CFG.SEQ_LENGTH, num_features)),\n        Bidirectional(LSTM(128, return_sequences=False)),\n        Dropout(0.4),\n        Dense(64, activation='relu'),\n        Dropout(0.4),\n        Dense(1, activation='sigmoid')\n    ])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n    return model\n\n# --- Data Loading Generator ---\ndef generate_mouse_data(dataset, traintest, traintest_directory=None, generate_single=True, generate_pair=True):\n    # This function remains largely the same\n    assert traintest in ['train', 'test']\n    if traintest_directory is None:\n        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n    for _, row in dataset.iterrows():\n        lab_id, video_id = row.lab_id, row.video_id\n        if lab_id.startswith('MABe22'): continue\n        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n        try:\n            vid = pd.read_parquet(path)\n        except Exception: continue\n        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n        del vid\n        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n        pvid /= row.pix_per_cm_approx\n        if pd.isna(row.behaviors_labeled): continue\n        vid_behaviors = pd.DataFrame([b.split(',') for b in sorted(list({b.replace(\"'\", \"\") for b in json.loads(row.behaviors_labeled)}))], columns=['agent', 'target', 'action'])\n        annot = None\n        if traintest == 'train':\n            try:\n                annot = pd.read_parquet(path.replace('train_tracking', 'train_annotation'))\n            except FileNotFoundError: continue\n        if generate_single:\n            for mouse_id_str in np.unique(vid_behaviors.query(\"target == 'self'\").agent):\n                try:\n                    mouse_id = int(mouse_id_str[-1])\n                    actions = np.unique(vid_behaviors.query(\"agent == @mouse_id_str\").action)\n                    data = pvid.loc[:, mouse_id]\n                    meta = pd.DataFrame({'video_id': video_id, 'agent_id': mouse_id_str, 'target_id': 'self', 'video_frame': data.index})\n                    if traintest == 'train':\n                        labels = pd.DataFrame(0.0, columns=actions, index=data.index)\n                        subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n                        for _, r in subset.iterrows():\n                            labels.loc[r['start_frame']:r['stop_frame'], r.action] = 1.0\n                        yield 'single', data, meta, labels\n                    else: yield 'single', data, meta, actions\n                except (KeyError, ValueError): pass\n        if generate_pair:\n            if len(vid_behaviors.query(\"target != 'self'\")) > 0:\n                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values('mouse_id')), 2):\n                    try:\n                        agent_str, target_str = f\"mouse{agent}\", f\"mouse{target}\"\n                        actions = np.unique(vid_behaviors.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n                        data = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n                        meta = pd.DataFrame({'video_id': video_id, 'agent_id': agent_str, 'target_id': target_str, 'video_frame': data.index})\n                        if traintest == 'train':\n                            labels = pd.DataFrame(0.0, columns=actions, index=data.index)\n                            subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n                            for _, r in subset.iterrows():\n                                labels.loc[r['start_frame']:r['stop_frame'], r.action] = 1.0\n                            yield 'pair', data, meta, labels\n                        else: yield 'pair', data, meta, actions\n                    except (KeyError, ValueError): pass\n\n# --- Post-Processing and Submission Formatting ---\ndef predict_multiclass_optimized(pred, meta, thresholds=None):\n    # This function remains largely the same\n    if pred.empty or pred.isnull().all().all(): return pd.DataFrame()\n    if thresholds is None: thresholds = {col: 0.5 for col in pred.columns}\n    pred_smooth = pred.rolling(5, min_periods=1, center=True).mean()\n    ama = np.argmax(pred_smooth.values, axis=1)\n    max_proba = pred_smooth.max(axis=1).values\n    threshold_array = np.array([thresholds.get(col, 0.5) for col in pred.columns])\n    action_thresholds = threshold_array[ama]\n    ama = np.where(max_proba >= action_thresholds, ama, -1)\n    ama = pd.Series(ama, index=meta.video_frame)\n    changes_mask = (ama != ama.shift(1)).values\n    ama_changes, meta_changes = ama[changes_mask], meta[changes_mask]\n    mask = ama_changes.values >= 0\n    if len(mask) > 0: mask[-1] = False\n    if np.sum(mask) == 0: return pd.DataFrame()\n    submission_part = pd.DataFrame({\n        'video_id': meta_changes['video_id'][mask].values, 'agent_id': meta_changes['agent_id'][mask].values,\n        'target_id': meta_changes['target_id'][mask].values, 'action': pred.columns[ama_changes[mask].values],\n        'start_frame': ama_changes.index[mask], 'stop_frame': ama_changes.index[1:][mask[:-1]]})\n    duration = submission_part.stop_frame - submission_part.start_frame\n    return submission_part[(duration >= 2) & (duration <= 10000)]\n\ndef robustify(submission, dataset):\n    # This function remains largely the same\n    if submission is None or submission.empty:\n        submission = pd.DataFrame(columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n    submission = submission[submission.start_frame < submission.stop_frame]\n    group_list = []\n    if not submission.empty:\n        for _, group in submission.groupby(['video_id', 'agent_id', 'target_id']):\n            group = group.sort_values('start_frame')\n            mask = np.ones(len(group), dtype=bool)\n            last_stop_frame = -1\n            for i, (_, row) in enumerate(group.iterrows()):\n                if row['start_frame'] < last_stop_frame: mask[i] = False\n                else: last_stop_frame = row['stop_frame']\n            group_list.append(group[mask])\n    if group_list: submission = pd.concat(group_list)\n    s_list = []\n    for _, row in dataset.iterrows():\n        if row.lab_id.startswith('MABe22'): continue\n        if not (submission.video_id == row.video_id).any():\n            s_list.append((row.video_id, 'mouse1', 'self', 'rear', 100, 200))\n    if len(s_list) > 0:\n        submission = pd.concat([submission, pd.DataFrame(s_list, columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])])\n    return submission.reset_index(drop=True)\n\n# ============= NEW MAIN PROCESSING LOOP =============\nmodels = {}\nprint(\"--- Starting Training Phase ---\")\n\nfor section, body_parts_tracked_str in enumerate(body_parts_tracked_list):\n    if pd.isna(body_parts_tracked_str): continue\n    try:\n        body_parts_tracked = json.loads(body_parts_tracked_str)\n        print(f\"\\nProcessing config {section}: {len(body_parts_tracked)} body parts\")\n        train_subset = train[train.body_parts_tracked == body_parts_tracked_str]\n        if len(train_subset) == 0: continue\n\n        # Process each behavior type (single/pair)\n        for switch, feature_func in [('single', feature_engineering_single_mouse), ('pair', feature_engineering_pair_mouse)]:\n            print(f\"  Processing {switch} mouse actions...\")\n            \n            # Aggregate all data for this config to find all possible actions\n            all_actions_in_config = set()\n            gen_args = {'generate_single': switch=='single', 'generate_pair': switch=='pair'}\n            data_gen_for_actions = generate_mouse_data(train_subset, 'train', **gen_args)\n            for _, _, _, labels in data_gen_for_actions:\n                all_actions_in_config.update(labels.columns)\n\n            # Train one model per action\n            for action in sorted(list(all_actions_in_config)):\n                print(f\"    Training model for action: {action}\")\n                \n                features_list, labels_list = [], []\n                data_gen_for_training = generate_mouse_data(train_subset, 'train', **gen_args)\n                \n                for _, data, _, labels in data_gen_for_training:\n                    if action in labels.columns:\n                        features = feature_func(data, body_parts_tracked)\n                        features_list.append(features)\n                        labels_list.append(labels[[action]])\n\n                if not features_list: continue\n\n                # Now concat and train\n                X_action = pd.concat(features_list).reset_index(drop=True)\n                y_action = pd.concat(labels_list).reset_index(drop=True)\n\n                if y_action.sum().iloc[0] < 20:\n                    print(f\"      Skipping {action} due to insufficient positive samples.\")\n                    continue\n                \n                train_gen = DataGenerator(X_action, y_action)\n                model = build_model(num_features=X_action.shape[1])\n                model.fit(train_gen, epochs=CFG.EPOCHS, verbose=CFG.VERBOSE)\n                models[(body_parts_tracked_str, switch, action)] = model\n                \n    except Exception as e:\n        print(f'  ***Exception***: {e}')\n        import traceback\n        traceback.print_exc()\n\n# --- Prediction Phase ---\nprint(\"\\n--- Starting Prediction Phase ---\")\nsubmission_list = []\nfor section, body_parts_tracked_str in enumerate(body_parts_tracked_list):\n    if pd.isna(body_parts_tracked_str): continue\n    try:\n        body_parts_tracked = json.loads(body_parts_tracked_str)\n        test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n        if len(test_subset) == 0: continue\n        \n        print(f\"\\nPredicting for config {section}: {len(body_parts_tracked)} body parts\")\n        for switch, feature_func in [('single', feature_engineering_single_mouse), ('pair', feature_engineering_pair_mouse)]:\n            print(f\"  Predicting {switch} mouse actions...\")\n            gen_args = {'generate_single': switch=='single', 'generate_pair': switch=='pair'}\n            generator = generate_mouse_data(test_subset, 'test', **gen_args)\n            \n            for _, data_te, meta_te, actions_te in generator:\n                if len(data_te) < CFG.SEQ_LENGTH: continue\n                X_te = feature_func(data_te, body_parts_tracked)\n                pred_df = pd.DataFrame(index=X_te.index)\n\n                for action in actions_te:\n                    model_key = (body_parts_tracked_str, switch, action)\n                    if model_key in models:\n                        print(f\"    Predicting action: {action}\")\n                        model = models[model_key]\n                        test_gen = DataGenerator(X_te.reset_index(drop=True), is_test=True)\n                        predictions = model.predict(test_gen, verbose=CFG.VERBOSE)\n                        \n                        pred_aligned = np.full(len(X_te), np.nan)\n                        num_preds = len(predictions)\n                        pred_aligned[test_gen.indices[:num_preds]] = predictions.flatten()\n                        pred_df[action] = pd.Series(pred_aligned, index=X_te.index).interpolate(limit_direction='both')\n\n                if not pred_df.empty:\n                    submission_part = predict_multiclass_optimized(pred_df, meta_te)\n                    submission_list.append(submission_part)\n    except Exception as e:\n        print(f'  ***Exception***: {e}')\n        import traceback\n        traceback.print_exc()\n\n# --- Finalization ---\nprint(\"\\nFinalizing submission...\")\nif submission_list:\n    submission = pd.concat([s for s in submission_list if not s.empty])\nelse:\n    submission = pd.DataFrame()\n\nsubmission_robust = robustify(submission, test)\nsubmission_robust.index.name = 'row_id'\nsubmission_robust.to_csv('submission.csv')\n\nprint(f\"\\nFinal submission shape: {submission_robust.shape}\")\nprint(\"Submission saved to submission.csv\")\nprint(submission_robust.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:49:39.579862Z","iopub.execute_input":"2025-09-28T06:49:39.580196Z"}},"outputs":[],"execution_count":null}]}