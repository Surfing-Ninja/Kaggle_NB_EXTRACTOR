{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59156,"databundleVersionId":13874099,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict, Counter\nimport ast\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning, module='pandas')\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nnp.seterr(invalid='ignore')\ntry:\n    import torch\n    from torch import nn, optim\n    from torch.utils.data import Dataset, DataLoader\n    from torch.cuda.amp import autocast, GradScaler\nexcept ImportError as e:\n    print(f\"Error importing PyTorch: {e}\")\n    raise\nimport os\nimport psutil\nimport gc\nfrom joblib import Parallel, delayed\n# =============================================================================\n# SHARED UTILITIES: Always load metadata and define functions\n# =============================================================================\nbase_path = '/kaggle/input/MABe-mouse-behavior-detection/'\n# Load metadata\ntrain_meta = pd.read_csv(f'{base_path}train.csv')\ntrain_meta['video_id'] = train_meta['video_id'].astype(str)\ntrain_meta['body_parts_tracked'] = train_meta['body_parts_tracked'].apply(\n    lambda x: ast.literal_eval(x) if pd.notna(x) else []\n)\ntrain_meta['behaviors_labeled'] = train_meta['behaviors_labeled'].apply(\n    lambda x: [b.strip().strip(\"'\\\"\") for b in ast.literal_eval(x)] if pd.notna(x) else []\n)\ntest_meta = pd.read_csv(f'{base_path}test.csv')\ntest_meta['video_id'] = test_meta['video_id'].astype(str)\ntest_meta['body_parts_tracked'] = test_meta['body_parts_tracked'].apply(\n    lambda x: ast.literal_eval(x) if pd.notna(x) else []\n)\ntest_meta['behaviors_labeled'] = test_meta['behaviors_labeled'].apply(\n    lambda x: [b.strip().strip(\"'\\\"\") for b in ast.literal_eval(x)] if pd.notna(x) else []\n)\ndef parse_behavior_label(b_label):\n    \"\"\"Parse 'mouse1,mouse2,approach' to (1,2,'approach').\"\"\"\n    parts = [p.strip().strip(\"'\\\"\") for p in b_label.split(',')]\n    if len(parts) >= 3:\n        agent_str = parts[0].replace('mouse', '').replace('self', '1')\n        agent = int(agent_str)\n        target_str = parts[1].replace('mouse', '').replace('self', str(agent))\n        target = int(target_str)\n        action = parts[2].strip()\n        return agent, target, action\n    return None\ndef load_video_features(lab, vid, is_train=True, N=10):\n    \"\"\"\n    Load tracking; add features. For train: incl. labels; for test: no labels.\n    \"\"\"\n    meta = train_meta if is_train else test_meta\n    row = meta[(meta['lab_id'] == lab) & (meta['video_id'] == vid)].iloc[0]\n    fps = row['frames_per_second']\n    pix_per_cm = row['pix_per_cm_approx']\n    total_frames = int(fps * row['video_duration_sec'])\n    # Load tracking\n    track_dir = 'train_tracking' if is_train else 'test_tracking'\n    track_path = f'{base_path}{track_dir}/{lab}/{vid}.parquet'\n    track = pd.read_parquet(track_path)\n    track['mouse_id'] = track['mouse_id'].astype(int)\n    track = track[track['video_frame'] % N == 0].copy()\n    track['x_cm'] = track['x'] / pix_per_cm\n    track['y_cm'] = track['y'] / pix_per_cm\n    anns = None\n    if is_train:\n        ann_path = f'{base_path}train_annotation/{lab}/{vid}.parquet'\n        anns = pd.read_parquet(ann_path)\n        def expand_to_frames(anns, total_frames):\n            frame_labels = np.full(total_frames, 'background')\n            for _, r in anns.iterrows():\n                action = f\"{int(r['agent_id'])}_{int(r['target_id'])}_{r['action']}\"\n                for f in range(int(r['start_frame']), min(int(r['stop_frame']) + 1, total_frames)):\n                    frame_labels[f] = action\n            return frame_labels\n        frame_labels = expand_to_frames(anns, total_frames)\n        track['label'] = track['video_frame'].apply(\n            lambda f: frame_labels[int(f)] if int(f) < len(frame_labels) else 'background'\n        )\n    # Per-mouse features - FIXED VERSION\n    bc = track[track['bodypart'] == 'body_center'].sort_values(['mouse_id', 'video_frame'])\n    \n    # Calculate diffs with forward fill for first frame\n    bc['x_cm_prev'] = bc.groupby('mouse_id')['x_cm'].shift(1)\n    bc['y_cm_prev'] = bc.groupby('mouse_id')['y_cm'].shift(1)\n    \n    # Fill NaN positions with forward fill, then backward fill, then 0\n    bc['x_cm'] = bc['x_cm'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n    bc['y_cm'] = bc['y_cm'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n    \n    # Safe diff calculation\n    bc['dx'] = bc['x_cm'] - bc['x_cm_prev']\n    bc['dy'] = bc['y_cm'] - bc['y_cm_prev']\n    bc['dx'] = bc['dx'].fillna(0)  # First frame has no previous, set velocity to 0\n    bc['dy'] = bc['dy'].fillna(0)\n    \n    # Safe speed calculation\n    bc['speed_sq'] = bc['dx']**2 + bc['dy']**2\n    bc['speed'] = np.sqrt(np.maximum(bc['speed_sq'], 0))  # Ensure non-negative\n    bc['speed'] = bc['speed'].fillna(0)\n    \n    # Safe heading calculation\n    valid_vel = (bc['dx']**2 + bc['dy']**2) > 1e-8\n    bc['heading'] = np.zeros_like(bc['dx'])\n    bc.loc[valid_vel, 'heading'] = np.arctan2(bc.loc[valid_vel, 'dy'], bc.loc[valid_vel, 'dx'])\n    bc['heading'] = bc['heading'].fillna(0)\n    \n    # Safe distance calculation\n    bc['dist_center_sq'] = bc['x_cm']**2 + bc['y_cm']**2\n    bc['dist_center'] = np.sqrt(np.maximum(bc['dist_center_sq'], 0))\n    bc['dist_center'] = bc['dist_center'].fillna(0)\n    \n    # Drop temporary columns\n    bc = bc.drop(['x_cm_prev', 'y_cm_prev', 'speed_sq', 'dist_center_sq'], axis=1)\n    \n    # Pairwise nose distances - FIXED VERSION\n    nose = track[track['bodypart'] == 'nose'].copy()\n    nose['x_cm'] = nose['x'] / pix_per_cm\n    nose['y_cm'] = nose['y'] / pix_per_cm\n    \n    # Fill missing nose positions\n    nose['x_cm'] = nose.groupby(['mouse_id', 'video_frame'])['x_cm'].fillna(0)\n    nose['y_cm'] = nose.groupby(['mouse_id', 'video_frame'])['y_cm'].fillna(0)\n    \n    pairs = [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)]\n    pair_dists = defaultdict(list)\n    pair_dists['video_frame'] = []\n    \n    for frame in sorted(nose['video_frame'].unique()):\n        frame_nose = nose[nose['video_frame'] == frame]\n        pair_dists['video_frame'].append(frame)\n        \n        for a, b in pairs:\n            # Get positions for each mouse, use mean if multiple detections\n            pos_a = frame_nose[frame_nose['mouse_id'] == a][['x_cm', 'y_cm']]\n            pos_b = frame_nose[frame_nose['mouse_id'] == b][['x_cm', 'y_cm']]\n            \n            x_a = pos_a['x_cm'].mean() if len(pos_a) > 0 else 0\n            y_a = pos_a['y_cm'].mean() if len(pos_a) > 0 else 0\n            x_b = pos_b['x_cm'].mean() if len(pos_b) > 0 else 0\n            y_b = pos_b['y_cm'].mean() if len(pos_b) > 0 else 0\n            \n            # Safe distance calculation\n            dx_pair = x_a - x_b\n            dy_pair = y_a - y_b\n            dist_sq = dx_pair**2 + dy_pair**2\n            dist = np.sqrt(np.maximum(dist_sq, 0))\n            pair_dists[f'dist_{a}_{b}'].append(dist)\n    \n    pair_df = pd.DataFrame(pair_dists)\n    \n    # Merge and final cleanup\n    features_df = bc.merge(pair_df, on='video_frame', how='left').fillna(0)\n    \n    # Ensure all feature columns are non-negative before any sqrt operations\n    numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if features_df[col].dtype in ['float64', 'float32']:\n            features_df[col] = np.maximum(features_df[col], 0)\n    \n    if is_train:\n        features_df['label'] = features_df['video_frame'].apply(\n            lambda f: frame_labels[int(f)] if int(f) < len(frame_labels) else 'background'\n        )\n    \n    return features_df, anns\n# Training\nnon_mabe_labs = ~train_meta['lab_id'].str.contains('MABe22')\ntrain_non_mabe = train_meta[non_mabe_labs]\nall_behaviors = []\nfor behaviors in train_non_mabe['behaviors_labeled']:\n    all_behaviors.extend(behaviors)\naction_types = [parse_behavior_label(b)[2] for b in all_behaviors if parse_behavior_label(b)]\nglobal_actions = sorted(set(action_types))\nprint(f\"Global actions from non-MABe22: {global_actions}\")\nprint(f\"Number of unique classes: {len(global_actions)}\")\nprint(\"Filtering non-MABe22 train data...\")\nannotated_non_mabe = train_non_mabe[train_non_mabe['behaviors_labeled'].apply(len) > 0]\nprint(f\"Non-MABe22 annotated videos: {len(annotated_non_mabe)}\")\ndef log_memory(phase):\n    ram_percent = psutil.virtual_memory().percent\n    print(f\"{phase} - RAM usage: {ram_percent:.2f}%\")\nlog_memory(\"After metadata load\")\nunique_labels = set(['background'])\ndef process_video(i):\n    row = annotated_non_mabe.iloc[i]\n    lab, vid = row['lab_id'], row['video_id']\n    if lab == 'PleasantMeerkat' and vid == '1375833299':\n        return None, set()\n    df, _ = load_video_features(lab, vid)\n    print(f\"Loaded features for {lab}/{vid}\")\n    label_set = set(df['label'].unique()) if 'label' in df.columns else set()\n    return df, label_set\nprint(\"Loading features in parallel...\")\nresults = Parallel(n_jobs=-1)(delayed(process_video)(i) for i in range(len(annotated_non_mabe)))\nall_dfs = [r[0] for r in results if r[0] is not None]\nfor r in results:\n    unique_labels.update(r[1])\nlog_memory(\"After feature loading\")\nglobal_le = LabelEncoder()\nglobal_le.fit(sorted(unique_labels))\nprint(f\"Global LE fitted on {len(global_le.classes_)} unique labels: {global_le.classes_}\")\nall_features = []\nfor i, df in enumerate(all_dfs):\n    if 'label' in df.columns and len(df[df['label'] != 'background']) > 0:\n        df['video_id'] = annotated_non_mabe.iloc[i]['video_id']\n        all_features.append(df)\nprint(f\"Loaded {len(all_features)} videos with non-background labels.\")\ndel all_dfs\ngc.collect()\nlog_memory(\"After label encoding\")\nif all_features:\n    feature_cols = ['x_cm', 'y_cm', 'speed', 'heading', 'dist_center', 'dist_1_2', 'dist_1_3', 'dist_1_4', 'dist_2_3', 'dist_2_4', 'dist_3_4', 'mouse_id']\n    mouse_categories = [1, 2, 3, 4]\n    seq_len = 3\n    seq_X_list = []\n    seq_y_list = []\n    feat_cols_base = ['x_cm', 'y_cm', 'speed', 'heading', 'dist_center', 'dist_1_2', 'dist_1_3', 'dist_1_4', 'dist_2_3', 'dist_2_4', 'dist_3_4']\n    dummy_cols = [f'mouse_{i}' for i in mouse_categories]\n    feat_lstm = feat_cols_base + dummy_cols\n    \n    for i, df in enumerate(all_features):\n        # Separate features and labels first\n        feature_df = df[feature_cols].copy()\n        feature_df = feature_df[feat_cols_base].fillna(0).astype(np.float32)  # Only numeric features\n        \n        # Mouse encoding separate\n        mouse_ids = df['mouse_id'].copy()\n        mouse_ids = pd.Categorical(mouse_ids, categories=mouse_categories)\n        dummies = pd.get_dummies(mouse_ids, prefix='mouse')\n        \n        # Add missing dummies\n        for col in set(dummy_cols) - set(dummies.columns):\n            dummies[col] = 0\n        \n        # Combine\n        feature_df = pd.concat([feature_df, dummies], axis=1)\n        feature_df = feature_df[feat_lstm].values  # Now safe to convert to numpy\n        \n        # Encode labels separately if they exist\n        if len(label_series) > 0 and 'label' in df.columns:\n            y_encoded = global_le.transform(label_series)\n        else:\n            y_encoded = np.array([])\n        \n        # Create sequences per mouse\n        for mouse_id in mouse_categories:\n            mouse_mask = df['mouse_id'] == mouse_id\n            if mouse_mask.sum() < seq_len:\n                continue\n                \n            mouse_features = feature_df[mouse_mask].values\n            mouse_labels = y_encoded[mouse_mask] if len(y_encoded) > 0 else np.array([])\n            \n            # Ensure we have enough frames\n            if len(mouse_features) < seq_len:\n                continue\n                \n            for j in range(len(mouse_features) - seq_len + 1):\n                window = mouse_features[j:j+seq_len]\n                seq_X_list.append(window)\n                \n                # Use the label from the last frame in the window\n                if len(mouse_labels) > 0:\n                    label_idx = j + seq_len - 1\n                    if label_idx < len(mouse_labels):\n                        seq_y_list.append(mouse_labels[label_idx])\n        \n        del feature_df, dummies\n        gc.collect()\n    \n    if seq_X_list:\n        seq_X = np.array(seq_X_list, dtype=np.float32)\n        seq_y = np.array(seq_y_list, dtype=np.int32)\n        print(f\"LSTM training samples: {len(seq_y)} sequences\")\n    else:\n        raise ValueError(\"No valid sequences found for training\")\n    \n    del seq_X_list, seq_y_list\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    log_memory(\"After evaluation and cleanup\")\n# =============================================================================\n# INFERENCE ON TEST SET & GENERATE SUBMISSION\n# =============================================================================\nsubmission_path = '/kaggle/working/submission.csv'\nsubmission_rows = []\nrow_id = 0\nprocessed_videos = set()\nif os.path.exists(submission_path):\n    existing_sub = pd.read_csv(submission_path)\n    submission_rows = existing_sub.to_dict('records')\n    row_id = existing_sub['row_id'].max() + 1 if not existing_sub.empty else 0\n    processed_videos = set(existing_sub['video_id'].unique())\n    print(f\"Loaded existing submission with {len(submission_rows)} rows. Resuming from row_id {row_id}. Processed videos: {len(processed_videos)}\")\nprint(\"Starting inference on test set...\")\nmouse_categories = [1, 2, 3, 4]\nfeat_cols_base = ['x_cm', 'y_cm', 'speed', 'heading', 'dist_center', 'dist_1_2', 'dist_1_3', 'dist_1_4', 'dist_2_3', 'dist_2_4', 'dist_3_4']\ndummy_cols = [f'mouse_{i}' for i in mouse_categories]\nfeat_lstm = feat_cols_base + dummy_cols\nseq_len = 3\n\nfor i, (idx, test_row) in enumerate(test_meta.iterrows()):\n    lab, vid = test_row['lab_id'], test_row['video_id']\n    if vid in processed_videos:\n        print(f\"Skipping already processed test video {i+1}/{len(test_meta)}: {lab}/{vid}\")\n        continue\n    \n    print(f\"Processing test video: {lab}/{vid}\")\n    df_test, _ = load_video_features(lab, vid, is_train=False)\n    print(f\"Loaded test features for {lab}/{vid}\")\n    \n    # Separate processing for test data (no labels)\n    df_test['video_id'] = vid\n    \n    # Process features separately from categorical columns\n    feature_cols_test = feat_cols_base + ['video_frame']\n    df_features = df_test[feature_cols_test].copy().fillna(0).astype(np.float32)\n    \n    # Handle mouse_id encoding separately\n    mouse_ids = df_test['mouse_id'].copy()\n    mouse_ids = pd.Categorical(mouse_ids, categories=mouse_categories)\n    dummies = pd.get_dummies(mouse_ids, prefix='mouse')\n    \n    # Add missing dummy columns\n    missing_dummies = set(dummy_cols) - set(dummies.columns)\n    for col in missing_dummies:\n        dummies[col] = 0\n    \n    # Combine features with dummies\n    df_test_full = pd.concat([df_features, dummies], axis=1)\n    df_test_full['mouse_id'] = mouse_ids  # Keep original mouse_id for grouping\n    \n    # Now we can safely work with numeric features only for LSTM\n    lstm_frame_probs = np.zeros((len(df_test), len(global_le.classes_)), dtype=np.float32)\n    has_lstm_pred = np.zeros(len(df_test), dtype=bool)\n    \n    groups_test = df_test_full.groupby('mouse_id')\n    lstm_model.eval()\n    \n    with torch.no_grad():\n        for mouse, group in groups_test:\n            if len(group) < seq_len:\n                continue\n                \n            # Get sorted frames and corresponding original indices\n            group_sorted = group.sort_values('video_frame').reset_index(drop=True)\n            original_indices = df_test[df_test['video_frame'] == group_sorted['video_frame'].iloc[0]].index\n            \n            # Extract feature windows\n            n_frames = len(group_sorted) - seq_len + 1\n            if n_frames > 0:\n                windows = []\n                for j in range(n_frames):\n                    window_features = group_sorted.iloc[j:j+seq_len][feat_lstm].values\n                    windows.append(window_features)\n                \n                windows = np.array(windows, dtype=np.float32)\n                \n                # Scale features\n                windows_reshaped = windows.reshape(-1, len(feat_lstm))\n                windows_scaled = scaler.transform(windows_reshaped).reshape(windows.shape)\n                \n                # Model inference\n                windows_t = torch.FloatTensor(windows_scaled).to(device, non_blocking=(device.type == 'cuda'))\n                with autocast(enabled=(device.type == 'cuda')):\n                    outs = lstm_model(windows_t)\n                probs = torch.softmax(outs, dim=1).cpu().numpy()\n                \n                # Assign predictions to frames\n                for j in range(n_frames):\n                    # Find the corresponding frame in original df_test\n                    target_frame = group_sorted['video_frame'].iloc[j + seq_len - 1]\n                    frame_matches = df_test['video_frame'] == target_frame\n                    if frame_matches.any():\n                        frame_idx = df_test[frame_matches].index[0]\n                        lstm_frame_probs[frame_idx] = probs[j]\n                        has_lstm_pred[frame_idx] = True\n                \n                if device.type == 'cuda':\n                    torch.cuda.empty_cache()\n    \n    # Use predictions for frames that have LSTM predictions, fallback to uniform for others\n    valid_mask = has_lstm_pred\n    frame_probs = lstm_frame_probs.copy()\n    if not np.all(valid_mask):\n        # For frames without LSTM predictions, use uniform distribution or background bias\n        fallback_probs = np.ones(len(global_le.classes_)) / len(global_le.classes_)\n        background_idx = global_le.transform(['background'])[0]\n        fallback_probs[background_idx] = 0.8  # Bias toward background\n        fallback_probs = fallback_probs / fallback_probs.sum()\n        frame_probs[~valid_mask] = fallback_probs\n    \n    frame_preds = np.argmax(frame_probs, axis=1)\n    frame_max_probs = np.max(frame_probs, axis=1)\n    def test_preds_to_intervals(preds, probs, frames, video_behaviors, thresh=0.3):\n        intervals = []\n        global row_id\n        current_action_idx = None\n        start = None\n        frames = frames.reset_index(drop=True)  # Ensure index starts from 0\n        for i, (pred, prob, frame) in enumerate(zip(preds, probs, frames)):\n            if prob > thresh and pred != current_action_idx:\n                if current_action_idx is not None:\n                    action_str = global_le.inverse_transform([current_action_idx])[0]\n                    parts = action_str.split('_')\n                    if len(parts) == 3:\n                        agent, target, action = int(parts[0]), int(parts[1]), parts[2]\n                        start_frame = int(start * 10)\n                        stop_frame = int(frame * 10 - 1)\n                        if stop_frame >= start_frame and any(parse_behavior_label(b) == (agent, target, action) for b in video_behaviors):\n                            intervals.append({\n                                'row_id': row_id,\n                                'video_id': int(vid),  # Ensure integer type\n                                'agent_id': f'mouse{agent}',\n                                'target_id': f'mouse{target}',\n                                'action': action,\n                                'start_frame': start_frame,\n                                'stop_frame': stop_frame\n                            })\n                            row_id += 1\n                current_action_idx = pred\n                start = frame\n            elif prob <= thresh and current_action_idx is not None:\n                action_str = global_le.inverse_transform([current_action_idx])[0]\n                parts = action_str.split('_')\n                if len(parts) == 3:\n                    agent, target, action = int(parts[0]), int(parts[1]), parts[2]\n                    start_frame = int(start * 10)\n                    stop_frame = int(frame * 10 - 1)\n                    if stop_frame >= start_frame and any(parse_behavior_label(b) == (agent, target, action) for b in video_behaviors):\n                        intervals.append({\n                            'row_id': row_id,\n                            'video_id': int(vid),  # Ensure integer type\n                            'agent_id': f'mouse{agent}',\n                            'target_id': f'mouse{target}',\n                            'action': action,\n                            'start_frame': start_frame,\n                            'stop_frame': stop_frame\n                        })\n                        row_id += 1\n                current_action_idx = None\n        if current_action_idx is not None:\n            action_str = global_le.inverse_transform([current_action_idx])[0]\n            parts = action_str.split('_')\n            if len(parts) == 3:\n                agent, target, action = int(parts[0]), int(parts[1]), parts[2]\n                start_frame = int(start * 10)\n                stop_frame = int(frames.iloc[-1] * 10 - 1)\n                if stop_frame >= start_frame and any(parse_behavior_label(b) == (agent, target, action) for b in video_behaviors):\n                    intervals.append({\n                        'row_id': row_id,\n                        'video_id': int(vid),  # Ensure integer type\n                        'agent_id': f'mouse{agent}',\n                        'target_id': f'mouse{target}',\n                        'action': action,\n                        'start_frame': start_frame,\n                        'stop_frame': stop_frame\n                    })\n                    row_id += 1\n        return intervals\n    video_behaviors = test_row['behaviors_labeled']\n    pred_ints = test_preds_to_intervals(frame_preds, frame_max_probs, df_test['video_frame'], video_behaviors)\n    submission_rows.extend(pred_ints)\n    print(f\"Detected {len(pred_ints)} intervals for {vid}\")\n    \n    del df_test, df_features, df_test_full, group_sorted\n    gc.collect()\n# Final submission formatting to match competition requirements\nsubmission_df = pd.DataFrame(submission_rows)\nif len(submission_df) > 0:\n    submission_df = submission_df[['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']]\n    submission_df['video_id'] = submission_df['video_id'].astype(int)      # Ensure integer type\n    submission_df['row_id'] = submission_df['row_id'].astype(int)          # Ensure integer type\n    submission_df['start_frame'] = submission_df['start_frame'].astype(int) # Ensure integer type\n    submission_df['stop_frame'] = submission_df['stop_frame'].astype(int)   # Ensure integer type\n    submission_df.sort_values(['video_id', 'start_frame'], inplace=True)\n    submission_df['row_id'] = range(len(submission_df))  # Reset row_id to be sequential\nelse:\n    # Create empty submission with correct format if no predictions\n    submission_df = pd.DataFrame(columns=['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\nsubmission_df.to_csv(submission_path, index=False)\nprint(f\"Submission saved to {submission_path}\")\nprint(f\"Final submission shape: {submission_df.shape}\")\nprint(\"Submission columns and dtypes:\")\nprint(submission_df.dtypes)\nprint(submission_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T12:42:45.080836Z","iopub.execute_input":"2025-10-15T12:42:45.081113Z","iopub.status.idle":"2025-10-15T12:43:44.073439Z","shell.execute_reply.started":"2025-10-15T12:42:45.081088Z","shell.execute_reply":"2025-10-15T12:43:44.072031Z"}},"outputs":[],"execution_count":null}]}