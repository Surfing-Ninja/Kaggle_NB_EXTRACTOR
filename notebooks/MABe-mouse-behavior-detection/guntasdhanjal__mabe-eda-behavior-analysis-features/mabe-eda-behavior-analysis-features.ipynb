{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59156,"databundleVersionId":13874099,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ­ MABe 2025: Decoding Mouse Behavior - A Strategic EDA\n\n<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 15px; color: white; text-align: center;\">\n    <h2 style=\"margin: 0; font-size: 2.5em;\">ğŸ§¬ Social Action Recognition in Mice</h2>\n    <p style=\"font-size: 1.2em; margin-top: 10px;\">Strategic Analysis for Competitive Modeling</p>\n</div>\n\n---\n\n## ğŸ“‹ What Makes This EDA Different?\n\nUnlike traditional exploratory analysis, this notebook focuses on **actionable insights** that directly inform your modeling strategy:\n\nâœ¨ **Key Questions We Answer:**\n1. ğŸ¯ Which behaviors are actually predictable from pose data?\n2. ğŸ” What spatial patterns distinguish different actions?\n3. â±ï¸ How do temporal dynamics vary across behavior types?\n4. ğŸ—ï¸ Which features should you engineer for maximum impact?\n5. ğŸ§ª How do cross-lab differences affect generalization?\n\n---\n\n## ğŸ—ºï¸ Notebook Roadmap\n\n| Section | Focus | Modeling Impact |\n|---------|-------|-----------------|\n| **1. Smart Data Loading** | Efficient data structures | Faster iteration |\n| **2. Behavior Complexity Analysis** | Predictability scoring | Priority ranking |\n| **3. Spatial Intelligence** | Arena zone analysis | Location features |\n| **4. Temporal Patterns** | Sequence mining | Window sizing |\n| **5. Feature Engineering Lab** | Custom feature testing | Model input design |\n| **6. Cross-Lab Strategy** | Domain adaptation | Train/val splits |\n\nLet's dive in! ğŸš€","metadata":{}},{"cell_type":"markdown","source":"---\n\n# ğŸ¯ TL;DR - Key Findings & Quick Wins\n\n<div style=\"background-color: #fff3cd; padding: 20px; border-left: 5px solid #ffc107; border-radius: 5px; margin: 20px 0;\">\n\n## ğŸ”¥ **Top 3 Insights** (Read This First!)\n\n### 1ï¸âƒ£ **Extreme Class Imbalance**\n- ğŸ“Š **Sniff dominates:** 45% of all annotations (37,837 instances)\n- âš ï¸ **Rare behaviors:** Some have <100 examples (12,600:1 ratio!)\n- ğŸ’¡ **Action:** Use focal loss + class weights for rare behaviors\n\n### 2ï¸âƒ£ **Only 9.6% of Data Has Labels**\n- ğŸ“‰ **848 labeled** out of 8,789 videos\n- ğŸ¯ **Opportunity:** Use semi-supervised learning on unlabeled data\n- ğŸ’¡ **Action:** Consider self-supervised pretraining\n\n### 3ï¸âƒ£ **Cross-Lab Variance is HUGE**\n- ğŸ¢ Different tracking systems, body parts, annotation styles\n- âš ï¸ **Models overfit to CalMS21** (56% of labeled data)\n- ğŸ’¡ **Action:** Use GroupKFold by lab_id + domain adaptation\n\n</div>\n\n---\n\n## âš¡ **Quick Wins for Your Model**\n\n<div style=\"background-color: #d4edda; padding: 15px; border-left: 5px solid #28a745; border-radius: 5px; margin: 10px 0;\">\n\n### âœ… **Immediate Actions:**\n1. **Train/Val Split:** Use `GroupKFold(n_splits=5, by='lab_id')`\n2. **Class Weights:** Inverse frequency weighting (critical!)\n3. **Feature Windows:** Use [5, 10, 30, 60] frame windows\n4. **Adaptive Thresholds:** Learn per-behavior thresholds (not global)\n5. **Focus First:** Train on top 15 behaviors (cover 90% of data)\n\n### ğŸ“ˆ **Expected Impact:**\n- Baseline: ~0.38\n- + Class weights: ~0.40\n- + Adaptive thresholds: ~0.42\n- + Advanced features: ~0.43+\n\n</div>\n\n---\n\n## ğŸ“Š **What's in This Notebook:**\n\n| Section | What You'll Learn | Time to Read |\n|---------|-------------------|--------------|\n| **Behavior Complexity** | Which behaviors are predictable? Ranking by difficulty | 3 min |\n| **Spatial Patterns** | Where do behaviors occur? Heatmaps & zones | 4 min |\n| **Temporal Dynamics** | Behavior transitions & sequences | 3 min |\n| **Feature Engineering** | What features matter most? | 5 min |\n| **Cross-Lab Analysis** | How to handle domain shift | 4 min |\n| **Strategy Summary** | Complete modeling roadmap | 2 min |\n\n**Total reading time: ~20 minutes** â±ï¸\n\n**Can't read it all?** Jump directly to Section 6 (Strategy Summary) for the action plan! ğŸ‘‡\n\n---\n\n**Let's explore the data! ğŸš€**","metadata":{}},{"cell_type":"code","source":"# Core libraries\nimport numpy as np\nimport pandas as pd\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.patches import Circle, Rectangle, FancyBboxPatch\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib.patheffects as path_effects\n\n# Statistical & ML utilities\nfrom scipy import stats\nfrom scipy.spatial.distance import cdist, euclidean\nfrom scipy.ndimage import gaussian_filter1d\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['savefig.dpi'] = 100\nplt.rcParams['font.size'] = 10\n\n# Custom color palettes\nBEHAVIOR_COLORS = plt.cm.tab20(np.linspace(0, 1, 20))\nLAB_COLORS = plt.cm.Set3(np.linspace(0, 1, 12))\n\nprint(\"âœ… Libraries loaded successfully!\")\nprint(f\"ğŸ“Š Pandas: {pd.__version__} | NumPy: {np.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:00.06482Z","iopub.execute_input":"2025-11-04T12:52:00.065164Z","iopub.status.idle":"2025-11-04T12:52:02.026229Z","shell.execute_reply.started":"2025-11-04T12:52:00.06513Z","shell.execute_reply":"2025-11-04T12:52:02.025008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load metadata\ntrain_df = pd.read_csv('/kaggle/input/MABe-mouse-behavior-detection/train.csv')\ntest_df = pd.read_csv('/kaggle/input/MABe-mouse-behavior-detection/test.csv')\n\nprint(\"=\"*70)\nprint(\"ğŸ“¦ DATASET OVERVIEW\")\nprint(\"=\"*70)\nprint(f\"Training videos: {len(train_df):,}\")\nprint(f\"Test videos: {len(test_df):,}\")\nprint(f\"\\nğŸ¢ Unique laboratories: {train_df['lab_id'].nunique()}\")\nprint(f\"ğŸ¬ Total video hours: {train_df['video_duration_sec'].sum() / 3600:.1f}h\")\n\n# Identify annotated videos\nannotated_mask = train_df['behaviors_labeled'].notna()\nannotated_df = train_df[annotated_mask].copy()\n\nprint(f\"\\nâœ… Annotated videos: {len(annotated_df):,} ({len(annotated_df)/len(train_df)*100:.1f}%)\")\nprint(f\"âŒ Unannotated videos: {(~annotated_mask).sum():,}\")\n\n# Quick lab breakdown\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ”¬ TOP CONTRIBUTING LABS\")\nprint(\"=\"*70)\ntop_labs = annotated_df['lab_id'].value_counts().head(10)\nfor i, (lab, count) in enumerate(top_labs.items(), 1):\n    bar = \"â–ˆ\" * int(count / top_labs.max() * 30)\n    print(f\"{i:2d}. {lab:25s} â”‚ {bar:30s} â”‚ {count:3d} videos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:02.028581Z","iopub.execute_input":"2025-11-04T12:52:02.029033Z","iopub.status.idle":"2025-11-04T12:52:02.229365Z","shell.execute_reply.started":"2025-11-04T12:52:02.029001Z","shell.execute_reply":"2025-11-04T12:52:02.228256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# ğŸ¯ Section 1: Behavior Complexity & Predictability Analysis\n\n<div style=\"background-color: #f0f8ff; padding: 20px; border-left: 5px solid #4682b4; border-radius: 5px;\">\n    <h3>ğŸ¤” Research Question</h3>\n    <p><strong>Not all behaviors are equally predictable from pose data.</strong></p>\n    <p>We'll compute a \"predictability score\" based on:</p>\n    <ul>\n        <li>ğŸ² <strong>Frequency:</strong> Rare behaviors lack training examples</li>\n        <li>â±ï¸ <strong>Duration stability:</strong> Consistent durations â†’ easier detection</li>\n        <li>ğŸ­ <strong>Temporal pattern:</strong> Clustered vs scattered occurrences</li>\n        <li>ğŸ¢ <strong>Cross-lab presence:</strong> Universal behaviors generalize better</li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"# Collect all annotations from parquet files\nall_annotations = []\n\nprint(\"ğŸ”„ Loading annotation files...\")\nfor idx, row in annotated_df.iterrows():\n    lab_id = row['lab_id']\n    video_id = row['video_id']\n    \n    annotation_path = f'/kaggle/input/MABe-mouse-behavior-detection/train_annotation/{lab_id}/{video_id}.parquet'\n    \n    try:\n        annot = pd.read_parquet(annotation_path)\n        annot['video_id'] = video_id\n        annot['lab_id'] = lab_id\n        annot['fps'] = row['frames_per_second']\n        annot['duration_frames'] = annot['stop_frame'] - annot['start_frame']\n        annot['duration_seconds'] = annot['duration_frames'] / annot['fps']\n        all_annotations.append(annot)\n    except FileNotFoundError:\n        continue\n\nannotations_full = pd.concat(all_annotations, ignore_index=True)\n\nprint(f\"âœ… Loaded {len(annotations_full):,} behavioral annotations\")\nprint(f\"ğŸ“Š Unique behaviors: {annotations_full['action'].nunique()}\")\nprint(f\"ğŸ¬ Videos with annotations: {annotations_full['video_id'].nunique()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:02.23024Z","iopub.execute_input":"2025-11-04T12:52:02.23048Z","iopub.status.idle":"2025-11-04T12:52:10.686943Z","shell.execute_reply.started":"2025-11-04T12:52:02.230462Z","shell.execute_reply":"2025-11-04T12:52:10.685816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute predictability metrics for each behavior\nbehavior_stats = []\n\nfor behavior in annotations_full['action'].unique():\n    behavior_data = annotations_full[annotations_full['action'] == behavior]\n    \n    # Frequency metrics\n    total_occurrences = len(behavior_data)\n    num_videos = behavior_data['video_id'].nunique()\n    num_labs = behavior_data['lab_id'].nunique()\n    \n    # Duration metrics\n    mean_duration = behavior_data['duration_seconds'].mean()\n    std_duration = behavior_data['duration_seconds'].std()\n    cv_duration = std_duration / mean_duration if mean_duration > 0 else np.inf\n    \n    # Temporal clustering (using video-level variance)\n    video_counts = behavior_data.groupby('video_id').size()\n    clustering_score = video_counts.std() / video_counts.mean() if len(video_counts) > 1 else 0\n    \n    # Predictability score (0-100)\n    # Higher = easier to predict\n    frequency_score = min(np.log10(total_occurrences + 1) / 4 * 100, 100)\n    stability_score = max(0, 100 - cv_duration * 20)\n    generalization_score = (num_labs / annotated_df['lab_id'].nunique()) * 100\n    \n    predictability = (frequency_score * 0.4 + stability_score * 0.3 + generalization_score * 0.3)\n    \n    behavior_stats.append({\n        'behavior': behavior,\n        'count': total_occurrences,\n        'videos': num_videos,\n        'labs': num_labs,\n        'mean_duration_sec': mean_duration,\n        'std_duration_sec': std_duration,\n        'cv_duration': cv_duration,\n        'clustering': clustering_score,\n        'predictability_score': predictability\n    })\n\nbehavior_df = pd.DataFrame(behavior_stats).sort_values('predictability_score', ascending=False)\n\nprint(\"ğŸ¯ Behavior Predictability Ranking (Top 10 Most Predictable)\")\nprint(\"=\"*80)\nfor i, row in behavior_df.head(10).iterrows():\n    print(f\"{row['behavior']:20s} â”‚ Score: {row['predictability_score']:5.1f} â”‚ \"\n          f\"Count: {row['count']:6,d} â”‚ Labs: {row['labs']:2d} â”‚ \"\n          f\"Duration: {row['mean_duration_sec']:5.2f}s Â± {row['std_duration_sec']:5.2f}s\")\n\nprint(\"\\nâš ï¸ Challenging Behaviors (Bottom 5)\")\nprint(\"=\"*80)\nfor i, row in behavior_df.tail(5).iterrows():\n    print(f\"{row['behavior']:20s} â”‚ Score: {row['predictability_score']:5.1f} â”‚ \"\n          f\"Count: {row['count']:6,d} â”‚ Labs: {row['labs']:2d}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:10.688463Z","iopub.execute_input":"2025-11-04T12:52:10.688762Z","iopub.status.idle":"2025-11-04T12:52:11.046727Z","shell.execute_reply.started":"2025-11-04T12:52:10.688728Z","shell.execute_reply":"2025-11-04T12:52:11.045515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 10))\ngs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n\n# Plot 1: Predictability Score Distribution\nax1 = fig.add_subplot(gs[0, 0])\ntop_15 = behavior_df.nlargest(15, 'predictability_score')\n\ncolors = plt.cm.RdYlGn(top_15['predictability_score'] / 100)\nbars = ax1.barh(range(len(top_15)), top_15['predictability_score'], color=colors, edgecolor='black', linewidth=1.2)\nax1.set_yticks(range(len(top_15)))\nax1.set_yticklabels(top_15['behavior'], fontsize=10)\nax1.set_xlabel('Predictability Score', fontsize=12, fontweight='bold')\nax1.set_title('ğŸ¯ Top 15 Most Predictable Behaviors', fontsize=13, fontweight='bold', pad=15)\nax1.axvline(50, color='red', linestyle='--', alpha=0.5, linewidth=2, label='Threshold')\nax1.grid(axis='x', alpha=0.3)\nax1.legend()\n\nfor i, (score, count) in enumerate(zip(top_15['predictability_score'], top_15['count'])):\n    ax1.text(score + 1, i, f\"{count:,}\", va='center', fontsize=9, fontweight='bold')\n\n# Plot 2: Frequency vs Duration Stability\nax2 = fig.add_subplot(gs[0, 1])\nscatter_data = behavior_df[behavior_df['count'] >= 10]  # Filter rare behaviors for clarity\n\nscatter = ax2.scatter(scatter_data['count'], \n                     scatter_data['cv_duration'],\n                     s=scatter_data['labs'] * 30,\n                     c=scatter_data['predictability_score'],\n                     cmap='viridis',\n                     alpha=0.6,\n                     edgecolors='black',\n                     linewidth=1)\n\nax2.set_xscale('log')\nax2.set_xlabel('Frequency (log scale)', fontsize=12, fontweight='bold')\nax2.set_ylabel('Duration Variability (CV)', fontsize=12, fontweight='bold')\nax2.set_title('ğŸ“Š Behavior Characteristics Map', fontsize=13, fontweight='bold', pad=15)\nax2.grid(True, alpha=0.3)\n\n# Annotate interesting behaviors\nfor _, row in scatter_data.nlargest(5, 'predictability_score').iterrows():\n    ax2.annotate(row['behavior'][:8], \n                (row['count'], row['cv_duration']),\n                fontsize=8, \n                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n\ncbar = plt.colorbar(scatter, ax=ax2)\ncbar.set_label('Predictability Score', fontsize=10, fontweight='bold')\n\n# Plot 3: Duration Distribution for Top Behaviors\nax3 = fig.add_subplot(gs[1, :])\ntop_behaviors = behavior_df.nlargest(8, 'count')['behavior'].tolist()\n\nduration_data = []\nlabels = []\nfor beh in top_behaviors:\n    durations = annotations_full[annotations_full['action'] == beh]['duration_seconds'].values\n    # Cap at 99th percentile to avoid outliers\n    durations_capped = durations[durations <= np.percentile(durations, 99)]\n    duration_data.append(durations_capped)\n    labels.append(f\"{beh}\\n(n={len(durations)})\")\n\nbp = ax3.boxplot(duration_data, \n                 labels=labels,\n                 patch_artist=True,\n                 showfliers=False,\n                 medianprops=dict(color='red', linewidth=2),\n                 boxprops=dict(facecolor='lightblue', edgecolor='black', linewidth=1.5),\n                 whiskerprops=dict(linewidth=1.5),\n                 capprops=dict(linewidth=1.5))\n\nax3.set_ylabel('Duration (seconds)', fontsize=12, fontweight='bold')\nax3.set_title('â±ï¸ Duration Distributions for Common Behaviors', fontsize=13, fontweight='bold', pad=15)\nax3.grid(axis='y', alpha=0.3)\nax3.set_ylim(0, ax3.get_ylim()[1])\n\nplt.suptitle('Behavior Complexity & Predictability Analysis', \n             fontsize=16, fontweight='bold', y=0.995)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:11.048094Z","iopub.execute_input":"2025-11-04T12:52:11.048347Z","iopub.status.idle":"2025-11-04T12:52:12.528142Z","shell.execute_reply.started":"2025-11-04T12:52:11.048326Z","shell.execute_reply":"2025-11-04T12:52:12.526997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸ’¡ Key Insights from Behavior Analysis\n\n<div style=\"background-color: #fff3cd; padding: 15px; border-left: 5px solid #ffc107; border-radius: 5px; margin: 10px 0;\">\n    <h4>ğŸ¯ Modeling Recommendations:</h4>\n    <ol>\n        <li><strong>Focus on high-predictability behaviors first</strong> - Build strong baseline with top 15 behaviors</li>\n        <li><strong>Duration-based features are critical</strong> - Most behaviors have characteristic time signatures</li>\n        <li><strong>Rare behavior strategy needed</strong> - Behaviors with <100 examples need special handling (augmentation, focal loss)</li>\n        <li><strong>Cross-lab presence matters</strong> - Prioritize behaviors appearing in 5+ labs for robust models</li>\n    </ol>\n</div>\n\n**Next:** Let's explore spatial patterns! ğŸ—ºï¸","metadata":{}},{"cell_type":"markdown","source":"---\n# ğŸ—ºï¸ Section 2: Spatial Intelligence - Where Do Behaviors Happen?\n\n<div style=\"background-color: #e7f3ff; padding: 20px; border-left: 5px solid #2196F3; border-radius: 5px;\">\n    <h3>ğŸ§­ Hypothesis</h3>\n    <p><strong>Different behaviors occur in different arena locations.</strong></p>\n    <p>Understanding spatial preferences enables:</p>\n    <ul>\n        <li>ğŸ“ Location-based features (distance to walls, corners, center)</li>\n        <li>ğŸ¯ Zone-specific behavior priors</li>\n        <li>ğŸ” Arena normalization strategies</li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"# Sample diverse videos for spatial analysis\ndef load_tracking_sample(num_videos=15):\n    \"\"\"Load tracking data from diverse sources\"\"\"\n    tracking_samples = []\n    \n    # Sample from different labs\n    sampled_videos = annotated_df.groupby('lab_id').apply(\n        lambda x: x.sample(min(2, len(x)), random_state=42)\n    ).reset_index(drop=True)[:num_videos]\n    \n    for idx, row in sampled_videos.iterrows():\n        lab_id = row['lab_id']\n        video_id = row['video_id']\n        \n        tracking_path = f'/kaggle/input/MABe-mouse-behavior-detection/train_tracking/{lab_id}/{video_id}.parquet'\n        annotation_path = f'/kaggle/input/MABe-mouse-behavior-detection/train_annotation/{lab_id}/{video_id}.parquet'\n        \n        try:\n            tracking = pd.read_parquet(tracking_path)\n            annotations = pd.read_parquet(annotation_path)\n            \n            # Normalize coordinates\n            tracking['x_norm'] = tracking['x'] / row['pix_per_cm_approx']\n            tracking['y_norm'] = tracking['y'] / row['pix_per_cm_approx']\n            \n            tracking['video_id'] = video_id\n            tracking['lab_id'] = lab_id\n            tracking['arena_width'] = row['arena_width_cm']\n            tracking['arena_height'] = row['arena_height_cm']\n            \n            tracking_samples.append({\n                'tracking': tracking,\n                'annotations': annotations,\n                'metadata': row\n            })\n            \n            print(f\"âœ“ Loaded: {lab_id[:20]:20s} | Video {video_id} | {len(tracking):,} frames\")\n            \n        except Exception as e:\n            continue\n    \n    return tracking_samples\n\nprint(\"ğŸ”„ Loading tracking data samples...\")\ntracking_data_samples = load_tracking_sample(num_videos=15)\nprint(f\"\\nâœ… Successfully loaded {len(tracking_data_samples)} videos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:12.531004Z","iopub.execute_input":"2025-11-04T12:52:12.531365Z","iopub.status.idle":"2025-11-04T12:52:15.894218Z","shell.execute_reply.started":"2025-11-04T12:52:12.531338Z","shell.execute_reply":"2025-11-04T12:52:15.892653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze spatial patterns for behaviors\ndef compute_spatial_features(tracking_samples):\n    \"\"\"Compute where behaviors occur in the arena\"\"\"\n    behavior_locations = defaultdict(list)\n    \n    for sample in tracking_samples:\n        tracking = sample['tracking']\n        annotations = sample['annotations']\n        metadata = sample['metadata']\n        \n        arena_w = metadata['arena_width_cm']\n        arena_h = metadata['arena_height_cm']\n        \n        # Get body center positions\n        body_center = tracking[tracking['bodypart'] == 'body_center'].copy()\n        \n        if body_center.empty:\n            # Fallback to any available bodypart\n            available_parts = tracking['bodypart'].unique()\n            if len(available_parts) > 0:\n                body_center = tracking[tracking['bodypart'] == available_parts[0]].copy()\n            else:\n                continue\n        \n        # Process each annotation\n        for _, ann in annotations.iterrows():\n            action = ann['action']\n            agent_id = ann['agent_id']\n            start_frame = ann['start_frame']\n            stop_frame = ann['stop_frame']\n            \n            # Get positions during this behavior\n            behavior_positions = body_center[\n                (body_center['mouse_id'] == agent_id) &\n                (body_center['video_frame'] >= start_frame) &\n                (body_center['video_frame'] <= stop_frame)\n            ]\n            \n            if len(behavior_positions) > 0:\n                # Compute spatial metrics (normalized to arena)\n                x_norm = behavior_positions['x_norm'].mean() / arena_w\n                y_norm = behavior_positions['y_norm'].mean() / arena_h\n                \n                # Distance to center\n                center_dist = np.sqrt((x_norm - 0.5)**2 + (y_norm - 0.5)**2)\n                \n                # Distance to nearest wall\n                wall_dist = min(x_norm, 1-x_norm, y_norm, 1-y_norm)\n                \n                behavior_locations[action].append({\n                    'x': x_norm,\n                    'y': y_norm,\n                    'center_dist': center_dist,\n                    'wall_dist': wall_dist\n                })\n    \n    return behavior_locations\n\nprint(\"ğŸ§® Computing spatial patterns...\")\nspatial_patterns = compute_spatial_features(tracking_data_samples)\n\n# Summarize\nprint(\"\\nğŸ“Š Spatial Pattern Summary:\")\nprint(\"=\"*70)\nfor behavior in sorted(spatial_patterns.keys())[:10]:\n    locs = spatial_patterns[behavior]\n    if len(locs) >= 5:\n        avg_center_dist = np.mean([l['center_dist'] for l in locs])\n        avg_wall_dist = np.mean([l['wall_dist'] for l in locs])\n        print(f\"{behavior:20s} â”‚ Samples: {len(locs):4d} â”‚ \"\n              f\"Center dist: {avg_center_dist:.3f} â”‚ Wall dist: {avg_wall_dist:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:15.895021Z","iopub.execute_input":"2025-11-04T12:52:15.895298Z","iopub.status.idle":"2025-11-04T12:52:20.581765Z","shell.execute_reply.started":"2025-11-04T12:52:15.895276Z","shell.execute_reply":"2025-11-04T12:52:20.580508Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\nâš ï¸ **Note on Negative Wall Distances:** \n\nThe negative values indicate coordinate normalization issues in some videos. This happens when:\n- Arena boundaries aren't perfectly detected\n- Tracking extends slightly outside arena  \n- Different labs use different coordinate systems\n\n**In your model:** Always normalize coordinates by `pix_per_cm_approx` AND clip to valid arena bounds!\n```python\n# Proper normalization - use this pattern!\nx_norm = np.clip(x / metadata['pix_per_cm_approx'], 0, metadata['arena_width_cm'])\ny_norm = np.clip(y / metadata['pix_per_cm_approx'], 0, metadata['arena_height_cm'])","metadata":{}},{"cell_type":"code","source":"# Visualize where behaviors occur\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\naxes = axes.flatten()\n\n# Select top behaviors by sample count\ntop_spatial_behaviors = sorted(spatial_patterns.items(), \n                               key=lambda x: len(x[1]), \n                               reverse=True)[:9]\n\nfor idx, (behavior, locations) in enumerate(top_spatial_behaviors):\n    ax = axes[idx]\n    \n    if len(locations) == 0:\n        continue\n    \n    # Extract coordinates\n    x_coords = [loc['x'] for loc in locations]\n    y_coords = [loc['y'] for loc in locations]\n    \n    # Create 2D histogram (heatmap)\n    heatmap, xedges, yedges = np.histogram2d(x_coords, y_coords, bins=20, range=[[0, 1], [0, 1]])\n    \n    # Smooth heatmap\n    heatmap_smooth = gaussian_filter1d(gaussian_filter1d(heatmap, sigma=1, axis=0), sigma=1, axis=1)\n    \n    # Plot\n    im = ax.imshow(heatmap_smooth.T, origin='lower', extent=[0, 1, 0, 1], \n                   cmap='hot', aspect='auto', alpha=0.8)\n    \n    # Draw arena boundary\n    arena_rect = Rectangle((0, 0), 1, 1, linewidth=3, edgecolor='cyan', facecolor='none')\n    ax.add_patch(arena_rect)\n    \n    # Mark center\n    ax.plot(0.5, 0.5, 'g*', markersize=15, markeredgecolor='white', markeredgewidth=1.5)\n    \n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect('equal')\n    ax.set_title(f'{behavior}\\n(n={len(locations)})', fontsize=11, fontweight='bold')\n    ax.set_xlabel('Normalized X')\n    ax.set_ylabel('Normalized Y')\n    \n    # Add colorbar\n    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\nplt.suptitle('ğŸ—ºï¸ Spatial Distribution of Behaviors in Arena', \n             fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:20.583544Z","iopub.execute_input":"2025-11-04T12:52:20.584052Z","iopub.status.idle":"2025-11-04T12:52:23.203222Z","shell.execute_reply.started":"2025-11-04T12:52:20.584021Z","shell.execute_reply":"2025-11-04T12:52:23.202022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# â±ï¸ Section 3: Temporal Dynamics - Behavior Sequences & Transitions\n\n<div style=\"background-color: #f3e5f5; padding: 20px; border-left: 5px solid #9c27b0; border-radius: 5px;\">\n    <h3>â³ Goal</h3>\n    <p><strong>Discover temporal patterns in behavior sequences.</strong></p>\n    <p>Key questions:</p>\n    <ul>\n        <li>ğŸ”„ Which behaviors frequently follow each other?</li>\n        <li>ğŸ“ˆ Are there predictable behavior chains?</li>\n        <li>âš¡ How quickly do behaviors transition?</li>\n    </ul>\n    <p>This informs: temporal modeling, sequence features, and window sizing.</p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Compute behavior transitions\ndef extract_behavior_sequences(annotations_df, min_videos=3):\n    \"\"\"Extract behavior transition patterns\"\"\"\n    \n    # Group by video and agent\n    sequences = []\n    \n    for (video_id, agent_id), group in annotations_df.groupby(['video_id', 'agent_id']):\n        # Sort by time\n        group_sorted = group.sort_values('start_frame')\n        \n        behaviors = group_sorted['action'].tolist()\n        timestamps = group_sorted['start_frame'].tolist()\n        \n        if len(behaviors) >= 2:\n            sequences.append({\n                'video_id': video_id,\n                'agent_id': agent_id,\n                'behaviors': behaviors,\n                'timestamps': timestamps\n            })\n    \n    # Compute transitions\n    transitions = defaultdict(int)\n    \n    for seq in sequences:\n        behaviors = seq['behaviors']\n        for i in range(len(behaviors) - 1):\n            transition = (behaviors[i], behaviors[i+1])\n            transitions[transition] += 1\n    \n    return sequences, transitions\n\nprint(\"ğŸ”„ Analyzing behavior sequences...\")\nsequences, transitions = extract_behavior_sequences(annotations_full)\n\nprint(f\"âœ… Extracted {len(sequences):,} behavior sequences\")\nprint(f\"ğŸ“Š Unique transitions: {len(transitions):,}\")\n\n# Top transitions\nprint(\"\\nğŸ” Most Common Behavior Transitions:\")\nprint(\"=\"*70)\nsorted_transitions = sorted(transitions.items(), key=lambda x: x[1], reverse=True)\nfor i, ((from_beh, to_beh), count) in enumerate(sorted_transitions[:15], 1):\n    print(f\"{i:2d}. {from_beh:15s} â†’ {to_beh:15s} â”‚ {count:4d} times\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:23.204381Z","iopub.execute_input":"2025-11-04T12:52:23.204769Z","iopub.status.idle":"2025-11-04T12:52:23.718212Z","shell.execute_reply.started":"2025-11-04T12:52:23.204737Z","shell.execute_reply":"2025-11-04T12:52:23.716934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create transition matrix for top behaviors\ntop_behaviors_for_matrix = behavior_df.nlargest(12, 'count')['behavior'].tolist()\n\n# Build transition matrix\ntransition_matrix = np.zeros((len(top_behaviors_for_matrix), len(top_behaviors_for_matrix)))\n\nfor (from_beh, to_beh), count in transitions.items():\n    if from_beh in top_behaviors_for_matrix and to_beh in top_behaviors_for_matrix:\n        i = top_behaviors_for_matrix.index(from_beh)\n        j = top_behaviors_for_matrix.index(to_beh)\n        transition_matrix[i, j] = count\n\n# Normalize by row (probability of transitioning to next behavior)\nrow_sums = transition_matrix.sum(axis=1, keepdims=True)\ntransition_probs = np.divide(transition_matrix, row_sums, \n                             where=row_sums!=0, \n                             out=np.zeros_like(transition_matrix))\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n\n# Raw counts\nim1 = ax1.imshow(transition_matrix, cmap='YlOrRd', aspect='auto')\nax1.set_xticks(range(len(top_behaviors_for_matrix)))\nax1.set_yticks(range(len(top_behaviors_for_matrix)))\nax1.set_xticklabels(top_behaviors_for_matrix, rotation=45, ha='right', fontsize=9)\nax1.set_yticklabels(top_behaviors_for_matrix, fontsize=9)\nax1.set_xlabel('To Behavior', fontsize=12, fontweight='bold')\nax1.set_ylabel('From Behavior', fontsize=12, fontweight='bold')\nax1.set_title('Behavior Transition Counts', fontsize=13, fontweight='bold', pad=15)\nplt.colorbar(im1, ax=ax1, label='Count')\n\n# Add text annotations for high values\nfor i in range(len(top_behaviors_for_matrix)):\n    for j in range(len(top_behaviors_for_matrix)):\n        if transition_matrix[i, j] >= 10:\n            text = ax1.text(j, i, int(transition_matrix[i, j]),\n                          ha=\"center\", va=\"center\", color=\"white\", fontsize=8, fontweight='bold')\n\n# Probabilities\nim2 = ax2.imshow(transition_probs, cmap='Blues', aspect='auto', vmin=0, vmax=0.5)\nax2.set_xticks(range(len(top_behaviors_for_matrix)))\nax2.set_yticks(range(len(top_behaviors_for_matrix)))\nax2.set_xticklabels(top_behaviors_for_matrix, rotation=45, ha='right', fontsize=9)\nax2.set_yticklabels(top_behaviors_for_matrix, fontsize=9)\nax2.set_xlabel('To Behavior', fontsize=12, fontweight='bold')\nax2.set_ylabel('From Behavior', fontsize=12, fontweight='bold')\nax2.set_title('Transition Probabilities', fontsize=13, fontweight='bold', pad=15)\nplt.colorbar(im2, ax=ax2, label='Probability')\n\n# Add text for high probabilities\nfor i in range(len(top_behaviors_for_matrix)):\n    for j in range(len(top_behaviors_for_matrix)):\n        if transition_probs[i, j] >= 0.1:\n            text = ax2.text(j, i, f'{transition_probs[i, j]:.2f}',\n                          ha=\"center\", va=\"center\", color=\"white\", fontsize=8, fontweight='bold')\n\nplt.suptitle('ğŸ”„ Behavior Transition Analysis', fontsize=16, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:23.719209Z","iopub.execute_input":"2025-11-04T12:52:23.719489Z","iopub.status.idle":"2025-11-04T12:52:24.981377Z","shell.execute_reply.started":"2025-11-04T12:52:23.719456Z","shell.execute_reply":"2025-11-04T12:52:24.980058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# ğŸ”¬ Section 4: Feature Engineering Laboratory\n\n<div style=\"background-color: #e8f5e9; padding: 20px; border-left: 5px solid #4caf50; border-radius: 5px;\">\n    <h3>ğŸ› ï¸ Objective</h3>\n    <p><strong>Test and visualize candidate features for behavior classification.</strong></p>\n    <p>We'll engineer and evaluate:</p>\n    <ul>\n        <li>ğŸ“ Distance features (inter-mouse, to walls)</li>\n        <li>ğŸƒ Velocity & acceleration features</li>\n        <li>ğŸ“ Angular features (body orientation, turning)</li>\n        <li>ğŸ”„ Temporal features (rolling statistics)</li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"def engineer_features_from_tracking(tracking_df, window_sizes=[5, 10, 30]):\n    \"\"\"\n    Comprehensive feature engineering from pose tracking data\n    \"\"\"\n    features_list = []\n    \n    # Process each mouse separately\n    for mouse_id in tracking_df['mouse_id'].unique():\n        mouse_data = tracking_df[tracking_df['mouse_id'] == mouse_id].copy()\n        \n        # Get body center (or fallback)\n        body_parts = ['body_center', 'nose', 'head']\n        center_data = None\n        for part in body_parts:\n            temp = mouse_data[mouse_data['bodypart'] == part].copy()\n            if len(temp) > 0:\n                center_data = temp.sort_values('video_frame')\n                break\n        \n        if center_data is None or len(center_data) < 10:\n            continue\n        \n        # Basic position\n        x = center_data['x_norm'].values\n        y = center_data['y_norm'].values\n        frames = center_data['video_frame'].values\n        \n        # Velocity (first derivative)\n        vx = np.diff(x, prepend=x[0])\n        vy = np.diff(y, prepend=y[0])\n        speed = np.sqrt(vx**2 + vy**2)\n        \n        # Acceleration (second derivative)\n        ax = np.diff(vx, prepend=vx[0])\n        ay = np.diff(vy, prepend=vy[0])\n        acceleration = np.sqrt(ax**2 + ay**2)\n        \n        # Angular features\n        angle = np.arctan2(vy, vx)\n        angle_change = np.abs(np.diff(angle, prepend=angle[0]))\n        angle_change = np.where(angle_change > np.pi, 2*np.pi - angle_change, angle_change)\n        \n        # Distance to center\n        arena_w = center_data['arena_width'].iloc[0]\n        arena_h = center_data['arena_height'].iloc[0]\n        center_x, center_y = arena_w / 2, arena_h / 2\n        dist_to_center = np.sqrt((x - center_x/arena_w)**2 + (y - center_y/arena_h)**2)\n        \n        # Distance to walls (normalized)\n        dist_to_wall = np.minimum.reduce([\n            x, \n            1 - x, \n            y, \n            1 - y\n        ])\n        \n        # Rolling statistics for multiple windows\n        feature_dict = {\n            'video_id': center_data['video_id'].iloc[0],\n            'mouse_id': mouse_id,\n            'frame': frames,\n            'x': x,\n            'y': y,\n            'speed': speed,\n            'acceleration': acceleration,\n            'angle_change': angle_change,\n            'dist_to_center': dist_to_center,\n            'dist_to_wall': dist_to_wall\n        }\n        \n        # Add rolling features\n        for window in window_sizes:\n            feature_dict[f'speed_mean_{window}'] = pd.Series(speed).rolling(window, min_periods=1, center=True).mean().values\n            feature_dict[f'speed_std_{window}'] = pd.Series(speed).rolling(window, min_periods=1, center=True).std().fillna(0).values\n            feature_dict[f'acceleration_mean_{window}'] = pd.Series(acceleration).rolling(window, min_periods=1, center=True).mean().values\n            feature_dict[f'angle_change_sum_{window}'] = pd.Series(angle_change).rolling(window, min_periods=1, center=True).sum().values\n        \n        features_df = pd.DataFrame(feature_dict)\n        features_list.append(features_df)\n    \n    if len(features_list) > 0:\n        return pd.concat(features_list, ignore_index=True)\n    return pd.DataFrame()\n\nprint(\"ğŸ”§ Engineering features from tracking data...\")\nprint(\"This may take a minute...\")\n\n# Engineer features for sample videos\nall_features = []\nfor i, sample in enumerate(tracking_data_samples[:10]):\n    features = engineer_features_from_tracking(sample['tracking'])\n    if len(features) > 0:\n        all_features.append(features)\n    print(f\"âœ“ Processed video {i+1}/10\")\n\nif len(all_features) > 0:\n    combined_features = pd.concat(all_features, ignore_index=True)\n    print(f\"\\nâœ… Generated {len(combined_features):,} feature vectors\")\n    print(f\"ğŸ“Š Feature columns: {len(combined_features.columns)}\")\nelse:\n    print(\"âš ï¸ No features generated\")\n    combined_features = pd.DataFrame()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:24.982875Z","iopub.execute_input":"2025-11-04T12:52:24.983432Z","iopub.status.idle":"2025-11-04T12:52:32.550953Z","shell.execute_reply.started":"2025-11-04T12:52:24.9834Z","shell.execute_reply":"2025-11-04T12:52:32.549922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge features with annotations to see feature distributions per behavior\nif len(combined_features) > 0 and len(tracking_data_samples) > 0:\n    \n    # Collect annotated features\n    behavior_features = []\n    \n    for sample in tracking_data_samples[:10]:\n        annotations = sample['annotations']\n        video_id = sample['metadata']['video_id']\n        \n        video_features = combined_features[combined_features['video_id'] == video_id]\n        \n        for _, ann in annotations.iterrows():\n            agent_id = ann['agent_id']\n            action = ann['action']\n            start = ann['start_frame']\n            stop = ann['stop_frame']\n            \n            # Get features for this behavior window\n            mask = (\n                (video_features['mouse_id'] == agent_id) &\n                (video_features['frame'] >= start) &\n                (video_features['frame'] <= stop)\n            )\n            \n            behavior_window = video_features[mask].copy()\n            \n            if len(behavior_window) > 0:\n                behavior_window['behavior'] = action\n                behavior_features.append(behavior_window)\n    \n    if len(behavior_features) > 0:\n        behavior_features_df = pd.concat(behavior_features, ignore_index=True)\n        \n        print(f\"âœ… Extracted features for {len(behavior_features_df):,} annotated frames\")\n        print(f\"ğŸ­ Behaviors represented: {behavior_features_df['behavior'].nunique()}\")\n        \n        # Select top behaviors for visualization\n        top_behaviors_for_features = behavior_features_df['behavior'].value_counts().head(6).index.tolist()\n        \n        # Plot feature distributions\n        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n        axes = axes.flatten()\n        \n        feature_cols = ['speed', 'acceleration', 'angle_change', 'dist_to_center', 'speed_mean_10', 'speed_std_30']\n        \n        for idx, feat in enumerate(feature_cols):\n            ax = axes[idx]\n            \n            for behavior in top_behaviors_for_features:\n                data = behavior_features_df[behavior_features_df['behavior'] == behavior][feat].dropna()\n                \n                # Remove extreme outliers for visualization\n                data = data[data <= data.quantile(0.99)]\n                \n                if len(data) > 10:\n                    ax.hist(data, bins=30, alpha=0.5, label=behavior, density=True)\n            \n            ax.set_xlabel(feat.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n            ax.set_ylabel('Density', fontsize=11, fontweight='bold')\n            ax.legend(fontsize=8, loc='upper right')\n            ax.grid(alpha=0.3)\n            ax.set_title(f'Distribution: {feat}', fontsize=12, fontweight='bold')\n        \n        plt.suptitle('ğŸ”¬ Feature Distributions Across Behaviors', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        \n        # Feature separability analysis\n        print(\"\\n\" + \"=\"*70)\n        print(\"ğŸ“Š FEATURE SEPARABILITY ANALYSIS\")\n        print(\"=\"*70)\n        \n        for feat in feature_cols:\n            # Compute between-behavior variance\n            behavior_means = []\n            for behavior in top_behaviors_for_features:\n                data = behavior_features_df[behavior_features_df['behavior'] == behavior][feat].dropna()\n                if len(data) > 0:\n                    behavior_means.append(data.mean())\n            \n            if len(behavior_means) > 1:\n                separability = np.std(behavior_means) / (np.mean(behavior_means) + 1e-6)\n                print(f\"{feat:25s} â”‚ Separability: {separability:.4f}\")\n    else:\n        print(\"âš ï¸ No behavior-feature mapping created\")\nelse:\n    print(\"âš ï¸ Skipping feature analysis - insufficient data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:32.552014Z","iopub.execute_input":"2025-11-04T12:52:32.552334Z","iopub.status.idle":"2025-11-04T12:52:39.037972Z","shell.execute_reply.started":"2025-11-04T12:52:32.552299Z","shell.execute_reply":"2025-11-04T12:52:39.036707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### ğŸ’¡ **What Does Separability Mean?**\n\n**Separability Score = How well this feature distinguishes behaviors**\n\n| Score Range | Quality | Action |\n|-------------|---------|--------|\n| **>0.3** | ğŸŸ¢ **Excellent** | Use as primary feature! |\n| **0.15-0.3** | ğŸŸ¡ **Good** | Combine with others |\n| **<0.15** | ğŸ”´ **Weak** | May not help much |\n\n**Key Findings:**\n- âœ… `angle_change` is best (0.28) - **turning behavior matters!**\n- âœ… `speed` is good (0.24) - **movement speed separates actions**\n- âœ… `acceleration` is decent (0.21) - **sudden movements are informative**\n- âš ï¸ `speed_std_30` is weak (0.09) - **short-term variance doesn't help much**\n\n**Recommendation:** Focus on **angular features** and **instantaneous velocity**, not just speed variance. Consider adding:\n- Direction change over longer windows (60+ frames)\n- Acceleration patterns\n- Velocity alignment between mice\n\n---","metadata":{}},{"cell_type":"markdown","source":"---\n# ğŸ¢ Section 5: Cross-Laboratory Domain Analysis\n\n<div style=\"background-color: #fff3e0; padding: 20px; border-left: 5px solid #ff9800; border-radius: 5px;\">\n    <h3>ğŸŒ Challenge</h3>\n    <p><strong>Models must generalize across laboratories with different setups.</strong></p>\n    <p>We'll analyze:</p>\n    <ul>\n        <li>ğŸ¥ Tracking system differences</li>\n        <li>ğŸ“Š Body part availability variations</li>\n        <li>ğŸ¯ Behavior annotation consistency</li>\n        <li>ğŸ“ Coordinate system normalization needs</li>\n    </ul>\n    <p><strong>Goal:</strong> Design domain adaptation strategies.</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"---\n\n### ğŸ’¡ **Key Observations from Lab Profiles**\n\n#### ğŸ† **Dominant Labs:**\n- **CalMS21 family** (474 videos, 56% of data) - Uses MARS tracking\n- **SparklingTapir** (54 videos) - DeepLabCut, variable FPS (30-120)\n- **JovialSwallow** (52 videos) - SLEAP, low FPS (10fps)\n\n#### ğŸ¯ **Critical Variations:**\n\n| Feature | Range | Impact on Model |\n|---------|-------|-----------------|\n| **FPS** | 10-120 fps | âš ï¸ Must normalize by time, not frames! |\n| **Body Parts** | 4-18 parts | âš ï¸ Need fallback logic for missing parts |\n| **Arena Size** | 15-120 cm | âš ï¸ Always normalize coordinates |\n| **Tracking Method** | MARS, DeepLabCut, SLEAP | âš ï¸ Different noise profiles |\n\n#### ğŸš¨ **Watch Out For:**\n- `split rectangluar` typo in PleasantMeerkat (should be \"rectangular\")\n- AdaptableSnail has 18 body parts (most detailed tracking)\n- BoisterousParrot has 44 hours from just 8 videos (very long recordings!)\n\n---","metadata":{}},{"cell_type":"code","source":"# Comprehensive lab profiling\nlab_profiles = []\n\nfor lab_id in annotated_df['lab_id'].unique():\n    lab_data = annotated_df[annotated_df['lab_id'] == lab_id]\n    \n    # Basic stats\n    num_videos = len(lab_data)\n    total_duration = lab_data['video_duration_sec'].sum()\n    \n    # Tracking info\n    tracking_methods = lab_data['tracking_method'].unique()\n    body_parts_sets = lab_data['body_parts_tracked'].unique()\n    \n    # Parse body parts\n    all_body_parts = set()\n    for bp_str in body_parts_sets:\n        try:\n            parts = json.loads(bp_str)\n            all_body_parts.update(parts)\n        except:\n            pass\n    \n    # Arena info\n    arena_shapes = lab_data['arena_shape'].unique()\n    avg_arena_size = lab_data['arena_width_cm'].mean()\n    \n    # Behavior info\n    all_behaviors = set()\n    for behaviors_str in lab_data['behaviors_labeled'].dropna():\n        try:\n            behaviors = json.loads(behaviors_str)\n            for b in behaviors:\n                # Parse \"mouse1,mouse2,action\" format\n                parts = b.split(',')\n                if len(parts) >= 3:\n                    all_behaviors.add(parts[2])\n        except:\n            pass\n    \n    # FPS\n    fps_values = lab_data['frames_per_second'].unique()\n    avg_fps = lab_data['frames_per_second'].mean()\n    \n    lab_profiles.append({\n        'lab_id': lab_id,\n        'num_videos': num_videos,\n        'total_hours': total_duration / 3600,\n        'tracking_methods': ', '.join(tracking_methods),\n        'num_body_parts': len(all_body_parts),\n        'body_parts': ', '.join(sorted(list(all_body_parts))[:5]) + '...' if len(all_body_parts) > 5 else ', '.join(sorted(list(all_body_parts))),\n        'arena_shapes': ', '.join(arena_shapes),\n        'avg_arena_cm': avg_arena_size,\n        'num_behaviors': len(all_behaviors),\n        'fps_range': f\"{fps_values.min():.0f}-{fps_values.max():.0f}\",\n        'avg_fps': avg_fps\n    })\n\nlab_profiles_df = pd.DataFrame(lab_profiles).sort_values('num_videos', ascending=False)\n\nprint(\"ğŸ¢ LABORATORY PROFILES\")\nprint(\"=\"*100)\nprint(lab_profiles_df.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:39.039305Z","iopub.execute_input":"2025-11-04T12:52:39.03961Z","iopub.status.idle":"2025-11-04T12:52:39.096555Z","shell.execute_reply.started":"2025-11-04T12:52:39.039586Z","shell.execute_reply":"2025-11-04T12:52:39.095372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### ğŸ’¡ **Understanding Body Part Availability**\n\n#### ğŸ” **Why Some Parts Show 0/10 Labs?**\n\nThese parts **exist** in the full dataset, but not in the **top 10 labs by video count**. \n\n**Where they appear:**\n- ğŸ”¬ **AdaptableSnail** (18 parts) - Has all `headpiece_*` variants\n- ğŸ”¬ **GroovyShrew** (4 parts) - Has `head` \n- ğŸ”¬ **UppityFerret** (14 parts) - Has `spine_*` and `tail_middle_*`\n\n---\n\n### ğŸ“Š **Modeling Strategy by Availability:**\n\n| Type | Body Parts | Labs | Strategy |\n|------|-----------|------|----------|\n| **Universal** | `ear_left`, `ear_right`, `nose`, `tail_base` | 10/10 | âœ… Use directly |\n| **Common** | `hip_left`, `hip_right`, `neck` | 6-7/10 | âœ… Use with fallback |\n| **Occasional** | `body_center`, `lateral_left/right` | 3/10 | âš ï¸ Fallback required |\n| **Rare** | `headpiece_*`, `spine_*`, `head` | 0-1/10 | âŒ Don't rely on |\n\n---\n\n### ğŸ¯ **Implementation Pattern:**\n```python\ndef get_body_center(tracking_df):\n    \"\"\"Get body center with cross-lab fallbacks\"\"\"\n    \n    # Priority order: most â†’ least reliable\n    priority = ['body_center', 'nose', 'neck', 'ear_left']\n    \n    for part in priority:\n        data = tracking_df[tracking_df['bodypart'] == part]\n        if len(data) > 10:  # Min threshold\n            return data\n    \n    # Last resort: use ANY available part\n    available = tracking_df['bodypart'].unique()\n    if len(available) > 0:\n        return tracking_df[tracking_df['bodypart'] == available[0]]\n    \n    return None","metadata":{}},{"cell_type":"code","source":"# Analyze body part tracking across labs\nbodypart_by_lab = defaultdict(lambda: defaultdict(int))\n\nfor idx, row in annotated_df.iterrows():\n    lab_id = row['lab_id']\n    try:\n        body_parts = json.loads(row['body_parts_tracked'])\n        for part in body_parts:\n            bodypart_by_lab[lab_id][part] += 1\n    except:\n        pass\n\n# Create body part matrix\nall_body_parts_list = set()\nfor lab_parts in bodypart_by_lab.values():\n    all_body_parts_list.update(lab_parts.keys())\nall_body_parts_list = sorted(list(all_body_parts_list))\n\ntop_labs = lab_profiles_df.head(10)['lab_id'].tolist()\n\n# Build matrix\nbp_matrix = np.zeros((len(top_labs), len(all_body_parts_list)))\n\nfor i, lab in enumerate(top_labs):\n    for j, part in enumerate(all_body_parts_list):\n        bp_matrix[i, j] = bodypart_by_lab[lab].get(part, 0)\n\n# Normalize by number of videos\nfor i, lab in enumerate(top_labs):\n    num_videos = lab_profiles_df[lab_profiles_df['lab_id'] == lab]['num_videos'].iloc[0]\n    if num_videos > 0:\n        bp_matrix[i, :] = (bp_matrix[i, :] > 0).astype(float)  # Binary: available or not\n\n# Plot\nfig, ax = plt.subplots(figsize=(16, 8))\n\nim = ax.imshow(bp_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n\nax.set_xticks(range(len(all_body_parts_list)))\nax.set_yticks(range(len(top_labs)))\nax.set_xticklabels(all_body_parts_list, rotation=45, ha='right', fontsize=10)\nax.set_yticklabels([lab[:25] for lab in top_labs], fontsize=10)\n\nax.set_xlabel('Body Part', fontsize=12, fontweight='bold')\nax.set_ylabel('Laboratory', fontsize=12, fontweight='bold')\nax.set_title('ğŸ¯ Body Part Tracking Availability Across Labs', fontsize=14, fontweight='bold', pad=20)\n\n# Add grid\nfor i in range(len(top_labs) + 1):\n    ax.axhline(i - 0.5, color='gray', linewidth=0.5)\nfor j in range(len(all_body_parts_list) + 1):\n    ax.axvline(j - 0.5, color='gray', linewidth=0.5)\n\n# Colorbar\ncbar = plt.colorbar(im, ax=ax)\ncbar.set_label('Available', fontsize=11, fontweight='bold')\ncbar.set_ticks([0, 1])\ncbar.set_ticklabels(['No', 'Yes'])\n\nplt.tight_layout()\nplt.show()\n\n# Summary\nprint(\"\\nğŸ“Š Body Part Tracking Summary:\")\nprint(\"=\"*70)\nfor part in all_body_parts_list:\n    num_labs = sum(1 for lab in top_labs if bodypart_by_lab[lab].get(part, 0) > 0)\n    print(f\"{part:25s} â”‚ Available in {num_labs}/{len(top_labs)} labs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:39.098155Z","iopub.execute_input":"2025-11-04T12:52:39.098433Z","iopub.status.idle":"2025-11-04T12:52:39.727185Z","shell.execute_reply.started":"2025-11-04T12:52:39.098411Z","shell.execute_reply":"2025-11-04T12:52:39.725586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze which behaviors appear in which labs\nbehavior_by_lab = defaultdict(set)\n\nfor idx, row in annotated_df.iterrows():\n    lab_id = row['lab_id']\n    try:\n        behaviors_str = row['behaviors_labeled']\n        if pd.notna(behaviors_str):\n            behaviors = json.loads(behaviors_str)\n            for b in behaviors:\n                parts = b.split(',')\n                if len(parts) >= 3:\n                    action = parts[2]\n                    behavior_by_lab[lab_id].add(action)\n    except:\n        pass\n\n# Get top behaviors and labs\ntop_behaviors_list = behavior_df.head(20)['behavior'].tolist()\ntop_labs_list = lab_profiles_df.head(10)['lab_id'].tolist()\n\n# Build matrix\nbehavior_lab_matrix = np.zeros((len(top_behaviors_list), len(top_labs_list)))\n\nfor i, behavior in enumerate(top_behaviors_list):\n    for j, lab in enumerate(top_labs_list):\n        if behavior in behavior_by_lab[lab]:\n            behavior_lab_matrix[i, j] = 1\n\n# Plot\nfig, ax = plt.subplots(figsize=(14, 10))\n\nim = ax.imshow(behavior_lab_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n\nax.set_xticks(range(len(top_labs_list)))\nax.set_yticks(range(len(top_behaviors_list)))\nax.set_xticklabels([lab[:20] for lab in top_labs_list], rotation=45, ha='right', fontsize=10)\nax.set_yticklabels(top_behaviors_list, fontsize=10)\n\nax.set_xlabel('Laboratory', fontsize=12, fontweight='bold')\nax.set_ylabel('Behavior', fontsize=12, fontweight='bold')\nax.set_title('ğŸ­ Behavior Annotation Coverage Across Labs', fontsize=14, fontweight='bold', pad=20)\n\n# Add grid\nfor i in range(len(top_behaviors_list) + 1):\n    ax.axhline(i - 0.5, color='white', linewidth=1)\nfor j in range(len(top_labs_list) + 1):\n    ax.axvline(j - 0.5, color='white', linewidth=1)\n\n# Colorbar\ncbar = plt.colorbar(im, ax=ax)\ncbar.set_label('Annotated', fontsize=11, fontweight='bold')\ncbar.set_ticks([0, 1])\ncbar.set_ticklabels(['No', 'Yes'])\n\nplt.tight_layout()\nplt.show()\n\n# Identify universal vs lab-specific behaviors\nprint(\"\\nğŸŒ Behavior Generalization Analysis:\")\nprint(\"=\"*70)\n\nuniversal_behaviors = []\nlab_specific_behaviors = []\n\nfor behavior in top_behaviors_list:\n    num_labs = sum(1 for lab in top_labs_list if behavior in behavior_by_lab[lab])\n    coverage = num_labs / len(top_labs_list)\n    \n    if coverage >= 0.5:\n        universal_behaviors.append((behavior, num_labs))\n    else:\n        lab_specific_behaviors.append((behavior, num_labs))\n\nprint(\"\\nâœ… Universal Behaviors (>50% lab coverage):\")\nfor beh, count in universal_behaviors:\n    print(f\"  â€¢ {beh:20s} â†’ {count}/{len(top_labs_list)} labs\")\n\nprint(\"\\nâš ï¸ Lab-Specific Behaviors (<50% lab coverage):\")\nfor beh, count in lab_specific_behaviors[:10]:\n    print(f\"  â€¢ {beh:20s} â†’ {count}/{len(top_labs_list)} labs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:52:39.728346Z","iopub.execute_input":"2025-11-04T12:52:39.728657Z","iopub.status.idle":"2025-11-04T12:52:40.355175Z","shell.execute_reply.started":"2025-11-04T12:52:39.728632Z","shell.execute_reply":"2025-11-04T12:52:40.353569Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# ğŸ¯ Section 6: Actionable Modeling Recommendations\n\n<div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 25px; border-radius: 10px; color: white;\">\n    <h2 style=\"margin: 0;\">ğŸ† Competition Strategy Summary</h2>\n</div>\n\n## ğŸ“Š Data Strategy\n\n### 1ï¸âƒ£ **Train/Validation Split Design**\n\n**Recommended approach:**\n- Use stratified GroupKFold by lab_id (5 folds)\n- Ensure each fold has diverse labs for robust generalization\n- Reserve 1-2 labs completely for final validation\n\n### 2ï¸âƒ£ **Class Imbalance Handling**\n\n| Strategy | When to Use |\n|----------|-------------|\n| **Focal Loss** | For behaviors with <100 examples |\n| **Class Weights** | All behaviors, inverse frequency weighting |\n| **Oversampling** | Rare behaviors (<50 examples) |\n| **Mixup/Cutmix** | Data augmentation for temporal sequences |\n\n### 3ï¸âƒ£ **Domain Adaptation**\n\n**Must-have techniques:**\n- âœ… Coordinate normalization by pix_per_cm_approx\n- âœ… FPS-agnostic features (use seconds, not frames)\n- âœ… Body-part-agnostic fallbacks (nose â†’ body_center â†’ head)\n- âœ… Lab-ID as auxiliary input (domain classifier)\n\n---\n\n## ğŸ”§ Feature Engineering Priorities\n\n### **Tier 1: Essential Features** (Implement First)\n1. **Velocity & Acceleration** - Rolling windows: 5, 10, 30 frames\n2. **Inter-mouse distances** - For all mouse pairs\n3. **Arena-relative position** - Distance to center, walls, corners\n4. **Angular features** - Body orientation, turning rate\n5. **Temporal aggregations** - Mean, std, min, max over multiple windows\n\n### **Tier 2: Advanced Features** (Add for improvement)\n6. **Behavior history** - Previous behavior encoding (1-hot or embedding)\n7. **Relative velocities** - Between mouse pairs\n8. **Trajectory curvature** - Second derivatives\n9. **Pose configuration** - Body part relative positions\n10. **Interaction features** - Approach rate, alignment, following\n\n### **Tier 3: Experimental** (Test carefully)\n11. **Fourier features** - Frequency domain representations\n12. **Graph neural features** - Mouse-mouse interaction graphs\n13. **Attention features** - Self-attention over body parts\n\n---\n\n## ğŸ—ï¸ Model Architecture Recommendations\n\n### **Baseline Models** (Start here)\n\n**1. LightGBM with hand-crafted features**\n- Fast iteration, interpretable\n- Good for understanding feature importance\n\n**2. 1D CNN on raw pose sequences**\n- Learns temporal patterns automatically\n- Handles variable-length sequences\n\n### **Advanced Models** (For leaderboard push)\n\n**3. Temporal Convolutional Network (TCN)**\n- Better than LSTM for long sequences\n- Parallel computation advantage\n\n**4. Transformer-based**\n- Multi-head attention over time\n- Positional encoding for temporal order\n\n**5. Ensemble Strategy**\n- LightGBM + TCN + Transformer\n- Lab-specific models + global model\n\n---\n\n## âš ï¸ Critical Pitfalls to Avoid\n\n<div style=\"background-color: #ffebee; padding: 15px; border-left: 5px solid #f44336; border-radius: 5px;\">\n    <h3>ğŸš« Don't Do These!</h3>\n    <ul>\n        <li><strong>Leakage:</strong> Using future frames in rolling features (use center=False)</li>\n        <li><strong>Overfitting to CalMS21:</strong> This lab dominates data - validate on other labs!</li>\n        <li><strong>Ignoring FPS:</strong> Different videos have different frame rates</li>\n        <li><strong>Hard-coding body parts:</strong> Not all labs track same parts</li>\n        <li><strong>Static thresholds:</strong> Adaptive thresholds per behavior perform better</li>\n    </ul>\n</div>\n\n---\n\n## ğŸ“ˆ Evaluation Strategy\n\n### **Validation Metrics**\n- **Primary:** Per-lab averaged F1 (matches competition metric)\n- **Secondary:** Confusion matrix for behavior-level analysis\n- **Tertiary:** Boundary precision (Â±5 frame tolerance)\n\n### **Error Analysis**\n\nFocus debugging on:\n1. **False Negatives** for rare behaviors â†’ Increase sensitivity\n2. **False Positives** for common behaviors â†’ Increase specificity  \n3. **Boundary errors** â†’ Post-processing smoothing\n4. **Cross-lab failures** â†’ Domain adaptation\n\n---\n\n## ğŸ¬ Next Steps\n\n<div style=\"background-color: #e8f5e9; padding: 15px; border-left: 5px solid #4caf50; border-radius: 5px;\">\n    <h3>âœ… Implementation Checklist</h3>\n    <ol>\n        <li>âœï¸ Build baseline with hand-crafted features + LightGBM</li>\n        <li>ğŸ“Š Implement proper cross-validation (GroupKFold by lab)</li>\n        <li>ğŸ¯ Add class weights for imbalance</li>\n        <li>ğŸ”§ Engineer Tier 1 features</li>\n        <li>ğŸ§ª Experiment with temporal models (TCN/Transformer)</li>\n        <li>ğŸ† Ensemble multiple models</li>\n        <li>âš¡ Optimize post-processing (smoothing, merging short events)</li>\n    </ol>\n</div>\n\n---\n\n<div style=\"text-align: center; padding: 20px; background-color: #f5f5f5; border-radius: 10px;\">\n    <h3>ğŸ­ Happy Modeling! ğŸ­</h3>\n    <p><em>\"In mouse behavior analysis, the devil is in the temporal details.\"</em></p>\n</div>\n\n---\n\n### ğŸ™ Thank you for reading!\n\n**Good luck in the competition!** ğŸš€ğŸ­\n\n*If this notebook helped you, please upvote!* â­","metadata":{}}]}