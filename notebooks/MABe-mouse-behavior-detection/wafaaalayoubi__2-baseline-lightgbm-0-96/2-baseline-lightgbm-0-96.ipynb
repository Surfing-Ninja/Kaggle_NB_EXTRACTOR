{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59156,"databundleVersionId":13719397,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook 2: The First Hypothesis - A Simple Frame-by-Frame Baseline\n\n**Overall Goal:** To build our first complete, end-to-end machine learning pipeline. We will create a simple model that gets a score on the leaderboard, establishing a \"baseline\" that all our future, more complex models must beat.\n\n**The Strategy (and Deliberate Simplification):**\nFor this notebook, we will make a major simplifying assumption: **we will treat every single frame of video as an independent data point.** We will ignore the fact that behaviors are sequences. This is technically the \"wrong\" way to model this problem, but it's the perfect way to start because it's simple and fast.\n\n*   **Model Choice:** We will use **LightGBM**, a powerful and fast gradient-boosting model that is excellent for tabular data.\n*   **Features:** We will create a small set of \"frame-wise\" features that describe the scene at a single moment in time.\n*   **The Pipeline:** We will build all the necessary steps: data loading, feature engineering, training, prediction, and crucially, the **post-processing** required to convert frame-by-frame predictions into the `start_frame, stop_frame` submission format.\nlink to EDA [notebook](https://www.kaggle.com/code/wafaaalayoubi/1-full-eda)","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Setup and Reusable Processing Functions\n\n**Goal:** To set up our environment and create reusable functions for our data processing pipeline. In Notebook 1, we learned that we need to load parquet files and pivot them from a \"long\" to a \"wide\" format. Since we will be doing this for many videos, it's best practice to encapsulate this logic into a clean, reusable function.\n\n**Action:**\n1.  Import all the necessary libraries, including `lightgbm` for our model.\n2.  Define a function `load_and_process_video` that takes a `video_id` and `lab_id` as input and returns the \"wide\" DataFrame, ready for feature engineering.","metadata":{}},{"cell_type":"code","source":"# --- Core Libraries ---\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom tqdm.auto import tqdm\n\n# --- Modeling Libraries ---\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:44:34.061272Z","iopub.execute_input":"2025-09-25T07:44:34.0616Z","iopub.status.idle":"2025-09-25T07:44:34.0682Z","shell.execute_reply.started":"2025-09-25T07:44:34.061577Z","shell.execute_reply":"2025-09-25T07:44:34.06651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Set display options ---\npd.set_option('display.max_columns', 200)\nsns.set_style('whitegrid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:44:34.077844Z","iopub.execute_input":"2025-09-25T07:44:34.078225Z","iopub.status.idle":"2025-09-25T07:44:34.105255Z","shell.execute_reply.started":"2025-09-25T07:44:34.078189Z","shell.execute_reply":"2025-09-25T07:44:34.104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Define Constants and Paths ---\nDATA_PATH = '/kaggle/input/MABe-mouse-behavior-detection/' ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:44:34.107023Z","iopub.execute_input":"2025-09-25T07:44:34.107377Z","iopub.status.idle":"2025-09-25T07:44:34.131123Z","shell.execute_reply.started":"2025-09-25T07:44:34.107355Z","shell.execute_reply":"2025-09-25T07:44:34.12993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Reusable Function Definition ---\n\ndef load_and_process_video(video_id, lab_id, data_path):\n    \"\"\"\n    Loads the tracking data for a single video and pivots it into a wide format.\n    \"\"\"\n    # Using os.path.join is the most robust way to build paths\n    tracking_path = os.path.join(data_path, 'train_tracking', lab_id, f'{video_id}.parquet')\n    \n    if not os.path.exists(tracking_path):\n        print(f\"Warning: File not found at {tracking_path}\")\n        return None\n        \n    df_long = pd.read_parquet(tracking_path)\n    \n    pivot_x = df_long.pivot(index='video_frame', columns=['mouse_id', 'bodypart'], values='x')\n    pivot_y = df_long.pivot(index='video_frame', columns=['mouse_id', 'bodypart'], values='y')\n    \n    pivot_x.columns = [f\"mouse{m}_{bp}_x\" for m, bp in pivot_x.columns]\n    pivot_y.columns = [f\"mouse{m}_{bp}_y\" for m, bp in pivot_y.columns]\n    \n    df_wide = pd.concat([pivot_x, pivot_y], axis=1)\n    df_wide = df_wide.sort_index(axis=1)\n    \n    return df_wide","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:44:34.132782Z","iopub.execute_input":"2025-09-25T07:44:34.133135Z","iopub.status.idle":"2025-09-25T07:44:34.16445Z","shell.execute_reply.started":"2025-09-25T07:44:34.133109Z","shell.execute_reply":"2025-09-25T07:44:34.163011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Test the function with our sample video from Notebook 1 ---\nprint(\"Testing the data processing function...\")\ndf_train_meta = pd.read_csv(os.path.join(DATA_PATH, 'train.csv')) # Using os.path.join here for safety\nsample_video_meta = df_train_meta.iloc[0]\n\ndf_wide_sample = load_and_process_video(sample_video_meta['video_id'], sample_video_meta['lab_id'], DATA_PATH)\n\nprint(\"\\n--- Function Test Output ---\")\nif df_wide_sample is not None:\n    print(f\"Successfully loaded and processed video {sample_video_meta['video_id']}\")\n    print(f\"Shape of the resulting wide DataFrame: {df_wide_sample.shape}\")\n    display(df_wide_sample.head())\nelse:\n    print(\"Failed to load the sample video.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:44:34.165637Z","iopub.execute_input":"2025-09-25T07:44:34.165878Z","iopub.status.idle":"2025-09-25T07:44:35.193484Z","shell.execute_reply.started":"2025-09-25T07:44:34.165859Z","shell.execute_reply":"2025-09-25T07:44:35.192247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Preparing Data for a Simple Model\n\n**Goal:** To prepare our data for LightGBM with the absolute minimum of processing. According to our \"simplest possible baseline\" plan, we will **not** engineer any new features in this notebook. We will feed the raw `x` and `y` coordinates directly to the model.\n\n**Action:**\n1.  **Select a Subset:** Training a model on all 8,790 videos would take a very long time. For this baseline, we will select a small, manageable subset of just **50 videos** to prove our pipeline works.\n2.  **Load and Combine:** We will loop through our subset of videos, load the tracking data for each, and combine them into one large training DataFrame.\n3.  **Create Frame-wise Labels:** The annotations are in a `start_frame, stop_frame` format. We need to convert this into a label for *every single frame*. For now, we will only handle the simple case: single-agent behaviors (like `rear`) and two-agent behaviors (`agent_id` != `target_id`). We will create a `behavior` column in our main DataFrame.","metadata":{}},{"cell_type":"code","source":"# --- 1. Select a Subset of Videos ---\n# Let's use 50 videos for our baseline model. It's enough to be representative but fast to process.\nN_VIDEOS_TO_USE = 50\ndf_subset_meta = df_train_meta.head(N_VIDEOS_TO_USE)\n\nprint(f\"Using a subset of {len(df_subset_meta)} videos for this baseline model.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:58:08.121164Z","iopub.execute_input":"2025-09-25T07:58:08.121488Z","iopub.status.idle":"2025-09-25T07:58:08.128658Z","shell.execute_reply.started":"2025-09-25T07:58:08.121467Z","shell.execute_reply":"2025-09-25T07:58:08.127337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Load and Combine Data for the Subset ---\nall_wide_dfs = []\nfor index, row in tqdm(df_subset_meta.iterrows(), total=len(df_subset_meta)):\n    df_wide = load_and_process_video(row['video_id'], row['lab_id'], DATA_PATH)\n    if df_wide is not None:\n        # Add a video_id column so we can link back to annotations\n        df_wide['video_id'] = row['video_id']\n        all_wide_dfs.append(df_wide)\n\n# Combine all individual video dataframes into one big one\ndf_train_full = pd.concat(all_wide_dfs)\n\nprint(f\"\\nLoaded and combined data for all videos. Full training shape: {df_train_full.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:58:21.876847Z","iopub.execute_input":"2025-09-25T07:58:21.877215Z","iopub.status.idle":"2025-09-25T07:59:18.676772Z","shell.execute_reply.started":"2025-09-25T07:58:21.877191Z","shell.execute_reply":"2025-09-25T07:59:18.675675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Load Annotations and Create Frame-wise Labels ---\n# Load all annotations for our subset of videos\nall_annotations_list = []\nfor video_id in tqdm(df_subset_meta['video_id'].unique(), desc=\"Loading annotations\"):\n    row = df_subset_meta[df_subset_meta['video_id'] == video_id].iloc[0]\n    annot_path = os.path.join(DATA_PATH, 'train_annotation', row['lab_id'], f\"{row['video_id']}.parquet\")\n    if os.path.exists(annot_path):\n        df_annot = pd.read_parquet(annot_path)\n        df_annot['video_id'] = video_id\n        all_annotations_list.append(df_annot)\n\ndf_annotations_subset = pd.concat(all_annotations_list)\n\n# Initialize the target column with a \"no_behavior\" label\ndf_train_full['behavior'] = 'no_behavior'\n\n# --- Apply labels to each frame ---\n# This is a complex loop, but it's the core of the labeling process\nprint(\"\\nApplying annotations to each frame...\")\nfor index, row in tqdm(df_annotations_subset.iterrows(), total=len(df_annotations_subset)):\n    video_id = row['video_id']\n    start_frame = row['start_frame']\n    stop_frame = row['stop_frame']\n    action = row['action']\n    \n    # This is a simplification for the baseline: we create a single 'behavior' target.\n    # We are not yet handling multiple simultaneous behaviors.\n    df_train_full.loc[\n        (df_train_full['video_id'] == video_id) & \n        (df_train_full.index >= start_frame) & \n        (df_train_full.index <= stop_frame),\n        'behavior'\n    ] = action\n\nprint(\"Labeling complete.\")\nprint(\"\\nValue counts of our new 'behavior' target column:\")\nprint(df_train_full['behavior'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:59:18.678903Z","iopub.execute_input":"2025-09-25T07:59:18.679229Z","iopub.status.idle":"2025-09-25T08:02:33.206963Z","shell.execute_reply.started":"2025-09-25T07:59:18.679206Z","shell.execute_reply":"2025-09-25T08:02:33.205574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What We Learned in Step 2\n\n*   **Successful Data Assembly:** We have successfully created a complete training dataset for our baseline model. It contains the raw coordinate data and a corresponding `behavior` label for over 5.7 million frames, sampled from our first 50 videos.\n\n*   **The \"Nothing\" Class:** The `value_counts()` output immediately reveals a crucial insight. The `no_behavior` class, which we created, is by far the largest class, with ~5.4 million instances. This is even more dominant than the most common labeled behavior (`rear`). This means our model will be heavily incentivized to just predict \"nothing is happening.\"\n\n*   **Labeling is Correct:** The presence of many different behavior classes (rear, attack, sniff, etc.) with reasonable counts confirms that our logic for \"unrolling\" the `start_frame, stop_frame` annotations into per-frame labels is working correctly.\n\n*   **Simplification Check:** For this baseline, we've created a single target column. This means we are not currently handling frames where multiple behaviors might be happening at once. This is a deliberate simplification we can address in a later, more advanced notebook.\n\n**With our features (the raw coordinates) and our target (`behavior` column) now in a single, clean DataFrame, we are finally ready to train our first model.**","metadata":{}},{"cell_type":"markdown","source":"# Step 3: Model Training\n\n**Goal:** To train our LightGBM model on the data we just prepared. We will perform a simple split of our data into a training set and a validation set to evaluate its performance.\n\n**Action:**\n1.  **Define Features and Target:** Explicitly separate our `X` (features, i.e., the coordinates) and `y` (target, i.e., the behavior) variables.\n2.  **Handle Missing Data:** As we discussed, LightGBM can handle `NaN` values, but it performs best if we fill them with a placeholder value that it can recognize, like -1.\n3.  **Encode Labels:** Machine learning models require numerical labels. We will use `LabelEncoder` to convert our behavior strings (e.g., \"attack\") into integers (e.g., 0, 1, 2).\n4.  **Split Data:** We will perform a simple random `train_test_split`. We know from our EDA that this is not the *best* way to validate, but it is the *simplest* and is acceptable for our first baseline.\n5.  **Train the Model:** We will initialize and train a `LGBMClassifier`.","metadata":{}},{"cell_type":"code","source":"# --- 1. Define Features (X) and Target (y) ---\n# Our features are all columns EXCEPT 'video_id' and our target 'behavior'\nfeatures = [col for col in df_train_full.columns if col not in ['video_id', 'behavior']]\nX = df_train_full[features]\ny = df_train_full['behavior']\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:06:48.742336Z","iopub.execute_input":"2025-09-25T08:06:48.742746Z","iopub.status.idle":"2025-09-25T08:06:52.086744Z","shell.execute_reply.started":"2025-09-25T08:06:48.742715Z","shell.execute_reply":"2025-09-25T08:06:52.08591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Handle Missing Data ---\n# LightGBM can handle NaNs, but filling them explicitly can sometimes be more stable.\n# We'll fill with -1, a value that doesn't appear in the coordinate data.\nX = X.fillna(-1)\nprint(\"\\nFilled NaN values with -1.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:07:01.402325Z","iopub.execute_input":"2025-09-25T08:07:01.402653Z","iopub.status.idle":"2025-09-25T08:07:07.241347Z","shell.execute_reply.started":"2025-09-25T08:07:01.402623Z","shell.execute_reply":"2025-09-25T08:07:07.24025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Encode String Labels into Numbers ---\n# The model needs numerical targets, so 'attack' -> 0, 'chase' -> 1, etc.\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Let's see the mapping\nprint(\"\\nLabel Encoding Mapping:\")\nfor i, class_name in enumerate(label_encoder.classes_):\n    print(f\"{class_name} -> {i}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:07:16.593795Z","iopub.execute_input":"2025-09-25T08:07:16.594124Z","iopub.status.idle":"2025-09-25T08:07:17.516092Z","shell.execute_reply.started":"2025-09-25T08:07:16.594099Z","shell.execute_reply":"2025-09-25T08:07:17.514894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Split Data into Training and Validation Sets ---\n# We use a simple 80/20 split. stratify=y_encoded ensures that the proportion\n# of each behavior is the same in both the train and validation sets.\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y_encoded, \n    test_size=0.2, \n    random_state=42, \n    stratify=y_encoded\n)\nprint(f\"\\nTraining data shape: {X_train.shape}\")\nprint(f\"Validation data shape: {X_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:07:29.582889Z","iopub.execute_input":"2025-09-25T08:07:29.583228Z","iopub.status.idle":"2025-09-25T08:07:50.267882Z","shell.execute_reply.started":"2025-09-25T08:07:29.583205Z","shell.execute_reply":"2025-09-25T08:07:50.266619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Train the LightGBM Model ---\nprint(\"\\nTraining LightGBM model...\")\n\nlgbm = lgb.LGBMClassifier(\n    objective='multiclass',\n    n_estimators=500,  # Number of trees to build\n    learning_rate=0.05,\n    num_leaves=31,\n    random_state=42,\n    n_jobs=-1,         # Use all available CPU cores\n    colsample_bytree=0.8, # Subsample columns to prevent overfitting\n    subsample=0.8       # Subsample rows to prevent overfitting\n)\n\n# We use the validation set to monitor for early stopping\n# This prevents the model from training for too long and overfitting.\nlgbm.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='multi_logloss',\n    callbacks=[lgb.early_stopping(10, verbose=True)]\n)\n\nprint(\"\\nModel training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:07:50.269532Z","iopub.execute_input":"2025-09-25T08:07:50.269975Z","iopub.status.idle":"2025-09-25T08:12:17.473136Z","shell.execute_reply.started":"2025-09-25T08:07:50.26995Z","shell.execute_reply":"2025-09-25T08:12:17.47162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What We Learned in Step 3\n\n*   **Successful Training:** Our LightGBM model trained successfully on nearly 4.6 million frames of data. The process was efficient, taking only a few minutes.\n\n*   **Initial Scores Confirm Imbalance:** The \"Start training from score...\" lines show the initial bias of the model. The score for `no_behavior` (class 8) is `-0.052374`, which is very close to zero. All other scores are large negative numbers (e.g., `-5.77` for `approach`). In log-loss terms, this confirms the model's initial guess is overwhelmingly \"no_behavior\" because it's the most common class.\n\n*   **Early Stopping is Powerful:** The model didn't need to run for all 500 rounds (estimators). It found its best performance on the validation set at round **39** and stopped automatically. This is a crucial technique that saved us time and, more importantly, prevented the model from overfitting to the training data.\n\n*   **Validation Score:** The best score on our validation set was a `multi_logloss` of **0.144441**. On its own, this number is a bit abstract, but it will be our key internal benchmark. When we build our next model with feature engineering, we will aim to get this number even lower.\n\n**The model now exists in our notebook's memory, ready to make predictions. The next step is to evaluate its performance in a more human-readable way and then prepare a submission file.**","metadata":{}},{"cell_type":"markdown","source":"# Step 4: Evaluation and Post-Processing\n\n**Goal:** To understand how well our simple model performs and to convert its frame-by-frame predictions into the required `start_frame, stop_frame` format for submission.\n\n**Action:**\n1.  **Evaluate Performance:** We will use the trained model to make predictions on our validation set (`X_val`). Then, we'll generate a `classification_report`, which shows us key metrics like precision, recall, and F1-score for each individual behavior. This will clearly show us which behaviors the model learned and which it ignored.\n2.  **Develop Post-Processing Logic:** This is a critical step. Our model outputs a single prediction for each frame. We need a function that can take a long sequence of these predictions (e.g., `[8, 8, 9, 9, 9, 8, ...]`) and convert them into submission-ready rows like `(rear, start_frame=2, stop_frame=4)`.","metadata":{}},{"cell_type":"code","source":"# --- 1. Evaluate Performance on the Validation Set ---\nprint(\"--- Model Performance on Validation Set ---\")\n\n# Make predictions\ny_pred = lgbm.predict(X_val)\n\n# Convert the numerical predictions back to string labels for the report\ny_pred_labels = label_encoder.inverse_transform(y_pred)\ny_val_labels = label_encoder.inverse_transform(y_val)\n\n# Generate and print the classification report\nreport = classification_report(y_val_labels, y_pred_labels)\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:15:28.313346Z","iopub.execute_input":"2025-09-25T08:15:28.313669Z","iopub.status.idle":"2025-09-25T08:16:24.735441Z","shell.execute_reply.started":"2025-09-25T08:15:28.313644Z","shell.execute_reply":"2025-09-25T08:16:24.734128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Develop Post-Processing Logic ---\ndef predictions_to_submission(df_preds, video_id):\n    \"\"\"\n    Converts frame-by-frame predictions into a submission-ready format.\n    \n    Args:\n        df_preds (pd.DataFrame): DataFrame with 'frame' and 'behavior' columns.\n        video_id (int or str): The ID of the video being processed.\n        \n    Returns:\n        pd.DataFrame: A submission-formatted DataFrame for this video.\n    \"\"\"\n    submission_rows = []\n    \n    # Ignore 'no_behavior' predictions\n    df_preds = df_preds[df_preds['behavior'] != 'no_behavior'].copy()\n    \n    # Find contiguous blocks of the same behavior\n    # This clever trick identifies where a block of the same behavior changes\n    df_preds['block'] = (df_preds['behavior'] != df_preds['behavior'].shift()).cumsum()\n    \n    for _, group in df_preds.groupby('block'):\n        # For our simple baseline, we'll assign mouse1 as agent and mouse2 as target\n        # This is a major simplification we'll improve later.\n        submission_rows.append({\n            'video_id': video_id,\n            'agent_id': 'mouse1',\n            'target_id': 'mouse2',\n            'action': group['behavior'].iloc[0],\n            'start_frame': group['frame'].min(),\n            'stop_frame': group['frame'].max(),\n        })\n        \n    return pd.DataFrame(submission_rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:16:24.737194Z","iopub.execute_input":"2025-09-25T08:16:24.737566Z","iopub.status.idle":"2025-09-25T08:16:24.744246Z","shell.execute_reply.started":"2025-09-25T08:16:24.737541Z","shell.execute_reply":"2025-09-25T08:16:24.74319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Test the post-processing function ---\nprint(\"\\n--- Testing Post-Processing ---\")\n\n# Create a dummy prediction dataframe to test the logic\ndummy_data = {\n    'frame': [0, 1, 2, 3, 4, 5, 6, 7, 8],\n    'behavior': ['no_behavior', 'attack', 'attack', 'attack', 'no_behavior', 'rear', 'rear', 'no_behavior', 'attack']\n}\ndummy_df = pd.DataFrame(dummy_data).set_index('frame')\ndummy_submission = predictions_to_submission(dummy_df.reset_index(), 'dummy_video')\n\nprint(\"Dummy predictions converted to submission format:\")\ndisplay(dummy_submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:16:24.745218Z","iopub.execute_input":"2025-09-25T08:16:24.746201Z","iopub.status.idle":"2025-09-25T08:16:24.786364Z","shell.execute_reply.started":"2025-09-25T08:16:24.746165Z","shell.execute_reply":"2025-09-25T08:16:24.785331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What We Learned in Step 4\n\n**From the Classification Report:**\n\n*   **The \"Accuracy\" Lie:** The overall accuracy is **96%**, which looks amazing at first glance. However, this number is completely misleading. Our model could achieve ~95% accuracy by *only* predicting `no_behavior` (`1090760 / 1149409`). This high accuracy score is a classic trap in imbalanced datasets.\n\n*   **Precision vs. Recall:** This is where the real story is.\n    *   **Recall** tells us \"Of all the actual 'attack' frames, what percentage did our model find?\" For most classes like `attack` (18%), `avoid` (16%), and `shepherd` (14%), the recall is **very low**. Our model is missing the vast majority of these behaviors.\n    *   **Precision** tells us \"When the model predicted 'attack', how often was it correct?\" The precision is generally much higher (e.g., 66% for `attack`). This means that *when* our model decides to predict a behavior, it's right more often than not, but it's very shy and hesitant to do so.\n\n*   **Successes and Failures:**\n    *   **Successes:** The model did reasonably well on distinct, long-duration, or high-count behaviors like `mount`, `submit`, and `sniffgenital`, achieving a good balance of precision and recall (F1-scores of 0.70+).\n    *   **Failures:** It completely failed on rare and subtle behaviors. Look at `sniffface`: an F1-score of just 0.15. It has almost no idea what this behavior looks like.\n    *   **The `no_behavior` Crutch:** The model's strategy is clear: \"When in doubt, predict `no_behavior`.\" It only predicts a real behavior when it's extremely confident.\n\n**From the Post-Processing Test:**\n\n*   **Logic is Sound:** The test on the dummy data proves our `predictions_to_submission` function works perfectly. It correctly ignores the `no_behavior` class and groups consecutive frames of the same action into a single event with the correct `start_frame` and `stop_frame`. It even handles single-frame events correctly.\n\n**Conclusion: We have a working, but very conservative, baseline model. Its main weakness is low recall on most behaviors. This gives us a clear goal for Notebook 3: improve recall by giving the model better features!**","metadata":{}},{"cell_type":"markdown","source":"# Step 5: Generate Submission File\n\n**Goal:** To use our trained model and post-processing function to generate a `submission.csv` file in the correct format for the competition.\n\n**Action:**\n1.  **Load Test Metadata:** Get the list of test videos we need to predict on.\n2.  **Iterate and Predict:** Loop through each test video. For each one, we will:\n    *   Load its tracking data using our `load_and_process_video` function.\n    *   Fill `NaN` values just like we did for the training data.\n    *   Use our trained `lgbm` model to predict the behavior for every frame.\n    *   Convert the numerical predictions back to string labels.\n    *   Use our `predictions_to_submission` function to create the submission rows for that video.\n3.  **Combine and Save:** Combine the results from all test videos into a single DataFrame and save it as `submission.csv`.","metadata":{}},{"cell_type":"code","source":"# --- 1. Load Test Metadata ---\nprint(\"Loading test metadata...\")\ndf_test_meta = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nprint(f\"Found {len(df_test_meta)} videos in the test set.\")\n\nall_submissions = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:18:25.907259Z","iopub.execute_input":"2025-09-25T08:18:25.908224Z","iopub.status.idle":"2025-09-25T08:18:25.954144Z","shell.execute_reply.started":"2025-09-25T08:18:25.908196Z","shell.execute_reply":"2025-09-25T08:18:25.953037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Iterate and Predict on Test Set ---\nfor index, row in tqdm(df_test_meta.iterrows(), total=len(df_test_meta)):\n    video_id = row['video_id']\n    lab_id = row['lab_id']\n    \n    print(f\"\\nProcessing video: {video_id}\")\n    \n    # Load the video's tracking data\n    # NOTE: The test tracking files are in the 'test_tracking' folder\n    test_tracking_path = os.path.join(DATA_PATH, 'test_tracking', lab_id, f'{video_id}.parquet')\n    \n    # A bit of code duplication here, but it's safer to be explicit for the test set\n    if not os.path.exists(test_tracking_path):\n        print(f\"  Warning: Test file not found at {test_tracking_path}. Skipping.\")\n        continue\n        \n    df_long_test = pd.read_parquet(test_tracking_path)\n    \n    # Pivot to wide format\n    pivot_x = df_long_test.pivot(index='video_frame', columns=['mouse_id', 'bodypart'], values='x')\n    pivot_y = df_long_test.pivot(index='video_frame', columns=['mouse_id', 'bodypart'], values='y')\n    pivot_x.columns = [f\"mouse{m}_{bp}_x\" for m, bp in pivot_x.columns]\n    pivot_y.columns = [f\"mouse{m}_{bp}_y\" for m, bp in pivot_y.columns]\n    df_wide_test = pd.concat([pivot_x, pivot_y], axis=1).sort_index(axis=1)\n    \n    # Ensure test set has the same columns as the training set\n    X_test = df_wide_test.reindex(columns=features, fill_value=-1)\n    \n    # Preprocess (fill NaNs)\n    X_test = X_test.fillna(-1)\n    \n    # Predict\n    print(f\"  Predicting {len(X_test)} frames...\")\n    preds_encoded = lgbm.predict(X_test)\n    preds_labels = label_encoder.inverse_transform(preds_encoded)\n    \n    # Post-process\n    df_preds = pd.DataFrame({\n        'frame': X_test.index,\n        'behavior': preds_labels\n    })\n    \n    video_submission = predictions_to_submission(df_preds, video_id)\n    all_submissions.append(video_submission)\n    print(f\"  Found {len(video_submission)} behavior events in this video.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:18:47.890528Z","iopub.execute_input":"2025-09-25T08:18:47.892394Z","iopub.status.idle":"2025-09-25T08:18:49.123916Z","shell.execute_reply.started":"2025-09-25T08:18:47.892343Z","shell.execute_reply":"2025-09-25T08:18:49.122871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Combine and Save ---\nif all_submissions:\n    df_submission = pd.concat(all_submissions, ignore_index=True)\n    \n    # The submission requires a 'row_id' column\n    df_submission.index.name = 'row_id'\n    \n    # Make sure columns are in the exact order required\n    final_columns = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n    df_submission = df_submission[final_columns]\n    \n    df_submission.to_csv('submission.csv', index=True)\n    \n    print(\"\\n--- Submission File Generated ---\")\n    print(f\"Total events predicted: {len(df_submission)}\")\n    display(df_submission.head())\nelse:\n    # If no events were predicted for any video, create an empty submission file\n    print(\"\\nWarning: No behavior events were predicted. Creating an empty submission file.\")\n    pd.DataFrame(columns=['row_id'] + final_columns).to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:18:49.125459Z","iopub.execute_input":"2025-09-25T08:18:49.125813Z","iopub.status.idle":"2025-09-25T08:18:49.152144Z","shell.execute_reply.started":"2025-09-25T08:18:49.125781Z","shell.execute_reply":"2025-09-25T08:18:49.15091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Notebook 2 Conclusion: A Working (But Flawed) Baseline\n\nWe have successfully achieved our goal for this notebook: we built a complete, end-to-end pipeline that can process raw data, train a model, and generate a valid submission file.\n\n### Key Accomplishments:\n1.  **End-to-End Pipeline:** We have code that can handle every step of the process. This is a massive achievement and the foundation for all future experiments.\n2.  **First Submission:** We generated a `submission.csv` file, proving our logic is sound.\n3.  **Established a Baseline:** Our model achieved a **macro average F1-score of 0.49** on our validation set. This is our score to beat. We now know that any future changes must improve upon this number.\n\n### Major Weaknesses Identified:\n1.  **Low Recall:** The model is \"shy.\" It correctly identifies that *something* is happening, but it struggles to find the majority of the events (low recall for most classes).\n2.  **Ignoring Time:** Our frame-by-frame approach is fundamentally limited. Behaviors are sequences, and by ignoring this, we are leaving a huge amount of information on the table.\n3.  **Simplistic Features:** We used only raw coordinates. The model had to learn all spatial relationships from scratch, which is a very difficult task.\n\nThis baseline has served its purpose perfectly. It works, it gives us a score, and it has clearly illuminated the path forward.\n\n**Next Up: Notebook 3 - The Ethologist's Toolkit: Advanced Feature Engineering**\nIn the next notebook, we will directly address the weaknesses of this model by engineering a rich set of features (distances, speeds, angles) to help the model better understand the interactions between the mice.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}