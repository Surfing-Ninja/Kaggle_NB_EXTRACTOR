{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":5549164,"sourceType":"datasetVersion","datasetId":3197305},{"sourceId":5792099,"sourceType":"datasetVersion","datasetId":3327296},{"sourceId":6247561,"sourceType":"datasetVersion","datasetId":3590060}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library Import","metadata":{}},{"cell_type":"code","source":"#library import\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:14.916175Z","iopub.execute_input":"2025-11-08T03:07:14.917008Z","iopub.status.idle":"2025-11-08T03:07:14.922672Z","shell.execute_reply.started":"2025-11-08T03:07:14.916969Z","shell.execute_reply":"2025-11-08T03:07:14.921557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#part 2\nimport time\nimport matplotlib.pyplot as plt\nimport os\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:15.172236Z","iopub.execute_input":"2025-11-08T03:07:15.172848Z","iopub.status.idle":"2025-11-08T03:07:15.177449Z","shell.execute_reply.started":"2025-11-08T03:07:15.172819Z","shell.execute_reply":"2025-11-08T03:07:15.176401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#part 3a basic\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:15.957919Z","iopub.execute_input":"2025-11-08T03:07:15.958235Z","iopub.status.idle":"2025-11-08T03:07:21.321543Z","shell.execute_reply.started":"2025-11-08T03:07:15.958212Z","shell.execute_reply":"2025-11-08T03:07:21.32054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#part 3b\nfrom torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau\nfrom torchmetrics.classification import MultilabelF1Score\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:21.322957Z","iopub.execute_input":"2025-11-08T03:07:21.323423Z","iopub.status.idle":"2025-11-08T03:07:35.258693Z","shell.execute_reply.started":"2025-11-08T03:07:21.323398Z","shell.execute_reply":"2025-11-08T03:07:35.257579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#part 4 adjustment\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.utils.data import Dataset #dataset\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:35.259945Z","iopub.execute_input":"2025-11-08T03:07:35.260612Z","iopub.status.idle":"2025-11-08T03:07:35.26687Z","shell.execute_reply.started":"2025-11-08T03:07:35.260579Z","shell.execute_reply":"2025-11-08T03:07:35.265816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#extra modification\nfrom torch.optim.lr_scheduler import LambdaLR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:35.268433Z","iopub.execute_input":"2025-11-08T03:07:35.268811Z","iopub.status.idle":"2025-11-08T03:07:35.394641Z","shell.execute_reply.started":"2025-11-08T03:07:35.268782Z","shell.execute_reply":"2025-11-08T03:07:35.393419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seeds for reproducibility\nnp.random.seed(36)\ntorch.manual_seed(36)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(36)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:48.609599Z","iopub.execute_input":"2025-11-08T03:07:48.611101Z","iopub.status.idle":"2025-11-08T03:07:48.619727Z","shell.execute_reply.started":"2025-11-08T03:07:48.611059Z","shell.execute_reply":"2025-11-08T03:07:48.618587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Datasate maker and  Loading","metadata":{}},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"#the first gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:50.010845Z","iopub.execute_input":"2025-11-08T03:07:50.011187Z","iopub.status.idle":"2025-11-08T03:07:50.278332Z","shell.execute_reply.started":"2025-11-08T03:07:50.011164Z","shell.execute_reply":"2025-11-08T03:07:50.277235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#main path from Kaggle\nmain_path = \"/kaggle/input\"\n#path concern in main data (competition)\nmain_df_path = f\"{main_path}/cafa-6-protein-function-prediction\" #main path for protein","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:07:51.379808Z","iopub.execute_input":"2025-11-08T03:07:51.380189Z","iopub.status.idle":"2025-11-08T03:07:51.385109Z","shell.execute_reply.started":"2025-11-08T03:07:51.380166Z","shell.execute_reply":"2025-11-08T03:07:51.38412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#data declaration in configuration\nclass Config:\n    \"\"\"Configuration settings for the model in main dataframe\"\"\"\n    def __init__(self):\n        self.main_dir = main_df_path\n        self.train_sequences_path = f\"{self.main_dir}/Train/train_sequences.fasta\"\n        self.train_labels_path = f\"{self.main_dir}/Train/train_terms.tsv\"\n        self.test_sequences_path = f\"{self.main_dir}/Test/testsuperset.fasta\"\n        self.ia_path = f\"{self.main_dir}/IA.tsv\"\n        \n        # Model parameters\n        self.num_labels = 500\n        self.n_epochs = 96\n        self.batch_size = 96\n        self.dropout_rate = 0.3\n        self.patience = 15\n        \n        # Base learning rate\n        self.lr = 4.75e-5  \n        self.min_lr = 9.95e-8\n        \n        # Dynamic learning rate configuration\n        self.use_dynamic_lr = True\n        self.lr_scheduler_type = \"exponential\"  # selections are ['exponential', 'cosine', 'plateau']\n        self.lr_params = {\n            \"decay_rate\": 0.991,\n            \"decay_steps\": 2,\n            \"min_lr\": self.min_lr,\n            \"T_max\": 10,  # cosine only\n            \"factor\": 0.5,  # plateau only\n        }\n\n        # Device\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Embedding configurations\n        self.embeds_map = {\n            \"ESM2\": \"cafa-5-ems-2-embeddings-numpy\",\n            \"ProtBERT\": \"protbert-embeddings-for-cafa5\",\n            \"T5\": \"t5embeds\"\n        }\n\n        self.embeds_dim = {\n            \"ESM2\": 1280,\n            \"ProtBERT\": 1024,\n            \"T5\": 1024\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:08:14.115114Z","iopub.execute_input":"2025-11-08T03:08:14.115447Z","iopub.status.idle":"2025-11-08T03:08:14.124056Z","shell.execute_reply.started":"2025-11-08T03:08:14.115425Z","shell.execute_reply":"2025-11-08T03:08:14.122921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class declaration\nconfig = Config()\nprint(f\"Using device: {config.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:08:15.737458Z","iopub.execute_input":"2025-11-08T03:08:15.73782Z","iopub.status.idle":"2025-11-08T03:08:15.74391Z","shell.execute_reply.started":"2025-11-08T03:08:15.737797Z","shell.execute_reply":"2025-11-08T03:08:15.742738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset loader","metadata":{}},{"cell_type":"code","source":"#dataset builder\nclass ProteinDataset(Dataset):\n    \"\"\"Dataset class for protein sequences and their GO annotations\"\"\"\n    \n    def __init__(self, datatype, embeddings_source, config):\n        super().__init__()\n        self.datatype = datatype\n        self.config = config\n        self.embeddings_source = embeddings_source\n        \n        # Load embeddings\n        self._load_embeddings()\n        \n        # Load labels for training data\n        if self.datatype == \"train\":\n            self._load_labels()\n        #loading embeded data\n    def _load_embeddings(self):\n        \"\"\"Load pre-computed embeddings\"\"\"\n        embed_dir = f\"{main_path}/{self.config.embeds_map[self.embeddings_source]}\"\n        \n        if self.embeddings_source == \"ESM2\":\n            embeds = np.load(f\"{embed_dir}/{self.datatype}_embeddings.npy\")\n            ids = np.load(f\"{embed_dir}/{self.datatype}_ids.npy\")\n        elif self.embeddings_source == \"ProtBERT\":\n            embeds = np.load(f\"{embed_dir}/{self.datatype}_embeddings.npy\")\n            ids = np.load(f\"{embed_dir}/{self.datatype}_ids.npy\")\n        elif self.embeddings_source == \"T5\":\n            embeds = np.load(f\"{embed_dir}/{self.datatype}_embeds.npy\")\n            ids = np.load(f\"{embed_dir}/{self.datatype}_ids.npy\")\n        \n        # Create DataFrame\n        self.df = pd.DataFrame({\n            \"EntryID\": ids,\n            \"embed\": [embeds[i] for i in range(embeds.shape[0])]\n        })\n    #loading labels\n    def _load_labels(self):\n        \"\"\"Load GO term labels for training data\"\"\"\n        # Load pre-processed top labels if available\n        label_file = f\"/kaggle/input/train-targets-top{self.config.num_labels}/train_targets_top{self.config.num_labels}.npy\"\n        \n        if os.path.exists(label_file):\n            np_labels = np.load(label_file)\n            df_labels = pd.DataFrame({\n                'EntryID': self.df['EntryID'],\n                'labels_vect': [row for row in np_labels]\n            })\n            self.df = self.df.merge(df_labels, on=\"EntryID\", how=\"inner\")\n        else:\n            # Process labels from scratch\n            self._process_labels_from_tsv()\n    def _process_labels_from_tsv(self):\n        \"\"\"Process labels from the TSV file\"\"\"\n        labels_df = pd.read_csv(self.config.train_labels_path, sep=\"\\t\", names=[\"EntryID\", \"term\", \"aspect\"])\n        \n        # Get top terms\n        top_terms = labels_df.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n        self.top_terms = top_terms[:self.config.num_labels].index.tolist()\n        \n        # Create label vectors\n        label_vectors = []\n        for entry_id in self.df['EntryID']:\n            entry_terms = labels_df[labels_df['EntryID'] == entry_id]['term'].tolist()\n            vector = [1 if term in entry_terms else 0 for term in self.top_terms]\n            label_vectors.append(vector)\n        \n        self.df['labels_vect'] = label_vectors\n    \n    def __len__(self):\n        return len(self.df)\n    #getting item\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        embed = torch.tensor(row[\"embed\"], dtype=torch.float32)\n        \n        if self.datatype == \"train\":\n            labels = torch.tensor(row[\"labels_vect\"], dtype=torch.float32)\n            return embed, labels\n        else:\n            return embed, row[\"EntryID\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:08:26.799252Z","iopub.execute_input":"2025-11-08T03:08:26.799618Z","iopub.status.idle":"2025-11-08T03:08:26.814534Z","shell.execute_reply.started":"2025-11-08T03:08:26.799593Z","shell.execute_reply":"2025-11-08T03:08:26.813372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Building and scheduler","metadata":{}},{"cell_type":"markdown","source":"## The LoRA schedule","metadata":{}},{"cell_type":"code","source":"# 1. Building Low-Rank Adapter Layer (LoRA)\nclass LoRALinear(nn.Module):\n    def __init__(self, in_features, out_features, r=6, alpha= 6, dropout= 0.035, bias=True):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha if alpha is not None else r\n        self.scaling = self.alpha / max(1, self.r)\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n\n        if r > 0:\n            # low-rank matrices\n            self.lora_A = nn.Parameter(torch.randn(r, in_features) * 0.01)\n            self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n        else:\n            self.register_parameter('lora_A', None)\n            self.register_parameter('lora_B', None)\n\n    def forward(self, x):\n        base = self.linear(x)\n        if self.r > 0:\n            # compute adapter output (initially 0 due to B = 0)\n            x_d = self.dropout(x)\n            a_out = F.linear(x_d, self.lora_A)       # (batch, r)\n            b_out = F.linear(a_out, self.lora_B)     # (batch, out)\n            return base + b_out * self.scaling\n        else:\n            return base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:08:31.354524Z","iopub.execute_input":"2025-11-08T03:08:31.354989Z","iopub.status.idle":"2025-11-08T03:08:31.365178Z","shell.execute_reply.started":"2025-11-08T03:08:31.35496Z","shell.execute_reply":"2025-11-08T03:08:31.363975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. LoRA-Enhanced Hybrid Model\nclass HybridModel(nn.Module):\n    \"\"\"Hybrid model combining CNN and LSTM features with LoRA for safe fine-tuning\"\"\"\n\n    def __init__(self, input_dim, num_classes, dropout_rate=0.3, lora_r=6, lora_alpha= 6, lora_dropout=0.035):\n        super().__init__()\n\n        # ----- CNN branch (unchanged) -----\n        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.AdaptiveMaxPool1d(128)\n\n        # ----- LSTM branch (unchanged) -----\n        self.lstm = nn.LSTM(input_dim, 256, num_layers=2,\n                            bidirectional=True, dropout=dropout_rate, batch_first=True)\n\n        # Optional small projection for LSTM output to apply LoRA safely\n        self.lstm_proj = LoRALinear(512, 512, r=lora_r, alpha=lora_alpha, dropout=lora_dropout)\n\n        # ----- Fusion and Classification (LoRA applied here) -----\n        self.fc1 = LoRALinear(64 * 128 + 512, 512, r=lora_r, alpha=lora_alpha, dropout=lora_dropout)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc2 = LoRALinear(512, 256, r=lora_r, alpha=lora_alpha, dropout=lora_dropout)\n        self.fc3 = LoRALinear(256, num_classes, r=lora_r, alpha=lora_alpha, dropout=lora_dropout)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n\n        # ----- CNN branch -----\n        cnn_x = x.unsqueeze(1)           # (B, 1, seq_len)\n        cnn_x = F.relu(self.conv1(cnn_x))\n        cnn_x = F.relu(self.conv2(cnn_x))\n        cnn_x = self.pool(cnn_x)\n        cnn_x = cnn_x.view(batch_size, -1)\n\n        # ----- LSTM branch -----\n        lstm_x = x.unsqueeze(1)          # maintain consistency with original\n        lstm_out, _ = self.lstm(lstm_x)\n        lstm_x = lstm_out[:, -1, :]      # take final hidden state\n        lstm_x = self.lstm_proj(lstm_x)  # safe LoRA projection\n\n        # ----- Concatenate and classify -----\n        combined = torch.cat([cnn_x, lstm_x], dim=1)\n\n        x = F.relu(self.fc1(combined))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:08:45.097899Z","iopub.execute_input":"2025-11-08T03:08:45.098589Z","iopub.status.idle":"2025-11-08T03:08:45.109407Z","shell.execute_reply.started":"2025-11-08T03:08:45.098563Z","shell.execute_reply":"2025-11-08T03:08:45.108329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#initiating 'smooth loss'\nclass SmoothBCEWithLogitsLoss(nn.Module):\n    def __init__(self, smoothing=0.0045):\n        super().__init__()\n        self.smoothing = smoothing\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, pred, target):\n        target = target * (1 - self.smoothing) + 0.5 * self.smoothing\n        return self.loss(pred, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:24.59677Z","iopub.execute_input":"2025-11-08T03:09:24.597142Z","iopub.status.idle":"2025-11-08T03:09:24.603749Z","shell.execute_reply.started":"2025-11-08T03:09:24.597118Z","shell.execute_reply":"2025-11-08T03:09:24.602748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#optional modificaiton in LoRA training\ndef mark_only_lora_trainable(model):\n    for name, param in model.named_parameters():\n        param.requires_grad = False\n    for module in model.modules():\n        if isinstance(module, LoRALinear):\n            if module.lora_A is not None:\n                module.lora_A.requires_grad = True\n            if module.lora_B is not None:\n                module.lora_B.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:24.825029Z","iopub.execute_input":"2025-11-08T03:09:24.82537Z","iopub.status.idle":"2025-11-08T03:09:24.831494Z","shell.execute_reply.started":"2025-11-08T03:09:24.825347Z","shell.execute_reply":"2025-11-08T03:09:24.830449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Gaining LoRA parameters\ndef get_lora_params(model):\n    return [p for p in model.parameters() if p.requires_grad]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:25.700748Z","iopub.execute_input":"2025-11-08T03:09:25.701426Z","iopub.status.idle":"2025-11-08T03:09:25.70622Z","shell.execute_reply.started":"2025-11-08T03:09:25.701401Z","shell.execute_reply":"2025-11-08T03:09:25.705144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extra adjutments in schedule","metadata":{}},{"cell_type":"code","source":"# Making scheduler before training\ndef get_scheduler(optimizer, config):\n    def lr_lambda(current_step):\n        warmup_steps = 8\n        total_steps = config.n_epochs\n        if current_step < warmup_steps:\n            return float(current_step) / float(max(1, warmup_steps))\n        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        return max(0.1, 0.5 * (1.0 + np.cos(np.pi * progress)))\n    return LambdaLR(optimizer, lr_lambda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:26.151208Z","iopub.execute_input":"2025-11-08T03:09:26.151562Z","iopub.status.idle":"2025-11-08T03:09:26.158134Z","shell.execute_reply.started":"2025-11-08T03:09:26.151539Z","shell.execute_reply":"2025-11-08T03:09:26.156899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The **'Running phase'** in training and validation","metadata":{}},{"cell_type":"code","source":"#gc case\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:26.824348Z","iopub.execute_input":"2025-11-08T03:09:26.825113Z","iopub.status.idle":"2025-11-08T03:09:27.043855Z","shell.execute_reply.started":"2025-11-08T03:09:26.825087Z","shell.execute_reply":"2025-11-08T03:09:27.042801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## running phase","metadata":{}},{"cell_type":"code","source":"#training phase\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    losses = []\n    \n    for embeddings, labels in tqdm(dataloader, desc=\"Training\"):\n        embeddings, labels = embeddings.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(embeddings)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n    \n    return np.mean(losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:28.057142Z","iopub.execute_input":"2025-11-08T03:09:28.057448Z","iopub.status.idle":"2025-11-08T03:09:28.064288Z","shell.execute_reply.started":"2025-11-08T03:09:28.057428Z","shell.execute_reply":"2025-11-08T03:09:28.063362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#validation\ndef validate_epoch(model, dataloader, criterion, metric, device):\n    \"\"\"Validate the result for one epoch\"\"\"\n    model.eval()\n    losses = []\n    scores = []\n    \n    with torch.no_grad():\n        for embeddings, labels in dataloader:\n            embeddings, labels = embeddings.to(device), labels.to(device)\n            \n            outputs = model(embeddings)\n            loss = criterion(outputs, labels)\n            score = metric(torch.sigmoid(outputs), labels.int())\n            \n            losses.append(loss.item())\n            scores.append(score.item())\n    \n    return np.mean(losses), np.mean(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:29.006873Z","iopub.execute_input":"2025-11-08T03:09:29.007213Z","iopub.status.idle":"2025-11-08T03:09:29.014526Z","shell.execute_reply.started":"2025-11-08T03:09:29.00719Z","shell.execute_reply":"2025-11-08T03:09:29.013361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#running the training function with stabilizer\ndef train_model(embeddings_source=\"ESM2\", model_type=\"hybrid\", train_ratio= 0.8):\n    print(f\"\\nTraining {model_type} model with {embeddings_source} embeddings...\")\n\n    # Dataset split\n    dataset = ProteinDataset(\"train\", embeddings_source, config)\n    train_size = int(len(dataset) * train_ratio)\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n\n    # Model\n    input_dim = config.embeds_dim[embeddings_source]\n    model = HybridModel(input_dim, config.num_labels, dropout_rate= 0.275, lora_r= 6, lora_alpha= 6, lora_dropout=0.1).to(config.device)\n\n    # Freeze all except LoRA\n    mark_only_lora_trainable(model)\n    lora_params = get_lora_params(model)\n\n    # Optimizer + Scheduler\n    criterion = SmoothBCEWithLogitsLoss(smoothing= 0.0045)\n    optimizer = torch.optim.AdamW(lora_params, lr=config.lr * 0.5, weight_decay= 4.4e-4)\n    scheduler = get_scheduler(optimizer, config)\n    metric = MultilabelF1Score(num_labels=config.num_labels, average='micro').to(config.device)\n\n    best_val_score = 0\n    train_losses, val_losses, val_scores, lrs = [], [], [], []\n\n    # ---------- Training Loop ----------\n    for epoch in range(config.n_epochs):\n        model.train()\n        epoch_losses = []\n        grad_means = []\n\n        for embeddings, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.n_epochs}\"):\n            embeddings, labels = embeddings.to(config.device), labels.to(config.device)\n            optimizer.zero_grad()\n            outputs = model(embeddings)\n            loss = criterion(outputs, labels)\n            loss.backward()\n\n            # Gradient clipping for stability\n            torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)\n\n            optimizer.step()\n            epoch_losses.append(loss.item())\n\n            # Track mean gradient magnitude (diagnostic)\n            with torch.no_grad():\n                grads = [p.grad.abs().mean().item() for p in lora_params if p.grad is not None]\n                if grads:\n                    grad_means.append(np.mean(grads))\n\n        train_loss = np.mean(epoch_losses)\n        mean_grad = np.mean(grad_means) if grad_means else 0.0\n\n        # ---------- Validation ----------\n        val_loss, val_score = validate_epoch(model, val_loader, criterion, metric, config.device)\n\n        # ---------- Scheduler step ----------\n        scheduler.step()\n        current_lr = optimizer.param_groups[0][\"lr\"]\n        lrs.append(current_lr)\n\n        # ---------- Logging ----------\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_scores.append(val_score)\n\n        print(f\"Epoch {epoch+1}/{config.n_epochs} | \"\n              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n              f\"Val F1: {val_score:.4f} | LR: {current_lr:.6e} | Mean Grad: {mean_grad:.6f}\")\n\n        # Save best model\n        if val_score > best_val_score:\n            best_val_score = val_score\n            best_model_state = model.state_dict().copy()\n\n    # ---------- Return ----------\n    return model, {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_scores': val_scores,\n        'lrs': lrs,\n        'best_score': best_val_score\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:35.116201Z","iopub.execute_input":"2025-11-08T03:09:35.116595Z","iopub.status.idle":"2025-11-08T03:09:35.131196Z","shell.execute_reply.started":"2025-11-08T03:09:35.116572Z","shell.execute_reply":"2025-11-08T03:09:35.130168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#another gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:09:37.233199Z","iopub.execute_input":"2025-11-08T03:09:37.233593Z","iopub.status.idle":"2025-11-08T03:09:37.457954Z","shell.execute_reply.started":"2025-11-08T03:09:37.233549Z","shell.execute_reply":"2025-11-08T03:09:37.456882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# actual running\nmodel, history = train_model(\n    embeddings_source=\"ESM2\",\n    model_type=\"hybrid\"\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## training evaluation","metadata":{}},{"cell_type":"code","source":"#plot in training\ndef plot_training_curves(history):\n    epochs = range(1, len(history['train_losses']) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_losses'], label='Train Loss', color='black')\n    plt.plot(epochs, history['val_losses'], label='Validation Loss', color='red')\n    plt.title('Training & Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['val_scores'], label='Validation F1', color='green')\n    plt.title('Validation F1 Score')\n    plt.xlabel('Epoch')\n    plt.ylabel('F1 Score')\n    plt.legend()\n\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plot in learning curve\ndef plot_learning_rate(history):\n    plt.figure(figsize=(6,4))\n    plt.plot(history['lrs'], marker='o', color='purple')\n    plt.title('Learning Rate over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('LR')\n    plt.yscale('log')\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tunning training plot\nplot_training_curves(history)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#running learning curve plot\nplot_learning_rate(history)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#final result display\nprint(f\"Best Validation F1 Score: {history['best_score']:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## prediction inside the validation","metadata":{}},{"cell_type":"code","source":"#function to predict actual results\ndef predict(model, embeddings_source=\"ESM2\"):\n    \"\"\"Generate predictions for test set\"\"\"\n    \n    print(\"\\nGenerating predictions...\")\n    \n    # Load test dataset\n    test_dataset = ProteinDataset(\"test\", embeddings_source, config)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n    \n    # Get label names\n    labels_df = pd.read_csv(config.train_labels_path, sep=\"\\t\", names=[\"EntryID\", \"term\", \"aspect\"])\n    top_terms = labels_df.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n    label_names = top_terms[:config.num_labels].index.tolist()\n    \n    # Generate predictions\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        for embeddings, protein_id in tqdm(test_loader, desc=\"Predicting\"):\n            embeddings = embeddings.to(config.device)\n            outputs = torch.sigmoid(model(embeddings)).cpu().numpy().squeeze()\n            \n            for i, conf in enumerate(outputs):\n                if conf > 0.05:  # Only include predictions above threshold\n                    predictions.append({\n                        'Id': protein_id[0],\n                        'GO term': label_names[i],\n                        'Confidence': min(conf, 0.95)  # Scientific standard\n                    })\n    \n    return pd.DataFrame(predictions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model prediciton\npreds = predict(model, embeddings_source=\"ESM2\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Submission","metadata":{}},{"cell_type":"code","source":"#declaring main prediction\nexisting_pred_path = f\"{main_path}/blast-quick-sprof-zero-pred/submission.tsv\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#preparing submission\nexisting = pd.read_csv(existing_pred_path, sep='\\t', header=None, names=['Id', 'GO term', 'Confidence'])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merging the data for actual submission\nmerged = pd.merge(existing, preds, on=['Id', 'GO term'], how='outer', suffixes=('_existing', '_new'))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#emergency gc\ngc.collect()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine confidences (take maximum or average)\nmerged['Confidence'] = merged[['Confidence_existing', 'Confidence_new']].max(axis=1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#actual submitted data\nfinal_predictions = merged[['Id', 'GO term', 'Confidence']].copy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving submission\nprint(\"\\nSaving submission file...\")\nfinal_predictions.to_csv('submission.tsv', sep='\\t', header=False, index=False)\nprint(f\"Submission saved with {len(final_predictions)} predictions\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T04:30:00.668Z"}},"outputs":[],"execution_count":null}]}