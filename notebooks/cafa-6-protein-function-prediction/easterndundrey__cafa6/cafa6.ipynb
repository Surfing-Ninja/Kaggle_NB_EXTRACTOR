{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## WARNING: Do not run this to get high score. This notebook is a case of breaking down bad assumptions. For high competition scores, check other posts. ","metadata":{}},{"cell_type":"markdown","source":"Many thanks to Seddik Turki (https://www.kaggle.com/code/seddiktrk/cafa-6-predictions) and VBS2004 (https://www.kaggle.com/code/vbs2004/cafa-6-protein-ensemble-silver-medal?scriptVersionId=271694892) for the public code releases.","metadata":{}},{"cell_type":"markdown","source":"### Disclaimer: Do not run this as your competition submission. This is for educational purposes only. Without significant fixes, this will crash!","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Prediction Files\n\nWe'll load both model predictions. Easy peasy - just read them all into memory at once!","metadata":{}},{"cell_type":"code","source":"# Load first model predictions\nmodel1 = pd.read_csv('/kaggle/input/cafa-6-t5-embeddings-with-ensemble/submission.tsv', \n                      sep='\\t', header=None, names=['protein', 'go_term', 'score'])\n\n# Load second model predictions  \nmodel2 = pd.read_csv('/kaggle/input/cafa-6-predictions/submission.tsv',\n                      sep='\\t', header=None, names=['protein', 'go_term', 'score'])\n\nprint(f\"Model 1: {len(model1)} predictions\")\nprint(f\"Model 2: {len(model2)} predictions\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Combine the Models\n\nNow we merge them together. We'll use a simple average because all models are equally good, right? No need for fancy weights!","metadata":{}},{"cell_type":"code","source":"# Rename score columns so we can track them\nmodel1 = model1.rename(columns={'score': 'score1'})\nmodel2 = model2.rename(columns={'score': 'score2'})\n\n# Merge on protein and go_term\ncombined = model1.merge(model2, on=['protein', 'go_term'], how='inner')\n\nprint(f\"Combined predictions: {len(combined)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Calculate Ensemble Score\n\nSimple averaging - just add the scores and divide by 2! Math is fun! âž•âž—","metadata":{}},{"cell_type":"code","source":"# Calculate the average score\ncombined['final_score'] = (combined['score1'] + combined['score2']) / 2\n\n# Keep only what we need\nresult = combined[['protein', 'go_term', 'final_score']]\n\nprint(f\"Final predictions: {len(result)}\")\nprint(f\"\\nSample predictions:\")\nprint(result.head())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Submission\n\nWrite it out to a TSV file. Done! ðŸŽ‰","metadata":{}},{"cell_type":"code","source":"result.to_csv('submission.tsv', sep='\\t', index=False, header=False)\n\nprint(\"\\n Submission saved successfully!\")\nprint(f\"Total predictions in submission: {len(result)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# Problems that may be detrimental to the submission\n\n### 1. **Memory Disaster** \n- Loading 31+ million predictions all at once into memory\n- No chunking or streaming - will crash on most systems\n- Original used `chunksize=10_000_000` for good reason!\n\n### 2. **Wrong Merge Type** \n- Using `how='inner'` instead of `how='outer'`\n- This throws away ALL predictions that don't appear in both models\n- You lose most of your predictions! Original had ~31M, this might have ~5M\n\n### 3. **No Handling of Missing Values** \n- When predictions are missing from one model, they become NaN\n- Should fill with 0 (no confidence) but we don't\n- Results in NaN final scores = invalid predictions\n\n### 4. **Equal Weights** \n- Original used `[0.2, 0.8]` weights because one model performs better\n- This uses `[0.5, 0.5]` - treats bad and good models equally\n- Drags down performance significantly\n\n### 5. **No Data Validation** \n- Missing `dropna()` checks for malformed rows\n- No dtype specifications in `read_csv`\n- No handling of duplicate predictions\n\n### 6. **Inefficient Processing** \n- No temporary file system for large operations\n- Keeps everything in memory all the time\n- No progress tracking for long operations\n\n### 7. **Lost Edge Cases** \n- Removed the clever `key` concatenation approach\n- Doesn't handle proteins/GO terms with underscores properly\n- Direct merge on two columns is less robust\n\n### 8. **Score Distribution Issues** \n- Simple averaging can create weird score distributions\n- No normalization or calibration\n- Weighted ensemble preserves confidence levels better\n\n## Performance Impact:\n\n- **Original**: Processes all 31.5M predictions efficiently\n- **This version**: \n  - Will crash due to memory on most systems\n  - If it runs, loses 80%+ of predictions due to inner merge\n  - Remaining predictions have equal-weight averaging \n  - Competition score would be bad\n\n## What Makes the Original Good:\n\nMemory-efficient chunking for huge datasets\n\nOuter merge preserves ALL predictions from both models\n\nProper filling of missing values (0 for absent predictions)\n\nWeighted ensemble based on model quality\n\nRobust handling of edge cases with key concatenation\n\nProgress tracking and temporary file management\n\nData validation and error handling\n","metadata":{}}]}