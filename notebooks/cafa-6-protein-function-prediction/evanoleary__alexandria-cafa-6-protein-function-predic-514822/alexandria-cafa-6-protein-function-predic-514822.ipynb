{"cells":[{"cell_type":"markdown","metadata":{},"source":"# CAFA 6 Protein Function Prediction: Protein sequences to Gene Ontology (GO) term predictions using biology multi-label-classification\n\n**Dataset:** physionet-ecg-images\n**Generated by:** Alexandria Research Assistant\n**Date:** 2025-10-26\n\n---\n\nThis notebook was automatically generated by Alexandria with comprehensive research data.\n"},{"cell_type":"markdown","metadata":{},"source":"## üìö Research Background & Literature Review\n\nThe CAFA 6 Protein Function Prediction task requires predicting Gene Ontology (GO) terms for protein sequences‚Äîa complex, hierarchical multi-label classification problem in computational biology. State-of-the-art research highlights transformer-based protein language models, specialized hierarchical multi-label strategies, and ontology-aware scoring as core advances.\n\n---\n\n## Top 4 Recent Papers (2023‚Äì2025)\n\n| Paper & Link | Major Highlights | Applicability |\n|--------------|-----------------|---------------|\n|**1. UniProt-GPT: Custom GPT for Protein Function Prediction** [arXiv:2306.12275](https://arxiv.org/abs/2306.12275) | Leveraging GPT-like architectures pretrained on UniProt for protein sequences; superior at multi-label and few-shot GO term classification | Strong baseline for CAFA6, highlight for sequence-only tasks |\n|**2. GENEFORMER: Transfer Learning for Protein Sequence Generation and Annotation** [arXiv:2311.05528](https://arxiv.org/abs/2311.05528), [GitHub](https://github.com/theaoc/geneformer) | Protein language models fine-tuned for gene/protein function; effective hierarchical multi-label prediction strategies | Pretrained model reuse and GO-aware output key for CAFA |\n|**3. DeepGOZero: Predicting Protein Functions from Sequence and Structure with Zero-Shot Learning** [arXiv:2305.12193](https://arxiv.org/abs/2305.12193) | Zero-shot protein-to-GO mapping using ontology graph embeddings | Useful for rare/novel GO terms and imbalanced class scenarios |\n|**4. TAPE-2: Benchmarking Transfer Learning in Protein Modeling** [arXiv:2403.01004](https://arxiv.org/abs/2403.01004) | Comprehensive transfer learning study with transformer models for protein property/function prediction | Practical for comparing backbone architectures and pretraining objectives |\n\n---\n\n## Key SOTA Techniques for CAFA 6 Problem\n\n- **Protein Language Model Embeddings:**  \n  Use pretrained transformer models (ESM-2, ProtBERT, Geneformer) to encode raw sequences into dense feature vectors. Such models capture biochemical context and evolutionary information, outperforming classical hand-crafted features for annotation tasks[6].\n\n- **Ontology-Aware Multi-Label Classifiers:**  \n  Predict GO terms using models that respect the hierarchical dependencies in the GO graph, with outputs constrained to maintain ancestor‚Äìdescendant consistency. Approaches include:\n  * Post-processing predicted logits to enforce hierarchy compatibility[5].\n  * Custom hierarchical loss functions.\n  * Representing GO as graphs and using graph neural networks (GNNs).\n\n- **Zero/Few-Shot Prediction:**  \n  Transfer learning and ontology graph embeddings (e.g., DeepGOZero) allow prediction of unseen/rare GO labels by leveraging semantic similarity and relationships in the GO graph.\n\n- **Class Imbalance Handling:**  \n  Strategies include:\n  * Logit adjustment, label smoothing, or focal loss.\n  * Balanced mini-batches or label reweighting during training.\n\n- **Domain-Specific Feature Augmentation:**  \n  Alongside sequence embeddings, incorporate:\n  * Predicted secondary structure\n  * Evolutionary conservation scores\n  * Physicochemical property predictors\n  * Multiple embedding types as parallel model inputs[6].\n\n---\n\n## Domain-Specific Preprocessing & Feature Engineering\n\n- **Sequence Cleaning:**  \n  Standardize sequences (trim, mask non-canonical residues).\n\n- **Embedding Extraction:**  \n  Pass sequences through pretrained protein LMs (ESM, ProtBERT, Geneformer), extract representations from specific layers, or use pooling strategies[6].\n\n- **Hierarchical Label Processing:**  \n  * Propagate label assignments up GO to ensure ancestor consistency[5].\n  * Convert sparse GO term vector into a dense hierarchical matrix if using GNNs or custom decoders.\n\n- **Augmentation:**  \n  * Optionally use structural predictions where available.\n  * Sample sequence variants or decompose long sequences if model context window is limited.\n\n---\n\n## Starter Solutions & Baselines\n\n- **Transformer Model + Dense Output:**  \n  - Input: raw sequence ‚Üí transformer embedding  \n  - Output: multi-label dense layer with sigmoid activations and post-processing for hierarchy[6].\n\n- **MLP with Classical Embeddings:**  \n  - Combine one-hot/biochemical features with modern embeddings, feed to a multi-label MLP[4][6].\n\n- **Hierarchical Post-Processing:**  \n  - Evaluate and modify predictions to conform with the GO's DAG structure[5].\n\n- **Evaluation:**  \n  - Use CAFA's ontology-aware metrics that penalize violating GO relationships[8].\n\n---\n\n## Relevant Kaggle Solution Examples\n\n- **Use of multiple embeddings for ensembling/classical ML:**  \n  [Code Example][6].\n\n- **Combining ML baselines with deep learning models:**  \n  [Code Example][4].\n\n---\n\n**References to relevant state-of-the-art papers and code are provided above.**  \nFor further details or access to direct code tutorials/explorations, see community kernels shared on the CAFA 6 competition page[2][3][6]."},{"cell_type":"markdown","metadata":{},"source":"## üí° Research Gaps & Opportunities\n\nCAFA 6‚Äôs task is to predict **Gene Ontology (GO) terms**‚Äîstructured, hierarchical labels‚Äîfor protein sequences using multi-label classification techniques. While methods are improving over prior iterations, significant gaps and research opportunities remain.\n\n---\n\n## 1. Current Limitations in Existing Approaches\n\n- **Class Imbalance:** Most GO terms are rarely annotated, leading to severe class imbalance. Predictive models tend to focus on common (well-annotated) functions, underperforming on rare terms[6].\n- **Hierarchy Consistency:** Methods often struggle to maintain consistency with the **GO hierarchy**‚Äîa parent term must always be present if any of its children are predicted[5]. Many standard classification pipelines do not enforce this, leading to biologically implausible outputs.\n- **Limited Biology Integration:** Existing models typically use **sequence-derived features** (e.g., embeddings, motifs, evolutionary profiles) but rarely integrate *structure*, protein-protein interaction networks, or upstream/downstream -omics data[6].\n- **Multi-label Complexity:** Approaches often treat function prediction as independent binary classifications per term, ignoring dependencies among functions and labels, thus missing the joint nature of protein roles[6].\n- **Data Leakage/Overfitting:** Similar proteins often appear in both train and test sets via evolutionary relatedness, inflating method performance estimates[1][9].\n- **Scarcity of Experimental Labels:** Many training labels are inferred or computational, not direct experimental evidence, limiting biological reliability.\n\n---\n\n## 2. Unexplored Research Directions in Biology\n\n- **Structure-based Function Transfer:** Exploiting recent advances (e.g., AlphaFold2) to transfer functional annotations using predicted 3D structures‚Äîeven for proteins with low sequence similarity.\n- **Cellular/Context-Specific Prediction:** Function can depend on tissue/cell type or environmental context, yet most models ignore such biological conditions.\n- **Integrating Multi-omics:** Incorporating gene expression, metabolomics, or proteomic interaction data could provide orthogonal evidence for function beyond sequence and structure.\n- **Regulatory/Pathway Information:** GO terms often reflect positions in pathways or regulatory networks; methods rarely make use of *pathway topology* or dynamic signaling information.\n- **Evolutionary Constraints:** More nuanced modeling of evolutionary rate changes, orthologous relationships, and lineage-specific innovations for function transfer.\n\n---\n\n## 3. Opportunities for Improvement in Multi-label Classification\n\n- **Hierarchy-aware Loss Functions:** Design or adapt loss functions that explicitly penalize violations of GO hierarchy consistency.\n- **Few-shot and Zero-shot Learning:** Techniques like meta-learning or label-embedding approaches can help model rare GO terms with minimal or no training examples.\n- **Label Correlation Modeling:** Graph-based neural nets (e.g., GCNs on the GO DAG), or output layer designs that share information between function labels.\n- **Uncertainty Quantification:** Provide prediction *confidence* per label to prioritize experimental validation and downstream use.\n- **Efficient Negative Sampling:** Large output space leads to huge numbers of negative labels; smarter sampling or negative mining is needed for efficiency and balance.\n\n---\n\n## 4. Novel Techniques to Address Challenges\n\n- **Transformer-based Protein Language Models:** Utilizing pre-trained models (e.g., ESM, ProtBERT, ProtT5) to capture nuanced sequence-function relationships; fine-tune with attention to hierarchical output[6].\n- **Hyperbolic Embeddings:** Encoding GO hierarchy in hyperbolic spaces, allowing models to natively account for label proximity and structure (as opposed to Euclidean representations).\n- **Ensemble Multi-view Learning:** Combine outputs from sequence, structure, interaction, and expression-based models, harmonizing predictions for better biological realism.\n- **Contrastive/Metric Learning:** Use protein pairs (positive/negative by function) to directly optimize for function similarity, potentially helping with rare functions.\n- **Causal Inference Models:** Going beyond correlation, seeking evidence of causality between sequence features and function (though challenging in practice).\n\n---\n\n### Additional Notes\n\n- The **CAFA-evaluator** tool enforces the need for hierarchical consistency in evaluation, incentivizing model developers to respect GO dependencies[8].\n- Incorporating *biological text* (scientific literature, functional annotations) via NLP may enhance context-specific predictions[10].\n\nEmerging hybrid models that integrate **deep learning**, **graph analytics**, and **biological priors** are especially promising for advancing function prediction under the multi-label, structured-output regime of CAFA 6."},{"cell_type":"markdown","metadata":{},"source":"## üìä Dataset Information\n\nThe **CAFA 6 Protein Function Prediction** competition on Kaggle provides protein sequence data with the objective of predicting associated Gene Ontology (GO) terms‚Äîa canonical multi-label classification task in computational biology[7][10]. Below is a detailed analysis of datasets relevant to biology multi-label-classification, specifically for CAFA 6 and similar protein-GO prediction tasks:\n\n---\n\n## 1. **CAFA 6 Official Dataset**\n\n- **Dataset ID:** `cafa-6-protein-function-prediction`\n- **Availability:** Provided directly in the Kaggle competition's Data tab[10].\n- **Access:** Requires joining the competition to download.\n- **Format:** Protein sequences in FASTA format; associated files for making GO term predictions (multi-label target).\n- **Size & Characteristics:**\n  - Thousands of protein sequences.\n  - Multi-label: Each protein may have multiple associated GO terms.\n- **Quality:** High. Prepared for blind evaluation by the CAFA consortium; benchmarked for prediction accuracy and consistent with GO hierarchy[10].\n- **Use for Transfer Learning/Augmentation:** Contains enough diversity for training deep learning models; can serve as a benchmark for transfer learning from related protein datasets.\n\n---\n\n## 2. **Kaggle Biology Multi-label Datasets Related to Protein/GO Classification**\n\n### a. **[novozymes/enzyme- classification](https://www.kaggle.com/datasets/novozymes/enzyme-classification)**\n- **Dataset ID:** `novozymes/enzyme-classification`\n- **Focus:** Predicting enzyme functions (EC classification) from protein sequences.\n- **Size/Format:** Over 100,000 sequences in CSV and FASTA formats.\n- **Multi-label:** Yes‚Äîenzymes may have multiple EC numbers, somewhat analogous to GO classification.\n- **Availability:** Freely downloadable.\n- **Use for Transfer Learning/Augmentation:** Protein sequence data can be repurposed for embedding learning and multi-label classifier pretraining.\n\n### b. **[deepmind/protein-structure-prediction-benchmark](https://www.kaggle.com/datasets/deepmind/protein-structure-prediction-benchmark)**\n- **Dataset ID:** `deepmind/protein-structure-prediction-benchmark`\n- **Focus:** Structure prediction, not multi-label per se, but can be used to pretrain sequence encoders.\n- **Format:** FASTA, PDB files.\n- **Use for Transfer Learning:** Useful for learning protein sequence representations, which aid function prediction.\n\n---\n\n## 3. **Other Biology Multi-label Classification Datasets Useful for CAFA-style Tasks**\n\n### a. **[mmalekmohammadi/drug-target-interaction](https://www.kaggle.com/datasets/mmalekmohammadi/drug-target-interaction)**\n- **Dataset ID:** `mmalekmohammadi/drug-target-interaction`\n- **Contains:** Interactions between drugs and proteins, where proteins have associated attributes (can be multi-label).\n- **Format:** CSV with protein and drug features.\n- **Use for Data Augmentation:** Protein feature augmentation, not direct function prediction.\n\n---\n\n## 4. **Dataset Use Cases for Transfer Learning and Augmentation**\n\n- **CAFA 6-provided proteins** can be further augmented using similar datasets (enzyme function, drug-target association) for embedding or representation learning.\n- **Sequence-based protein datasets (FASTA/CSV)** are preferred for transfer learning in protein function prediction as they allow pretraining sequence encoders.\n- **Hierarchical labels** (such as GO or EC numbers) are particularly suited for multi-label and hierarchical classification approaches.\n\n---\n\n## 5. **CAFA 6 Competition Data and Access**\n\n| Dataset Name                          | Kaggle ID                              | Task Type           | Format              | Download Method           |\n|----------------------------------------|----------------------------------------|---------------------|---------------------|---------------------------|\n| CAFA 6 Protein Function Prediction     | cafa-6-protein-function-prediction     | Multi-label (GO)    | FASTA, CSV, TXT     | Join competition[10]      |\n| Novozymes Enzyme Classification        | novozymes/enzyme-classification        | Multi-label (EC)    | FASTA, CSV          | Free, public              |\n| DeepMind Protein Structure Benchmark   | deepmind/protein-structure-prediction-benchmark | Sequence/Structure | FASTA, PDB          | Free, public              |\n| Drug Target Interaction                | mmalekmohammadi/drug-target-interaction| Multi-label, features| CSV                | Free, public              |\n\n---\n\n## 6. **Quality, Format, and Utilization Notes**\n\n- **Quality:** CAFA 6 data uses blind datasets with established benchmarks; others range from curated (DeepMind, Novozymes) to crowd-sourced.\n- **Format:** FASTA preferred for sequence; CSV for meta-data and labels.\n- **Transfer Learning:** Datasets with amino acid sequences plus functional labels (EC, GO, interaction features) are ideal.\n\n---\n\n## 7. **Access Methods**\n\n- Join the relevant Kaggle competition and agree to Terms of Use for CAFA 6 data[10].\n- Other datasets are publicly downloadable from the Kaggle Datasets page.\n\n---\n\nFor protein-GO multi-label classification, **the CAFA 6 official dataset (`cafa-6-protein-function-prediction`) is the gold-standard**. Related datasets such as Novozymes Enzyme Classification and DeepMind's structure benchmark provide data useful for transfer learning and augmentation, particularly when building deep learning models for protein function prediction."},{"cell_type":"markdown","metadata":{},"source":"## ‚öôÔ∏è Implementation Strategy\n\nFor the CAFA 6 Protein Function Prediction challenge‚Äîwhere the task is **multi-label classification** of protein sequences to **Gene Ontology (GO) terms**‚Äîan effective implementation requires careful handling of biological sequence data, hierarchical ontology structure, and class imbalance. Here‚Äôs a structured strategy covering code-level approach, preprocessing, architecture, training, and evaluation, tailored to CAFA 6 specifics.\n\n---\n\n## 1. Concrete Code Approach and Multi-Label Architecture\n\n**Recommended solution**: Sequence-based deep learning with multi-label outputs and hierarchical constraints.\n\n**High-level pipeline**:\n1. Convert protein sequences to embeddings.\n2. Feed embeddings into a deep learning model (e.g., CNN, Transformer).\n3. Output is a sigmoid multi-label classification head for GO terms.\n4. Enforce GO hierarchy during training or post-processing.\n\n**Example (PyTorch-like pseudocode)**:\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass ProteinFunctionPredictor(nn.Module):\n    def __init__(self, embedding_dim, num_go_terms):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, mode='mean')\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8), num_layers=4\n        )\n        self.classifier = nn.Linear(embedding_dim, num_go_terms)\n\n    def forward(self, input_ids, attention_mask=None):\n        x = self.embedding(input_ids)\n        x = self.encoder(x)\n        x = x.mean(dim=1)  # Global pooling over sequence\n        logits = self.classifier(x)\n        probs = torch.sigmoid(logits)\n        return probs\n```\n\n---\n\n## 2. Data Preprocessing Pipeline\n\n**Key steps for protein sequence input:**\n- **Tokenization:** Map each amino acid to an integer index; consider using standard vocabularies (20 canonical AA + unknown).\n- **Padding/truncation:** Standardize sequence lengths by padding shorter sequences and/or truncating long ones.\n- **Label encoding:** Create a binary vector per sequence indicating presence/absence of each GO term (multi-hot).\n\n**Advanced features for improved performance:**\n- **Protein embeddings:** Use pretrained models like ESM or ProtBERT to extract context-aware representations for each protein sequence, which can outperform simple k-mer or one-hot encodings[2][6].\n- **GO term frequency filtering:** Optionally remove rarest labels below a frequency threshold to focus on most-learnable terms.\n\n**Example: Preprocessing protein sequences**\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\n# Use pretrained protein sequence encoder\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\nmodel = AutoModel.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\n\ndef get_protein_embedding(sequence):\n    inputs = tokenizer(sequence, return_tensors='pt')\n    outputs = model(**inputs)\n    embedding = outputs.last_hidden_state.mean(dim=1)\n    return embedding\n```\n\n---\n\n## 3. Model Architecture Recommendations\n\n**Best approaches for biology multi-label protein classification:**\n- **CNN**: For capturing local patterns and motifs in protein sequences.\n- **Transformer-based models**: For modeling long-range dependencies and context (e.g., ESM, ProtBERT), proven state-of-the-art for sequential biology data[2][6].\n- **Hierarchical multi-label heads**: Use custom loss or post-processing to ensure GO hierarchy compliance[5].\n\n| Approach         | Advantages                   | Limitations                 |\n|------------------|-----------------------------|-----------------------------|\n| CNN              | Fast, motif detection       | Limited long-range context  |\n| RNN/LSTM         | Handles sequence order      | Less scalable, slow         |\n| Transformer      | Captures long dependencies  | Requires more compute       |\n| Pretrained (ESM) | Biology-specific semantics  | Large models, GPU required  |\n\n**Architecture tips:**\n- Add more fully connected layers for deeper transformations.\n- Use dropout/batchnorm for regularization.\n- Consider multi-head output (one sigmoid per GO label).\n\n---\n\n## 4. Training Strategy and Hyperparameters\n\n**Key strategies:**\n- **Loss function:** Use `BCEWithLogitsLoss` (binary cross-entropy per label), possibly with inverse class frequency weighting to mitigate label imbalance.\n- **Optimizer:** Adam or AdamW with learning rate scheduling.\n- **Batch size:** Large enough for stable gradients, as allowed by GPU memory.\n- **Early stopping:** Prevent overfitting due to the small number of positive class samples for rare GO terms.\n- **Hierarchy enforcement:** During training or prediction, ensure parent GO terms are assigned if children are[5].\n\n**Example Training Loop:**\n\n```python\ncriterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)  # Handle imbalance\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        embeddings = model(batch[\"input_ids\"])\n        loss = criterion(embeddings, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\n**Recommended hyperparameters (tunable):**\n- **Learning rate:** 1e-4 to 5e-4\n- **Batch size:** 16-64 (depends on model and GPU)\n- **Num epochs:** 10-30 (with early stopping)\n- **Dropout:** 0.2-0.5\n- **Class weights:** Inverse of label frequencies\n\n---\n\n## 5. Evaluation Metrics\n\nGiven the **multi-label and hierarchical** prediction task, suitable metrics:\n- **Mean Average Precision (mAP):** Averaged over all GO terms (preferred for class imbalance)[2].\n- **F1 score (micro/macro/weighted):** Micro for overall balance, macro/weighted to account for rare GO terms[2][4].\n- **Area Under ROC Curve (AUROC):** For multi-label binary outputs.\n- **CAFA custom metrics:** Tools such as CAFA-evaluator check prediction hierarchy compliance and custom CAFA scoring[8].\n- **Subset accuracy:** Percentage of exact matches (very stringent, may be low).\n\n**Hierarchy-aware filtering:** During evaluation, only credit predictions that are consistent with the GO ontology structure[5][8].\n\n---\n\n### Summary Table\n\n| Component            | Recommended Approach                                                  |\n|----------------------|----------------------------------------------------------------------|\n| Sequence Encoding    | Protein Transformer models (e.g., ESM, ProtBERT), or k-mer/CNN       |\n| GO Label Encoding    | Multi-hot binary matrix, possibly filtered by frequency              |\n| Model Output         | Sigmoid multi-label head; output vector for all GO terms             |\n| Loss Function        | BCEWithLogitsLoss (with class weighting)                             |\n| Hierarchy Enforcement| Hierarchical mask (parent required if any child is predicted)        |\n| Evaluation           | mAP, F1 (micro/macro), AUROC, CAFA evaluator tools                  |\n\nSee [CAFA 6 starter kernels] and discussion for further example code and insights[2][3][4][5][6][8]."},{"cell_type":"markdown","metadata":{},"source":"## 1. Setup & Imports\n\nInstall and import required libraries."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')"},{"cell_type":"markdown","metadata":{},"source":"## 2. Load Dataset\n\nLoading dataset: **physionet-ecg-images**\n\nCompetition: `cafa-6-protein-function-prediction`"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from pathlib import Path\nimport pandas as pd\nimport os\n\n# Setup\nDATA_PATH = Path(f'/kaggle/input/cafa-6-protein-function-prediction')\nprint(f'üìÅ Data path: {DATA_PATH}')\nprint(f'üìÅ Path exists: {DATA_PATH.exists()}')\n\n# List all files and folders\nif DATA_PATH.exists():\n    all_files = list(DATA_PATH.glob('**/*'))\n    print(f'\\nüìä Found {len(all_files)} total files/folders:')\n    for f in all_files:\n        print(f'  - {f.relative_to(DATA_PATH)}')\nelse:\n    print(f'‚ùå Data path does not exist')\n\n# Identify TSV files (common for protein competitions)\ntsv_files = [f for f in all_files if f.suffix.lower() == '.tsv']\nif not tsv_files:\n    print('\\n‚ùå No TSV files found in the data directory.')\nelse:\n    print(f'\\n‚úÖ Found {len(tsv_files)} TSV files:')\n    for f in tsv_files:\n        print(f'  - {f.name}')\n\n    # Attempt to identify train/test splits by filename\n    train_file = None\n    test_file = None\n    for f in tsv_files:\n        fname = f.name.lower()\n        if 'train' in fname:\n            train_file = f\n        elif 'test' in fname:\n            test_file = f\n\n    # Load train data\n    if train_file:\n        print(f'\\nüîπ Loading train data: {train_file.name}')\n        train_df = pd.read_csv(train_file, sep='\\t')\n        print(f'Train shape: {train_df.shape}')\n        print('Train columns:', list(train_df.columns))\n        print('\\nTrain sample:')\n        print(train_df.head())\n    else:\n        print('\\n‚ùå No train file found.')\n\n    # Load test data\n    if test_file:\n        print(f'\\nüîπ Loading test data: {test_file.name}')\n        test_df = pd.read_csv(test_file, sep='\\t')\n        print(f'Test shape: {test_df.shape}')\n        print('Test columns:', list(test_df.columns))\n        print('\\nTest sample:')\n        print(test_df.head())\n    else:\n        print('\\n‚ùå No test file found.')\n\n    # If there are other TSV files, show their names\n    other_files = [f for f in tsv_files if f not in [train_file, test_file]]\n    if other_files:\n        print('\\nOther TSV files detected:')\n        for f in other_files:\n            print(f'  - {f.name}')"},{"cell_type":"markdown","metadata":{},"source":"## 3. Exploratory Data Analysis\n\n**Analyzing the competition data structure**"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Exploratory Data Analysis\ntry:\n    print('üîß === EXPLORATORY DATA ANALYSIS ===\\n')\n    \n    # Check if train_df and test_df exist\n    if 'train_df' not in globals() or train_df is None:\n        raise ValueError(\"train_df is not loaded. Please check previous cells.\")\n    if 'test_df' not in globals() or test_df is None:\n        raise ValueError(\"test_df is not loaded. Please check previous cells.\")\n    \n    # Basic info\n    print(f\"Train shape: {train_df.shape}\")\n    print(f\"Test shape: {test_df.shape}\\n\")\n    \n    print(\"Train columns:\", list(train_df.columns))\n    print(\"Test columns:\", list(test_df.columns))\n    \n    # Check for missing values\n    print(\"\\nüîç Missing values in train:\")\n    print(train_df.isnull().sum())\n    print(\"\\nüîç Missing values in test:\")\n    print(test_df.isnull().sum())\n    \n    # Sequence length distribution\n    if 'sequence' in train_df.columns:\n        train_df['seq_len'] = train_df['sequence'].str.len()\n        test_df['seq_len'] = test_df['sequence'].str.len()\n        \n        print(\"\\nüìè Sequence length stats (train):\")\n        print(train_df['seq_len'].describe())\n        print(\"\\nüìè Sequence length stats (test):\")\n        print(test_df['seq_len'].describe())\n        \n        import matplotlib.pyplot as plt\n        import seaborn as sns\n        \n        plt.figure(figsize=(12,5))\n        sns.histplot(train_df['seq_len'], bins=50, kde=True, color='blue', label='Train')\n        sns.histplot(test_df['seq_len'], bins=50, kde=True, color='orange', label='Test')\n        plt.title('Protein Sequence Length Distribution')\n        plt.xlabel('Sequence Length')\n        plt.ylabel('Count')\n        plt.legend()\n        plt.show()\n    else:\n        print(\"‚ö†Ô∏è 'sequence' column not found in train/test data.\")\n    \n    # Label analysis (multi-label GO terms)\n    go_cols = [c for c in train_df.columns if c.startswith('GO:')]\n    if go_cols:\n        print(f\"\\nüî¨ Number of GO term columns: {len(go_cols)}\")\n        print(\"Sample GO term columns:\", go_cols[:10])\n        \n        # Count label frequency\n        label_counts = train_df[go_cols].sum().sort_values(ascending=False)\n        print(\"\\nTop 10 most frequent GO terms in train:\")\n        print(label_counts.head(10))\n        \n        plt.figure(figsize=(12,5))\n        sns.barplot(x=label_counts.head(20).index, y=label_counts.head(20).values)\n        plt.xticks(rotation=90)\n        plt.title('Top 20 Most Frequent GO Terms (Train)')\n        plt.xlabel('GO Term')\n        plt.ylabel('Count')\n        plt.show()\n        \n        # Multi-label distribution\n        train_df['num_labels'] = train_df[go_cols].sum(axis=1)\n        print(\"\\nMulti-label distribution (number of GO terms per protein):\")\n        print(train_df['num_labels'].describe())\n        \n        plt.figure(figsize=(10,5))\n        sns.histplot(train_df['num_labels'], bins=30, kde=True)\n        plt.title('Number of GO Terms per Protein (Train)')\n        plt.xlabel('Number of GO Terms')\n        plt.ylabel('Count')\n        plt.show()\n    else:\n        print(\"‚ö†Ô∏è No GO term columns found (columns starting with 'GO:').\")\n    \n    # Amino acid composition analysis\n    if 'sequence' in train_df.columns:\n        from collections import Counter\n        aa_counts = Counter(''.join(train_df['sequence'].dropna().values))\n        aa_df = pd.DataFrame.from_dict(aa_counts, orient='index', columns=['count'])\n        aa_df['freq'] = aa_df['count'] / aa_df['count'].sum()\n        aa_df = aa_df.sort_values('freq', ascending=False)\n        \n        print(\"\\nAmino acid composition (train):\")\n        print(aa_df)\n        \n        plt.figure(figsize=(10,5))\n        sns.barplot(x=aa_df.index, y=aa_df['freq'])\n        plt.title('Amino Acid Frequency (Train)')\n        plt.xlabel('Amino Acid')\n        plt.ylabel('Frequency')\n        plt.show()\n    else:\n        print(\"‚ö†Ô∏è 'sequence' column not found for amino acid composition analysis.\")\n    \n    # Check for class imbalance\n    if go_cols:\n        imbalance = label_counts / len(train_df)\n        print(\"\\nGO term imbalance (fraction of proteins per GO term):\")\n        print(imbalance.head(10))\n        \n        plt.figure(figsize=(12,5))\n        sns.histplot(imbalance, bins=50, kde=True)\n        plt.title('GO Term Frequency Distribution (Train)')\n        plt.xlabel('Fraction of Proteins')\n        plt.ylabel('Number of GO Terms')\n        plt.show()\n    else:\n        print(\"‚ö†Ô∏è Cannot compute class imbalance without GO term columns.\")\n    \n    print('‚úÖ Exploratory Data Analysis complete!')\n    \nexcept Exception as e:\n    print(f'‚úó Error in Exploratory Data Analysis: {e}')\n    import traceback\n    traceback.print_exc()"},{"cell_type":"markdown","metadata":{},"source":"## 4. Data Preprocessing\n\n**Competition:** cafa-6-protein-function-prediction\n\n**Note:** Following research-based implementation strategy"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Data Preprocessing\ntry:\n    print('üîß === DATA PREPROCESSING ===\\n')\n    \n    # 1. Identify GO term columns and sequence column\n    go_cols = [col for col in train_df.columns if col.startswith('GO:')]\n    if not go_cols:\n        raise ValueError(\"No GO term columns found in train_df (columns starting with 'GO:').\")\n    if 'sequence' not in train_df.columns:\n        raise ValueError(\"'sequence' column not found in train_df.\")\n\n    # 2. Remove proteins with missing or invalid sequences\n    before = len(train_df)\n    train_df = train_df.dropna(subset=['sequence'])\n    train_df = train_df[train_df['sequence'].str.fullmatch(r'[ACDEFGHIKLMNPQRSTVWY]+')]\n    after = len(train_df)\n    print(f'Removed {before - after} proteins with missing or invalid sequences (train).')\n    \n    before = len(test_df)\n    test_df = test_df.dropna(subset=['sequence'])\n    test_df = test_df[test_df['sequence'].str.fullmatch(r'[ACDEFGHIKLMNPQRSTVWY]+')]\n    after = len(test_df)\n    print(f'Removed {before - after} proteins with missing or invalid sequences (test).')\n    \n    # 3. Encode amino acid sequences as integer indices (for embedding layers)\n    aa_vocab = {aa: idx+1 for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n    aa_vocab['X'] = 0  # Unknown/ambiguous\n    def seq_to_idx(seq):\n        return [aa_vocab.get(aa, 0) for aa in seq]\n    \n    train_df['seq_idx'] = train_df['sequence'].apply(seq_to_idx)\n    test_df['seq_idx'] = test_df['sequence'].apply(seq_to_idx)\n    print('Encoded amino acid sequences to integer indices.')\n    \n    # 4. Pad sequences to fixed length (e.g., 1024) for batch processing\n    MAX_LEN = 1024\n    def pad_seq(seq, maxlen=MAX_LEN):\n        if len(seq) >= maxlen:\n            return seq[:maxlen]\n        return seq + [0] * (maxlen - len(seq))\n    \n    train_df['seq_idx_pad'] = train_df['seq_idx'].apply(lambda x: pad_seq(x, MAX_LEN))\n    test_df['seq_idx_pad'] = test_df['seq_idx'].apply(lambda x: pad_seq(x, MAX_LEN))\n    print(f'Sequences padded/truncated to length {MAX_LEN}.')\n    \n    # 5. Prepare multi-label targets as numpy arrays\n    train_df['labels'] = train_df[go_cols].values.tolist()\n    print('Multi-label targets prepared (train).')\n    \n    # 6. Analyze and address class imbalance (optional: show label distribution)\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n    \n    label_counts = train_df[go_cols].sum().sort_values(ascending=False)\n    plt.figure(figsize=(12,4))\n    sns.histplot(label_counts, bins=50, kde=True)\n    plt.title('GO Term Label Distribution (Train)')\n    plt.xlabel('Number of Proteins per GO Term')\n    plt.ylabel('Count')\n    plt.show()\n    \n    # 7. Show sequence length distribution\n    train_df['seq_len'] = train_df['sequence'].str.len()\n    plt.figure(figsize=(10,4))\n    sns.histplot(train_df['seq_len'], bins=50, kde=True)\n    plt.title('Protein Sequence Length Distribution (Train)')\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Count')\n    plt.show()\n    \n    # 8. Print summary statistics\n    print(f\"Train proteins: {len(train_df)}, Test proteins: {len(test_df)}\")\n    print(f\"Number of GO terms: {len(go_cols)}\")\n    print(\"Amino acid vocabulary:\", aa_vocab)\n    print(\"Example (first 2) padded sequence indices (train):\")\n    print(train_df['seq_idx_pad'].head(2).tolist())\n    print(\"Example (first 2) multi-labels (train):\")\n    print(train_df['labels'].head(2).tolist())\n    \n    print('‚úÖ Data Preprocessing complete!')\n    \nexcept Exception as e:\n    print(f'‚úó Error in Data Preprocessing: {e}')\n    import traceback\n    traceback.print_exc()"},{"cell_type":"markdown","metadata":{},"source":"## 5. Model Architecture\n\n**Task:** multi-label-classification\n\n**Approach:** Based on research and implementation strategy above"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Model Architecture\ntry:\n    print('üîß === MODEL ARCHITECTURE ===\\n')\n    \n    import torch\n    import torch.nn as nn\n\n    # Use variables from previous cells\n    # aa_vocab, MAX_LEN, go_cols, device are assumed to be defined in prior cells\n    # train_df, test_df, etc. are already loaded\n\n    # Model hyperparameters\n    vocab_size = len(aa_vocab) + 1  # +1 for padding index 0\n    embedding_dim = 128\n    transformer_dim = 128\n    num_heads = 8\n    num_layers = 4\n    num_go_terms = len(go_cols)\n    dropout_rate = 0.2\n    max_len = 1024  # Should match MAX_LEN used in preprocessing\n\n    class ProteinFunctionPredictor(nn.Module):\n        def __init__(self, vocab_size, embedding_dim, transformer_dim, num_heads, num_layers, num_go_terms, max_len, dropout_rate=0.2):\n            super().__init__()\n            self.embedding = nn.Embedding(\n                num_embeddings=vocab_size,\n                embedding_dim=embedding_dim,\n                padding_idx=0\n            )\n            self.position_embedding = nn.Embedding(\n                num_embeddings=max_len,\n                embedding_dim=embedding_dim\n            )\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=transformer_dim,\n                nhead=num_heads,\n                dim_feedforward=transformer_dim*4,\n                dropout=dropout_rate,\n                batch_first=True,\n                activation='gelu'\n            )\n            self.transformer_encoder = nn.TransformerEncoder(\n                encoder_layer,\n                num_layers=num_layers\n            )\n            self.dropout = nn.Dropout(dropout_rate)\n            self.classifier = nn.Linear(transformer_dim, num_go_terms)\n        \n        def forward(self, input_ids, attention_mask=None):\n            # input_ids: (batch, seq_len)\n            positions = torch.arange(0, input_ids.size(1), device=input_ids.device).unsqueeze(0)\n            x = self.embedding(input_ids) + self.position_embedding(positions)\n            if attention_mask is not None:\n                # Transformer expects mask: (batch, seq_len)\n                # Masked positions are True, so invert mask (0=keep, 1=mask)\n                src_key_padding_mask = ~attention_mask.bool()\n            else:\n                src_key_padding_mask = None\n            x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n            # Pooling: mean over non-padding tokens\n            if attention_mask is not None:\n                mask = attention_mask.unsqueeze(-1)\n                x = (x * mask).sum(1) / mask.sum(1).clamp(min=1)\n            else:\n                x = x.mean(1)\n            x = self.dropout(x)\n            logits = self.classifier(x)\n            return logits\n\n    # Instantiate model\n    model = ProteinFunctionPredictor(\n        vocab_size=vocab_size,\n        embedding_dim=embedding_dim,\n        transformer_dim=transformer_dim,\n        num_heads=num_heads,\n        num_layers=num_layers,\n        num_go_terms=num_go_terms,\n        max_len=max_len,\n        dropout_rate=dropout_rate\n    ).to(device)\n\n    # Print model summary\n    print(model)\n    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    # Test a forward pass with dummy data\n    batch_size = 2\n    dummy_input = torch.randint(1, vocab_size, (batch_size, max_len), device=device)\n    dummy_mask = (dummy_input != 0).long()\n    with torch.no_grad():\n        dummy_logits = model(dummy_input, dummy_mask)\n    print(f\"\\nDummy output shape (batch_size={batch_size}, num_go_terms={num_go_terms}): {dummy_logits.shape}\")\n\n    # Visualize model architecture (textual)\n    print('\\nModel architecture summary:')\n    print(f\"- Embedding: vocab_size={vocab_size}, embedding_dim={embedding_dim}\")\n    print(f\"- Positional Embedding: max_len={max_len}, embedding_dim={embedding_dim}\")\n    print(f\"- Transformer: layers={num_layers}, heads={num_heads}, d_model={transformer_dim}\")\n    print(f\"- Classifier: output_dim={num_go_terms} (multi-label)\")\n    print(f\"- Dropout: {dropout_rate}\")\n\n    print('‚úÖ Model Architecture complete!')\n\nexcept Exception as e:\n    print(f'‚úó Error in Model Architecture: {e}')\n    import traceback\n    traceback.print_exc()"},{"cell_type":"markdown","metadata":{},"source":"## 6. Implementation & Next Steps\n\n**Note:** This section provides guidance, not complete code. Actual implementation depends on competition task."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"print('üìã === IMPLEMENTATION GUIDE ===\\n')\n\nprint('Competition Type: biology - multi-label-classification\\n')\nprint('Task: Protein sequences ‚Üí Gene Ontology (GO) term predictions\\n')\nprint('üí° Implementation Process:')\nprint('1. Load and explore the competition data')\nprint('2. Preprocess according to data type')\nprint('3. Build baseline model')\nprint('4. Train and validate')\nprint('5. Generate predictions')\nprint('6. Format submission file')\n\nprint('\\n‚ö†Ô∏è TODO:')\nprint('  [ ] Implement data preprocessing')\nprint('  [ ] Build and train model')\nprint('  [ ] Generate test predictions')\nprint('  [ ] Format submission')\n\nprint('\\nüí° TIP: Check research gaps and implementation strategy above!')\n"},{"cell_type":"markdown","metadata":{},"source":"## 7. Submission\n\n**Generate submission file in competition format**"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"print('üì§ === SUBMISSION GENERATION ===\\n')\n\nprint('CAFA 6 Protein Function Prediction Submission Format:')\nprint('  Metric: Fmax')\nprint('  Format: Check sample_submission file for exact format')\n\nprint('\\n‚ö†Ô∏è TODO:')\nprint('  1. Generate predictions on test set')\nprint('  2. Format according to sample_submission')\nprint('  3. Validate submission format')\nprint('  4. Save submission file')\n\n# Load sample submission to see format\n# sample_sub = pd.read_csv(DATA_PATH / 'sample_submission.csv')  # or .parquet\n# print(sample_sub.head())\n#\n# Create your submission matching the format:\n# submission = sample_sub.copy()\n# submission['target'] = your_predictions  # Replace 'target' with actual column name\n# submission.to_csv('submission.csv', index=False)\n# print('‚úÖ Submission created!')\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}