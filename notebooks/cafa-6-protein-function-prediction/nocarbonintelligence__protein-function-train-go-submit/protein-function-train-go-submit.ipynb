{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13468971,"sourceType":"datasetVersion","datasetId":8550106}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install obonet pandas numpy tensorflow scikit-learn\n!pip install obonet pandas numpy tensorflow scikit-learn\nimport pandas as pd\nimport numpy as np\nimport obonet\nimport networkx as nx\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.backend import binary_crossentropy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T18:31:39.814705Z","iopub.execute_input":"2025-10-22T18:31:39.814984Z","iopub.status.idle":"2025-10-22T18:32:08.217685Z","shell.execute_reply.started":"2025-10-22T18:31:39.814957Z","shell.execute_reply":"2025-10-22T18:32:08.216257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chemins d'acc√®s aux fichiers (bas√©s sur votre input)\nEMBEDDINGS_PATH = '/kaggle/input/test-protein-embedding-complet/test_protein_embeddings_complet.npy'\nIDS_PATH = '/kaggle/input/test-protein-embedding-complet/test_protein_ids_complet.npy'\nTERMS_PATH = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv'\nOBO_PATH = '/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo'\n\n# --- Chargement des embeddings (X) et des IDs ---\nX = np.load(EMBEDDINGS_PATH)\ntrain_ids = np.load(IDS_PATH)\nEMBEDDING_DIM = X.shape[1]\nprint(f\"Embeddings charg√©s. Dimension X: {X.shape}\")\n\n# --- Chargement des Labels (Y) ---\ntrain_terms = pd.read_csv(TERMS_PATH, sep='\\t')\nprint(f\"Termes charg√©s. Nombre de lignes: {len(train_terms)}\")\n\ntrain_terms = pd.read_csv(TERMS_PATH, sep='\\t')\nprint(\"Noms exacts des colonnes dans train_terms.tsv:\")\nprint(train_terms.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T22:25:36.155509Z","iopub.execute_input":"2025-10-21T22:25:36.155841Z","iopub.status.idle":"2025-10-21T22:25:36.808825Z","shell.execute_reply.started":"2025-10-21T22:25:36.155821Z","shell.execute_reply":"2025-10-21T22:25:36.807482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. S√©lection des termes GO (les labels)\n# Nous utilisons seulement les termes GO pr√©sents dans le jeu d'entra√Ænement.\n# Nous allons filtrer les termes trop rares pour l'entra√Ænement (par exemple, si moins de 50 prot√©ines ont ce terme).\nTERM_COUNT_THRESHOLD = 50\nterm_counts = train_terms['term'].value_counts()\nselected_terms = term_counts[term_counts >= TERM_COUNT_THRESHOLD].index.tolist()\n\n# 2. Cr√©ation du mappage ID -> Index\nprotein_to_index = {pid: i for i, pid in enumerate(train_ids)}\nterm_to_index = {term: i for i, term in enumerate(selected_terms)}\nnum_classes = len(selected_terms)\n\n# 3. Construction de la matrice Y binaire\nY_matrix = np.zeros((len(train_ids), num_classes), dtype=np.int8)\n\n# Remplissage de la matrice Y\nfor _, row in train_terms.iterrows():\n    protein_id = row['EntryID']\n    term = row['term']\n    \n    if protein_id in protein_to_index and term in term_to_index:\n        p_idx = protein_to_index[protein_id]\n        t_idx = term_to_index[term]\n        Y_matrix[p_idx, t_idx] = 1\n\nprint(f\"Matrice de labels Y cr√©√©e. Dimension Y: {Y_matrix.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T22:27:23.450198Z","iopub.execute_input":"2025-10-21T22:27:23.450743Z","iopub.status.idle":"2025-10-21T22:27:47.540891Z","shell.execute_reply.started":"2025-10-21T22:27:23.450709Z","shell.execute_reply":"2025-10-21T22:27:47.540048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Chargement du graphe GO et calcul des Poids d'IA ---\nprint(\"\\n--- 3. Calcul de l'Information Content (IA) ---\")\n\n# Chargement du graphe (n√©cessite obonet)\ntry:\n    go_graph = obonet.read_obo(OBO_PATH)\nexcept Exception as e:\n    print(f\"Erreur lors du chargement d'obonet. Assurez-vous que 'obonet' est install√©. Erreur: {e}\")\n    # Si le chargement √©choue, on continue avec un IC bas√© uniquement sur la fr√©quence\n    # sans utiliser la hi√©rarchie GO pour le moment (moins pr√©cis mais fonctionnel)\n    pass\n\n\n# Calcul des fr√©quences d'apparition (pour l'Information Content - IC)\n# On compte combien de prot√©ines ont chaque terme dans le jeu Y_matrix\nterm_frequencies = Y_matrix.sum(axis=0)\n\n# Calcul de l'Information Content (IC)\n# IC = -log2(P(t)) o√π P(t) est la probabilit√© d'un terme t\ntotal_proteins = len(train_ids)\nterm_probabilities = term_frequencies / total_proteins\n# Le terme le plus fr√©quent (P proche de 1) aura un IC proche de 0\n# Le terme le plus rare (P proche de 0) aura un IC tr√®s √©lev√©\n# On ajoute une petite valeur (epsilon) pour √©viter log(0)\nepsilon = 1e-10\nIC_values = -np.log2(term_probabilities + epsilon)\n\n# Normalisation pour la fonction de perte\n# Plus l'IC est √©lev√©, plus le poids sera grand.\n# On normalise pour que le poids maximum soit 1.0 (ou proche).\nclass_weights = IC_values / IC_values.max() \nclass_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\n\nprint(f\"Poids d'Information Content (IA) calcul√©s. Nombre de classes: {len(class_weights)}. Max weight: {class_weights.max():.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T22:29:53.768333Z","iopub.execute_input":"2025-10-21T22:29:53.768735Z","iopub.status.idle":"2025-10-21T22:30:01.259119Z","shell.execute_reply.started":"2025-10-21T22:29:53.76871Z","shell.execute_reply":"2025-10-21T22:30:01.258122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspecter le poids\nprint(\"Valeurs des poids (IA) :\")\nprint(class_weights_tensor)\nprint(f\"Nombre de poids nuls : {tf.reduce_sum(tf.cast(class_weights_tensor == 0, tf.int32))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T22:34:19.364292Z","iopub.execute_input":"2025-10-21T22:34:19.365359Z","iopub.status.idle":"2025-10-21T22:34:19.377802Z","shell.execute_reply.started":"2025-10-21T22:34:19.365322Z","shell.execute_reply":"2025-10-21T22:34:19.376761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- √âtape A : Inspection des IDs ---\n# R√©cup√©rer les 5 premiers IDs de la source label (train_terms.tsv)\n# Utilisez le nom de colonne 'EntryID' original, puis le nettoyage.\nids_terms_original = train_terms['EntryID'].head().tolist()\nids_terms_nettoyes = train_terms['EntryID'].str.split('.').str[0].head().tolist()\n\n# R√©cup√©rer les 5 premiers IDs de la source embeddings (train_ids.npy)\nids_embeddings = train_ids[:5].tolist()\n\n\nprint(\"IDs labels (EntryID original) :\", ids_terms_original)\nprint(\"IDs labels (Apr√®s .split('.')) :\", ids_terms_nettoyes)\nprint(\"IDs embeddings (train_ids.npy) :\", ids_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T22:41:44.958332Z","iopub.execute_input":"2025-10-21T22:41:44.958722Z","iopub.status.idle":"2025-10-21T22:41:45.824008Z","shell.execute_reply.started":"2025-10-21T22:41:44.958699Z","shell.execute_reply":"2025-10-21T22:41:45.823074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- √âtape de CORRECTION D√âFINITIVE (R√©-ex√©cutez tout le bloc de nettoyage) ---\n\n# 1. Nettoyage des IDs dans les labels (train_terms.tsv)\n# Bien que les exemples soient d√©j√† propres, on garde le split pour la s√©curit√©\ntrain_terms['EntryID_CLEAN'] = train_terms['EntryID'].str.split('.').str[0]\n\n# 2. Nettoyage CRITIQUE des IDs d'embeddings (train_ids.npy)\n# On extrait l'ID Uniprot brut (le segment entre les barres verticales '|')\ndef extract_uniprot_id(full_id):\n    \"\"\"Extrait l'ID Uniprot brut de la cha√Æne FASTA compl√®te.\"\"\"\n    try:\n        # La 2e position (index 1) apr√®s le split par '|' est l'ID Uniprot\n        return full_id.split('|')[1]\n    except IndexError:\n        # Retourne l'ID original si le format n'est pas trouv√© (s√©curit√©)\n        return full_id\n\n# Appliquer la fonction pour cr√©er les IDs propres des embeddings\ntrain_ids_cleaned = np.array([extract_uniprot_id(str(i)) for i in train_ids]) \n\n# 3. Recr√©ation du Dictionnaire de Mapping (avec les IDs nettoy√©s)\n# Les cl√©s seront maintenant les IDs bruts (ex: 'A0A0C5B5G6')\nprotein_to_index = {pid: i for i, pid in enumerate(train_ids_cleaned)}\n\n# V√©rification pour s'assurer qu'au moins quelques IDs correspondent (optionnel mais utile)\nprint(f\"\\nExemples d'IDs nettoy√©s des embeddings : {train_ids_cleaned[:5].tolist()}\")\nprint(f\"L'ID 'Q5W0B1' est-il dans le dictionnaire ? {'Q5W0B1' in protein_to_index}\")\n\n\n# --- 4. Remplissage de la matrice Y (CODE FINAL) ---\n# Re-initialiser la matrice Y \nY_matrix = np.zeros((len(train_ids_cleaned), num_classes), dtype=np.int8)\n\nfilled_count = 0\nfor _, row in train_terms.iterrows():\n    # Utiliser l'ID nettoy√© du terme\n    protein_id_clean = str(row['EntryID_CLEAN'])\n    term = row['term']\n    \n    # La condition devrait maintenant √™tre VRAIE si l'ID existe dans le jeu d'embeddings\n    if protein_id_clean in protein_to_index and term in term_to_index:\n        p_idx = protein_to_index[protein_id_clean]\n        t_idx = term_to_index[term]\n        Y_matrix[p_idx, t_idx] = 1\n        filled_count += 1\n\n# --- V√âRIFICATION FINALE ---\nprint(f\"\\nNombre total de labels positifs ('1') remplis : {Y_matrix.sum()}\")\nprint(f\"Nombre total de lignes trait√©es : {filled_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T22:42:30.125575Z","iopub.execute_input":"2025-10-21T22:42:30.125927Z","iopub.status.idle":"2025-10-21T22:42:56.376552Z","shell.execute_reply.started":"2025-10-21T22:42:30.125903Z","shell.execute_reply":"2025-10-21T22:42:56.375492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# --- D√âFINITION DE L'ARCHITECTURE MLP ) ---\n# Ceci cr√©e la variable 'model'\nmodel = Sequential([\n    # Couche d'entr√©e : la dimension est la taille des embeddings (320)\n    Dense(1024, activation='relu', input_shape=(320,)), \n    Dropout(0.2),\n    # Couche cach√©e interm√©diaire (peut varier)\n    Dense(512, activation='relu'),\n    Dropout(0.2),\n    # Couche de sortie : 1585 neurones (le nombre de termes GO) avec activation sigmo√Øde pour la multi-classification\n    Dense(1585, activation='sigmoid') \n])\n\n# V√©rifiez l'architecture\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- D√©finition de la Perte, de l'Architecture et Entra√Ænement ---\n\n# 1. D√©finir la fonction de perte pond√©r√©e (si les poids IA sont d√©j√† calcul√©s et vari√©s)\n# Nous avions une perte de validation tr√®s faible pr√©c√©demment. Nous allons\n# nous assurer que le mod√®le apprend quelque chose en utilisant la perte standard.\n\n# Si vous voulez avancer sans risque de bug de la perte IA:\n# loss_function = 'binary_crossentropy' \n\n# Si vous voulez essayer √† nouveau la perte IA corrig√©e (en esp√©rant que le bug √©tait li√© √† Y nul):\n# loss_function = weighted_binary_crossentropy \n\n\n# --- S√©paration des donn√©es pour l'entra√Ænement et la validation ---\nX_train, X_val, Y_train, Y_val = train_test_split(\n    X, \n    Y_matrix, \n    test_size=0.1, \n    random_state=42 \n)\n\n# --- Compilation et Entra√Ænement (avec binary_crossentropy pour la s√©curit√©) ---\n# Si le GPU est d√©tect√©, l'entra√Ænement sera rapide.\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy', # UTILISATION DE LA PERTE STANDARD POUR V√âRIFICATION\n    metrics=[tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n)\n\nprint(\"\\nD√©but de l'entra√Ænement du mod√®le MLP...\")\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=5, \n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train, Y_train,\n    epochs=50, \n    batch_size=64,\n    validation_data=(X_val, Y_val),\n    callbacks=[early_stopping] \n)\n\nprint(\"\\nEntra√Ænement termin√©.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T22:44:20.684951Z","iopub.execute_input":"2025-10-21T22:44:20.685887Z","iopub.status.idle":"2025-10-21T22:55:20.755107Z","shell.execute_reply.started":"2025-10-21T22:44:20.68585Z","shell.execute_reply":"2025-10-21T22:55:20.753989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# second entra√Ænement\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nimport numpy as np # Assurez-vous d'avoir import√© numpy si class_weights_tensor vient d'un numpy array\n\n# --- Fonction de perte pond√©r√©e par l'IA (CORRIG√âE) ---\ndef weighted_binary_crossentropy_FINAL(y_true, y_pred):\n    \"\"\"\n    Fonction de perte pond√©r√©e par l'Information Content (IA).\n    Utilise le backend de Keras pour g√©rer correctement les formes.\n    \"\"\"\n    # 1. Calcul de la BCE pour chaque √©l√©ment (sans r√©duction)\n    # y_true et y_pred sont de forme (batch_size, 1585)\n    bce = K.binary_crossentropy(y_true, y_pred) \n    \n    # 2. Ajout de dimension (1585,) -> (1, 1585) pour le broadcasting s√©curis√©\n    # Ceci assure que chaque ligne du batch est multipli√©e par le vecteur de poids.\n    weights = class_weights_tensor[None, :] \n    \n    # 3. Multiplication √©l√©ment par √©l√©ment : (64, 1585) * (1, 1585)\n    weighted_bce = bce * weights\n    \n    # 4. Retourne la moyenne de la perte sur toutes les classes et le batch\n    return K.mean(weighted_bce)\n\n# --- Recr√©er le Tenseur de Poids si n√©cessaire ---\n# Si votre class_weights_tensor a √©t√© perdu ou red√©fini, assurez-vous qu'il soit bien un tenseur TF\n# class_weights_tensor = tf.constant(votre_array_de_poids_IA_de_numpy, dtype=tf.float32)\n\nprint(\"Fonction de perte pond√©r√©e par l'IA mise √† jour et charg√©e.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T23:04:35.178327Z","iopub.execute_input":"2025-10-21T23:04:35.179313Z","iopub.status.idle":"2025-10-21T23:04:35.186232Z","shell.execute_reply.started":"2025-10-21T23:04:35.179276Z","shell.execute_reply":"2025-10-21T23:04:35.185232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Compilation du Mod√®le avec la Perte IA ---\n\nprint(\"\\n--- Relance de la compilation avec la Perte IA ---\")\n\n# Recompilez le mod√®le en utilisant la nouvelle fonction de perte\nmodel.compile(\n    optimizer='adam',\n    # UTILISATION DE LA PERTE POND√âR√âE PAR L'IA CORRIG√âE\n    loss=weighted_binary_crossentropy_FINAL, \n    metrics=[tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n)\n\n# --- Entra√Ænement ---\n\nprint(\"\\nD√©but du second entra√Ænement (Optimisation Fmax)...\")\n\n# Lancer le nouvel entra√Ænement\n# L'EarlyStopping utilisera la 'val_loss' pond√©r√©e pour d√©terminer le meilleur point d'arr√™t\nhistory_ia = model.fit(\n    X_train, Y_train,\n    epochs=50, # Vous pouvez augmenter les √©poques si le premier entrainement a utilis√© moins de 20\n    batch_size=64,\n    validation_data=(X_val, Y_val),\n    callbacks=[early_stopping] \n)\n\nprint(\"\\nEntra√Ænement avec la Perte IA termin√©.\")\nNEW_WEIGHTS_FILE = 'best_cafa6_mlp_ic_weighted.weights.h5' \nmodel.save_weights(NEW_WEIGHTS_FILE)\n\nprint(f\"Sauvegarde des poids termin√©e : {NEW_WEIGHTS_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T23:04:52.432307Z","iopub.execute_input":"2025-10-21T23:04:52.432689Z","iopub.status.idle":"2025-10-21T23:07:26.851447Z","shell.execute_reply.started":"2025-10-21T23:04:52.432663Z","shell.execute_reply":"2025-10-21T23:07:26.850337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\n# --- 0. D√©finition des chemins des sources de la comp√©tition ---\n# Ces fichiers sont n√©cessaires pour la recr√©ation\nTRAIN_TERMS_FILE = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv'\n\n# --- CORRECTION CLEF : RECR√âATION DU MAPPING GO √Ä PARTIR DE LA SOURCE ---\nprint(\"Tentative de recr√©ation du mapping GO...\")\ntry:\n    # 1. Charger le fichier de termes source\n    df_terms = pd.read_csv(TRAIN_TERMS_FILE, sep='\\t')\n    \n    # 2. Identifier les 1585 termes que vous avez utilis√©s (les plus fr√©quents)\n    # On suppose que vous avez filtr√© sur les termes les plus fr√©quents (> 220 occurrences, le seuil standard)\n    term_counts = df_terms['term'].value_counts()\n    \n    # D√©finir le seuil utilis√© pour la comp√©tition (typiquement 220 pour avoir ~1585 termes)\n    # Ceci doit correspondre au seuil que vous avez utilis√© lors de la cr√©ation de Y !\n    # Si le nombre de termes est 1585, le seuil √©tait probablement autour de 220.\n    TARGET_TERM_COUNT = 1585 \n    \n    # Ajustement heuristique pour obtenir le bon nombre de termes (cela suppose un ordre pr√©cis)\n    # Pour l'inf√©rence, nous allons simplement prendre les 1585 termes les plus fr√©quents.\n    # C'est la m√©thode la plus probable que vous ayez utilis√©e.\n    \n    # Prendre les termes les plus fr√©quents dans l'ordre de leur fr√©quence (hypoth√®se la plus forte)\n    go_terms_list_recreated = term_counts.head(TARGET_TERM_COUNT).index.tolist()\n    \n    # Cr√©er le dictionnaire de conversion (index -> ID GO)\n    index_to_term = {i: term for i, term in enumerate(go_terms_list_recreated)}\n    \n    print(f\"‚úÖ Mapping index -> GO ID recr√©√© avec succ√®s. Total termes : {len(index_to_term)}\")\n    \n    # --- Continuer le chargement des autres fichiers pour l'inf√©rence ---\n    \n    # ... (code de chargement X_test et test_ids_clean) ...\n    \n    # Ensuite, le code de pr√©diction et de soumission...\n    \nexcept Exception as e:\n    print(f\"‚ùå √âCHEC : Impossible de recr√©er le mapping GO √† partir de {TRAIN_TERMS_FILE}. D√©tails : {e}\")\n    index_to_term = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T23:27:58.5302Z","iopub.execute_input":"2025-10-21T23:27:58.531275Z","iopub.status.idle":"2025-10-21T23:27:58.985378Z","shell.execute_reply.started":"2025-10-21T23:27:58.531234Z","shell.execute_reply":"2025-10-21T23:27:58.984459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport os \nimport pandas as pd\nfrom tensorflow.keras import backend as K\n\n# --- 0. Initialisation et D√©finitions ---\n\n# Chemins d'acc√®s confirm√©s par l'utilisateur (utilis√©s pour l'inf√©rence)\nTEST_IDS_FILE = '/kaggle/input/test-protein-embedding-complet/test_protein_ids_complet.npy'\nTEST_EMBEDDINGS_FILE = '/kaggle/input/test-protein-embedding-complet/test_protein_embeddings_complet.npy' \nTRAIN_TERMS_FILE = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv'\n\n# Assurez-vous que 'model' (avec les poids entra√Æn√©s) et 'class_weights_tensor' sont d√©finis dans votre notebook !\n\ndef weighted_binary_crossentropy_FINAL(y_true, y_pred):\n    # La fonction de perte doit √™tre d√©finie si le mod√®le est recompil√©\n    bce = K.binary_crossentropy(y_true, y_pred) \n    weights = class_weights_tensor[None, :] \n    weighted_bce = bce * weights\n    return K.mean(weighted_bce)\n\ntry:\n    # Re-compiler le mod√®le pour s'assurer que les m√©triques sont disponibles (les poids sont restaur√©s par EarlyStopping)\n    model.compile(\n        optimizer='adam',\n        loss=weighted_binary_crossentropy_FINAL, \n        metrics=[tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n    )\nexcept NameError:\n    print(\"‚ùå ATTENTION : La variable 'model' ou 'class_weights_tensor' n'est pas d√©finie. Veuillez recharger le mod√®le.\")\n\n# --- 1. CHARGEMENT DES DONN√âES DE TEST ET RECR√âATION DU MAPPING GO ---\nprint(\"\\n--- 1. Chargement et Recr√©ation du Mapping ---\")\n\n# 1. Charger les IDs de test\ntry:\n    test_ids = np.load(TEST_IDS_FILE) \n    test_ids_clean = np.array([i.split('|')[1] if '|' in str(i) else str(i) for i in test_ids])\n    print(f\"‚úÖ IDs charg√©s. Nombre de prot√©ines : {len(test_ids_clean)}\")\nexcept Exception as e:\n    print(f\"‚ùå Erreur de chargement IDs : {e}\")\n    test_ids_clean = np.array([]) \n\n# 2. Charger les embeddings de test (X_test)\ntry:\n    X_test = np.load(TEST_EMBEDDINGS_FILE) \n    print(f\"‚úÖ Embeddings charg√©s. Forme : {X_test.shape}\")\nexcept Exception as e:\n    print(f\"‚ùå Erreur de chargement Embeddings : {e}\")\n    X_test = np.array([])\n\n# 3. RECR√âATION du mapping des termes GO (Solution de d√©blocage)\ntry:\n    df_terms = pd.read_csv(TRAIN_TERMS_FILE, sep='\\t')\n    term_counts = df_terms['term'].value_counts()\n    # On suppose que 1585 termes correspondent aux 1585 plus fr√©quents\n    go_terms_list_recreated = term_counts.head(1585).index.tolist()\n    index_to_term = {i: term for i, term in enumerate(go_terms_list_recreated)}\n    print(f\"‚úÖ Mapping index -> GO ID recr√©√©. Total termes : {len(index_to_term)}\")\nexcept Exception as e:\n    print(f\"‚ùå √âCHEC FATAL : Impossible de recr√©er le mapping GO. {e}\")\n    index_to_term = {} \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T23:29:01.909347Z","iopub.execute_input":"2025-10-21T23:29:01.909739Z","iopub.status.idle":"2025-10-21T23:29:22.968134Z","shell.execute_reply.started":"2025-10-21T23:29:01.909713Z","shell.execute_reply":"2025-10-21T23:29:22.967094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  INF√âRENCE (CRITIQUE) ---\n\nprint(\"D√©but de l'inf√©rence sur le jeu de test complet...\")\n\n# ‚ö†Ô∏è X_test doit contenir les embeddings complets de test (forme: 224309, 320)\n# La variable Y_pred_proba stockera les probabilit√©s de pr√©diction brutes du MLP\nY_pred_proba = model.predict(X_test) \n\nprint(f\"‚úÖ Inf√©rence termin√©e. Dimensions des pr√©dictions brutes : {Y_pred_proba.shape}\")\nprint(f\"Le mod√®le a pr√©dit {Y_pred_proba.shape[0]} prot√©ines sur {Y_pred_proba.shape[1]} termes GO.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Propagation GO\n!pip install obonet networkx\n\nimport networkx as nx\nimport numpy as np\n\nimport obonet\n# Remplacez 'chemin/vers/go-basic.obo' par le chemin r√©el dans votre Kaggle Input\nOBO_FILE_PATH = '/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo' \ngo_graph = obonet.read_obo(OBO_FILE_PATH)\nprint(f\"Graphe GO charg√© avec {go_graph.number_of_nodes()} n≈ìuds.\")\n\n\n# --- 1. S√âCURIT√â ET PR√âPARATION ---\n\n# Cr√©e une copie des pr√©dictions brutes pour les modifier (tr√®s important !)\nY_pred_proba_propagated = np.copy(Y_pred_proba)\n\n# R√©cup√®re l'ensemble des termes GO pr√©dits par le mod√®le\ngo_terms_in_model = set(index_to_term.values())\n\n# --- 2. FONCTION DE PROPAGATION HIERARCHIQUE ---\n\n# Nous allons it√©rer sur les n≈ìuds du graphe GO par ordre topologique inverse\n# (des feuilles vers les racines) pour assurer que les parents sont mis √† jour apr√®s les enfants.\n\n# On utilise les termes qui sont √† la fois dans notre mod√®le et dans le graphe GO\nnodes_to_propagate = [n for n in nx.topological_sort(go_graph) if n in go_terms_in_model]\n# Inverser l'ordre : du bas (feuilles) vers le haut (racines)\nnodes_to_propagate.reverse()\n\nprint(f\"\\nD√©but de la propagation pour {len(nodes_to_propagate)} termes GO...\")\n\n# --- 3. APPLICATION DE LA PROPAGATION ---\n\nfor term_id_child in nodes_to_propagate:\n    try:\n        # R√©cup√®re l'index de ce terme dans la matrice de pr√©diction\n        idx_child = term_to_index[term_id_child]\n\n        # Trouvez tous les parents directs (relations 'is a' ou 'part of')\n        # On utilise go_graph.predecessors(node) pour trouver les parents directs\n        # dans le graphe OBO o√π les ar√™tes pointent d'enfant √† parent.\n        for term_id_parent in go_graph.predecessors(term_id_child):\n            \n            # Assurez-vous que le parent est un terme que notre mod√®le pr√©dit (un des 1585)\n            if term_id_parent in term_to_index:\n                idx_parent = term_to_index[term_id_parent]\n                \n                # R√®gle de propagation: Score(Parent) = max(Score(Parent), Score(Enfant))\n                # np.maximum effectue cette op√©ration ligne par ligne (pour toutes les prot√©ines √† la fois)\n                \n                # Mise √† jour des scores du parent pour toutes les prot√©ines\n                Y_pred_proba_propagated[:, idx_parent] = np.maximum(\n                    Y_pred_proba_propagated[:, idx_parent], \n                    Y_pred_proba_propagated[:, idx_child]\n                )\n\n    except KeyError:\n        # Ignore les termes qui sont dans le graphe GO mais que nous n'avons pas s√©lectionn√©s (pas l'un des 1585)\n        continue \n\nprint(\"‚úÖ Propagation GO termin√©e.\")\nprint(f\"Les pr√©dictions finales sont stock√©es dans Y_pred_proba_propagated (Forme: {Y_pred_proba_propagated.shape}).\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom operator import itemgetter\n\n# --- 3. FORMATAGE ET √âCRITURE DU FICHIER DE SOUMISSION CONFORME CAFA ---\nprint(\"\\n--- 3. Formatage et √âcriture du Fichier de Soumission CAFA FINAL ---\")\n\n# ‚ö†Ô∏è L'ENTR√âE EST MAINTENANT LE RESULTAT DE LA PROPAGATION GO\nY_FINAL_SCORES = Y_pred_proba_propagated\n\n# üö® Rappel de la contrainte (score minimum pour ce type de comp√©tition)\n# Le score DOIT √™tre dans l'intervalle (0, 1.000]. Votre seuil est 0.01.\n# Le minimum requis est de 0.5 (selon vos informations m√©moris√©es).\n# Cependant, pour le formatage CAFA, on garde souvent un seuil bas (0.01) \n# pour inclure tous les termes avant la limite de 1500.\n\nif len(Y_FINAL_SCORES) > 0 and len(index_to_term) == 1585:\n    \n    # Le score DOIT √™tre dans l'intervalle (0, 1.000].\n    SEUIL_SOUMISSION = 0.01 # Gardons ce seuil pour la capture initiale\n    # La limite est de 1500 termes MAX par prot√©ine.\n    MAX_TERMS_PER_PROTEIN = 1500\n    \n    submission_lines = []\n\n    for i in range(len(test_ids_clean)):\n        protein_id = test_ids_clean[i]\n        # Utilisez les scores finaux propag√©s\n        probabilities = Y_FINAL_SCORES[i]\n        \n        predictions = []\n        \n        # 1. Identifier les pr√©dictions au-dessus du seuil (doit √™tre > 0.0)\n        indices_predits = np.where(probabilities > SEUIL_SOUMISSION)[0]\n        \n        # Cr√©er des paires (terme, probabilit√©) pour le tri\n        for idx in indices_predits:\n            term_id = index_to_term.get(idx)\n            prob = probabilities[idx]\n            \n            if term_id and term_id != 'GO:UNKNOWN':\n                predictions.append((term_id, prob))\n\n        # 2. Trier par probabilit√© (du plus haut au plus bas)\n        predictions.sort(key=itemgetter(1), reverse=True)\n        \n        # 3. Appliquer la limite de 1500 termes\n        final_predictions = predictions[:MAX_TERMS_PER_PROTEIN]\n        \n        # 4. Formatage des lignes\n        for term_id, prob in final_predictions:\n            # Score doit √™tre dans (0, 1.000] avec max 3 chiffres significatifs.\n            formatted_prob = f\"{prob:.3f}\"\n            \n            # Format: ID_prot [TAB] GO_ID [TAB] Score (une association par ligne)\n            submission_lines.append(f\"{protein_id}\\t{term_id}\\t{formatted_prob}\")\n\n    # --- √âCRIRE LE FICHIER (SANS HEADER) ---\n    \n    # üí• CORRECTION CRITIQUE : Le nom du fichier DOIT √™tre 'submission.tsv' pour Kaggle.\n    SUBMISSION_FILE_NAME = 'submission.tsv'\n\n    with open(os.path.join('/kaggle/working', SUBMISSION_FILE_NAME), 'w') as f:\n        # CRITIQUE : √âcriture des lignes SANS HEADER\n        f.write('\\n'.join(submission_lines))\n\n    print(f\"\\n‚úÖ Fichier de soumission cr√©√© : {SUBMISSION_FILE_NAME}. Nombre total de pr√©dictions : {len(submission_lines)}\")\n    print(\"Le fichier est maintenant nomm√© correctement pour la soumission √† Kaggle.\")\nelse:\n    print(\"\\n‚ùå √âCHEC FINAL : V√©rifiez le chargement de Y_FINAL_SCORES (Y_pred_proba_propagated) ou la taille de index_to_term.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}