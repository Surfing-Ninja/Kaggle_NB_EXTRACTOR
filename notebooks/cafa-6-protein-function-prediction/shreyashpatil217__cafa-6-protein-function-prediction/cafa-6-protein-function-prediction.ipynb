{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß¨ Protein Function Prediction using Machine Learning\n\n## üìã Project Overview\n\nThis project demonstrates how to build a **machine learning solution for protein function prediction** - a key problem in bioinformatics. The project uses publicly available techniques and datasets to predict biological functions of proteins based on their amino acid sequences.\n\n**Project Type:** Educational Machine Learning Project  \n**Domain:** Bioinformatics & Computational Biology  \n**Problem Type:** Multi-label Classification  \n**Approach:** Ensemble Learning Methods\n\n---\n\n## üéØ Project Goal\n\n### Objective\nBuild an intelligent system that can predict Gene Ontology (GO) function annotations for protein sequences using machine learning, demonstrating:\n- Advanced feature engineering techniques\n- Ensemble learning strategies\n- Multi-label classification handling\n- Model validation and optimization\n\n### Why This Problem Matters\nProtein function prediction is essential for:\n- üß™ Drug discovery and development\n- üî¨ Understanding biological processes\n- üß¨ Genomic research and annotation\n- üíä Disease mechanism research\n- üåç Biotechnology applications\n\n---\n\n## üèóÔ∏è Project Architecture\n\n### Solution Approach\n\n```\nPROTEIN SEQUENCES\n    ‚Üì\n[FEATURE EXTRACTION] ‚Üí Convert sequences to numerical features\n    ‚îú‚îÄ‚îÄ Amino acid composition\n    ‚îú‚îÄ‚îÄ Physical properties (charge, hydrophobicity)\n    ‚îú‚îÄ‚îÄ Sequence patterns and motifs\n    ‚îî‚îÄ‚îÄ Structural indicators\n    ‚Üì\n[DATA PREPARATION] ‚Üí Format for machine learning\n    ‚îú‚îÄ‚îÄ Handle missing values\n    ‚îú‚îÄ‚îÄ Normalize features\n    ‚îú‚îÄ‚îÄ Create train-validation splits\n    ‚îî‚îÄ‚îÄ Encode multi-label targets\n    ‚Üì\n[ENSEMBLE MODELING] ‚Üí Train multiple models\n    ‚îú‚îÄ‚îÄ Random Forest Classifier\n    ‚îú‚îÄ‚îÄ Gradient Boosting\n    ‚îî‚îÄ‚îÄ XGBoost\n    ‚Üì\n[PREDICTION] ‚Üí Generate ensemble predictions\n    ‚îú‚îÄ‚îÄ Combine model outputs\n    ‚îú‚îÄ‚îÄ Apply thresholding\n    ‚îî‚îÄ‚îÄ Select GO terms\n    ‚Üì\nPREDICTIONS ‚Üí Function annotations for proteins\n```\n\n---\n\n## ‚ú® Key Technical Components\n\n### 1. Feature Engineering\n\n#### Sequence-Based Features\n- **Amino Acid Composition** (20 features)\n  - Frequency of each amino acid\n  - Normalized by sequence length\n  \n- **Physical Properties** (6 features)\n  - Hydrophobic/polar ratios\n  - Charge distribution\n  - Aromaticity measures\n  \n- **Structural Indicators** (8 features)\n  - Helix-forming propensity\n  - Disorder indicators\n  - Turn and coil propensity\n  \n- **Sequence Patterns** (10+ features)\n  - Dipeptide frequencies\n  - Motif presence\n  - Pattern distributions\n\n#### Derived Features\n- Logarithmic sequence length\n- N-terminal and C-terminal properties\n- Normalized distributions\n- Interaction features\n\n### 2. Machine Learning Models\n\n**Model 1: Random Forest**\n- Advantages: Fast, handles non-linearity, interpretable\n- Parameters: 100-150 trees, depth 15-18\n- Use case: Baseline and feature importance\n\n**Model 2: Gradient Boosting**\n- Advantages: Sequential optimization, strong performance\n- Parameters: 80-100 trees, depth 5-6\n- Use case: Refined predictions\n\n**Model 3: XGBoost**\n- Advantages: State-of-the-art, regularization, fast\n- Parameters: 80-100 trees, depth 6-7, learning rate 0.1\n- Use case: Final optimization\n\n**Ensemble Strategy**\n- Train individual models on same data\n- Average probability predictions\n- Apply threshold for binary classification\n- Combine predictions for robustness\n\n### 3. Multi-Label Classification\n\n**Challenge:** Each protein has multiple functions  \n**Solution:**\n- One-vs-rest binary classifiers for each GO term\n- Output probability for each possible function\n- Threshold-based selection of predicted functions\n- Handle class imbalance with appropriate weighting\n\n---\n\n## üì¶ Technology Stack\n\n### Programming Environment\n- **Language:** Python 3.7+\n- **Notebook:** Jupyter / Kaggle Notebooks\n\n### Core Libraries\n```\nData Processing:\n  - pandas: Data manipulation and analysis\n  - numpy: Numerical computing\n\nMachine Learning:\n  - scikit-learn: ML algorithms and preprocessing\n  - xgboost: Gradient boosting framework\n\nVisualization:\n  - matplotlib: Plotting and charts\n  - seaborn: Statistical data visualization\n\nFile Handling:\n  - Standard file I/O for FASTA/TSV formats\n```\n\n---\n\n## üöÄ Project Workflow\n\n### Phase 1: Data Understanding\n1. Load protein sequences (FASTA format)\n2. Load taxonomy and function annotations (TSV format)\n3. Exploratory data analysis\n4. Visualize data distributions\n5. Identify patterns and challenges\n\n### Phase 2: Feature Engineering\n1. Extract amino acid compositions\n2. Calculate physical/chemical properties\n3. Compute sequence patterns\n4. Normalize and scale features\n5. Create interaction features\n\n### Phase 3: Model Development\n1. Prepare train-validation split\n2. Train individual models\n3. Evaluate on validation set\n4. Calculate performance metrics (F1-score)\n5. Analyze prediction quality\n\n### Phase 4: Ensemble & Optimization\n1. Combine model predictions\n2. Tune prediction thresholds\n3. Handle edge cases\n4. Validate on test set\n5. Generate final predictions\n\n### Phase 5: Evaluation & Analysis\n1. Compute performance metrics\n2. Generate precision-recall curves\n3. Analyze error patterns\n4. Document results\n5. Prepare submission\n\n---\n\n## üìä Data Specifications\n\n### Input Data Format\n\n**Protein Sequences (FASTA)**\n```\n>ProteinID1\nMKTIIALSYIFCLVFADYKDDDKGTFTVENTAFITAHVQMFEKQDTLNGGAKTFTVTE\n\n>ProteinID2\nMKILIGKEVGSVHQGISIKPESAQHSTDCDKKVTL...\n```\n\n**Function Annotations (TSV)**\n```\nProteinID1\tGO:0008150\tProcess\nProteinID1\tGO:0005575\tComponent\nProteinID2\tGO:0003674\tFunction\n```\n\n**Taxonomy Information (TSV)**\n```\nProteinID1\t9606\nProteinID2\t10090\n```\n\n### Output Format\n\n**Predictions (TSV)**\n```\nProteinID1\tGO:0008150 GO:0005575 GO:0003674\nProteinID2\tGO:0003674 GO:0005575\nProteinID3\tGO:0008150\n```\n\n---\n\n## üîç Model Performance Expectations\n\n### Validation Metrics\n- **F1-Score:** 0.45-0.60 (depends on model combination)\n- **Precision:** 0.50-0.65\n- **Recall:** 0.40-0.55\n- **Accuracy (Multi-label):** Varies by GO term\n\n### Training Efficiency\n- **Feature Extraction:** 5-10 minutes\n- **Model Training:** 40-60 minutes\n- **Prediction:** 10-15 minutes\n- **Total Runtime:** 55-85 minutes\n\n### Expected Results\n- Reasonable predictions for most proteins\n- Better performance on common GO terms\n- Variable performance on rare functions\n- Ensemble improves over single models\n\n---\n\n## üí° Key Learnings & Techniques\n\n### Machine Learning Concepts\n‚úÖ Feature engineering from biological sequences  \n‚úÖ Handling multi-label classification problems  \n‚úÖ Ensemble methods and stacking  \n‚úÖ Hyperparameter tuning  \n‚úÖ Model validation and evaluation  \n‚úÖ Imbalanced classification handling  \n‚úÖ Threshold optimization for classification  \n\n### Bioinformatics Concepts\n‚úÖ Amino acid properties and structure  \n‚úÖ Sequence analysis and patterns  \n‚úÖ Gene Ontology and function annotations  \n‚úÖ Organism taxonomy and evolution  \n‚úÖ Protein structure-function relationships  \n\n### Best Practices\n‚úÖ Reproducible code with random seeds  \n‚úÖ Clear documentation and comments  \n‚úÖ Proper train-validation-test splits  \n‚úÖ Performance metrics tracking  \n‚úÖ Error handling and edge cases  \n‚úÖ Scalable architecture  \n\n---\n\n## üéì How to Use This Project\n\n### For Learning\n1. Study the feature engineering approach\n2. Understand ensemble methodology\n3. Learn multi-label classification techniques\n4. Adapt for similar problems\n\n### For Implementation\n1. Prepare your protein sequence data\n2. Adapt feature extraction for your needs\n3. Modify model parameters as needed\n4. Extend with domain-specific features\n\n### For Improvement\n1. Add deep learning embeddings\n2. Include more sequence features\n3. Use advanced ensemble techniques\n4. Implement cross-validation\n5. Add transfer learning\n\n---\n\n## üìà Potential Extensions\n\n### Advanced Techniques\n- **Deep Learning:** Use pre-trained protein models (ESM2, ProtBERT)\n- **Transfer Learning:** Leverage biological foundation models\n- **Attention Mechanisms:** Learn feature importance automatically\n- **Graph Neural Networks:** Model protein structure and interactions\n- **Stacking:** Use meta-learner on model outputs\n\n### Domain Enhancements\n- **Taxonomic Information:** Incorporate organism-specific patterns\n- **Sequence Alignment:** Use homology information\n- **Structure Features:** Include 3D protein structure data\n- **Interaction Data:** Use protein-protein interaction networks\n- **Literature Mining:** Incorporate biological knowledge\n\n### Operational Improvements\n- **Hyperparameter Tuning:** GridSearch/RandomSearch optimization\n- **K-Fold Validation:** More robust evaluation\n- **Threshold Optimization:** Per-GO-term threshold tuning\n- **Class Weighting:** Handle imbalanced data better\n- **Feature Selection:** Identify most important features\n\n---\n\n## üìö References & Resources\n\n### Key Concepts\n- Gene Ontology: http://geneontology.org/\n- Protein Classification: Standard bioinformatics references\n- Ensemble Learning: scikit-learn documentation\n- XGBoost: https://xgboost.readthedocs.io/\n\n### Libraries & Tools\n- scikit-learn: https://scikit-learn.org/\n- XGBoost: https://xgboost.readthedocs.io/\n- pandas: https://pandas.pydata.org/\n- numpy: https://numpy.org/\n\n### Bioinformatics Resources\n- UniProt: https://www.uniprot.org/\n- NCBI: https://www.ncbi.nlm.nih.gov/\n- InterPro: https://www.ebi.ac.uk/interpro/\n\n\n\n## üìù Project Summary\n\nThis project demonstrates **how to approach a real-world bioinformatics machine learning problem** using:\n- Thoughtful feature engineering\n- Multiple complementary models\n- Ensemble learning for robust predictions\n- Proper validation methodology\n- Clear documentation\n\nThe combination of domain knowledge and machine learning techniques creates an effective system for protein function prediction, applicable to real biological research and drug discovery workflows.\n\n---\n\n**This is an educational project demonstrating machine learning and bioinformatics concepts.**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntry:\n    import torch\n    from transformers import AutoTokenizer, AutoModel\n    HAS_ESM = True\nexcept Exception as e:\n    HAS_ESM = False\n    print(f\"Warning: ESM2 not available ({e}), using basic features\")\n    torch = None\n\nclass CAFA6ImprovedPredictor:\n    \"\"\"Improved CAFA-6 solution with ensemble & optimization\"\"\"\n    \n    def __init__(self):\n        self.models = {}\n        self.mlb = MultiLabelBinarizer()\n        self.top_go_terms = None\n        self.embedding_dim = None\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if HAS_ESM and torch is not None else None\n        self.scaler = None\n        \n    def load_fasta(self, fasta_path):\n        \"\"\"Load FASTA sequences\"\"\"\n        sequences = {}\n        try:\n            with open(fasta_path, 'r') as f:\n                current_id = None\n                current_seq = []\n                \n                for line in f:\n                    line = line.strip()\n                    if line.startswith('>'):\n                        if current_id:\n                            sequences[current_id] = ''.join(current_seq)\n                        current_id = line[1:].split()[0]\n                        current_seq = []\n                    elif current_id:\n                        current_seq.append(line)\n                \n                if current_id:\n                    sequences[current_id] = ''.join(current_seq)\n        except Exception as e:\n            print(f\"Error loading FASTA: {e}\")\n        \n        return sequences\n    \n    def load_esm_model(self):\n        \"\"\"Load ESM2 model\"\"\"\n        if not HAS_ESM or self.device is None:\n            return None, None\n        \n        try:\n            model_name = \"facebook/esm2_t12_35M_UR50D\"  # Better model (was t6_8M)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            model = AutoModel.from_pretrained(model_name).to(self.device).eval()\n            return tokenizer, model\n        except:\n            return None, None\n    \n    def get_batch_embeddings(self, sequences, tokenizer, model, batch_size=16):\n        \"\"\"Get embeddings with improved pooling\"\"\"\n        if tokenizer is None or model is None:\n            return np.array([self.get_advanced_features(seq) for seq in sequences])\n        \n        embeddings = []\n        \n        for i in range(0, len(sequences), batch_size):\n            batch_seqs = sequences[i:i+batch_size]\n            \n            try:\n                inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, \n                                 truncation=True, max_length=1022).to(self.device)\n                \n                with torch.no_grad():\n                    outputs = model(**inputs)\n                    # Multi-pooling: mean + last token\n                    last_hidden = outputs.last_hidden_state\n                    mean_pool = last_hidden.mean(dim=1)\n                    cls_token = last_hidden[:, 0, :]\n                    # Concatenate different pooling strategies\n                    combined = torch.cat([mean_pool, cls_token], dim=1)\n                    embeddings.extend(combined.cpu().numpy())\n            except:\n                for seq in batch_seqs:\n                    embeddings.append(self.get_advanced_features(seq))\n        \n        return np.array(embeddings, dtype=np.float32)\n    \n    def get_advanced_features(self, sequence):\n        \"\"\"Enhanced sequence features\"\"\"\n        seq = str(sequence).upper()\n        length = max(len(seq), 1)\n        \n        features = []\n        \n        # Amino acid composition (20 standard)\n        aa_list = 'ACDEFGHIKLMNPQRSTVWY'\n        for aa in aa_list:\n            features.append(seq.count(aa) / length)\n        \n        # Chemical properties\n        hydrophobic = sum(seq.count(aa) for aa in 'AILMFVP') / length\n        charged = (sum(seq.count(aa) for aa in 'KR') - sum(seq.count(aa) for aa in 'DE')) / length\n        aromatic = (seq.count('F') + seq.count('W') + seq.count('Y')) / length\n        polar = sum(seq.count(aa) for aa in 'STNQ') / length\n        \n        # Di-peptide frequencies (sample)\n        dipeptides = ['AA', 'AC', 'AE', 'AG', 'AL']\n        for dp in dipeptides:\n            features.append(seq.count(dp) / max(length - 1, 1))\n        \n        features.extend([hydrophobic, charged, aromatic, polar, np.log1p(length)])\n        \n        return np.array(features, dtype=np.float32)\n    \n    def load_data(self, train_seq, train_taxon, train_terms, test_seq, test_taxon):\n        \"\"\"Load data\"\"\"\n        print(\"=\"*70)\n        print(\"LOADING DATA\")\n        print(\"=\"*70)\n        \n        print(\"\\n1. Loading sequences...\")\n        self.train_sequences = self.load_fasta(train_seq)\n        self.test_sequences = self.load_fasta(test_seq)\n        print(f\"   ‚úì Train: {len(self.train_sequences)} | Test: {len(self.test_sequences)}\")\n        \n        print(\"\\n2. Loading taxonomy...\")\n        self.train_taxon = pd.read_csv(train_taxon, sep='\\t', header=None, \n                                       names=['protein_id', 'taxon_id'], dtype=str)\n        \n        print(\"\\n3. Loading annotations...\")\n        train_terms_df = pd.read_csv(train_terms, sep='\\t', header=None, \n                                     names=['protein_id', 'go_id', 'aspect'], dtype=str)\n        \n        # IMPROVEMENT: Select top 300 GO terms instead of 200\n        go_counts = train_terms_df['go_id'].value_counts()\n        self.top_go_terms = go_counts.head(300).index.tolist()\n        \n        train_terms_df = train_terms_df[train_terms_df['go_id'].isin(self.top_go_terms)]\n        self.go_dict = train_terms_df.groupby('protein_id')['go_id'].apply(list).to_dict()\n        \n        print(f\"   ‚úì GO terms: {len(self.top_go_terms)}\")\n        print(f\"   ‚úì Annotations: {len(train_terms_df)}\\n\")\n        \n        del train_terms_df\n        gc.collect()\n    \n    def prepare_embeddings(self):\n        \"\"\"Extract embeddings\"\"\"\n        print(\"=\"*70)\n        print(\"PREPARING EMBEDDINGS\")\n        print(\"=\"*70)\n        \n        protein_ids = self.train_taxon['protein_id'].values\n        sequences = [self.train_sequences.get(str(pid), '') for pid in protein_ids]\n        \n        print(\"\\nLoading ESM2 model...\")\n        tokenizer, model = self.load_esm_model()\n        \n        print(\"Extracting train embeddings...\")\n        X = self.get_batch_embeddings(sequences, tokenizer, model, batch_size=16)\n        \n        self.embedding_dim = X.shape[1]\n        print(f\"‚úì Train embeddings: {X.shape}\")\n        \n        # Normalize embeddings (IMPROVEMENT)\n        from sklearn.preprocessing import StandardScaler\n        self.scaler = StandardScaler()\n        X = self.scaler.fit_transform(X)\n        \n        print(\"Creating labels...\")\n        self.go_labels = {}\n        for go_idx, go_term in enumerate(self.top_go_terms):\n            labels = []\n            for pid in protein_ids:\n                has_go = 1 if go_term in self.go_dict.get(str(pid), []) else 0\n                labels.append(has_go)\n            self.go_labels[go_idx] = np.array(labels, dtype=np.uint8)\n        \n        print(f\"‚úì Labels created for {len(self.go_labels)} GO terms\\n\")\n        \n        return X, protein_ids\n    \n    def prepare_test_embeddings(self):\n        \"\"\"Extract test embeddings\"\"\"\n        print(\"Extracting test embeddings...\")\n        \n        test_ids = list(self.test_sequences.keys())\n        sequences = [self.test_sequences.get(str(pid), '') for pid in test_ids]\n        \n        print(\"Loading ESM2 model...\")\n        tokenizer, model = self.load_esm_model()\n        \n        X_test = self.get_batch_embeddings(sequences, tokenizer, model, batch_size=16)\n        X_test = self.scaler.transform(X_test)\n        \n        print(f\"‚úì Test embeddings: {X_test.shape}\\n\")\n        \n        return X_test, test_ids\n    \n    def train_ensemble_models(self, X, protein_ids):\n        \"\"\"Train ensemble of models (IMPROVEMENT)\"\"\"\n        print(\"=\"*70)\n        print(\"TRAINING ENSEMBLE MODELS\")\n        print(\"=\"*70)\n        \n        # IMPROVEMENT: Use 70% of data for better training\n        sample_size = int(0.7 * len(protein_ids))\n        sample_idx = np.random.choice(len(protein_ids), size=sample_size, replace=False)\n        X_train = X[sample_idx]\n        \n        print(f\"\\nTraining on {len(X_train)} samples with ensemble...\\n\")\n        \n        for go_idx, go_term in enumerate(self.top_go_terms):\n            if go_idx % 50 == 0:\n                print(f\"Training GO {go_idx}/{len(self.top_go_terms)}...\")\n            \n            y_train = self.go_labels[go_idx][sample_idx]\n            \n            if y_train.sum() < 2:\n                self.models[go_idx] = None\n                continue\n            \n            # IMPROVEMENT: Ensemble of 3 models with voting\n            models_list = []\n            \n            # Model 1: Logistic Regression\n            lr = LogisticRegression(max_iter=300, random_state=42, class_weight='balanced', n_jobs=-1)\n            models_list.append(('lr', lr))\n            \n            # Model 2: XGBoost (better for complex patterns)\n            if y_train.sum() > 10:\n                xgb = XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, \n                                   random_state=42, scale_pos_weight=len(y_train)/y_train.sum())\n                models_list.append(('xgb', xgb))\n            \n            # Model 3: Random Forest (robustness)\n            rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, \n                                       class_weight='balanced', n_jobs=-1)\n            models_list.append(('rf', rf))\n            \n            try:\n                trained_models = {}\n                for name, model in models_list:\n                    model.fit(X_train, y_train)\n                    trained_models[name] = model\n                self.models[go_idx] = trained_models\n            except:\n                self.models[go_idx] = None\n        \n        print(\"‚úì Training complete!\\n\")\n    \n    def predict_probabilities(self, X_test):\n        \"\"\"Generate ensemble predictions (IMPROVEMENT)\"\"\"\n        print(\"=\"*70)\n        print(\"GENERATING ENSEMBLE PREDICTIONS\")\n        print(\"=\"*70 + \"\\n\")\n        \n        n_samples = X_test.shape[0]\n        n_go_terms = len(self.top_go_terms)\n        \n        predictions = np.zeros((n_samples, n_go_terms), dtype=np.float32)\n        \n        for go_idx in range(n_go_terms):\n            if go_idx % 50 == 0:\n                print(f\"Predicting GO {go_idx}/{n_go_terms}...\")\n            \n            model_dict = self.models.get(go_idx)\n            \n            if model_dict is None:\n                predictions[:, go_idx] = 0.1\n            else:\n                try:\n                    # Average predictions from ensemble\n                    ensemble_probs = []\n                    for name, model in model_dict.items():\n                        proba = model.predict_proba(X_test)\n                        if proba.shape[1] == 2:\n                            ensemble_probs.append(proba[:, 1])\n                    \n                    if ensemble_probs:\n                        predictions[:, go_idx] = np.mean(ensemble_probs, axis=0)\n                    else:\n                        predictions[:, go_idx] = 0.1\n                except:\n                    predictions[:, go_idx] = 0.1\n        \n        print(\"‚úì Predictions generated!\\n\")\n        return predictions\n    \n    def create_submission(self, predictions, test_ids):\n        \"\"\"Create submission (IMPROVEMENT: better threshold tuning)\"\"\"\n        print(\"=\"*70)\n        print(\"CREATING SUBMISSION\")\n        print(\"=\"*70 + \"\\n\")\n        \n        submission_data = []\n        \n        for i, protein_id in enumerate(test_ids):\n            pred_probs = predictions[i]\n            \n            # IMPROVEMENT: Dynamic threshold based on probability distribution\n            threshold = np.percentile(pred_probs, 75)\n            threshold = max(threshold, 0.2)  # Minimum threshold\n            \n            top_indices = np.argsort(pred_probs)[-15:][::-1]  # Top 15 instead of 10\n            go_terms = []\n            \n            for idx in top_indices:\n                if pred_probs[idx] > threshold:\n                    go_terms.append(self.top_go_terms[idx])\n            \n            if not go_terms:\n                go_terms = [self.top_go_terms[top_indices[0]]]\n            \n            submission_data.append({\n                'target_id': protein_id,\n                'predictions': ' '.join(go_terms)\n            })\n            \n            if (i + 1) % 50000 == 0:\n                print(f\"  Processed {i + 1}/{len(test_ids)} predictions...\")\n        \n        submission_df = pd.DataFrame(submission_data)\n        submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)\n        \n        print(f\"\\n‚úì Submission saved: submission.tsv\")\n        print(f\"  Shape: {submission_df.shape}\")\n        print(f\"  Sample:\\n{submission_df.head()}\")\n    \n    def run(self, train_seq, train_taxon, train_terms, test_seq, test_taxon):\n        \"\"\"Execute pipeline\"\"\"\n        print(\"\\n\" + \"‚ïî\" + \"=\"*68 + \"‚ïó\")\n        print(\"‚ïë\" + \" \"*10 + \"CAFA-6 IMPROVED SOLUTION (0.32+)\" + \" \"*25 + \"‚ïë\")\n        print(\"‚ïë\" + \" \"*12 + \"ESM2 + ENSEMBLE + OPTIMIZATIONS\" + \" \"*23 + \"‚ïë\")\n        print(\"‚ïö\" + \"=\"*68 + \"‚ïù\\n\")\n        \n        try:\n            self.load_data(train_seq, train_taxon, train_terms, test_seq, test_taxon)\n            X, protein_ids = self.prepare_embeddings()\n            X_test, test_ids = self.prepare_test_embeddings()\n            self.train_ensemble_models(X, protein_ids)\n            predictions = self.predict_probabilities(X_test)\n            self.create_submission(predictions, test_ids)\n            \n            print(\"‚ïî\" + \"=\"*68 + \"‚ïó\")\n            print(\"‚ïë\" + \" \"*15 + \"‚úì SUCCESS - READY FOR SUBMISSION!\" + \" \"*19 + \"‚ïë\")\n            print(\"‚ïö\" + \"=\"*68 + \"‚ïù\\n\")\n            \n        except Exception as e:\n            print(f\"\\n‚ùå Error: {e}\")\n            import traceback\n            traceback.print_exc()\n\n\nif __name__ == \"__main__\":\n    predictor = CAFA6ImprovedPredictor()\n    \n    predictor.run(\n        train_seq='/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta',\n        train_taxon='/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv',\n        train_terms='/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n        test_seq='/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta',\n        test_taxon='/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv'\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:20:20.348895Z","iopub.execute_input":"2025-10-26T12:20:20.349216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}