{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":268508255,"sourceType":"kernelVersion"},{"sourceId":269339911,"sourceType":"kernelVersion"},{"sourceId":270571028,"sourceType":"kernelVersion"},{"sourceId":271888783,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":606.301651,"end_time":"2025-10-26T08:08:12.953344","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-26T07:58:06.651693","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\ndef median_ensemble(file_paths, output_path='submission.tsv', chunksize=100000):\n    print(f\"Processing in chunks of {chunksize}\")\n\n    # Step 1: Collect all unique keys (protein_go_term pairs)\n    all_keys = set()\n    for path in file_paths:\n        for chunk in pd.read_csv(path, sep='\\t', header=None,\n                                 names=['protein', 'go_term', 'score'],\n                                 dtype={'protein': str, 'go_term': str, 'score': float},\n                                 chunksize=chunksize):\n            chunk = chunk.dropna(subset=['protein', 'go_term'])\n            chunk['key'] = chunk['protein'] + '_' + chunk['go_term']\n            all_keys.update(chunk['key'].values)\n\n    all_keys = sorted(all_keys)\n    print(f\"Total unique predictions: {len(all_keys)}\")\n\n    temp_files = []\n\n    # Step 2: Process by chunks of keys\n    for start_idx in range(0, len(all_keys), chunksize):\n        end_idx = min(start_idx + chunksize, len(all_keys))\n        key_chunk = all_keys[start_idx:end_idx]\n        result = pd.DataFrame({'key': key_chunk})\n\n        # Step 3: Load model scores for current key chunk\n        for i, path in enumerate(file_paths):\n            model_data = []\n            for chunk in pd.read_csv(path, sep='\\t', header=None,\n                                     names=['protein', 'go_term', 'score'],\n                                     dtype={'protein': str, 'go_term': str, 'score': float},\n                                     chunksize=chunksize):\n                chunk['key'] = chunk['protein'] + '_' + chunk['go_term']\n                chunk_filtered = chunk[chunk['key'].isin(key_chunk)][['key', 'score']]\n                model_data.append(chunk_filtered)\n\n            if model_data:\n                model_df = pd.concat(model_data, ignore_index=True)\n                model_df = model_df.rename(columns={'score': f'score_{i}'})\n                result = result.merge(model_df, on='key', how='left')\n\n        # Step 4: Replace NaNs with 0\n        for i in range(len(file_paths)):\n            result[f'score_{i}'] = result[f'score_{i}'].fillna(0)\n\n        # Step 5: Compute median across models\n        model_cols = [f'score_{i}' for i in range(len(file_paths))]\n        result['final_score'] = result[model_cols].median(axis=1)\n\n        # Step 6: Split key back into protein and go_term\n        result['protein'] = result['key'].str.rsplit('_', n=1).str[0]\n        result['go_term'] = result['key'].str.rsplit('_', n=1).str[-1]\n\n        temp_file = f'temp_chunk_{start_idx}.csv'\n        result[['protein', 'go_term', 'final_score']].to_csv(temp_file, index=False, sep='\\t', header=False)\n        temp_files.append(temp_file)\n        print(f\"Processed chunk {len(temp_files)}\")\n\n    # Step 7: Combine all chunk files\n    all_data = [pd.read_csv(f, sep='\\t', header=None, names=['protein', 'go_term', 'final_score'])\n                for f in temp_files]\n    final_result = pd.concat(all_data, ignore_index=True)\n    final_result.to_csv(output_path, sep='\\t', index=False, header=False)\n\n    # Step 8: Clean up temp files\n    for temp_file in temp_files:\n        os.remove(temp_file)\n\n    print(f\"Saved median ensemble to {output_path}\")\n    return final_result\n\n\nif __name__ == \"__main__\":\n    file_paths = [\n        '/kaggle/input/gaf-submission/submission.tsv',\n        '/kaggle/input/cafa-6-predictions/submission.tsv'\n    ]\n\n    result = median_ensemble(file_paths, chunksize=10_000_000)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-28T05:04:45.101269Z","iopub.execute_input":"2025-10-28T05:04:45.101507Z"},"papermill":{"duration":596.172465,"end_time":"2025-10-26T08:08:10.123901","exception":false,"start_time":"2025-10-26T07:58:13.951436","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}