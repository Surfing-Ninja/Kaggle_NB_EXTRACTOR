{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook demonstrates how to use [ESM-2](https://github.com/facebookresearch/esm) transformer protein language model to extract embeddings from provided protein sequences. The following code calculates the embeddings for each protein sequence in the training and test FASTA files and saves them as individual `.pt` files. \n\nThe resulting embedding can be loaded using the following code:\n```\nimport torch\n\nembedding = torch.load('[EntryID].pt')\nembedding = embedding['mean_representations'][33].numpy()\n```\n\nComputing the embeddings and subsequently reading in the `.pt` files can take a while. The resulting numpy arrays can be found [here](https://www.kaggle.com/datasets/viktorfairuschin/cafa-5-ems-2-embeddings-numpy).\n\n**Note** that the test FASTA file contains duplicate entries. For this reason, this notebook uses cleaned FASTA files, which can be found [here](https://www.kaggle.com/datasets/viktorfairuschin/cafa-5-fasta-files).","metadata":{}},{"cell_type":"code","source":"!pip install -q fair-esm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-10-24T13:09:19.413963Z","iopub.execute_input":"2025-10-24T13:09:19.414202Z","iopub.status.idle":"2025-10-24T13:09:29.897443Z","shell.execute_reply.started":"2025-10-24T13:09:19.414177Z","shell.execute_reply":"2025-10-24T13:09:29.896098Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pathlib\nimport torch\n\nfrom esm import FastaBatchedDataset, pretrained","metadata":{"execution":{"iopub.status.busy":"2025-10-24T13:09:29.901156Z","iopub.execute_input":"2025-10-24T13:09:29.901655Z","iopub.status.idle":"2025-10-24T13:09:32.537608Z","shell.execute_reply.started":"2025-10-24T13:09:29.901608Z","shell.execute_reply":"2025-10-24T13:09:32.536753Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_embeddings(model_name, fasta_file, output_dir, tokens_per_batch=4096, seq_length=1022, repr_layers=[33], num_gpus=2):\n    \n    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n    model.eval()\n    \n    if torch.cuda.is_available():\n        # Wrap model with DataParallel for multi-GPU usage\n        if num_gpus > 1 and torch.cuda.device_count() >= num_gpus:\n            print(f\"Using {num_gpus} GPUs: {list(range(num_gpus))}\")\n            model = torch.nn.DataParallel(model, device_ids=list(range(num_gpus)))\n        model = model.cuda()\n    \n    dataset = FastaBatchedDataset.from_file(fasta_file)\n    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, \n        collate_fn=alphabet.get_batch_converter(seq_length), \n        batch_sampler=batches,\n        num_workers=0,  # Keep 0 to avoid issues with CUDA and multiprocessing\n        pin_memory=True  # Enable for faster data transfer to GPU\n    )\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    with torch.no_grad():\n        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n            if (batch_idx % 1000) == 0:\n                print(f'Processing batch {batch_idx + 1} of {len(batches)}')\n            \n            if torch.cuda.is_available():\n                toks = toks.to(device=\"cuda\", non_blocking=True)\n            \n            out = model(toks, repr_layers=repr_layers, return_contacts=False)\n            \n            logits = out[\"logits\"].to(device=\"cpu\")\n            representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n            \n            for i, label in enumerate(labels):\n                entry_id = label.split()[0]\n                \n                filename = output_dir / f\"{entry_id}.pt\"\n                truncate_len = min(seq_length, len(strs[i]))\n                result = {\"entry_id\": entry_id}\n                result[\"mean_representations\"] = {\n                    layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n                    for layer, t in representations.items()\n                }\n                torch.save(result, filename)","metadata":{"execution":{"iopub.status.busy":"2025-10-24T13:09:32.538662Z","iopub.execute_input":"2025-10-24T13:09:32.539093Z","iopub.status.idle":"2025-10-24T13:09:32.550559Z","shell.execute_reply.started":"2025-10-24T13:09:32.539068Z","shell.execute_reply":"2025-10-24T13:09:32.54965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Process train file","metadata":{}},{"cell_type":"code","source":"model_name = 'esm2_t33_650M_UR50D'\nfasta_file = pathlib.Path('/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta')\noutput_dir = pathlib.Path('train_embeddings')\n\nextract_embeddings(model_name, fasta_file, output_dir)","metadata":{"execution":{"iopub.status.busy":"2025-10-24T13:09:32.551927Z","iopub.execute_input":"2025-10-24T13:09:32.552331Z","iopub.status.idle":"2025-10-24T13:10:24.522171Z","shell.execute_reply.started":"2025-10-24T13:09:32.552254Z","shell.execute_reply":"2025-10-24T13:10:24.52063Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Process test file","metadata":{}},{"cell_type":"code","source":"model_name = 'esm2_t33_650M_UR50D'\nfasta_file = pathlib.Path('/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta')\noutput_dir = pathlib.Path('test_embeddings')\n\nextract_embeddings(model_name, fasta_file, output_dir)","metadata":{"execution":{"iopub.status.busy":"2025-10-24T13:10:24.523149Z","iopub.status.idle":"2025-10-24T13:10:24.523536Z","shell.execute_reply.started":"2025-10-24T13:10:24.523329Z","shell.execute_reply":"2025-10-24T13:10:24.523347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}