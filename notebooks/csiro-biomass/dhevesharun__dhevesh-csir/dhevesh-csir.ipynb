{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:06.554457Z","iopub.execute_input":"2025-10-29T14:51:06.55526Z","iopub.status.idle":"2025-10-29T14:51:08.144804Z","shell.execute_reply.started":"2025-10-29T14:51:06.555227Z","shell.execute_reply":"2025-10-29T14:51:08.144018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom PIL import Image\nimport cv2\n\n# Deep learning feature extraction\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:08.145741Z","iopub.execute_input":"2025-10-29T14:51:08.146017Z","iopub.status.idle":"2025-10-29T14:51:08.532005Z","shell.execute_reply.started":"2025-10-29T14:51:08.146Z","shell.execute_reply":"2025-10-29T14:51:08.531422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths to input data\ndata_path = \"/kaggle/input/csiro-biomass\"\ntrain_df = pd.read_csv(f\"{data_path}/train.csv\")\ntest_df = pd.read_csv(f\"{data_path}/test.csv\")\nsample_sub = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:08.532668Z","iopub.execute_input":"2025-10-29T14:51:08.53298Z","iopub.status.idle":"2025-10-29T14:51:08.607382Z","shell.execute_reply.started":"2025-10-29T14:51:08.532954Z","shell.execute_reply":"2025-10-29T14:51:08.606811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dir = f\"{data_path}/train\"\ntest_dir = f\"{data_path}/test\"\n\nprint(\"Number of train images:\", len(os.listdir(train_dir)))\nprint(\"Number of test images:\", len(os.listdir(test_dir)))\n\nimport matplotlib.pyplot as plt\nimport random\n\nsample_images = random.sample(os.listdir(train_dir), 3)\nplt.figure(figsize=(12, 4))\nfor i, img_name in enumerate(sample_images):\n    img = Image.open(os.path.join(train_dir, img_name))\n    plt.subplot(1, 3, i+1)\n    plt.imshow(img)\n    plt.title(img_name)\n    plt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:08.608848Z","iopub.execute_input":"2025-10-29T14:51:08.609102Z","iopub.status.idle":"2025-10-29T14:51:09.738775Z","shell.execute_reply.started":"2025-10-29T14:51:08.609085Z","shell.execute_reply":"2025-10-29T14:51:09.73803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_preprocess_image(img_path, target_size=(224, 224)):\n    img = image.load_img(img_path, target_size=target_size)\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = preprocess_input(img_array)\n    return img_array\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:09.739491Z","iopub.execute_input":"2025-10-29T14:51:09.739699Z","iopub.status.idle":"2025-10-29T14:51:09.743681Z","shell.execute_reply.started":"2025-10-29T14:51:09.739683Z","shell.execute_reply":"2025-10-29T14:51:09.743092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\n\n\nbase_model = ResNet50(weights=None, include_top=False, pooling='avg')\n\n\nmodel = Model(inputs=base_model.input, outputs=base_model.output)\n\nprint(\"âœ… ResNet50 model created successfully (no pretrained weights).\")\n\ndef extract_features(image_folder, image_paths):\n    features = []\n    for img_path in tqdm(image_paths):\n        # Support full paths and base names\n        if not os.path.exists(img_path):\n            img_path = os.path.join(image_folder, os.path.basename(img_path))\n        if os.path.exists(img_path):\n            img_data = load_and_preprocess_image(img_path)\n            feat = model.predict(img_data, verbose=0)\n            features.append(feat.flatten())\n        else:\n            features.append(np.zeros(2048))  # fallback\n    return np.array(features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:09.744444Z","iopub.execute_input":"2025-10-29T14:51:09.745087Z","iopub.status.idle":"2025-10-29T14:51:10.522567Z","shell.execute_reply.started":"2025-10-29T14:51:09.745063Z","shell.execute_reply":"2025-10-29T14:51:10.521801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train columns:\", train_df.columns.tolist())\nprint(\"Test columns:\", test_df.columns.tolist())\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:10.523434Z","iopub.execute_input":"2025-10-29T14:51:10.523699Z","iopub.status.idle":"2025-10-29T14:51:10.534433Z","shell.execute_reply.started":"2025-10-29T14:51:10.523676Z","shell.execute_reply":"2025-10-29T14:51:10.53374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_features = extract_features(train_dir, train_df['image_path'])\ntest_features = extract_features(test_dir, test_df['image_path'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:51:10.535016Z","iopub.execute_input":"2025-10-29T14:51:10.535248Z","iopub.status.idle":"2025-10-29T14:55:10.869492Z","shell.execute_reply.started":"2025-10-29T14:51:10.535233Z","shell.execute_reply":"2025-10-29T14:55:10.868902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n\ntarget_cols = train_df['target_name'].unique().tolist()\nprint(\"Targets found:\", target_cols)\n\n\nscaler = StandardScaler()\nX = scaler.fit_transform(train_features)\nX_test = scaler.transform(test_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:55:10.870331Z","iopub.execute_input":"2025-10-29T14:55:10.870736Z","iopub.status.idle":"2025-10-29T14:55:10.989352Z","shell.execute_reply.started":"2025-10-29T14:55:10.870718Z","shell.execute_reply":"2025-10-29T14:55:10.988506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntarget_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n\nscaler = StandardScaler()\nX = scaler.fit_transform(train_features)\nX_test = scaler.transform(test_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:55:10.991234Z","iopub.execute_input":"2025-10-29T14:55:10.991474Z","iopub.status.idle":"2025-10-29T14:55:11.062801Z","shell.execute_reply.started":"2025-10-29T14:55:10.991455Z","shell.execute_reply":"2025-10-29T14:55:11.062149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom tqdm import tqdm\n\nmodels = {}\npreds = []\n\n# List of targets\ntarget_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n\nfor target in target_cols:\n    print(f\"Training model for {target}...\")\n\n   \n    target_df = train_df[train_df['target_name'] == target].reset_index(drop=True)\n\n  \n    image_ids = target_df['image_path'].apply(lambda x: x.split('/')[-1].replace('.jpg', ''))\n\n  \n    X_target = []\n    for img_id in image_ids:\n        idx = train_df['image_path'].apply(lambda x: x.split('/')[-1].replace('.jpg', '')).tolist().index(img_id)\n        X_target.append(train_features[idx])\n    X_target = np.array(X_target)\n\n\n    y = target_df['target'].values\n\n    \n    model = RandomForestRegressor(n_estimators=200, random_state=42)\n    model.fit(X_target, y)\n    models[target] = model\n\n  \n    preds.append(model.predict(X_test))\n\n\npreds = np.column_stack(preds)  \nprint(\"âœ… Prediction shape:\", preds.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T14:55:11.063553Z","iopub.execute_input":"2025-10-29T14:55:11.063824Z","iopub.status.idle":"2025-10-29T15:00:23.941253Z","shell.execute_reply.started":"2025-10-29T14:55:11.063797Z","shell.execute_reply":"2025-10-29T15:00:23.940571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… Ensure preds and test_df match\nnum_preds = len(preds)\nnum_test = len(test_df)\n\nprint(f\"ðŸ” Predictions: {num_preds}, Test Images: {num_test}\")\n\nif num_preds != num_test:\n    print(\"âš ï¸ Warning: Mismatch detected! Adjusting...\")\n    if num_preds > num_test:\n        preds = preds[:num_test]\n    else:\n        avg_pred = np.mean(preds, axis=0)\n        while len(preds) < num_test:\n            preds = np.vstack([preds, avg_pred])\n\n# âœ… Create submission\nsubmission = []\n\n# Extract image IDs without file extensions\ntest_image_ids = test_df['image_path'].apply(lambda x: x.split('/')[-1].replace('.jpg', '')).values\n\nfor i, img_id in enumerate(test_image_ids):\n    for j, target in enumerate(target_cols):\n        sample_id = f\"{img_id}__{target}\"\n        submission.append([sample_id, preds[i][j]])\n\n# âœ… Convert to DataFrame and save\nsubmission_df = pd.DataFrame(submission, columns=['sample_id', 'target'])\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"âœ… submission.csv created successfully!\")\nprint(f\"ðŸ“„ Total rows in submission: {len(submission_df)}\")\n\n# Show first few rows for sanity check\nsubmission_df.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T15:03:22.400481Z","iopub.execute_input":"2025-10-29T15:03:22.40079Z","iopub.status.idle":"2025-10-29T15:03:22.414616Z","shell.execute_reply.started":"2025-10-29T15:03:22.400749Z","shell.execute_reply":"2025-10-29T15:03:22.413666Z"}},"outputs":[],"execution_count":null}]}