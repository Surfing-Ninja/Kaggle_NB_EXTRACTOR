{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":272104372,"sourceType":"kernelVersion"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm\nimport cv2\nfrom tqdm import tqdm\nimport gc\n\n# ===============================================================\n# 1. âš™ï¸ é…ç½® \n# ===============================================================\nclass CFG:\n    # --- è·¯å¾„  ---\n    BASE_PATH = '/kaggle/input/csiro-biomass'\n    TEST_CSV = os.path.join(BASE_PATH, 'test.csv')\n    TEST_IMAGE_DIR = os.path.join(BASE_PATH, 'test')\n    \n    # åŒ…å« 5 ä¸ª .pth æ–‡ä»¶çš„ç›®å½•\n    MODEL_DIR = '/kaggle/input/csiro/'\n    SUBMISSION_FILE = 'submission.csv'\n    \n    # --- æ¨¡å‹è®¾ç½® ---\n    MODEL_NAME = 'convnext_tiny'\n    IMG_SIZE = 768              \n    \n    # --- æ¨ç†è®¾ç½® ---\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    BATCH_SIZE = 1 # æ¨ç†æ—¶å¯ä»¥å¢å¤§å­¦ä¹ ç‡\n    NUM_WORKERS = 1\n    N_FOLDS = 5\n    \n    # --- ç›®æ ‡ä¸æŸå¤± (å¿…é¡»åŒ¹é…) ---\n    # æ¨¡å‹é¢„æµ‹çš„ 3 ä¸ªç›®æ ‡\n    TARGET_COLS = ['Dry_Total_g', 'GDM_g', 'Dry_Green_g']\n    \n    # æäº¤æ‰€éœ€çš„ 5 ä¸ªç›®æ ‡\n    ALL_TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n\nprint(f\"ä½¿ç”¨è®¾å¤‡: {CFG.DEVICE}\")\nprint(f\"æ¨¡å‹éª¨å¹²: {CFG.MODEL_NAME}\")\nprint(f\"æ¨ç†å›¾ç‰‡å°ºå¯¸: {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")\n\n\n# ===============================================================\n# 2. ğŸï¸ æ•°æ®å¢å¼º\n# ===============================================================\nfrom albumentations import (\n    Compose, \n    Resize, \n    Normalize,\n    HorizontalFlip, \n    VerticalFlip\n)\n\ndef get_tta_transforms():\n    \"\"\"\n    è¿”å›ä¸€ä¸ªç”¨äº TTA çš„ transform pipeline åˆ—è¡¨ã€‚\n    æ¯ä¸ª pipeline éƒ½æ˜¯å›¾åƒçš„ä¸€ä¸ªä¸åŒâ€œè§†è§’â€ã€‚\n    \"\"\"\n    \n    # è¿™æ˜¯åŸºæœ¬çš„æ ‡å‡†åŒ–æ­¥éª¤\n    base_transforms = [\n        Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        ),\n        ToTensorV2()\n    ]\n    \n    # -----------------\n    # è§†è§’ 1: åŸå§‹å›¾åƒ \n    # -----------------\n    original_view = Compose([\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # -----------------\n    # è§†è§’ 2: æ°´å¹³ç¿»è½¬ \n    # -----------------\n    hflip_view = Compose([\n        HorizontalFlip(p=1.0), # å§‹ç»ˆç¿»è½¬\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # -----------------\n    # è§†è§’ 3: å‚ç›´ç¿»è½¬ \n    # -----------------\n    vflip_view = Compose([\n        VerticalFlip(p=1.0), # å§‹ç»ˆç¿»è½¬\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    return [original_view, hflip_view, vflip_view]\n\nprint(\"å·²å®šä¹‰ get_tta_transforms() å‡½æ•°ã€‚\")\n\n\nclass TestBiomassDataset(Dataset):\n    \"\"\"\n    ç”¨äºæµ‹è¯•å›¾åƒçš„è‡ªå®šä¹‰ Dataset (åŒæµç­–ç•¥)ã€‚\n    ä¿®æ”¹ä¸ºæ¥å—ä¸€ä¸ªç‰¹å®šçš„ TTA transform pipelineã€‚\n    \"\"\"\n    def __init__(self, df, transform_pipeline, image_dir):\n        self.df = df\n        self.transforms = transform_pipeline \n        self.image_dir = image_dir\n        self.image_paths = df['image_path'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # 1. è·å–ä¿¡æ¯\n        img_path_suffix = self.image_paths[idx]\n        \n        # 2. è¯»å–åŸå§‹å›¾åƒ (2000x1000)\n        filename = os.path.basename(img_path_suffix)\n        full_path = os.path.join(self.image_dir, filename)\n        \n        image = cv2.imread(full_path)\n        if image is None:\n            print(f\"è­¦å‘Š: æ— æ³•è¯»å–å›¾åƒ: {full_path}ã€‚è¿”å›é»‘è‰²å›¾åƒã€‚\")\n            image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 3. è£å‰ªæˆ 2 å¼ å›¾åƒ (å·¦ä¾§å’Œå³ä¾§)\n        height, width, _ = image.shape\n        mid_point = width // 2\n        img_left = image[:, :mid_point]\n        img_right = image[:, mid_point:]\n        \n        # 4. åº”ç”¨ TTA Transform (å¯¹ä¸¤è€…ä½¿ç”¨ç›¸åŒçš„ Transform)\n        img_left_tensor = self.transforms(image=img_left)['image']\n        img_right_tensor = self.transforms(image=img_right)['image']\n        \n        # 5. è¿”å›\n        return img_left_tensor, img_right_tensor\n\n# ===============================================================\n# 4. ğŸ§  æ¨¡å‹æ¶æ„\n# ===============================================================\nclass BiomassModel(nn.Module):\n    \"\"\"\n    æ¨¡å‹æ¶æ„ (åŒæµ, ä¸‰è¾“å‡º)\n    å¿…é¡»ä¸è®­ç»ƒæ–‡ä»¶å®Œå…¨ä¸€è‡´ã€‚\n    \"\"\"\n    def __init__(self, model_name, pretrained, n_targets=3):\n        super(BiomassModel, self).__init__()\n        \n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool='avg'\n        )\n        \n        self.n_features = self.backbone.num_features\n        self.n_combined_features = self.n_features * 2\n        \n        # --- Dry_Total_g çš„è¾“å‡ºå¤´ ---\n        self.head_total = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # --- GDM_g çš„è¾“å‡ºå¤´ ---\n        self.head_gdm = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # --- Dry_Green_g çš„è¾“å‡ºå¤´ ---\n        self.head_green = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n\n    def forward(self, img_left, img_right):\n        features_left = self.backbone(img_left)\n        features_right = self.backbone(img_right)\n        combined = torch.cat([features_left, features_right], dim=1)\n        \n        out_total = self.head_total(combined)\n        out_gdm = self.head_gdm(combined)\n        out_green = self.head_green(combined)\n        \n        return out_total, out_gdm, out_green\n\n\ndef predict_one_view(models_list, test_loader, device):\n    \"\"\"\n    å­å‡½æ•°: ä¸ºå•ä¸ª TTA è§†è§’è¿è¡Œ 5 æŠ˜é›†æˆé¢„æµ‹ã€‚\n    \"\"\"\n    view_preds_3 = {'total': [], 'gdm': [], 'green': []}\n    \n    with torch.no_grad():\n        for (img_left, img_right) in tqdm(test_loader, desc=\"  Predicting View\", leave=False):\n            img_left = img_left.to(device)\n            img_right = img_right.to(device)\n            \n            batch_preds_3_folds = {'total': [], 'gdm': [], 'green': []}\n            \n            # 1. 5 æŠ˜é›†æˆå¾ªç¯\n            for model in models_list:\n                pred_total, pred_gdm, pred_green = model(img_left, img_right)\n                batch_preds_3_folds['total'].append(pred_total.cpu())\n                batch_preds_3_folds['gdm'].append(pred_gdm.cpu())\n                batch_preds_3_folds['green'].append(pred_green.cpu())\n            \n            # 2. è®¡ç®— 5 æŠ˜çš„å¹³å‡å€¼\n            avg_pred_total = torch.mean(torch.stack(batch_preds_3_folds['total']), dim=0)\n            avg_pred_gdm = torch.mean(torch.stack(batch_preds_3_folds['gdm']), dim=0)\n            avg_pred_green = torch.mean(torch.stack(batch_preds_3_folds['green']), dim=0)\n            \n            view_preds_3['total'].append(avg_pred_total.numpy())\n            view_preds_3['gdm'].append(avg_pred_gdm.numpy())\n            view_preds_3['green'].append(avg_pred_green.numpy())\n\n    # 3. åˆå¹¶æ­¤è§†è§’çš„æ‰¹å¤„ç†ç»“æœ\n    preds_np = {\n        'total': np.concatenate(view_preds_3['total']).flatten(),\n        'gdm':   np.concatenate(view_preds_3['gdm']).flatten(),\n        'green': np.concatenate(view_preds_3['green']).flatten()\n    }\n    return preds_np\n\n\ndef run_inference_with_tta():\n    \"\"\"\n    ä¸»æ¨ç†å‡½æ•°, æ‰§è¡Œ TTA x é›†æˆã€‚\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"ğŸš€ å¼€å§‹æ¨ç† (ä½¿ç”¨ TTA) ğŸš€\")\n    print(f\"{'='*50}\")\n\n    # --- 1. åŠ è½½æµ‹è¯•æ•°æ® ---\n    print(f\"æ­£åœ¨åŠ è½½ {CFG.TEST_CSV}...\")\n    try:\n        test_df_long = pd.read_csv(CFG.TEST_CSV)\n        test_df_unique = test_df_long.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n        print(f\"æ‰¾åˆ° {len(test_df_unique)} å¼ å”¯ä¸€çš„æµ‹è¯•å›¾åƒã€‚\")\n    except FileNotFoundError:\n        print(f\"é”™è¯¯: æœªæ‰¾åˆ° {CFG.TEST_CSV}\")\n        return None, None, None\n\n    # --- 2. åŠ è½½ 5 ä¸ªæ¨¡å‹ (é›†æˆ) ---\n    print(\"\\næ­£åœ¨åŠ è½½ 5 ä¸ªå·²è®­ç»ƒçš„æ¨¡å‹...\")\n    models_list = []\n    for fold in range(CFG.N_FOLDS):\n        model_path = os.path.join(CFG.MODEL_DIR, f'best_model_fold{fold}.pth')\n        if not os.path.exists(model_path):\n            print(f\"é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}\")\n            return None, None, None\n        model = BiomassModel(CFG.MODEL_NAME, pretrained=False)\n        try:\n            model.load_state_dict(torch.load(model_path, map_location=CFG.DEVICE))\n        except RuntimeError:\n            state_dict = torch.load(model_path, map_location=CFG.DEVICE)\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k.replace('module.', '')\n                new_state_dict[name] = v\n            model.load_state_dict(new_state_dict)\n        model.eval()\n        model.to(CFG.DEVICE)\n        models_list.append(model)\n    print(f\"âœ“ æˆåŠŸåŠ è½½ {len(models_list)} ä¸ªæ¨¡å‹ã€‚\")\n\n    # --- 3. TTA å¾ªç¯  ---\n    tta_transforms = get_tta_transforms()\n    print(f\"\\nå¼€å§‹ä½¿ç”¨ {len(tta_transforms)} ä¸ª TTA è§†è§’è¿›è¡Œé¢„æµ‹...\")\n    \n    all_tta_view_preds = [] \n\n    for i, tta_transform in enumerate(tta_transforms):\n        print(f\"--- æ­£åœ¨è¿è¡Œ TTA è§†è§’ {i+1}/{len(tta_transforms)} ---\")\n        \n        # ä¸ºæ­¤ TTA è§†è§’åˆ›å»ºæ–°çš„ Dataset/Loader\n        test_dataset = TestBiomassDataset(\n            df=test_df_unique,\n            transform_pipeline=tta_transform,\n            image_dir=CFG.TEST_IMAGE_DIR\n        )\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=CFG.BATCH_SIZE,\n            shuffle=False,\n            num_workers=CFG.NUM_WORKERS,\n            pin_memory=True\n        )\n        \n        # ä¸ºæ­¤è§†è§’è¿è¡Œ 5 æŠ˜é›†æˆ\n        view_preds_np = predict_one_view(models_list, test_loader, CFG.DEVICE)\n        all_tta_view_preds.append(view_preds_np)\n        print(f\"âœ“ å®Œæˆ TTA è§†è§’ {i+1}\")\n\n    # --- 4. é›†æˆ (å¹³å‡) TTA ç»“æœ ---\n    print(\"\\næ­£åœ¨é›†æˆ TTA è§†è§’çš„ç»“æœ...\")\n    final_ensembled_preds = {\n        'total': np.mean([d['total'] for d in all_tta_view_preds], axis=0),\n        'gdm':   np.mean([d['gdm'] for d in all_tta_view_preds], axis=0),\n        'green': np.mean([d['green'] for d in all_tta_view_preds], axis=0)\n    }\n    \n    print(\"âœ“ é¢„æµ‹å®Œæˆã€‚\")\n    \n    del models_list, test_loader, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return final_ensembled_preds, test_df_long, test_df_unique\n# ===============================================================\n# 6. âœï¸ åˆ›å»ºæäº¤æ–‡ä»¶çš„å‡½æ•°\n# ===============================================================\ndef create_submission(preds_np, test_df_long, test_df_unique):\n    \"\"\"\n    æ­¤å‡½æ•°æ¥æ”¶ 3 ä¸ªé›†æˆåçš„é¢„æµ‹ç»“æœï¼Œ\n    è®¡ç®—å‰©ä½™ 2 ä¸ªé¢„æµ‹ç»“æœï¼Œ\n    å¹¶æ ¼å¼åŒ–æäº¤æ–‡ä»¶ã€‚\n    \"\"\"\n    if preds_np is None:\n        print(\"å› ä¸Šè¿°é”™è¯¯ï¼Œè·³è¿‡åˆ›å»ºæäº¤æ–‡ä»¶ã€‚\")\n        return\n\n    print(\"\\næ­£åœ¨åå¤„ç†å¹¶åˆ›å»ºæäº¤æ–‡ä»¶...\")\n\n    # 1. è·å– 3 ä¸ªé›†æˆåçš„é¢„æµ‹ç»“æœ\n    pred_total_final = preds_np['total']\n    pred_gdm_final = preds_np['gdm']\n    pred_green_final = preds_np['green']\n\n    # 2. è®¡ç®—å‰©ä½™ 2 ä¸ªç›®æ ‡ (åå¤„ç†)\n    # ä½¿ç”¨ np.maximum(0, ...) ç¡®ä¿æ²¡æœ‰è´Ÿå€¼\n    pred_clover_final = np.maximum(0, pred_gdm_final - pred_green_final)\n    pred_dead_final = np.maximum(0, pred_total_final - pred_gdm_final)\n\n    # 3. åˆ›å»ºä¸€ä¸ªåŒ…å« 5 ä¸ªé¢„æµ‹ç»“æœçš„â€œå®½â€DataFrame\n    # (ç¡®ä¿ 5 ä¸ªåˆ—çš„é¡ºåºä¸ CFG.ALL_TARGET_COLS ç›¸åŒ)\n    preds_wide_df = pd.DataFrame({\n        'image_path': test_df_unique['image_path'],\n        'Dry_Green_g': pred_green_final,\n        'Dry_Dead_g': pred_dead_final,\n        'Dry_Clover_g': pred_clover_final,\n        'GDM_g': pred_gdm_final,\n        'Dry_Total_g': pred_total_final\n    })\n\n    # 4. â€œé€†é€è§†â€ DataFrame (è½¬æ¢ä¸ºâ€œé•¿â€æ ¼å¼)\n    # å°†å…¶ä» 5 åˆ—è½¬æ¢ä¸ºâ€œé•¿â€æ ¼å¼ (ä¸ sample_submission ç±»ä¼¼)\n    preds_long_df = preds_wide_df.melt(\n        id_vars=['image_path'],\n        value_vars=CFG.ALL_TARGET_COLS, # 5 ä¸ªç›®æ ‡åˆ—\n        var_name='target_name',        # ç›®æ ‡åç§°åˆ—\n        value_name='target'            # é¢„æµ‹å€¼åˆ—\n    )\n\n    # 5. ä¸åŸå§‹ test.csv æ–‡ä»¶ (test_df_long) åˆå¹¶\n    # è¿™æ˜¯è·å–æ­£ç¡® 'sample_id' çš„å…³é”®æ­¥éª¤\n    # (ä¾‹å¦‚: 'ID1001187975__Dry_Clover_g')\n    submission_df = pd.merge(\n        test_df_long[['sample_id', 'image_path', 'target_name']],\n        preds_long_df,\n        on=['image_path', 'target_name'],\n        how='left'\n    )\n\n    # 6. æ¸…ç†å¹¶ä¿å­˜\n    # ä»…ä¿ç•™æ‰€éœ€çš„ 2 ä¸ªåˆ—\n    submission_df = submission_df[['sample_id', 'target']]\n    \n    # ä¿å­˜æ–‡ä»¶\n    submission_df.to_csv(CFG.SUBMISSION_FILE, index=False)\n\n    print(f\"\\nğŸ‰ å®Œæˆ! æäº¤æ–‡ä»¶å·²ä¿å­˜è‡³: {CFG.SUBMISSION_FILE}\")\n    print(\"--- æäº¤æ–‡ä»¶çš„å‰ 5 è¡Œ ---\")\n    print(submission_df.head())\n    print(\"\\n--- æäº¤æ–‡ä»¶çš„å 5 è¡Œ ---\")\n    print(submission_df.tail())\n    \n# ===============================================================\n# 8. ğŸ è¿è¡Œç¨‹åº (å·²ä¿®æ”¹)\n# ===============================================================\nif __name__ == \"__main__\":\n    # 1. è¿è¡Œé¢„æµ‹ (å·²åŒ…å« TTA)\n    all_preds_np, df_long, df_unique = run_inference_with_tta()\n    \n    # 2. åˆ›å»ºæäº¤æ–‡ä»¶ (create_submission å‡½æ•°ä¿æŒä¸å˜)\n    create_submission(all_preds_np, df_long, df_unique)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:37:31.585364Z","iopub.execute_input":"2025-10-30T13:37:31.586001Z","iopub.status.idle":"2025-10-30T13:37:37.021319Z","shell.execute_reply.started":"2025-10-30T13:37:31.585975Z","shell.execute_reply":"2025-10-30T13:37:37.020506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}