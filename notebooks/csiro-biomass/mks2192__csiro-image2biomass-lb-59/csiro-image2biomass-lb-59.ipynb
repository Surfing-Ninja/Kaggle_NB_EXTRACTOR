{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":13569643,"sourceType":"datasetVersion","datasetId":8620094},{"sourceId":272104372,"sourceType":"kernelVersion"},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":4537,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3329,"modelId":986}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Reference - https://www.kaggle.com/code/none00000/lb-0-57-infer-model-code   https://www.kaggle.com/code/carsoncheng/dinov2-lasso-baseline-lb-0-54","metadata":{}},{"cell_type":"code","source":"# ===============================================================\n# CSIRO Image2Biomass â€“ INFERENCE (5-Fold + 4Ã— TTA) â€“ FINAL\n# MODEL_DIR = /kaggle/input/notebook2c4874f566\n# ===============================================================\nimport os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm\n\n# ---------------------------------------------------------------\n# 1. CONFIG\n# ---------------------------------------------------------------\nclass CFG:\n    TEST_IMAGE_DIR = '/kaggle/input/csiro-biomass/test'\n    MODEL_DIR      = '/kaggle/input/csiro-trained-model'\n    SUBMISSION_DIR = '/kaggle/working/'\n\n    MODEL_NAME = 'convnext_tiny'\n    IMG_SIZE   = 512\n    N_FOLDS    = 5\n    BATCH_SIZE = 8\n    NUM_WORKERS = 2\n    TTA_STEPS  = 4\n\n    ALL_TARGET_COLS = ['Dry_Green_g','Dry_Dead_g','Dry_Clover_g','GDM_g','Dry_Total_g']\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Inference on {CFG.DEVICE}\")\nprint(f\"Models from: {CFG.MODEL_DIR}\")\n\n# ---------------------------------------------------------------\n# 2. TTA TRANSFORMS â€“ FIXED\n# ---------------------------------------------------------------\ndef get_tta_transforms():\n    normalize = A.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n    to_tensor = ToTensorV2()\n\n    return [\n        A.Compose([A.Resize(CFG.IMG_SIZE, CFG.IMG_SIZE), normalize, to_tensor]),\n        A.Compose([A.Resize(CFG.IMG_SIZE, CFG.IMG_SIZE), A.HorizontalFlip(p=1.0), normalize, to_tensor]),\n        A.Compose([A.Resize(CFG.IMG_SIZE, CFG.IMG_SIZE), A.VerticalFlip(p=1.0),   normalize, to_tensor]),\n        A.Compose([A.Resize(CFG.IMG_SIZE, CFG.IMG_SIZE), A.HorizontalFlip(p=1.0), A.VerticalFlip(p=1.0), normalize, to_tensor]),\n    ]\n\n# ---------------------------------------------------------------\n# 3. TEST DATASET\n# ---------------------------------------------------------------\nclass BiomassTestDataset(Dataset):\n    def __init__(self, img_dir):\n        self.img_dir = img_dir\n        self.paths = sorted([\n            os.path.join(img_dir, f) for f in os.listdir(img_dir)\n            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n        ])\n        self.filenames = [os.path.basename(p) for p in self.paths]\n\n    def __len__(self): return len(self.paths)\n\n    def __getitem__(self, idx):\n        path = self.paths[idx]\n        img = cv2.imread(path)\n        if img is None:\n            img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        h, w = img.shape[:2]\n        mid = w // 2\n        left  = img[:, :mid].copy()\n        right = img[:, mid:].copy()\n\n        return left, right, self.filenames[idx]\n\n# ---------------------------------------------------------------\n# 4. MODEL\n# ---------------------------------------------------------------\nclass BiomassModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=False, num_classes=0, global_pool='avg')\n        nf = self.backbone.num_features\n        comb = nf * 2\n        head = lambda: nn.Sequential(nn.Linear(comb, comb//2), nn.ReLU(inplace=True),\n                                     nn.Dropout(0.3), nn.Linear(comb//2, 1))\n        self.head_total = head(); self.head_gdm = head(); self.head_green = head()\n\n    def forward(self, left, right):\n        fl = self.backbone(left)\n        fr = self.backbone(right)\n        x = torch.cat([fl, fr], dim=1)\n        return self.head_total(x), self.head_gdm(x), self.head_green(x)\n\n# ---------------------------------------------------------------\n# 5. TTA PREDICTION\n# ---------------------------------------------------------------\n@torch.no_grad()\ndef predict_tta(model, left_np, right_np, tta_tfms):\n    preds = []\n    for tfm in tta_tfms:\n        l = tfm(image=left_np)['image'].unsqueeze(0).to(CFG.DEVICE)\n        r = tfm(image=right_np)['image'].unsqueeze(0).to(CFG.DEVICE)\n        p_tot, p_gdm, p_green = model(l, r)\n        p_tot, p_gdm, p_green = p_tot.item(), p_gdm.item(), p_green.item()\n        p_clover = max(p_gdm - p_green, 0)\n        p_dead   = max(p_tot - p_gdm, 0)\n        preds.append([p_green, p_dead, p_clover, p_gdm, p_tot])\n    return np.mean(preds, axis=0)\n\n# ---------------------------------------------------------------\n# 6. MAIN INFERENCE\n# ---------------------------------------------------------------\ndef run_inference():\n    models = []\n    for fold in range(CFG.N_FOLDS):\n        path = os.path.join(CFG.MODEL_DIR, f'best_model_fold{fold}.pth')\n        if not os.path.exists(path):\n            raise FileNotFoundError(path)\n        m = BiomassModel(CFG.MODEL_NAME)\n        m.load_state_dict(torch.load(path, map_location=CFG.DEVICE))\n        m.eval().to(CFG.DEVICE)\n        models.append(m)\n        print(f\"Loaded fold {fold}\")\n\n    tta_tfms = get_tta_transforms()\n    dataset = BiomassTestDataset(CFG.TEST_IMAGE_DIR)\n\n    loader = DataLoader(\n        dataset,\n        batch_size=CFG.BATCH_SIZE,\n        shuffle=False,\n        num_workers=CFG.NUM_WORKERS,\n        pin_memory=True,\n        collate_fn=lambda x: x\n    )\n\n    all_preds = []\n    all_names = []\n\n    print(f\"Running inference on {len(dataset)} images...\")\n    for batch in tqdm(loader, desc=\"Inference\"):\n        for left_np, right_np, name in batch:\n            fold_preds = [predict_tta(m, left_np, right_np, tta_tfms) for m in models]\n            final_pred = np.mean(fold_preds, axis=0)\n            all_preds.append(final_pred)\n            all_names.append(name)\n\n    sub = pd.DataFrame(all_preds, columns=CFG.ALL_TARGET_COLS)\n    sub.insert(0, 'image_path', all_names)\n    sub = sub.sort_values('image_path').reset_index(drop=True)\n    out_path = os.path.join(CFG.SUBMISSION_DIR, 'submission.csv')\n    sub.to_csv(out_path, index=False)\n    print(f\"\\nSubmission saved: {out_path}\")\n    print(sub.head())\n    return sub\n\n# ---------------------------------------------------------------\n# 7. RUN\n# ---------------------------------------------------------------\nif __name__ == '__main__':\n    submission = run_inference()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:53:17.47118Z","iopub.execute_input":"2025-11-01T12:53:17.471974Z","iopub.status.idle":"2025-11-01T12:53:27.567508Z","shell.execute_reply.started":"2025-11-01T12:53:17.471953Z","shell.execute_reply":"2025-11-01T12:53:27.566751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\nimport os\n\n# ---------------------------------------------------------------\n# CONFIG\n# ---------------------------------------------------------------\nINPUT_SUBMISSION  = '/kaggle/working/submission.csv'   # <-- your current file\nTEST_CSV          = '/kaggle/input/csiro-biomass/test.csv'\nOUTPUT_SUBMISSION = '/kaggle/working/submission1.csv'\n\n# Target column names (must match exactly)\nTARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n\n# ---------------------------------------------------------------\n# 1. Load your predictions (wide format)\n# ---------------------------------------------------------------\nprint(\"Loading your predictions...\")\ndf_wide = pd.read_csv(INPUT_SUBMISSION)\nprint(f\"Loaded {len(df_wide)} rows\")\nprint(df_wide.head())\n\n# ---------------------------------------------------------------\n# 2. Extract image name without .jpg\n# ---------------------------------------------------------------\ndf_wide['image_path'] = df_wide['image_path'].astype(str)\ndf_wide['image_name'] = df_wide['image_path'].str.replace('.jpg', '', regex=False)\n\n# ---------------------------------------------------------------\n# 3. Melt to long format\n# ---------------------------------------------------------------\ndf_long = df_wide.melt(\n    id_vars='image_name',\n    value_vars=TARGET_COLS,\n    var_name='target_name',\n    value_name='target'\n)\n\n# ---------------------------------------------------------------\n# 4. Create sample_id = image_name + __ + target_name\n# ---------------------------------------------------------------\ndf_long['sample_id'] = df_long['image_name'] + '__' + df_long['target_name']\n\n# ---------------------------------------------------------------\n# 5. Final DataFrame\n# ---------------------------------------------------------------\nsubmission = df_long[['sample_id', 'target']].copy()\nsubmission = submission.sort_values('sample_id').reset_index(drop=True)\n\n# ---------------------------------------------------------------\n# 6. Save\n# ---------------------------------------------------------------\nsubmission.to_csv(OUTPUT_SUBMISSION, index=False)\nprint(f\"\\nCompetition submission saved â†’ {OUTPUT_SUBMISSION}\")\nprint(submission.head(10))\n","metadata":{"execution":{"iopub.status.busy":"2025-11-01T12:53:27.569151Z","iopub.execute_input":"2025-11-01T12:53:27.569471Z","iopub.status.idle":"2025-11-01T12:53:27.598494Z","shell.execute_reply.started":"2025-11-01T12:53:27.56945Z","shell.execute_reply":"2025-11-01T12:53:27.597662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model2","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm\nimport cv2\nfrom tqdm import tqdm\nimport gc\n\n# ===============================================================\n# 1. âš™ï¸ CONFIGURATION (PHáº¢I GIá»NG Há»†T FILE TRAINING)\n# ===============================================================\nclass CFG:\n    # --- ÄÆ°á»ng dáº«n (Paths) ---\n    # (HÃ£y Ä‘iá»u chá»‰nh cÃ¡c Ä‘Æ°á»ng dáº«n nÃ y cho Ä‘Ãºng vá»›i mÃ´i trÆ°á»ng cá»§a báº¡n)\n    BASE_PATH = '/kaggle/input/csiro-biomass'\n    TEST_CSV = os.path.join(BASE_PATH, 'test.csv')\n    TEST_IMAGE_DIR = os.path.join(BASE_PATH, 'test')\n    \n    # ThÆ° má»¥c chá»©a 5 file .pth\n    MODEL_DIR = '/kaggle/input/csiro/' # Giáº£ sá»­ 5 file .pth náº±m cÃ¹ng thÆ° má»¥c\n    SUBMISSION_FILE = 'submission.csv'\n    \n    # --- CÃ i Ä‘áº·t MÃ´ hÃ¬nh (PHáº¢I TRÃ™NG KHá»šP) ---\n    MODEL_NAME = 'convnext_tiny' # PHáº¢I GIá»NG Há»†T LÃšC TRAIN\n    IMG_SIZE = 768               # PHáº¢I GIá»NG Há»†T LÃšC TRAIN\n    \n    # --- CÃ i Ä‘áº·t Inference ---\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    BATCH_SIZE = 1 # CÃ³ thá»ƒ tÄƒng batch size khi inference\n    NUM_WORKERS = 1\n    N_FOLDS = 5\n    \n    # --- Má»¥c tiÃªu & Loss (PHáº¢I TRÃ™NG KHá»šP) ---\n    # 3 má»¥c tiÃªu model Ä‘Ã£ dá»± Ä‘oÃ¡n\n    TARGET_COLS = ['Dry_Total_g', 'GDM_g', 'Dry_Green_g']\n    \n    # 5 má»¥c tiÃªu Ä‘á»ƒ ná»™p bÃ i\n    ALL_TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n\nprint(f\"Sá»­ dá»¥ng thiáº¿t bá»‹: {CFG.DEVICE}\")\nprint(f\"Backbone mÃ´ hÃ¬nh: {CFG.MODEL_NAME}\")\nprint(f\"KÃ­ch thÆ°á»›c áº£nh inference: {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")\n\n\n# ===============================================================\n# 2. ðŸžï¸ AUGMENTATIONS (CHá»ˆ DÃ™NG VALIDATION)\n# ===============================================================\nfrom albumentations import (\n    Compose, \n    Resize, \n    Normalize,\n    HorizontalFlip, \n    VerticalFlip\n)\n\ndef get_tta_transforms():\n    \"\"\"\n    Tráº£ vá» má»™t LIST cÃ¡c pipeline transform cho TTA.\n    Má»—i pipeline lÃ  má»™t \"view\" khÃ¡c nhau cá»§a áº£nh.\n    \"\"\"\n    \n    # ÄÃ¢y lÃ  cÃ¡c bÆ°á»›c chuáº©n hÃ³a cÆ¡ báº£n\n    base_transforms = [\n        Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        ),\n        ToTensorV2()\n    ]\n    \n    # -----------------\n    # View 1: áº¢nh gá»‘c (Chá»‰ Resize + Normalize)\n    # -----------------\n    original_view = Compose([\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # -----------------\n    # View 2: Láº­t ngang (HFlip)\n    # -----------------\n    hflip_view = Compose([\n        HorizontalFlip(p=1.0), # LuÃ´n luÃ´n láº­t\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # -----------------\n    # View 3: Láº­t dá»c (VFlip)\n    # -----------------\n    vflip_view = Compose([\n        VerticalFlip(p=1.0), # LuÃ´n luÃ´n láº­t\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    return [original_view, hflip_view, vflip_view]\n\nprint(\"ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m get_tta_transforms().\")\n\n\nclass TestBiomassDataset(Dataset):\n    \"\"\"\n    Dataset tÃ¹y chá»‰nh cho áº£nh test (Chiáº¿n lÆ°á»£c \"Hai luá»“ng\").\n    Sá»­a Ä‘á»•i Ä‘á»ƒ cháº¥p nháº­n má»™t pipeline transform cá»¥ thá»ƒ cho TTA.\n    \"\"\"\n    def __init__(self, df, transform_pipeline, image_dir):\n        self.df = df\n        # (Sá»¬A Äá»”I) Cháº¥p nháº­n má»™t pipeline Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o\n        self.transforms = transform_pipeline \n        self.image_dir = image_dir\n        self.image_paths = df['image_path'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # 1. Láº¥y thÃ´ng tin\n        img_path_suffix = self.image_paths[idx]\n        \n        # 2. Äá»c áº£nh gá»‘c (2000x1000)\n        filename = os.path.basename(img_path_suffix)\n        full_path = os.path.join(self.image_dir, filename)\n        \n        image = cv2.imread(full_path)\n        if image is None:\n            print(f\"Warning: KhÃ´ng thá»ƒ Ä‘á»c áº£nh: {full_path}. Tráº£ vá» áº£nh Ä‘en.\")\n            image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 3. Cáº¯t (Crop) thÃ nh 2 áº£nh (TrÃ¡i vÃ  Pháº£i)\n        height, width, _ = image.shape\n        mid_point = width // 2\n        img_left = image[:, :mid_point]\n        img_right = image[:, mid_point:]\n        \n        # 4. Ãp dá»¥ng TTA Transform (CÃ™NG Má»˜T TRANSFORM cho cáº£ 2)\n        # (VÃ­ dá»¥: Cáº£ 2 áº£nh Ä‘á»u bá»‹ láº­t ngang)\n        img_left_tensor = self.transforms(image=img_left)['image']\n        img_right_tensor = self.transforms(image=img_right)['image']\n        \n        # 5. Tráº£ vá»\n        return img_left_tensor, img_right_tensor\n\n# ===============================================================\n# 4. ðŸ§  MODEL ARCHITECTURE (SAO CHÃ‰P Tá»ª FILE TRAIN)\n# ===============================================================\nclass BiomassModel(nn.Module):\n    \"\"\"\n    Kiáº¿n trÃºc mÃ´ hÃ¬nh (Hai luá»“ng, Ba Ä‘áº§u ra)\n    PHáº¢I GIá»NG Há»†T file training.\n    \"\"\"\n    def __init__(self, model_name, pretrained, n_targets=3):\n        super(BiomassModel, self).__init__()\n        \n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained, # Sáº½ lÃ  False khi inference\n            num_classes=0,\n            global_pool='avg'\n        )\n        \n        self.n_features = self.backbone.num_features\n        self.n_combined_features = self.n_features * 2\n        \n        # --- Äáº§u cho Dry_Total_g ---\n        self.head_total = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # --- Äáº§u cho GDM_g ---\n        self.head_gdm = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # --- Äáº§u cho Dry_Green_g ---\n        self.head_green = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n\n    def forward(self, img_left, img_right):\n        features_left = self.backbone(img_left)\n        features_right = self.backbone(img_right)\n        combined = torch.cat([features_left, features_right], dim=1)\n        \n        out_total = self.head_total(combined)\n        out_gdm = self.head_gdm(combined)\n        out_green = self.head_green(combined)\n        \n        return out_total, out_gdm, out_green\n\n\ndef predict_one_view(models_list, test_loader, device):\n    \"\"\"\n    HÃ m con: Cháº¡y dá»± Ä‘oÃ¡n ensemble 5-fold cho Má»˜T view TTA.\n    \"\"\"\n    view_preds_3 = {'total': [], 'gdm': [], 'green': []}\n    \n    with torch.no_grad():\n        for (img_left, img_right) in tqdm(test_loader, desc=\"  Predicting View\", leave=False):\n            img_left = img_left.to(device)\n            img_right = img_right.to(device)\n            \n            batch_preds_3_folds = {'total': [], 'gdm': [], 'green': []}\n            \n            # 1. VÃ²ng láº·p Ensemble 5-Fold\n            for model in models_list:\n                pred_total, pred_gdm, pred_green = model(img_left, img_right)\n                batch_preds_3_folds['total'].append(pred_total.cpu())\n                batch_preds_3_folds['gdm'].append(pred_gdm.cpu())\n                batch_preds_3_folds['green'].append(pred_green.cpu())\n            \n            # 2. Láº¥y trung bÃ¬nh 5 Fold\n            avg_pred_total = torch.mean(torch.stack(batch_preds_3_folds['total']), dim=0)\n            avg_pred_gdm = torch.mean(torch.stack(batch_preds_3_folds['gdm']), dim=0)\n            avg_pred_green = torch.mean(torch.stack(batch_preds_3_folds['green']), dim=0)\n            \n            view_preds_3['total'].append(avg_pred_total.numpy())\n            view_preds_3['gdm'].append(avg_pred_gdm.numpy())\n            view_preds_3['green'].append(avg_pred_green.numpy())\n\n    # 3. GhÃ©p káº¿t quáº£ cÃ¡c batch cá»§a view nÃ y\n    preds_np = {\n        'total': np.concatenate(view_preds_3['total']).flatten(),\n        'gdm':   np.concatenate(view_preds_3['gdm']).flatten(),\n        'green': np.concatenate(view_preds_3['green']).flatten()\n    }\n    return preds_np\n\n\ndef run_inference_with_tta():\n    \"\"\"\n    HÃ m inference chÃ­nh, thá»±c hiá»‡n TTA x Ensemble.\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"ðŸš€ Báº®T Äáº¦U INFERENCE (vá»›i TTA) ðŸš€\")\n    print(f\"{'='*50}\")\n\n    # --- 1. Táº£i Dá»¯ liá»‡u Test ---\n    print(f\"Äang táº£i {CFG.TEST_CSV}...\")\n    try:\n        test_df_long = pd.read_csv(CFG.TEST_CSV)\n        test_df_unique = test_df_long.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n        print(f\"TÃ¬m tháº¥y {len(test_df_unique)} áº£nh test duy nháº¥t.\")\n    except FileNotFoundError:\n        print(f\"Lá»–I: KhÃ´ng tÃ¬m tháº¥y {CFG.TEST_CSV}\")\n        return None, None, None\n\n    # --- 2. Táº£i 5 MÃ´ hÃ¬nh (Ensemble) ---\n    print(\"\\nÄang táº£i 5 mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n...\")\n    models_list = []\n    # (Code táº£i 5 mÃ´ hÃ¬nh... giá»‘ng há»‡t bÆ°á»›c 16 cá»§a file trÆ°á»›c)\n    for fold in range(CFG.N_FOLDS):\n        model_path = os.path.join(CFG.MODEL_DIR, f'best_model_fold{fold}.pth')\n        if not os.path.exists(model_path):\n            print(f\"Lá»–I: KhÃ´ng tÃ¬m tháº¥y file mÃ´ hÃ¬nh: {model_path}\")\n            return None, None, None\n        model = BiomassModel(CFG.MODEL_NAME, pretrained=False)\n        try:\n            model.load_state_dict(torch.load(model_path, map_location=CFG.DEVICE))\n        except RuntimeError:\n            state_dict = torch.load(model_path, map_location=CFG.DEVICE)\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k.replace('module.', '')\n                new_state_dict[name] = v\n            model.load_state_dict(new_state_dict)\n        model.eval()\n        model.to(CFG.DEVICE)\n        models_list.append(model)\n    print(f\"âœ“ ÄÃ£ táº£i thÃ nh cÃ´ng {len(models_list)} mÃ´ hÃ¬nh.\")\n\n    # --- 3. VÃ²ng láº·p TTA (VÃ²ng láº·p ngoÃ i) ---\n    tta_transforms = get_tta_transforms()\n    print(f\"\\nBáº¯t Ä‘áº§u dá»± Ä‘oÃ¡n vá»›i {len(tta_transforms)} TTA views...\")\n    \n    all_tta_view_preds = [] # List Ä‘á»ƒ lÆ°u káº¿t quáº£ cá»§a má»—i view TTA\n\n    for i, tta_transform in enumerate(tta_transforms):\n        print(f\"--- Äang cháº¡y TTA View {i+1}/{len(tta_transforms)} ---\")\n        \n        # Táº¡o Dataset/Loader Má»šI cho view TTA nÃ y\n        test_dataset = TestBiomassDataset(\n            df=test_df_unique,\n            transform_pipeline=tta_transform, # Truyá»n pipeline TTA\n            image_dir=CFG.TEST_IMAGE_DIR\n        )\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=CFG.BATCH_SIZE,\n            shuffle=False,\n            num_workers=CFG.NUM_WORKERS,\n            pin_memory=True\n        )\n        \n        # Cháº¡y ensemble 5-fold cho view nÃ y\n        view_preds_np = predict_one_view(models_list, test_loader, CFG.DEVICE)\n        all_tta_view_preds.append(view_preds_np)\n        print(f\"âœ“ HoÃ n thÃ nh TTA View {i+1}\")\n\n    # --- 4. Ensemble (Láº¥y trung bÃ¬nh) káº¿t quáº£ TTA ---\n    print(\"\\nÄang ensemble káº¿t quáº£ cá»§a cÃ¡c TTA views...\")\n    final_ensembled_preds = {\n        'total': np.mean([d['total'] for d in all_tta_view_preds], axis=0),\n        'gdm':   np.mean([d['gdm'] for d in all_tta_view_preds], axis=0),\n        'green': np.mean([d['green'] for d in all_tta_view_preds], axis=0)\n    }\n    \n    print(\"âœ“ Dá»± Ä‘oÃ¡n hoÃ n táº¥t.\")\n    \n    del models_list, test_loader, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return final_ensembled_preds, test_df_long, test_df_unique\n# ===============================================================\n# 6. âœï¸ HÃ€M Táº O FILE SUBMISSION\n# ===============================================================\ndef create_submission(preds_np, test_df_long, test_df_unique):\n    \"\"\"\n    HÃ m nÃ y nháº­n 3 dá»± Ä‘oÃ¡n Ä‘Ã£ ensemble,\n    tÃ­nh toÃ¡n 2 dá»± Ä‘oÃ¡n cÃ²n láº¡i,\n    vÃ  Ä‘á»‹nh dáº¡ng file ná»™p bÃ i.\n    \"\"\"\n    if preds_np is None:\n        print(\"Bá» qua táº¡o submission do lá»—i á»Ÿ trÃªn.\")\n        return\n\n    print(\"\\nÄang háº­u xá»­ lÃ½ vÃ  táº¡o file submission...\")\n\n    # 1. Láº¥y 3 dá»± Ä‘oÃ¡n Ä‘Ã£ ensemble\n    pred_total_final = preds_np['total']\n    pred_gdm_final = preds_np['gdm']\n    pred_green_final = preds_np['green']\n\n    # 2. TÃ­nh 2 má»¥c tiÃªu cÃ²n láº¡i (Háº­u xá»­ lÃ½)\n    # DÃ¹ng np.maximum(0, ...) Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng cÃ³ giÃ¡ trá»‹ Ã¢m\n    pred_clover_final = np.maximum(0, pred_gdm_final - pred_green_final)\n    pred_dead_final = np.maximum(0, pred_total_final - pred_gdm_final)\n\n    # 3. Táº¡o má»™t DataFrame \"wide\" chá»©a 5 dá»± Ä‘oÃ¡n\n    # (Äáº£m báº£o thá»© tá»± 5 cá»™t giá»‘ng CFG.ALL_TARGET_COLS)\n    preds_wide_df = pd.DataFrame({\n        'image_path': test_df_unique['image_path'],\n        'Dry_Green_g': pred_green_final,\n        'Dry_Dead_g': pred_dead_final,\n        'Dry_Clover_g': pred_clover_final,\n        'GDM_g': pred_gdm_final,\n        'Dry_Total_g': pred_total_final\n    })\n\n    # 4. \"Un-pivot\" DataFrame (Chuyá»ƒn sang dáº¡ng \"long\")\n    # Biáº¿n nÃ³ tá»« 5 cá»™t vá» dáº¡ng \"long\" (giá»‘ng sample_submission)\n    preds_long_df = preds_wide_df.melt(\n        id_vars=['image_path'],\n        value_vars=CFG.ALL_TARGET_COLS, # 5 cá»™t má»¥c tiÃªu\n        var_name='target_name',        # Cá»™t tÃªn má»¥c tiÃªu\n        value_name='target'            # Cá»™t giÃ¡ trá»‹ dá»± Ä‘oÃ¡n\n    )\n\n    # 5. Merge vá»›i file test.csv gá»‘c (test_df_long)\n    # ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng Ä‘á»ƒ láº¥y Ä‘Ãºng 'sample_id'\n    # (vÃ­ dá»¥: 'ID1001187975__Dry_Clover_g')\n    submission_df = pd.merge(\n        test_df_long[['sample_id', 'image_path', 'target_name']],\n        preds_long_df,\n        on=['image_path', 'target_name'],\n        how='left'\n    )\n\n    # 6. Dá»n dáº¹p vÃ  LÆ°u\n    # Chá»‰ giá»¯ láº¡i 2 cá»™t Ä‘Æ°á»£c yÃªu cáº§u\n    submission_df = submission_df[['sample_id', 'target']]\n    \n    # LÆ°u file\n    submission_df.to_csv(CFG.SUBMISSION_FILE, index=False)\n\n    print(f\"\\nðŸŽ‰ HOÃ€N Táº¤T! ÄÃ£ lÆ°u file submission táº¡i: {CFG.SUBMISSION_FILE}\")\n    print(\"--- 5 hÃ ng Ä‘áº§u cá»§a file submission ---\")\n    print(submission_df.head())\n    print(\"\\n--- 5 hÃ ng cuá»‘i cá»§a file submission ---\")\n    print(submission_df.tail())\n    \n# ===============================================================\n# 8. ðŸ CHáº Y CHÆ¯Æ NG TRÃŒNH (ÄÃ£ sá»­a)\n# ===============================================================\nif __name__ == \"__main__\":\n    # 1. Cháº¡y dá»± Ä‘oÃ¡n (Ä‘Ã£ bao gá»“m TTA)\n    all_preds_np, df_long, df_unique = run_inference_with_tta()\n    \n    # 2. Táº¡o file submission (HÃ m create_submission giá»¯ nguyÃªn)\n    create_submission(all_preds_np, df_long, df_unique)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:53:27.599622Z","iopub.execute_input":"2025-11-01T12:53:27.599907Z","iopub.status.idle":"2025-11-01T12:53:33.001032Z","shell.execute_reply.started":"2025-11-01T12:53:27.599883Z","shell.execute_reply":"2025-11-01T12:53:33.000374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model 3","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torchvision\nimport os\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset, Dataset\nfrom PIL import Image\n!cp -r \"/kaggle/input/rsna-models/facebookresearch_dinov2_main (1)/root/.cache/torch/hub/facebookresearch_dinov2_main\" /kaggle/working/dinov2\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoImageProcessor, AutoModel\nprocessor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1/')\nmodel = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1/')\nmodel = model.cuda()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeds = []\ntargets = [[] for i in range(5)]\ncounter = 0\nimport torchvision.transforms as transforms\nimport pandas as pd\nfrom PIL import Image\n#transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), transforms.Normalize(mean, std)])\ntrain_df = pd.read_csv(\"/kaggle/input/csiro-biomass/train.csv\")\nroot = \"/kaggle/input/csiro-biomass/\"\nfor i in range(len(train_df)):\n    entry = train_df.iloc[i]\n    file_path = root + entry['image_path']\n    y = torch.tensor([[entry['target']]])\n    targets[i % 5].append(y)\n    if i % 5 == 0:\n        img = Image.open(file_path)\n        x = torch.tensor(processor(img).pixel_values)\n        with torch.no_grad():\n            x = x.cuda()\n            embeds.append(model(x).pooler_output.cpu())\n            counter += 1\n            if counter % 100 == 0:\n                print(f\"{counter} batches processed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\n\n# Create indices and shuffle once\nlst = list(range(len(embeds)))\nrandom.seed(42)\nrandom.shuffle(lst)\n\n# Create multiple random 80/20 splits\nn_splits = 5\nsplits = []\n\nfor i in range(n_splits):\n    # Reshuffle for each split while maintaining same splits across targets\n    temp_lst = lst.copy()\n    random.seed(42 + i)  # Different seed for each split\n    random.shuffle(temp_lst)\n    \n    split_point = int(len(temp_lst) * 0.8)\n    train_idxs = temp_lst[:split_point]\n    val_idxs = temp_lst[split_point:]\n    splits.append((train_idxs, val_idxs))\n\n# Convert embeds to numpy array once for efficiency\nembeds_np = np.array(torch.cat(embeds))\nregressors = [[None for i in range(5)] for j in range(5)]\n# Now iterate through each target\nfor i in range(5):\n    print(f\"\\n=== Target {i+1} ===\")\n    targets_np = np.array(torch.cat(targets[i]))\n    \n    split_scores = []\n    \n    for split_idx, (train_idxs, val_idxs) in enumerate(splits):\n        print(f\"Fold {split_idx+1}:\")\n        X_train, y_train = embeds_np[train_idxs], targets_np[train_idxs]\n        X_val, y_val = embeds_np[val_idxs], targets_np[val_idxs]\n        reg = Lasso()\n        reg.fit(X_train, y_train)\n        train_preds = reg.predict(X_train)\n        train_preds[train_preds < 0.0] = 0.0\n        train_r2 = r2_score(y_train, train_preds)\n        val_preds = reg.predict(X_val)\n        val_preds[val_preds < 0.0] = 0.0\n        val_r2 = r2_score(y_val, val_preds)\n        print(f\"  Train RÂ²: {train_r2:.4f}\")\n        print(f\"  Val RÂ²: {val_r2:.4f}\")\n        split_scores.append((train_r2, val_r2))\n        regressors[i][split_idx] = reg\n    \n    # Print summary for this target\n    avg_train_r2 = np.mean([score[0] for score in split_scores])\n    avg_val_r2 = np.mean([score[1] for score in split_scores])\n    print(f\"\\nTarget {i+1} Average:\")\n    print(f\"  Avg Train RÂ²: {avg_train_r2:.4f}\")\n    print(f\"  Avg Val RÂ²: {avg_val_r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mapping = {\"Dry_Clover_g\": 0, \"Dry_Dead_g\": 1, \"Dry_Green_g\": 2, \"Dry_Total_g\": 3, \"GDM_g\": 4}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_embeds = {}\ncounter = 0\nimport torchvision.transforms as transforms\nimport pandas as pd\nfrom PIL import Image\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), transforms.Normalize(mean, std)])\ntest_df = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")\nroot = \"/kaggle/input/csiro-biomass/\"\nsample_ids = []\nfor i in range(len(test_df)):\n    entry = test_df.iloc[i]\n    file_path = root + entry['image_path']\n    sample_id = entry['sample_id']\n    #y = torch.tensor([[entry['target']]])\n    if sample_id not in sample_ids:\n        img = Image.open(file_path)\n        x = torch.tensor(processor(img).pixel_values)\n        with torch.no_grad():\n            x = x.cuda()\n            test_embeds[sample_id.split(\"_\")[0]] = model(x).pooler_output.cpu()\n            counter += 1\n        sample_ids.append(sample_id)\n    if counter % 100 == 0:\n        print(f\"{counter} batches processed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = []\nsample_ids = []\ntest_df = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")\nfor i in range(len(test_df)):\n    try:\n        entry = test_df.iloc[i]\n        X = np.array(test_embeds[entry['sample_id'].split(\"__\")[0]])\n        sample_ids.append(entry['sample_id'])\n        models = regressors[mapping[entry['sample_id'].split(\"__\")[1]]]\n        prediction = 0.0\n        for item in models:\n            single_pred = item.predict(X)\n            if single_pred < 0.0:\n                single_pred = 0.0\n            prediction += single_pred\n        prediction = prediction / 5\n        predictions.append(float(prediction))\n    except Exception as e:\n        predictions.append(0.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working\nsubmission = pd.DataFrame({\n    'sample_id': sample_ids,\n    'target': predictions\n})\n\nsubmission.to_csv('submission2.csv', index=False)\nsubmission\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MODEL","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torchvision\nimport os\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset, Dataset\nfrom PIL import Image\n!cp -r \"/kaggle/input/rsna-models/facebookresearch_dinov2_main (1)/root/.cache/torch/hub/facebookresearch_dinov2_main\" /kaggle/working/dinov2\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoImageProcessor, AutoModel\nprocessor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/giant/1')\nmodel = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/giant/1')\nmodel = model.cuda()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeds = []\ntargets = [[] for i in range(5)]\ncounter = 0\nimport torchvision.transforms as transforms\nimport pandas as pd\nfrom PIL import Image\n#transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), transforms.Normalize(mean, std)])\ntrain_df = pd.read_csv(\"/kaggle/input/csiro-biomass/train.csv\")\nroot = \"/kaggle/input/csiro-biomass/\"\nfor i in range(len(train_df)):\n    entry = train_df.iloc[i]\n    file_path = root + entry['image_path']\n    y = torch.tensor([[entry['target']]])\n    targets[i % 5].append(y)\n    if i % 5 == 0:\n        img = Image.open(file_path)\n        x = torch.tensor(processor(img).pixel_values)\n        with torch.no_grad():\n            x = x.cuda()\n            embeds.append(model(x).pooler_output.cpu())\n            counter += 1\n            if counter % 100 == 0:\n                print(f\"{counter} batches processed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\n\n# Create indices and shuffle once\nlst = list(range(len(embeds)))\nrandom.seed(42)\nrandom.shuffle(lst)\n\n# Create multiple random 80/20 splits\nn_splits = 5\nsplits = []\n\nfor i in range(n_splits):\n    # Reshuffle for each split while maintaining same splits across targets\n    temp_lst = lst.copy()\n    random.seed(42 + i)  # Different seed for each split\n    random.shuffle(temp_lst)\n    \n    split_point = int(len(temp_lst) * 0.8)\n    train_idxs = temp_lst[:split_point]\n    val_idxs = temp_lst[split_point:]\n    splits.append((train_idxs, val_idxs))\n\n# Convert embeds to numpy array once for efficiency\nembeds_np = np.array(torch.cat(embeds))\nregressors = [[None for i in range(5)] for j in range(5)]\n# Now iterate through each target\nfor i in range(5):\n    print(f\"\\n=== Target {i+1} ===\")\n    targets_np = np.array(torch.cat(targets[i]))\n    \n    split_scores = []\n    \n    for split_idx, (train_idxs, val_idxs) in enumerate(splits):\n        print(f\"Fold {split_idx+1}:\")\n        X_train, y_train = embeds_np[train_idxs], targets_np[train_idxs]\n        X_val, y_val = embeds_np[val_idxs], targets_np[val_idxs]\n        reg = Lasso()\n        reg.fit(X_train, y_train)\n        train_preds = reg.predict(X_train)\n        train_preds[train_preds < 0.0] = 0.0\n        train_r2 = r2_score(y_train, train_preds)\n        val_preds = reg.predict(X_val)\n        val_preds[val_preds < 0.0] = 0.0\n        val_r2 = r2_score(y_val, val_preds)\n        print(f\"  Train RÂ²: {train_r2:.4f}\")\n        print(f\"  Val RÂ²: {val_r2:.4f}\")\n        split_scores.append((train_r2, val_r2))\n        regressors[i][split_idx] = reg\n    \n    # Print summary for this target\n    avg_train_r2 = np.mean([score[0] for score in split_scores])\n    avg_val_r2 = np.mean([score[1] for score in split_scores])\n    print(f\"\\nTarget {i+1} Average:\")\n    print(f\"  Avg Train RÂ²: {avg_train_r2:.4f}\")\n    print(f\"  Avg Val RÂ²: {avg_val_r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mapping = {\"Dry_Clover_g\": 0, \"Dry_Dead_g\": 1, \"Dry_Green_g\": 2, \"Dry_Total_g\": 3, \"GDM_g\": 4}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_embeds = {}\ncounter = 0\nimport torchvision.transforms as transforms\nimport pandas as pd\nfrom PIL import Image\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), transforms.Normalize(mean, std)])\ntest_df = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")\nroot = \"/kaggle/input/csiro-biomass/\"\nsample_ids = []\nfor i in range(len(test_df)):\n    entry = test_df.iloc[i]\n    file_path = root + entry['image_path']\n    sample_id = entry['sample_id']\n    #y = torch.tensor([[entry['target']]])\n    if sample_id not in sample_ids:\n        img = Image.open(file_path)\n        x = torch.tensor(processor(img).pixel_values)\n        with torch.no_grad():\n            x = x.cuda()\n            test_embeds[sample_id.split(\"_\")[0]] = model(x).pooler_output.cpu()\n            counter += 1\n        sample_ids.append(sample_id)\n    if counter % 100 == 0:\n        print(f\"{counter} batches processed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = []\nsample_ids = []\ntest_df = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")\nfor i in range(len(test_df)):\n    try:\n        entry = test_df.iloc[i]\n        X = np.array(test_embeds[entry['sample_id'].split(\"__\")[0]])\n        sample_ids.append(entry['sample_id'])\n        models = regressors[mapping[entry['sample_id'].split(\"__\")[1]]]\n        prediction = 0.0\n        for item in models:\n            single_pred = item.predict(X)\n            if single_pred < 0.0:\n                single_pred = 0.0\n            prediction += single_pred\n        prediction = prediction / 5\n        predictions.append(float(prediction))\n    except Exception as e:\n        predictions.append(0.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working\nsubmission = pd.DataFrame({\n    'sample_id': sample_ids,\n    'target': predictions\n})\n\nsubmission.to_csv('submission_giant.csv', index=False)\nsubmission\ndf1 = pd.read_csv(\"/kaggle/working/submission1.csv\").set_index(\"sample_id\")\ndf2 = pd.read_csv(\"/kaggle/working/submission2.csv\").set_index(\"sample_id\")\ndf3 = pd.read_csv(\"/kaggle/working/submission.csv\").set_index(\"sample_id\")\ndf4 = pd.read_csv(\"/kaggle/working/submission_giant.csv\").set_index(\"sample_id\")\ndf = .65*(.01*df1 + .59*df2 + .4*df3) + .35*df4\ndf.to_csv(\"submission.csv\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}