{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm  # Th∆∞ vi·ªán tuy·ªát v·ªùi cho c√°c backbone\nimport cv2  # OpenCV ƒë·ªÉ ƒë·ªçc ·∫£nh\nfrom tqdm import tqdm # Thanh ti·∫øn tr√¨nh\nimport matplotlib.pyplot as plt\n\n# --- L·ªöP C·∫§U H√åNH TRUNG T√ÇM ---\n# Qu·∫£n l√Ω t·∫•t c·∫£ c√°c si√™u tham s·ªë (hyperparameters) t·∫°i ƒë√¢y\nclass CFG:\n    # --- ƒê∆∞·ªùng d·∫´n (Paths) ---\n    # (H√£y ƒëi·ªÅu ch·ªânh c√°c ƒë∆∞·ªùng d·∫´n n√†y cho ƒë√∫ng v·ªõi m√¥i tr∆∞·ªùng c·ªßa b·∫°n)\n    BASE_PATH = '/kaggle/input/csiro-biomass'\n    TRAIN_CSV = os.path.join(BASE_PATH, 'train.csv')\n    IMAGE_DIR = os.path.join(BASE_PATH, 'train')\n    \n    # --- C√†i ƒë·∫∑t M√¥ h√¨nh ---\n    MODEL_NAME = 'convnext_tiny' # B·∫°n c√≥ th·ªÉ ƒë·ªïi sang 'resnet50'\n    PRETRAINED = True\n    IMG_SIZE = 768 # K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o (t·ª´ 1000x1000 n√©n xu·ªëng)\n    \n    # --- C√†i ƒë·∫∑t Hu·∫•n luy·ªán ---\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    BATCH_SIZE = 2   # B·∫Øt ƒë·∫ßu v·ªõi s·ªë nh·ªè (v√¨ 2 lu·ªìng ·∫£nh 768x768 s·∫Ω t·ªën VRAM)\n    EPOCHS = 30\n    LEARNING_RATE = 1e-4      # LR cho Giai ƒëo·∫°n 1 (hu·∫•n luy·ªán heads)\n    \n    # --- C√†i ƒë·∫∑t Giai ƒëo·∫°n 2 (Fine-tuning) ---\n    FREEZE_EPOCHS = 15         # S·ªë epochs ch·ªâ hu·∫•n luy·ªán 'heads'\n    FINETUNE_LR = 1e-5        # LR th·∫•p h∆°n cho Giai ƒëo·∫°n 2 (to√†n b·ªô m√¥ h√¨nh)\n    \n    NUM_WORKERS = 2  # S·ªë lu·ªìng t·∫£i d·ªØ li·ªáu\n    \n    # --- M·ª•c ti√™u & Loss ---\n    TARGET_COLS = ['Dry_Total_g', 'GDM_g', 'Dry_Green_g']\n    LOSS_WEIGHTS = {\n        'total_loss': 0.5,\n        'gdm_loss': 0.2,\n        'green_loss': 0.1\n    }\n    # 5 M·ª§C TI√äU ƒê·ªÇ T√çNH ƒêI·ªÇM (theo ƒë√∫ng th·ª© t·ª± c·ªßa cu·ªôc thi)\n    ALL_TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n    \n    # Tr·ªçng s·ªë R2 (theo th·ª© t·ª± c·ªßa ALL_TARGET_COLS)\n    R2_WEIGHTS = [0.1, 0.1, 0.1, 0.2, 0.5]\n\n# --- In ra ƒë·ªÉ x√°c nh·∫≠n ---\nprint(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {CFG.DEVICE}\")\nprint(f\"Backbone m√¥ h√¨nh: {CFG.MODEL_NAME}\")\nprint(f\"K√≠ch th∆∞·ªõc ·∫£nh hu·∫•n luy·ªán: {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:23.477943Z","iopub.execute_input":"2025-10-30T11:32:23.47816Z","iopub.status.idle":"2025-10-30T11:32:28.935116Z","shell.execute_reply.started":"2025-10-30T11:32:23.478127Z","shell.execute_reply":"2025-10-30T11:32:28.934442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- T·∫£i d·ªØ li·ªáu v√† Pivot ---\n\nprint(f\"ƒêang t·∫£i {CFG.TRAIN_CSV}...\")\ntry:\n    # 1. ƒê·ªçc file CSV g·ªëc (d·∫°ng long)\n    df_long = pd.read_csv(CFG.TRAIN_CSV)\n    print(f\"B·∫£ng 'long' g·ªëc c√≥ {len(df_long)} h√†ng.\")\n    \n    # 2. Th·ª±c hi·ªán Pivot\n    # Ch√∫ng ta d√πng 'image_path' l√†m ch·ªâ m·ª•c (index) duy nh·∫•t\n    # Xoay c·ªôt 'target_name' th√†nh c√°c c·ªôt m·ªõi\n    # L·∫•y gi√° tr·ªã t·ª´ c·ªôt 'target'\n    df_wide = df_long.pivot(\n        index='image_path',\n        columns='target_name',\n        values='target'\n    )\n    \n    # 3. D·ªçn d·∫πp DataFrame\n    # Sau khi pivot, 'image_path' tr·ªü th√†nh index.\n    # 'reset_index()' s·∫Ω bi·∫øn n√≥ tr·ªü l·∫°i th√†nh m·ªôt c·ªôt b√¨nh th∆∞·ªùng.\n    df_wide = df_wide.reset_index()\n    df_wide.columns.name = None # X√≥a t√™n 'target_name' kh·ªèi tr·ª•c c·ªôt\n    \n    print(f\"ƒê√£ pivot! B·∫£ng 'wide' m·ªõi c√≥ {len(df_wide)} h√†ng (m·ªói h√†ng 1 ·∫£nh).\")\n    \n    # 4. Hi·ªÉn th·ªã 5 h√†ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng 'wide' m·ªõi\n    print(\"\\n--- 5 h√†ng ƒë·∫ßu c·ªßa df_wide ---\")\n    print(df_wide.head())\n    \n    # 5. (T√πy ch·ªçn) Ki·ªÉm tra xem c√°c c·ªôt m·ª•c ti√™u c·ªßa ch√∫ng ta c√≥ ·ªü ƒë√≥ kh√¥ng\n    print(\"\\n--- C√°c c·ªôt c√≥ trong df_wide ---\")\n    print(df_wide.columns.tolist())\n    \nexcept FileNotFoundError:\n    print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file {CFG.TRAIN_CSV}\")\n    print(\"Vui l√≤ng ki·ªÉm tra l·∫°i CFG.TRAIN_CSV\")\n    # Gi·∫£ l·∫≠p df_wide n·∫øu kh√¥ng t√¨m th·∫•y file ƒë·ªÉ c√°c b∆∞·ªõc sau kh√¥ng b·ªã l·ªói\n    df_wide = pd.DataFrame(columns=['image_path'] + CFG.TARGET_COLS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:28.938287Z","iopub.execute_input":"2025-10-30T11:32:28.938596Z","iopub.status.idle":"2025-10-30T11:32:28.959306Z","shell.execute_reply.started":"2025-10-30T11:32:28.938567Z","shell.execute_reply":"2025-10-30T11:32:28.95854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from albumentations import (\n    Compose, \n    Resize, \n    Normalize,\n    HorizontalFlip, \n    VerticalFlip,\n    RandomRotate90,  # Ch·ªâ xoay 90, 180, 270\n    ColorJitter\n)\n\n# --- ƒê·ªãnh nghƒ©a Augmentations ---\n\n# Ch√∫ng ta s·∫Ω d√πng pipeline N√ÄY cho C·∫¢ hai ·∫£nh (tr√°i v√† ph·∫£i) M·ªòT C√ÅCH ƒê·ªòC L·∫¨P\ndef get_train_transforms():\n    \"\"\"\n    TƒÉng c∆∞·ªùng d·ªØ li·ªáu cho hu·∫•n luy·ªán.\n    S·∫Ω ƒë∆∞·ª£c √°p d·ª•ng ƒë·ªôc l·∫≠p cho img_left v√† img_right.\n    \"\"\"\n    return Compose([\n        # 1. TƒÉng c∆∞·ªùng h√¨nh h·ªçc (Geometric)\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        RandomRotate90(p=0.5), # T·ª± ƒë·ªông xoay 90, 180, ho·∫∑c 270\n\n        # 2. TƒÉng c∆∞·ªùng m√†u s·∫Øc (Photometric)\n        ColorJitter(\n            brightness=0.2, \n            contrast=0.2, \n            saturation=0.2, \n            hue=0.1, \n            p=0.75\n        ),\n\n        # 3. Chu·∫©n h√≥a (Normalize)\n        # Gi√° tr·ªã mean/std ti√™u chu·∫©n c·ªßa ImageNet\n        Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        ),\n        \n        # 4. N√©n ·∫£nh v√† Chuy·ªÉn sang Tensor\n        # (N√©n sau khi augment ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng)\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        ToTensorV2()\n    ])\n\ndef get_valid_transforms():\n    \"\"\"\n    Ch·ªâ chu·∫©n h√≥a v√† n√©n ·∫£nh cho t·∫≠p validation (kh√¥ng augment).\n    \"\"\"\n    return Compose([\n        Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        ),\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        ToTensorV2()\n    ])\n\n# --- In ra ƒë·ªÉ x√°c nh·∫≠n ---\nprint(\"ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m Augmentation.\")\nprint(f\"·∫¢nh hu·∫•n luy·ªán s·∫Ω ƒë∆∞·ª£c augment v√† n√©n xu·ªëng: {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")\nprint(f\"·∫¢nh validation s·∫Ω ch·ªâ ƒë∆∞·ª£c n√©n xu·ªëng: {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:28.961563Z","iopub.execute_input":"2025-10-30T11:32:28.961972Z","iopub.status.idle":"2025-10-30T11:32:28.968888Z","shell.execute_reply.started":"2025-10-30T11:32:28.961954Z","shell.execute_reply":"2025-10-30T11:32:28.968207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (ƒê√¢y l√† phi√™n b·∫£n M·ªöI c·ªßa l·ªõp BiomassDataset)\n\nclass BiomassDataset(Dataset):\n    \"\"\"\n    Dataset t√πy ch·ªânh cho chi·∫øn l∆∞·ª£c \"Hai lu·ªìng\".\n    \n    S·∫Ω tr·∫£ v·ªÅ:\n    (img_left, img_right, train_targets (3), all_targets (5))\n    \"\"\"\n    def __init__(self, df, transforms_fn, image_dir, train_target_cols, all_target_cols):\n        self.df = df\n        self.transforms_fn = transforms_fn\n        self.image_dir = image_dir\n        \n        # L∆∞u tr·ªØ s·∫µn ƒë·ªÉ truy c·∫≠p nhanh\n        self.image_paths = df['image_path'].values\n        # 3 m·ª•c ti√™u cho vi·ªác t√≠nh loss\n        self.train_targets = df[train_target_cols].values\n        # 5 m·ª•c ti√™u cho vi·ªác t√≠nh ƒëi·ªÉm R2\n        self.all_targets = df[all_target_cols].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # 1. L·∫•y th√¥ng tin\n        img_path_suffix = self.image_paths[idx]\n        train_target_vals = self.train_targets[idx]\n        all_target_vals = self.all_targets[idx]\n        \n        # 2. ƒê·ªçc ·∫£nh g·ªëc (2000x1000)\n        filename = os.path.basename(img_path_suffix)\n        full_path = os.path.join(self.image_dir, filename)\n        image = cv2.imread(full_path)\n        if image is None:\n            raise FileNotFoundError(f\"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {full_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 3. C·∫Øt (Crop) th√†nh 2 ·∫£nh (Tr√°i v√† Ph·∫£i)\n        height, width, _ = image.shape\n        mid_point = width // 2\n        img_left = image[:, :mid_point]\n        img_right = image[:, mid_point:]\n        \n        # 4. √Åp d·ª•ng Augmentations ƒê·ªòC L·∫¨P\n        transforms = self.transforms_fn()\n        img_left_tensor = transforms(image=img_left)['image']\n        \n        # L·∫•y pipeline M·ªöI cho ·∫£nh ph·∫£i (ƒë·ªÉ augment ƒë·ªôc l·∫≠p)\n        transforms_2 = self.transforms_fn()\n        img_right_tensor = transforms_2(image=img_right)['image']\n        \n        # 5. L·∫•y m·ª•c ti√™u (Targets)\n        train_target_tensor = torch.tensor(train_target_vals, dtype=torch.float32)\n        all_targets_tensor = torch.tensor(all_target_vals, dtype=torch.float32)\n        \n        # 6. Tr·∫£ v·ªÅ\n        return img_left_tensor, img_right_tensor, train_target_tensor, all_targets_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:28.969682Z","iopub.execute_input":"2025-10-30T11:32:28.969924Z","iopub.status.idle":"2025-10-30T11:32:28.983342Z","shell.execute_reply.started":"2025-10-30T11:32:28.969906Z","shell.execute_reply":"2025-10-30T11:32:28.982521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Th·ª≠ nghi·ªám Dataset v√† DataLoader ---\n\nprint(\"\\nƒêang ki·ªÉm tra Dataset & DataLoader...\")\n\ntry:\n    # 1. T·∫°o m·ªôt dataset (d√πng augmentation c·ªßa t·∫≠p train)\n    # L·∫•y 10 h√†ng ƒë·∫ßu ti√™n c·ªßa df_wide ƒë·ªÉ th·ª≠ nghi·ªám\n    test_df_subset = df_wide.head(10) \n    \n    train_dataset = BiomassDataset(\n        df=test_df_subset,\n        transforms_fn=get_train_transforms, # Truy·ªÅn t√™n H√ÄM\n        image_dir=CFG.IMAGE_DIR,\n        target_cols=CFG.TARGET_COLS\n    )\n    \n    # 2. T·∫°o DataLoader\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CFG.BATCH_SIZE,\n        shuffle=True,\n        num_workers=CFG.NUM_WORKERS\n    )\n    \n    # 3. L·∫•y m·ªôt batch d·ªØ li·ªáu\n    print(f\"ƒêang t·∫£i 1 batch (batch_size={CFG.BATCH_SIZE})...\")\n    img_left_batch, img_right_batch, targets_batch = next(iter(train_loader))\n    \n    # 4. In ra k√≠ch th∆∞·ªõc (shape)\n    print(\"\\n--- K√≠ch th∆∞·ªõc (Shape) c·ªßa Batch ƒë·∫ßu ra ---\")\n    print(f\"  ·∫¢nh tr√°i (Left): {img_left_batch.shape}\")\n    print(f\"  ·∫¢nh ph·∫£i (Right): {img_right_batch.shape}\")\n    print(f\"  M·ª•c ti√™u (Targets): {targets_batch.shape}\")\n    \n    print(\"\\n--- K√≠ch th∆∞·ªõc mong ƒë·ª£i ---\")\n    print(f\"  ·∫¢nh (K·ª≥ v·ªçng):   [Batch, Channels, Height, Width] -> [{CFG.BATCH_SIZE}, 3, {CFG.IMG_SIZE}, {CFG.IMG_SIZE}]\")\n    print(f\"  M·ª•c ti√™u (K·ª≥ v·ªçng): [Batch, Num_Targets] -> [{CFG.BATCH_SIZE}, {len(CFG.TARGET_COLS)}]\")\n    \n    # (T√πy ch·ªçn) Hi·ªÉn th·ªã m·ªôt ·∫£nh\n    plt.imshow(img_left_batch[0].permute(1, 2, 0) * 0.229 + 0.485) # ƒê√£ chu·∫©n h√≥a\n    plt.title(\"M·ªôt ·∫£nh m·∫´u t·ª´ Batch (Tr√°i)\")\n    plt.show()\n\nexcept Exception as e:\n    print(f\"\\nL·ªñI khi ki·ªÉm tra DataLoader: {e}\")\n    print(\"H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n trong CFG v√† c·∫•u tr√∫c file ·∫£nh.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:28.984203Z","iopub.execute_input":"2025-10-30T11:32:28.984447Z","iopub.status.idle":"2025-10-30T11:32:28.997499Z","shell.execute_reply.started":"2025-10-30T11:32:28.984423Z","shell.execute_reply":"2025-10-30T11:32:28.996803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BiomassModel(nn.Module):\n    \"\"\"\n    M√¥ h√¨nh Hai lu·ªìng, Ba ƒë·∫ßu ra chuy√™n bi·ªát.\n    \n    1. M·ªôt backbone (v√≠ d·ª•: ConvNeXt) d√πng chung.\n    2. Backbone ƒë∆∞·ª£c √°p d·ª•ng cho img_left v√† img_right.\n    3. Hai vector ƒë·∫∑c tr∆∞ng ƒë∆∞·ª£c gh√©p (concatenate) l·∫°i.\n    4. Vector h·ª£p nh·∫•t ƒë∆∞·ª£c ƒë∆∞a v√†o 3 \"ƒë·∫ßu\" MLP ri√™ng bi·ªát.\n    \"\"\"\n    def __init__(self, model_name, pretrained, n_targets=3):\n        super(BiomassModel, self).__init__()\n        \n        # 1. T·∫£i Backbone (Th√¢n) d√πng chung\n        # num_classes=0: Lo·∫°i b·ªè l·ªõp ph√¢n lo·∫°i g·ªëc.\n        # global_pool='avg': Th√™m l·ªõp Global Average Pooling\n        #                      ƒë·ªÉ l·∫•y ra m·ªôt vector ƒë·∫∑c tr∆∞ng 1D.\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool='avg'\n        )\n        \n        # 2. L·∫•y s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng (feature dimension) t·ª´ backbone\n        # V√≠ d·ª•: convnext_tiny l√† 768, resnet50 l√† 2048\n        self.n_features = self.backbone.num_features\n        \n        # 3. T√≠nh s·ªë chi·ªÅu sau khi h·ª£p nh·∫•t (fusion)\n        # (features_left + features_right)\n        self.n_combined_features = self.n_features * 2\n        \n        # 4. ƒê·ªãnh nghƒ©a Ba (3) \"ƒê·∫ßu\" MLP ri√™ng bi·ªát\n        # M·ªói \"ƒë·∫ßu\" l√† m·ªôt m·∫°ng n∆°-ron nh·ªè chuy√™n bi·ªát\n        \n        # --- ƒê·∫ßu cho Dry_Total_g ---\n        self.head_total = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1) # ƒê·∫ßu ra 1 gi√° tr·ªã\n        )\n        \n        # --- ƒê·∫ßu cho GDM_g ---\n        self.head_gdm = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1) # ƒê·∫ßu ra 1 gi√° tr·ªã\n        )\n        \n        # --- ƒê·∫ßu cho Dry_Green_g ---\n        self.head_green = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1) # ƒê·∫ßu ra 1 gi√° tr·ªã\n        )\n\n    def forward(self, img_left, img_right):\n        # 1. Ch·∫°y Lu·ªìng 1 (Tr√°i)\n        features_left = self.backbone(img_left) # Shape: [batch, n_features]\n        \n        # 2. Ch·∫°y Lu·ªìng 2 (Ph·∫£i)\n        features_right = self.backbone(img_right) # Shape: [batch, n_features]\n        \n        # 3. H·ª£p nh·∫•t (Fusion)\n        # Gh√©p 2 vector ƒë·∫∑c tr∆∞ng l·∫°i\n        combined = torch.cat([features_left, features_right], dim=1) # Shape: [batch, n_combined_features]\n        \n        # 4. Cho qua c√°c \"ƒê·∫ßu\" ri√™ng bi·ªát\n        out_total = self.head_total(combined)\n        out_gdm = self.head_gdm(combined)\n        out_green = self.head_green(combined)\n        \n        # 5. Tr·∫£ v·ªÅ 3 gi√° tr·ªã d·ª± ƒëo√°n\n        # Ch√∫ng ta tr·∫£ v·ªÅ 3 tensor ri√™ng bi·ªát ƒë·ªÉ h√†m loss d·ªÖ x·ª≠ l√Ω\n        return out_total, out_gdm, out_green","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:28.998128Z","iopub.execute_input":"2025-10-30T11:32:28.99836Z","iopub.status.idle":"2025-10-30T11:32:29.009267Z","shell.execute_reply.started":"2025-10-30T11:32:28.998336Z","shell.execute_reply":"2025-10-30T11:32:29.008545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Th·ª≠ nghi·ªám Ki·∫øn tr√∫c M√¥ h√¨nh ---\nprint(\"\\nƒêang ki·ªÉm tra ki·∫øn tr√∫c m√¥ h√¨nh...\")\n\ntry:\n    # 1. T·∫°o m·ªôt m√¥ h√¨nh\n    model = BiomassModel(\n        model_name=CFG.MODEL_NAME,\n        pretrained=False # Kh√¥ng c·∫ßn t·∫£i tr·ªçng s·ªë, ch·ªâ ki·ªÉm tra ki·∫øn tr√∫c\n    ).to(CFG.DEVICE)\n    \n    # 2. T·∫°o d·ªØ li·ªáu gi·∫£ (dummy data)\n    # (Batch size, channels, height, width)\n    dummy_left = torch.randn(CFG.BATCH_SIZE, 3, CFG.IMG_SIZE, CFG.IMG_SIZE).to(CFG.DEVICE)\n    dummy_right = torch.randn(CFG.BATCH_SIZE, 3, CFG.IMG_SIZE, CFG.IMG_SIZE).to(CFG.DEVICE)\n    \n    print(f\"ƒêang ƒë∆∞a 2 batch {dummy_left.shape} v√†o m√¥ h√¨nh...\")\n    \n    # 3. Ch·∫°y forward pass\n    out_total, out_gdm, out_green = model(dummy_left, dummy_right)\n    \n    # 4. In ra k√≠ch th∆∞·ªõc ƒë·∫ßu ra\n    print(\"\\n--- K√≠ch th∆∞·ªõc (Shape) c·ªßa ƒê·∫ßu ra M√¥ h√¨nh ---\")\n    print(f\"  ƒê·∫ßu ra Total: {out_total.shape}\")\n    print(f\"  ƒê·∫ßu ra GDM:   {out_gdm.shape}\")\n    print(f\"  ƒê·∫ßu ra Green: {out_green.shape}\")\n    \n    print(\"\\n--- K√≠ch th∆∞·ªõc mong ƒë·ª£i (m·ªói ƒë·∫ßu ra) ---\")\n    print(f\"  K·ª≥ v·ªçng: [Batch, 1] -> [{CFG.BATCH_SIZE}, 1]\")\n\nexcept Exception as e:\n    print(f\"\\nL·ªñI khi ki·ªÉm tra m√¥ h√¨nh: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:29.00984Z","iopub.execute_input":"2025-10-30T11:32:29.010022Z","iopub.status.idle":"2025-10-30T11:32:30.352297Z","shell.execute_reply.started":"2025-10-30T11:32:29.010007Z","shell.execute_reply":"2025-10-30T11:32:30.351559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WeightedBiomassLoss(nn.Module):\n    \"\"\"\n    H√†m loss t·ªïng h·ª£p c√≥ tr·ªçng s·ªë.\n    \n    T√≠nh to√°n 3 h√†m MSELoss ri√™ng bi·ªát v√† c·ªông ch√∫ng l·∫°i\n    v·ªõi c√°c tr·ªçng s·ªë t·ª´ CFG.\n    \"\"\"\n    def __init__(self, loss_weights_dict):\n        super(WeightedBiomassLoss, self).__init__()\n        # Ch√∫ng ta c√≥ th·ªÉ d√πng m·ªôt (1) h√†m MSELoss\n        # v√† g·ªçi n√≥ 3 l·∫ßn\n        self.criterion = nn.SmoothL1Loss()\n        \n        # L∆∞u tr·ªØ c√°c tr·ªçng s·ªë\n        self.weights = loss_weights_dict\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        predictions: M·ªôt tuple (out_total, out_gdm, out_green) t·ª´ m√¥ h√¨nh\n        targets: M·ªôt tensor [batch_size, 3] t·ª´ dataloader\n        \"\"\"\n        \n        # 1. T√°ch c√°c d·ª± ƒëo√°n\n        pred_total, pred_gdm, pred_green = predictions\n        \n        # 2. T√°ch c√°c m·ª•c ti√™u (ground truth)\n        # targets shape l√† [batch, 3]\n        # C·ªôt 0: Dry_Total_g\n        # C·ªôt 1: GDM_g\n        # C·ªôt 2: Dry_Green_g\n        true_total = targets[:, 0].unsqueeze(-1) # Shape [batch, 1]\n        true_gdm   = targets[:, 1].unsqueeze(-1) # Shape [batch, 1]\n        true_green = targets[:, 2].unsqueeze(-1) # Shape [batch, 1]\n        \n        # 3. T√≠nh 3 h√†m loss ri√™ng bi·ªát\n        loss_total = self.criterion(pred_total, true_total)\n        loss_gdm   = self.criterion(pred_gdm, true_gdm)\n        loss_green = self.criterion(pred_green, true_green)\n        \n        # 4. √Åp d·ª•ng tr·ªçng s·ªë v√† t√≠nh t·ªïng\n        total_loss = (\n            self.weights['total_loss'] * loss_total +\n            self.weights['gdm_loss'] * loss_gdm +\n            self.weights['green_loss'] * loss_green\n        )\n        \n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:30.352998Z","iopub.execute_input":"2025-10-30T11:32:30.353246Z","iopub.status.idle":"2025-10-30T11:32:30.360285Z","shell.execute_reply.started":"2025-10-30T11:32:30.353226Z","shell.execute_reply":"2025-10-30T11:32:30.359523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Kh·ªüi t·∫°o v√† Ki·ªÉm tra H√†m Loss ---\n\nprint(\"\\nƒêang kh·ªüi t·∫°o h√†m loss...\")\n\n# 1. Kh·ªüi t·∫°o\ncriterion = WeightedBiomassLoss(loss_weights_dict=CFG.LOSS_WEIGHTS)\ncriterion.to(CFG.DEVICE)\n\nprint(f\"H√†m loss ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi c√°c tr·ªçng s·ªë: {CFG.LOSS_WEIGHTS}\")\n\n# 2. Ki·ªÉm tra\nprint(\"\\nƒêang ki·ªÉm tra h√†m loss...\")\ntry:\n    # T·∫°o d·ªØ li·ªáu gi·∫£\n    # (Gi·ªëng nh∆∞ output c·ªßa m√¥ h√¨nh)\n    dummy_preds = (\n        torch.randn(CFG.BATCH_SIZE, 1).to(CFG.DEVICE),\n        torch.randn(CFG.BATCH_SIZE, 1).to(CFG.DEVICE),\n        torch.randn(CFG.BATCH_SIZE, 1).to(CFG.DEVICE)\n    )\n    \n    # (Gi·ªëng nh∆∞ output c·ªßa dataloader)\n    dummy_targets = torch.randn(CFG.BATCH_SIZE, len(CFG.TARGET_COLS)).to(CFG.DEVICE)\n    \n    # T√≠nh loss\n    loss_value = criterion(dummy_preds, dummy_targets)\n    \n    print(f\"  D·ªØ li·ªáu gi·∫£: {CFG.BATCH_SIZE} m·∫´u\")\n    print(f\"  Gi√° tr·ªã loss t√≠nh ƒë∆∞·ª£c: {loss_value.item():.4f}\")\n    print(\"‚úì H√†m loss ho·∫°t ƒë·ªông ch√≠nh x√°c.\")\n    \nexcept Exception as e:\n    print(f\"\\nL·ªñI khi ki·ªÉm tra h√†m loss: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:30.361224Z","iopub.execute_input":"2025-10-30T11:32:30.361496Z","iopub.status.idle":"2025-10-30T11:32:30.391997Z","shell.execute_reply.started":"2025-10-30T11:32:30.361473Z","shell.execute_reply":"2025-10-30T11:32:30.391485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\n# --- Th√™m v√†o l·ªõp CFG ---\nCFG.N_FOLDS = 5  # Ch√∫ng ta s·∫Ω d√πng 5-Fold Cross-Validation\nCFG.RANDOM_STATE = 42 # ƒê·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ c√≥ th·ªÉ t√°i l·∫≠p\n\nprint(f\"\\nƒêang chu·∫©n b·ªã {CFG.N_FOLDS}-Fold Cross-Validation...\")\n\n# 1. T·∫°o m·ªôt c·ªôt 'fold' m·ªõi trong df_wide, m·∫∑c ƒë·ªãnh l√† -1\ndf_wide['fold'] = -1\n\n# 2. T·∫°o Bins (Nh√≥m) cho m·ª•c ti√™u quan tr·ªçng nh·∫•t (Dry_Total_g)\n# 'pd.cut' s·∫Ω chia c√°c gi√° tr·ªã li√™n t·ª•c th√†nh 10 nh√≥m\n# (q=10: quantile-based, ƒë·∫£m b·∫£o m·ªói nh√≥m c√≥ s·ªë l∆∞·ª£ng m·∫´u t∆∞∆°ng ƒë∆∞∆°ng)\nnum_bins = int(np.floor(1 + np.log2(len(df_wide)))) # C√¥ng th·ª©c Sturges (ho·∫∑c d√πng 10)\nif len(df_wide) > 100: # D√πng 10 bins n·∫øu ƒë·ªß d·ªØ li·ªáu\n    num_bins = 10\n    \nprint(f\"S·ª≠ d·ª•ng {num_bins} bins ƒë·ªÉ ph√¢n t·∫ßng (stratify) tr√™n 'Dry_Total_g'\")\n\ndf_wide['total_bin'] = pd.cut(\n    df_wide['Dry_Total_g'], \n    bins=num_bins, \n    labels=False # Ch·ªâ c·∫ßn nh√£n s·ªë\n)\n\n# 3. Kh·ªüi t·∫°o StratifiedKFold\n# Ch√∫ng ta chia d·ª±a tr√™n c√°c 'bins' v·ª´a t·∫°o\nskf = StratifiedKFold(\n    n_splits=CFG.N_FOLDS, \n    shuffle=True, \n    random_state=CFG.RANDOM_STATE\n)\n\n# 4. G√°n s·ªë 'fold' cho m·ªói h√†ng\n# 'skf.split' tr·∫£ v·ªÅ (train_indices, valid_indices)\n# Ch√∫ng ta ch·ªâ c·∫ßn valid_indices ƒë·ªÉ g√°n s·ªë fold\nfor fold_num, (train_idx, valid_idx) in enumerate(skf.split(df_wide, df_wide['total_bin'])):\n    # G√°n s·ªë fold (0, 1, 2, 3, 4) cho c√°c h√†ng trong t·∫≠p validation\n    df_wide.loc[valid_idx, 'fold'] = fold_num\n\n# 5. Ki·ªÉm tra k·∫øt qu·∫£\nprint(\"\\n--- Ph√¢n ph·ªëi s·ªë l∆∞·ª£ng m·∫´u trong m·ªói Fold ---\")\nprint(df_wide['fold'].value_counts().sort_index())\n\nprint(\"\\n--- df_wide sau khi th√™m c·ªôt 'fold' (5 h√†ng ƒë·∫ßu) ---\")\nprint(df_wide.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:30.392722Z","iopub.execute_input":"2025-10-30T11:32:30.392956Z","iopub.status.idle":"2025-10-30T11:32:30.736136Z","shell.execute_reply.started":"2025-10-30T11:32:30.392935Z","shell.execute_reply":"2025-10-30T11:32:30.735378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ndef calculate_competition_score(all_preds_3, all_targets_5):\n    \"\"\"\n    T√≠nh ƒëi·ªÉm R^2 c√≥ tr·ªçng s·ªë c·ªßa cu·ªôc thi.\n    \n    Args:\n        all_preds_3 (dict): Dict ch·ª©a 3 m·∫£ng numpy d·ª± ƒëo√°n t·ª´ model.\n        all_targets_5 (np.array): M·∫£ng [N, 5] ch·ª©a 5 gi√° tr·ªã ground truth.\n    \n    Returns:\n        float: ƒêi·ªÉm R^2 cu·ªëi c√πng.\n    \"\"\"\n    \n    # 1. T√°i c·∫•u tr√∫c 5 d·ª± ƒëo√°n t·ª´ 3 ƒë·∫ßu ra\n    pred_total = all_preds_3['total']\n    pred_gdm = all_preds_3['gdm']\n    pred_green = all_preds_3['green']\n    \n    # ƒê·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m\n    pred_clover = np.maximum(0, pred_gdm - pred_green)\n    pred_dead = np.maximum(0, pred_total - pred_gdm)\n    \n    # Gh√©p 5 d·ª± ƒëo√°n (ph·∫£i ƒë√∫ng th·ª© t·ª± c·ªßa CFG.ALL_TARGET_COLS)\n    # ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n    y_preds = np.stack([\n        pred_green,\n        pred_dead,\n        pred_clover,\n        pred_gdm,\n        pred_total\n    ], axis=1) # Shape: [N, 5]\n    \n    y_true = all_targets_5 # Shape: [N, 5]\n\n    # 2. T√≠nh R^2 cho t·ª´ng m·ª•c ti√™u\n    # (r2_score c·ªßa sklearn c√≥ th·ªÉ x·ª≠ l√Ω multi-output)\n    r2_scores = r2_score(y_true, y_preds, multioutput='raw_values')\n    \n    # 3. √Åp d·ª•ng tr·ªçng s·ªë\n    weighted_r2_total = 0.0\n    for i, weight in enumerate(CFG.R2_WEIGHTS):\n        weighted_r2_total += r2_scores[i] * weight\n        \n    return weighted_r2_total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:30.736863Z","iopub.execute_input":"2025-10-30T11:32:30.737334Z","iopub.status.idle":"2025-10-30T11:32:30.742921Z","shell.execute_reply.started":"2025-10-30T11:32:30.737315Z","shell.execute_reply":"2025-10-30T11:32:30.742209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\nimport time\nimport gc\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"\n    Ch·∫°y 1 epoch hu·∫•n luy·ªán.\n    (ƒê√£ c·∫≠p nh·∫≠t ƒë·ªÉ x·ª≠ l√Ω 4 ƒë·∫ßu ra t·ª´ Dataset)\n    \"\"\"\n    model.train()  # Chuy·ªÉn m√¥ h√¨nh sang ch·∫ø ƒë·ªô .train()\n    epoch_loss = 0.0\n    \n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    \n    # *** THAY ƒê·ªîI CH√çNH ·ªû ƒê√ÇY ***\n    # Dataloader gi·ªù tr·∫£ v·ªÅ 4 gi√° tr·ªã.\n    # Ch√∫ng ta d√πng d·∫•u g·∫°ch d∆∞·ªõi (_) ƒë·ªÉ b·ªè qua gi√° tr·ªã th·ª© 4 (all_targets).\n    for (img_left, img_right, train_targets, _all_targets_ignored) in pbar:\n        \n        # 1. Chuy·ªÉn 3 d·ªØ li·ªáu c·∫ßn thi·∫øt sang device (GPU/CPU)\n        img_left = img_left.to(device)\n        img_right = img_right.to(device)\n        targets = train_targets.to(device) # Ch·ªâ 3 m·ª•c ti√™u cho loss\n        \n        # 2. X√≥a gradients c≈©\n        optimizer.zero_grad()\n        \n        # 3. Forward pass (Lan truy·ªÅn ti·∫øn)\n        predictions = model(img_left, img_right)\n        \n        # 4. T√≠nh to√°n Loss (v·ªõi 3 m·ª•c ti√™u)\n        loss = criterion(predictions, targets)\n        \n        # 5. Backward pass (Lan truy·ªÅn ng∆∞·ª£c)\n        loss.backward()\n        \n        # 6. C·∫≠p nh·∫≠t tr·ªçng s·ªë\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        pbar.set_postfix(loss=f'{loss.item():.4f}') # Hi·ªÉn th·ªã loss hi·ªán t·∫°i\n        \n    # Tr·∫£ v·ªÅ loss trung b√¨nh c·ªßa epoch\n    return epoch_loss / len(loader)\n\ndef validate_one_epoch(model, loader, criterion, device):\n    \"\"\"\n    Ch·∫°y 1 epoch ƒë√°nh gi√° V√Ä t√≠nh ƒëi·ªÉm R^2.\n    \"\"\"\n    model.eval()\n    epoch_loss = 0.0\n    \n    # List ƒë·ªÉ thu th·∫≠p t·∫•t c·∫£ d·ª± ƒëo√°n v√† nh√£n\n    all_preds_3 = {'total': [], 'gdm': [], 'green': []}\n    all_targets_list = []\n\n    with torch.no_grad():\n        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n        # Ch√∫ √Ω: Dataloader gi·ªù tr·∫£ v·ªÅ 4 gi√° tr·ªã\n        for (img_left, img_right, train_targets, all_targets) in pbar:\n            \n            img_left = img_left.to(device)\n            img_right = img_right.to(device)\n            train_targets = train_targets.to(device) # 3 m·ª•c ti√™u cho loss\n            # all_targets v·∫´n ·ªü CPU, ch√∫ng ta s·∫Ω x·ª≠ l√Ω sau\n            \n            # 1. Forward pass\n            pred_total, pred_gdm, pred_green = model(img_left, img_right)\n            \n            # 2. T√≠nh to√°n Loss (ch·ªâ d·ª±a tr√™n 3 m·ª•c ti√™u)\n            predictions_tuple = (pred_total, pred_gdm, pred_green)\n            loss = criterion(predictions_tuple, train_targets)\n            epoch_loss += loss.item()\n            \n            # 3. Thu th·∫≠p k·∫øt qu·∫£ ƒë·ªÉ t√≠nh R^2\n            # Chuy·ªÉn v·ªÅ CPU v√† l∆∞u d∆∞·ªõi d·∫°ng numpy\n            all_preds_3['total'].append(pred_total.cpu().numpy())\n            all_preds_3['gdm'].append(pred_gdm.cpu().numpy())\n            all_preds_3['green'].append(pred_green.cpu().numpy())\n            all_targets_list.append(all_targets.cpu().numpy())\n\n    # --- K·∫øt th√∫c Epoch ---\n    \n    # 4. Gh√©p t·∫•t c·∫£ c√°c batch l·∫°i\n    # Gh√©p c√°c list m·∫£ng numpy th√†nh m·ªôt m·∫£ng numpy l·ªõn\n    preds_dict_np = {\n        'total': np.concatenate(all_preds_3['total']).flatten(),\n        'gdm':   np.concatenate(all_preds_3['gdm']).flatten(),\n        'green': np.concatenate(all_preds_3['green']).flatten()\n    }\n    targets_np_5 = np.concatenate(all_targets_list) # Shape [N, 5]\n    \n    # 5. T√≠nh ƒëi·ªÉm R^2\n    competition_score = calculate_competition_score(preds_dict_np, targets_np_5)\n    \n    avg_epoch_loss = epoch_loss / len(loader)\n    \n    return avg_epoch_loss, competition_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:30.745287Z","iopub.execute_input":"2025-10-30T11:32:30.745497Z","iopub.status.idle":"2025-10-30T11:32:30.761909Z","shell.execute_reply.started":"2025-10-30T11:32:30.745481Z","shell.execute_reply":"2025-10-30T11:32:30.761328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport torch.optim as optim\nimport gc\n\ndef run_training(fold_to_run):\n    \"\"\"\n    H√†m ch√≠nh ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t v·ªõi chi·∫øn l∆∞·ª£c 2 giai ƒëo·∫°n (Freeze/Unfreeze).\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN FOLD {fold_to_run} (Chi·∫øn l∆∞·ª£c 2 giai ƒëo·∫°n) üöÄ\")\n    print(f\"{'='*50}\")\n    \n    start_time = time.time()\n    \n    # 1. Chia d·ªØ li·ªáu (Gi·ªØ nguy√™n)\n    print(f\"ƒêang chia d·ªØ li·ªáu cho Fold {fold_to_run}...\")\n    train_df = df_wide[df_wide['fold'] != fold_to_run].reset_index(drop=True)\n    valid_df = df_wide[df_wide['fold'] == fold_to_run].reset_index(drop=True)\n    \n    # 2. T·∫°o Datasets (Gi·ªØ nguy√™n)\n    # (Gi·∫£ s·ª≠ b·∫°n ƒëang d√πng BiomassDataset tr·∫£ v·ªÅ 4 gi√° tr·ªã)\n    train_dataset = BiomassDataset(\n        df=train_df, transforms_fn=get_train_transforms, image_dir=CFG.IMAGE_DIR,\n        train_target_cols=CFG.TARGET_COLS, all_target_cols=CFG.ALL_TARGET_COLS\n    )\n    valid_dataset = BiomassDataset(\n        df=valid_df, transforms_fn=get_valid_transforms, image_dir=CFG.IMAGE_DIR,\n        train_target_cols=CFG.TARGET_COLS, all_target_cols=CFG.ALL_TARGET_COLS\n    )\n    \n    # 3. T·∫°o DataLoaders (Gi·ªØ nguy√™n)\n    train_loader = DataLoader(\n        train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True,\n        num_workers=CFG.NUM_WORKERS, pin_memory=True\n    )\n    valid_loader = DataLoader(\n        valid_dataset, batch_size=CFG.BATCH_SIZE * 2, shuffle=False,\n        num_workers=CFG.NUM_WORKERS, pin_memory=True\n    )\n    \n    # 4. Kh·ªüi t·∫°o M√¥ h√¨nh & Loss (H·ªó tr·ª£ Multi-GPU)\n    print(f\"ƒêang t·∫£i backbone '{CFG.MODEL_NAME}'...\")\n    model_base = BiomassModel(CFG.MODEL_NAME, CFG.PRETRAINED)\n    \n    if torch.cuda.device_count() > 1:\n        print(f\"S·ª≠ d·ª•ng {torch.cuda.device_count()} GPU v·ªõi nn.DataParallel.\")\n        model = nn.DataParallel(model_base)\n    else:\n        model = model_base\n    model.to(CFG.DEVICE)\n    \n    criterion = WeightedBiomassLoss(CFG.LOSS_WEIGHTS).to(CFG.DEVICE)\n    \n    # =================================================================\n    # ‚ú® GIAI ƒêO·∫†N 1: HU·∫§N LUY·ªÜN \"HEADS\" (ƒê√ìNG BƒÇNG BACKBONE)\n    # =================================================================\n    print(f\"\\n--- GIAI ƒêO·∫†N 1: ƒê√≥ng bƒÉng Backbone (Training Heads) ---\")\n    print(f\"Epochs: 1 ƒë·∫øn {CFG.FREEZE_EPOCHS} | LR: {CFG.LEARNING_RATE}\")\n    \n    # ƒê√≥ng bƒÉng t·∫•t c·∫£ c√°c tham s·ªë c·ªßa backbone\n    # (D√πng .module. ƒë·ªÉ truy c·∫≠p m√¥ h√¨nh g·ªëc b√™n trong DataParallel)\n    for param in model.module.backbone.parameters():\n        param.requires_grad = False\n        \n    # T·∫°o optimizer CH·ªà cho c√°c \"heads\" (tham s·ªë c√≥ requires_grad=True)\n    optimizer = optim.Adam(\n        filter(lambda p: p.requires_grad, model.parameters()), \n        lr=CFG.LEARNING_RATE\n    )\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=2 # Gi·∫£m patience\n    )\n    \n    best_score = -float('inf') # Theo d√µi ƒëi·ªÉm R^2 t·ªët nh·∫•t qua C·∫¢ 2 GIAI ƒêO·∫†N\n\n    for epoch in range(1, CFG.FREEZE_EPOCHS + 1):\n        print(f\"\\n--- Epoch {epoch}/{CFG.EPOCHS} (Giai ƒëo·∫°n 1) ---\")\n        \n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, CFG.DEVICE)\n        valid_loss, competition_score = validate_one_epoch(model, valid_loader, criterion, CFG.DEVICE)\n        \n        scheduler.step(valid_loss)\n        \n        print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f} | Score (R^2): {competition_score:.4f}\")\n        \n        if competition_score > best_score:\n            best_score = competition_score\n            print(f\"‚ú® Score R^2 c·∫£i thi·ªán! ƒêang l∆∞u m√¥ h√¨nh 'best_model_fold{fold_to_run}.pth'...\")\n            torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), \n                       f'best_model_fold{fold_to_run}.pth')\n\n    # =================================================================\n    # ‚ú® GIAI ƒêO·∫†N 2: HU·∫§N LUY·ªÜN TO√ÄN B·ªò (R√É ƒê√îNG BACKBONE)\n    # =================================================================\n    print(f\"\\n--- GIAI ƒêO·∫†N 2: R√£ ƒë√¥ng Backbone (Fine-tuning) ---\")\n    print(f\"Epochs: {CFG.FREEZE_EPOCHS + 1} ƒë·∫øn {CFG.EPOCHS} | LR: {CFG.FINETUNE_LR}\")\n\n    # R√£ ƒë√¥ng to√†n b·ªô m√¥ h√¨nh\n    for param in model.module.backbone.parameters():\n        param.requires_grad = True\n        \n    # T·∫°o m·ªôt optimizer M·ªöI cho TO√ÄN B·ªò m√¥ h√¨nh v·ªõi LR th·∫•p\n    optimizer = optim.Adam(\n        model.parameters(), \n        lr=CFG.FINETUNE_LR # S·ª≠ d·ª•ng LR m·ªõi\n    )\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.2, patience=3\n    )\n    \n    # Ti·∫øp t·ª•c v√≤ng l·∫∑p epoch\n    for epoch in range(CFG.FREEZE_EPOCHS + 1, CFG.EPOCHS + 1):\n        print(f\"\\n--- Epoch {epoch}/{CFG.EPOCHS} (Giai ƒëo·∫°n 2) ---\")\n        \n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, CFG.DEVICE)\n        valid_loss, competition_score = validate_one_epoch(model, valid_loader, criterion, CFG.DEVICE)\n        \n        scheduler.step(valid_loss)\n        \n        print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f} | Score (R^2): {competition_score:.4f}\")\n        \n        if competition_score > best_score:\n            best_score = competition_score\n            print(f\"‚ú® Score R^2 c·∫£i thi·ªán! ƒêang l∆∞u m√¥ h√¨nh 'best_model_fold{fold_to_run}.pth'...\")\n            torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), \n                       f'best_model_fold{fold_to_run}.pth')\n            \n    # --- K·∫æT TH√öC HU·∫§N LUY·ªÜN ---\n    end_time = time.time()\n    print(f\"\\nüéâ Ho√†n th√†nh Fold {fold_to_run} sau {(end_time - start_time)/60:.2f} ph√∫t.\")\n    print(f\"ƒêi·ªÉm R^2 t·ªët nh·∫•t: {best_score:.4f}\")\n    \n    # D·ªçn d·∫πp\n    del model, train_loader, valid_loader, train_dataset, valid_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# --- B·∫ÆT ƒê·∫¶U CH·∫†Y HU·∫§N LUY·ªÜN ---\n# (Gi·ªØ nguy√™n)\ntry:\n    for i in range(CFG.N_FOLDS):\n        run_training(fold_to_run=i)\nexcept Exception as e:\n    gc.collect()\n    torch.cuda.empty_cache()\n    raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T11:32:30.76278Z","iopub.execute_input":"2025-10-30T11:32:30.763025Z","execution_failed":"2025-10-30T11:32:58.837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}