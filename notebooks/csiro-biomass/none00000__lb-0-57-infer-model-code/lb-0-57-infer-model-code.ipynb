{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":272090986,"sourceType":"kernelVersion"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n\n# Methodology: CSIRO Pasture Biomass Prediction\n\n## 1. Core Strategy: Predicting Key Components\n\nThe primary goal is to predict five biomass targets. Based on exploratory data analysis (EDA), we identified linear dependencies:\n* `Dry_Total_g` $\\approx$ `Dry_Green_g` + `Dry_Dead_g` + `Dry_Clover_g`\n* `GDM_g` $\\approx$ `Dry_Green_g` + `Dry_Clover_g`\n\nTo avoid redundancy, the model is trained to predict only the **three most visually distinct and/or highest-weighted targets**:\n* `Dry_Total_g` (50% of the score)\n* `GDM_g` (20% of the score)\n* `Dry_Green_g` (10% of the score)\n\nThe remaining two targets (`Dry_Dead_g` and `Dry_Clover_g`) are then **calculated during validation and inference** using subtraction (e.g., `pred_Clover = max(0, pred_GDM - pred_Green)`).\n\n---\n\n## 2. Data Handling & K-Fold Strategy\n\n* **Image Input:** All source images are high-resolution (`2000x1000` pixels).\n* **Two-Stream Processing:** To preserve fine-grained details (like clover leaves) that would be lost by resizing the entire image, the `Dataset` class crops each image into two `1000x1000` patches (a \"left\" and \"right\" half).\n* **High-Resolution Input:** Each `1000x1000` patch is then resized to **`768x768`**, maintaining a high level of detail.\n* **K-Fold Strategy:** We use a **5-Fold Cross-Validation** strategy due to the small dataset (357 images).\n* **Robust Splitting (GroupKFold):** To prevent data leakage (where similar images from the same day are in both train and validation), we use `GroupKFold` grouped by `Sampling_Date`. This ensures the model is validated on dates it has never seen.\n\n---\n\n## 3. Model Architecture: Two-Stream, Multi-Head\n\nThe model uses a \"Two-Stream, Multi-Head\" architecture.\n* **Shared Backbone:** A single `timm` backbone (e.g., `convnext_tiny`) with pre-trained ImageNet weights is used.\n* **Two-Stream Input:**\n    * `img_left` $\\rightarrow$ `backbone` $\\rightarrow$ `features_left`\n    * `img_right` $\\rightarrow$ (same) `backbone` $\\rightarrow$ `features_right`\n* **Fusion:** The two feature vectors are concatenated: `combined_features = torch.cat([features_left, features_right])`.\n* **Multi-Head Output:** This combined vector is fed into **three separate, specialized MLP heads** (one for each target: `head_total`, `head_gdm`, `head_green`) to allow for task specialization.\n\n---\n\n## 4. Data Augmentation\n\nTo compensate for the small dataset, augmentations are applied **independently** to the `img_left` and `img_right` patches.\n* `HorizontalFlip (p=0.5)`\n* `VerticalFlip (p=0.5)`\n* `RandomRotate90 (p=0.5)` (Only 90-degree rotations)\n* `ColorJitter`\n\nThis independent application creates a much larger variety of training combinations.\n\n---\n\n## 5. Loss Function: Weighted SmoothL1Loss\n\nThe model is optimized using a custom weighted loss function that aligns with the competition's scoring metric.\n* **Base Loss:** `nn.SmoothL1Loss` (Huber Loss) is used instead of `MSELoss` to make training more stable and less sensitive to outliers.\n* **Weighted Sum:** The final loss is a weighted sum of the individual losses, using the competition's scoring weights:\n    $$Loss = (0.5 \\cdot Loss_{Total}) + (0.2 \\cdot Loss_{GDM}) + (0.1 \\cdot Loss_{Green})$$\n\n---\n\n## 6. Training Strategy: Two-Stage Fine-Tuning\n\nA two-stage \"Freeze/Unfreeze\" strategy is used to stabilize training on the small dataset.\n* **Stage 1 (Freeze):**\n    * **Epochs:** 1-5\n    * **Action:** The entire `backbone` is frozen. Only the three MLP heads are trained.\n    * **LR:** `1e-4`\n* **Stage 2 (Unfreeze/Fine-Tuning):**\n    * **Epochs:** 6-20\n    * **Action:** The `backbone` is \"unfrozen,\" and the entire model is trained.\n    * **LR:** A very low learning rate (`1e-5`) is used to slowly adapt the backbone features.\n* **Model Saving:** A `ModelCheckpoint` saves the model based on the **highest `Score (R^2)`** on the validation set, *not* the lowest loss. This is critical for capturing the model's peak performance (like the `R^2=0.64` spike at Epoch 11) and ignoring the unstable, overfitted epochs.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm\nimport cv2\nfrom tqdm import tqdm\nimport gc\n\n# ===============================================================\n# 1. ‚öôÔ∏è CONFIGURATION (PH·∫¢I GI·ªêNG H·ªÜT FILE TRAINING)\n# ===============================================================\nclass CFG:\n    # --- ƒê∆∞·ªùng d·∫´n (Paths) ---\n    # (H√£y ƒëi·ªÅu ch·ªânh c√°c ƒë∆∞·ªùng d·∫´n n√†y cho ƒë√∫ng v·ªõi m√¥i tr∆∞·ªùng c·ªßa b·∫°n)\n    BASE_PATH = '/kaggle/input/csiro-biomass'\n    TEST_CSV = os.path.join(BASE_PATH, 'test.csv')\n    TEST_IMAGE_DIR = os.path.join(BASE_PATH, 'test')\n    \n    # Th∆∞ m·ª•c ch·ª©a 5 file .pth\n    MODEL_DIR = '/kaggle/input/csiro/' # Gi·∫£ s·ª≠ 5 file .pth n·∫±m c√πng th∆∞ m·ª•c\n    SUBMISSION_FILE = 'submission.csv'\n    \n    # --- C√†i ƒë·∫∑t M√¥ h√¨nh (PH·∫¢I TR√ôNG KH·ªöP) ---\n    MODEL_NAME = 'convnext_tiny' # PH·∫¢I GI·ªêNG H·ªÜT L√öC TRAIN\n    IMG_SIZE = 768               # PH·∫¢I GI·ªêNG H·ªÜT L√öC TRAIN\n    \n    # --- C√†i ƒë·∫∑t Inference ---\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    BATCH_SIZE = 1 # C√≥ th·ªÉ tƒÉng batch size khi inference\n    NUM_WORKERS = 1\n    N_FOLDS = 5\n    \n    # --- M·ª•c ti√™u & Loss (PH·∫¢I TR√ôNG KH·ªöP) ---\n    # 3 m·ª•c ti√™u model ƒë√£ d·ª± ƒëo√°n\n    TARGET_COLS = ['Dry_Total_g', 'GDM_g', 'Dry_Green_g']\n    \n    # 5 m·ª•c ti√™u ƒë·ªÉ n·ªôp b√†i\n    ALL_TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n\nprint(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {CFG.DEVICE}\")\nprint(f\"Backbone m√¥ h√¨nh: {CFG.MODEL_NAME}\")\nprint(f\"K√≠ch th∆∞·ªõc ·∫£nh inference: {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")\n\n\n# ===============================================================\n# 2. üèûÔ∏è AUGMENTATIONS (CH·ªà D√ôNG VALIDATION)\n# ===============================================================\nfrom albumentations import (\n    Compose, \n    Resize, \n    Normalize,\n    HorizontalFlip, \n    VerticalFlip\n)\n\ndef get_tta_transforms():\n    \"\"\"\n    Tr·∫£ v·ªÅ m·ªôt LIST c√°c pipeline transform cho TTA.\n    M·ªói pipeline l√† m·ªôt \"view\" kh√°c nhau c·ªßa ·∫£nh.\n    \"\"\"\n    \n    # ƒê√¢y l√† c√°c b∆∞·ªõc chu·∫©n h√≥a c∆° b·∫£n\n    base_transforms = [\n        Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        ),\n        ToTensorV2()\n    ]\n    \n    # -----------------\n    # View 1: ·∫¢nh g·ªëc (Ch·ªâ Resize + Normalize)\n    # -----------------\n    original_view = Compose([\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # -----------------\n    # View 2: L·∫≠t ngang (HFlip)\n    # -----------------\n    hflip_view = Compose([\n        HorizontalFlip(p=1.0), # Lu√¥n lu√¥n l·∫≠t\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # -----------------\n    # View 3: L·∫≠t d·ªçc (VFlip)\n    # -----------------\n    vflip_view = Compose([\n        VerticalFlip(p=1.0), # Lu√¥n lu√¥n l·∫≠t\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    return [original_view, hflip_view, vflip_view]\n\nprint(\"ƒê√£ ƒë·ªãnh nghƒ©a h√†m get_tta_transforms().\")\n\n\nclass TestBiomassDataset(Dataset):\n    \"\"\"\n    Dataset t√πy ch·ªânh cho ·∫£nh test (Chi·∫øn l∆∞·ª£c \"Hai lu·ªìng\").\n    S·ª≠a ƒë·ªïi ƒë·ªÉ ch·∫•p nh·∫≠n m·ªôt pipeline transform c·ª• th·ªÉ cho TTA.\n    \"\"\"\n    def __init__(self, df, transform_pipeline, image_dir):\n        self.df = df\n        # (S·ª¨A ƒê·ªîI) Ch·∫•p nh·∫≠n m·ªôt pipeline ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o\n        self.transforms = transform_pipeline \n        self.image_dir = image_dir\n        self.image_paths = df['image_path'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # 1. L·∫•y th√¥ng tin\n        img_path_suffix = self.image_paths[idx]\n        \n        # 2. ƒê·ªçc ·∫£nh g·ªëc (2000x1000)\n        filename = os.path.basename(img_path_suffix)\n        full_path = os.path.join(self.image_dir, filename)\n        \n        image = cv2.imread(full_path)\n        if image is None:\n            print(f\"Warning: Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {full_path}. Tr·∫£ v·ªÅ ·∫£nh ƒëen.\")\n            image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 3. C·∫Øt (Crop) th√†nh 2 ·∫£nh (Tr√°i v√† Ph·∫£i)\n        height, width, _ = image.shape\n        mid_point = width // 2\n        img_left = image[:, :mid_point]\n        img_right = image[:, mid_point:]\n        \n        # 4. √Åp d·ª•ng TTA Transform (C√ôNG M·ªòT TRANSFORM cho c·∫£ 2)\n        # (V√≠ d·ª•: C·∫£ 2 ·∫£nh ƒë·ªÅu b·ªã l·∫≠t ngang)\n        img_left_tensor = self.transforms(image=img_left)['image']\n        img_right_tensor = self.transforms(image=img_right)['image']\n        \n        # 5. Tr·∫£ v·ªÅ\n        return img_left_tensor, img_right_tensor\n\n# ===============================================================\n# 4. üß† MODEL ARCHITECTURE (SAO CH√âP T·ª™ FILE TRAIN)\n# ===============================================================\nclass BiomassModel(nn.Module):\n    \"\"\"\n    Ki·∫øn tr√∫c m√¥ h√¨nh (Hai lu·ªìng, Ba ƒë·∫ßu ra)\n    PH·∫¢I GI·ªêNG H·ªÜT file training.\n    \"\"\"\n    def __init__(self, model_name, pretrained, n_targets=3):\n        super(BiomassModel, self).__init__()\n        \n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained, # S·∫Ω l√† False khi inference\n            num_classes=0,\n            global_pool='avg'\n        )\n        \n        self.n_features = self.backbone.num_features\n        self.n_combined_features = self.n_features * 2\n        \n        # --- ƒê·∫ßu cho Dry_Total_g ---\n        self.head_total = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # --- ƒê·∫ßu cho GDM_g ---\n        self.head_gdm = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # --- ƒê·∫ßu cho Dry_Green_g ---\n        self.head_green = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n\n    def forward(self, img_left, img_right):\n        features_left = self.backbone(img_left)\n        features_right = self.backbone(img_right)\n        combined = torch.cat([features_left, features_right], dim=1)\n        \n        out_total = self.head_total(combined)\n        out_gdm = self.head_gdm(combined)\n        out_green = self.head_green(combined)\n        \n        return out_total, out_gdm, out_green\n\n\ndef predict_one_view(models_list, test_loader, device):\n    \"\"\"\n    H√†m con: Ch·∫°y d·ª± ƒëo√°n ensemble 5-fold cho M·ªòT view TTA.\n    \"\"\"\n    view_preds_3 = {'total': [], 'gdm': [], 'green': []}\n    \n    with torch.no_grad():\n        for (img_left, img_right) in tqdm(test_loader, desc=\"  Predicting View\", leave=False):\n            img_left = img_left.to(device)\n            img_right = img_right.to(device)\n            \n            batch_preds_3_folds = {'total': [], 'gdm': [], 'green': []}\n            \n            # 1. V√≤ng l·∫∑p Ensemble 5-Fold\n            for model in models_list:\n                pred_total, pred_gdm, pred_green = model(img_left, img_right)\n                batch_preds_3_folds['total'].append(pred_total.cpu())\n                batch_preds_3_folds['gdm'].append(pred_gdm.cpu())\n                batch_preds_3_folds['green'].append(pred_green.cpu())\n            \n            # 2. L·∫•y trung b√¨nh 5 Fold\n            avg_pred_total = torch.mean(torch.stack(batch_preds_3_folds['total']), dim=0)\n            avg_pred_gdm = torch.mean(torch.stack(batch_preds_3_folds['gdm']), dim=0)\n            avg_pred_green = torch.mean(torch.stack(batch_preds_3_folds['green']), dim=0)\n            \n            view_preds_3['total'].append(avg_pred_total.numpy())\n            view_preds_3['gdm'].append(avg_pred_gdm.numpy())\n            view_preds_3['green'].append(avg_pred_green.numpy())\n\n    # 3. Gh√©p k·∫øt qu·∫£ c√°c batch c·ªßa view n√†y\n    preds_np = {\n        'total': np.concatenate(view_preds_3['total']).flatten(),\n        'gdm':   np.concatenate(view_preds_3['gdm']).flatten(),\n        'green': np.concatenate(view_preds_3['green']).flatten()\n    }\n    return preds_np\n\n\ndef run_inference_with_tta():\n    \"\"\"\n    H√†m inference ch√≠nh, th·ª±c hi·ªán TTA x Ensemble.\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"üöÄ B·∫ÆT ƒê·∫¶U INFERENCE (v·ªõi TTA) üöÄ\")\n    print(f\"{'='*50}\")\n\n    # --- 1. T·∫£i D·ªØ li·ªáu Test ---\n    print(f\"ƒêang t·∫£i {CFG.TEST_CSV}...\")\n    try:\n        test_df_long = pd.read_csv(CFG.TEST_CSV)\n        test_df_unique = test_df_long.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n        print(f\"T√¨m th·∫•y {len(test_df_unique)} ·∫£nh test duy nh·∫•t.\")\n    except FileNotFoundError:\n        print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y {CFG.TEST_CSV}\")\n        return None, None, None\n\n    # --- 2. T·∫£i 5 M√¥ h√¨nh (Ensemble) ---\n    print(\"\\nƒêang t·∫£i 5 m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán...\")\n    models_list = []\n    # (Code t·∫£i 5 m√¥ h√¨nh... gi·ªëng h·ªát b∆∞·ªõc 16 c·ªßa file tr∆∞·ªõc)\n    for fold in range(CFG.N_FOLDS):\n        model_path = os.path.join(CFG.MODEL_DIR, f'best_model_fold{fold}.pth')\n        if not os.path.exists(model_path):\n            print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh: {model_path}\")\n            return None, None, None\n        model = BiomassModel(CFG.MODEL_NAME, pretrained=False)\n        try:\n            model.load_state_dict(torch.load(model_path, map_location=CFG.DEVICE))\n        except RuntimeError:\n            state_dict = torch.load(model_path, map_location=CFG.DEVICE)\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k.replace('module.', '')\n                new_state_dict[name] = v\n            model.load_state_dict(new_state_dict)\n        model.eval()\n        model.to(CFG.DEVICE)\n        models_list.append(model)\n    print(f\"‚úì ƒê√£ t·∫£i th√†nh c√¥ng {len(models_list)} m√¥ h√¨nh.\")\n\n    # --- 3. V√≤ng l·∫∑p TTA (V√≤ng l·∫∑p ngo√†i) ---\n    tta_transforms = get_tta_transforms()\n    print(f\"\\nB·∫Øt ƒë·∫ßu d·ª± ƒëo√°n v·ªõi {len(tta_transforms)} TTA views...\")\n    \n    all_tta_view_preds = [] # List ƒë·ªÉ l∆∞u k·∫øt qu·∫£ c·ªßa m·ªói view TTA\n\n    for i, tta_transform in enumerate(tta_transforms):\n        print(f\"--- ƒêang ch·∫°y TTA View {i+1}/{len(tta_transforms)} ---\")\n        \n        # T·∫°o Dataset/Loader M·ªöI cho view TTA n√†y\n        test_dataset = TestBiomassDataset(\n            df=test_df_unique,\n            transform_pipeline=tta_transform, # Truy·ªÅn pipeline TTA\n            image_dir=CFG.TEST_IMAGE_DIR\n        )\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=CFG.BATCH_SIZE,\n            shuffle=False,\n            num_workers=CFG.NUM_WORKERS,\n            pin_memory=True\n        )\n        \n        # Ch·∫°y ensemble 5-fold cho view n√†y\n        view_preds_np = predict_one_view(models_list, test_loader, CFG.DEVICE)\n        all_tta_view_preds.append(view_preds_np)\n        print(f\"‚úì Ho√†n th√†nh TTA View {i+1}\")\n\n    # --- 4. Ensemble (L·∫•y trung b√¨nh) k·∫øt qu·∫£ TTA ---\n    print(\"\\nƒêang ensemble k·∫øt qu·∫£ c·ªßa c√°c TTA views...\")\n    final_ensembled_preds = {\n        'total': np.mean([d['total'] for d in all_tta_view_preds], axis=0),\n        'gdm':   np.mean([d['gdm'] for d in all_tta_view_preds], axis=0),\n        'green': np.mean([d['green'] for d in all_tta_view_preds], axis=0)\n    }\n    \n    print(\"‚úì D·ª± ƒëo√°n ho√†n t·∫•t.\")\n    \n    del models_list, test_loader, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return final_ensembled_preds, test_df_long, test_df_unique\n# ===============================================================\n# 6. ‚úçÔ∏è H√ÄM T·∫†O FILE SUBMISSION\n# ===============================================================\ndef create_submission(preds_np, test_df_long, test_df_unique):\n    \"\"\"\n    H√†m n√†y nh·∫≠n 3 d·ª± ƒëo√°n ƒë√£ ensemble,\n    t√≠nh to√°n 2 d·ª± ƒëo√°n c√≤n l·∫°i,\n    v√† ƒë·ªãnh d·∫°ng file n·ªôp b√†i.\n    \"\"\"\n    if preds_np is None:\n        print(\"B·ªè qua t·∫°o submission do l·ªói ·ªü tr√™n.\")\n        return\n\n    print(\"\\nƒêang h·∫≠u x·ª≠ l√Ω v√† t·∫°o file submission...\")\n\n    # 1. L·∫•y 3 d·ª± ƒëo√°n ƒë√£ ensemble\n    pred_total_final = preds_np['total']\n    pred_gdm_final = preds_np['gdm']\n    pred_green_final = preds_np['green']\n\n    # 2. T√≠nh 2 m·ª•c ti√™u c√≤n l·∫°i (H·∫≠u x·ª≠ l√Ω)\n    # D√πng np.maximum(0, ...) ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m\n    pred_clover_final = np.maximum(0, pred_gdm_final - pred_green_final)\n    pred_dead_final = np.maximum(0, pred_total_final - pred_gdm_final)\n\n    # 3. T·∫°o m·ªôt DataFrame \"wide\" ch·ª©a 5 d·ª± ƒëo√°n\n    # (ƒê·∫£m b·∫£o th·ª© t·ª± 5 c·ªôt gi·ªëng CFG.ALL_TARGET_COLS)\n    preds_wide_df = pd.DataFrame({\n        'image_path': test_df_unique['image_path'],\n        'Dry_Green_g': pred_green_final,\n        'Dry_Dead_g': pred_dead_final,\n        'Dry_Clover_g': pred_clover_final,\n        'GDM_g': pred_gdm_final,\n        'Dry_Total_g': pred_total_final\n    })\n\n    # 4. \"Un-pivot\" DataFrame (Chuy·ªÉn sang d·∫°ng \"long\")\n    # Bi·∫øn n√≥ t·ª´ 5 c·ªôt v·ªÅ d·∫°ng \"long\" (gi·ªëng sample_submission)\n    preds_long_df = preds_wide_df.melt(\n        id_vars=['image_path'],\n        value_vars=CFG.ALL_TARGET_COLS, # 5 c·ªôt m·ª•c ti√™u\n        var_name='target_name',        # C·ªôt t√™n m·ª•c ti√™u\n        value_name='target'            # C·ªôt gi√° tr·ªã d·ª± ƒëo√°n\n    )\n\n    # 5. Merge v·ªõi file test.csv g·ªëc (test_df_long)\n    # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ l·∫•y ƒë√∫ng 'sample_id'\n    # (v√≠ d·ª•: 'ID1001187975__Dry_Clover_g')\n    submission_df = pd.merge(\n        test_df_long[['sample_id', 'image_path', 'target_name']],\n        preds_long_df,\n        on=['image_path', 'target_name'],\n        how='left'\n    )\n\n    # 6. D·ªçn d·∫πp v√† L∆∞u\n    # Ch·ªâ gi·ªØ l·∫°i 2 c·ªôt ƒë∆∞·ª£c y√™u c·∫ßu\n    submission_df = submission_df[['sample_id', 'target']]\n    \n    # L∆∞u file\n    submission_df.to_csv(CFG.SUBMISSION_FILE, index=False)\n\n    print(f\"\\nüéâ HO√ÄN T·∫§T! ƒê√£ l∆∞u file submission t·∫°i: {CFG.SUBMISSION_FILE}\")\n    print(\"--- 5 h√†ng ƒë·∫ßu c·ªßa file submission ---\")\n    print(submission_df.head())\n    print(\"\\n--- 5 h√†ng cu·ªëi c·ªßa file submission ---\")\n    print(submission_df.tail())\n    \n# ===============================================================\n# 8. üèÅ CH·∫†Y CH∆Ø∆†NG TR√åNH (ƒê√£ s·ª≠a)\n# ===============================================================\nif __name__ == \"__main__\":\n    # 1. Ch·∫°y d·ª± ƒëo√°n (ƒë√£ bao g·ªìm TTA)\n    all_preds_np, df_long, df_unique = run_inference_with_tta()\n    \n    # 2. T·∫°o file submission (H√†m create_submission gi·ªØ nguy√™n)\n    create_submission(all_preds_np, df_long, df_unique)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:37:31.585364Z","iopub.execute_input":"2025-10-30T13:37:31.586001Z","iopub.status.idle":"2025-10-30T13:37:37.021319Z","shell.execute_reply.started":"2025-10-30T13:37:31.585975Z","shell.execute_reply":"2025-10-30T13:37:37.020506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}