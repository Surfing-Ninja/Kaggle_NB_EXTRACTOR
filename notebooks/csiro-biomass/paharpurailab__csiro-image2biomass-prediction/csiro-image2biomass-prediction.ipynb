{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":272104372,"sourceType":"kernelVersion"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport timm\nimport cv2\nfrom tqdm import tqdm\nimport gc\n\n# ===============================================================\n# 1. ‚öôÔ∏è CONFIGURATION (MUST MATCH THE TRAINING FILE)\n# ===============================================================\nclass CFG:\n    # --- Paths ---\n    BASE_PATH = '/kaggle/input/csiro-biomass'\n    TEST_CSV = os.path.join(BASE_PATH, 'test.csv')\n    TEST_IMAGE_DIR = os.path.join(BASE_PATH, 'test')\n    \n    # Directory containing 5 model (.pth) files\n    MODEL_DIR = '/kaggle/input/csiro/'  # Assuming all 5 .pth files are here\n    SUBMISSION_FILE = 'submission.csv'\n    \n    # --- Model Settings (MUST MATCH TRAINING) ---\n    MODEL_NAME = 'convnext_tiny'  # Must be identical to the one used during training\n    IMG_SIZE = 768                # Must match the training image size\n    \n    # --- Inference Settings ---\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    BATCH_SIZE = 1\n    NUM_WORKERS = 1\n    N_FOLDS = 5\n    \n    # --- Target Columns (MUST MATCH TRAINING) ---\n    TARGET_COLS = ['Dry_Total_g', 'GDM_g', 'Dry_Green_g']\n    \n    # --- Columns Required for Submission ---\n    ALL_TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n\nprint(f\"Using device: {CFG.DEVICE}\")\nprint(f\"Model backbone: {CFG.MODEL_NAME}\")\nprint(f\"Inference image size: {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")\n\n\n# ===============================================================\n# 2. üèûÔ∏è AUGMENTATIONS (FOR VALIDATION / TTA ONLY)\n# ===============================================================\nfrom albumentations import (\n    Compose, \n    Resize, \n    Normalize,\n    HorizontalFlip, \n    VerticalFlip\n)\n\ndef get_tta_transforms():\n    \"\"\"\n    Returns a list of transform pipelines for TTA (Test-Time Augmentation).\n    Each pipeline represents a different \"view\" of the same image.\n    \"\"\"\n    \n    # Base normalization steps\n    base_transforms = [\n        Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        ),\n        ToTensorV2()\n    ]\n    \n    # View 1: Original image\n    original_view = Compose([\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # View 2: Horizontal flip\n    hflip_view = Compose([\n        HorizontalFlip(p=1.0),\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    # View 3: Vertical flip\n    vflip_view = Compose([\n        VerticalFlip(p=1.0),\n        Resize(CFG.IMG_SIZE, CFG.IMG_SIZE),\n        *base_transforms\n    ])\n    \n    return [original_view, hflip_view, vflip_view]\n\nprint(\"Defined function get_tta_transforms().\")\n\n\nclass TestBiomassDataset(Dataset):\n    \"\"\"\n    Custom dataset for test images (Dual-stream strategy).\n    Modified to accept a specific TTA transform pipeline.\n    \"\"\"\n    def __init__(self, df, transform_pipeline, image_dir):\n        self.df = df\n        self.transforms = transform_pipeline \n        self.image_dir = image_dir\n        self.image_paths = df['image_path'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # 1. Get image path\n        img_path_suffix = self.image_paths[idx]\n        \n        # 2. Read original image (2000x1000)\n        filename = os.path.basename(img_path_suffix)\n        full_path = os.path.join(self.image_dir, filename)\n        \n        image = cv2.imread(full_path)\n        if image is None:\n            print(f\"Warning: Unable to read image: {full_path}. Returning black image.\")\n            image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 3. Crop into left and right halves\n        height, width, _ = image.shape\n        mid_point = width // 2\n        img_left = image[:, :mid_point]\n        img_right = image[:, mid_point:]\n        \n        # 4. Apply the same TTA transform to both halves\n        img_left_tensor = self.transforms(image=img_left)['image']\n        img_right_tensor = self.transforms(image=img_right)['image']\n        \n        # 5. Return both halves\n        return img_left_tensor, img_right_tensor\n\n\n# ===============================================================\n# 4. üß† MODEL ARCHITECTURE (COPY EXACTLY FROM TRAIN FILE)\n# ===============================================================\nclass BiomassModel(nn.Module):\n    \"\"\"\n    Dual-stream architecture with three outputs.\n    Must match the training file exactly.\n    \"\"\"\n    def __init__(self, model_name, pretrained, n_targets=3):\n        super(BiomassModel, self).__init__()\n        \n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained, \n            num_classes=0,\n            global_pool='avg'\n        )\n        \n        self.n_features = self.backbone.num_features\n        self.n_combined_features = self.n_features * 2\n        \n        # Head for Dry_Total_g\n        self.head_total = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # Head for GDM_g\n        self.head_gdm = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n        \n        # Head for Dry_Green_g\n        self.head_green = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n\n    def forward(self, img_left, img_right):\n        features_left = self.backbone(img_left)\n        features_right = self.backbone(img_right)\n        combined = torch.cat([features_left, features_right], dim=1)\n        \n        out_total = self.head_total(combined)\n        out_gdm = self.head_gdm(combined)\n        out_green = self.head_green(combined)\n        \n        return out_total, out_gdm, out_green\n\n\ndef predict_one_view(models_list, test_loader, device):\n    \"\"\"\n    Predicts ensemble 5-fold results for ONE TTA view.\n    \"\"\"\n    view_preds_3 = {'total': [], 'gdm': [], 'green': []}\n    \n    with torch.no_grad():\n        for (img_left, img_right) in tqdm(test_loader, desc=\"  Predicting View\", leave=False):\n            img_left = img_left.to(device)\n            img_right = img_right.to(device)\n            \n            batch_preds_3_folds = {'total': [], 'gdm': [], 'green': []}\n            \n            # Ensemble across 5 folds\n            for model in models_list:\n                pred_total, pred_gdm, pred_green = model(img_left, img_right)\n                batch_preds_3_folds['total'].append(pred_total.cpu())\n                batch_preds_3_folds['gdm'].append(pred_gdm.cpu())\n                batch_preds_3_folds['green'].append(pred_green.cpu())\n            \n            # Average over 5 folds\n            avg_pred_total = torch.mean(torch.stack(batch_preds_3_folds['total']), dim=0)\n            avg_pred_gdm = torch.mean(torch.stack(batch_preds_3_folds['gdm']), dim=0)\n            avg_pred_green = torch.mean(torch.stack(batch_preds_3_folds['green']), dim=0)\n            \n            view_preds_3['total'].append(avg_pred_total.numpy())\n            view_preds_3['gdm'].append(avg_pred_gdm.numpy())\n            view_preds_3['green'].append(avg_pred_green.numpy())\n\n    # Concatenate all batch predictions for this view\n    preds_np = {\n        'total': np.concatenate(view_preds_3['total']).flatten(),\n        'gdm':   np.concatenate(view_preds_3['gdm']).flatten(),\n        'green': np.concatenate(view_preds_3['green']).flatten()\n    }\n    return preds_np\n\n\ndef run_inference_with_tta():\n    \"\"\"\n    Main inference function performing TTA + 5-Fold Ensemble.\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"üöÄ STARTING INFERENCE (with TTA) üöÄ\")\n    print(f\"{'='*50}\")\n\n    # --- 1. Load Test Data ---\n    print(f\"Loading {CFG.TEST_CSV}...\")\n    try:\n        test_df_long = pd.read_csv(CFG.TEST_CSV)\n        test_df_unique = test_df_long.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n        print(f\"Found {len(test_df_unique)} unique test images.\")\n    except FileNotFoundError:\n        print(f\"ERROR: Test CSV not found: {CFG.TEST_CSV}\")\n        return None, None, None\n\n    # --- 2. Load 5 Trained Models ---\n    print(\"\\nLoading 5 trained models...\")\n    models_list = []\n    for fold in range(CFG.N_FOLDS):\n        model_path = os.path.join(CFG.MODEL_DIR, f'best_model_fold{fold}.pth')\n        if not os.path.exists(model_path):\n            print(f\"ERROR: Model file not found: {model_path}\")\n            return None, None, None\n        model = BiomassModel(CFG.MODEL_NAME, pretrained=False)\n        try:\n            model.load_state_dict(torch.load(model_path, map_location=CFG.DEVICE))\n        except RuntimeError:\n            state_dict = torch.load(model_path, map_location=CFG.DEVICE)\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k.replace('module.', '')\n                new_state_dict[name] = v\n            model.load_state_dict(new_state_dict)\n        model.eval()\n        model.to(CFG.DEVICE)\n        models_list.append(model)\n    print(f\"‚úì Successfully loaded {len(models_list)} models.\")\n\n    # --- 3. Loop over TTA Views ---\n    tta_transforms = get_tta_transforms()\n    print(f\"\\nStarting predictions with {len(tta_transforms)} TTA views...\")\n    \n    all_tta_view_preds = []\n\n    for i, tta_transform in enumerate(tta_transforms):\n        print(f\"--- Running TTA View {i+1}/{len(tta_transforms)} ---\")\n        \n        test_dataset = TestBiomassDataset(\n            df=test_df_unique,\n            transform_pipeline=tta_transform,\n            image_dir=CFG.TEST_IMAGE_DIR\n        )\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=CFG.BATCH_SIZE,\n            shuffle=False,\n            num_workers=CFG.NUM_WORKERS,\n            pin_memory=True\n        )\n        \n        view_preds_np = predict_one_view(models_list, test_loader, CFG.DEVICE)\n        all_tta_view_preds.append(view_preds_np)\n        print(f\"‚úì Completed TTA View {i+1}\")\n\n    # --- 4. Average TTA Results ---\n    print(\"\\nAveraging predictions across TTA views...\")\n    final_ensembled_preds = {\n        'total': np.mean([d['total'] for d in all_tta_view_preds], axis=0),\n        'gdm':   np.mean([d['gdm'] for d in all_tta_view_preds], axis=0),\n        'green': np.mean([d['green'] for d in all_tta_view_preds], axis=0)\n    }\n    \n    print(\"‚úì Inference complete.\")\n    \n    del models_list, test_loader, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return final_ensembled_preds, test_df_long, test_df_unique\n\n\n# ===============================================================\n# 6. ‚úçÔ∏è CREATE SUBMISSION FILE\n# ===============================================================\ndef create_submission(preds_np, test_df_long, test_df_unique):\n    \"\"\"\n    Takes 3 predicted outputs, calculates 2 additional targets,\n    and formats them into the submission CSV file.\n    \"\"\"\n    if preds_np is None:\n        print(\"Skipping submission creation due to previous error.\")\n        return\n\n    print(\"\\nPost-processing and creating submission file...\")\n\n    pred_total_final = preds_np['total']\n    pred_gdm_final = preds_np['gdm']\n    pred_green_final = preds_np['green']\n\n    # Calculate two missing targets\n    pred_clover_final = np.maximum(0, pred_gdm_final - pred_green_final)\n    pred_dead_final = np.maximum(0, pred_total_final - pred_gdm_final)\n\n    preds_wide_df = pd.DataFrame({\n        'image_path': test_df_unique['image_path'],\n        'Dry_Green_g': pred_green_final,\n        'Dry_Dead_g': pred_dead_final,\n        'Dry_Clover_g': pred_clover_final,\n        'GDM_g': pred_gdm_final,\n        'Dry_Total_g': pred_total_final\n    })\n\n    # Convert from wide to long format\n    preds_long_df = preds_wide_df.melt(\n        id_vars=['image_path'],\n        value_vars=CFG.ALL_TARGET_COLS,\n        var_name='target_name',\n        value_name='target'\n    )\n\n    # Merge with original test.csv to get 'sample_id'\n    submission_df = pd.merge(\n        test_df_long[['sample_id', 'image_path', 'target_name']],\n        preds_long_df,\n        on=['image_path', 'target_name'],\n        how='left'\n    )\n\n    submission_df = submission_df[['sample_id', 'target']]\n    submission_df.to_csv(CFG.SUBMISSION_FILE, index=False)\n\n    print(f\"\\nüéâ DONE! Submission saved at: {CFG.SUBMISSION_FILE}\")\n    print(\"--- First 5 rows of submission ---\")\n    print(submission_df.head())\n    print(\"\\n--- Last 5 rows of submission ---\")\n    print(submission_df.tail())\n\n\n# ===============================================================\n# 8. üèÅ RUN THE PROGRAM\n# ===============================================================\nif __name__ == \"__main__\":\n    all_preds_np, df_long, df_unique = run_inference_with_tta()\n    create_submission(all_preds_np, df_long, df_unique)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:37:31.585364Z","iopub.execute_input":"2025-10-30T13:37:31.586001Z","iopub.status.idle":"2025-10-30T13:37:37.021319Z","shell.execute_reply.started":"2025-10-30T13:37:31.585975Z","shell.execute_reply":"2025-10-30T13:37:37.020506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}