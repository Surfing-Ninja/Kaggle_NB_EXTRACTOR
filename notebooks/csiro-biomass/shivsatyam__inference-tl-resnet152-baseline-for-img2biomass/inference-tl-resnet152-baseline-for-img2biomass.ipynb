{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":633422,"sourceType":"modelInstanceVersion","modelInstanceId":477581,"modelId":493530}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ResNet152 Inference Notebook\n\n- I am using ResNet152 pretrained on ImageNet weights for image-to-biomass prediction.  \n- This will not give you an impeccable score on the test set. [LB 0.24]  \n- A Vision Transformer would be a better choice for this kind of task.  \n\n# Why this notebook?\n- Anyone having trouble with processing and inference can refer to this notebook.  \n- Anyone who wants to practice tuning (transfer learning models) for a real competition may find this notebook helpful.  \n\nI have used a pretrained model in this notebook. You can also check out the public [Training Notebook](https://www.kaggle.com/code/shivsatyam/baseline-resnet152-tl-simple-tuning-example).  \n\nReferences for those who want to learn more:  \n- Transfer Learning: https://www.ibm.com/think/topics/transfer-learning  \n- ResNet152: https://arxiv.org/pdf/1512.03385\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport random\n\nimport PIL\nfrom PIL import Image\nimport torch.nn as nn\nimport torch\n\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms, datasets, models\nfrom tqdm.auto import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/csiro-biomass/'\nN_CLASSES = 5\nBATCH_SIZE = 8\nNUM_WORKERS = 0\n\ntrain_df = pd.read_csv(DATA_PATH + 'train.csv')\ntest_df = pd.read_csv(DATA_PATH + 'test.csv')\ntrain_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:28.220734Z","iopub.execute_input":"2025-11-08T16:29:28.220969Z","iopub.status.idle":"2025-11-08T16:29:28.224627Z","shell.execute_reply.started":"2025-11-08T16:29:28.220951Z","shell.execute_reply":"2025-11-08T16:29:28.223967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the class names would be in order of [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g]\n\ndef get_unique_dataframe(df, target_parameter):\n    return np.unique(df[target_parameter].tolist())\n\ndef mf_dataframe(df, target_parameter, inference = False):\n    mod_df = pd.DataFrame()\n    image_paths_unique = get_unique_dataframe(df, target_parameter)\n    for image_path in tqdm(image_paths_unique, desc = \"Processing Dataframe\"):\n        selective = df[df[target_parameter] == image_path]\n        if inference:\n            current_series = pd.Series({\n                'path': image_path,\n            })\n        else:\n            current_series = pd.Series({\n                'path': image_path,\n                'Dry_Clover_g': selective[selective[\"target_name\"] == \"Dry_Clover_g\"][\"target\"].tolist()[0],\n                'Dry_Dead_g': selective[selective[\"target_name\"] == \"Dry_Dead_g\"][\"target\"].tolist()[0],\n                'Dry_Green_g': selective[selective[\"target_name\"] == \"Dry_Green_g\"][\"target\"].tolist()[0],\n                'Dry_Total_g': selective[selective[\"target_name\"] == \"Dry_Total_g\"][\"target\"].tolist()[0],\n                'GDM_g': selective[selective[\"target_name\"] == \"GDM_g\"][\"target\"].tolist()[0],\n            })\n        mod_df = pd.concat([mod_df, current_series.to_frame().T], ignore_index = True)\n    return mod_df\n\nunique_train_df = mf_dataframe(df = train_df, target_parameter = 'image_path')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:28.315552Z","iopub.execute_input":"2025-11-08T16:29:28.316225Z","iopub.status.idle":"2025-11-08T16:29:29.166314Z","shell.execute_reply.started":"2025-11-08T16:29:28.316197Z","shell.execute_reply":"2025-11-08T16:29:29.165303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_transform = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(size = 224, scale = (0.8, 1.0), ratio = (0.9, 1.1)),\n        transforms.RandomHorizontalFlip(p = 0.5),\n        transforms.RandomRotation(degrees = 15),\n        transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2, hue = 0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                             std = [0.229, 0.224, 0.225])\n    ]),\n\n    'valid': transforms.Compose([\n        transforms.Resize(size = 256),\n        transforms.CenterCrop(size = 224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                             std = [0.229, 0.224, 0.225])\n    ]),\n\n    'test': transforms.Compose([\n        transforms.Resize(size = 256),\n        transforms.CenterCrop(size = 224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                             std = [0.229, 0.224, 0.225])\n    ]),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:29.167289Z","iopub.execute_input":"2025-11-08T16:29:29.167551Z","iopub.status.idle":"2025-11-08T16:29:29.174545Z","shell.execute_reply.started":"2025-11-08T16:29:29.167522Z","shell.execute_reply":"2025-11-08T16:29:29.173692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Subset, random_split, DataLoader\n\nclass Configure(nn.Module):\n    def __init__(self, dataframe, task_type = 'train'):\n        self.dataframe = dataframe\n        self.task_type = task_type\n    def __len__(self):\n        return len(self.dataframe)\n    def __getitem__(self, index):\n        image = self.dataframe.iloc[index][\"path\"]\n        pil = Image.open(DATA_PATH + image)\n        pil = img_transform[self.task_type](pil)\n        if self.task_type == \"test\":\n            return pil\n        labels = self.dataframe.iloc[index][1:].tolist()\n        return pil, torch.tensor(labels)\n\n\ndataset = Configure(dataframe = unique_train_df, task_type = 'train')\ntrain_size = int(0.8 * len(dataset))\nvalid_size = len(dataset) - train_size\n\ntrain_dataset, valid_indices = random_split(dataset, [train_size, valid_size])\n\nvalid_dataset = Subset(Configure(dataframe = unique_train_df, task_type = 'valid'), valid_indices.indices)\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:29.175451Z","iopub.execute_input":"2025-11-08T16:29:29.17584Z","iopub.status.idle":"2025-11-08T16:29:29.191896Z","shell.execute_reply.started":"2025-11-08T16:29:29.175816Z","shell.execute_reply":"2025-11-08T16:29:29.191184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataloaders = {\n    'train': train_dataloader,\n    'valid': valid_dataloader\n}\n\nuse_cuda = torch.cuda.is_available()\ndevice = \"cuda\" if use_cuda else \"cpu\"\nmodel = models.resnet152(weights = None)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 5)\nmodel.load_state_dict(torch.load('/kaggle/input/img2biomass-hypertuned-resnet152/pytorch/v1/1/safetensor.pth', map_location = 'cpu'))\nmodel.to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:29.208212Z","iopub.execute_input":"2025-11-08T16:29:29.208422Z","iopub.status.idle":"2025-11-08T16:29:33.306333Z","shell.execute_reply.started":"2025-11-08T16:29:29.208403Z","shell.execute_reply":"2025-11-08T16:29:33.305688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_test_df = mf_dataframe(df = test_df, target_parameter = 'image_path', inference = True)\ndataset = Configure(dataframe = unique_test_df, task_type = 'test')\ntest_dataloader = DataLoader(dataset, batch_size = 1, num_workers = NUM_WORKERS, shuffle = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:33.352209Z","iopub.execute_input":"2025-11-08T16:29:33.352457Z","iopub.status.idle":"2025-11-08T16:29:33.378939Z","shell.execute_reply.started":"2025-11-08T16:29:33.35244Z","shell.execute_reply":"2025-11-08T16:29:33.378126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(model):\n    model.eval()\n    submission_rows = []\n    with torch.no_grad():\n        for batch_idx, images in enumerate(tqdm(test_dataloader)):\n            images = images.to(device)\n            outputs = model(images).squeeze().cpu().numpy()\n            name = unique_test_df.iloc[batch_idx].path.split('test/')[1].split('.jpg')[0].strip()\n            suffixes = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n            for i, suffix in enumerate(suffixes):\n                submission_rows.append({\n                    'sample_id': f'{name}__{suffix}'.strip(),\n                    'target': float(outputs[i])\n                })\n    \n    submission = pd.DataFrame(submission_rows, columns = ['sample_id', 'target'])\n    submission['sample_id'] = submission['sample_id'].str.strip()\n    submission['target'] = submission['target'].astype(float)\n    return submission\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:33.403805Z","iopub.execute_input":"2025-11-08T16:29:33.404094Z","iopub.status.idle":"2025-11-08T16:29:33.414694Z","shell.execute_reply.started":"2025-11-08T16:29:33.404077Z","shell.execute_reply":"2025-11-08T16:29:33.41397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_resnet = inference(model)\nsubmission_resnet.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:33.415364Z","iopub.execute_input":"2025-11-08T16:29:33.415601Z","iopub.status.idle":"2025-11-08T16:29:34.059741Z","shell.execute_reply.started":"2025-11-08T16:29:33.415578Z","shell.execute_reply":"2025-11-08T16:29:34.058966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_resnet.to_csv('submission.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:34.060516Z","iopub.execute_input":"2025-11-08T16:29:34.061261Z","iopub.status.idle":"2025-11-08T16:29:34.068619Z","shell.execute_reply.started":"2025-11-08T16:29:34.061239Z","shell.execute_reply":"2025-11-08T16:29:34.067906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_resnet.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:29:34.069396Z","iopub.execute_input":"2025-11-08T16:29:34.069595Z","iopub.status.idle":"2025-11-08T16:29:34.080366Z","shell.execute_reply.started":"2025-11-08T16:29:34.06958Z","shell.execute_reply":"2025-11-08T16:29:34.079695Z"}},"outputs":[],"execution_count":null}]}