{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":2645,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":1911,"modelId":257},{"sourceId":624620,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":470041,"modelId":485924},{"sourceId":624645,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":470059,"modelId":485942}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport os\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:23:51.196905Z","iopub.execute_input":"2025-10-30T20:23:51.197211Z","iopub.status.idle":"2025-10-30T20:24:16.08775Z","shell.execute_reply.started":"2025-10-30T20:23:51.197178Z","shell.execute_reply":"2025-10-30T20:24:16.086926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/csiro-biomass/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:24:16.08921Z","iopub.execute_input":"2025-10-30T20:24:16.089927Z","iopub.status.idle":"2025-10-30T20:24:16.123469Z","shell.execute_reply.started":"2025-10-30T20:24:16.089902Z","shell.execute_reply":"2025-10-30T20:24:16.122559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_wide = df_train.pivot_table(\n    index='image_path', \n    columns='target_name', \n    values='target'\n).reset_index()\n\n# Merge back with other features (take first occurrence for each image)\navailable_feature_cols = [col for col in ['Sampling_Date', 'State', 'Species', \n                                           'Pre_GSHH_NDVI', 'Height_Ave_cm'] \n                          if col in df_train.columns]\ndf_train_features = df_train.groupby('image_path')[available_feature_cols].first().reset_index()\ndf_train_wide = df_train_wide.merge(df_train_features, on='image_path')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:25:12.822771Z","iopub.execute_input":"2025-10-30T20:25:12.823112Z","iopub.status.idle":"2025-10-30T20:25:12.872663Z","shell.execute_reply.started":"2025-10-30T20:25:12.823087Z","shell.execute_reply":"2025-10-30T20:25:12.871805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_unique = df_test.drop_duplicates(subset=['image_path']).copy()\n\n# Get available features from test\navailable_test_features = [col for col in ['Sampling_Date', 'State', 'Species', \n                                            'Pre_GSHH_NDVI', 'Height_Ave_cm'] \n                           if col in df_test.columns]\n\nif available_test_features:\n    df_test_features = df_test.groupby('image_path')[available_test_features].first().reset_index()\n    df_test_unique = df_test_features\nelse:\n    # If no features in test.csv, keep only image_path\n    df_test_unique = df_test[['image_path']].drop_duplicates().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:25:15.852095Z","iopub.execute_input":"2025-10-30T20:25:15.853272Z","iopub.status.idle":"2025-10-30T20:25:15.861131Z","shell.execute_reply.started":"2025-10-30T20:25:15.853227Z","shell.execute_reply":"2025-10-30T20:25:15.860022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = EfficientNetB0(include_top=False, pooling='avg',weights=None)\nbase_model.load_weights('/kaggle/input/effnet-b0-keras/keras/default/1/model_weights/efficientnetb0_notop.h5')\n\ndef extract_features(img_path, base_path='/kaggle/input/csiro-biomass/'):\n    \"\"\"Extract features from an image using the CNN\"\"\"\n    try:\n        full_path = os.path.join(base_path, img_path)\n        img = image.load_img(full_path, target_size=(224, 224))\n        img_array = image.img_to_array(img)\n        img_array = np.expand_dims(img_array, axis=0)\n        img_array = preprocess_input(img_array)\n        features = base_model.predict(img_array, verbose=0)\n        return features.flatten()\n    except Exception as e:\n        print(f\"Error processing {img_path}: {e}\")\n        return np.zeros(1280)  # EfficientNetB0 outputs 1280 features\n\n# Extract CNN features for TRAIN\nprint(\"\\nExtracting CNN features from TRAIN images...\")\ntrain_cnn_features = []\nfor img_path in tqdm(df_train_wide['image_path']):\n    features = extract_features(img_path)\n    train_cnn_features.append(features)\ntrain_cnn_features = np.array(train_cnn_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:25:29.032243Z","iopub.execute_input":"2025-10-30T20:25:29.033447Z","iopub.status.idle":"2025-10-30T20:25:37.777344Z","shell.execute_reply.started":"2025-10-30T20:25:29.033397Z","shell.execute_reply":"2025-10-30T20:25:37.776003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train CNN features shape: {train_cnn_features.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:25:17.947171Z","iopub.status.idle":"2025-10-30T20:25:17.947444Z","shell.execute_reply.started":"2025-10-30T20:25:17.947317Z","shell.execute_reply":"2025-10-30T20:25:17.947328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nExtracting CNN features from TEST images...\")\ntest_cnn_features = []\nfor img_path in tqdm(df_test_unique['image_path']):\n    features = extract_features(img_path)\n    test_cnn_features.append(features)\ntest_cnn_features = np.array(test_cnn_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tabular_features = []\n\n# Process State if available\nif 'State' in df_train_wide.columns:\n    le_state = LabelEncoder()\n    # Fit on train, transform both\n    df_train_wide['State_encoded'] = le_state.fit_transform(df_train_wide['State'])\n    if 'State' in df_test_unique.columns:\n        # Handle unseen states in test\n        df_test_unique['State_encoded'] = df_test_unique['State'].apply(\n            lambda x: le_state.transform([x])[0] if x in le_state.classes_ else -1\n        )\n    else:\n        df_test_unique['State_encoded'] = -1\n    tabular_features.append('State_encoded')\n\n# Process Species if available\nif 'Species' in df_train_wide.columns:\n    le_species = LabelEncoder()\n    df_train_wide['Species_encoded'] = le_species.fit_transform(df_train_wide['Species'])\n    if 'Species' in df_test_unique.columns:\n        df_test_unique['Species_encoded'] = df_test_unique['Species'].apply(\n            lambda x: le_species.transform([x])[0] if x in le_species.classes_ else -1\n        )\n    else:\n        df_test_unique['Species_encoded'] = -1\n    tabular_features.append('Species_encoded')\n\n# Process Date features if available\nif 'Sampling_Date' in df_train_wide.columns:\n    df_train_wide['Sampling_Date'] = pd.to_datetime(df_train_wide['Sampling_Date'])\n    df_train_wide['Year'] = df_train_wide['Sampling_Date'].dt.year\n    df_train_wide['Month'] = df_train_wide['Sampling_Date'].dt.month\n    df_train_wide['Day'] = df_train_wide['Sampling_Date'].dt.day\n    \n    if 'Sampling_Date' in df_test_unique.columns:\n        df_test_unique['Sampling_Date'] = pd.to_datetime(df_test_unique['Sampling_Date'])\n        df_test_unique['Year'] = df_test_unique['Sampling_Date'].dt.year\n        df_test_unique['Month'] = df_test_unique['Sampling_Date'].dt.month\n        df_test_unique['Day'] = df_test_unique['Sampling_Date'].dt.day\n    else:\n        # Use median values from train\n        df_test_unique['Year'] = df_train_wide['Year'].median()\n        df_test_unique['Month'] = df_train_wide['Month'].median()\n        df_test_unique['Day'] = df_train_wide['Day'].median()\n    \n    tabular_features.extend(['Year', 'Month', 'Day'])\n\n# Process numerical features\nfor num_col in ['Pre_GSHH_NDVI', 'Height_Ave_cm']:\n    if num_col in df_train_wide.columns:\n        # Fill missing values with median\n        train_median = df_train_wide[num_col].median()\n        df_train_wide[num_col] = df_train_wide[num_col].fillna(train_median)\n        \n        if num_col in df_test_unique.columns:\n            df_test_unique[num_col] = df_test_unique[num_col].fillna(train_median)\n        else:\n            df_test_unique[num_col] = train_median\n        \n        tabular_features.append(num_col)\n\nprint(f\"Tabular features: {tabular_features}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if tabular_features:\n    X_train_tabular = df_train_wide[tabular_features].values\n    X_train_combined = np.concatenate([X_train_tabular, train_cnn_features], axis=1)\nelse:\n    X_train_combined = train_cnn_features\n\n# Test features\nif tabular_features:\n    X_test_tabular = df_test_unique[tabular_features].values\n    X_test_combined = np.concatenate([X_test_tabular, test_cnn_features], axis=1)\nelse:\n    X_test_combined = test_cnn_features\n\nprint(f\"Train combined features shape: {X_train_combined.shape}\")\nprint(f\"Test combined features shape: {X_test_combined.shape}\")\n\n# Get target columns\nexclude_cols = ['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', \n                'Height_Ave_cm', 'State_encoded', 'Species_encoded', 'Year', 'Month', 'Day']\ntarget_cols = [col for col in df_train_wide.columns if col not in exclude_cols]\nprint(f\"Target columns: {target_cols}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nTraining LightGBM models...\")\n\n# Split train data for validation\nX_tr, X_val, y_tr_df, y_val_df = train_test_split(\n    X_train_combined, \n    df_train_wide[target_cols], \n    test_size=0.2, \n    random_state=42\n)\n\n# Store models and predictions\nmodels = {}\npredictions = {}\nmetrics = {}\n\nlgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'n_estimators': 500,\n    'random_state': 42\n}\n\nfor target_col in tqdm(target_cols, desc=\"Training models\"):\n    # Get target values\n    y_tr = y_tr_df[target_col].values\n    y_val = y_val_df[target_col].values\n    \n    # Train LightGBM model\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_val, y_val)],\n        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n    )\n    \n    # Store model\n    models[target_col] = model\n    \n    # Make predictions\n    y_pred = model.predict(X_val)\n    predictions[target_col] = y_pred\n    \n    # Calculate metrics\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    mae = mean_absolute_error(y_val, y_pred)\n    metrics[target_col] = {'RMSE': rmse, 'MAE': mae}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics_df = pd.DataFrame(metrics).T\nmetrics_df = metrics_df.sort_values('RMSE')\nprint(metrics_df.to_string())\n\nprint(f\"\\nAverage RMSE: {metrics_df['RMSE'].mean():.4f}\")\nprint(f\"Average MAE: {metrics_df['MAE'].mean():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = pd.DataFrame()\ntest_predictions['image_path'] = df_test_unique['image_path']\n\nfor target_col in tqdm(target_cols, desc=\"Predicting\"):\n    preds = models[target_col].predict(X_test_combined)\n    test_predictions[target_col] = preds\n\nprint(\"\\nTest predictions generated!\")\nprint(test_predictions.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = test_predictions.melt(\n    id_vars=['image_path'],\n    value_vars=target_cols,\n    var_name='target_name',\n    value_name='target'\n)\n\n# Create sample_id by combining image identifier and target_name\nsubmission['image_id'] = submission['image_path'].str.extract(r'/(ID\\d+)\\.')[0]\nsubmission['sample_id'] = submission['image_id'] + '__' + submission['target_name']\n\n# Select and reorder columns for final submission\nsubmission_final = submission[['sample_id', 'target']].copy()\n\n# Ensure predictions are non-negative (biomass can't be negative)\nsubmission_final['target'] = submission_final['target'].clip(lower=0)\n\nprint(\"\\nSubmission format:\")\nprint(submission_final.head(10))\nprint(f\"\\nTotal predictions: {len(submission_final)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_path = 'submission.csv'\nsubmission_final.to_csv(output_path, index=False)\nprint(f\"\\nâœ“ Submission saved to: {output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}