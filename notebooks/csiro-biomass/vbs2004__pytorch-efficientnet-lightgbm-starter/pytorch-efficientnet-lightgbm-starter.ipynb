{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":3729,"sourceType":"modelInstanceVersion","modelInstanceId":2656,"modelId":312},{"sourceId":624915,"sourceType":"modelInstanceVersion","modelInstanceId":470279,"modelId":486172}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:22.82475Z","iopub.execute_input":"2025-10-31T07:00:22.825047Z","iopub.status.idle":"2025-10-31T07:00:28.443612Z","shell.execute_reply.started":"2025-10-31T07:00:22.825017Z","shell.execute_reply":"2025-10-31T07:00:28.442824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ndf_train = pd.read_csv(\"/kaggle/input/csiro-biomass/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")\n\n# Pivot train data to wide format\ndf_train_wide = df_train.pivot_table(\n    index='image_path', \n    columns='target_name', \n    values='target'\n).reset_index()\n\n# Merge back with other features (take first occurrence for each image)\navailable_feature_cols = [col for col in ['Sampling_Date', 'State', 'Species', \n                                           'Pre_GSHH_NDVI', 'Height_Ave_cm'] \n                          if col in df_train.columns]\ndf_train_features = df_train.groupby('image_path')[available_feature_cols].first().reset_index()\ndf_train_wide = df_train_wide.merge(df_train_features, on='image_path')\n\n# Process test data\ndf_test_unique = df_test.drop_duplicates(subset=['image_path']).copy()\n\n# Get available features from test\navailable_test_features = [col for col in ['Sampling_Date', 'State', 'Species', \n                                            'Pre_GSHH_NDVI', 'Height_Ave_cm'] \n                           if col in df_test.columns]\n\nif available_test_features:\n    df_test_features = df_test.groupby('image_path')[available_test_features].first().reset_index()\n    df_test_unique = df_test_features\nelse:\n    # If no features in test.csv, keep only image_path\n    df_test_unique = df_test[['image_path']].drop_duplicates().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:28.445394Z","iopub.execute_input":"2025-10-31T07:00:28.445908Z","iopub.status.idle":"2025-10-31T07:00:28.473904Z","shell.execute_reply.started":"2025-10-31T07:00:28.445887Z","shell.execute_reply":"2025-10-31T07:00:28.473354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load EfficientNet-B0 model (PyTorch version)\nbase_model = models.efficientnet_b0(weights=None)\n#torch.save(base_model.state_dict(), \"efficientnet_b0_imagenet_pytorch.pth\")\n\nbase_model.load_state_dict(torch.load(\"/kaggle/input/efficientnet-b0-imagenet-pytorch/pytorch/default/2/efficientnet_b0_imagenet_pytorch.pth\",weights_only=True))\n# Remove the classification head to get features\nbase_model = nn.Sequential(*list(base_model.children())[:-1])  # Remove classifier\nbase_model.eval()\nbase_model.to(device)\n\n# Define image preprocessing transforms\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:28.474501Z","iopub.execute_input":"2025-10-31T07:00:28.474705Z","iopub.status.idle":"2025-10-31T07:00:28.770991Z","shell.execute_reply.started":"2025-10-31T07:00:28.474688Z","shell.execute_reply":"2025-10-31T07:00:28.770331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features(img_path, base_path='/kaggle/input/csiro-biomass/'):\n    \"\"\"Extract features from an image using the CNN\"\"\"\n    try:\n        full_path = os.path.join(base_path, img_path)\n        img = Image.open(full_path).convert('RGB')\n        img_tensor = preprocess(img).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            features = base_model(img_tensor)\n            features = features.squeeze()  # Remove batch and spatial dimensions\n            \n        return features.cpu().numpy().flatten()\n    except Exception as e:\n        print(f\"Error processing {img_path}: {e}\")\n        return np.zeros(1280)  # EfficientNetB0 outputs 1280 features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:28.771772Z","iopub.execute_input":"2025-10-31T07:00:28.772037Z","iopub.status.idle":"2025-10-31T07:00:28.777402Z","shell.execute_reply.started":"2025-10-31T07:00:28.772009Z","shell.execute_reply":"2025-10-31T07:00:28.776713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract CNN features for TRAIN\nprint(\"\\nExtracting CNN features from TRAIN images...\")\ntrain_cnn_features = []\nfor img_path in tqdm(df_train_wide['image_path']):\n    features = extract_features(img_path)\n    train_cnn_features.append(features)\ntrain_cnn_features = np.array(train_cnn_features)\nprint(f\"Train CNN features shape: {train_cnn_features.shape}\")\n\n# Extract CNN features for TEST\nprint(\"\\nExtracting CNN features from TEST images...\")\ntest_cnn_features = []\nfor img_path in tqdm(df_test_unique['image_path']):\n    features = extract_features(img_path)\n    test_cnn_features.append(features)\ntest_cnn_features = np.array(test_cnn_features)\nprint(f\"Test CNN features shape: {test_cnn_features.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:28.77812Z","iopub.execute_input":"2025-10-31T07:00:28.778419Z","iopub.status.idle":"2025-10-31T07:00:53.582069Z","shell.execute_reply.started":"2025-10-31T07:00:28.778396Z","shell.execute_reply":"2025-10-31T07:00:53.58105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process tabular features\ntabular_features = []\n\nle_state = LabelEncoder()\n\ndf_train_wide['State_encoded'] = le_state.fit_transform(df_train_wide['State'])\ndf_test_unique['State_encoded'] = -1\n\ntabular_features.append('State_encoded')\n\n\nle_species = LabelEncoder()\ndf_train_wide['Species_encoded'] = le_species.fit_transform(df_train_wide['Species'])\ndf_test_unique['Species_encoded'] = -1\ntabular_features.append('Species_encoded')\n\n\nfor num_col in ['Pre_GSHH_NDVI', 'Height_Ave_cm']:\n        train_median = df_train_wide[num_col].median()\n        df_test_unique[num_col] = train_median\n        \n        tabular_features.append(num_col)\n\nprint(f\"Tabular features: {tabular_features}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:53.582818Z","iopub.execute_input":"2025-10-31T07:00:53.583033Z","iopub.status.idle":"2025-10-31T07:00:53.592616Z","shell.execute_reply.started":"2025-10-31T07:00:53.583015Z","shell.execute_reply":"2025-10-31T07:00:53.591983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine features\nX_train_tabular = df_train_wide[tabular_features].values\nX_train_combined = np.concatenate([X_train_tabular, train_cnn_features], axis=1)\n\n\nX_test_tabular = df_test_unique[tabular_features].values\nX_test_combined = np.concatenate([X_test_tabular, test_cnn_features], axis=1)\n\nprint(f\"Train combined features shape: {X_train_combined.shape}\")\nprint(f\"Test combined features shape: {X_test_combined.shape}\")\n\n# Get target columns\nexclude_cols = ['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', \n                'Height_Ave_cm', 'State_encoded', 'Species_encoded', 'Year', 'Month', 'Day']\ntarget_cols = [col for col in df_train_wide.columns if col not in exclude_cols]\nprint(f\"Target columns: {target_cols}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:53.593349Z","iopub.execute_input":"2025-10-31T07:00:53.593569Z","iopub.status.idle":"2025-10-31T07:00:53.617161Z","shell.execute_reply.started":"2025-10-31T07:00:53.593553Z","shell.execute_reply":"2025-10-31T07:00:53.616291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nTraining LightGBM models with K-Fold Cross Validation...\")\n\n# K-Fold setup\nN_FOLDS = 5\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n\n# Store models and predictions\nmodels = {}  # Will store list of models for each target\noof_predictions = {}  # Out-of-fold predictions for validation\ntest_predictions_folds = {}  # Test predictions from each fold\nmetrics = {}\n\nlgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'n_estimators': 500,\n    'random_state': 42\n}\n\nfor target_col in tqdm(target_cols, desc=\"Training models\"):\n    print(f\"\\nTraining {target_col}...\")\n    \n    # Initialize storage for this target\n    models[target_col] = []\n    oof_preds = np.zeros(len(X_train_combined))\n    test_preds = np.zeros(len(X_test_combined))\n    \n    # Get target values\n    y_train = df_train_wide[target_col].values\n    \n    # K-Fold cross validation\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_combined), 1):\n        X_tr, X_val = X_train_combined[train_idx], X_train_combined[val_idx]\n        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n        \n        # Train LightGBM model\n        model = lgb.LGBMRegressor(**lgb_params)\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_val, y_val)],\n            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n        )\n        \n        # Store model\n        models[target_col].append(model)\n        \n        # Out-of-fold predictions\n        oof_preds[val_idx] = model.predict(X_val)\n        \n        # Test predictions (will be averaged later)\n        test_preds += model.predict(X_test_combined) / N_FOLDS\n        \n        # Fold metrics\n        fold_rmse = np.sqrt(mean_squared_error(y_val, oof_preds[val_idx]))\n        fold_mae = mean_absolute_error(y_val, oof_preds[val_idx])\n        print(f\"  Fold {fold} - RMSE: {fold_rmse:.4f}, MAE: {fold_mae:.4f}\")\n    \n    # Store predictions\n    oof_predictions[target_col] = oof_preds\n    test_predictions_folds[target_col] = test_preds\n    \n    # Calculate overall CV metrics\n    cv_rmse = np.sqrt(mean_squared_error(y_train, oof_preds))\n    cv_mae = mean_absolute_error(y_train, oof_preds)\n    metrics[target_col] = {'CV_RMSE': cv_rmse, 'CV_MAE': cv_mae}\n    print(f\"  Overall CV - RMSE: {cv_rmse:.4f}, MAE: {cv_mae:.4f}\")\n\nmetrics_df = pd.DataFrame(metrics).T\nmetrics_df = metrics_df.sort_values('CV_RMSE')\nprint(\"\\n\" + \"=\"*60)\nprint(\"Cross-Validation Results:\")\nprint(\"=\"*60)\nprint(metrics_df.to_string())\n\nprint(f\"\\nAverage CV RMSE: {metrics_df['CV_RMSE'].mean():.4f}\")\nprint(f\"Average CV MAE: {metrics_df['CV_MAE'].mean():.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:00:53.619446Z","iopub.execute_input":"2025-10-31T07:00:53.619843Z","iopub.status.idle":"2025-10-31T07:01:31.143496Z","shell.execute_reply.started":"2025-10-31T07:00:53.619823Z","shell.execute_reply":"2025-10-31T07:01:31.142683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate test predictions (already computed during k-fold)\ntest_predictions = pd.DataFrame()\ntest_predictions['image_path'] = df_test_unique['image_path']\n\nfor target_col in target_cols:\n    test_predictions[target_col] = test_predictions_folds[target_col]\n\nprint(\"\\nTest predictions generated!\")\nprint(test_predictions.head())\n\n# Create submission\nsubmission = test_predictions.melt(\n    id_vars=['image_path'],\n    value_vars=target_cols,\n    var_name='target_name',\n    value_name='target'\n)\n\n# Create sample_id by combining image identifier and target_name\nsubmission['image_id'] = submission['image_path'].str.extract(r'/(ID\\d+)\\.')[0]\nsubmission['sample_id'] = submission['image_id'] + '__' + submission['target_name']\n\n# Select and reorder columns for final submission\nsubmission_final = submission[['sample_id', 'target']].copy()\n\n# Ensure predictions are non-negative (biomass can't be negative)\nsubmission_final['target'] = submission_final['target'].clip(lower=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:01:31.144297Z","iopub.execute_input":"2025-10-31T07:01:31.1446Z","iopub.status.idle":"2025-10-31T07:01:31.163068Z","shell.execute_reply.started":"2025-10-31T07:01:31.144573Z","shell.execute_reply":"2025-10-31T07:01:31.162229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nSubmission format:\")\nprint(submission_final.head(10))\nprint(f\"\\nTotal predictions: {len(submission_final)}\")\n\noutput_path = 'submission.csv'\nsubmission_final.to_csv(output_path, index=False)\nprint(f\"\\nâœ“ Submission saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T07:01:31.163784Z","iopub.execute_input":"2025-10-31T07:01:31.163977Z","iopub.status.idle":"2025-10-31T07:01:31.189004Z","shell.execute_reply.started":"2025-10-31T07:01:31.163963Z","shell.execute_reply":"2025-10-31T07:01:31.18832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}