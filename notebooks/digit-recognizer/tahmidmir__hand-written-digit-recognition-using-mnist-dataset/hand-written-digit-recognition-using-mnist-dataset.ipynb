{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport math\nimport cv2 as cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\n\n#import keras\nfrom keras import models\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import LeakyReLU\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.utils import Sequence, to_categorical\n#from keras import utils as np_utils\n\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau,TensorBoard, Callback\nfrom keras.optimizers import Adam, Optimizer\nimport keras.backend as K\n\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom sklearn.utils import class_weight\n\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\n\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport random\n\n%matplotlib inline\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:35.434892Z","iopub.execute_input":"2024-06-09T15:36:35.435687Z","iopub.status.idle":"2024-06-09T15:36:35.450878Z","shell.execute_reply.started":"2024-06-09T15:36:35.435644Z","shell.execute_reply":"2024-06-09T15:36:35.449795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****1. We will Implement a basic Convolutional Neural Networks(CNN) using TensorFlow 2.x Keras API. The dataset that we will work it is the MNIST dataset.**\n\n****   To prepare our notebook, run the next cell to import the necessary packages and change the accelerator from ```None``` to ```GPU```.****","metadata":{}},{"cell_type":"code","source":"# LOAD LIBRARIES\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\n#from keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\nfrom keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import datasets, layers, models\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau\n# GLOBAL VARIABLES\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)\nstyles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:35.452559Z","iopub.execute_input":"2024-06-09T15:36:35.452861Z","iopub.status.idle":"2024-06-09T15:36:35.467841Z","shell.execute_reply.started":"2024-06-09T15:36:35.452837Z","shell.execute_reply":"2024-06-09T15:36:35.466978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:35.469083Z","iopub.execute_input":"2024-06-09T15:36:35.469612Z","iopub.status.idle":"2024-06-09T15:36:35.4841Z","shell.execute_reply.started":"2024-06-09T15:36:35.469584Z","shell.execute_reply":"2024-06-09T15:36:35.483152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preprocessing\n\n*1. **Before building any ML model, it is important to preprocess the data. In fact, data preprocessing will generally take up the most time in any ML pipeline. The following module goes over the steps to preprocess the MNIST dataset for our purposes.**","metadata":{}},{"cell_type":"markdown","source":"****## 2.1 Load Data\n\nOur first step is to load the data and divide it into a training and testing dataset. The MNIST dataset can be downloaded directly from TensorFlow and has already been divided. Run the next cell to import the data.\n\n``` x_train ``` is the dataset of 28x28 images of handwritten digits that the model will be trained on.\n\n```y_train``` is the dataset of labels that correspond to ```x_train```. \n\n``` x_test ``` is the dataset of 28x28 images of handwritten digits that the model will be tested on.\n\n```y_test``` is the dataset of labels that correspond to ```x_test```. ********","metadata":{}},{"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:35.485281Z","iopub.execute_input":"2024-06-09T15:36:35.485552Z","iopub.status.idle":"2024-06-09T15:36:35.754643Z","shell.execute_reply.started":"2024-06-09T15:36:35.485527Z","shell.execute_reply":"2024-06-09T15:36:35.753871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run the following code to see the counts of each digit present in our training dataset.**","metadata":{}},{"cell_type":"code","source":"# show image of training data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (10, 10)) # set size of figure 10x10\nrand_indexes = np.random.randint(0, x_train.shape[0], 8) # select 8 digits(0~9) randomly \nprint(rand_indexes)\n\nfor index,im_index in enumerate(rand_indexes):\n    plt.subplot(4, 4, index+1)\n    plt.imshow(x_train[im_index], cmap = 'gray', interpolation = 'none')\n    plt.title('Class %d' % y_train[im_index])\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:35.756995Z","iopub.execute_input":"2024-06-09T15:36:35.757345Z","iopub.status.idle":"2024-06-09T15:36:37.984414Z","shell.execute_reply.started":"2024-06-09T15:36:35.757316Z","shell.execute_reply":"2024-06-09T15:36:37.983461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:37.985863Z","iopub.execute_input":"2024-06-09T15:36:37.986653Z","iopub.status.idle":"2024-06-09T15:36:38.202181Z","shell.execute_reply.started":"2024-06-09T15:36:37.986615Z","shell.execute_reply":"2024-06-09T15:36:38.201225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are similar counts for each digit. This is good as the model will have \nenough images for each class to train the features for each class. There is no need to downsample or upweigh.**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Check for NaN Values\n","metadata":{}},{"cell_type":"code","source":"np.isnan(x_train).any()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.203457Z","iopub.execute_input":"2024-06-09T15:36:38.203799Z","iopub.status.idle":"2024-06-09T15:36:38.225555Z","shell.execute_reply.started":"2024-06-09T15:36:38.203764Z","shell.execute_reply":"2024-06-09T15:36:38.224534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.isnan(x_test).any()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.226967Z","iopub.execute_input":"2024-06-09T15:36:38.227299Z","iopub.status.idle":"2024-06-09T15:36:38.234859Z","shell.execute_reply.started":"2024-06-09T15:36:38.227259Z","shell.execute_reply":"2024-06-09T15:36:38.233704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no NaN values in our dataset. There is no need to preprocess the data to deal with Nan's.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Normalization and Reshaping\n\nSince the values in our ```x_train``` dataset are 28x28 images, our input shape must be specified so that our model will know what is being inputed.\n\nThe first convolution layer expects a single 60000x28x28x1 tensor instead of 60000 28x28x1 tensors.\n\nModels generally run better on normalized values. The best way to normalize the data depends on each individual dataset. For the MNIST dataset, we want each value to be between 0.0 and 1.0. As all values originally fall under the 0.0-255.0 range, divide by 255.0.\n\nRun the following cell to define the ```input_shape``` and to normalize and reshape the data.","metadata":{}},{"cell_type":"code","source":"input_shape = (28, 28, 1)\n\nx_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\nx_train=x_train / 255.0\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\nx_test=x_test/255.0","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.236164Z","iopub.execute_input":"2024-06-09T15:36:38.236448Z","iopub.status.idle":"2024-06-09T15:36:38.384596Z","shell.execute_reply.started":"2024-06-09T15:36:38.236423Z","shell.execute_reply":"2024-06-09T15:36:38.383774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Label Encoding\n\nThe labels for the training and the testing dataset are currently categorical and is not continuous. To include categorical dataset in our model, our labels should be converted to one-hot encodings.\n\nFor example, ```2``` becomes ```[0,0,1,0,0,0,0,0,0,0]``` and ```7``` becomes ```[0,0,0,0,0,0,0,1,0,0]```.\n\nRun the following cell to transform the labels into one-hot encodings","metadata":{}},{"cell_type":"code","source":"y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\ny_test = tf.one_hot(y_test.astype(np.int32), depth=10)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.385559Z","iopub.execute_input":"2024-06-09T15:36:38.38582Z","iopub.status.idle":"2024-06-09T15:36:38.392782Z","shell.execute_reply.started":"2024-06-09T15:36:38.385797Z","shell.execute_reply":"2024-06-09T15:36:38.391779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 Visualize Data\n\nRun the following cell to visualize an image in our dataset.","metadata":{}},{"cell_type":"code","source":"plt.imshow(x_train[100][:,:,0])\nprint(y_train[100])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.394161Z","iopub.execute_input":"2024-06-09T15:36:38.394603Z","iopub.status.idle":"2024-06-09T15:36:38.667952Z","shell.execute_reply.started":"2024-06-09T15:36:38.39455Z","shell.execute_reply":"2024-06-09T15:36:38.66693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The image is an image of a handwritten ```5```. The one-hot encoding holds the value of ```5```.","metadata":{}},{"cell_type":"markdown","source":"# 3. CNN\n\nIn this module, we will build our CNN model.","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Define the Model\nRun the following cell to define ```batch_size```, ```num_classes```, and ```epochs```. Try changing the values and test how different values affect the accuracy of the CNN model.","metadata":{}},{"cell_type":"code","source":"batch_size = 64\nnum_classes = 10\nepochs = 5","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.669491Z","iopub.execute_input":"2024-06-09T15:36:38.670378Z","iopub.status.idle":"2024-06-09T15:36:38.674883Z","shell.execute_reply.started":"2024-06-09T15:36:38.670339Z","shell.execute_reply":"2024-06-09T15:36:38.673896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the following cell to build the model. The model contains various layers stacked on top of each other. The output of one layer feeds into the input of the next layer.\n\nConv2D layers are convolutions. Each filter (32 in the first two convolution layers and 64 in the next two convolution layers) transforms a part of the image (5x5 for the first two Conv2D layers and 3x3 for the next two Conv2D layers). The transformation is applied on the whole image.\n\nMaxPool2D is a downsampling filter. It reduces a 2x2 matrix of the image to a single pixel with the maximum value of the 2x2 matrix. The filter aims to conserve the main features of the image while reducing the size.\n\n\n```relu``` is the rectifier, and it is used to find nonlinearity in the data. It works by returning the input value if the input value >= 0. If the input is negative, it returns 0.\n\nFlatten converts the tensors into a 1D vector.\n\nThe Dense layers are an artificial neural network. The last layer returns the probability that an image is in each class (one for each digit).\n\nAs this model aims to categorize the images, we will use a ```categorical_crossentropy``` loss function. ","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape),\n    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(strides=(2,2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.676196Z","iopub.execute_input":"2024-06-09T15:36:38.676499Z","iopub.status.idle":"2024-06-09T15:36:38.741876Z","shell.execute_reply.started":"2024-06-09T15:36:38.676474Z","shell.execute_reply":"2024-06-09T15:36:38.740913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Fit the Training Data\n\nThe next step is to fit our training data. If we achieve a certain level of accuracy, it may not be necessary to continue training the model, especially if time and resources are limited.\n\nThe following cell defines a CallBack so that if 99.5% accuracy is achieved, the model stops training.","metadata":{}},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('acc')>0.995):\n      print(\"\\nReached 99.5% accuracy so cancelling training!\")\n      self.model.stop_training = True\n\ncallbacks = myCallback()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.746064Z","iopub.execute_input":"2024-06-09T15:36:38.746338Z","iopub.status.idle":"2024-06-09T15:36:38.751664Z","shell.execute_reply.started":"2024-06-09T15:36:38.746314Z","shell.execute_reply":"2024-06-09T15:36:38.750813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing the model on a validation dataset prevents overfitting of the data. We specified a 10% validation and 90% training split.","metadata":{}},{"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.1,\n                    callbacks=[callbacks])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:36:38.753045Z","iopub.execute_input":"2024-06-09T15:36:38.753421Z","iopub.status.idle":"2024-06-09T15:37:03.482365Z","shell.execute_reply.started":"2024-06-09T15:36:38.753363Z","shell.execute_reply":"2024-06-09T15:37:03.481549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluate the Model\nEvaluate the model on the test set and analyze the accuracy\n","metadata":{}},{"cell_type":"code","source":"#test model\npreds = model.predict(x_test[0].reshape(-1,28,28,1))\nprint(int(np.argmax(preds)))","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:03.483664Z","iopub.execute_input":"2024-06-09T15:37:03.483988Z","iopub.status.idle":"2024-06-09T15:37:03.930862Z","shell.execute_reply.started":"2024-06-09T15:37:03.483962Z","shell.execute_reply":"2024-06-09T15:37:03.929909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate Model\nscore = model.evaluate(x_test, y_test)\nprint('Test loss: ', score[0])\nprint('Test accuracy: ', score[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:03.931935Z","iopub.execute_input":"2024-06-09T15:37:03.932202Z","iopub.status.idle":"2024-06-09T15:37:05.18719Z","shell.execute_reply.started":"2024-06-09T15:37:03.932178Z","shell.execute_reply":"2024-06-09T15:37:05.18628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(x_test[100][:,:,0])\nprint(y_test[100])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:05.188321Z","iopub.execute_input":"2024-06-09T15:37:05.188595Z","iopub.status.idle":"2024-06-09T15:37:05.398225Z","shell.execute_reply.started":"2024-06-09T15:37:05.188559Z","shell.execute_reply":"2024-06-09T15:37:05.397191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on test data\ntest_loss, test_acc = model.evaluate(x_test, y_test,verbose=2)\nprint(\"Test Accuracy: \", test_acc)\nprint(\"Test loss:\", test_loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:05.399428Z","iopub.execute_input":"2024-06-09T15:37:05.399785Z","iopub.status.idle":"2024-06-09T15:37:06.123869Z","shell.execute_reply.started":"2024-06-09T15:37:05.399757Z","shell.execute_reply":"2024-06-09T15:37:06.122899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1 Loss and Accuracy Curves\nRun the following cell to evaluate the loss and accuracy of our model.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training Accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation Accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:06.124971Z","iopub.execute_input":"2024-06-09T15:37:06.125277Z","iopub.status.idle":"2024-06-09T15:37:06.641766Z","shell.execute_reply.started":"2024-06-09T15:37:06.125239Z","shell.execute_reply":"2024-06-09T15:37:06.640784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy increases over time and the loss decreases over time. However, the accuracy of our validation set seems to slightly decrease towards the end even thought our training accuracy increased. Running the model for more epochs might cause our model to be susceptible to overfitting.","metadata":{}},{"cell_type":"markdown","source":"Identify any overfitting and take measures for overfitting (e.g. add dropout layer) and plot training vs testing accuracy and loss.\n\nDropout is a regularization layer. In our model, 25% of the nodes in the layer are randomly ignores, allowing the network to learn different features. This prevents overfitting.\n","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape),\n    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(strides=(2,2)),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:06.643111Z","iopub.execute_input":"2024-06-09T15:37:06.6435Z","iopub.status.idle":"2024-06-09T15:37:06.716256Z","shell.execute_reply.started":"2024-06-09T15:37:06.643463Z","shell.execute_reply":"2024-06-09T15:37:06.715412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('acc')>0.995):\n      print(\"\\nReached 99.5% accuracy so cancelling training!\")\n      self.model.stop_training = True\n\ncallbacks = myCallback()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:06.71744Z","iopub.execute_input":"2024-06-09T15:37:06.717783Z","iopub.status.idle":"2024-06-09T15:37:06.724623Z","shell.execute_reply.started":"2024-06-09T15:37:06.71773Z","shell.execute_reply":"2024-06-09T15:37:06.723797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the following cell to evaluate the loss and accuracy of our model.","metadata":{}},{"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.1,\n                    callbacks=[callbacks])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:06.725687Z","iopub.execute_input":"2024-06-09T15:37:06.72597Z","iopub.status.idle":"2024-06-09T15:37:38.140723Z","shell.execute_reply.started":"2024-06-09T15:37:06.725945Z","shell.execute_reply":"2024-06-09T15:37:38.13977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot training vs validation accuracy and loss \n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"Test Loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training Accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Test Accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:38.14226Z","iopub.execute_input":"2024-06-09T15:37:38.142582Z","iopub.status.idle":"2024-06-09T15:37:38.604019Z","shell.execute_reply.started":"2024-06-09T15:37:38.142554Z","shell.execute_reply":"2024-06-09T15:37:38.602999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Predict Results","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:38.605241Z","iopub.execute_input":"2024-06-09T15:37:38.605619Z","iopub.status.idle":"2024-06-09T15:37:39.995063Z","shell.execute_reply.started":"2024-06-09T15:37:38.605573Z","shell.execute_reply":"2024-06-09T15:37:39.994118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"::Our model runs pretty well, with an accuracy of 99.0% on our testing data.\n","metadata":{}},{"cell_type":"markdown","source":"## 4.3 Confusion Matrix\n\nRun the following cell to compute our confusion matrix using TensorFlow.","metadata":{}},{"cell_type":"code","source":"# Predict the values from the testing dataset\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert testing observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1)\n# compute the confusion matrix\nconfusion_mtx = tf.math.confusion_matrix(Y_true, Y_pred_classes) ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:39.99649Z","iopub.execute_input":"2024-06-09T15:37:39.996814Z","iopub.status.idle":"2024-06-09T15:37:41.232296Z","shell.execute_reply.started":"2024-06-09T15:37:39.996786Z","shell.execute_reply":"2024-06-09T15:37:41.231172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx, annot=True, fmt='g')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:41.233481Z","iopub.execute_input":"2024-06-09T15:37:41.233771Z","iopub.status.idle":"2024-06-09T15:37:41.890652Z","shell.execute_reply.started":"2024-06-09T15:37:41.233733Z","shell.execute_reply":"2024-06-09T15:37:41.889598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be a slightly higher confusion between (0,6) and (4,9). This is reasonable as 0's and 6's look similar with their loops and 4's and 9's can be mistaken when the 4's are more rounded and 9's are more angular.","metadata":{}},{"cell_type":"code","source":"loss, acc = model.evaluate(x_test, y_test, verbose=0)\nprint('Accuracy: %.3f'  % acc)\nprint('Loss: %.3f' % loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:41.891907Z","iopub.execute_input":"2024-06-09T15:37:41.89219Z","iopub.status.idle":"2024-06-09T15:37:42.550896Z","shell.execute_reply.started":"2024-06-09T15:37:41.892165Z","shell.execute_reply":"2024-06-09T15:37:42.549896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Plot a few correct predicted samples and a few misclassified samples.\n# ","metadata":{}},{"cell_type":"code","source":"# Show some wrong results, and the difference between the predicted label and the real labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = x_test[errors]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:42.552595Z","iopub.execute_input":"2024-06-09T15:37:42.55327Z","iopub.status.idle":"2024-06-09T15:37:42.559073Z","shell.execute_reply.started":"2024-06-09T15:37:42.55323Z","shell.execute_reply":"2024-06-09T15:37:42.558069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:42.560211Z","iopub.execute_input":"2024-06-09T15:37:42.560565Z","iopub.status.idle":"2024-06-09T15:37:42.570262Z","shell.execute_reply.started":"2024-06-09T15:37:42.56053Z","shell.execute_reply":"2024-06-09T15:37:42.56904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:42.571521Z","iopub.execute_input":"2024-06-09T15:37:42.571868Z","iopub.status.idle":"2024-06-09T15:37:43.645558Z","shell.execute_reply.started":"2024-06-09T15:37:42.571836Z","shell.execute_reply":"2024-06-09T15:37:43.644681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the predictions for the test data\nY_pred = model.predict(x_test)\n#get the indices to be plotted\ny_true = np.argmax(y_test,axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:43.646827Z","iopub.execute_input":"2024-06-09T15:37:43.647117Z","iopub.status.idle":"2024-06-09T15:37:44.314027Z","shell.execute_reply.started":"2024-06-09T15:37:43.647091Z","shell.execute_reply":"2024-06-09T15:37:44.313109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = np.nonzero(Y_pred_classes==y_true)[0]\nmisclassified = np.nonzero(Y_pred_classes!=y_true)[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:44.315211Z","iopub.execute_input":"2024-06-09T15:37:44.31551Z","iopub.status.idle":"2024-06-09T15:37:44.320547Z","shell.execute_reply.started":"2024-06-09T15:37:44.315484Z","shell.execute_reply":"2024-06-09T15:37:44.319518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Misclassified predicted classes:\",misclassified.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:44.321785Z","iopub.execute_input":"2024-06-09T15:37:44.322076Z","iopub.status.idle":"2024-06-09T15:37:44.330241Z","shell.execute_reply.started":"2024-06-09T15:37:44.322051Z","shell.execute_reply":"2024-06-09T15:37:44.329348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Displaying some Correct predicted samples","metadata":{}},{"cell_type":"code","source":"def plot_images(data_index,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(5,5, figsize=(12,12))\n\n    for i, indx in enumerate(data_index[:25]):\n        ax[i//5, i%5].imshow(x_test[indx].reshape(28,28), cmap=cmap)\n        ax[i//5, i%5].axis('off')\n        ax[i//5, i%5].set_title(\"True:{}  Pred:{}\".format(y_true[indx],Y_pred_classes[indx]))\n    plt.show()    \n\nplot_images(correct, \"Greens\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:44.331273Z","iopub.execute_input":"2024-06-09T15:37:44.331533Z","iopub.status.idle":"2024-06-09T15:37:46.242197Z","shell.execute_reply.started":"2024-06-09T15:37:44.331509Z","shell.execute_reply":"2024-06-09T15:37:46.241235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Displaying some Misclassified predicted samples","metadata":{}},{"cell_type":"code","source":"plot_images(misclassified, \"Reds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T15:37:46.24345Z","iopub.execute_input":"2024-06-09T15:37:46.243812Z","iopub.status.idle":"2024-06-09T15:37:47.757803Z","shell.execute_reply.started":"2024-06-09T15:37:46.24378Z","shell.execute_reply":"2024-06-09T15:37:47.756793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thank you**","metadata":{}}]}