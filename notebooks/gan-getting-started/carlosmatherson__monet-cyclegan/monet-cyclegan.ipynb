{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CycleGAN for Generating Monet Painting from Photos\n\nUniversity of Colorado Boulder\n\nDTSA 5511 W5\n\n## 1. Introduction and Problem Description\n\nThis project aims to solve the Kaggle \"Monet Painting Dataset\" competition, where the goal is to generate Monet-style paintings using deep learning techniques. The challenge is to build a generative adversarial network (GAN) that generates 7,000-10,000 images in the style of Monet paintings. Evaluation is based on MiFID (Memorization-informed FrÃ©chet Inception Distance), which measures the quality and originality of the generated images. Lower MiFID scores indicate better quality.\n\n### Dataset Overview\nThe dataset contains 300 Monet paintings sized 256x256 and 7,028 photos sized 256x256. I will use TFRecords format as recommended by the competition organizers.\n\n## 2. Library Imports and Environment Setup","metadata":{"_uuid":"0b4bf5c9-f343-4274-b1bf-5cb6fff1d99a","_cell_guid":"5a8ca504-b95d-4406-9276-536b4b9d7a1b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Would you like to tune hyperparameters?\nfine_tuning = True","metadata":{"_uuid":"262dc0aa-f2d1-4425-b583-23e45a69dc8f","_cell_guid":"9cf8cb47-6795-4559-bb50-0f6a2fe2a438","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:16:22.588014Z","iopub.execute_input":"2025-04-20T06:16:22.588566Z","iopub.status.idle":"2025-04-20T06:16:22.591867Z","shell.execute_reply.started":"2025-04-20T06:16:22.588542Z","shell.execute_reply":"2025-04-20T06:16:22.591315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# need to set up tmp directory because of kaggle kernel limit of 500 files\nTMP_DIR = '../tmp'\nIMAGES_TMP_DIR = f'{TMP_DIR}/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:16:22.864527Z","iopub.execute_input":"2025-04-20T06:16:22.865019Z","iopub.status.idle":"2025-04-20T06:16:22.868256Z","shell.execute_reply.started":"2025-04-20T06:16:22.864999Z","shell.execute_reply":"2025-04-20T06:16:22.86762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NOTE: use GroupNormalization(groups=-1) instead of InstanceNormalization \n# from tensorflow_addons (deprecated) as suggested in tutorial notebook\n# USE: result.add(layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init))\nfrom tensorflow.keras import layers \n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport os\nimport time\nimport gc\nimport io\nimport shutil\n\nimport zipfile\nimport PIL\nfrom IPython.display import display, clear_output","metadata":{"_uuid":"4ce29346-87b8-43ba-b10f-1d2c69bf3a4f","_cell_guid":"3bbba716-37d3-4fcd-9472-035bb01de584","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:16:23.149473Z","iopub.execute_input":"2025-04-20T06:16:23.150021Z","iopub.status.idle":"2025-04-20T06:16:23.153891Z","shell.execute_reply.started":"2025-04-20T06:16:23.15Z","shell.execute_reply":"2025-04-20T06:16:23.153309Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_strategy():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        device = 'TPU'\n    except:\n        gpus = tf.config.list_physical_devices('GPU')\n        if gpus:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            strategy = tf.distribute.MirroredStrategy() if len(gpus) > 1 else tf.distribute.get_strategy()\n            device = f'{len(gpus)} GPU(s)'\n        else:\n            strategy = tf.distribute.get_strategy()\n            device = 'CPU'\n    print(f'Using {device} with {strategy.num_replicas_in_sync} replicas')\n    return strategy\n\nstrategy = setup_strategy()\nAUTOTUNE = tf.data.experimental.AUTOTUNE","metadata":{"_uuid":"90065b20-53d3-4da6-8714-060bad0c943d","_cell_guid":"7bae5f0d-5471-4d44-89a6-3d41c3d90d36","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:16:24.774069Z","iopub.execute_input":"2025-04-20T06:16:24.774565Z","iopub.status.idle":"2025-04-20T06:16:25.704442Z","shell.execute_reply.started":"2025-04-20T06:16:24.774544Z","shell.execute_reply":"2025-04-20T06:16:25.703714Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 3. Data Loading and Exploration","metadata":{"_uuid":"04e19892-223f-41b6-ad8e-6e6cee06eba0","_cell_guid":"7d9bcb42-19ae-4fbf-846a-7ba7ba72fc74","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 3.1 Load TFRecord Files","metadata":{"_uuid":"ddb8e780-6cc3-47c8-9185-76724b4254ec","_cell_guid":"017cac0e-8ace-4374-95f9-7d86feaad60f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"MONET_FILENAMES = tf.io.gfile.glob('/kaggle/input/gan-getting-started/monet_tfrec/*.tfrec')\nPHOTO_FILENAMES = tf.io.gfile.glob('/kaggle/input/gan-getting-started/photo_tfrec/*.tfrec')\n\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))","metadata":{"_uuid":"94926dd4-f85d-4289-8921-5744b8db56f7","_cell_guid":"cf59c992-f07a-4338-942e-ffc931e545cd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:16:25.705398Z","iopub.execute_input":"2025-04-20T06:16:25.705603Z","iopub.status.idle":"2025-04-20T06:16:25.740246Z","shell.execute_reply.started":"2025-04-20T06:16:25.705586Z","shell.execute_reply":"2025-04-20T06:16:25.739532Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_image(image):\n    \"\"\"Convert compressed image to float tensor with values in [-1, 1]\"\"\"\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    \"\"\"Extract image from TFRecord\"\"\"\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\ndef load_dataset(filenames, batch_size=1):\n    \"\"\"Load and prepare a TFRecord dataset\"\"\"\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","metadata":{"_uuid":"4acdac63-d90c-428d-8305-c3d8ebd28239","_cell_guid":"b0c721d6-2d27-4cd8-878c-2079de5cd93b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:16:27.930876Z","iopub.execute_input":"2025-04-20T06:16:27.931155Z","iopub.status.idle":"2025-04-20T06:16:27.937217Z","shell.execute_reply.started":"2025-04-20T06:16:27.931135Z","shell.execute_reply":"2025-04-20T06:16:27.936499Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\n\nBATCH_SIZE = 1\nmonet_ds = load_dataset(MONET_FILENAMES, BATCH_SIZE)\nphoto_ds = load_dataset(PHOTO_FILENAMES, BATCH_SIZE)","metadata":{"_uuid":"9ed75389-8368-4b54-ab96-2107471205f0","_cell_guid":"fa323576-4235-4aca-a9b4-da23f03b56ce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:16:29.658416Z","iopub.execute_input":"2025-04-20T06:16:29.658691Z","iopub.status.idle":"2025-04-20T06:16:29.942109Z","shell.execute_reply.started":"2025-04-20T06:16:29.65867Z","shell.execute_reply":"2025-04-20T06:16:29.941101Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 EDA","metadata":{"_uuid":"6010af0e-99b1-4128-bd1c-e08653f34ee3","_cell_guid":"7cde5e5e-a04a-42c6-be03-519e72375702","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"example_monet  = next(iter(monet_ds))\nexample_photo  = next(iter(photo_ds))","metadata":{"_uuid":"d057c12e-1326-4881-bef5-444d65837103","_cell_guid":"a595efa2-92ed-437b-8e68-b0aa8cf8a1d7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:16:30.251617Z","iopub.execute_input":"2025-04-20T06:16:30.252145Z","iopub.status.idle":"2025-04-20T06:16:30.367271Z","shell.execute_reply.started":"2025-04-20T06:16:30.252123Z","shell.execute_reply":"2025-04-20T06:16:30.366671Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_comparison(monet, photo):\n\n        plt.figure(figsize=(10, 10))\n        \n        plt.subplot(1, 2, 1)\n        plt.title('Monet Painting 1')\n        plt.imshow(example_monet[0] * 0.5 + 0.5)\n        \n        plt.subplot(1, 2, 2)\n        plt.title('Photo 1')\n        plt.imshow(example_photo[0] * 0.5 + 0.5)\n                \n        plt.show()","metadata":{"_uuid":"187ea33a-75d1-446a-9826-5527aa2c3834","_cell_guid":"53d4d83c-56af-4626-9586-b331085f835e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:16:31.736079Z","iopub.execute_input":"2025-04-20T06:16:31.736356Z","iopub.status.idle":"2025-04-20T06:16:31.740669Z","shell.execute_reply.started":"2025-04-20T06:16:31.736337Z","shell.execute_reply":"2025-04-20T06:16:31.740013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_comparison(example_monet, example_photo)","metadata":{"_uuid":"f90020ee-efd0-4856-90fb-38767b2093f0","_cell_guid":"7fd257b1-ef3a-4c50-ba8c-d27822e4a2b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:18:26.108378Z","iopub.execute_input":"2025-04-20T06:18:26.108653Z","iopub.status.idle":"2025-04-20T06:18:26.507922Z","shell.execute_reply.started":"2025-04-20T06:18:26.108634Z","shell.execute_reply":"2025-04-20T06:18:26.507207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_samples(dataset, title, n):\n    \n    plt.figure(figsize=(15, 5))\n    for i, img in enumerate(dataset.take(n)):\n        plt.subplot(1, n, i+1)\n        plt.title(f'{title} sample {i+1}')\n        plt.imshow(img[0] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","metadata":{"_uuid":"67fd9e6e-479c-47fb-aa0e-bc563f9cbe2d","_cell_guid":"5c194a80-1f64-4595-b10e-db522dc15d18","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:18:41.429464Z","iopub.execute_input":"2025-04-20T06:18:41.429741Z","iopub.status.idle":"2025-04-20T06:18:41.434464Z","shell.execute_reply.started":"2025-04-20T06:18:41.42972Z","shell.execute_reply":"2025-04-20T06:18:41.433657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_samples(dataset=monet_ds, title='Monet Painting', n=3)\nvisualize_samples(dataset=photo_ds, title='Photo', n=3)","metadata":{"_uuid":"041edff8-6596-4916-af38-58e8fc1eddff","_cell_guid":"4de9f972-66a3-4e1c-b411-5caabce35831","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:18:42.155237Z","iopub.execute_input":"2025-04-20T06:18:42.155501Z","iopub.status.idle":"2025-04-20T06:18:43.010578Z","shell.execute_reply.started":"2025-04-20T06:18:42.155481Z","shell.execute_reply":"2025-04-20T06:18:43.009874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_RGB_distribution(image, title):\n    # rescale from [-1,1] to [0,255]\n    img = ((image[0] * 0.5 + 0.5) * 255).numpy().astype(np.uint8)\n    \n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 4, 1)\n    plt.title(title)\n    plt.imshow(img)\n    \n    # histogram for each channel\n    colors = ['red', 'green', 'blue']\n    for i, color in enumerate(colors):\n        plt.subplot(1, 4, i+2)\n        plt.title(f'{color.capitalize()} Channel')\n        plt.hist(img[:,:,i].flatten(), bins=50, color=color, alpha=0.7)\n        plt.xlim([0, 255])\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"b509eb20-bcdd-4c4a-972b-74884775712f","_cell_guid":"4bdd12e5-eaba-4eab-b2c9-ee913efbd8d3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:18:43.657787Z","iopub.execute_input":"2025-04-20T06:18:43.658055Z","iopub.status.idle":"2025-04-20T06:18:43.663714Z","shell.execute_reply.started":"2025-04-20T06:18:43.658035Z","shell.execute_reply":"2025-04-20T06:18:43.662991Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_RGB_distribution(example_monet, 'Monet Painting Color Distribution')\nvisualize_RGB_distribution(example_photo, 'Photo Color Distribution')","metadata":{"_uuid":"9b38764f-7ef5-4a93-8406-076970e8e93a","_cell_guid":"8106b9c7-2c06-4af9-96d3-5092f1378088","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:18:45.240311Z","iopub.execute_input":"2025-04-20T06:18:45.241011Z","iopub.status.idle":"2025-04-20T06:18:46.77242Z","shell.execute_reply.started":"2025-04-20T06:18:45.24099Z","shell.execute_reply":"2025-04-20T06:18:46.771648Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3 EDA Findings and Insights\nLooking at the dataset, I see several key differences between Monet paintings and regular photographs.\n\nThe color palette used in the paintings is comprised of softer tones, especially blues and greens. The color histograms show that there is much grater variation in each color channel in the photographs compared to the paintings. I imagine this makes sense because the photographs can capture more variation in color than a painter could likley mix on his paint palette. Also, the natural colors of the earth aren't all exactly sold in aluminum squeeze tubes. Monet's style is characterized by visible brush strokes and textured surfaces that are not present in high-fidelity photos. Both the paintings and the photos feature landscapes, water scenes, and natural settings. As we would expect, the photographs contain sharper details, while the paintings have a blended and blurred appearance.\n\n## 4. Model Architecture\n\nI implement a CycleGAN architecture, which is effective for unpaired image-to-image translation. This model consists of two generators. There is a Monet generator that transforms photos into paintings, and there is a photo generator that turns paintings into photos. Two discriminators are also part of the architecture. The Monet discriminator differentiates real monet paintings from generated ones while the photo discriminators has a similar function for photos.\n\n### 4.1 Generator Architecture","metadata":{"_uuid":"da1526e9-8d0e-4f5b-aeb5-8fddd5c1a6a0","_cell_guid":"ec52756c-3f0c-46ac-99ab-370a76cf313f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3  # RGB images","metadata":{"_uuid":"9dba8bc9-f4c5-4dea-b325-6cee74b52239","_cell_guid":"3f83bd43-9948-4d3b-98ea-537213e3a8b4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:19:17.03794Z","iopub.execute_input":"2025-04-20T06:19:17.038254Z","iopub.status.idle":"2025-04-20T06:19:17.041929Z","shell.execute_reply.started":"2025-04-20T06:19:17.038229Z","shell.execute_reply":"2025-04-20T06:19:17.041205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def downsample(filters, size, apply_instancenorm=True):\n    \"\"\"Downsampling block for the generator\"\"\"\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init))\n\n    result.add(layers.LeakyReLU())\n\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n    \"\"\"Upsampling block for the generator\"\"\"\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result\n\ndef Generator():\n    \"\"\"U-Net based generator with skip connections\"\"\"\n    inputs = layers.Input(shape=[256, 256, 3])\n\n    # Downsampling path\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False),  # (bs, 128, 128, 64)\n        downsample(128, 4),  # (bs, 64, 64, 128)\n        downsample(256, 4),  # (bs, 32, 32, 256)\n        downsample(512, 4),  # (bs, 16, 16, 512)\n        downsample(512, 4),  # (bs, 8, 8, 512)\n        downsample(512, 4),  # (bs, 4, 4, 512)\n        downsample(512, 4),  # (bs, 2, 2, 512)\n        downsample(512, 4),  # (bs, 1, 1, 512)\n    ]\n\n    # Upsampling path\n    up_stack = [\n        upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n        upsample(512, 4),  # (bs, 16, 16, 1024)\n        upsample(256, 4),  # (bs, 32, 32, 512)\n        upsample(128, 4),  # (bs, 64, 64, 256)\n        upsample(64, 4),  # (bs, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                 strides=2,\n                                 padding='same',\n                                 kernel_initializer=initializer,\n                                 activation='tanh')  # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{"_uuid":"73b8ce9e-bbf9-42ad-be78-1970fc943d6c","_cell_guid":"4f94435e-9855-42a3-a1d1-e114dd91bfe6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:17.47113Z","iopub.execute_input":"2025-04-20T06:19:17.471429Z","iopub.status.idle":"2025-04-20T06:19:17.481384Z","shell.execute_reply.started":"2025-04-20T06:19:17.47141Z","shell.execute_reply":"2025-04-20T06:19:17.480602Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2 Discriminator Architecture","metadata":{"_uuid":"334aa36f-47d3-4878-bb1a-de28e0932b36","_cell_guid":"93e39181-7542-4fb9-94ff-97cb90a4ef29","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def Discriminator():\n    \"\"\"PatchGAN discriminator that classifies patches of an image as real or fake\"\"\"\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n\n    # Downsampling layers\n    down1 = downsample(64, 4, apply_instancenorm=False)(x)  # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1)  # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2)  # (bs, 32, 32, 256)\n\n    # Additional layers\n    zero_pad1 = layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                        kernel_initializer=initializer,\n                        use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n\n    norm1 = layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init)(conv)\n    leaky_relu = layers.LeakyReLU()(norm1)\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n\n    # Output layer - no activation to use with BCE loss\n    last = layers.Conv2D(1, 4, strides=1,\n                        kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"_uuid":"4b785ce4-7796-470e-9aa0-d5757e8d25a2","_cell_guid":"b5626b1e-055b-4cdc-8cb9-682e1fd82d2d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:19:20.099294Z","iopub.execute_input":"2025-04-20T06:19:20.099561Z","iopub.status.idle":"2025-04-20T06:19:20.105688Z","shell.execute_reply.started":"2025-04-20T06:19:20.099541Z","shell.execute_reply":"2025-04-20T06:19:20.104938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3 Initialize Models","metadata":{"_uuid":"e065f682-628a-45d5-9e87-59db044b0468","_cell_guid":"5ec5b7cb-5dad-4ab3-8a66-5d2021682bc3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Initialize models once\nwith strategy.scope():\n    monet_generator = Generator()  # transforms photos to Monet-style paintings\n    photo_generator = Generator()  # transforms Monet paintings to photo-like images\n    monet_discriminator = Discriminator()  # discriminates real vs fake Monet paintings\n    photo_discriminator = Discriminator()  # discriminates real vs fake photos","metadata":{"_uuid":"0442f37b-ce3f-49fb-9945-5855bb8e063f","_cell_guid":"f10546d5-0ead-4fd5-9548-9a52b48afa2b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:21.405829Z","iopub.execute_input":"2025-04-20T06:19:21.406336Z","iopub.status.idle":"2025-04-20T06:19:21.860864Z","shell.execute_reply.started":"2025-04-20T06:19:21.406314Z","shell.execute_reply":"2025-04-20T06:19:21.860328Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test the untrained generator\nto_monet = monet_generator(example_photo)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Original Photo\")\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Untrained Generator Output\")\nplt.imshow(to_monet[0] * 0.5 + 0.5)\nplt.show()","metadata":{"_uuid":"d488da43-6916-4c0d-b4fd-d7c17d7d04df","_cell_guid":"3f695365-9d6f-4dbc-b32a-7c775e7eabd3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:21.861934Z","iopub.execute_input":"2025-04-20T06:19:21.862196Z","iopub.status.idle":"2025-04-20T06:19:22.312844Z","shell.execute_reply.started":"2025-04-20T06:19:21.862153Z","shell.execute_reply":"2025-04-20T06:19:22.312227Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Loss Functions, Tuning, and Model Training\n\n### 5.1 Define Loss Functions","metadata":{"_uuid":"080d933d-8e06-4b27-b479-0b2d4f1097a2","_cell_guid":"ee00a66f-5eb4-4371-95e1-71efffeac21a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        \"\"\"Discriminator loss - tries to correctly classify real vs generated images\"\"\"\n        real_loss = tf.keras.losses.BinaryCrossentropy(\n            from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n            \n        generated_loss = tf.keras.losses.BinaryCrossentropy(\n            from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n            \n        total_disc_loss = real_loss + generated_loss\n        \n        return total_disc_loss * 0.5  # Average the losses\n\n    def generator_loss(generated):\n        \"\"\"Generator loss - tries to fool discriminator to classify generated as real\"\"\"\n        return tf.keras.losses.BinaryCrossentropy(\n            from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        \"\"\"Cycle consistency loss - ensures we can reconstruct original image\"\"\"\n        loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n        return LAMBDA * loss\n\n    def identity_loss(real_image, same_image, LAMBDA):\n        \"\"\"Identity loss - encourages generators to preserve content when input is of target domain\"\"\"\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{"_uuid":"959541dd-4122-4c7b-8dad-d21f53e8e6ae","_cell_guid":"18646640-1d2c-433c-a735-f74ace7180f1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:22.947726Z","iopub.execute_input":"2025-04-20T06:19:22.948505Z","iopub.status.idle":"2025-04-20T06:19:22.954232Z","shell.execute_reply.started":"2025-04-20T06:19:22.948481Z","shell.execute_reply":"2025-04-20T06:19:22.953565Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 CycleGAN Model","metadata":{"_uuid":"69295104-98d6-4134-a3e2-477d77ac174e","_cell_guid":"3db28013-1236-429a-be65-d9ffdd69565f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    \"\"\"CycleGAN model that combines generators and discriminators with cycle consistency\"\"\"\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super().__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super().compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        \"\"\"Execute one training step\"\"\"\n        real_monet, real_photo = batch_data\n        \n        print(f\"train_step received shapes: {real_monet.shape}, {real_photo.shape}\")\n    \n        # Ensure 4D tensors - reshape if needed\n        if len(real_monet.shape) > 4:\n            real_monet = tf.reshape(real_monet, [-1, 256, 256, 3])\n        if len(real_photo.shape) > 4:\n            real_photo = tf.reshape(real_photo, [-1, 256, 256, 3])\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # Photo to Monet to Photo (forward cycle)\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n            \n            # Monet to Photo to Monet (backward cycle)\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n            \n            # Identity mapping\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n            \n            # Discriminator outputs\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            \n            disc_real_photo = self.p_disc(real_photo, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n            \n            # Generator losses\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n            \n            # Cycle consistency losses\n            total_cycle_loss = (\n                self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) +\n                self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n            )\n            \n            # Identity losses\n            total_monet_gen_loss = (\n                monet_gen_loss + \n                total_cycle_loss + \n                self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            )\n            \n            total_photo_gen_loss = (\n                photo_gen_loss + \n                total_cycle_loss + \n                self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n            )\n            \n            # Discriminator losses\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n            \n        # Calculate gradients\n        monet_generator_gradients = tape.gradient(\n            total_monet_gen_loss, self.m_gen.trainable_variables\n        )\n        photo_generator_gradients = tape.gradient(\n            total_photo_gen_loss, self.p_gen.trainable_variables\n        )\n        \n        monet_discriminator_gradients = tape.gradient(\n            monet_disc_loss, self.m_disc.trainable_variables\n        )\n        photo_discriminator_gradients = tape.gradient(\n            photo_disc_loss, self.p_disc.trainable_variables\n        )\n        \n        # Apply gradients\n        self.m_gen_optimizer.apply_gradients(\n            zip(monet_generator_gradients, self.m_gen.trainable_variables)\n        )\n        self.p_gen_optimizer.apply_gradients(\n            zip(photo_generator_gradients, self.p_gen.trainable_variables)\n        )\n        \n        self.m_disc_optimizer.apply_gradients(\n            zip(monet_discriminator_gradients, self.m_disc.trainable_variables)\n        )\n        self.p_disc_optimizer.apply_gradients(\n            zip(photo_discriminator_gradients, self.p_disc.trainable_variables)\n        )\n        \n        # Return metrics\n        return {\n            \"monet_gen_loss\": tf.reduce_mean(total_monet_gen_loss),\n            \"photo_gen_loss\": tf.reduce_mean(total_photo_gen_loss),\n            \"monet_disc_loss\": tf.reduce_mean(monet_disc_loss),\n            \"photo_disc_loss\": tf.reduce_mean(photo_disc_loss)\n        }","metadata":{"_uuid":"36c37ee8-1768-4fba-a92e-1df7481e1e61","_cell_guid":"8d949e9d-d032-488a-b0d1-0e8b982c7368","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:26.198698Z","iopub.execute_input":"2025-04-20T06:19:26.199146Z","iopub.status.idle":"2025-04-20T06:19:26.21073Z","shell.execute_reply.started":"2025-04-20T06:19:26.199125Z","shell.execute_reply":"2025-04-20T06:19:26.209944Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.3 Hyperparameter Tuning and Model Compilation","metadata":{"_uuid":"a3356b15-9046-4868-8323-c3e74c62cdb7","_cell_guid":"516b6d3c-0e05-4d76-957f-74355e8ee602","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def create_tuning_dataset(monet_ds, photo_ds, num_samples, batch_size=1):\n    # Ensure both datasets are unbatched individual samples\n    if hasattr(monet_ds, '_batch_size') and monet_ds._batch_size is not None:\n        monet_ds = monet_ds.unbatch()\n    if hasattr(photo_ds, '_batch_size') and photo_ds._batch_size is not None:\n        photo_ds = photo_ds.unbatch()\n    \n    # Take samples from each dataset\n    monet_subset = monet_ds.take(num_samples)\n    photo_subset = photo_ds.take(num_samples)\n    \n    # Create a dataset of pairs (this is the key)\n    tuning_dataset = tf.data.Dataset.zip((monet_subset, photo_subset))\n    tuning_dataset = tuning_dataset.batch(batch_size)\n    \n    return tuning_dataset\n\n# Utility function to configure optimizers\ndef configure_optimizers(learning_rate, beta1):\n    \"\"\"Create optimizers with learning rate schedule.\"\"\"\n    lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n        learning_rate,\n        decay_steps=500,\n        decay_rate=0.8,\n        staircase=True\n    )\n    return [\n        tf.keras.optimizers.Adam(lr_scheduler, beta_1=beta1),  # monet_generator_optimizer\n        tf.keras.optimizers.Adam(lr_scheduler, beta_1=beta1),  # photo_generator_optimizer\n        tf.keras.optimizers.Adam(lr_scheduler, beta_1=beta1),  # monet_discriminator_optimizer\n        tf.keras.optimizers.Adam(lr_scheduler, beta_1=beta1)   # photo_discriminator_optimizer\n    ]\n\ndef build_tunable_cyclegan(hp):\n    \"\"\"Build CycleGAN with tunable hyperparameters using existing models.\"\"\"\n    lr = hp.Choice('learning_rate', values=[1e-4, 3e-4])\n    beta1 = hp.Choice('beta1', values=[0.5, 0.7])\n    lambda_cycle = hp.Choice('lambda_cycle', values=[5, 10])\n    \n    with strategy.scope():\n\n        initial_weights = [\n            monet_generator.get_weights(),\n            photo_generator.get_weights(),\n            monet_discriminator.get_weights(),\n            photo_discriminator.get_weights()\n        ]\n        optimizers = configure_optimizers(lr, beta1)\n        model = CycleGan(\n            monet_generator,\n            photo_generator,\n            monet_discriminator,\n            photo_discriminator,\n            lambda_cycle=lambda_cycle\n        )\n        model.compile(\n            m_gen_optimizer=optimizers[0],\n            p_gen_optimizer=optimizers[1],\n            m_disc_optimizer=optimizers[2],\n            p_disc_optimizer=optimizers[3],\n            gen_loss_fn=generator_loss,\n            disc_loss_fn=discriminator_loss,\n            cycle_loss_fn=calc_cycle_loss,\n            identity_loss_fn=identity_loss\n        )\n\n        # Restore initial weights to ensure consistent starting point\n        monet_generator.set_weights(initial_weights[0])\n        photo_generator.set_weights(initial_weights[1])\n        monet_discriminator.set_weights(initial_weights[2])\n        photo_discriminator.set_weights(initial_weights[3])\n    \n    return model","metadata":{"_uuid":"ec78420c-07ce-4ea6-858d-10f24c763ecb","_cell_guid":"48c8355f-decc-4776-a9f4-d8b57ff33d50","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:30.945342Z","iopub.execute_input":"2025-04-20T06:19:30.945821Z","iopub.status.idle":"2025-04-20T06:19:30.954024Z","shell.execute_reply.started":"2025-04-20T06:19:30.9458Z","shell.execute_reply":"2025-04-20T06:19:30.953331Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if fine_tuning:\n\n    if os.path.exists('./hyperparameter_tuning'):\n        shutil.rmtree('./hyperparameter_tuning')\n\n    tf.keras.backend.clear_session()\n    gc.collect()\n    \n    TUNING_BATCH_SIZE = 1\n    TUNING_SAMPLES = 50\n    tuning_dataset = create_tuning_dataset(monet_ds, photo_ds, num_samples=TUNING_SAMPLES)\n    tuning_dataset","metadata":{"_uuid":"82dd797a-9a73-4307-aaac-4a9b342a8b5e","_cell_guid":"f4b40c1d-0034-447d-869b-2c3087662249","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:32.240005Z","iopub.execute_input":"2025-04-20T06:19:32.240355Z","iopub.status.idle":"2025-04-20T06:19:32.244542Z","shell.execute_reply.started":"2025-04-20T06:19:32.240336Z","shell.execute_reply":"2025-04-20T06:19:32.243922Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tuner = kt.GridSearch(\n    build_tunable_cyclegan,\n    objective=kt.Objective('monet_gen_loss', direction=\"min\"),\n    max_trials=9,\n    directory=f'{TMP_DIR}/hyperparameter_tuning',\n    project_name='cyclegan_grid_tuning'\n)","metadata":{"_uuid":"96433a1a-45bb-4411-8cc8-9b183c0c09ed","_cell_guid":"bb13d88c-24f9-4e92-83e1-49093c5023e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:19:33.99661Z","iopub.execute_input":"2025-04-20T06:19:33.996895Z","iopub.status.idle":"2025-04-20T06:19:35.09866Z","shell.execute_reply.started":"2025-04-20T06:19:33.996866Z","shell.execute_reply":"2025-04-20T06:19:35.09805Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if fine_tuning:\n    \n    tuner.search(\n        tuning_dataset,\n        epochs=5,\n        callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)],\n        verbose=1\n    )\n    \n    best_hps = tuner.get_best_hyperparameters(1)[0]\n    learning_rate = best_hps.get('learning_rate')\n    beta1 = best_hps.get('beta1')\n    lambda_cycle = best_hps.get('lambda_cycle')\n    print(f\"Best hyperparameters: Learning rate={learning_rate}, Beta1={beta1}, Lambda cycle={lambda_cycle}\")\n\n    # Get best hyperparameters\n    best_hps = tuner.get_best_hyperparameters(1)[0]\n    print(f\"Best hyperparameters found:\")\n    print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n    print(f\"Beta1: {best_hps.get('beta1')}\")\n    print(f\"Lambda cycle: {best_hps.get('lambda_cycle')}\")\n\nelse:\n    # default to last run's best hyper parameters (no need to set fine_tuning=True) \n    learning_rate, beta1, lambda_cycle = 1e-4, 0.7, 5\n\n    # Use last run's best as default\n    print(f\"Defaulting to Last Run's Best:\")\n    print(f\"Learning rate: {learning_rate}\")\n    print(f\"Beta1: {beta1}\")\n    print(f\"Lambda cycle: {lambda_cycle}\")\n    ","metadata":{"_uuid":"b745f021-32cc-4125-bc1f-55ea68fd8951","_cell_guid":"3d940745-379d-46e9-9e5e-1279e32c099c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:22:02.16445Z","iopub.execute_input":"2025-04-20T06:22:02.165Z","iopub.status.idle":"2025-04-20T06:22:02.170891Z","shell.execute_reply.started":"2025-04-20T06:22:02.164978Z","shell.execute_reply":"2025-04-20T06:22:02.170126Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Apply the best hyperparameters to the final model\n# learning_rate = best_hps.get('learning_rate')\n# beta1 = best_hps.get('beta1')\n# lambda_cycle = best_hps.get('lambda_cycle')","metadata":{"_uuid":"56883e31-c853-4a34-a2f7-d254f77c1e10","_cell_guid":"16d3cde1-db21-4633-9b0c-02208f3a446b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:22:44.333282Z","iopub.execute_input":"2025-04-20T06:22:44.333976Z","iopub.status.idle":"2025-04-20T06:22:44.336943Z","shell.execute_reply.started":"2025-04-20T06:22:44.333952Z","shell.execute_reply":"2025-04-20T06:22:44.336342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_hyperparam_importance_from_oracle(tuner):\n    # Access trials from the oracle\n    trials = tuner.oracle.trials\n    \n    # Organize data for visualization\n    param_data = {}\n    \n    # Extract hyperparameter values and corresponding scores\n    for trial_id, trial in trials.items():\n        score = trial.score  # This contains the objective metric value\n        \n        if score is not None:  # Ensure the trial completed and has a score\n            for param_name, param_value in trial.hyperparameters.values.items():\n                if param_name not in param_data:\n                    param_data[param_name] = {'values': [], 'scores': []}\n                \n                param_data[param_name]['values'].append(param_value)\n                param_data[param_name]['scores'].append(score)\n    \n    # Create visualizations\n    num_params = len(param_data)\n    plt.figure(figsize=(12, 4 * num_params))\n    \n    for i, (param_name, data) in enumerate(param_data.items()):\n        plt.subplot(num_params, 1, i+1)\n        \n        # Convert values to strings for categorical plotting\n        values = data['values']\n        scores = data['scores']\n        \n        # Create boxplot for categorical parameters or scatterplot for numerical\n        if len(set(values)) <= 5:  # Likely categorical or few distinct values\n            # Group scores by parameter value\n            value_groups = {}\n            for val, score in zip(values, scores):\n                if val not in value_groups:\n                    value_groups[val] = []\n                value_groups[val].append(score)\n            \n            # Prepare data for boxplot\n            box_data = [scores for val, scores in sorted(value_groups.items())]\n            labels = [str(val) for val in sorted(value_groups.keys())]\n            \n            plt.boxplot(box_data, labels=labels)\n            plt.title(f'Impact of {param_name} on model performance')\n        else:  # Numerical with many values\n            plt.scatter(values, scores)\n            plt.title(f'Impact of {param_name} on model performance')\n        \n        plt.xlabel(param_name)\n        plt.ylabel('Score (monet_gen_loss)')\n        plt.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"30303220-68dc-46f5-a8a0-cabd0c78f5ae","_cell_guid":"ec321f5f-b085-433d-86d3-5f916f640070","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:22:45.525716Z","iopub.execute_input":"2025-04-20T06:22:45.52598Z","iopub.status.idle":"2025-04-20T06:22:45.533745Z","shell.execute_reply.started":"2025-04-20T06:22:45.525961Z","shell.execute_reply":"2025-04-20T06:22:45.533093Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if fine_tuning:\n    plot_hyperparam_importance_from_oracle(tuner)","metadata":{"_uuid":"b0d09a76-e98c-40d8-b55a-2ab0c073bc05","_cell_guid":"f15eadcd-15de-4a75-a4c2-4cbdc0a009a3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:23:22.036916Z","iopub.execute_input":"2025-04-20T06:23:22.037838Z","iopub.status.idle":"2025-04-20T06:23:22.041443Z","shell.execute_reply.started":"2025-04-20T06:23:22.037806Z","shell.execute_reply":"2025-04-20T06:23:22.040713Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile final CycleGAN model with best hyperparameters\nwith strategy.scope():\n    optimizers = configure_optimizers(learning_rate, beta1)\n    cycle_gan_model = CycleGan(\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=lambda_cycle\n    )\n    cycle_gan_model.compile(\n        m_gen_optimizer=optimizers[0],\n        p_gen_optimizer=optimizers[1],\n        m_disc_optimizer=optimizers[2],\n        p_disc_optimizer=optimizers[3],\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss\n    )","metadata":{"_uuid":"2a0112da-c213-4218-9076-4a8a73dd4c2d","_cell_guid":"2be247c8-c700-4407-9d6c-16b58591dcee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:23:23.932968Z","iopub.execute_input":"2025-04-20T06:23:23.933267Z","iopub.status.idle":"2025-04-20T06:23:23.947568Z","shell.execute_reply.started":"2025-04-20T06:23:23.933245Z","shell.execute_reply":"2025-04-20T06:23:23.947008Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.4 Epoch Visualization Functions","metadata":{"_uuid":"16029d78-d64d-4696-9b1a-b8ac14957424","_cell_guid":"73f7d122-8a1b-4e70-ba98-b435136b233b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Consolidated visualization functions for training progress\ndef generate_images(model, test_input):\n    \"\"\"Generate and display Monet-style images from test input\"\"\"\n    prediction = model(test_input, training=False)\n    \n    plt.figure(figsize=(10, 5))\n    display_list = [test_input[0], prediction[0]]\n    title = ['Input Photo', 'Monet-style Generated']\n    \n    for i in range(2):\n        plt.subplot(1, 2, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    \n    plt.show()\n\nclass DisplayMonitor(keras.callbacks.Callback):\n    \"\"\"Custom callback to display generated images during training\"\"\"\n    def __init__(self, test_input, interval=5):\n        self.test_input = test_input\n        self.interval = interval\n        \n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.interval == 0:\n            clear_output(wait=True)\n            print(f\"\\nEpoch {epoch + 1} Results:\")\n            generate_images(self.model.m_gen, self.test_input)","metadata":{"_uuid":"f06e1807-8e19-4590-900e-79bad386042f","_cell_guid":"80b3e82b-35f5-4d54-957d-f9c1ce1da2f4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:23:25.690974Z","iopub.execute_input":"2025-04-20T06:23:25.691723Z","iopub.status.idle":"2025-04-20T06:23:25.697397Z","shell.execute_reply.started":"2025-04-20T06:23:25.691701Z","shell.execute_reply":"2025-04-20T06:23:25.696572Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.5 Train the Model","metadata":{"_uuid":"c069eb5f-ad0d-4856-863f-97bba750ca46","_cell_guid":"b8e29ea7-7595-4a09-b3ad-3fca2f81c2b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# test batch for visualization after training\ntest_photos = tf.data.Dataset.list_files('/kaggle/input/gan-getting-started/photo_tfrec/*.tfrec')\ntest_photos = load_dataset(test_photos, BATCH_SIZE).take(1)","metadata":{"_uuid":"8caa5d35-c7f7-4e9c-b236-05c7632f03de","_cell_guid":"2ad12c62-9f2f-4124-9f22-41e28b89af76","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:23:31.26759Z","iopub.execute_input":"2025-04-20T06:23:31.268195Z","iopub.status.idle":"2025-04-20T06:23:31.660809Z","shell.execute_reply.started":"2025-04-20T06:23:31.268146Z","shell.execute_reply":"2025-04-20T06:23:31.66021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"steps_per_epoch = 300 // BATCH_SIZE # we know there are 300 monet images \n# would like a way to calcualte this easier","metadata":{"_uuid":"59801ca5-42f0-4cc6-ae71-bc497844f011","_cell_guid":"121bee90-75b4-47d3-ba83-91c0160fbd2a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:23:31.661948Z","iopub.execute_input":"2025-04-20T06:23:31.662204Z","iopub.status.idle":"2025-04-20T06:23:31.665618Z","shell.execute_reply.started":"2025-04-20T06:23:31.662177Z","shell.execute_reply":"2025-04-20T06:23:31.664976Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define callbacks\ncallbacks = [\n    DisplayMonitor(next(iter(test_photos)), interval=5),\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=f'{TMP_DIR}/monet_generator_checkpoint.weights.h5',\n        save_weights_only=True,\n        save_best_only=True,\n        monitor='monet_gen_loss'\n    ),\n    tf.keras.callbacks.TensorBoard(log_dir=f'{TMP_DIR}/logs'),\n    tf.keras.callbacks.EarlyStopping(\n        monitor='monet_gen_loss',\n        mode='min',\n        patience=6,\n        restore_best_weights=True\n    )\n]","metadata":{"_uuid":"ecb4a713-e5c8-4b24-bc88-5dad05c50514","_cell_guid":"b2079f16-f0f8-414d-8570-8c7f8440836f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:23:32.908549Z","iopub.execute_input":"2025-04-20T06:23:32.909258Z","iopub.status.idle":"2025-04-20T06:23:32.943466Z","shell.execute_reply.started":"2025-04-20T06:23:32.909234Z","shell.execute_reply":"2025-04-20T06:23:32.942904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nEPOCHS = 15\n\nhistory = cycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=EPOCHS,\n    steps_per_epoch=steps_per_epoch,\n    callbacks=callbacks\n)\n\n# Save model weights\nmonet_generator.save_weights(f'{TMP_DIR}/monet_generator_weights.weights.h5')","metadata":{"_uuid":"a043e3b8-96eb-4250-bfe7-014a22b6f3da","_cell_guid":"c3bc4ade-9c6f-43fd-8399-b754b07f135c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:23:34.206864Z","iopub.execute_input":"2025-04-20T06:23:34.207462Z","iopub.status.idle":"2025-04-20T06:29:22.565586Z","shell.execute_reply.started":"2025-04-20T06:23:34.207439Z","shell.execute_reply":"2025-04-20T06:29:22.564943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.6 Training Results Visualization","metadata":{"_uuid":"0170579b-e438-475d-8943-86bcc5f8ec92","_cell_guid":"455fd514-40a4-4481-8dd1-55a07444f79c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Plot training history\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['monet_gen_loss'], label='Monet Generator')\nplt.plot(history.history['photo_gen_loss'], label='Photo Generator')\nplt.title('Generator Losses')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['monet_disc_loss'], label='Monet Discriminator')\nplt.plot(history.history['photo_disc_loss'], label='Photo Discriminator')\nplt.title('Discriminator Losses')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"fd01992c-6458-4d62-94b7-abb74b25c043","_cell_guid":"51d19f61-f373-40c5-8287-594170ea7de8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:29:22.566825Z","iopub.execute_input":"2025-04-20T06:29:22.567089Z","iopub.status.idle":"2025-04-20T06:29:22.928415Z","shell.execute_reply.started":"2025-04-20T06:29:22.567067Z","shell.execute_reply":"2025-04-20T06:29:22.927467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Results and Evaluation","metadata":{"_uuid":"f96cc1a6-ab52-4b64-be41-f8c2ffa30a9e","_cell_guid":"323f649b-d3ad-48d5-9638-f02fd061251c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Display a grid of results\ndef display_grid(model, ds, num_images=5, figsize=(15, 15)):\n    \"\"\"Display a grid of original and generated images\"\"\"\n    fig, axes = plt.subplots(num_images, 2, figsize=figsize)\n    \n    for i, img in enumerate(ds.take(num_images)):\n        prediction = model(img, training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        \n        img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n        \n        axes[i, 0].imshow(img)\n        axes[i, 1].imshow(prediction)\n        axes[i, 0].set_title(\"Original Photo\")\n        axes[i, 1].set_title(\"Monet-style Generated\")\n        axes[i, 0].axis(\"off\")\n        axes[i, 1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display results\nprint(\"Displaying results grid...\")\ndisplay_grid(monet_generator, photo_ds, num_images=5)","metadata":{"_uuid":"3382fca2-1632-4119-9cad-3d9f763ccefa","_cell_guid":"05ed132b-f5b6-4ca6-9593-08890e4a8870","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:29:22.929291Z","iopub.execute_input":"2025-04-20T06:29:22.92948Z","iopub.status.idle":"2025-04-20T06:29:24.291012Z","shell.execute_reply.started":"2025-04-20T06:29:22.929465Z","shell.execute_reply":"2025-04-20T06:29:24.289854Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Generate Submission Images","metadata":{"_uuid":"f2464639-2c35-439e-b233-ea865318b4c2","_cell_guid":"2300b713-7178-42b4-98ef-60e097e60471","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create a directory for generated images\nif not os.path.exists(IMAGES_TMP_DIR):\n    os.makedirs(IMAGES_TMP_DIR)","metadata":{"_uuid":"baf195bf-7c2b-4b8e-b67a-eac441bbb9eb","_cell_guid":"9c62174a-b13b-4b34-96b1-d9e53fdff77b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:29:24.293013Z","iopub.execute_input":"2025-04-20T06:29:24.293496Z","iopub.status.idle":"2025-04-20T06:29:24.297819Z","shell.execute_reply.started":"2025-04-20T06:29:24.293473Z","shell.execute_reply":"2025-04-20T06:29:24.29708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file directly\nsubmission_file = \"/kaggle/working/images.zip\"\nprint(\"Generating images and creating submission zip file...\")\n\n# Open the zip file once and add images as we generate them\nwith zipfile.ZipFile(submission_file, 'w') as zip_ref:\n    count = 0\n    for img in photo_ds:\n        if count % 100 == 0:\n            print(f\"Processed {count} images\")\n        \n        # Generate the Monet-style image\n        prediction = monet_generator(img, training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        \n        # Convert to PIL image\n        im = PIL.Image.fromarray(prediction)\n        \n        # Save directly to zip without intermediate file\n        img_byte_arr = io.BytesIO()\n        im.save(img_byte_arr, format='JPEG')\n        img_byte_arr.seek(0)\n        \n        # Add to zip with appropriate name\n        zip_ref.writestr(f\"{count+1}.jpg\", img_byte_arr.getvalue())\n        \n        count += 1\n\nprint(f\"Total images generated and added to zip: {count}\")\nprint(f\"Submission file created: {submission_file}\")","metadata":{"_uuid":"1bdf3eb3-f6a1-488a-b829-d74f87ffcd5e","_cell_guid":"c8c20509-33f9-465c-9524-54674338b1c8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T06:43:26.14915Z","iopub.execute_input":"2025-04-20T06:43:26.149835Z","iopub.status.idle":"2025-04-20T06:50:47.37931Z","shell.execute_reply.started":"2025-04-20T06:43:26.14981Z","shell.execute_reply":"2025-04-20T06:50:47.378698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Clean up any existing folders in /kaggle/working/\n# path = '/kaggle/working/images.zip'\n# if os.path.exists(path):\n#     os.remove(path)\n#     #shutil.rmtree(path)","metadata":{"_uuid":"4cf83c9d-e28f-4693-832b-8f5e5caa8c74","_cell_guid":"6362c8d9-68ca-4c68-abf4-7edf7d3bc306","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T06:32:08.677203Z","iopub.execute_input":"2025-04-20T06:32:08.677874Z","iopub.status.idle":"2025-04-20T06:32:08.698566Z","shell.execute_reply.started":"2025-04-20T06:32:08.677849Z","shell.execute_reply":"2025-04-20T06:32:08.697873Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Conclusion and Discussion\nIn this project, I successfully implemented a CycleGAN model to generate Monet-style paintings from photographs.\n\nAs of my last run (not this one that you're probably looking at now), I used a learning rate of 3e-4, beta1 of 0.5, and a 10 lambda_cycle. To help with convergence, I added learning rate scheduling with exponential decay. Training took about an hour (didn't time it) and 25 or fewer epochs by design. The training did not converge and I'm still scratching my head. This is something I would like to address in future work.\n\nIn future iterations, I would find it worthwhile to introduce various batch sizes and image augmentations to improve the generalization of the model. Additionally, I would make some architecture changes that I struggled to work around such as the strange dimension mismatch in my train step (if you can see what I did wrong, please comment with your critique). I would also prefer to use the TPU provided by kaggle, but I did not want to wait for my turn (ha). With more compute, better hyperparameter tuning could improve my model's performance.\n\n\nAll in all, the CycleGAN approach proved effective for this unpaired image-to-image translation task, allowing me to sort of capture the essence of Monet's style.\n\n## 9 References\n\nThank you to Amy Jang for putting together the fantastic explanation of CycleGAN:\n\nhttps://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial","metadata":{"_uuid":"c29e0a96-fb4e-481c-9282-254bdc1adf8c","_cell_guid":"3fe95b3d-2e6f-494b-ac8d-70f964cf5323","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}