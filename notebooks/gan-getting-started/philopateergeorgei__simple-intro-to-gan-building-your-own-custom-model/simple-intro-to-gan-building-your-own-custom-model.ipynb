{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-29T08:55:56.868536Z","iopub.execute_input":"2023-09-29T08:55:56.868996Z","iopub.status.idle":"2023-09-29T08:55:58.884245Z","shell.execute_reply.started":"2023-09-29T08:55:56.868967Z","shell.execute_reply":"2023-09-29T08:55:58.882953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction to the course and loading our libraries","metadata":{}},{"cell_type":"markdown","source":"Hi and welcome to this notebook Which is made to help yall in doing GANs \n\n**what Does gans even stand for?**\nGan stands for Generative adversarial networks and pretty much means AI which makes Art\nfor more info go [here](https://en.wikipedia.org/wiki/Generative_adversarial_network) this is a   wikipedia articale explaining everything\n\n**What language and what frameworks will we use?**\nwe will use Python with tensorflow since this is a simple intro course I will use the 'simplier' high level api Keras so that any one can get it easily if you want to learn via Pytorch this is another course which explains it pretty will [here is the course](https://www.youtube.com/watch?v=j3htLzG-Y_I)\n\n**What will I learn on this 'intro course' ?**\nthis course will explain simple GANs we will build A custom model from scratch I will help you understand how everything works and I will have some refarals to understand the theory even more\n\n**What data will we use?**\nwe will be using the data in [this](https://www.kaggle.com/competitions/gan-getting-started) competion (a competion made by the kaggle staff for GANs)\n\n**Finally what should you do if you dont understand anything?**\nIf you dont get anything send me a message or even better just comment what you dont understand and tag me or you can just google it :)\n\nAlr here we go GL and have fun :)","metadata":{}},{"cell_type":"markdown","source":"Lets now load our library","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf #this is loading tensorflow\nfrom tensorflow.keras import utils,Sequential,layers\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D # Bring in the layers for the neural network\nimport matplotlib.pyplot as plt\n#thats it will matplolib is option I will just use it for visualizition no more so if you arent proficeint in it dont even bother","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:55:58.886584Z","iopub.execute_input":"2023-09-29T08:55:58.887092Z","iopub.status.idle":"2023-09-29T08:55:58.89406Z","shell.execute_reply.started":"2023-09-29T08:55:58.887051Z","shell.execute_reply":"2023-09-29T08:55:58.892643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Preprocessing our data\n\n","metadata":{}},{"cell_type":"markdown","source":"lets now load the data there is a fancy way to do it (but I am too lazy) so I am just gonna use the built in [tf.keras.utils.image_dataset_from_directory()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) cuz its easier","metadata":{}},{"cell_type":"code","source":"true = utils.image_dataset_from_directory(\n    '/kaggle/input/gan-getting-started',#this is the dir which has the data\n    image_size=(256,256)#the size we want the image to be\n)\n#yes thats it really no joke we just loaded all the data (I will change one thing when training later on)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:55:58.896091Z","iopub.execute_input":"2023-09-29T08:55:58.897411Z","iopub.status.idle":"2023-09-29T08:55:59.651541Z","shell.execute_reply.started":"2023-09-29T08:55:58.897324Z","shell.execute_reply":"2023-09-29T08:55:59.650345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now we will normaliza the data by going through it","metadata":{}},{"cell_type":"code","source":"def process(image,label):\n    image = tf.cast(image/255. ,tf.float32) #this is the line that normilizes the data\n    return image,label\n\ntrue = true.map(process) #this goes through every single image and normilizes it","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:55:59.653851Z","iopub.execute_input":"2023-09-29T08:55:59.654182Z","iopub.status.idle":"2023-09-29T08:55:59.672576Z","shell.execute_reply.started":"2023-09-29T08:55:59.654143Z","shell.execute_reply":"2023-09-29T08:55:59.671501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the generating model","metadata":{}},{"cell_type":"markdown","source":"so the next part is kinda of hard i would recommend [this vid](https://www.youtube.com/watch?v=5g1eXmQtl0E) explains theortical part\n\nnote: I gain nothing from ANY of the sources i provide","metadata":{}},{"cell_type":"code","source":"def build_generator(): \n    model = Sequential()\n    \n    # Takes in random values and reshapes it to 16,16,256\n    # Beginnings of a generated image\n    model.add(Dense(16*16, input_shape=(16,16,256)))\n    model.add(LeakyReLU(0.2))\n    model.add(Reshape((16,16,256)))\n    \n    # Upsampling block 1 \n    model.add(UpSampling2D())\n    model.add(Conv2D(128, 5, padding='same'))\n    model.add(LeakyReLU(0.2))\n    \n    # Upsampling block 2 \n    model.add(UpSampling2D())\n    model.add(Conv2D(128, 5, padding='same'))\n    model.add(LeakyReLU(0.2))\n    \n    # Convolutional block 3\n    model.add(UpSampling2D())\n    model.add(Conv2D(128, 4, padding='same'))\n    model.add(LeakyReLU(0.2))\n    \n    # Convolutional block 4\n    model.add(Conv2D(128, 4, padding='same'))\n    model.add(LeakyReLU(0.2))\n    \n    # Conv layer to get to one channel\n    model.add(UpSampling2D())\n    model.add(Conv2D(3, 4, padding='same', activation='sigmoid'))\n    \n    return model\ngenerator = build_generator()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:55:59.673842Z","iopub.execute_input":"2023-09-29T08:55:59.67433Z","iopub.status.idle":"2023-09-29T08:55:59.881532Z","shell.execute_reply.started":"2023-09-29T08:55:59.674302Z","shell.execute_reply":"2023-09-29T08:55:59.880142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:55:59.883429Z","iopub.execute_input":"2023-09-29T08:55:59.883877Z","iopub.status.idle":"2023-09-29T08:55:59.940755Z","shell.execute_reply.started":"2023-09-29T08:55:59.883843Z","shell.execute_reply":"2023-09-29T08:55:59.93936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets now some cool prediction made by our not trained ai","metadata":{}},{"cell_type":"code","source":"img = generator.predict(np.random.randn(4,16,16,256))\n# Setup the subplot formatting \nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\n# Loop four times and get images \nfor idx, img in enumerate(img): \n    # Plot the image using a specific subplot \n    ax[idx].imshow(np.squeeze(img))\n    # Appending the image label as the plot title \n    ax[idx].title.set_text(idx)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:55:59.942315Z","iopub.execute_input":"2023-09-29T08:55:59.942953Z","iopub.status.idle":"2023-09-29T08:56:02.708199Z","shell.execute_reply.started":"2023-09-29T08:55:59.942919Z","shell.execute_reply":"2023-09-29T08:56:02.704989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"wow what an amazing art (JK) \n\nour model never was trained so thats makes some sense \nnow lets make the detector ","metadata":{}},{"cell_type":"markdown","source":"# Building our dector model","metadata":{}},{"cell_type":"code","source":"def build_discriminator(): \n    model = Sequential()\n    \n    # First Conv Block\n    model.add(Conv2D(32, 5, input_shape = (256,256,3)))\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.4))\n    model.add(layers.MaxPool2D())\n\n    # Second Conv Block\n    model.add(Conv2D(64, 5))\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.4))\n    model.add(layers.MaxPool2D())\n    # Third Conv Block\n    model.add(Conv2D(128, 5))\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.4))\n    model.add(layers.MaxPool2D())\n    # Fourth Conv Block\n    model.add(Conv2D(256, 5))\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.4))\n    model.add(layers.MaxPool2D())\n    model.add(Dropout(0.4))\n    model.add(layers.MaxPool2D())\n    model.add(Conv2D(256, 5))\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.4))\n    model.add(layers.MaxPool2D())\n\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model ","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:02.709797Z","iopub.execute_input":"2023-09-29T08:56:02.710312Z","iopub.status.idle":"2023-09-29T08:56:02.719262Z","shell.execute_reply.started":"2023-09-29T08:56:02.710282Z","shell.execute_reply":"2023-09-29T08:56:02.718259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector = build_discriminator()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:02.721083Z","iopub.execute_input":"2023-09-29T08:56:02.721483Z","iopub.status.idle":"2023-09-29T08:56:02.976388Z","shell.execute_reply.started":"2023-09-29T08:56:02.72145Z","shell.execute_reply":"2023-09-29T08:56:02.975238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:02.981158Z","iopub.execute_input":"2023-09-29T08:56:02.981643Z","iopub.status.idle":"2023-09-29T08:56:03.058747Z","shell.execute_reply.started":"2023-09-29T08:56:02.981612Z","shell.execute_reply":"2023-09-29T08:56:03.05712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils.plot_model(detector)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:03.060869Z","iopub.execute_input":"2023-09-29T08:56:03.061324Z","iopub.status.idle":"2023-09-29T08:56:03.464559Z","shell.execute_reply.started":"2023-09-29T08:56:03.061282Z","shell.execute_reply":"2023-09-29T08:56:03.46344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils.plot_model(generator)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:03.465826Z","iopub.execute_input":"2023-09-29T08:56:03.466264Z","iopub.status.idle":"2023-09-29T08:56:03.567317Z","shell.execute_reply.started":"2023-09-29T08:56:03.466226Z","shell.execute_reply":"2023-09-29T08:56:03.566226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"will now were done the next part can be either the easiest or the hardest\n\neasiest if you can easily build custom training function hardest if you can not","metadata":{}},{"cell_type":"markdown","source":"# training the model","metadata":{}},{"cell_type":"markdown","source":"code explination (generated by gpt 4) :\nThe code is about creating a special kind of model called a GAN. A GAN stands for Generative Adversarial Network. It is a way of making new things that look like real things, such as images, sounds, or texts. For example, a GAN can make new images of cats that look like real cats, but are not actually real cats.\n\nA GAN has two parts: a generator and a discriminator. The generatorâ€™s job is to make new things, and the discriminatorâ€™s job is to tell if the things are real or fake. The generator and the discriminator are like two players in a game. The generator tries to fool the discriminator by making fake things that look real, and the discriminator tries to catch the generator by telling if the things are real or fake. The generator and the discriminator learn from each other and get better at their jobs over time.\n\nThe code defines a class called GAN_MODEL that inherits from another class called ks.models.Model. A class is like a blueprint that tells how to make something. In this case, the class tells how to make a GAN model.\n\nThe first part of the code is the __init__ method. This method is like a constructor that tells how to create an instance of the class. An instance is like an object that is made from the blueprint. In this case, the method tells how to create a GAN model object.\n\nThe method takes four arguments: generator, discriminator, *args, and **kwargs. The first two arguments are the generator and the discriminator models that are used to make the GAN model. The last two arguments are special symbols that mean any other arguments that are not specified.\n\nThe method calls another method called super().__init__(*args, **kwargs). This method is like calling the constructor of the parent class, which is ks.models.Model. This means that the GAN model object will have all the features of the parent class, plus some extra features that are defined in the child class.\n\nThe method then creates two attributes for the GAN model object: self.generator and self.discriminator. These attributes are like variables that store some values inside the object. In this case, they store the generator and the discriminator models that are passed as arguments.\n\nThe second part of the code is the compile method. This method is like a preparer that tells how to get ready for training the model. Training is like teaching the model how to do its job.\n\nThe method takes six arguments: genrator_optimezer, detector_optimizer, gen_loss, detector_loss, *args, and **kwargs. The first four arguments are some tools that are used to train the model. The last two arguments are again special symbols that mean any other arguments that are not specified.\n\nThe method calls another method called super().compile(*args, **kwargs). This method is like calling the preparer of the parent class, which is ks.models.Model. This means that the GAN model object will get ready for training using all the tools of the parent class, plus some extra tools that are defined in the child class.\n\nThe method then creates four more attributes for the GAN model object: self.g_opt, self.d_opt, self.g_loss, and self.d_loss. These attributes store the tools that are passed as arguments.\n\nThe third part of the code is the train_step method. This method is like a trainer that tells how to train the model for one step. A step is like one round of playing the game between the generator and the discriminator.\n\nThe method takes one argument: batch. This argument is like a bunch of data that is used to train the model. In this case, it is a bunch of real images.\n\nThe method does several things:\n\nIt gets some data by assigning real_images to be equal to batch. This means that it uses the bunch of real images as data.\nIt makes some new things by calling self.generator(tf.random.normal((input_shape)), training=False) and assigning it to be equal to fake_images. This means that it uses the generator model to make some fake images from some random numbers.\nIt trains the discriminator by doing these things:\nIt passes both real and fake images to the discriminator model by calling self.discriminator(real_images, training=True) and self.discriminator(fake_images, training=True) and assigning them to be equal to yhat_real and yhat_fake. These mean that it gets some predictions from the discriminator model about whether the images are real or fake.\nIt combines both predictions into one by calling tf.concat([yhat_real, yhat_fake], axis=0) and assigning it to be equal to yhat_realfake. This means that it puts the predictions together in one list.\nIt creates some labels for real and fake images by calling tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0) and assigning it to be equal to y_realfake. This means that it makes a list of zeros and ones, where zeros mean real and ones mean fake.\nIt adds some noise to the labels by doing these things:\nIt makes some random numbers between 0 and 0.15 by calling 0.15*tf.random.uniform(tf.shape(yhat_real)) and 0.15*tf.random.uniform(tf.shape(yhat_fake)) and assigning them to be equal to noise_real and noise_fake. This means that it makes some small numbers that are different for each prediction.\nIt adds the noise to the labels by calling tf.concat([noise_real, noise_fake], axis=0) and adding it to y_realfake. This means that it makes the labels a little bit different from zeros and ones, so that they are not too easy or too hard for the discriminator model to learn.\nIt calculates the loss by calling self.d_loss(y_realfake, yhat_realfake) and assigning it to be equal to total_d_loss. This means that it measures how well the discriminator model did its job by comparing the labels and the predictions. The loss is like a score that tells how much the discriminator model needs to improve.\nIt applies backpropagation by doing these things:\nIt calculates the gradients by calling d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) and assigning it to be equal to dgrad. This means that it finds out how much each part of the discriminator model contributed to the loss.\nIt updates the discriminator model by calling self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables)). This means that it changes the parts of the discriminator model a little bit, so that they can do better next time.\nIt trains the generator by doing these things:\nIt makes some new images by calling self.generator(tf.random.normal((input_shape)), training=True) and assigning it to be equal to gen_images. This means that it uses the generator model to make some more fake images from some random numbers.\nIt creates some predicted labels by calling self.discriminator(gen_images, training=False) and assigning it to be equal to predicted_labels. This means that it gets some predictions from the discriminator model about whether the new fake images are real or fake.\nIt calculates the loss by calling self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) and assigning it to be equal to total_g_loss. This means that it measures how well the generator model did its job by comparing the predictions with zeros. The loss is like a score that tells how much the generator model needs to improve. The generator model wants to make fake images that look real, so it wants the predictions to be close to zeros.\nIt applies backpropagation by doing these things:\nIt calculates the gradients by calling g_tape.gradient(total_g_loss, self.generator.trainable_variables) and assigning it to be equal to ggrad. This means that it finds out how much each part of the generator model contributed to the loss.\nIt updates the generator model by calling self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables)). This means that it changes the parts of the generator model a little bit, so that they can do better next time.\nThe method then returns a dictionary with two values: \"detector_loss\" and \"generator_loss\". A dictionary is like a container that stores some values with some names. In this case, it stores the losses of both models with their names.\n\nThe last part of the code is outside of the class definition. It does these things:\n\nIt creates a GAN model object called fashgan by calling GAN_MODEL(generator, detector). This means that it uses the blueprint of the class to make an object with a generator model and a discriminator model as arguments.\nIt prepares the GAN model object for training by calling fashgan.compile(genrator_optimezer, detector_optimizer, gen_loss, detector_loss). This means that it uses the preparer method of the class with some tools as arguments.\nIt checks if there are any callbacks or validation data by using some if-else statements. A callback is like a helper function that does something during training, such as saving or logging. A validation data is like a test data that is used\n","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras as ks\ndef training_model_for_GANs(genrator_optimezer, detector_optimizer, gen_loss, detector_loss,generator, detector,training_data,epochs,input_shape,Callbacks=None,val_data=None):\n  class GAN_MODEL(ks.models.Model): \n      def __init__(self, generator, discriminator, *args, **kwargs):\n          # Pass through args and kwargs to base class \n          super().__init__(*args, **kwargs)\n          \n          # Create attributes for gen and disc\n          self.generator = generator \n          self.discriminator =  detector\n          \n      def compile(self, genrator_optimezer, detector_optimizer, gen_loss, detector_loss, *args, **kwargs): \n          # Compile with base class\n          super().compile(*args, **kwargs)\n          \n          # Create attributes for losses and optimizers\n          self.g_opt = genrator_optimezer\n          self.d_opt = detector_optimizer\n          self.g_loss = gen_loss\n          self.d_loss = detector_loss\n\n      def train_step(self, batch):\n          # Get the data \n          real_images = batch\n          fake_images = self.generator(tf.random.normal((input_shape)), training=False)\n          \n          # Train the discriminator\n          with tf.GradientTape() as d_tape: \n              # Pass the real and fake images to the discriminator model\n              yhat_real = self.discriminator(real_images, training=True) \n              yhat_fake = self.discriminator(fake_images, training=True)\n              yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n              \n              # Create labels for real and fakes images\n              y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n              \n              # Add some noise to the TRUE outputs\n              noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n              noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n              y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n              \n              # Calculate loss - BINARYCROSS \n              total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n              \n          # Apply backpropagation - nn learn \n          dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n          self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n          \n          # Train the generator \n          with tf.GradientTape() as g_tape: \n              # Generate some new images\n              gen_images = self.generator(tf.random.normal((input_shape)), training=True)\n                                          \n              # Create the predicted labels\n              predicted_labels = self.discriminator(gen_images, training=False)\n                                          \n              # Calculate loss - trick to training to fake out the discriminator\n              total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n              \n          # Apply backprop\n          ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n          self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n          \n          return {\"detector_loss\":total_d_loss, \"generator_loss\":total_g_loss}\n  fashgan = GAN_MODEL(generator, detector)\n  fashgan.compile(genrator_optimezer, detector_optimizer, gen_loss, detector_loss)\n  ## read before runing IF there are labels to your code (EG you loaded it from git hub or kaggle)\n  training_data = training_data.as_numpy_iterator().next()\n  ## and when your fitting use this\n  ## fashgan.fit(training_data[0].....)\n  if Callbacks == None and val_data==  None:\n    return fashgan.fit(training_data[0],epochs=epochs)\n  elif Callbacks == None and val_data != None:\n    return fashgan.fit(training_data[0],epochs=epochs,validation_data=val_data)\n  elif Callbacks != None and val_data == None:\n    return fashgan.fit(training_data[0],epochs=epochs,callbacks=[Callbacks])\n  else:\n    return fashgan.fit(training_data[0],epochs=epochs,callbacks=[Callbacks],validation_data=val_data)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:03.569302Z","iopub.execute_input":"2023-09-29T08:56:03.569751Z","iopub.status.idle":"2023-09-29T08:56:03.588718Z","shell.execute_reply.started":"2023-09-29T08:56:03.569705Z","shell.execute_reply":"2023-09-29T08:56:03.587041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now lets train our model!!!!\ntraining_model_for_GANs(genrator_optimezer = tf.keras.optimizers.Adam()\n                        , detector_optimizer  = tf.keras.optimizers.Adam()\n                        , gen_loss  = tf.keras.losses.binary_crossentropy\n                        , detector_loss = tf.keras.losses.categorical_crossentropy\n                        ,generator = generator\n                        , detector = detector\n                        ,training_data = true\n                        ,epochs = 3\n                        ,input_shape = (1,16, 16, 256)\n                        ,Callbacks=None\n                        ,val_data=None)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:03.590678Z","iopub.execute_input":"2023-09-29T08:56:03.592028Z","iopub.status.idle":"2023-09-29T08:56:38.569176Z","shell.execute_reply.started":"2023-09-29T08:56:03.591987Z","shell.execute_reply":"2023-09-29T08:56:38.567938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obv I did low epochs cuz Its just an explination so i did so but you can try for longer and you can add callbacks if you want or val_data its all up to you","metadata":{}},{"cell_type":"markdown","source":"# predicting with our model","metadata":{}},{"cell_type":"markdown","source":"obv we trained on very low epoch (would suggest 2000+) but I am just explaining so the images will look awful but if you trained for longer it will get better obv","metadata":{}},{"cell_type":"code","source":"img = generator.predict(np.random.randn(4,16,16,256))\n# Setup the subplot formatting \nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\n# Loop four times and get images \nfor idx, img in enumerate(img): \n    # Plot the image using a specific subplot \n    ax[idx].imshow(np.squeeze(img))\n    # Appending the image label as the plot title \n    ax[idx].title.set_text(idx)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T08:56:38.571989Z","iopub.execute_input":"2023-09-29T08:56:38.572774Z","iopub.status.idle":"2023-09-29T08:56:40.690354Z","shell.execute_reply.started":"2023-09-29T08:56:38.572726Z","shell.execute_reply":"2023-09-29T08:56:40.68927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"wow something changed which is good\n\nit actually looks not half bad tbh it looks 90% bad lol ðŸ˜‚ðŸ˜‚","metadata":{}},{"cell_type":"markdown","source":"alr thats it hope that yall enjoyed this cool intro course and stay happy cia :D","metadata":{}},{"cell_type":"markdown","source":"if you still dont know how to use yolo v8 I have an intro course [here](https://www.kaggle.com/code/philopateergeorgei/intro-to-yolo-v8-for-object-detection)\n\nIf you like any of them toss me an upvote","metadata":{}}]}