{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":30066,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHello there,\n\nFirst of all, if you are completely new to data science field I highly recommend checking out [kaggle courses](https://www.kaggle.com/learn/overview) to get started. Furthermore, I'd like to recommend a few amazing kernels about this particular competition:\n1. https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n2. https://www.kaggle.com/cheesu/house-prices-1st-approach-to-data-science-process\n3. https://www.kaggle.com/angqx95/data-science-workflow-top-2-with-tuning\n4. https://www.kaggle.com/datafan07/top-1-approach-eda-new-models-and-stacking\n\nThese notebooks are amazing, and I learnt a ton from them so hope you will too :)\n\nIn this kernel you will find my approach to this regression problem.\n\nHere's a table of contents:\n\n1. Meeting our data\n\n2. Visualization and data analysis\n    \n    2.1 Target variable and numerical data\n    \n    2.2 Categorical data\n    \n3. Data cleaning\n\n    3.1 Dealing with null values\n    \n    3.2 Label encoding\n    \n    3.3 Dealing with outliers\n    \n4. Feature engineering\n\n5. Data normalization and one-hot encoding\n\n6. Creating and evaluating a model\n\n    6.1 Parameter tuning\n    \n    6.2 Models evaluations\n    \n    6.3 Model stacking","metadata":{}},{"cell_type":"markdown","source":"# 1. Meeting our data:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv', index_col = 'Id')\ntest = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv', index_col = 'Id')\n\ntrain.tail(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:14.202826Z","iopub.execute_input":"2025-04-15T16:44:14.203274Z","iopub.status.idle":"2025-04-15T16:44:14.329914Z","shell.execute_reply.started":"2025-04-15T16:44:14.20318Z","shell.execute_reply":"2025-04-15T16:44:14.328674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:15.471666Z","iopub.execute_input":"2025-04-15T16:44:15.472025Z","iopub.status.idle":"2025-04-15T16:44:15.504489Z","shell.execute_reply.started":"2025-04-15T16:44:15.471992Z","shell.execute_reply":"2025-04-15T16:44:15.503291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.dtypes.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:17.614645Z","iopub.execute_input":"2025-04-15T16:44:17.615Z","iopub.status.idle":"2025-04-15T16:44:17.621074Z","shell.execute_reply.started":"2025-04-15T16:44:17.614969Z","shell.execute_reply":"2025-04-15T16:44:17.62019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.select_dtypes(exclude = 'object').describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:18.941156Z","iopub.execute_input":"2025-04-15T16:44:18.941542Z","iopub.status.idle":"2025-04-15T16:44:19.044848Z","shell.execute_reply.started":"2025-04-15T16:44:18.941503Z","shell.execute_reply":"2025-04-15T16:44:19.043864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.select_dtypes(include = ['object']).describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:19.357519Z","iopub.execute_input":"2025-04-15T16:44:19.357883Z","iopub.status.idle":"2025-04-15T16:44:19.477715Z","shell.execute_reply.started":"2025-04-15T16:44:19.357848Z","shell.execute_reply":"2025-04-15T16:44:19.47675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target = train.SalePrice.copy()\ntarget.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:20.655323Z","iopub.execute_input":"2025-04-15T16:44:20.655737Z","iopub.status.idle":"2025-04-15T16:44:20.666094Z","shell.execute_reply.started":"2025-04-15T16:44:20.655665Z","shell.execute_reply":"2025-04-15T16:44:20.664887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('In train data there are: {} categorical features;\\n\\t\\t\\t {} numerical features'.format(train.select_dtypes(include = ['object']).columns.size,\n                                                                                   train.drop('SalePrice', axis = 1).select_dtypes(exclude = ['object']).columns.size))\n\nprint('In test data there are: {} categorical features;\\n\\t\\t\\t {} numerical features.'.format(test.select_dtypes(include = ['object']).columns.size,\n                                                                                   test.select_dtypes(exclude = ['object']).columns.size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:21.871842Z","iopub.execute_input":"2025-04-15T16:44:21.872214Z","iopub.status.idle":"2025-04-15T16:44:21.888195Z","shell.execute_reply.started":"2025-04-15T16:44:21.872179Z","shell.execute_reply":"2025-04-15T16:44:21.887204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train.drop('SalePrice', axis = 1).columns).equals(test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:23.212419Z","iopub.execute_input":"2025-04-15T16:44:23.212791Z","iopub.status.idle":"2025-04-15T16:44:23.220867Z","shell.execute_reply.started":"2025-04-15T16:44:23.212722Z","shell.execute_reply":"2025-04-15T16:44:23.219863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Visualization and data analysis","metadata":{}},{"cell_type":"code","source":"# Setting up Seaborn library\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom scipy import stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:25.934926Z","iopub.execute_input":"2025-04-15T16:44:25.935575Z","iopub.status.idle":"2025-04-15T16:44:26.915224Z","shell.execute_reply.started":"2025-04-15T16:44:25.935515Z","shell.execute_reply":"2025-04-15T16:44:26.914108Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2.1 Target variable and numerical data","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6), gridspec_kw={'width_ratios': [3, 2]})\n\n# Left plot: Histogram (transparent black) + KDE (solid red)\nsns.histplot(target, kde=False, color='black', stat='density', alpha=0.45, ax=ax1)  # Filled bars\nsns.kdeplot(target, color='red', lw=3, ax=ax1)  # Bold red curve\nax1.set_title('Histogram of SalePrice', fontsize=16)\n\n# Right plot: Probability plot (red points, black line)\nstats.probplot(target, plot=ax2)\nax2.get_lines()[0].set_color('red')  # Points\nax2.get_lines()[1].set_color('black') # Reference line\nax2.set_title('Probability Plot of SalePrice', fontsize=16)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:50:56.188448Z","iopub.execute_input":"2025-04-15T16:50:56.188812Z","iopub.status.idle":"2025-04-15T16:50:56.82172Z","shell.execute_reply.started":"2025-04-15T16:50:56.188777Z","shell.execute_reply":"2025-04-15T16:50:56.820768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_grid(data, fig_size, grid_size, plot_type, target = ''):\n    \"\"\"\n    Custom function for plotting grid of plots.\n    It takes: DataFrame of data, size of a grid, type of plots, string name of target variable;\n    And it outputs: grid of plots.\n    \"\"\"\n    fig = plt.figure(figsize = fig_size)\n    if plot_type == 'histplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.histplot(data[column_name], kde = True, color = 'red', stat = 'count')\n            plot.set_xlabel(column_name, fontsize = 16)\n    if plot_type == 'boxplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.boxplot(x = data[column_name], color = 'red')\n            plot.set_xlabel(column_name, fontsize = 16)\n    if plot_type == 'scatterplot':\n        for i, column_name in enumerate(data.drop(target, axis = 1).select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.scatterplot(x = data[column_name], y = data[target], color = 'red')\n            plot.set_xlabel(column_name, fontsize = 16)\n    if plot_type == 'boxplot_cat':\n        for i, column_name in enumerate(data.select_dtypes(include = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            sort = data.groupby([column_name])[target].median().sort_values(ascending = False) # This is here to make sure boxes are sorted by median\n            plot = sns.boxplot(x = data[column_name], y = data[target], order = sort.index, palette = 'Reds')\n            plot.set_xlabel(column_name, fontsize = 16)\n    plt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# numerical_data = train.drop('SalePrice', axis = 1).select_dtypes(exclude = 'object')\n    \nplot_grid(train.drop('SalePrice', axis = 1), fig_size = (20, 40), grid_size = (12, 3), plot_type = 'histplot')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correlation = train.corr()\nplt.figure(figsize = (20,10))\nsns.heatmap(correlation.loc[::-1,::-1], \n            square = True, \n            vmax = 0.8,)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Heatmap of numerical features correlation with target sorted by value of correlation coefficient in descending order\nplt.figure(figsize = (40,20))\nsns.heatmap(correlation.sort_values(by = 'SalePrice', axis = 0, ascending = False).iloc[:,-1:], \n            square = True, \n            annot = True, \n            fmt = '.2f', \n            cbar = False,)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Heatmap for first n numerical features that correlate with target the most \nn = 20\nplt.figure(figsize = (32,10))\nsns.heatmap(train[correlation.nlargest(n, 'SalePrice').index].corr(), \n            annot = True, \n            fmt = '.2f', \n            square = True, \n            cbar = False,)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_grid(train, fig_size = (20, 40), grid_size = (12, 3), plot_type = 'scatterplot', target = 'SalePrice')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_grid(train.drop('SalePrice', axis = 1), fig_size = (20, 40), grid_size = (12, 3), plot_type = 'boxplot')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2.2 Categorical data","metadata":{}},{"cell_type":"code","source":"train.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plotting categorical features sorted by cardinality in descending order.","metadata":{}},{"cell_type":"code","source":"plot_grid(pd.concat([train[list(train.select_dtypes(include = 'object').nunique().sort_values(ascending = False).index)], \n                     target], axis = 1), \n          fig_size = (20, 40), grid_size = (15, 3), plot_type = 'boxplot_cat', target = 'SalePrice')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_grid(train[['Neighborhood', 'Exterior2nd', 'Exterior1st', 'SalePrice']], \n          fig_size = (20, 40), grid_size = (3, 1), \n          plot_type = 'boxplot_cat', target = 'SalePrice')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Data cleaning","metadata":{}},{"cell_type":"markdown","source":"# 3.1 Dealing with null values","metadata":{}},{"cell_type":"code","source":"train_cleaning = train.drop('SalePrice', axis = 1).copy()\ntest_cleaning = test.copy()\ntrain_test = pd.concat([train_cleaning, test_cleaning])\nmissing_values = pd.concat([train_test.isnull().sum().sort_values(ascending = False),\n                            train_test.isnull().sum().sort_values(ascending = False).apply(lambda x: (x / train_test.shape[0]) * 100)],\n                            axis = 1, keys = ['Values missing', 'Percent of missing'])\nmissing_values[missing_values['Values missing'] > 0].style.background_gradient('Reds')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"replace_zero = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea', 'BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF1', 'GarageCars', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea', 'BsmtFinSF2']\nreplace_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType', 'BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'MasVnrType', 'Exterior2nd', 'Exterior1st']\nreplace_mode = ['Functional', 'Utilities', 'KitchenQual', 'SaleType', 'Electrical']\n\n# Replace null values in MSZoning according to MSSubClass\n# train_cleaning.MSZoning = train_cleaning.groupby('MSSubClass')['MSZoning'].apply(lambda x: x.fillna(x.mode()[0]))\n\ntrain_cleaning[replace_zero] = train_cleaning[replace_zero].fillna(0)\n\ntrain_cleaning[replace_none] = train_cleaning[replace_none].fillna('None')\n\nfor col_name in replace_mode:\n    train_cleaning[col_name].replace(np.nan, train_cleaning[col_name].mode()[0], inplace = True)\n\n# Replace null values in test data separately from train data  \ntest_cleaning.MSZoning = test_cleaning.groupby('MSSubClass')['MSZoning'].apply(lambda x: x.fillna(x.mode()[0]))\n\ntest_cleaning[replace_zero] = test_cleaning[replace_zero].fillna(0)\n\ntest_cleaning[replace_none] = test_cleaning[replace_none].fillna('None')\n\nfor col_name in replace_mode:\n    test_cleaning[col_name].replace(np.nan, train_cleaning[col_name].mode()[0], inplace = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cleaning.isnull().sum().max()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_cleaning.isnull().sum().max()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.2 Label encoding","metadata":{}},{"cell_type":"markdown","source":"Label encoding three categorical features with high cardinality.","metadata":{}},{"cell_type":"code","source":"# Converting some of the categorical values to numeric ones. Choosing similar values for closer groups to balance linear relations\nneigh_map = {'MeadowV': 1, \n             'IDOTRR': 1, \n             'BrDale': 1,\n             'BrkSide': 2,\n             'OldTown': 2,\n             'Edwards': 2,\n             'Sawyer': 3,\n             'Blueste': 3,\n             'SWISU': 3,\n             'NPkVill': 3,\n             'NAmes': 3,\n             'Mitchel': 4,\n             'SawyerW': 5,\n             'NWAmes': 5,\n             'Gilbert': 5,\n             'Blmngtn': 5,\n             'CollgCr': 5,\n             'ClearCr': 6,\n             'Crawfor': 6,\n             'Veenker': 7,\n             'Somerst': 7,\n             'Timber': 8,\n             'StoneBr': 9,\n             'NridgHt': 10,\n             'NoRidge': 10}\ntrain_cleaning['Neighborhood'] = train_cleaning['Neighborhood'].map(neigh_map).astype('int')\ntest_cleaning['Neighborhood'] = test_cleaning['Neighborhood'].map(neigh_map).astype('int')\n\n# Replacing misspelled values\ntest_cleaning['Exterior2nd'] = test_cleaning['Exterior2nd'].apply(lambda x: 'BrkComm' if (x == 'Brk Cmn') else 'CemntBd' if (x == 'CmentBd') else x)\ntrain_cleaning['Exterior2nd'] = train_cleaning['Exterior2nd'].apply(lambda x: 'BrkComm' if (x == 'Brk Cmn') else 'CemntBd' if (x == 'CmentBd') else x)\n# Creating new simple feature\ntrain_cleaning['ExteriorSame'] = (train_cleaning['Exterior1st'] == train_cleaning['Exterior2nd']).apply(lambda x: 1 if x == True else 0)\ntest_cleaning['ExteriorSame'] = (test_cleaning['Exterior1st'] == test_cleaning['Exterior2nd']).apply(lambda x: 1 if x == True else 0)\n\next1_map = {'None': 0, \n            'BrkComm': 1, \n            'AsphShn': 2,\n            'CBlock': 2,\n            'AsbShng': 3,\n            'WdShing': 4,\n            'Wd Sdng': 5,\n            'MetalSd': 5,\n            'Stucco': 6,\n            'HdBoard': 7,\n            'BrkFace': 8,\n            'Plywood': 8,\n            'VinylSd': 9,\n            'CemntBd': 10,\n            'Stone': 11,\n            'ImStucc': 12}\ntrain_cleaning['Exterior1st'] = train_cleaning['Exterior1st'].map(ext1_map).astype('int')\ntest_cleaning['Exterior1st'] = test_cleaning['Exterior1st'].map(ext1_map).astype('int')\n\next2_map = {'None': 0, \n            'BrkComm': 4, \n            'AsphShn': 3,\n            'CBlock': 1,\n            'AsbShng': 2,\n            'WdShing': 4,\n            'Wd Sdng': 3,\n            'Wd Shng': 3,\n            'MetalSd': 3,\n            'Stucco': 4,\n            'HdBoard': 5,\n            'BrkFace': 6,\n            'Plywood': 6,\n            'VinylSd': 9,\n            'CemntBd': 10,\n            'Stone': 7,\n            'ImStucc': 8,\n            'Other': 11}\ntrain_cleaning['Exterior2nd'] = train_cleaning['Exterior2nd'].map(ext2_map).astype('int')\ntest_cleaning['Exterior2nd'] = test_cleaning['Exterior2nd'].map(ext2_map).astype('int')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Label encoding other features where it's appropriate. (you can check it by looking into dataset documentation)","metadata":{}},{"cell_type":"code","source":"qual_map = {'None': 0, \n            'Po': 1, \n            'Fa': 2, \n            'TA': 3, \n            'Gd': 4, \n            'Ex': 5}\ntrain_cleaning['ExterQual'] = train_cleaning['ExterQual'].map(qual_map).astype('int')\ntest_cleaning['ExterQual'] = test_cleaning['ExterQual'].map(qual_map).astype('int')\n\ntrain_cleaning['ExterCond'] = train_cleaning['ExterCond'].map(qual_map).astype('int')\ntest_cleaning['ExterCond'] = test_cleaning['ExterCond'].map(qual_map).astype('int')\n\ntrain_cleaning['BsmtQual'] = train_cleaning['BsmtQual'].map(qual_map).astype('int')\ntest_cleaning['BsmtQual'] = test_cleaning['BsmtQual'].map(qual_map).astype('int')\n\ntrain_cleaning['BsmtCond'] = train_cleaning['BsmtCond'].map(qual_map).astype('int')\ntest_cleaning['BsmtCond'] = test_cleaning['BsmtCond'].map(qual_map).astype('int')\n\ntrain_cleaning['HeatingQC'] = train_cleaning['HeatingQC'].map(qual_map).astype('int')\ntest_cleaning['HeatingQC'] = test_cleaning['HeatingQC'].map(qual_map).astype('int')\n\ntrain_cleaning['KitchenQual'] = train_cleaning['KitchenQual'].map(qual_map).astype('int')\ntest_cleaning['KitchenQual'] = test_cleaning['KitchenQual'].map(qual_map).astype('int')\n\ntrain_cleaning['FireplaceQu'] = train_cleaning['FireplaceQu'].map(qual_map).astype('int')\ntest_cleaning['FireplaceQu'] = test_cleaning['FireplaceQu'].map(qual_map).astype('int')\n\ntrain_cleaning['GarageQual'] = train_cleaning['GarageQual'].map(qual_map).astype('int')\ntest_cleaning['GarageQual'] = test_cleaning['GarageQual'].map(qual_map).astype('int')\n\ntrain_cleaning['GarageCond'] = train_cleaning['GarageCond'].map(qual_map).astype('int')\ntest_cleaning['GarageCond'] = test_cleaning['GarageCond'].map(qual_map).astype('int')\n\nbsmtexposure_map = {'None': 0, \n                    'No': 1, \n                    'Mn': 2, \n                    'Av': 3, \n                    'Gd': 4}\ntrain_cleaning['BsmtExposure'] = train_cleaning['BsmtExposure'].map(bsmtexposure_map).astype('int')\ntest_cleaning['BsmtExposure'] = test_cleaning['BsmtExposure'].map(bsmtexposure_map).astype('int')\n\nfence_map = {'None': 0, \n             'MnWw': 1, \n             'GdWo': 2, \n             'MnPrv': 3, \n             'GdPrv': 4}\ntrain_cleaning['Fence'] = train_cleaning['Fence'].map(fence_map).astype('int')\ntest_cleaning['Fence'] = test_cleaning['Fence'].map(fence_map).astype('int')\n\nbsmf_map = {'None': 0,\n            'Unf': 1,\n            'LwQ': 2,\n            'Rec': 3,\n            'BLQ': 4,\n            'ALQ': 5,\n            'GLQ': 6}\ntrain_cleaning['BsmtFinType1'] = train_cleaning['BsmtFinType1'].map(bsmf_map).astype('int')\ntest_cleaning['BsmtFinType1'] = test_cleaning['BsmtFinType1'].map(bsmf_map).astype('int')\ntrain_cleaning['BsmtFinType2'] = train_cleaning['BsmtFinType2'].map(bsmf_map).astype('int')\ntest_cleaning['BsmtFinType2'] = test_cleaning['BsmtFinType2'].map(bsmf_map).astype('int')\n\ngaragef_map = {'None': 0,\n               'Unf': 1,\n               'RFn': 2,\n               'Fin': 3}\ntrain_cleaning['GarageFinish'] = train_cleaning['GarageFinish'].map(garagef_map).astype('int')\ntest_cleaning['GarageFinish'] = test_cleaning['GarageFinish'].map(garagef_map).astype('int')\n\npoolqc_map = {'None': 0, \n              'Fa': 2, \n              'TA': 3, \n              'Gd': 4, \n              'Ex': 5}\ntrain_cleaning['PoolQC'] = train_cleaning['PoolQC'].map(poolqc_map).astype('int')\ntest_cleaning['PoolQC'] = test_cleaning['PoolQC'].map(poolqc_map).astype('int')\n\nstr_all_map = {'None': 0, \n               'Grvl': 1, \n               'Pave': 2}\ntrain_cleaning['Street'] = train_cleaning['Street'].map(str_all_map).astype('int')\ntest_cleaning['Street'] = test_cleaning['Street'].map(str_all_map).astype('int')\ntrain_cleaning['Alley'] = train_cleaning['Alley'].map(str_all_map).astype('int')\ntest_cleaning['Alley'] = test_cleaning['Alley'].map(str_all_map).astype('int')\n\ncent_air_map = {'N': 0, \n                'Y': 1}\ntrain_cleaning['CentralAir'] = train_cleaning['CentralAir'].map(cent_air_map).astype('int')\ntest_cleaning['CentralAir'] = test_cleaning['CentralAir'].map(cent_air_map).astype('int')\n\npave_drive_map = {'N': 0, \n                  'P': 1,\n                  'Y': 2}\ntrain_cleaning['PavedDrive'] = train_cleaning['PavedDrive'].map(pave_drive_map).astype('int')\ntest_cleaning['PavedDrive'] = test_cleaning['PavedDrive'].map(pave_drive_map).astype('int')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cleaning.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.3 Dealing with outliers","metadata":{}},{"cell_type":"code","source":"def get_outliers(X_y, cols):\n    \"\"\"\n    Custom function for dealing with outliers.\n    It takes: DataFrame of data, list of columns;\n    And it returns: list of unique indexes of outliers.(Also it outputs all outliers with indexes for each column)\n    (value is considered an outlier if absolute value of its z-score is > 3)\n    \"\"\"\n    outliers_index = []\n    for col in cols:\n        right_outliers = X_y[col][(X_y[col] - X_y[col].mean()) / X_y[col].std() > 3]\n        left_outliers = X_y[col][(X_y[col] - X_y[col].mean()) / X_y[col].std() < -3]\n        all_outliers = right_outliers.append(left_outliers)\n        outliers_index += (list(all_outliers.index))\n        print('{} right outliers:\\n{} \\n {} left outliers:\\n{} \\n {} has TOTAL {} rows of outliers\\n'.format(col, right_outliers, col, left_outliers, col, all_outliers.count()))\n    outliers_index = list(set(outliers_index)) # Removing duplicates\n    print('There are {} unique rows with outliers in dataset'.format(len(outliers_index)))\n    return outliers_index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols = ['GrLivArea', 'TotalBsmtSF', 'FullBath', 'YearBuilt', 'YearRemodAdd']\nX_y = pd.concat([train_cleaning, target], axis = 1)\noutliers_index = get_outliers(X_y, cols)\nX_y = X_y.drop(outliers_index, axis = 0)\n\ntrain_cleaning = X_y.drop('SalePrice', axis = 1).copy()\ntarget_cleaned = X_y.SalePrice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cleaning","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Feature engineering","metadata":{}},{"cell_type":"code","source":"train_test = pd.concat([train_cleaning, test_cleaning], keys = ['train', 'test'], axis = 0)\n\ntrain_test['TotalPorchSF'] = (train_test['OpenPorchSF'] + train_test['3SsnPorch'] + \n                              train_test['EnclosedPorch'] + train_test['ScreenPorch'] + train_test['WoodDeckSF'])\n\ntrain_test['TotalSF'] = (train_test['BsmtFinSF1'] + train_test['BsmtFinSF2'] + \n                         train_test['1stFlrSF'] + train_test['2ndFlrSF'] + \n                         train_test['TotalPorchSF'])\n\ntrain_test['TotalBathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) + \n                                train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test['TotalRms'] = (train_test['TotRmsAbvGrd'] + train_test['TotalBathrooms'])\n\ntrain_test['YearSold'] = ((train_test['MoSold'] / 12) + train_test['YrSold']).astype('int')\n\ntrain_test['YearsAfterB'] = (train_test['YearSold'] - train_test['YearBuilt'])\n\ntrain_test['YearsAfterR'] = (train_test['YearSold'] - train_test['YearRemodAdd'])\n    \n# Merging quality and conditions\n\ntrain_test['TotalExtQual'] = (train_test['ExterQual'] + train_test['ExterCond'])\n\ntrain_test['TotalBsmtQual'] = (train_test['BsmtQual'] + train_test['BsmtCond'] + \n                               train_test['BsmtFinType1'] + train_test['BsmtFinType2'] + train_test['BsmtExposure'])\n\ntrain_test['TotalGrgQual'] = (train_test['GarageQual'] + train_test['GarageCond'] + train_test['GarageFinish'])\n\n# train_test['TotalPaved'] = (train_test['Street'] + train_test['Alley'] + train_test['PavedDrive'])\n\ntrain_test['TotalQual'] = (train_test['OverallQual'] + train_test['OverallCond'] + \n                           train_test['TotalExtQual'] + train_test['TotalBsmtQual'] + \n                           train_test['TotalGrgQual'] + train_test['KitchenQual'] + train_test['HeatingQC'] + \n                           train_test['FireplaceQu'] + train_test['PoolQC'] + train_test['Fence'] + \n                           train_test['CentralAir'])\n\n# Creating new features by using new quality indicators\n\ntrain_test['QualGr'] = train_test['TotalQual'] * train_test['GrLivArea']\n\ntrain_test['QualBsm'] = train_test['TotalBsmtQual'] * (train_test['BsmtFinSF1'] + train_test['BsmtFinSF2'])\n\ntrain_test['QualPorch'] = train_test['TotalExtQual'] * train_test['TotalPorchSF']\n\ntrain_test['QualExt'] = (train_test['TotalExtQual'] * \n                        (train_test['Exterior1st'] + train_test['Exterior2nd']) * train_test['MasVnrArea'])\n\ntrain_test['QualGrg'] = train_test['TotalGrgQual'] * train_test['GarageArea']\n\ntrain_test['QualFirepl'] = train_test['FireplaceQu'] * train_test['Fireplaces']\n\ntrain_test['QlLivArea'] = (train_test['GrLivArea'] - train_test['LowQualFinSF']) * (train_test['TotalQual'])\n\ntrain_test['QualSFNg'] = train_test['QualGr'] * train_test['Neighborhood']\n\ntrain_test['QualSF'] = train_test['TotalQual'] * train_test['TotalSF']\ntrain_test['QlSF'] = (train_test['TotalSF'] - train_test['LowQualFinSF']) * (train_test['TotalQual'])\ntrain_test['QualSFNg2'] = (train_test['QualGr'] + train_test['QualSF']) * train_test['Neighborhood']\ntrain_test['QualGrgNg'] = train_test['QualGrg'] * train_test['Neighborhood']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating new simple features\n\ntrain_test['HasPool'] = train_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['Has2ndFloor'] = train_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasGarage'] = train_test['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasBsmt'] = train_test['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasFireplace'] = train_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasPorch'] = train_test['QualPorch'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasLotFr'] = train_test['LotFrontage'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasFence'] = train_test['Fence'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['WasRemod'] = (train_test['YearRemodAdd'] != train_test['YearBuilt']).apply(lambda x: 1 if x == True else 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dropping all of the features, I found out to be useless during exploratory data analysis.","metadata":{}},{"cell_type":"code","source":"to_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtFinType2',\n    'BsmtQual',\n    'GarageQual',\n    'GarageFinish',\n    'KitchenQual',\n    'HeatingQC',\n    'FireplaceQu',\n    'YearSold',\n    'MiscVal',\n    'MiscFeature',\n    'Alley',\n    'PoolArea',\n    'LowQualFinSF',\n]\ntrain_test.drop(columns = to_drop, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizing new features\ntrain_cleaned = train_test.xs('train').copy()\n\nplot_grid(pd.concat([train_cleaned[['TotalPorchSF',\n                                    'TotalSF',\n                                    'TotalBathrooms',\n                                    'TotalRms',\n                                    'YearsAfterB',\n                                    'YearsAfterR',\n                                    'TotalExtQual',\n                                    'TotalBsmtQual',\n                                    'TotalGrgQual',\n                                    'TotalQual',\n                                    'QualGr',\n                                    'QualBsm',\n                                    'QualPorch',\n                                    'QualExt',\n                                    'QualGrg',\n                                    'QualFirepl',\n                                    'QlLivArea',\n                                    'QualSFNg',\n                                    'QualSF',\n                                    'QlSF',\n                                    'QualSFNg2',\n                                    'QualGrgNg']], target_cleaned], axis = 1), \n          fig_size = (20, 40), grid_size = (8, 3), plot_type = 'scatterplot', target = 'SalePrice')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize = (32,16))\nfor i, col_name in enumerate(['HasPool',\n                              'Has2ndFloor',\n                              'HasGarage',\n                              'HasBsmt',\n                              'HasFireplace',\n                              'HasPorch',\n                              'HasLotFr',\n                              'HasFence',\n                              'WasRemod']):\n    fig.add_subplot(5, 2, i + 1)\n    plot = sns.boxplot(x = train_cleaned[col_name], y = target_cleaned, palette = 'Reds')\n    plot.set_xlabel(col_name, fontsize = 16)\nplt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizing all numeric\nplot_grid(train_cleaned, fig_size = (20, 60), grid_size = (25, 3), plot_type = 'histplot')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Data normalization and one-hot encoding","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew, boxcox_normmax\nfrom scipy.special import boxcox1p\n\nskewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'Fence', 'TotalSF', 'TotalRms', 'TotalQual', 'TotalPorchSF',\n    'TotalBsmtQual', 'TotalGrgQual', 'QualPorch', 'QualFirepl', 'QualGr', \n    'QualGrg', 'QlLivArea', 'QualSFNg', 'QualExt',\n    'QualSF', 'QlSF', 'QualSFNg2', 'QualGrgNg', 'ExterCond', \n    'BsmtFinType1', 'BsmtCond', 'BsmtExposure', 'GarageCond',\n]\n\n# Finding skewness of the numerical features.\nskew_train = np.abs(train_cleaned[skewed].apply(lambda x: skew(x))).sort_values(ascending = False)\n\n# Filtering skewed features.\nhigh_skew_train = skew_train[skew_train > 0.3]\n\n# Taking column names of high skew.\nskew_columns_train = high_skew_train.index\n\ntest_cleaned = train_test.xs('test').copy()\n\n# Applying boxcox transformation to fix skewness.\nfor i in skew_columns_train:\n    lamb = boxcox_normmax(train_cleaned[i] + 1)\n    train_cleaned[i] = boxcox1p(train_cleaned[i], lamb)\n    test_cleaned[i] = boxcox1p(test_cleaned[i], lamb)\n    \nhigh_skew_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"skew_train = np.abs(train_cleaned[skewed].apply(lambda x: skew(x))).sort_values(ascending = False)\nhigh_skew_train = skew_train[skew_train > 0.3]\nhigh_skew_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_grid(train_cleaned[skewed], fig_size = (20, 40), grid_size = (14, 3), plot_type = 'histplot')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# categorical_features = [col_name for col_name in train_test.columns \n#                         if ((train_test[col_name].dtype == 'object' and train_test[col_name].nunique() < 10) \n#                             or (train_test[col_name].dtype in ['int64', 'float64']))]\ntrain_test_cleaned = pd.concat([train_cleaned, test_cleaned], keys = ['train', 'test'], axis = 0)\ntrain_test = pd.get_dummies(train_test_cleaned)\n\n# for col_name in train_test.columns:\n#     train_test[col_name] = (train_test[col_name] - train_test[col_name].mean()) / train_test[col_name].std()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from mlxtend.preprocessing import minmax_scaling\n\n# train_test = minmax_scaling(train_test, columns = train_test.columns)\n\nX_train_full, X_test = train_test.xs('train'), train_test.xs('test')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_full","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_full = np.log1p(target_cleaned)\ny_train_full","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Comparing distributions of a log transformed target variable and just target variable.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize = (24, 12), gridspec_kw={'width_ratios': [3, 2]})\n\nsns.histplot(y_train_full, kde = True, color = 'red', stat = 'count', ax = axs[0, 0])\naxs[0, 0].set_title('Histogram of log transfomed SalePrice', fontsize = 16)\nstats.probplot(y_train_full, plot = sns.lineplot(ax = axs[0, 1]))\naxs[0, 1].set_title('Probability Plot of log transfomed SalePrice', fontsize = 16)\naxs[0, 1].get_lines()[0].set_color('red')\naxs[0, 1].get_lines()[1].set_color('black')\n\nsns.histplot(target, kde = True, color = 'red', stat = 'count', ax = axs[1, 0])\naxs[1, 0].set_title('Histogram of SalePrice', fontsize = 16)\nstats.probplot(target, plot = sns.lineplot(ax = axs[1, 1]))\naxs[1, 1].set_title('Probability Plot of SalePrice', fontsize = 16)\naxs[1, 1].get_lines()[0].set_color('red')\naxs[1, 1].get_lines()[1].set_color('black')\n\nfig.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Creating and evaluating a model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ElasticNetCV\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# def scorer(y, y_pred):\n#     return mean_absolute_error(np.expm1(y_pred), np.expm1(y))\n\nmodel = LinearRegression()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of LinearRegression model: {}'.format(-scores.mean()))\n\nmodel = Lasso()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of Lasso model: {}'.format(-scores.mean()))\n\nmodel = Ridge()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of Ridge model: {}'.format(-scores.mean()))\n\nmodel = LGBMRegressor()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of LGBM model: {}'.format(-scores.mean()))\n\n# from sklearn.metrics import SCORERS # This two lines were written to see a list of possible strings for scoring\n# SCORERS.keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6.1 Parameter tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# def scorer(y, y_pred):\n#     return mean_absolute_error(np.expm1(y_pred), np.expm1(y))\ndef get_best_parameters(model, parameters, cv, search):\n    if (search == 'grid'):\n        grid = GridSearchCV(model, \n                            parameters,\n                            cv = cv, \n                            scoring = 'neg_root_mean_squared_error',\n                            n_jobs = -1)\n    \n    elif (search == 'randomized'):\n        grid = RandomizedSearchCV(model,\n                                  param_distributions = parameters,\n                                  n_iter = 100,\n                                  cv = cv, \n                                  scoring = 'neg_root_mean_squared_error',\n                                  n_jobs = -1)\n    \n    grid.fit(X_train_full, y_train_full)\n    return str(grid.best_params_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ridge_params = {\n    'alpha': [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, \n              6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10, 10.5]\n}\n\nlasso_params = {\n    'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, \n              0.0006, 0.0007, 0.0008, 0.0009]\n}\n\nelasticnet_params = {\n    'alpha' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007],\n    'l1_ratio' : [0, 0.5, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, \n                  0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99, 1]\n}\n\nxgboost_params = {\n    'learning_rate' : [0.01, 0.1, 0.15, 0.3, 0.5],\n    'n_estimators' : [100, 500, 1000, 2000, 3000],\n    'max_depth' : [3, 6, 9],\n    'min_child_weight' : [1, 5, 10, 20],\n    'reg_alpha' : [0.001, 0.01, 0.1],\n    'reg_lambda' : [0.001, 0.01, 0.1]\n}\n\nlightgbm_params = {\n    'max_depth' : [2, 5, 8, 10],\n    'learning_rate' : [0.001, 0.01, 0.1, 0.2],\n    'n_estimators' : [100, 300, 500, 1000, 1500],\n    'lambda_l1' : [0.0001, 0.001, 0.01],\n    'lambda_l2' : [0, 0.0001, 0.001, 0.01],\n    'feature_fraction' : [0.4, 0.6, 0.8],\n    'min_child_samples' : [5, 10, 20, 25]\n}\n\ngbr_params = {\n    'learning_rate' : [0.01, 0.1, 0.15, 0.3, 0.5],\n    'n_estimators' : [500, 1000, 1500, 2000, 2500, 3000, 3500],\n    'max_depth' : [3, 6, 9]\n}\n\ncbr_params = {\n    'n_estimators' : [100, 300, 500, 1000, 1300, 1600],\n    'learning_rate' : [0.0001, 0.001, 0.01, 0.1],\n    'l2_leaf_reg' : [0.001, 0.01, 0.1],\n    'random_strength' : [0.25, 0.5 ,1],\n    'max_depth' : [3, 6, 9],\n    'min_child_samples' : [2, 5, 10, 15, 20],\n    'rsm' : [0.5, 0.7, 0.9],\n}\n\nsvr_params = {\n    'svr__C' : [10, 10.5, 11, 11.5, 12, 12.5, 13, 13.5, 14, 14.5, 15, 15.5, 16,],\n    'svr__gamma' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007],\n}\n\nridge = Ridge() \nlasso = Lasso() \nelasticnet = ElasticNet()\nxgboost = XGBRegressor(booster = 'gbtree', objective = 'reg:squarederror')\nlightgbm = LGBMRegressor(boosting_type = 'gbdt',objective = 'regression')\ngbr = GradientBoostingRegressor()\ncbr = CatBoostRegressor(loss_function = 'RMSE', allow_writing_files = False, logging_level='Silent')\nsvr = make_pipeline(StandardScaler(), SVR())\n\nestimators = [ridge,\n              lasso,\n              elasticnet,]\n#               svr, \n#               lightgbm, \n#               gbr, \n#               cbr, \n#               xgboost]\nlabels = ['Ridge',\n          'Lasso',\n          'Elasticnet',]\n#           'SVR',\n#           'LightGBM', \n#           'GBR', \n#           'CBR']\n#           'XGBoost']\nestimators_params = [ridge_params,\n                     lasso_params,\n                     elasticnet_params,]\n#                      svr_params\n#                      lightgbm_params, \n#                      gbr_params,\n#                      cbr_params]\n#                      xgboost_params]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finding the best parameters for all of these models using GridSearchCV and RandomizedSearchCV is time consuming, especially gradient boosting models. (in previous version of this kernel during commit it took almost 7 hours to compute parameters for all of these models excluding SVR and XGBoost)\n\nIf you want to try it yourself, then don't forget that Kaggle kernel stops working automatically after 9 hours of a session and that they are running on cloud machines so computation abilities are limited. It's better to download jupyter notebook and try it on your own computer. (also don't forget to set n_jobs parameter to -1 in both GridSearchCV and RandomizedSearchCV)\n\nSo here, for the sake of saving some time, I'm gonna find parameters only for 'Ridge', 'Lasso' and 'Elasticnet' models.\nFor other models I will use parameters from this great kernel:\nhttps://www.kaggle.com/datafan07/top-1-approach-eda-new-models-and-stacking","metadata":{}},{"cell_type":"code","source":"best_parameters = pd.DataFrame(columns = ['Model name', 'Best parameters'])\n\nfor i in range(len(estimators)):\n    best_parameters.loc[i, 'Model name'] = labels[i]\n    if (labels[i] in ['XGBoost', 'LightGBM', 'GBR', 'CBR']):\n        best_parameters.loc[i, 'Best parameters'] = get_best_parameters(estimators[i], \n                                                                    estimators_params[i], \n                                                                    cv = 10, \n                                                                    search = 'randomized')\n    else:\n        best_parameters.loc[i, 'Best parameters'] = get_best_parameters(estimators[i], \n                                                                    estimators_params[i], \n                                                                    cv = 10, \n                                                                    search = 'grid')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6.2 Models evaluations","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_validate\n\ndef test_estimators(X, y, estimators, labels, cv):\n    ''' \n    A function for testing multiple estimators.\n    It takes: full train data and target, list of estimators, \n              list of labels or names of estimators,\n              cross validation splitting strategy;\n    And it returns: a DataFrame of table with results of tests\n    '''\n    result_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        est_name = label\n        result_table.loc[row_index, 'Model Name'] = est_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv = cv,\n                                    scoring = 'neg_root_mean_squared_error',\n#                                     return_train_score = True,\n                                    n_jobs = -1)\n\n#         result_table.loc[row_index, 'Train RMSE'] = -cv_results['train_score'].mean()\n        result_table.loc[row_index, 'Test RMSE'] = -cv_results['test_score'].mean()\n        result_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        result_table.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    result_table.sort_values(by=['Test RMSE'], ascending = True, inplace = True)\n\n    return result_table","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ast import literal_eval # To convert string to dictionary\n\nlinear = LinearRegression()\nridge = Ridge(**literal_eval(best_parameters.loc[0, 'Best parameters']))\nlasso = Lasso(**literal_eval(best_parameters.loc[1, 'Best parameters']) )\nelasticnet = ElasticNet(**literal_eval(best_parameters.loc[2, 'Best parameters']))\n\nsvr = make_pipeline(StandardScaler(), SVR(C = 21,\n                                          epsilon = 0.0099, \n                                          gamma = 0.00017, \n                                          tol = 0.000121))\n\nlightgbm = LGBMRegressor(objective = 'regression',\n                         n_estimators = 3500,\n                         num_leaves = 5,\n                         learning_rate = 0.00721,\n                         max_bin = 163,\n                         bagging_fraction = 0.35711,\n                         n_jobs = -1,\n                         bagging_seed = 42,\n                         feature_fraction_seed = 42,\n                         bagging_freq = 7,\n                         feature_fraction = 0.1294,\n                         min_data_in_leaf = 8)\n\ngbr = GradientBoostingRegressor(n_estimators = 2900,\n                                learning_rate = 0.0161,\n                                max_depth = 4,\n                                max_features = 'sqrt',\n                                min_samples_leaf = 17,\n                                loss = 'huber',\n                                random_state = 42)\n\ncbr = CatBoostRegressor(loss_function = 'RMSE', \n                        allow_writing_files = False, \n                        logging_level='Silent')\n\nxgboost = XGBRegressor(learning_rate = 0.0139,\n                       n_estimators = 4500,\n                       max_depth = 4,\n                       min_child_weight = 0,\n                       subsample = 0.7968,\n                       colsample_bytree = 0.4064,\n                       nthread = -1,\n                       scale_pos_weight = 2,\n                       seed = 42,)\n\nestimators = [linear,\n              ridge, \n              lasso, \n              elasticnet, \n              svr,\n              lightgbm, \n              gbr, \n              cbr,] \n#               xgboost]\n\nlabels = ['Linear',\n          'Ridge', \n          'Lasso', \n          'Elasticnet',\n          'SVR', \n          'LightGBM', \n          'GBR', \n          'CBR',]\n#           'XGBoost']\n\nresults = test_estimators(X_train_full, y_train_full, estimators, labels, cv = 10)\nresults.style.background_gradient(cmap = 'Reds')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6.3 Model stacking","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\n\nestimators = [\n    ('1', ridge),\n    ('2', lasso),\n    ('3', elasticnet),\n    ('4', lightgbm),\n    ('5', gbr),\n    ('6', cbr),\n    ('7', xgboost),\n    ('8', svr)\n]\n\nstacked = StackingRegressor(estimators = estimators, final_estimator = elasticnet, \n                            n_jobs = -1, verbose = 4, cv = 10)\nstacked.fit(X_train_full, y_train_full)\n\npredictions = np.floor(np.expm1(stacked.predict(X_test)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\nsubmission.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}