{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-16T17:15:06.690698Z","iopub.execute_input":"2022-02-16T17:15:06.690926Z","iopub.status.idle":"2022-02-16T17:15:06.708899Z","shell.execute_reply.started":"2022-02-16T17:15:06.690903Z","shell.execute_reply":"2022-02-16T17:15:06.707515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**According to Winston Churchill, \" We shape our homes, then our homes shape us \"**\n\nHence, in this notebook let's predict the house price using XGBoost.","metadata":{}},{"cell_type":"markdown","source":"**IMPORTING LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:00.358935Z","iopub.execute_input":"2022-02-16T17:10:00.360354Z","iopub.status.idle":"2022-02-16T17:10:00.849122Z","shell.execute_reply.started":"2022-02-16T17:10:00.360293Z","shell.execute_reply":"2022-02-16T17:10:00.848042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To load the training and validation sets in X_train, X_valid, y_train, and y_valid. The test set is loaded in X_test.**","metadata":{}},{"cell_type":"code","source":"# Read the data\nX = pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\nX_test_full = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:00.850204Z","iopub.execute_input":"2022-02-16T17:10:00.850414Z","iopub.status.idle":"2022-02-16T17:10:00.995719Z","shell.execute_reply.started":"2022-02-16T17:10:00.850382Z","shell.execute_reply":"2022-02-16T17:10:00.99485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBOOST - Extreme Gradient Boosting**\n\nXGBOOST is an implementation of gradient boosting with several additional features focused on performance and speed.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Build and train the model**\n\nBy defining the model with my_model_1 = XGBRegressor(random_state=0). Then, we can fit the model with the fit() method.\n\n","metadata":{}},{"cell_type":"code","source":"# Define the model\nmodel_1 = XGBRegressor(random_state=0) # Your code here\n\n# Fit the model\nmodel_1.fit(X_train, y_train) # Your code here","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:00.997459Z","iopub.execute_input":"2022-02-16T17:10:00.998073Z","iopub.status.idle":"2022-02-16T17:10:01.874749Z","shell.execute_reply.started":"2022-02-16T17:10:00.998034Z","shell.execute_reply":"2022-02-16T17:10:01.873194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Set `predictions_1` to the model's predictions for the validation data.**\n\n Using the predict() method to generate validation predictions.\n\n","metadata":{}},{"cell_type":"code","source":"# Get predictions\npredictions_1 = model_1.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:01.876807Z","iopub.execute_input":"2022-02-16T17:10:01.877019Z","iopub.status.idle":"2022-02-16T17:10:01.890106Z","shell.execute_reply.started":"2022-02-16T17:10:01.876994Z","shell.execute_reply":"2022-02-16T17:10:01.889321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finally, using the mean_absolute_error() function to calculate the mean absolute error (MAE) corresponding to the predictions for the validation set. Note that the labels for the validation data are stored in y_valid.**\n\n","metadata":{}},{"cell_type":"code","source":"# Calculate MAE\nmae_1 = mean_absolute_error(predictions_1, y_valid)\nprint(\"Mean Absolute Error:\" , mae_1)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:01.891197Z","iopub.execute_input":"2022-02-16T17:10:01.891392Z","iopub.status.idle":"2022-02-16T17:10:01.901257Z","shell.execute_reply.started":"2022-02-16T17:10:01.891367Z","shell.execute_reply":"2022-02-16T17:10:01.900689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Improving the model**\n\nInitially we trained a default model as baseline, it's time to tinker with the parameters, to see if we can get better performance!\nAccording to this problem, we chose to increase the number of trees in the model (with the n_estimators parameter) and decrease the learning rate (with the learning_rate parameter).","metadata":{}},{"cell_type":"code","source":"# Define the model\nmodel_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\n# Fit the model\nmodel_2.fit(X_train, y_train)\n\n# Get predictions\npredictions_2 = model_2.predict(X_valid)\n\n# Calculate MAE\nmae_2 = mean_absolute_error(predictions_2, y_valid)\nprint(\"Mean Absolute Error:\" , mae_2)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:01.902243Z","iopub.execute_input":"2022-02-16T17:10:01.903005Z","iopub.status.idle":"2022-02-16T17:10:09.258037Z","shell.execute_reply.started":"2022-02-16T17:10:01.902974Z","shell.execute_reply":"2022-02-16T17:10:09.257479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hence, our model in model_2 has lower MAE than the model in model_1.**","metadata":{}},{"cell_type":"code","source":"test_preds = model_2.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:09.261301Z","iopub.execute_input":"2022-02-16T17:10:09.262971Z","iopub.status.idle":"2022-02-16T17:10:09.303071Z","shell.execute_reply.started":"2022-02-16T17:10:09.262935Z","shell.execute_reply":"2022-02-16T17:10:09.302135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'Id': X_test_full.Id,'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\nprint(\"Successfully Predicted âœ the ðŸ¡ðŸ’’ðŸ  Price using XGBoostâœ¨ðŸ’°\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:10:09.306951Z","iopub.execute_input":"2022-02-16T17:10:09.30881Z","iopub.status.idle":"2022-02-16T17:10:09.323325Z","shell.execute_reply.started":"2022-02-16T17:10:09.308771Z","shell.execute_reply":"2022-02-16T17:10:09.321998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nimport os\n\nImage('/kaggle/input/home-sweet-home-pic/home_sweet_home.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:15:26.482345Z","iopub.execute_input":"2022-02-16T17:15:26.483317Z","iopub.status.idle":"2022-02-16T17:15:26.495176Z","shell.execute_reply.started":"2022-02-16T17:15:26.483271Z","shell.execute_reply":"2022-02-16T17:15:26.494422Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**According to Phillip Moffitt, \"A house is a home when it shelters the body & comforts the soul\"**\n\nHence, in this notebook we have successfully predicted the house price using XGBoost.","metadata":{}}]}