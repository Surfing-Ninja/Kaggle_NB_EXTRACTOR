{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:10px;color:black;margin:0;font-size:450%;text-align:center;display:fill;border-radius:5px;background-color:#e1f5e6;overflow:hidden;font-weight:500\"><b>Regression | EDA, Stacking & Ensembling</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üßæ Introduction</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"# üßæ  Introduction","metadata":{}},{"cell_type":"markdown","source":"## Stacking\nStacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance.\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190515104518/stacking.png)","metadata":{}},{"cell_type":"markdown","source":"## Ensemble\nEnsemble learning is a general meta approach to machine learning that seeks better predictive performance by combining the predictions from multiple models.\n![](https://machinelearningmastery.com/wp-content/uploads/2020/11/Bagging-Ensemble.png)","metadata":{}},{"cell_type":"markdown","source":"****<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìÑ Modules and Settings</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"# üìÑ 1. Modules and Settings","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Data Analysis Modules","metadata":{}},{"cell_type":"code","source":"import datetime\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt  \nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:09.450516Z","iopub.execute_input":"2022-09-19T01:02:09.450944Z","iopub.status.idle":"2022-09-19T01:02:10.346732Z","shell.execute_reply.started":"2022-09-19T01:02:09.450853Z","shell.execute_reply":"2022-09-19T01:02:10.345549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:10.348855Z","iopub.execute_input":"2022-09-19T01:02:10.350814Z","iopub.status.idle":"2022-09-19T01:02:10.355652Z","shell.execute_reply.started":"2022-09-19T01:02:10.350777Z","shell.execute_reply":"2022-09-19T01:02:10.354826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Data Analysis","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm, skew, kurtosis, boxcox #for some statistics\nfrom scipy.special import boxcox1p, inv_boxcox, inv_boxcox1p\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, LabelEncoder, RobustScaler, StandardScaler\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.feature_selection import SelectKBest, chi2\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\nfrom subprocess import check_output\nStartTime = datetime.datetime.now()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:10.356751Z","iopub.execute_input":"2022-09-19T01:02:10.357509Z","iopub.status.idle":"2022-09-19T01:02:10.608769Z","shell.execute_reply.started":"2022-09-19T01:02:10.357437Z","shell.execute_reply":"2022-09-19T01:02:10.607518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3. Modeling Modules and Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_validate\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import Pool, CatBoostRegressor","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:10.611966Z","iopub.execute_input":"2022-09-19T01:02:10.612419Z","iopub.status.idle":"2022-09-19T01:02:12.945055Z","shell.execute_reply.started":"2022-09-19T01:02:10.612383Z","shell.execute_reply":"2022-09-19T01:02:12.944066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4. Deep Learning Modeling Modules and Libraries","metadata":{}},{"cell_type":"code","source":"import keras\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\n#from keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, History, LambdaCallback\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.layers import Conv1D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPool1D\nfrom keras.layers import Flatten\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.optimizers import Adam   #for adam optimizer","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:12.948611Z","iopub.execute_input":"2022-09-19T01:02:12.94894Z","iopub.status.idle":"2022-09-19T01:02:19.318233Z","shell.execute_reply.started":"2022-09-19T01:02:12.948911Z","shell.execute_reply":"2022-09-19T01:02:19.317225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5. Helper Functions","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"class MyTimer():\n    # usage:\n    #with MyTimer():                            \n    #    rf.fit(X_train, y_train)\n    \n    def __init__(self):\n        self.start = time.time()\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        end = time.time()\n        runtime = end - self.start\n        msg = 'The function took {time} seconds to complete'\n        print(msg.format(time=runtime))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-19T01:02:19.31998Z","iopub.execute_input":"2022-09-19T01:02:19.320816Z","iopub.status.idle":"2022-09-19T01:02:19.331315Z","shell.execute_reply.started":"2022-09-19T01:02:19.320779Z","shell.execute_reply":"2022-09-19T01:02:19.328885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìú Data Loading</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"# üìú 2. Data Loading","metadata":{}},{"cell_type":"code","source":"competition = 'SR' # StackedRegression\n\ntry:\n    a = check_output([\"ls\", \"../input\"]).decode(\"utf8\") # new Kaggle competition\nexcept:\n    a=''\nfinally:\n    print('')\ntry:\n    b = check_output([\"ls\", \"-rlt\", \"../StackedRegression\"]).decode(\"utf8\")\nexcept:\n    b=''\nfinally:\n    print('')    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-19T01:02:19.33412Z","iopub.execute_input":"2022-09-19T01:02:19.334503Z","iopub.status.idle":"2022-09-19T01:02:19.394262Z","shell.execute_reply.started":"2022-09-19T01:02:19.33446Z","shell.execute_reply":"2022-09-19T01:02:19.39306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (len(a) > 0): # new competition\n    competition = 'SR'\n    train = pd.read_csv('../input/home-data-for-ml-course/train.csv')#,index_col='Id')\n    test = pd.read_csv('../input/home-data-for-ml-course/test.csv')#,index_col='Id')\nelif (len(b)): # run locally\n    competition = 'SR'\n    train = pd.read_csv('input/train.csv')\n    test = pd.read_csv('input/test.csv')\nelse: # old competition\n    competition = 'SRP_2'\n    train = pd.read_csv('../input/train.csv')\n    test = pd.read_csv('../input/test.csv')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-19T01:02:19.396355Z","iopub.execute_input":"2022-09-19T01:02:19.39708Z","iopub.status.idle":"2022-09-19T01:02:19.459544Z","shell.execute_reply.started":"2022-09-19T01:02:19.397041Z","shell.execute_reply":"2022-09-19T01:02:19.458494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:19.46321Z","iopub.execute_input":"2022-09-19T01:02:19.463494Z","iopub.status.idle":"2022-09-19T01:02:19.495253Z","shell.execute_reply.started":"2022-09-19T01:02:19.463468Z","shell.execute_reply":"2022-09-19T01:02:19.494191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:19.500321Z","iopub.execute_input":"2022-09-19T01:02:19.501163Z","iopub.status.idle":"2022-09-19T01:02:19.524598Z","shell.execute_reply.started":"2022-09-19T01:02:19.501126Z","shell.execute_reply":"2022-09-19T01:02:19.523625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìâ Data Pre-processing</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"# üìâ 3. Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"## 3.1. Checking Outliers","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\n#ax.scatter(x = train['GrLivArea'], y = train['SalePrice']\nax.scatter(x = train['GrLivArea'], y = np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n#m = slope, b=intercept\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:19.526056Z","iopub.execute_input":"2022-09-19T01:02:19.526587Z","iopub.status.idle":"2022-09-19T01:02:19.897139Z","shell.execute_reply.started":"2022-09-19T01:02:19.526533Z","shell.execute_reply":"2022-09-19T01:02:19.896106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Checking column values","metadata":{}},{"cell_type":"code","source":"train.shape[1]\na = 6\nb = int(train.shape[1]/4)\nr = int(train.shape[1]/a)\nc = int(train.shape[1]/b)\ni = 0\nfig, ax = plt.subplots(nrows=r, ncols=c, figsize=(25, 60))\nfor row in ax:\n    for col in row:\n        try:\n            col.scatter(x = train[train.columns[i]], y = np.log(train['SalePrice']))\n            col.title.set_text(train.columns[i])\n        except:\n            temp=1\n        #except Exception as e:\n        #    print(e.message, e.args)\n        finally:\n            temp=1\n        i = i + 1\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:19.898523Z","iopub.execute_input":"2022-09-19T01:02:19.902586Z","iopub.status.idle":"2022-09-19T01:02:28.968512Z","shell.execute_reply.started":"2022-09-19T01:02:19.902532Z","shell.execute_reply":"2022-09-19T01:02:28.967617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3. Flitering Data","metadata":{}},{"cell_type":"code","source":"train = train.drop(train[(train['OverallQual']>9) & (train['SalePrice']<220000)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:28.96946Z","iopub.execute_input":"2022-09-19T01:02:28.969821Z","iopub.status.idle":"2022-09-19T01:02:28.988748Z","shell.execute_reply.started":"2022-09-19T01:02:28.969788Z","shell.execute_reply":"2022-09-19T01:02:28.987914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4. Droping Id columns","metadata":{}},{"cell_type":"code","source":"train_ID = train['Id']\ntest_ID = test['Id']\n\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:28.990182Z","iopub.execute_input":"2022-09-19T01:02:28.990829Z","iopub.status.idle":"2022-09-19T01:02:29.002024Z","shell.execute_reply.started":"2022-09-19T01:02:28.990791Z","shell.execute_reply":"2022-09-19T01:02:29.001103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>‚úÇÔ∏è Data Wrangling</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"# ‚úÇÔ∏è 4. Data Wrangling","metadata":{}},{"cell_type":"markdown","source":"## 4.1. GrLivArea - column","metadata":{}},{"cell_type":"markdown","source":"### 4.1.2. GrLivArea - column : Trying linear regression","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:29.003784Z","iopub.execute_input":"2022-09-19T01:02:29.004619Z","iopub.status.idle":"2022-09-19T01:02:29.236536Z","shell.execute_reply.started":"2022-09-19T01:02:29.004583Z","shell.execute_reply":"2022-09-19T01:02:29.235799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.2. GrLivArea - column : Trying polynomial regression","metadata":{}},{"cell_type":"code","source":"x_data = train['GrLivArea']\ny_data = train['SalePrice']\nlog_y_data = np.log(train['SalePrice'])\n\ncurve_fit = np.polyfit(x_data, log_y_data, 1)\nprint(curve_fit)\ny = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, y_data, \"o\")\nplt.scatter(x_data, y,c='red')","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:29.238141Z","iopub.execute_input":"2022-09-19T01:02:29.238876Z","iopub.status.idle":"2022-09-19T01:02:29.466255Z","shell.execute_reply.started":"2022-09-19T01:02:29.238836Z","shell.execute_reply":"2022-09-19T01:02:29.465603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.2. GrLivArea - column : Trying log transformation","metadata":{}},{"cell_type":"code","source":"y = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:29.46775Z","iopub.execute_input":"2022-09-19T01:02:29.468392Z","iopub.status.idle":"2022-09-19T01:02:29.701573Z","shell.execute_reply.started":"2022-09-19T01:02:29.468354Z","shell.execute_reply":"2022-09-19T01:02:29.700542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. SalesPrice - Target Analysis","metadata":{}},{"cell_type":"markdown","source":"### 4.2.1. Checking Distribution","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['SalePrice'] , fit=norm)\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:29.702975Z","iopub.execute_input":"2022-09-19T01:02:29.705169Z","iopub.status.idle":"2022-09-19T01:02:30.516449Z","shell.execute_reply.started":"2022-09-19T01:02:29.705131Z","shell.execute_reply":"2022-09-19T01:02:30.515484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2. Transformation","metadata":{}},{"cell_type":"code","source":"lam_l = 0.35 \ntrain[\"SalePrice\"] = boxcox1p(train[\"SalePrice\"], lam_l) ","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:30.517999Z","iopub.execute_input":"2022-09-19T01:02:30.518337Z","iopub.status.idle":"2022-09-19T01:02:30.524329Z","shell.execute_reply.started":"2022-09-19T01:02:30.518302Z","shell.execute_reply":"2022-09-19T01:02:30.523425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.3. Visualizing changes","metadata":{}},{"cell_type":"code","source":"x = np.linspace(0, 20)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()\n\nx = np.linspace(0, 100000)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-19T01:02:30.525984Z","iopub.execute_input":"2022-09-19T01:02:30.526782Z","iopub.status.idle":"2022-09-19T01:02:31.092705Z","shell.execute_reply.started":"2022-09-19T01:02:30.52674Z","shell.execute_reply":"2022-09-19T01:02:31.09158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.4. Check new distributions","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['SalePrice'] , fit=norm);\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:31.094286Z","iopub.execute_input":"2022-09-19T01:02:31.095327Z","iopub.status.idle":"2022-09-19T01:02:31.726548Z","shell.execute_reply.started":"2022-09-19T01:02:31.095281Z","shell.execute_reply":"2022-09-19T01:02:31.725488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üõ† Feature Engineeering</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"# üõ† 5. Feature Engineeering","metadata":{}},{"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-19T01:02:31.72808Z","iopub.execute_input":"2022-09-19T01:02:31.728437Z","iopub.status.idle":"2022-09-19T01:02:31.75818Z","shell.execute_reply.started":"2022-09-19T01:02:31.728402Z","shell.execute_reply":"2022-09-19T01:02:31.757126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_gla2(row, p):\n    return (row.GrLivArea**p)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-19T01:02:31.759445Z","iopub.execute_input":"2022-09-19T01:02:31.75994Z","iopub.status.idle":"2022-09-19T01:02:31.765734Z","shell.execute_reply.started":"2022-09-19T01:02:31.759898Z","shell.execute_reply":"2022-09-19T01:02:31.764628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1. Check Result distribution","metadata":{}},{"cell_type":"code","source":"correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain], y_train)\ncorrelation_xy = correlation_matrix[0,1]\nr_squared = correlation_xy**2\n\nprint(r_squared)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:31.767459Z","iopub.execute_input":"2022-09-19T01:02:31.768266Z","iopub.status.idle":"2022-09-19T01:02:31.776646Z","shell.execute_reply.started":"2022-09-19T01:02:31.76823Z","shell.execute_reply":"2022-09-19T01:02:31.775273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2. All Data","metadata":{}},{"cell_type":"code","source":"all_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:31.778286Z","iopub.execute_input":"2022-09-19T01:02:31.778828Z","iopub.status.idle":"2022-09-19T01:02:31.806604Z","shell.execute_reply.started":"2022-09-19T01:02:31.778789Z","shell.execute_reply":"2022-09-19T01:02:31.805737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3. Missing Data","metadata":{}},{"cell_type":"code","source":"all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(30)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:31.807678Z","iopub.execute_input":"2022-09-19T01:02:31.808111Z","iopub.status.idle":"2022-09-19T01:02:31.829626Z","shell.execute_reply.started":"2022-09-19T01:02:31.80806Z","shell.execute_reply":"2022-09-19T01:02:31.828546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4. Missing Data - analysis","metadata":{}},{"cell_type":"code","source":"all_numerical = all_data.select_dtypes(include=np.number).columns.tolist()\nmissing_data.index.values.tolist()\nmissing_df = all_data[missing_data.index.values.tolist()]\nmissing_numerical = missing_df.select_dtypes(include=np.number).columns.tolist()\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:31.831707Z","iopub.execute_input":"2022-09-19T01:02:31.832493Z","iopub.status.idle":"2022-09-19T01:02:32.337485Z","shell.execute_reply.started":"2022-09-19T01:02:31.832449Z","shell.execute_reply":"2022-09-19T01:02:32.336506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.5. Data Correlation","metadata":{}},{"cell_type":"code","source":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:32.345948Z","iopub.execute_input":"2022-09-19T01:02:32.346927Z","iopub.status.idle":"2022-09-19T01:02:33.664427Z","shell.execute_reply.started":"2022-09-19T01:02:32.346888Z","shell.execute_reply":"2022-09-19T01:02:33.663289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:33.665857Z","iopub.execute_input":"2022-09-19T01:02:33.666291Z","iopub.status.idle":"2022-09-19T01:02:33.676451Z","shell.execute_reply.started":"2022-09-19T01:02:33.666257Z","shell.execute_reply":"2022-09-19T01:02:33.675228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.6 Imputing missing values \n\nIn statistics, imputation is the process of replacing missing data with substituted values. When substituting for a data point, it is known as \"unit imputation\"; when substituting for a component of a data point, it is known as \"item imputation\". Imputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\nDetails : https://www.kaggle.com/code/azminetoushikwasi/all-imputation-techniques-with-pros-and-cons","metadata":{}},{"cell_type":"markdown","source":"We impute them  by proceeding sequentially  through features with missing values \n\n- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. \n- **MiscFeature** : data description says NA means \"no misc feature\"\n- **Alley** : data description says NA means \"no alley access\"\n- **Fence** : data description says NA means \"no fence\"\n- **FireplaceQu** : data description says NA means \"no fireplace\"\n- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\n- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.","metadata":{}},{"cell_type":"code","source":"ImputeToNone = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\nfor col in ImputeToNone:  \n    all_data[col].fillna(\"None\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:33.678125Z","iopub.execute_input":"2022-09-19T01:02:33.678815Z","iopub.status.idle":"2022-09-19T01:02:33.693536Z","shell.execute_reply.started":"2022-09-19T01:02:33.678774Z","shell.execute_reply":"2022-09-19T01:02:33.692665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data[missing_numerical]","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:33.694948Z","iopub.execute_input":"2022-09-19T01:02:33.6955Z","iopub.status.idle":"2022-09-19T01:02:33.716151Z","shell.execute_reply.started":"2022-09-19T01:02:33.695465Z","shell.execute_reply":"2022-09-19T01:02:33.714629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat2 = all_data[missing_numerical].corr(method='kendall')\ncorrmat2","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:33.718143Z","iopub.execute_input":"2022-09-19T01:02:33.718605Z","iopub.status.idle":"2022-09-19T01:02:33.77146Z","shell.execute_reply.started":"2022-09-19T01:02:33.718531Z","shell.execute_reply":"2022-09-19T01:02:33.77026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ImputeData(all_data, numerical_input, col_to_impute):\n    from sklearn.impute import KNNImputer\n    \n    Missing = all_data[numerical_input]\n    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n    imputer.fit(Missing)\n    Xtrans = imputer.transform(Missing)\n    df_miss = pd.DataFrame(Xtrans,columns = Missing.columns)\n    all_data[col_to_impute] = df_miss[col_to_impute]\n    return (all_data)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:33.773054Z","iopub.execute_input":"2022-09-19T01:02:33.774112Z","iopub.status.idle":"2022-09-19T01:02:33.781478Z","shell.execute_reply.started":"2022-09-19T01:02:33.774071Z","shell.execute_reply":"2022-09-19T01:02:33.780053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = ImputeData(all_data, all_numerical, 'LotFrontage')\nall_data","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:33.783197Z","iopub.execute_input":"2022-09-19T01:02:33.78364Z","iopub.status.idle":"2022-09-19T01:02:34.06692Z","shell.execute_reply.started":"2022-09-19T01:02:33.783604Z","shell.execute_reply":"2022-09-19T01:02:34.066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n- **SaleType** : Fill in again with most frequent which is \"WD\"","metadata":{}},{"cell_type":"code","source":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.068412Z","iopub.execute_input":"2022-09-19T01:02:34.068882Z","iopub.status.idle":"2022-09-19T01:02:34.09301Z","shell.execute_reply.started":"2022-09-19T01:02:34.068846Z","shell.execute_reply":"2022-09-19T01:02:34.092009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.094212Z","iopub.execute_input":"2022-09-19T01:02:34.094608Z","iopub.status.idle":"2022-09-19T01:02:34.118571Z","shell.execute_reply.started":"2022-09-19T01:02:34.094571Z","shell.execute_reply":"2022-09-19T01:02:34.117587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.7. Transforming some numerical variables that are really categorical","metadata":{}},{"cell_type":"code","source":"import datetime\nYr = all_data['YrSold'].min()\nMo = all_data['MoSold'].min()\nt = datetime.datetime(Yr, Mo, 1, 0, 0)\n\ndef calculateYrMo (row):   \n    return int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds())\n\nall_data['YrMoSold'] = all_data.apply(lambda row: calculateYrMo(row), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.120294Z","iopub.execute_input":"2022-09-19T01:02:34.120726Z","iopub.status.idle":"2022-09-19T01:02:34.198595Z","shell.execute_reply.started":"2022-09-19T01:02:34.120687Z","shell.execute_reply":"2022-09-19T01:02:34.197603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.200026Z","iopub.execute_input":"2022-09-19T01:02:34.200706Z","iopub.status.idle":"2022-09-19T01:02:34.217818Z","shell.execute_reply.started":"2022-09-19T01:02:34.200665Z","shell.execute_reply":"2022-09-19T01:02:34.216611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.8 Label Encoding","metadata":{}},{"cell_type":"code","source":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'YrMoSold')\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.220529Z","iopub.execute_input":"2022-09-19T01:02:34.221041Z","iopub.status.idle":"2022-09-19T01:02:34.310019Z","shell.execute_reply.started":"2022-09-19T01:02:34.221002Z","shell.execute_reply":"2022-09-19T01:02:34.30913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.9 Ading new features","metadata":{}},{"cell_type":"code","source":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd'] # A-\nall_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.31151Z","iopub.execute_input":"2022-09-19T01:02:34.312128Z","iopub.status.idle":"2022-09-19T01:02:34.322049Z","shell.execute_reply.started":"2022-09-19T01:02:34.31209Z","shell.execute_reply":"2022-09-19T01:02:34.320898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.10 Skewwness Determination and Reduction","metadata":{}},{"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\n\nlam_f = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam_f)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.324057Z","iopub.execute_input":"2022-09-19T01:02:34.32445Z","iopub.status.idle":"2022-09-19T01:02:34.387886Z","shell.execute_reply.started":"2022-09-19T01:02:34.324413Z","shell.execute_reply":"2022-09-19T01:02:34.386841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.10 Dummy Variables for Categorical Features","metadata":{}},{"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.389566Z","iopub.execute_input":"2022-09-19T01:02:34.389947Z","iopub.status.idle":"2022-09-19T01:02:34.421932Z","shell.execute_reply.started":"2022-09-19T01:02:34.389908Z","shell.execute_reply":"2022-09-19T01:02:34.420829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.11 PCA and Droping lowest co-relation columns","metadata":{}},{"cell_type":"code","source":"correlations = corrmat['SalePrice'].sort_values(ascending=False)\ndf_corr = correlations.to_frame()\nprint(df_corr.query(\"abs(SalePrice) < 0.011\"))\nlow_corr = df_corr.query(\"abs(SalePrice) < 0.011\").index.values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.42359Z","iopub.execute_input":"2022-09-19T01:02:34.423956Z","iopub.status.idle":"2022-09-19T01:02:34.438882Z","shell.execute_reply.started":"2022-09-19T01:02:34.423922Z","shell.execute_reply":"2022-09-19T01:02:34.437761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA().fit(all_data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.440861Z","iopub.execute_input":"2022-09-19T01:02:34.441312Z","iopub.status.idle":"2022-09-19T01:02:34.819685Z","shell.execute_reply.started":"2022-09-19T01:02:34.441265Z","shell.execute_reply":"2022-09-19T01:02:34.818536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = np.round(pca.components_, 3) \nev = np.round(pca.explained_variance_ratio_, 3)\nprint('explained variance ratio',ev)\npca_wt = pd.DataFrame(weights)#, columns=all_data.columns)\npca_wt.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.822976Z","iopub.execute_input":"2022-09-19T01:02:34.823305Z","iopub.status.idle":"2022-09-19T01:02:34.851699Z","shell.execute_reply.started":"2022-09-19T01:02:34.823276Z","shell.execute_reply":"2022-09-19T01:02:34.850415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat = train.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nplt.title(\"Kendall's Correlation Matrix Initial Train Set\", fontsize=16)\nsns.heatmap(corrmat, vmax=0.9, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:34.853045Z","iopub.execute_input":"2022-09-19T01:02:34.853973Z","iopub.status.idle":"2022-09-19T01:02:36.095851Z","shell.execute_reply.started":"2022-09-19T01:02:34.853938Z","shell.execute_reply":"2022-09-19T01:02:36.094596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_orig = train.copy()\ntrain_orig['SalePrice'] = y_train\ncorrmat = train_orig.corr(method='kendall')\n\ncorrelations = corrmat[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[0:10]\nsns.pairplot(train[features], height = 2.5)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:36.09756Z","iopub.execute_input":"2022-09-19T01:02:36.098241Z","iopub.status.idle":"2022-09-19T01:02:54.570723Z","shell.execute_reply.started":"2022-09-19T01:02:36.098197Z","shell.execute_reply":"2022-09-19T01:02:54.569636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nrc = RobustScaler()\n\nuse_pca = 0 # using PCA currently hurts the score\nuse_normalization = 0 # using StandardScaler doesn't work, try RobustScaler now\n\nif (use_pca == 1):\n    all_data_pca = pd.DataFrame(all_data_pca)\n    train = all_data_pca[:ntrain]\n    test = all_data_pca[ntrain:]\n    all_data_pca.head()\nelif (use_normalization == 1):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = sc.fit_transform(train)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = sc.transform(test)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)   \nelif (use_normalization == 2):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = rc.fit_transform(train)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = rc.transform(test)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)  \nelse:\n    # back to original splits (from train.csv and test.csv)\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:54.572333Z","iopub.execute_input":"2022-09-19T01:02:54.573273Z","iopub.status.idle":"2022-09-19T01:02:54.586837Z","shell.execute_reply.started":"2022-09-19T01:02:54.573238Z","shell.execute_reply":"2022-09-19T01:02:54.585662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_data = 0\nif (save_data == 1):\n    df1 = train.copy()\n    df1['SalePrice'] = inv_boxcox1p(y_train, lam_l)\n    df1.insert(0, 'Id', list(train_ID), allow_duplicates=False)\n    df1.to_csv('HousePricesTrain.csv', index=False)  \n    df2 = test.copy()\n    df2.insert(0, 'Id', list(test_ID), allow_duplicates=False)\n    df2.to_csv('HousePricesTest.csv', index=False) ","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:54.589547Z","iopub.execute_input":"2022-09-19T01:02:54.591211Z","iopub.status.idle":"2022-09-19T01:02:54.621348Z","shell.execute_reply.started":"2022-09-19T01:02:54.591176Z","shell.execute_reply":"2022-09-19T01:02:54.620393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.12 Co-relation with prediction variable","metadata":{}},{"cell_type":"code","source":"corrmat = train.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:02:54.622678Z","iopub.execute_input":"2022-09-19T01:02:54.623479Z","iopub.status.idle":"2022-09-19T01:03:08.595759Z","shell.execute_reply.started":"2022-09-19T01:02:54.623441Z","shell.execute_reply":"2022-09-19T01:03:08.594721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.13 Co-relation with prediction variable (each-other)","metadata":{}},{"cell_type":"code","source":"corrmat = test.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:03:08.597426Z","iopub.execute_input":"2022-09-19T01:03:08.597815Z","iopub.status.idle":"2022-09-19T01:03:23.259237Z","shell.execute_reply.started":"2022-09-19T01:03:08.597778Z","shell.execute_reply":"2022-09-19T01:03:23.25821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.14 Observing Values","metadata":{}},{"cell_type":"code","source":"train.hist(bins=20, figsize=(30,20))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:03:23.260785Z","iopub.execute_input":"2022-09-19T01:03:23.261159Z","iopub.status.idle":"2022-09-19T01:03:53.287979Z","shell.execute_reply.started":"2022-09-19T01:03:23.261123Z","shell.execute_reply":"2022-09-19T01:03:53.286625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.hist(bins=20, figsize=(30,20))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:03:53.289823Z","iopub.execute_input":"2022-09-19T01:03:53.290452Z","iopub.status.idle":"2022-09-19T01:04:24.003312Z","shell.execute_reply.started":"2022-09-19T01:03:53.290411Z","shell.execute_reply":"2022-09-19T01:04:24.002403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.15 Brief Check","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:24.004874Z","iopub.execute_input":"2022-09-19T01:04:24.005522Z","iopub.status.idle":"2022-09-19T01:04:24.458772Z","shell.execute_reply.started":"2022-09-19T01:04:24.005468Z","shell.execute_reply":"2022-09-19T01:04:24.457588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:24.460204Z","iopub.execute_input":"2022-09-19T01:04:24.460629Z","iopub.status.idle":"2022-09-19T01:04:24.893658Z","shell.execute_reply.started":"2022-09-19T01:04:24.460585Z","shell.execute_reply":"2022-09-19T01:04:24.892594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:24.894923Z","iopub.execute_input":"2022-09-19T01:04:24.8984Z","iopub.status.idle":"2022-09-19T01:04:24.919199Z","shell.execute_reply.started":"2022-09-19T01:04:24.898371Z","shell.execute_reply":"2022-09-19T01:04:24.918287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:24.920662Z","iopub.execute_input":"2022-09-19T01:04:24.921043Z","iopub.status.idle":"2022-09-19T01:04:24.940937Z","shell.execute_reply.started":"2022-09-19T01:04:24.920989Z","shell.execute_reply":"2022-09-19T01:04:24.939739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.16 Adjusting Distributions","metadata":{}},{"cell_type":"code","source":"use_feature_selection = 0\nif (use_feature_selection == 1):\n    import pickle\n\n    with open('X_train_sfs_50.pkl', 'rb') as fid:\n        train = pickle.load(fid)\n    with open('X_test_sfs_50.pkl', 'rb') as fid:\n        test = pickle.load(fid)\n    train = pd.DataFrame(train, index=train_ID)\n    test = pd.DataFrame(test, index=test_ID)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:24.942855Z","iopub.execute_input":"2022-09-19T01:04:24.943738Z","iopub.status.idle":"2022-09-19T01:04:24.951606Z","shell.execute_reply.started":"2022-09-19T01:04:24.943694Z","shell.execute_reply":"2022-09-19T01:04:24.950398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm = MinMaxScaler().fit(train)\ntrain_norm_arr = norm.transform(train)\ntest_norm_arr = norm.transform(test)\ntrain_norm = pd.DataFrame(norm.transform(train), index=train.index, columns=train.columns)\ntest_norm = pd.DataFrame(norm.transform(test), index=test.index, columns=test.columns)\ntrain_norm","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:24.953438Z","iopub.execute_input":"2022-09-19T01:04:24.954305Z","iopub.status.idle":"2022-09-19T01:04:25.001511Z","shell.execute_reply.started":"2022-09-19T01:04:24.954255Z","shell.execute_reply":"2022-09-19T01:04:25.000393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_norm","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.003148Z","iopub.execute_input":"2022-09-19T01:04:25.003764Z","iopub.status.idle":"2022-09-19T01:04:25.029955Z","shell.execute_reply.started":"2022-09-19T01:04:25.003728Z","shell.execute_reply":"2022-09-19T01:04:25.028637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stand = train.copy()\ntest_stand = test.copy()\n\nfor i in all_numerical:\n    # fit on training data column\n    scale = StandardScaler().fit(train_stand[[i]])\n    # transform the training data column\n    train_stand[i] = scale.transform(train_stand[[i]])\n    # transform the testing data column\n    test_stand[i] = scale.transform(test_stand[[i]])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.031906Z","iopub.execute_input":"2022-09-19T01:04:25.032369Z","iopub.status.idle":"2022-09-19T01:04:25.234995Z","shell.execute_reply.started":"2022-09-19T01:04:25.032326Z","shell.execute_reply":"2022-09-19T01:04:25.234047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stand","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.236378Z","iopub.execute_input":"2022-09-19T01:04:25.236838Z","iopub.status.idle":"2022-09-19T01:04:25.262918Z","shell.execute_reply.started":"2022-09-19T01:04:25.236804Z","shell.execute_reply":"2022-09-19T01:04:25.26164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìì Modeling</b></div>\n","metadata":{}},{"cell_type":"markdown","source":"# üìì 6. Modeling","metadata":{}},{"cell_type":"code","source":"method = 'stacked'\n\nimport random as rn\nrn.seed(1) # random\nfrom numpy.random import seed\nseed(7) # or 7\nimport tensorflow as tf\ntf.random.set_seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.2657Z","iopub.execute_input":"2022-09-19T01:04:25.266599Z","iopub.status.idle":"2022-09-19T01:04:25.273796Z","shell.execute_reply.started":"2022-09-19T01:04:25.266533Z","shell.execute_reply":"2022-09-19T01:04:25.272694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Neural Network Based Model</b></div>","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam, SGD, RMSprop   #for adam optimizer\ndef baseline_model(dim=223, opt_sel=\"adam\", learning_rate = 0.001, neurons = 1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, decay = 0.0002, momentum=0.9):\n    def bm():\n        # create model\n        model = Sequential()\n        #model.add(Dense(neurons, input_dim=223, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(neurons, input_dim=dim, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(1, kernel_initializer='normal'))\n        #model.add(Dense(1, kernel_initializer='normal')) # added to v86\n        # Compile model\n        if (opt_sel == \"adam\"):\n            #opt = Adam(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=amsgrad) # added to v86\n            opt = Adam(lr=learning_rate)\n        elif(opt_sel == \"sgd\"):\n            opt = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay, nesterov=True)\n        model.compile(loss='mean_squared_error', optimizer=opt)\n        return model\n    return bm","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.275475Z","iopub.execute_input":"2022-09-19T01:04:25.276167Z","iopub.status.idle":"2022-09-19T01:04:25.286781Z","shell.execute_reply.started":"2022-09-19T01:04:25.276123Z","shell.execute_reply":"2022-09-19T01:04:25.285725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs =  preprocessing.scale(train)\nn_cols = train_inputs.shape[1]\ninput_shape = (n_cols, )\n# Creates a model given an activation and learning rate\n# Create the model object with default arguments\ndef create_model(learning_rate = 0.001, activation='relu'):\n  \n    # Set Adam optimizer with the given learning rate\n    opt = Adam(lr = learning_rate)\n  \n    # Create your binary classification model  \n    model = Sequential()\n    model.add(Dense(128,\n                    activation = activation,\n                    input_shape = input_shape,\n                    activity_regularizer = regularizers.l2(1e-5)))\n    model.add(Dropout(0.50))\n    model.add(Dense(128,\n                    activation = activation, \n                    activity_regularizer = regularizers.l2(1e-5)))\n    model.add(Dropout(0.50))\n    model.add(Dense(1, activation = activation))\n    # Compile the model\n    model.compile(optimizer = opt,\n                  #loss = \"mean_absolute_error\",\n                  loss = \"mean_squared_error\",\n                  metrics = ['mse', \"mape\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.28824Z","iopub.execute_input":"2022-09-19T01:04:25.288611Z","iopub.status.idle":"2022-09-19T01:04:25.311383Z","shell.execute_reply.started":"2022-09-19T01:04:25.288575Z","shell.execute_reply":"2022-09-19T01:04:25.310143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimize_nn = 0\nif (optimize_nn == 1):\n    # Create a KerasClassifier object\n    model = KerasRegressor(build_fn = create_model,\n                           verbose = 0)\n    # Define the hyperparameter space\n    params = {'activation': [\"relu\"],#, \"tanh\"],\n              'batch_size': [1, 4],#, 2, 4], \n              'epochs': [100, 150, 200],\n              'neurons':[8, 16, 32],\n              'learning_rate': [0.01, 0.005, 0.001]}\n    # Create a randomize search cv object \n    random_search = RandomizedSearchCV(model,\n                                       param_distributions = params,\n                                       cv = KFold(10))\n    random_search_results = random_search.fit(train_inputs, y_train)\n    print(\"Best Score: \",\n          random_search_results.best_score_,\n          \"and Best Params: \",\n          random_search_results.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.313255Z","iopub.execute_input":"2022-09-19T01:04:25.31479Z","iopub.status.idle":"2022-09-19T01:04:25.322172Z","shell.execute_reply.started":"2022-09-19T01:04:25.314754Z","shell.execute_reply":"2022-09-19T01:04:25.321376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (optimize_nn == 1):\n    model = KerasRegressor(build_fn = create_model,\n                           epochs = 100, \n                           batch_size = 16,\n                           verbose = 0)\n    # Calculate the accuracy score for each fold\n    kfolds = cross_val_score(model,\n                             train_inputs,\n                             train_targets,\n                             cv = 10)\n    # Print the mean accuracy\n    print('The mean accuracy was:', kfolds.mean())\n    # Print the accuracy standard deviation\n    print('With a standard deviation of:', kfolds.std())","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.323496Z","iopub.execute_input":"2022-09-19T01:04:25.324214Z","iopub.status.idle":"2022-09-19T01:04:25.332858Z","shell.execute_reply.started":"2022-09-19T01:04:25.32418Z","shell.execute_reply":"2022-09-19T01:04:25.331876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define variable learning rate function\ndef step_decay(epoch, lr):\n    drop = 0.995 # was .999\n    epochs_drop = 175.0 # was 175, sgd likes 200+, adam likes 100\n    lrate = lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n    print(\"epoch=\" + str(epoch) + \" lr=\" + str(lr) + \" lrate=\" + str(lrate))\n    return lrate","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.33409Z","iopub.execute_input":"2022-09-19T01:04:25.335184Z","iopub.status.idle":"2022-09-19T01:04:25.346299Z","shell.execute_reply.started":"2022-09-19T01:04:25.335104Z","shell.execute_reply":"2022-09-19T01:04:25.345209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomCallback(keras.callbacks.Callback):\n    def __init__(self, train=None, validation=None):\n        super(CustomCallback, self).__init__()\n        self.validation = validation\n        self.train = train\n        \n    def on_train_begin(self, logs={}):\n        #val_loss_hist = []\n        #train_loss_hist = []\n        #lr_hist = []\n        self.val_loss_hist   = []\n        self.train_loss_hist = []\n        self.lr_hist         = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        keys = list(logs.keys())\n        #val_loss_hist.append([logs['val_loss']])\n        #train_loss_hist.append([logs['loss']])\n        #lr_hist.append([logs['lr']])\n        self.val_loss_hist.append([logs['val_loss']])\n        self.train_loss_hist.append([logs['loss']])\n        self.lr_hist.append([logs['lr']])\n        #print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n        \nlogging_callback = LambdaCallback(\n    on_epoch_end=lambda epoch, logs: print('val_loss:', logs['val_loss'])\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.347914Z","iopub.execute_input":"2022-09-19T01:04:25.348577Z","iopub.status.idle":"2022-09-19T01:04:25.358303Z","shell.execute_reply.started":"2022-09-19T01:04:25.34852Z","shell.execute_reply":"2022-09-19T01:04:25.357133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model # .0005 -> 54, .001 -> 53, .005 -> 48, .01 -> 55\nlrate = LearningRateScheduler(step_decay)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, mode='auto', restore_best_weights = True)\ndnn_history = CustomCallback()\ncallbacks_list = [lrate, early_stopping, dnn_history] \n# num_epochs = 1000 # added in v86\nnum_epochs = 100\nkeras_optimizer = \"adam\"\n\nif (keras_optimizer == \"adam\"): # train loss 47, val loss 70\n    # v86 had learning_rate = 0.001, batch size 2\n    dnn = KerasRegressor(build_fn=baseline_model(dim=223, opt_sel=keras_optimizer, learning_rate = 0.005, neurons = 8, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False), epochs=num_epochs, batch_size=1, verbose=1)\n    #dnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel=keras_optimizer, learning_rate = 0.001, neurons = 8, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False), epochs=num_epochs, batch_size=2, verbose=1)\n\nelif (keras_optimizer == \"sgd\"): # loss 27, val loss 69\n    dnn = KerasRegressor(build_fn=baseline_model(dim=223, opt_sel=keras_optimizer, learning_rate=0.000005, neurons=32, decay=0.000001, momentum=0.9), epochs=num_epochs, batch_size=8, verbose=1)\n    # can't get sgd to give decent results, only adam works as a metamodel\n    #dnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel=keras_optimizer, learning_rate=0.000005, neurons=32, decay=0.000001, momentum=0.9), epochs=num_epochs, batch_size=8, verbose=1)\n\ndnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel=\"adam\", learning_rate = 0.001, neurons = 8, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False), epochs=num_epochs, batch_size=2, verbose=1)\n    \nif (optimize_nn == 1):\n    kfold = KFold(n_splits=10)\n    results = cross_val_score(dnn, train, y_train, cv=kfold)\n    print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.35969Z","iopub.execute_input":"2022-09-19T01:04:25.360292Z","iopub.status.idle":"2022-09-19T01:04:25.37254Z","shell.execute_reply.started":"2022-09-19T01:04:25.36025Z","shell.execute_reply":"2022-09-19T01:04:25.371299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn.fit(train, y_train)\ndnn_train_pred = inv_boxcox1p(dnn.predict(train), lam_l)\ndnn_pred = inv_boxcox1p(dnn.predict(test), lam_l)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:04:25.373915Z","iopub.execute_input":"2022-09-19T01:04:25.374445Z","iopub.status.idle":"2022-09-19T01:08:54.697238Z","shell.execute_reply.started":"2022-09-19T01:04:25.374411Z","shell.execute_reply":"2022-09-19T01:08:54.696223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds=5 # was 5 => better score but twice as slow now\n\ndef rmsle_cv(model):\n    print(\"running rmsle_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    rmse= np.sqrt(-cross_val_score(model, train, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) # also r2\n    print(\"raw rmse scores for each fold:\", rmse)\n    return(rmse)\n\ndef r2_cv(model):\n    print(\"running r2_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    r2= cross_val_score(model, train, y_train, scoring=\"r2\", cv = kf) # also r2\n    print(\"raw r2 scores for each fold:\", r2)\n    return(r2)\n\n# used for another competition\ndef mae_cv(model):\n    print(\"running mae_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    mae = -cross_val_score(model, train, y_train, scoring=\"neg_mean_absolute_error\", cv = kf) # also r2\n    print(\"raw mae scores for each fold:\", mae)\n    return(mae)\n\ndef all_cv(model, n_folds, cv):\n    print(\"running cross_validate\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error', \n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    scores = cross_validate(model, train, y_train, scoring=scorers,\n                           cv=kf, return_train_score=True)\n    return(scores)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:08:54.700681Z","iopub.execute_input":"2022-09-19T01:08:54.700985Z","iopub.status.idle":"2022-09-19T01:08:54.712351Z","shell.execute_reply.started":"2022-09-19T01:08:54.700959Z","shell.execute_reply":"2022-09-19T01:08:54.711059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runGSCV(num_trials, features, y_values):\n    non_nested_scores = np.zeros(num_trials) # INCREASES BIAS\n    nested_scores = np.zeros(num_trials)\n    # Loop for each trial\n    for i in range(num_trials):\n        print(\"Running GridSearchCV:\")\n        with MyTimer():    \n            #grid_result = gsc.fit(train, y_train)  \n            grid_result = gsc.fit(features, y_values)  \n        non_nested_scores[i] = grid_result.best_score_\n        if (competition == 'SR'):\n            print(\"Best mae %f using %s\" % ( -grid_result.best_score_, grid_result.best_params_))\n        else:\n            print(\"Best rmse %f using %s\" % ( np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n        \n        # nested/non-nested cross validation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n        with MyTimer():    \n            #nested_score = cross_val_score(gsc, X=train, y=y_train, cv=outer_cv, verbose=0).mean() \n            nested_score = cross_val_score(gsc, X=features, y=y_values, cv=outer_cv, verbose=0).mean() \n            # source code for cross_val_score is here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_validation.py#L137\n        if (competition == 'SR'):\n            print(\"nested mae score from KFold %0.3f\" % -nested_score)\n        else:\n            print(\"nested rmse score from KFold %0.3f\" % np.sqrt(-nested_score))\n        \n        nested_scores[i] = nested_score\n        print('grid_result',grid_result)\n        print(\"mean scores: r2(%0.3f) mae(%0.3f) nmse(%0.3f) nmsle(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), -grid_result.cv_results_['mean_test_mae'].mean(),  np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_nmsle'].mean() ))\n        #print(\"mean scores: r2(%0.3f) nmse(%0.3f) mae(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_mae'].mean()))\n    return grid_result\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:08:54.714234Z","iopub.execute_input":"2022-09-19T01:08:54.714825Z","iopub.status.idle":"2022-09-19T01:08:54.727326Z","shell.execute_reply.started":"2022-09-19T01:08:54.714787Z","shell.execute_reply":"2022-09-19T01:08:54.726356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_all_scores(model, n_folds=5, cv=5):\n    scores = all_cv(model, n_folds, cv)\n    #scores['train_<scorer1_name>'']\n    #scores['test_<scorer1_name>'']\n    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format( (-scores['test_mae']).mean(), scores['test_mae'].std() ))\n    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format( (np.sqrt(-scores['test_nmse'])).mean(), scores['test_nmse'].std() ))\n    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format( scores['test_r2'].mean(), scores['test_r2'].std() ))\n    return (scores)\n\n# useful when you can't decide on parameter setting from best_params_\n# result_details(grid_result,'mean_test_nmse',100)\ndef result_details(grid_result,sorting='mean_test_nmse',cols=100):\n    param_df = pd.DataFrame.from_records(grid_result.cv_results_['params'])\n    param_df['mean_test_nmse'] = np.sqrt(-grid_result.cv_results_['mean_test_nmse'])\n    param_df['std_test_nmse'] = np.sqrt(grid_result.cv_results_['std_test_nmse'])\n    param_df['mean_test_mae'] = -grid_result.cv_results_['mean_test_mae']\n    param_df['std_test_mae'] = -grid_result.cv_results_['std_test_mae']\n    param_df['mean_test_r2'] = -grid_result.cv_results_['mean_test_r2']\n    param_df['std_test_r2'] = -grid_result.cv_results_['std_test_r2']\n    return param_df.sort_values(by=[sorting]).tail(cols)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef mae(y, y_pred):\n    return mean_absolute_error(y,y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:08:54.728789Z","iopub.execute_input":"2022-09-19T01:08:54.729249Z","iopub.status.idle":"2022-09-19T01:08:54.741878Z","shell.execute_reply.started":"2022-09-19T01:08:54.729214Z","shell.execute_reply":"2022-09-19T01:08:54.740919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Lasso Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"# initialize the algorithm for the GridSearchCV function\nlasso = Lasso()\ntuningLasso = 1 # takes 2 minutes to complete\n\nif (tuningLasso == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[0.01,], # done, lower keeps getting better, but don't want to go too low and begin overfitting (alpha is related to L1 reg)\n        'fit_intercept':[True], # done, big difference\n        'normalize':[False], # done, big difference\n        'precompute':[False], # done, no difference\n        'copy_X':[True], # done, no difference\n        'max_iter':[200], # done\n        'tol':[0.05], # done, not much difference # was 0.005 but that would cause error: ConvergenceWarning: Objective did not converge\n        'warm_start':[False], # done, no difference\n        'positive':[False], # done, big difference\n        'random_state':[1],\n        'selection':['cyclic'] # done both are same, cyclic is default\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[0.2], \n        'fit_intercept':[True],\n        'normalize':[False],\n        'precompute':[False],\n        'copy_X':[True],\n        'max_iter':[200],\n        'tol':[0.0001],\n        'warm_start':[False],\n        'positive':[False],\n        'random_state':[None],\n        'selection':['cyclic']\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error', \n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=lasso,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'random_state',100)\nrd[['random_state','alpha','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','alpha'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:08:54.743396Z","iopub.execute_input":"2022-09-19T01:08:54.74387Z","iopub.status.idle":"2022-09-19T01:08:57.252456Z","shell.execute_reply.started":"2022-09-19T01:08:54.743837Z","shell.execute_reply":"2022-09-19T01:08:57.251017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuning_lasso = 1\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, tol=0.05, random_state=1)) # was 1 \nlasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:08:57.254389Z","iopub.execute_input":"2022-09-19T01:08:57.254964Z","iopub.status.idle":"2022-09-19T01:08:57.268228Z","shell.execute_reply.started":"2022-09-19T01:08:57.254926Z","shell.execute_reply":"2022-09-19T01:08:57.2666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_lasso == 1):\n    #TEMP\n    model_results = [] # model flow, mae, rmsle\n    models = [lasso, lasso_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,5,5)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:08:57.270137Z","iopub.execute_input":"2022-09-19T01:08:57.270661Z","iopub.status.idle":"2022-09-19T01:08:58.85084Z","shell.execute_reply.started":"2022-09-19T01:08:57.270623Z","shell.execute_reply":"2022-09-19T01:08:58.849574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, tol=0.05, random_state=1)) # was 1 \n\nif (tuning_lasso == 1):\n    for i in [2,5,20,42,99]:\n        from sklearn.linear_model import Lasso\n        print('random_state =',i)\n\n        l = {'alpha': 0.01, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False, 'selection': 'cyclic', 'tol': 0.05, 'warm_start': False}\n        lasso_new = make_pipeline(RobustScaler(), Lasso(**l, random_state=i))\n        #lasso_new = Lasso(**l, random_state=i)\n\n        model_results = [] # model flow, mae, rmsle\n        models = [lasso, lasso_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    lasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_, random_state=i))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:08:58.857141Z","iopub.execute_input":"2022-09-19T01:08:58.860784Z","iopub.status.idle":"2022-09-19T01:09:07.091189Z","shell.execute_reply.started":"2022-09-19T01:08:58.860725Z","shell.execute_reply":"2022-09-19T01:09:07.08739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Elastic Net Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"# initialize the algorithm for the GridSearchCV function\nif (method == \"ensemble\"):\n    enet_tol = 0.01 # or try 0.01 - default is 0.0001\n    ENet = ElasticNet(tol=enet_tol) # added tol=0.05 to avoid errors\n    tuningENet = 0 # takes 2 minutes to complete\n\n    if (tuningENet == 1):\n        # use this when tuning\n        param_grid={\n            'alpha':[0.01],\n            'l1_ratio':[0.75,0.8,0.85,0.9],\n            'fit_intercept':[True], # ,False\n            'normalize':[False], # True,\n            'max_iter':range(350,450,50),\n            'selection':['random'], # 'cyclic',\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'alpha':[0.01],\n            'l1_ratio':[.9],\n            'fit_intercept':[True],\n            'normalize':[False],\n            'max_iter':[350], # default 1000\n            'selection':['random'],\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\nelse: # (method == \"stacked\")\n    enet_tol = 0.0001 # or try 0.01 \n    ENet = ElasticNet() # added tol=0.05 to avoid errors\n    tuningENet = 0 # takes 2 minutes to complete\n\n    if (tuningENet == 1):\n        # use this when tuning\n        param_grid={\n            'alpha':[0.01,0.05],\n            'l1_ratio':[0.8,0.85,0.9],\n            'fit_intercept':[True], # ,False\n            'normalize':[False], # True,\n            'max_iter':range(350,450,50),\n            'selection':['random'], # 'cyclic',\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'alpha':[0.05],\n            'l1_ratio':[.85],\n            'fit_intercept':[True],\n            'normalize':[False],\n            'max_iter':[500], # default 1000\n            'selection':['random'],\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=ENet,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(5, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['max_iter','l1_ratio','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:07.093106Z","iopub.execute_input":"2022-09-19T01:09:07.093446Z","iopub.status.idle":"2022-09-19T01:09:23.661704Z","shell.execute_reply.started":"2022-09-19T01:09:07.093412Z","shell.execute_reply":"2022-09-19T01:09:23.660531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rd = result_details(grid_result,'mean_test_nmse',100)\nrd[['max_iter','l1_ratio','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['max_iter','l1_ratio'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:23.666303Z","iopub.execute_input":"2022-09-19T01:09:23.67103Z","iopub.status.idle":"2022-09-19T01:09:23.731625Z","shell.execute_reply.started":"2022-09-19T01:09:23.670966Z","shell.execute_reply":"2022-09-19T01:09:23.730617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Linear Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"# initialize the algorithm for the GridSearchCV function\nlr1 = LinearRegression()\ntuningLR = 0 # takes 2 minutes to complete\n\nif (tuningLR == 1):\n    # use this when tuning\n    param_grid={\n        'fit_intercept':[True,False], \n        'normalize':[True,False]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'fit_intercept':[False], \n        'normalize':[False]\n    }\n\nscorers = {\n    'r2': 'r2',\n    #'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=lr1,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = gsc.fit(train, y_train) \n#grid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['fit_intercept','normalize','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:23.735461Z","iopub.execute_input":"2022-09-19T01:09:23.735973Z","iopub.status.idle":"2022-09-19T01:09:24.510015Z","shell.execute_reply.started":"2022-09-19T01:09:23.73594Z","shell.execute_reply":"2022-09-19T01:09:24.508771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\nlr1 = LinearRegression(fit_intercept=True,normalize=False) # defaults fit_intercept=True, normalize=False\nfrom sklearn.feature_selection import f_regression, f_classif\n#lr_poly = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", LinearRegression(fit_intercept=True,normalize=False))])\n\n# using PCA\nlr_poly = Pipeline([(\"polynomial_features\", polynomial_features), ('reduce_dim', PCA(n_components=360)), (\"linear_regression\", LinearRegression())])\n\ntrans = PolynomialFeatures(degree=2)\ndata = trans.fit_transform(train)\nprint(data)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:24.556462Z","iopub.execute_input":"2022-09-19T01:09:24.564885Z","iopub.status.idle":"2022-09-19T01:09:24.801672Z","shell.execute_reply.started":"2022-09-19T01:09:24.564833Z","shell.execute_reply":"2022-09-19T01:09:24.800523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Ridge Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"tune_kr = 1\nif (tune_kr == 1):\n    # initialize the algorithm for the GridSearchCV function\n    KRR = KernelRidge()\n    tuningKRR = 0 # this took 40 mins, 20 per iteration\n\n    if (tuningKRR == 1):\n        # use this when tuning\n        param_grid={\n            'alpha':[2.2,2.4], \n            'kernel':['polynomial'], #for entire list see: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics\n            'gamma':[0.0001,0.001,0.01,0.1],\n            'degree':[1,2,3,4,5,6], \n            'coef0':[0.1,0.3,0.5,1.0,2.0]\n        }\n\n    else:\n        # use this when not tuning\n        # nmse: Best mae 583416973.611280 using {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n        # mae: Best mae 15805.764347 using {'alpha': 2.0, 'coef0': 0.1, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n        param_grid={\n            'alpha':[2.2], \n            'kernel':['polynomial'], # 'linear', 'rbf'\n            'gamma':[0.001],\n            'degree':[4], \n            'coef0':[1.0]\n        }\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=KRR,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n\n    rd = result_details(grid_result,'mean_test_nmse',100)\n    rd[['alpha','degree','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:24.805959Z","iopub.execute_input":"2022-09-19T01:09:24.808267Z","iopub.status.idle":"2022-09-19T01:09:36.3758Z","shell.execute_reply.started":"2022-09-19T01:09:24.808228Z","shell.execute_reply":"2022-09-19T01:09:36.374229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"krr = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\nKRR = KernelRidge(**krr)\n#KRR = KernelRidge(alpha=2.2, coef0=0.5, degree=5, gamma=0.001, kernel='polynomial')\n\nif (tune_kr == 1):\n    KRR_new = KernelRidge(**grid_result.best_params_)\nelse:\n    krr_new = {'alpha': 2.3, 'coef0': 1.0, 'degree': 4, 'gamma': 0.001, 'kernel': 'polynomial'}\n    KRR_new = KernelRidge(**krr)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:36.382749Z","iopub.execute_input":"2022-09-19T01:09:36.38722Z","iopub.status.idle":"2022-09-19T01:09:36.401805Z","shell.execute_reply.started":"2022-09-19T01:09:36.387166Z","shell.execute_reply":"2022-09-19T01:09:36.4Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Random Forest Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"# initialize the algorithm for the GridSearchCV function\nrf = RandomForestRegressor()\ntuningRF = 0 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningRF == 1):\n    # use this when tuning\n    param_grid={\n        'max_depth':[3,4,5],\n        'max_features':[None,'sqrt','log2'], \n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators':range(25,100,25), \n        #'class_weight':[None,'balanced'],  \n        'min_samples_leaf':range(5,15,5), \n        'min_samples_split':range(10,30,10), \n        'criterion':['mse', 'mae'] \n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'max_depth':[5],\n        'max_features':[None], # max_features is None is default and works here, removing 'sqrt','log2'\n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators': [50], # number of trees selecting 100, removing range(50,126,25)\n        #'class_weight':[None], # None was selected, removing 'balanced'\n        'min_samples_leaf': [5], #selecting 10, removing range 10,40,10)\n        'min_samples_split': [10], # selecting 20, removing range(20,80,10),\n        'criterion':['mse'] # remove gini as it is never selected\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:36.410586Z","iopub.execute_input":"2022-09-19T01:09:36.414342Z","iopub.status.idle":"2022-09-19T01:09:55.59224Z","shell.execute_reply.started":"2022-09-19T01:09:36.41429Z","shell.execute_reply":"2022-09-19T01:09:55.590956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bps=grid_result.best_params_\n#bps['reg_alpha']=bps.pop(['alpha'])\nRF = make_pipeline(StandardScaler(), RandomForestRegressor(**bps)) # better than default, but still not good\nRF_new = make_pipeline(RobustScaler(), RandomForestRegressor(**bps)) # better than default, but still not good","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.594047Z","iopub.execute_input":"2022-09-19T01:09:55.594844Z","iopub.status.idle":"2022-09-19T01:09:55.600586Z","shell.execute_reply.started":"2022-09-19T01:09:55.594806Z","shell.execute_reply":"2022-09-19T01:09:55.599726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Optimize GBoost: \", datetime.datetime.now())","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.601996Z","iopub.execute_input":"2022-09-19T01:09:55.602348Z","iopub.status.idle":"2022-09-19T01:09:55.61699Z","shell.execute_reply.started":"2022-09-19T01:09:55.602315Z","shell.execute_reply":"2022-09-19T01:09:55.61598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Gradient Boosting Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5) # was 5\n# learning_ratefloat, default=0.1\n# learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\ntuning_gb = 0\nif (tuning_gb == 1):\n    # initialize the algorithm for the GridSearchCV function\n    GBoost_new = GradientBoostingRegressor()\n    tuningGB = 1\n    if (tuningGB == 1):\n        # use this when tuning\n        param_grid={\n            #'loss':['ls','lad','huber','quantile'],\n            'loss':['huber'], # done\n            'learning_rate':[0.05],\n            'n_estimators':[3000], # done\n            'subsample':[1.0],\n            'criterion':['friedman_mse'], # done\n            'min_samples_split':[10],\n            'min_samples_leaf':[15],\n            'min_weight_fraction_leaf':[0.0],\n            'max_depth':[2,3,4], # done\n            'min_impurity_decrease':[0.0],\n            'min_impurity_split':[None],\n            'init':[None],\n            'random_state':[None],\n            'max_features':[None,'auto','sqrt','log2'],\n            'max_features':['sqrt'], # done\n            'alpha':[0.60], # done\n            'verbose':[0],\n            'max_leaf_nodes':[None],\n            'warm_start':[False],\n            'presort':['deprecated'],\n            'validation_fraction':[0.1],\n            'n_iter_no_change':[None],\n            'tol':[0.0001],\n            'ccp_alpha':[0.0],\n            'random_state':[5,20,42]\n        }\n    else:\n        # use this when not tuning\n        param_grid={\n            'loss':['huber'], \n            'learning_rate':[0.05],\n            'n_estimators':[3000], \n            'subsample':[1.0],\n            'criterion':['friedman_mse'], \n            'min_samples_split':[10],\n            'min_samples_leaf':[15],\n            'min_weight_fraction_leaf':[0.0],\n            'max_depth':[2], \n            'min_impurity_decrease':[0.0],\n            'min_impurity_split':[None],\n            'init':[None],\n            'random_state':[None],\n            'max_features':['sqrt'], \n            'alpha':[0.60], \n            'verbose':[0],\n            'max_leaf_nodes':[None],\n            'warm_start':[False],\n            'presort':['deprecated'],\n            'validation_fraction':[0.1],\n            'n_iter_no_change':[None],\n            'tol':[0.0001],\n            'ccp_alpha':[0.0],\n            'random_state':[5]\n        }\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=GBoost_new,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, subtrain, y_subtrain)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['criterion','max_depth','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.618952Z","iopub.execute_input":"2022-09-19T01:09:55.619536Z","iopub.status.idle":"2022-09-19T01:09:55.651767Z","shell.execute_reply.started":"2022-09-19T01:09:55.619385Z","shell.execute_reply":"2022-09-19T01:09:55.6508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_gb == 1):\n    GBoost_new = GradientBoostingRegressor(**grid_result.best_params_)\nelse:\n    gbr  = {'alpha': 0.6, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 15, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 3000, 'n_iter_no_change': None, 'random_state': 5, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n    GBoost_new = GradientBoostingRegressor(**gbr)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.653961Z","iopub.execute_input":"2022-09-19T01:09:55.654537Z","iopub.status.idle":"2022-09-19T01:09:55.664718Z","shell.execute_reply.started":"2022-09-19T01:09:55.65448Z","shell.execute_reply":"2022-09-19T01:09:55.663377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_gb == 1):\n    #TEMP\n    model_results = [] \n    models = [GBoost, GBoost_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,5,5)\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.666538Z","iopub.execute_input":"2022-09-19T01:09:55.667311Z","iopub.status.idle":"2022-09-19T01:09:55.677161Z","shell.execute_reply.started":"2022-09-19T01:09:55.667266Z","shell.execute_reply":"2022-09-19T01:09:55.676537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testng - random test cases","metadata":{}},{"cell_type":"code","source":"if (tuning_gb == 1):\n    GBoost.fit(subtrain.values, y_subtrain)\n    gboost_train_pred = inv_boxcox1p(GBoost.predict(subtrain.values), lam_l)\n    gboost_val_pred = inv_boxcox1p(GBoost.predict(val.values), lam_l)\n    print('GBoost')\n    print('train results')\n    print(mae(y_subtrain, gboost_train_pred))\n    print(rmsle(y_subtrain, gboost_train_pred))\n    print('test results')\n    print(mae(y_val, gboost_val_pred))\n    print(rmsle(y_val, gboost_val_pred))\n\n    GBoost_new.fit(subtrain.values, y_subtrain)\n    gboost_train_pred = inv_boxcox1p(GBoost_new.predict(subtrain.values), lam_l)\n    gboost_val_pred = inv_boxcox1p(GBoost_new.predict(val.values), lam_l)\n    print('GBoost_new')\n    print('train results')\n    print(mae(y_subtrain, gboost_train_pred))\n    print(rmsle(y_subtrain, gboost_train_pred))\n    print('test results')\n    print(mae(y_val, gboost_val_pred))\n    print(rmsle(y_val, gboost_val_pred))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.678406Z","iopub.execute_input":"2022-09-19T01:09:55.679009Z","iopub.status.idle":"2022-09-19T01:09:55.687768Z","shell.execute_reply.started":"2022-09-19T01:09:55.678975Z","shell.execute_reply":"2022-09-19T01:09:55.687076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_gb == 1):\n    GBoost_new.fit(train, y_train)\n    gboost_pred = inv_boxcox1p(GBoost_new.predict(test), lam_l)\n    gboost_pred\n    sub = pd.DataFrame()\n    #sub['Id'] = test['Id']\n    sub['Id'] = test_ID\n    sub['SalePrice'] = gboost_pred\n    sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.688944Z","iopub.execute_input":"2022-09-19T01:09:55.689512Z","iopub.status.idle":"2022-09-19T01:09:55.697185Z","shell.execute_reply.started":"2022-09-19T01:09:55.689478Z","shell.execute_reply":"2022-09-19T01:09:55.696603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_gb == 1):\n    GBoost.fit(train, y_train)\n    gboost_pred = inv_boxcox1p(GBoost.predict(test), lam_l)\n    gboost_pred\n    sub = pd.DataFrame()\n    #sub['Id'] = test['Id']\n    sub['Id'] = test_ID\n    sub['SalePrice'] = gboost_pred\n    sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.698335Z","iopub.execute_input":"2022-09-19T01:09:55.698919Z","iopub.status.idle":"2022-09-19T01:09:55.712285Z","shell.execute_reply.started":"2022-09-19T01:09:55.698884Z","shell.execute_reply":"2022-09-19T01:09:55.71089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_gb == 1):\n\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost, GBoost_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,10,10)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.714422Z","iopub.execute_input":"2022-09-19T01:09:55.71489Z","iopub.status.idle":"2022-09-19T01:09:55.72575Z","shell.execute_reply.started":"2022-09-19T01:09:55.714848Z","shell.execute_reply":"2022-09-19T01:09:55.724792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Extreame Gradient Boosting (XGBoost) Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"tuning_xgb = 0\nif (tuning_xgb == 1):\n    # initialize the algorithm for the GridSearchCV function# initialize the algorithm for the GridSearchCV function\n    xgb1 = xgb.XGBRegressor()\n    tuningXGB = 1 # this took 2 hours last time, 1 hour per iteration\n\n    if (tuningXGB == 1):\n        # use this when tuning\n        param_grid={\n            'colsample_bytree':[0.4603],\n            'gamma':[0.0468], # done - all values almost identical results\n            'colsample_bylevel':[0.3], # done - all give same result\n            'objective':['reg:squarederror'], # done - Default:'reg:squarederror', None, reg:pseudohubererror, reg:squaredlogerror, reg:gamma\n            'booster':['gbtree'], # done - Default: 'gbtree', 'gblinear' or 'dart'\n            'learning_rate':[0.04], # done\n            'max_depth':[3], # - done\n            'importance_type':['gain'], # done - all give same value, Default:'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n            'min_child_weight':[1.7817], # done - no difference with several values\n            'n_estimators':[1000], # done\n            'reg_alpha':[0.4], # done\n            'reg_lambda':[0.8571], # done\n            'subsample':[0.5], # done\n            'silent':[1],\n            'random_state':[35],\n            'scale_pos_weight':[1],\n            'eval_metric':['rmse'], # done - all options have same results  Default:rmse for regression rmse, mae, rmsle, logloss, cox-nloglik\n            #'nthread ':[-1],\n            'verbosity':[0]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'colsample_bytree':[0.4603],\n            'gamma':[0.0468],\n            'colsample_bylevel':[0.3],\n            'objective':['reg:squarederror'], # 'binary:logistic', 'reg:squarederror', 'rank:pairwise', None\n            'booster':['gbtree'], # 'gbtree', 'gblinear' or 'dart'\n            'learning_rate':[0.04],\n            'max_depth':[3],\n            'importance_type':['gain'], # 'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n            'min_child_weight':[1.7817],\n            'n_estimators':[1000],\n            'reg_alpha':[0.4],\n            'reg_lambda':[0.8571],\n            'subsample':[0.5],\n            'silent':[1],\n            'random_state':[35],\n            'scale_pos_weight':[1],\n            'eval_metric':['rmse'],\n            #'nthread ':[-1],\n            'nthread ':[-1],\n            'verbosity':[0]\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=xgb1,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.727162Z","iopub.execute_input":"2022-09-19T01:09:55.727765Z","iopub.status.idle":"2022-09-19T01:09:55.742212Z","shell.execute_reply.started":"2022-09-19T01:09:55.727728Z","shell.execute_reply":"2022-09-19T01:09:55.741243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_xgb == 1):\n    rd = result_details(grid_result,'mean_test_nmse',100)\n\n    rd[['random_state','eval_metric','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.745851Z","iopub.execute_input":"2022-09-19T01:09:55.746155Z","iopub.status.idle":"2022-09-19T01:09:55.75575Z","shell.execute_reply.started":"2022-09-19T01:09:55.746131Z","shell.execute_reply":"2022-09-19T01:09:55.754878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                             learning_rate=0.05, max_depth=3, # importance_type (‚Äúgain‚Äù, ‚Äúweight‚Äù, ‚Äúcover‚Äù, ‚Äútotal_gain‚Äù or ‚Äútotal_cover‚Äù.)\n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=35) # was random_state=7, cannot set to None \n\nif (tuning_xgb == 1):\n    import xgboost as xgb\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n\n        model_xgb_new = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (‚Äúgain‚Äù, ‚Äúweight‚Äù, ‚Äúcover‚Äù, ‚Äútotal_gain‚Äù or ‚Äútotal_cover‚Äù.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.45, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None \n\n        model_xgb_new2 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (‚Äúgain‚Äù, ‚Äúweight‚Äù, ‚Äúcover‚Äù, ‚Äútotal_gain‚Äù or ‚Äútotal_cover‚Äù.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.5, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None \n\n        model_xgb_new3 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (‚Äúgain‚Äù, ‚Äúweight‚Äù, ‚Äúcover‚Äù, ‚Äútotal_gain‚Äù or ‚Äútotal_cover‚Äù.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.5213, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None\n\n        model_results = [] # model flow, mae, rmsle\n        models = [model_xgb, model_xgb_new, model_xgb_new2, model_xgb_new3]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    model_xgb_new = xgb.XGBRegressor(**grid_result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.757163Z","iopub.execute_input":"2022-09-19T01:09:55.757551Z","iopub.status.idle":"2022-09-19T01:09:55.772439Z","shell.execute_reply.started":"2022-09-19T01:09:55.757515Z","shell.execute_reply":"2022-09-19T01:09:55.771139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_metrics = 0\nif (show_metrics == 1):\n    import graphviz\n    model_xgb.fit(train, y_train,  verbose=False) #  eval_set=[(X_test, y_test)]\n    xgb.plot_importance(model_xgb)\n    xgb.to_graphviz(model_xgb, num_trees=20)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.774534Z","iopub.execute_input":"2022-09-19T01:09:55.775014Z","iopub.status.idle":"2022-09-19T01:09:55.786545Z","shell.execute_reply.started":"2022-09-19T01:09:55.774975Z","shell.execute_reply":"2022-09-19T01:09:55.785497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Light Gradient Boosting (LGBoost) Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"tuning_lgb = 0\nif (tuning_lgb == 1):\n    lgb1 = lgb.LGBMRegressor()\n    tuningLGB = 0\n\n    if (tuningLGB == 1):\n        # use this when tuning\n        param_grid={\n            'objective':['regression'], # - only one option for regression\n            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n            'num_leaves':[5,6], # - done\n            'learning_rate':[0.05], # - done\n            'n_estimators':[650,750], # - done\n            'max_bin':[45,55], # - done\n            'bagging_fraction':[0.85], # - done\n            'bagging_freq':[5], # - done\n            'feature_fraction':[0.2319], # - done\n            'feature_fraction_seed':[9], \n            'bagging_seed':[9],\n            'min_data_in_leaf':[9], # - done\n            'min_sum_hessian_in_leaf':[11], # - done\n            'max_depth':[-1], # - -1 means no limit\n            'subsample_for_bin':[500,1000], # - done\n            'class_weight':[None],\n            'min_split_gain':[0.0],\n            'min_child_weight':[0.001],\n            'min_child_samples':[5], # - done\n            'subsample':[1.0],\n            'subsample_freq':[0],\n            'colsample_bytree':[1.0],\n            'reg_alpha':[0.0], # - l1 regularization done\n            'reg_lambda':[0.0], # - L2 regularization done\n            'random_state':[1],\n            'importance_type':['split'] # - done\n        }\n    else:\n        # use this when not tuning\n        param_grid={\n            'objective':['regression'], # - only one option for regression\n            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n            'num_leaves':[5], # - done, maybe 5 is okay too\n            'learning_rate':[0.05], # - done\n            'n_estimators':[650], # - done\n            'max_bin':[55], # - done\n            'bagging_fraction':[0.85], # - done\n            'bagging_freq':[5], # - done\n            'feature_fraction':[0.2319], # - done\n            'feature_fraction_seed':[9], \n            'bagging_seed':[9],\n            'min_data_in_leaf':[9], # - done\n            'min_sum_hessian_in_leaf':[11], # - done\n            'max_depth':[-1], # - -1 means no limit\n            'subsample_for_bin':[1000], # - done\n            'class_weight':[None],\n            'min_split_gain':[0.0],\n            'min_child_weight':[0.001],\n            'min_child_samples':[5], # - done\n            'subsample':[1.0],\n            'subsample_freq':[0],\n            'colsample_bytree':[1.0],\n            'reg_alpha':[0.0], # - l1 regularization done\n            'reg_lambda':[0.0], # - L2 regularization done\n            'random_state':[1],\n            'importance_type':['split'] # - done\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=lgb1,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.789862Z","iopub.execute_input":"2022-09-19T01:09:55.790148Z","iopub.status.idle":"2022-09-19T01:09:55.805549Z","shell.execute_reply.started":"2022-09-19T01:09:55.790114Z","shell.execute_reply":"2022-09-19T01:09:55.804601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9, random_state=10,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nif (tuning_lgb == 1):\n    model_lgb_new = lgb.LGBMRegressor(**grid_result.best_params_)\nelse:\n    lgbm = {'bagging_fraction': 0.85, 'bagging_freq': 5, 'bagging_seed': 9, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'feature_fraction': 0.2319, 'feature_fraction_seed': 9, 'importance_type': 'split', 'learning_rate': 0.05, 'max_bin': 55, 'max_depth': -1, 'min_child_samples': 5, 'min_child_weight': 0.001, 'min_data_in_leaf': 9, 'min_split_gain': 0.0, 'min_sum_hessian_in_leaf': 11, 'n_estimators': 650, 'num_leaves': 5, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 1000, 'subsample_freq': 0}\n    model_lgb_new = lgb.LGBMRegressor(**lgbm)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.808616Z","iopub.execute_input":"2022-09-19T01:09:55.808867Z","iopub.status.idle":"2022-09-19T01:09:55.819542Z","shell.execute_reply.started":"2022-09-19T01:09:55.808844Z","shell.execute_reply":"2022-09-19T01:09:55.818717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Bayesian Ridge Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"# initialize the algorithm for the GridSearchCV function\nbr = BayesianRidge()\ntuningBR = 1 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningBR == 1):\n    # use this when tuning\n    param_grid={\n        'n_iter':[50],\n        'tol':[0.001],\n        'alpha_1':[1e-06],\n        'alpha_2':[1e-05],\n        'lambda_1':[1e-05],\n        'lambda_2':[1e-06],\n        'alpha_init':[None],\n        'lambda_init':[None],\n        'compute_score':[True,False],\n        'fit_intercept':[True,False],\n        'normalize':[False,True],\n        'copy_X':[True],\n        'verbose':[False]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'n_iter':[50],\n        'tol':[0.001],\n        'alpha_1':[1e-06],\n        'alpha_2':[1e-05],\n        'lambda_1':[1e-05],\n        'lambda_2':[1e-06],\n        'alpha_init':[None],\n        'lambda_init':[None],\n        'compute_score':[True,False],\n        'fit_intercept':[True,False],\n        'normalize':[False,True],\n        'copy_X':[True],\n        'verbose':[False]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=br,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:09:55.821491Z","iopub.execute_input":"2022-09-19T01:09:55.821856Z","iopub.status.idle":"2022-09-19T01:10:36.89029Z","shell.execute_reply.started":"2022-09-19T01:09:55.821814Z","shell.execute_reply":"2022-09-19T01:10:36.889029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuning_br = 0\nBR = BayesianRidge()\nif (tuning_br == 1):\n    BR_new = BayesianRidge(**grid_result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:10:36.896243Z","iopub.execute_input":"2022-09-19T01:10:36.896982Z","iopub.status.idle":"2022-09-19T01:10:36.905839Z","shell.execute_reply.started":"2022-09-19T01:10:36.896927Z","shell.execute_reply":"2022-09-19T01:10:36.904368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rd = result_details(grid_result,'alpha_1',100)\nrd[['alpha_1','alpha_2','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:10:36.907385Z","iopub.execute_input":"2022-09-19T01:10:36.907949Z","iopub.status.idle":"2022-09-19T01:10:36.962779Z","shell.execute_reply.started":"2022-09-19T01:10:36.907908Z","shell.execute_reply":"2022-09-19T01:10:36.961739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_br == 1):\n    model_results = [] # model flow, mae, rmsle\n    models = [BR, BR_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,10,10)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:10:36.964034Z","iopub.execute_input":"2022-09-19T01:10:36.965654Z","iopub.status.idle":"2022-09-19T01:10:36.981587Z","shell.execute_reply.started":"2022-09-19T01:10:36.965616Z","shell.execute_reply":"2022-09-19T01:10:36.979954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize the algorithm for the GridSearchCV function\nR = Ridge(alpha=1.0)\ntuningR = 1 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningR == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[8], # done\n        'fit_intercept':[True], # done\n        'normalize':[False], # done\n        'copy_X':[True],\n        'max_iter':[None], # done - no difference\n        'tol':[0.001],\n        'solver':['auto'], # done - Default:‚Äòauto‚Äô, ‚Äòsvd‚Äô, ‚Äòcholesky‚Äô, ‚Äòlsqr‚Äô, ‚Äòsparse_cg‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô\n        'random_state':[1,10,42,99,127]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[1.0],\n        'fit_intercept':[True],\n        'normalize':[False],\n        'copy_X':[True],\n        'max_iter':[None],\n        'tol':[0.001],\n        'solver':['auto'],\n        'random_state':[None]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=R,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(5, train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:10:36.983993Z","iopub.execute_input":"2022-09-19T01:10:36.985868Z","iopub.status.idle":"2022-09-19T01:11:05.373838Z","shell.execute_reply.started":"2022-09-19T01:10:36.98582Z","shell.execute_reply":"2022-09-19T01:11:05.372601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rd = result_details(grid_result,'mean_test_nmse',100)\nsummary = rd[['alpha','random_state','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])\n#summary.groupby(['fit_intercept'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_nmse': 'std', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])\nsummary.groupby(['alpha'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:05.379437Z","iopub.execute_input":"2022-09-19T01:11:05.382941Z","iopub.status.idle":"2022-09-19T01:11:05.451483Z","shell.execute_reply.started":"2022-09-19T01:11:05.382887Z","shell.execute_reply":"2022-09-19T01:11:05.450043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuning_r = 1\nif (tuning_r == 1):\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n\n        r= {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001}\n        #R_new = make_pipeline(RobustScaler(), Ridge(**r, random_state=i))\n        R_new = Ridge(**r, random_state=i)\n\n        model_results = [] # model flow, mae, rmsle\n        models = [R, R_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    #R_new = make_pipeline(RobustScaler(), Ridge(**grid_result.best_params_))\n    R_new = Ridge(**grid_result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:05.452923Z","iopub.execute_input":"2022-09-19T01:11:05.453252Z","iopub.status.idle":"2022-09-19T01:11:08.522539Z","shell.execute_reply.started":"2022-09-19T01:11:05.45322Z","shell.execute_reply":"2022-09-19T01:11:08.520775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>CatBoost Regression Model</b></div>","metadata":{}},{"cell_type":"code","source":"#CBoost = CatBoostRegressor(logging_level='Silent',random_seed=0)\nCBoost = CatBoostRegressor(logging_level='Silent', random_state=0, depth=6, n_estimators=1000,eval_metric='RMSE',bagging_temperature=1,grow_policy='SymmetricTree',bootstrap_type='MVS') # l2_leaf_reg, learning_rate unknown\n\ntuning_cb = 0\nif (tuning_cb == 1):\n    # initialize the algorithm for the GridSearchCV function\n    cb = CatBoostRegressor()\n    if (tuningCB == 1):\n        # use this when tuning\n        param_grid={\n            'nan_mode':['Min'],\n            'eval_metric':['RMSE'],\n            'iterations':[1000],\n            'sampling_frequency':['PerTree'],\n            'leaf_estimation_method':['Newton'],\n            'grow_policy':['SymmetricTree'],\n            'penalties_coefficient':[1],\n            'boosting_type':['Plain'],\n            'model_shrink_mode':['Constant'],\n            'feature_border_type':['GreedyLogSum'],\n            #'bayesian_matrix_reg':[0.10000000149011612],\n            'l2_leaf_reg':[3],\n            'random_strength':[1],\n            'rsm':[1],\n            'boost_from_average':[True],\n            'model_size_reg':[0.5],\n            'subsample':[0.800000011920929],\n            'use_best_model':[False],\n            'random_seed':[0,2,15],\n            'depth':[6], # done\n            'border_count':[254],\n            #'classes_count':[0],\n            #'auto_class_weights':['None'],\n            'sparse_features_conflict_fraction':[0],\n            'leaf_estimation_backtracking':['AnyImprovement'],\n            'best_model_min_trees':[1],\n            'model_shrink_rate':[0],\n            'min_data_in_leaf':[1],\n            'loss_function':['RMSE'],\n            'learning_rate':[0.04174000024795532],\n            'score_function':['Cosine'],\n            'task_type':['CPU'],\n            'leaf_estimation_iterations':[1],\n            'bootstrap_type':['MVS'],\n            'max_leaves':[31],\n            'logging_level':['Silent']\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'nan_mode':['Min'],\n            'eval_metric':['RMSE'],\n            'iterations':[4000],\n            'sampling_frequency':['PerTree'],\n            'leaf_estimation_method':['Newton'],\n            'grow_policy':['SymmetricTree'],\n            'penalties_coefficient':[1],\n            'boosting_type':['Plain'],\n            'model_shrink_mode':['Constant'],\n            'feature_border_type':['GreedyLogSum'],\n            #'bayesian_matrix_reg':[0.10000000149011612],\n            'l2_leaf_reg':[3],\n            'random_strength':[1],\n            'rsm':[1],\n            'boost_from_average':[True],\n            'model_size_reg':[0.5],\n            'subsample':[0.800000011920929],\n            'use_best_model':[False],\n            'random_seed':[15],\n            'depth':[6],\n            'border_count':[254],\n            #'classes_count':[0],\n            #'auto_class_weights':['None'],\n            'sparse_features_conflict_fraction':[0],\n            'leaf_estimation_backtracking':['AnyImprovement'],\n            'best_model_min_trees':[1],\n            'model_shrink_rate':[0],\n            'min_data_in_leaf':[1],\n            'loss_function':['RMSE'],\n            'learning_rate':[0.04174000024795532],\n            'score_function':['Cosine'],\n            'task_type':['CPU'],\n            'leaf_estimation_iterations':[1],\n            'bootstrap_type':['MVS'],\n            'max_leaves':[31],\n            'logging_level':['Silent']\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=cb,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse'#, # use this one for optimizing\n        #plot=True\n    )\n    grid_result = runGSCV(2, train, y_train)\n    rd = result_details(grid_result,'depth',100)\n    rd[['random_state','depth','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:08.529215Z","iopub.execute_input":"2022-09-19T01:11:08.533845Z","iopub.status.idle":"2022-09-19T01:11:08.580558Z","shell.execute_reply.started":"2022-09-19T01:11:08.533792Z","shell.execute_reply":"2022-09-19T01:11:08.579182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif (tuning_cb == 1):\n    for i in [2,15,20,42,99]:\n        print('random_state =', i)\n        #CBoost     = CatBoostRegressor(logging_level='Silent', random_state=i) # don't touch this variable\n        #CBoost_def = CatBoostRegressor(logging_level='Silent', random_state=i, depth=6, n_estimators=1000,eval_metric='RMSE',bagging_temperature=1,grow_policy='SymmetricTree',bootstrap_type='MVS') # l2_leaf_reg, learning_rate unknown    \n        # 'max_leaves': 64, creates nan values, default of 31 is fine, or remove entirely...\n        CBoost_new = CatBoostRegressor(**grid_result.best_params_) \n\n        model_results = [] # model flow, mae, rmsle\n        models = [CBoost, CBoost_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            model_results.append([model.get_params(), (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    # was n_estimators=3500\n    CBoost_new = CatBoostRegressor(logging_level='Silent', random_state=15, depth=5, l2_leaf_reg=1.0, n_estimators=1700,eval_metric='RMSE',learning_rate=0.025,random_strength=3.7,bagging_temperature=1.0,grow_policy='SymmetricTree',bootstrap_type='Bayesian')#,bayesian_matrix_reg=0.10000000149011612)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:08.586803Z","iopub.execute_input":"2022-09-19T01:11:08.589916Z","iopub.status.idle":"2022-09-19T01:11:08.608483Z","shell.execute_reply.started":"2022-09-19T01:11:08.589864Z","shell.execute_reply":"2022-09-19T01:11:08.607123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\nimport pickle\nCBoost_new2 = deepcopy(CBoost_new)\nCBoost_new2.fit(train,y_train)\nwith open('CBoost_new.pkl', 'wb') as fid:\n    pickle.dump(CBoost_new2, fid)    \n# load it again\nwith open('CBoost_new.pkl', 'rb') as fid:\n    CBoost_new2 = pickle.load(fid)\nCBoost_new2.predict(train[0:1])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:08.61573Z","iopub.execute_input":"2022-09-19T01:11:08.619595Z","iopub.status.idle":"2022-09-19T01:11:16.591602Z","shell.execute_reply.started":"2022-09-19T01:11:08.619534Z","shell.execute_reply":"2022-09-19T01:11:16.590516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nCBoost_poly = Pipeline([(\"polynomial_features\", polynomial_features), (\"cboost_regression\", CBoost_new)])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:16.593113Z","iopub.execute_input":"2022-09-19T01:11:16.593598Z","iopub.status.idle":"2022-09-19T01:11:16.598623Z","shell.execute_reply.started":"2022-09-19T01:11:16.593544Z","shell.execute_reply":"2022-09-19T01:11:16.597679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AB = AdaBoostRegressor()\nfrom sklearn.svm import SVR\nSVR = SVR()\nDT = DecisionTreeRegressor()\nKN = KNeighborsRegressor()\nB = BaggingRegressor()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:16.600134Z","iopub.execute_input":"2022-09-19T01:11:16.600793Z","iopub.status.idle":"2022-09-19T01:11:16.609047Z","shell.execute_reply.started":"2022-09-19T01:11:16.60076Z","shell.execute_reply":"2022-09-19T01:11:16.608067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f2dac7;overflow:hidden;font-weight:500\"><b>Scores Analysis</b></div>","metadata":{}},{"cell_type":"code","source":"if (tuning_gb == 1):\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost, GBoost_orig]#, GBoost] # model_lgb_op, lasso_ns, \n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:16.611038Z","iopub.execute_input":"2022-09-19T01:11:16.612995Z","iopub.status.idle":"2022-09-19T01:11:16.621084Z","shell.execute_reply.started":"2022-09-19T01:11:16.612969Z","shell.execute_reply":"2022-09-19T01:11:16.620038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models = 0\nif (compare_models == 1):\n    model_results = [] # model flow, mae, rmsle\n    # GBoost_new is better than GBoost, but has lower final score, think this may be  overfitting\n    # BR_new has same results, here, but better final score\n    models = [lasso, lasso_new, model_lgb, ENet, ENet_new, KRR, GBoost, GBoost_new, model_xgb, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B, R, R_new, CBoost, CBoost_new] # worse or same: BR_new, model_lgb_new, lasso_ns, model_xgb_new,\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model)\n        #print(\"------------------------------------------\")\n        try:\n            print(model)\n            label = model\n        except KeyError as err:\n            print(\"KeyError error: {0}\".format(err))\n            label = model.__class__()\n        except Exception as e:\n            print(e.message, e.args)\n            label = model.__class__()\n        finally:\n            print(\"Continue\") \n        print('label', label)    \n        #model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n        model_results.append([label, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:16.622889Z","iopub.execute_input":"2022-09-19T01:11:16.623321Z","iopub.status.idle":"2022-09-19T01:11:16.638283Z","shell.execute_reply.started":"2022-09-19T01:11:16.623258Z","shell.execute_reply":"2022-09-19T01:11:16.6371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_mr.sort_values(by=['rmsle'])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:16.640061Z","iopub.execute_input":"2022-09-19T01:11:16.640802Z","iopub.status.idle":"2022-09-19T01:11:16.656866Z","shell.execute_reply.started":"2022-09-19T01:11:16.64076Z","shell.execute_reply":"2022-09-19T01:11:16.655579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìö Combing All</b></div>","metadata":{}},{"cell_type":"markdown","source":"# 7. üìö Combing All","metadata":{}},{"cell_type":"code","source":"# Variant A\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model_name = model.__class__.__name__\n            model_details = str(model)\n            print('model_name:', model_name)\n            print('model_details:', model_details)\n            #model.fit(X, y)\n            if (\"KerasRegressor\" in model_name):\n                model.fit(X, y, shuffle=True, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n            if (\"keras\" in model_details):\n                #model.fit(X, y, shuffle=True, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n                model.fit(X, y, dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list)\n            else:\n                model.fit(X, y) # fit the model for this fold\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        # IDEA: return weighted means\n        return np.mean(predictions, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:16.658247Z","iopub.execute_input":"2022-09-19T01:11:16.658671Z","iopub.status.idle":"2022-09-19T01:11:16.669153Z","shell.execute_reply.started":"2022-09-19T01:11:16.658639Z","shell.execute_reply":"2022-09-19T01:11:16.668207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score","metadata":{}},{"cell_type":"code","source":"#use_voting_regressor = 0 # AveragingModels Averages\nuse_voting_regressor = 1 # VotingRegressor Averages \n#CBoost_new_10 = CatBoostRegressor(logging_level='Silent', random_state=10, depth=5, l2_leaf_reg=1.0, n_estimators=1700,eval_metric='RMSE',learning_rate=0.025,random_strength=3.7,bagging_temperature=1.0,grow_policy='SymmetricTree',bootstrap_type='Bayesian')\n\nwith MyTimer():\n    if (use_voting_regressor == 1):\n        print(\"running VotingRegressor\")\n        from sklearn.ensemble import VotingRegressor\n        estimator_list = [('CBoost', CBoost_new),('xgb',model_xgb),('ENet',ENet),('gboost',GBoost),('krr',KRR),('br',BR)]\n        weight_list = [4,2,2,2,2,2]\n        #estimator_list = [('dnn', dnn)]\n        averaged_models = VotingRegressor(estimators=estimator_list, weights=weight_list) \n        averaged_models.fit(train, y_train)\n        averaged_train_pred = averaged_models.predict(train)\n        averaged_pred = inv_boxcox1p(averaged_models.predict(test), lam_l)\n    else:\n        print(\"running AveragingModels\")\n        #AveragingModels will fit and predict each model and predict using the mean of the individual predictions\n        averaged_models = AveragingModels(models = (CBoost_new,CBoost_new,model_xgb,ENet,GBoost,KRR,BR))# ENet, model_xgb,dnn_pipe)) # Adding ENet and RF is worse, model_xgb_new is worse        \n        averaged_models.fit(train, y_train)\n        averaged_train_pred = averaged_models.predict(train)\n        averaged_pred = inv_boxcox1p(averaged_models.predict(test), lam_l)\n\nshow_metrics = 0\nif (show_metrics == 1):\n    score = mae_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    score = rmsle_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))        \n\n    print(mae(y_train, averaged_train_pred))\n    print(rmsle(y_train, averaged_train_pred))\nprint(averaged_pred)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:16.670918Z","iopub.execute_input":"2022-09-19T01:11:16.671456Z","iopub.status.idle":"2022-09-19T01:11:52.830879Z","shell.execute_reply.started":"2022-09-19T01:11:16.671276Z","shell.execute_reply":"2022-09-19T01:11:52.829634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ap=pd.DataFrame(averaged_pred)\nap.to_csv('ap2.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:52.832981Z","iopub.execute_input":"2022-09-19T01:11:52.833832Z","iopub.status.idle":"2022-09-19T01:11:52.851899Z","shell.execute_reply.started":"2022-09-19T01:11:52.833788Z","shell.execute_reply":"2022-09-19T01:11:52.850448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CBoost_new.fit(train,y_train)\nprint(CBoost_new.predict(test))\nmodel_xgb.fit(train,y_train)\nprint(model_xgb.predict(test))\nENet.fit(train,y_train)\nprint(ENet.predict(test))\nGBoost.fit(train,y_train)\nprint(GBoost.predict(test))\nKRR.fit(train,y_train)\nprint(KRR.predict(test))\nBR.fit(train,y_train)\nprint(BR.predict(test))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:11:52.853637Z","iopub.execute_input":"2022-09-19T01:11:52.854298Z","iopub.status.idle":"2022-09-19T01:12:27.264429Z","shell.execute_reply.started":"2022-09-19T01:11:52.85426Z","shell.execute_reply":"2022-09-19T01:12:27.263064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"averaged_models.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:12:27.266107Z","iopub.execute_input":"2022-09-19T01:12:27.266455Z","iopub.status.idle":"2022-09-19T01:12:27.327405Z","shell.execute_reply.started":"2022-09-19T01:12:27.26642Z","shell.execute_reply":"2022-09-19T01:12:27.326109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìï Compraing Predictions</b></div>","metadata":{}},{"cell_type":"markdown","source":"# üìï 8. Compraing Predictions","metadata":{}},{"cell_type":"code","source":"if (use_voting_regressor == 1):\n    model_xgb.fit(train, y_train)\n    model_lgb.fit(train, y_train)\n    GBoost.fit(train, y_train)\n    CBoost.fit(train, y_train)\n    KRR.fit(train, y_train)\n    BR.fit(train, y_train)\n    averaged_models_temp = VotingRegressor(estimators=estimator_list, weights=weight_list) \n    averaged_models_temp.fit(train, y_train)\n\n    xt = test[:20]\n\n    pred1 = model_xgb.predict(xt)\n    pred2 = model_lgb.predict(xt)\n    pred3 = GBoost.predict(xt)\n    pred4 = CBoost.predict(xt)\n    pred5 = KRR.predict(xt)\n    pred6 = BR.predict(xt)\n    pred7 = averaged_models_temp.predict(xt)\n    plt.figure(figsize=(12,12))\n    plt.plot(pred1, 'gd', label='XGBRegressor')\n    plt.plot(pred2, 'bd', label='LGBRegressor')\n    plt.plot(pred3, 'yd', label='GradientBoostingRegressor')\n    plt.plot(pred4, 'rd', label='CatBoostRegressor')\n    plt.plot(pred5, 'gs', label='KernelRidge')\n    plt.plot(pred6, 'bs', label='BayesianRidge')\n    plt.plot(pred7, 'r*', ms=10, label='VotingRegressor')\n\n    plt.tick_params(axis='x', which='both', bottom=False, top=False,\n                    labelbottom=False)\n    plt.ylabel('predicted')\n    plt.xlabel('training samples')\n    plt.legend(loc=\"best\")\n    plt.title('Regressor predictions and their average')\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:12:27.329181Z","iopub.execute_input":"2022-09-19T01:12:27.329573Z","iopub.status.idle":"2022-09-19T01:13:37.095251Z","shell.execute_reply.started":"2022-09-19T01:12:27.329536Z","shell.execute_reply":"2022-09-19T01:13:37.094329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìí Stacking</b></div>","metadata":{}},{"cell_type":"markdown","source":"# üìí 9. Stacking","metadata":{}},{"cell_type":"code","source":"# Variant B\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=10, shuffle=True): # increasing the n_folds value should give a more accurate prediction, averaged over n_fold iterations\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n        self.shuffle = shuffle\n\n    # Fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # was 156\n        print(\"shuffle=\" + str(self.shuffle))\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models): # for each model passed in\n            with MyTimer():\n                model_name = model.__class__.__name__\n                model_details = str(model)\n                print('model_name:', model_name)\n                print('model_details:', model_details)\n                for train_index, holdout_index in kfold.split(X, y): # create train,holdout splits for the number of folds\n                    instance = clone(model)\n                    self.base_models_[i].append(instance)\n                    #if (\"KerasRegressor\" in model_name):\n                    if (\"KerasRegressor\" in model_name):\n                        hist = instance.fit(X[train_index], y[train_index], shuffle=self.shuffle, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n                    elif (\"keras\" in model_details):\n                        instance.fit(X[train_index], y[train_index], dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list)\n                    else:\n                        instance.fit(X[train_index], y[train_index]) # fit the model for this fold\n                    y_pred = instance.predict(X[holdout_index]) # predict values for this fold\n                    out_of_fold_predictions[holdout_index, i] = y_pred # add predictions for this model and fold (random rows)\n                    #print('out_of_fold_predictions', out_of_fold_predictions)\n        # Now train the cloned  meta-model using the out-of-fold predictions as new and only feature\n        print(\"out_of_fold_predictions\", out_of_fold_predictions)\n        \n        meta_model_name = self.meta_model_.__class__.__name__\n        print(\"meta_model_name:\", meta_model_name)\n        if (\"KerasRegressor\" in meta_model_name):\n            self.meta_model_.fit(out_of_fold_predictions, y, shuffle=self.shuffle, validation_split=0.3, callbacks=callbacks_list) # need to see out_of_fold_predictions feature set\n        elif (\"keras\" in str(self.meta_model_)):\n            self.meta_model_.fit(out_of_fold_predictions, y, dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list) # need to see out_of_fold_predictions feature set\n        else:\n            self.meta_model_.fit(out_of_fold_predictions, y) # need to see out_of_fold_predictions feature set\n\n        #self.meta_model_.fit(out_of_fold_predictions, y) # need to see out_of_fold_predictions feature set\n        return self \n    \n    # Calculate the predictions of all base models on the test data and use the averaged predictions as \n    # meta-features for the final prediction which is calculated by the meta-model\n    \n    # add MinMax\n    def predict(self, X):\n        # column_stack() function is used to stack 1-D arrays as columns into a 2-D array.\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:13:37.099386Z","iopub.execute_input":"2022-09-19T01:13:37.099745Z","iopub.status.idle":"2022-09-19T01:13:37.130974Z","shell.execute_reply.started":"2022-09-19T01:13:37.099709Z","shell.execute_reply":"2022-09-19T01:13:37.129917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, KRR, GBoost, model_xgb, lr_poly),#, dnn),\n                                                 meta_model = R_new, shuffle=True) \n\nif (compare_models == 1):\n    with MyTimer():\n        if (competition == 'SR'):\n            score = mae_cv(stacked_averaged_models)\n            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n        else:\n            score = rmsle_cv(stacked_averaged_models)\n            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:13:37.132537Z","iopub.execute_input":"2022-09-19T01:13:37.133053Z","iopub.status.idle":"2022-09-19T01:13:37.1492Z","shell.execute_reply.started":"2022-09-19T01:13:37.133003Z","shell.execute_reply":"2022-09-19T01:13:37.148022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_Regressor = 0\nif (use_Regressor == 1):\n    from sklearn.ensemble import StackingRegressor\n    from sklearn.linear_model import RidgeCV, LassoCV\n    from sklearn.linear_model import ElasticNetCV\n\n    use_cv = 1\n    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    if (use_cv == 1):\n        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))),\n                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n                      ('krr', KernelRidge(**k)),\n                      ('br', BayesianRidge())]\n        reg = StackingRegressor(\n            estimators=estimators,\n            final_estimator=RidgeCV(**r))\n    else:\n        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'tol': 0.05}\n        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.05, 'cv': None, 'gcv_mode': 'auto', 'random_state': 99} #  \n        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))),\n                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n                      ('krr', KernelRidge(**k)),\n                      ('br', BayesianRidge())]\n        reg = StackingRegressor(\n            estimators=estimators,\n            final_estimator=Ridge(**r))\n\n    reg.fit(train, y_train)\n    stacked_pred = inv_boxcox1p(reg.predict(test), lam_l)\n    #reg.transform(inv_boxcox1p(stacked_averaged_models_temp, lam_l))\n    print(stacked_pred)\n    print(reg)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:13:37.151219Z","iopub.execute_input":"2022-09-19T01:13:37.151888Z","iopub.status.idle":"2022-09-19T01:13:37.166856Z","shell.execute_reply.started":"2022-09-19T01:13:37.151846Z","shell.execute_reply":"2022-09-19T01:13:37.166134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (use_Regressor == 2):\n    from mlxtend.regressor import StackingCVRegressor\n    from sklearn.linear_model import RidgeCV, LassoCV\n    from sklearn.linear_model import ElasticNetCV\n\n    use_cv = 0\n    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    if (use_cv == 1):\n        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n        enet = make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))\n        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n        krr = KernelRidge(**k)\n        br = BayesianRidge()\n        r = RidgeCV(**r)\n        reg = StackingCVRegressor(\n            regressors=(enet, gboost, krr, br),\n            meta_regressor=r)\n    else:\n        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'random_state': 99} #  \n        enet = make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))\n        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n        krr = KernelRidge(**k)\n        br = BayesianRidge()\n        r = Ridge(**r)\n        reg = StackingCVRegressor(\n            regressors=(enet, gboost, krr, br),\n            meta_regressor=r)\n\n    reg.fit(train, y_train)\n    stacked_pred = inv_boxcox1p(reg.predict(test), lam_l)\n    print(stacked_pred)\n    print(reg)\n    \n    print('5-fold cross validation scores:\\n')\n    for clf, label in zip([enet, gboost, krr, br], ['enet', 'gboost', \n                                                'krr', 'br',\n                                                'StackingCVRegressor']):\n        scores = cross_val_score(clf, train, y_train, cv=5, scoring='neg_mean_squared_error')\n        print(\"Neg. MSE Score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n        \n        #scores = cross_val_score(clf, train, y_train, cv=5)\n        #print(\"R^2 Score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:13:37.168513Z","iopub.execute_input":"2022-09-19T01:13:37.169275Z","iopub.status.idle":"2022-09-19T01:13:37.184441Z","shell.execute_reply.started":"2022-09-19T01:13:37.169235Z","shell.execute_reply":"2022-09-19T01:13:37.183262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìò Ensembling</b></div>","metadata":{}},{"cell_type":"markdown","source":"# üìò 9. Ensembling","metadata":{}},{"cell_type":"markdown","source":"## Process Stacked Regressor","metadata":{}},{"cell_type":"code","source":"averaged_models.fit(train, y_train)\naveraged_train_pred = averaged_models.predict(train)\nif (use_voting_regressor == 0):\n    averaged_pred = inv_boxcox1p(averaged_models.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, averaged_train_pred))\nelse:\n    print(rmsle(y_train, averaged_train_pred))\n    \nplt.figure(figsize=(15,15))\nplt.scatter(averaged_train_pred, y_train, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:13:37.18664Z","iopub.execute_input":"2022-09-19T01:13:37.187239Z","iopub.status.idle":"2022-09-19T01:14:12.407072Z","shell.execute_reply.started":"2022-09-19T01:13:37.187204Z","shell.execute_reply":"2022-09-19T01:14:12.405975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (method == \"stacked\"):\n    with MyTimer():\n        stacked_averaged_models.fit(train.values, y_train)\n    with MyTimer():\n        stacked_train_pred = stacked_averaged_models.predict(train.values)\n    if (use_Regressor == 0):\n        stacked_pred = inv_boxcox1p(stacked_averaged_models.predict(test.values), lam_l)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:14:12.408802Z","iopub.execute_input":"2022-09-19T01:14:12.409216Z","iopub.status.idle":"2022-09-19T01:20:31.613183Z","shell.execute_reply.started":"2022-09-19T01:14:12.409177Z","shell.execute_reply":"2022-09-19T01:20:31.611716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_pred(train, y_train, test, model):\n    #model.fit(train, y_train)\n    \n    model_name = model.__class__.__name__\n    print('model_name:', model_name)\n    if (\"XGBRegressor\" in model_name):\n        model.fit(train, y_train) # fit the model for this fold\n    elif (\"KerasRegressor\" in model_name):\n        model.fit(train, y_train, shuffle=False, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n    elif (\"keras\" in str(model)):\n        model.fit(train, y_train, dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list) # fit the model for this fold\n    else:\n        print(\"train\", train)\n        print(\"y_train\", y_train)\n        model.fit(train, y_train) # fit the model for this fold\n\n    model_train_pred = model.predict(train)\n    model_pred = inv_boxcox1p(model.predict(test), lam_l)\n    return(model_train_pred, model_pred)\n    \nmodels      = [ CBoost, lasso, lasso_new, ENet, KRR, GBoost, model_xgb, BR ,  AB, KN, B] # model_lgb,\nmodel_names = ['CBoost', 'lasso', 'lasso_new', 'ENet', 'KRR', 'GBoost',  'model_xgb', 'BR',  'AB', 'KN', 'B']\ndf_train_pred = pd.DataFrame()\ndf_test_pred = pd.DataFrame()\nwith MyTimer():\n    for i in range(0,len(models)):\n        #print(\"models[i]\", models[i])\n        #print(\"model_names[i]:\", model_names[i])\n        mn = model_names[i]+\"_pred\"\n        train_pred, test_pred = fit_pred(train, y_train, test, models[i])\n        df_train_pred[mn] = train_pred\n        df_test_pred[mn] = test_pred\n        #print(mn, test_pred)    \n  ","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:20:31.619775Z","iopub.execute_input":"2022-09-19T01:20:31.620508Z","iopub.status.idle":"2022-09-19T01:21:08.135058Z","shell.execute_reply.started":"2022-09-19T01:20:31.620457Z","shell.execute_reply":"2022-09-19T01:21:08.133968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process XGB","metadata":{}},{"cell_type":"code","source":"replace_xgb = 0 # new optimized model is worse, was overfit\nif (replace_xgb == 1):\n    model_xgb_new.fit(train, y_train)\n    xgb_train_pred = model_xgb_new.predict(train)\n    xgb_pred = inv_boxcox1p(model_xgb_new.predict(test), lam_l)\nelse:\n    model_xgb.fit(train, y_train)\n    xgb_train_pred = model_xgb.predict(train)\n    xgb_pred = inv_boxcox1p(model_xgb.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, xgb_train_pred))\nelse:\n    print(rmsle(y_train, xgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:21:08.13664Z","iopub.execute_input":"2022-09-19T01:21:08.137009Z","iopub.status.idle":"2022-09-19T01:21:20.290282Z","shell.execute_reply.started":"2022-09-19T01:21:08.136972Z","shell.execute_reply":"2022-09-19T01:21:20.28944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process LGBM","metadata":{}},{"cell_type":"code","source":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = inv_boxcox1p(model_lgb.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, lgb_train_pred))\nelse:\n    print(rmsle(y_train, lgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:21:20.291948Z","iopub.execute_input":"2022-09-19T01:21:20.292346Z","iopub.status.idle":"2022-09-19T01:21:20.792109Z","shell.execute_reply.started":"2022-09-19T01:21:20.292311Z","shell.execute_reply":"2022-09-19T01:21:20.79111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (tuning_lgb == 1):\n    model_lgb_op.fit(train, y_train)\n    lgb_train_pred_op = model_lgb_op.predict(train)\n    lgb_pred_op = inv_boxcox1p(model_lgb_op.predict(test), lam_l)\n\n    if (competition == 'SR'):\n        print(mae(y_train, lgb_train_pred_op))\n    else:\n        print(rmsle(y_train, lgb_train_pred_op))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:21:20.795521Z","iopub.execute_input":"2022-09-19T01:21:20.79765Z","iopub.status.idle":"2022-09-19T01:21:20.804281Z","shell.execute_reply.started":"2022-09-19T01:21:20.797617Z","shell.execute_reply":"2022-09-19T01:21:20.803635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare values with optimization\nprint(lgb_train_pred)\nif (tuning_lgb == 1):\n    print(lgb_train_pred_op)\nprint(lgb_pred)\nif (tuning_lgb == 1):\n    print(lgb_pred_op)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:21:20.805881Z","iopub.execute_input":"2022-09-19T01:21:20.806281Z","iopub.status.idle":"2022-09-19T01:21:20.817606Z","shell.execute_reply.started":"2022-09-19T01:21:20.806242Z","shell.execute_reply":"2022-09-19T01:21:20.816685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('CBoost_new.pkl', 'rb') as fid:\n    CBoost_new = pickle.load(fid)\n    \nCBoost_new.fit\ncb_pred = inv_boxcox1p(CBoost_new.predict(test), lam_l)\ncb_train_pred = inv_boxcox1p(CBoost_new.predict(train), lam_l)\nprint(cb_pred)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:21:20.819158Z","iopub.execute_input":"2022-09-19T01:21:20.81958Z","iopub.status.idle":"2022-09-19T01:21:20.871544Z","shell.execute_reply.started":"2022-09-19T01:21:20.819532Z","shell.execute_reply":"2022-09-19T01:21:20.870414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENet.fit(train,y_train)\nKRR.fit(train,y_train)\nGBoost.fit(train,y_train)\nlr_poly.fit(train,y_train)\nmodel_xgb.fit(train,y_train)\nenet_pred = inv_boxcox1p(ENet.predict(test), lam_l)\nkrr_pred = inv_boxcox1p(KRR.predict(test), lam_l)\ngboost_pred = inv_boxcox1p(GBoost.predict(test), lam_l)\nlr_pred = inv_boxcox1p(lr_poly.predict(test), lam_l)\n\nbr_pred = inv_boxcox1p(BR.predict(test), lam_l)\nbr_train_pred = inv_boxcox1p(BR.predict(train), lam_l)\n# all models being used in final model\n#print(cb_pred)\n#print(xgb_pred)\n#print(lgb_pred)\nprint(br_pred)\nprint(enet_pred) \nprint(krr_pred)\nprint(gboost_pred)\nprint(lr_pred) # changes each time\n#print(dnn_pred)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:21:20.872932Z","iopub.execute_input":"2022-09-19T01:21:20.873293Z","iopub.status.idle":"2022-09-19T01:22:00.345013Z","shell.execute_reply.started":"2022-09-19T01:21:20.873257Z","shell.execute_reply":"2022-09-19T01:22:00.343497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (method == 'stacked'):\n    testing_ratio = 0\n    if (testing_ratio == 1):\n        stkr = 1.00\n        xgbr = 0.00\n        lgbr = 0.00\n        cbr  = 0.00\n        brr  = 0.00\n    else:\n        stkr = 0.70\n        xgbr = 0.10 # .10\n        lgbr = 0.00\n        cbr  = 0.20 # .20\n        brr  = 0.00 # 0\n\n    '''RMSE on the entire Train data when averaging'''\n\n    print('RMSLE score on train data:')\n    print(mae(y_train,stacked_train_pred*stkr+ xgb_train_pred*xgbr + lgb_train_pred*lgbr + cb_train_pred*cbr + br_train_pred*brr ))\n    print(rmsle(y_train,stacked_train_pred*stkr + xgb_train_pred*xgbr + lgb_train_pred*lgbr + cb_train_pred*cbr + br_train_pred*brr ))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.352298Z","iopub.execute_input":"2022-09-19T01:22:00.356183Z","iopub.status.idle":"2022-09-19T01:22:00.377043Z","shell.execute_reply.started":"2022-09-19T01:22:00.356122Z","shell.execute_reply":"2022-09-19T01:22:00.37558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(method)\nif (method == 'stacked'):\n    ensemble = stacked_pred*stkr + xgb_pred*xgbr + lgb_pred*lgbr + cb_pred*cbr + br_pred*brr  # if using averaged_pred, need to add averaged_pred here\nelse:\n    ensemble = averaged_pred","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.379058Z","iopub.execute_input":"2022-09-19T01:22:00.379775Z","iopub.status.idle":"2022-09-19T01:22:00.386599Z","shell.execute_reply.started":"2022-09-19T01:22:00.379739Z","shell.execute_reply":"2022-09-19T01:22:00.385653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(xgb_pred)\nprint(lgb_pred)\nprint(cb_pred)\nprint(br_pred)\nprint(averaged_pred)\nprint(ensemble)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.388193Z","iopub.execute_input":"2022-09-19T01:22:00.388841Z","iopub.status.idle":"2022-09-19T01:22:00.405952Z","shell.execute_reply.started":"2022-09-19T01:22:00.388805Z","shell.execute_reply":"2022-09-19T01:22:00.404733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (method == 'stacked'):\n    print(y_train,stacked_train_pred * stkr + xgb_train_pred * xgbr + lgb_train_pred * lgbr) # if using averaged_pred, need to add averaged_pred here\n    print(y_train,stacked_train_pred)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.411913Z","iopub.execute_input":"2022-09-19T01:22:00.41386Z","iopub.status.idle":"2022-09-19T01:22:00.432214Z","shell.execute_reply.started":"2022-09-19T01:22:00.41381Z","shell.execute_reply":"2022-09-19T01:22:00.430437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_train = pd.DataFrame()\nsub_train['Id'] = train_ID\nif (method == 'stacked'):\n    sub_train['SalePrice'] = inv_boxcox1p(stacked_train_pred, lam_l)\nelse:\n    sub_train['SalePrice'] = inv_boxcox1p(averaged_train_pred, lam_l)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.437481Z","iopub.execute_input":"2022-09-19T01:22:00.438929Z","iopub.status.idle":"2022-09-19T01:22:00.459447Z","shell.execute_reply.started":"2022-09-19T01:22:00.438866Z","shell.execute_reply":"2022-09-19T01:22:00.457971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Predicted = sub_train['SalePrice']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.460994Z","iopub.execute_input":"2022-09-19T01:22:00.461606Z","iopub.status.idle":"2022-09-19T01:22:00.790442Z","shell.execute_reply.started":"2022-09-19T01:22:00.461549Z","shell.execute_reply":"2022-09-19T01:22:00.789489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre-adjustment score\nprint(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice']))\nprint(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice']))\nprint(\"mae for SalePrice\",mae(Actual, Predicted))\nprint(\"mse for SalePrice\",rmsle(Actual, Predicted))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.791828Z","iopub.execute_input":"2022-09-19T01:22:00.79251Z","iopub.status.idle":"2022-09-19T01:22:00.802408Z","shell.execute_reply.started":"2022-09-19T01:22:00.792474Z","shell.execute_reply":"2022-09-19T01:22:00.801242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Findling best cutoff","metadata":{}},{"cell_type":"code","source":"def AdjustHigh(sub_train, y_train):\n    AdjustedScores = []\n    for i in np.arange(.994, 1.000, 0.01):\n        for j in np.arange(1.00, 1.10, .01):\n\n            q1 = sub_train['SalePrice'].quantile(0.0025)\n            q2 = sub_train['SalePrice'].quantile(0.0045)\n            q3 = sub_train['SalePrice'].quantile(i)\n\n            #Verify the cutoffs for the adjustment\n            # adjust at low end\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n            # adjust at high end\n            sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n\n            Predicted = sub_train['SalePrice2']\n            Actual = inv_boxcox1p(y_train, lam_l)\n\n            # Pre-adjustment score\n            #print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n            #print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n            #print(\"mae for SalePrice\",mae(Actual, Predicted))\n            #print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n\n            AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n    print(q1,q2,q3)\n    df_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])\n    print('quantiles vs coefficients')\n    df_adj.sort_values(by=['RMSE_BC'])\n    print(df_adj)\n    df2 = df_adj.sort_values(by=['RMSE_BC']).reset_index()\n    coef_hi = df2.COEF[0]\n    quant_hi = df2.QUANT[0]\n    return (coef_hi, quant_hi)\n\nch, qh = AdjustHigh(sub_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.803945Z","iopub.execute_input":"2022-09-19T01:22:00.805316Z","iopub.status.idle":"2022-09-19T01:22:00.87574Z","shell.execute_reply.started":"2022-09-19T01:22:00.805272Z","shell.execute_reply":"2022-09-19T01:22:00.874621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('ch',ch)\nprint('qh',qh)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.877146Z","iopub.execute_input":"2022-09-19T01:22:00.877511Z","iopub.status.idle":"2022-09-19T01:22:00.882884Z","shell.execute_reply.started":"2022-09-19T01:22:00.877476Z","shell.execute_reply":"2022-09-19T01:22:00.881926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1 = sub_train['SalePrice'].quantile(0.0015)\nq2 = sub_train['SalePrice'].quantile(0.01)\nq3 = sub_train['SalePrice'].quantile(qh)\n\nprint(q1,q2,q3)\n\nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*ch)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.884286Z","iopub.execute_input":"2022-09-19T01:22:00.885172Z","iopub.status.idle":"2022-09-19T01:22:00.903402Z","shell.execute_reply.started":"2022-09-19T01:22:00.885127Z","shell.execute_reply":"2022-09-19T01:22:00.90221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Adjusted Predicted Price')\nplt.title('Averaged Model')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:00.905607Z","iopub.execute_input":"2022-09-19T01:22:00.906178Z","iopub.status.idle":"2022-09-19T01:22:01.211686Z","shell.execute_reply.started":"2022-09-19T01:22:00.906144Z","shell.execute_reply":"2022-09-19T01:22:01.210614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_train.query(\"SalePrice != SalePrice2\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:01.213383Z","iopub.execute_input":"2022-09-19T01:22:01.214042Z","iopub.status.idle":"2022-09-19T01:22:01.229239Z","shell.execute_reply.started":"2022-09-19T01:22:01.214004Z","shell.execute_reply":"2022-09-19T01:22:01.228312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Predicted = sub_train['SalePrice2']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Adj Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:01.231983Z","iopub.execute_input":"2022-09-19T01:22:01.232811Z","iopub.status.idle":"2022-09-19T01:22:01.550433Z","shell.execute_reply.started":"2022-09-19T01:22:01.232775Z","shell.execute_reply":"2022-09-19T01:22:01.549523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Post adjustment for high score\nprint(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\nprint(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\nprint(\"mae for SalePrice2\",mae(Actual, Predicted))\nprint(\"mse for SalePrice2\",rmsle(Actual, Predicted))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:01.551879Z","iopub.execute_input":"2022-09-19T01:22:01.552223Z","iopub.status.idle":"2022-09-19T01:22:01.561098Z","shell.execute_reply.started":"2022-09-19T01:22:01.552196Z","shell.execute_reply":"2022-09-19T01:22:01.560058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cutoff at low","metadata":{}},{"cell_type":"code","source":"def AdjustLow(sub_train, y_train):\n    AdjustedScores = []\n    for i in np.arange(.00, .02, 0.001):\n        for j in np.arange(.90, 1.10, 0.01):\n\n            q1 = sub_train['SalePrice'].quantile(i)\n            q2 = sub_train['SalePrice'].quantile(0.1)\n            q3 = sub_train['SalePrice'].quantile(.995)\n\n            #Verify the cutoffs for the adjustment\n            #print(q1,q2,q3)\n            # adjust at low end\n            sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*j)\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n            # adjust at high end\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n\n            Predicted = sub_train['SalePrice2']\n            Actual = inv_boxcox1p(y_train, lam_l)\n\n            # Pre-adjustment score\n            #print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n            #print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n            #print(\"mae for SalePrice\",mae(Actual, Predicted))\n            #print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n\n            AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n  \n    df_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])\n    print('quantiles vs coefficients')\n    df_adj.sort_values(by=['RMSE_BC'])\n    print(df_adj)\n    df2 = df_adj.sort_values(by=['RMSE_BC']).reset_index()\n    coef_lo = df2.COEF[0]\n    quant_lo = df2.QUANT[0]\n    return (coef_lo, quant_lo)\n\ncl, ql = AdjustLow(sub_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:01.562836Z","iopub.execute_input":"2022-09-19T01:22:01.563644Z","iopub.status.idle":"2022-09-19T01:22:03.764249Z","shell.execute_reply.started":"2022-09-19T01:22:01.563603Z","shell.execute_reply":"2022-09-19T01:22:03.763051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('cl',cl)\nprint('ql',ql)\n\nq1 = sub_train['SalePrice'].quantile(ql)\nq2 = sub_train['SalePrice'].quantile(0.1)\nq3 = sub_train['SalePrice'].quantile(qh)\n\nprint(q1,q2,q3)\n\n\nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x*cl if x < q1 else ( x*ch if x > q3 else x))\nsub_train.query(\"SalePrice != SalePrice2\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:03.766224Z","iopub.execute_input":"2022-09-19T01:22:03.76697Z","iopub.status.idle":"2022-09-19T01:22:03.790501Z","shell.execute_reply.started":"2022-09-19T01:22:03.766927Z","shell.execute_reply":"2022-09-19T01:22:03.789504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x*cl if x < q1 else ( x*ch if x > q3 else x))\n\nsub_train.query(\"SalePrice != SalePrice2\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:03.792103Z","iopub.execute_input":"2022-09-19T01:22:03.792466Z","iopub.status.idle":"2022-09-19T01:22:03.809513Z","shell.execute_reply.started":"2022-09-19T01:22:03.792432Z","shell.execute_reply":"2022-09-19T01:22:03.80837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Adjusted Predicted Price')\nplt.title('Averaged Model')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:03.8114Z","iopub.execute_input":"2022-09-19T01:22:03.812371Z","iopub.status.idle":"2022-09-19T01:22:04.126619Z","shell.execute_reply.started":"2022-09-19T01:22:03.812336Z","shell.execute_reply":"2022-09-19T01:22:04.125524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Predicted = sub_train['SalePrice2']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Adj Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:04.128101Z","iopub.execute_input":"2022-09-19T01:22:04.128452Z","iopub.status.idle":"2022-09-19T01:22:04.452631Z","shell.execute_reply.started":"2022-09-19T01:22:04.128416Z","shell.execute_reply":"2022-09-19T01:22:04.451575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_train.query(\"SalePrice != SalePrice2\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:04.45399Z","iopub.execute_input":"2022-09-19T01:22:04.455056Z","iopub.status.idle":"2022-09-19T01:22:04.469547Z","shell.execute_reply.started":"2022-09-19T01:22:04.455011Z","shell.execute_reply":"2022-09-19T01:22:04.468482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Post adjustment for low and high score\nprint(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\nprint(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\nprint(\"mae for SalePrice2\",mae(Actual, Predicted))\nprint(\"mse for SalePrice2\",rmsle(Actual, Predicted))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:04.471158Z","iopub.execute_input":"2022-09-19T01:22:04.472048Z","iopub.status.idle":"2022-09-19T01:22:04.482384Z","shell.execute_reply.started":"2022-09-19T01:22:04.47201Z","shell.execute_reply":"2022-09-19T01:22:04.481237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:10px;padding-left: 50px;color:black;margin:0;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#e9f2c7;overflow:hidden;font-weight:500\"><b>üìÅ Submission</b></div>","metadata":{}},{"cell_type":"markdown","source":"# üìÅ 10. Submission ","metadata":{}},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\n\nadjust_both = 0\nadjust_low = 0\nadjust_high = 1\n\nq1 = sub['SalePrice'].quantile(ql)\nq2 = sub['SalePrice'].quantile(0.1)\nq3 = sub['SalePrice'].quantile(qh)\n\nif (adjust_both == 1):\n    sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x*cl if (x < q1) else ( x*ch if (x > q3) else x))\nelif (adjust_low == 1):\n    sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if (x > q1) else x*cl) \nelif (adjust_high == 1):\n    sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if (x < q3) else x*ch)\nelse:\n    print(\"no adjustments made\")\n    \nsub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T01:22:04.484591Z","iopub.execute_input":"2022-09-19T01:22:04.485079Z","iopub.status.idle":"2022-09-19T01:22:04.506173Z","shell.execute_reply.started":"2022-09-19T01:22:04.485039Z","shell.execute_reply":"2022-09-19T01:22:04.505036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:60px;color:black;margin:0;font-size:450%;text-align:center;display:fill;border-radius:5px;background-color:#fac70f;overflow:hidden;font-weight:800\"><b>Upvote, Comment</br>Fork, Share</b></div>","metadata":{}},{"cell_type":"markdown","source":"## ","metadata":{}}]}