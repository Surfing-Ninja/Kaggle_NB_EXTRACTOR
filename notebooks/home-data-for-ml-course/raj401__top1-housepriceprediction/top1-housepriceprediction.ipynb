{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nHello fellow Kagglers! In this notebook I will show you how you can achieve top 1% rank in most simple and efficient way. First of all I am really thankful to the amazing Kaggle community which helped me learn so many things. I have created this notebook for learning purpose and give back to the community. So I will keep updating it from time to time.\n\n#### My main objectives on this project are:\n+ Get to top 1% with minimum lines of code.\n+ Learn to use Pipeline.\n+ To explain each and every step and the logic behind it.\n+ Create our own prediction from scratch without using public kernels.\n+ Learn to using three models:- RandomForestRegressor, GradientBoostingRegressor, CatBoostRegressor.\n<a id='top'></a> <br>\n## NOTEBOOK CONTENT\n1. [Imports](#1)\n1. [Load Data](#2)\n1. [Preprocessing](#3)\n1. [Implement Pipeline](#4)\n1. [Create Models](#5)\n1. [Evaluate Models](#6)\n1. [Predict Test set](#7)\n1. [Make Submission](#8)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## 1: Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.323261Z","iopub.execute_input":"2021-08-27T21:13:55.323843Z","iopub.status.idle":"2021-08-27T21:13:55.328705Z","shell.execute_reply.started":"2021-08-27T21:13:55.32376Z","shell.execute_reply":"2021-08-27T21:13:55.32786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 2: Load Data","metadata":{}},{"cell_type":"code","source":"X_full = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\", index_col= 'Id')\nX_test_full = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\", index_col = 'Id')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.381656Z","iopub.execute_input":"2021-08-27T21:13:55.382228Z","iopub.status.idle":"2021-08-27T21:13:55.454058Z","shell.execute_reply.started":"2021-08-27T21:13:55.382193Z","shell.execute_reply":"2021-08-27T21:13:55.452634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the shape of our datasets by .shape attribute","metadata":{}},{"cell_type":"code","source":"X_full.shape, X_test_full.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.492731Z","iopub.execute_input":"2021-08-27T21:13:55.493224Z","iopub.status.idle":"2021-08-27T21:13:55.502497Z","shell.execute_reply.started":"2021-08-27T21:13:55.493155Z","shell.execute_reply":"2021-08-27T21:13:55.501077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see X_full has 80 columns (all 79 features + 1 target variable) and X_test_full has 79 columns(all 79 features)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 3: Preprocessing\n### 3.a Remove rows with missing target, separate target variable from feature variables\n.dropna() methods with axis=0 drops rows which has null value, here we have set subset=['SalePrice'] which means we drop all the rows whose 'SalePrice' is null. This subset tells it to look at only 'SalePrice' columns.\nBasically here we are dropping all the rows in our dataset whoose target value is null as it is of no use in training our model to make it even better.","metadata":{}},{"cell_type":"code","source":"X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.572235Z","iopub.execute_input":"2021-08-27T21:13:55.572695Z","iopub.status.idle":"2021-08-27T21:13:55.586952Z","shell.execute_reply.started":"2021-08-27T21:13:55.572652Z","shell.execute_reply":"2021-08-27T21:13:55.5856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n### 3.b Split our data into train and validation","metadata":{}},{"cell_type":"code","source":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.622027Z","iopub.execute_input":"2021-08-27T21:13:55.622447Z","iopub.status.idle":"2021-08-27T21:13:55.634263Z","shell.execute_reply.started":"2021-08-27T21:13:55.622413Z","shell.execute_reply":"2021-08-27T21:13:55.633095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_full.shape, X_valid_full.shape, y_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.651516Z","iopub.execute_input":"2021-08-27T21:13:55.651866Z","iopub.status.idle":"2021-08-27T21:13:55.659213Z","shell.execute_reply.started":"2021-08-27T21:13:55.651834Z","shell.execute_reply":"2021-08-27T21:13:55.658202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 3.c Select categorical columns with relatively low cardinality.\nIn this problem we convert all the categorical columns into one hot encoding.\nNote:- If a categorical variable has 100 columns then its one hot encoding will create 100 new columns.So our data will become of high dimensional.\nThis makes training hard. This phenomena is called \n#### CURSE OF DIMENSIONALITY\nSo we first filter our categorical columns which has less than 10 unique values.\n\n(Note:- There are many other ways of tackling this issue, one method is we look at their frequencies, and combine values with less than .05% frequency into one category)","metadata":{}},{"cell_type":"code","source":"categorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.712099Z","iopub.execute_input":"2021-08-27T21:13:55.712571Z","iopub.status.idle":"2021-08-27T21:13:55.755447Z","shell.execute_reply.started":"2021-08-27T21:13:55.712533Z","shell.execute_reply":"2021-08-27T21:13:55.754163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 3.d Select numerical columns\nA numerical value can be either int type or float type","metadata":{}},{"cell_type":"code","source":"# numerical_cols = [cname for cname in X_train_full.columns if \n#                 X_train_full[cname].dtype in ['int64', 'float64']]\n# update:- \nnumerical_cols = X_train_full.select_dtypes(exclude=['object']).columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.781808Z","iopub.execute_input":"2021-08-27T21:13:55.782244Z","iopub.status.idle":"2021-08-27T21:13:55.788091Z","shell.execute_reply.started":"2021-08-27T21:13:55.782204Z","shell.execute_reply":"2021-08-27T21:13:55.787221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 3.e categorical_cols + numerical_cols\nwe create copies so that we don't tamper with the original dataset\n","metadata":{}},{"cell_type":"code","source":"my_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.85217Z","iopub.execute_input":"2021-08-27T21:13:55.853089Z","iopub.status.idle":"2021-08-27T21:13:55.870229Z","shell.execute_reply.started":"2021-08-27T21:13:55.853034Z","shell.execute_reply":"2021-08-27T21:13:55.868445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 4: Implement Pipeline\nPipeline is very useful it can save time.\n\nSome benefits of pipeline:-\n\n1) Cleaner code\n\n2) Fewer Bugs\n\n3) Easier to Productionize\n\n4) More options for Model Validation\n#### So lets implement pipeline\n### 4.a Imports","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.902085Z","iopub.execute_input":"2021-08-27T21:13:55.902471Z","iopub.status.idle":"2021-08-27T21:13:55.908699Z","shell.execute_reply.started":"2021-08-27T21:13:55.902435Z","shell.execute_reply":"2021-08-27T21:13:55.907018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 4.a Preprocessing for numerical data\nLets use SimpleImputer to fill all missing values in our numerical columns","metadata":{}},{"cell_type":"code","source":"numerical_transformer = SimpleImputer(strategy='constant')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:55.982397Z","iopub.execute_input":"2021-08-27T21:13:55.98298Z","iopub.status.idle":"2021-08-27T21:13:55.989783Z","shell.execute_reply.started":"2021-08-27T21:13:55.982932Z","shell.execute_reply":"2021-08-27T21:13:55.988641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 4.b Preprocessing for categorical data\nFor categorical data we create pipeline for preprocessing\n1) We first fill all missing values with the most_frequent value in that column also called as the mode.\n\n2) We convert to one hot encoding.","metadata":{}},{"cell_type":"code","source":"categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:56.05164Z","iopub.execute_input":"2021-08-27T21:13:56.052014Z","iopub.status.idle":"2021-08-27T21:13:56.05785Z","shell.execute_reply.started":"2021-08-27T21:13:56.051978Z","shell.execute_reply":"2021-08-27T21:13:56.056704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 4.c Bundle preprocessing for numerical and categorical data\nWe use ColumnTransformer from sklearn, this lets us apply preprocessing on selected columns.\nHere we apply numerical_transformer on all the numerical columns and categorical_transformer on categorical columns.\n\n(Note:- numerical_cols is a list containing names of all numerical columns)","metadata":{}},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:56.112159Z","iopub.execute_input":"2021-08-27T21:13:56.112929Z","iopub.status.idle":"2021-08-27T21:13:56.118875Z","shell.execute_reply.started":"2021-08-27T21:13:56.112878Z","shell.execute_reply":"2021-08-27T21:13:56.117396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 5: Create Models\nNow we create 3 different models and compare their results. \n\n(We can do hyperparameter tuning to even further fine tune our models using GridSeachCV, RandomizedSeachCV etc but for now we use the deafault models)\n### 5.a Model1:- RandomForestRegressor\nHere we create another pipeline which has two steps:- preprocessor and model1.\n\nWhen we do .fit() it fits and transform for the preprocessor and fits for model1.\n\nWhen we do .predict() it transform for the preprocessor and predicts for model1.\n\n(Note:- we can create our own custom objects for creating pipeline but that will be an advance topic)","metadata":{}},{"cell_type":"code","source":"# Define model1\nmodel1 = RandomForestRegressor(n_estimators=800,random_state=20)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline1 = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model1)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline1.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds1 = my_pipeline1.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:13:56.199962Z","iopub.execute_input":"2021-08-27T21:13:56.200338Z","iopub.status.idle":"2021-08-27T21:14:17.354956Z","shell.execute_reply.started":"2021-08-27T21:13:56.200298Z","shell.execute_reply":"2021-08-27T21:14:17.35358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n### 5.b Model2:- GradientBoostingRegressor","metadata":{}},{"cell_type":"code","source":"# Define model2\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel2 = GradientBoostingRegressor(n_estimators=600, random_state=32)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline2 = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model2)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline2.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds2 = my_pipeline2.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:14:17.356679Z","iopub.execute_input":"2021-08-27T21:14:17.357028Z","iopub.status.idle":"2021-08-27T21:14:23.078972Z","shell.execute_reply.started":"2021-08-27T21:14:17.356994Z","shell.execute_reply":"2021-08-27T21:14:23.077924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n### 5.c Model3:- CatBoostRegressor","metadata":{}},{"cell_type":"code","source":"# Define model3\nimport catboost as cb\nmodel3 = cb.CatBoostRegressor(loss_function='RMSE',random_state=20,verbose=False)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline3 = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model3)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline3.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds3 = my_pipeline3.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:14:23.083556Z","iopub.execute_input":"2021-08-27T21:14:23.083931Z","iopub.status.idle":"2021-08-27T21:14:27.648937Z","shell.execute_reply.started":"2021-08-27T21:14:23.083896Z","shell.execute_reply":"2021-08-27T21:14:27.647826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 6: Evaluate models\n## 6.a Let's Look at MAE of each model","metadata":{}},{"cell_type":"code","source":"score = mean_absolute_error(y_valid, preds1)\nprint('MAE:', score)\nscore = mean_absolute_error(y_valid, preds2)\nprint('MAE:', score)\nscore = mean_absolute_error(y_valid, preds3)\nprint('MAE:', score)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:14:27.650839Z","iopub.execute_input":"2021-08-27T21:14:27.651294Z","iopub.status.idle":"2021-08-27T21:14:27.660876Z","shell.execute_reply.started":"2021-08-27T21:14:27.651227Z","shell.execute_reply":"2021-08-27T21:14:27.659608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see their MAE are as follows:-\n\nRandomForestRegressor:-17226.008848458903\n\nGradientBoostingRegressor:-15563.235198675247\n\nCatBoostRegressor:-16001.783475611544\n<a id=\"2\"></a> <br>\n### 6.b Average of their predictions\nGenerally average of very different models with same score gives great boost on leaderboard.\n   \nLogic:-Because if models have same score means they have same no of correct predictions and if the models are very much different means they are correct at different data points. Thus averaging their predictions increases no of correct predictions.","metadata":{}},{"cell_type":"code","source":"preds= (preds1+ preds2+ preds3)/3\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:14:27.662605Z","iopub.execute_input":"2021-08-27T21:14:27.663052Z","iopub.status.idle":"2021-08-27T21:14:27.673982Z","shell.execute_reply.started":"2021-08-27T21:14:27.663005Z","shell.execute_reply":"2021-08-27T21:14:27.672664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly we can see that the averaging of predictions has improved our score.\n<a id=\"2\"></a> <br>\n## 7: Predict Test set\nThe real power of pipeline can be seen here. Now we don't need to preprocess test set separately. We will just do .predict() and it will automatically preprocess it.","metadata":{}},{"cell_type":"code","source":"# Preprocessing of test data, fit model\npreds_test1 = my_pipeline1.predict(X_test)\npreds_test2 = my_pipeline2.predict(X_test)\npreds_test3 = my_pipeline3.predict(X_test)\npreds_test = (preds_test1+preds_test2+ preds_test3 )/3","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:14:43.772877Z","iopub.execute_input":"2021-08-27T21:14:43.773297Z","iopub.status.idle":"2021-08-27T21:14:44.361977Z","shell.execute_reply.started":"2021-08-27T21:14:43.77325Z","shell.execute_reply":"2021-08-27T21:14:44.360866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 8: Make submission","metadata":{}},{"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission73.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:14:44.601803Z","iopub.execute_input":"2021-08-27T21:14:44.602142Z","iopub.status.idle":"2021-08-27T21:14:44.615382Z","shell.execute_reply.started":"2021-08-27T21:14:44.602112Z","shell.execute_reply":"2021-08-27T21:14:44.61413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End Note\n## **Things you can try to improve performance.**\n#### 1. Use hyperparameter tuning to find the best parameters\n#### 2. Use StandardScaler to scale all the features (Note: don't scale target variables)\n#### 3. Try using other model like XGBoostRegressor, Neural Network   \n##### (Remember: average of predictions of very different models like (tree base/NN) with similar score will give great boost in performance)\n#### 4. When you get the best combination of models make a for loop and generate 50 different predictions using different random seeds and take its average. You will see there is some improvement in performance.\n## <font color='orange'><b>If you have any doubts feel free to ask below, I would be happy to help.</b></font>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}