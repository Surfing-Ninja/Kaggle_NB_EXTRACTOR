{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color = 'blue'>\nContent: \n\n1. [Thanks to Kaggle Community as Introduction](#1)\n2. [Load Python Pakages and Data](#2)\n3. [First look to data](#3)\n4. [Exploratory Data Analysis](#4)   \n5. [Feature Engineering](#5)   \n6. [Preprocesing](#6)\n   * [Tree preprocessor](#7)\n   * [Linear preprocessor](#8)\n   * [A custom pipeline for Feature Engineering](#9)\n7. [Putting pieces together](#10)\n8. [Modeling and hyperparameter tuning](#11)\n    * [Elasticnet example](#12)\n    * [All tunned regressors](#13)\n9. [Stacking](#14)\n10. [Submission](#15)\n11. [Appendix](#16)\n    * [Notes](#17)\n    * [Optuna tuning settings for different models](#18)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-12-13T11:19:31.437575Z","iopub.execute_input":"2022-12-13T11:19:31.437915Z","iopub.status.idle":"2022-12-13T11:19:31.47087Z","shell.execute_reply.started":"2022-12-13T11:19:31.437847Z","shell.execute_reply":"2022-12-13T11:19:31.469669Z"}}},{"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n# Thanks to Kaggle Community as Introduction","metadata":{}},{"cell_type":"markdown","source":"I tried to apply what I have learned so far, and it became one of the best-performing notebooks. It would not be possible without the Kaggle community's generous knowledge share. I tried many things, which include inspirations from others' works or using directly their codeblocks. \n\nSpecial thanks to the following notebooks and their owners:\n\n* To ERTUÄžRUL DEMIR for a cristal clear workflow - https://www.kaggle.com/code/datafan07/top-1-approach-eda-new-models-and-stacking \n* To GUNES EVITAN for his art-like visualizations - https://www.kaggle.com/code/gunesevitan/house-prices-advanced-stacking-tutorial\n* To LUCA BASANISI for the best tutorial on the entire internet for pipelines https://www.kaggle.com/code/lucabasa/understand-and-use-a-pipeline/notebook\n* To Wojciech Sylwester for a good baseline with a single regressor  https://www.kaggle.com/code/wojteksy/housing-prices-pipelines-custom-transformer\n\n\n\nThis notebook benefits best preprocessing and feature engineering practices posted by the community, and differentiates itself with the followings:\n\n* 1- Almost an obsession about doing everything in a pipeline, including feature engineering, preprocessing, and hyperparameter tuning\n* 2- An aggressive hyperparameter search for ensemble models with Optuna package\n* 3- Add some extra features not included in others' works\n\nLet's get started...\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n# Load Python Pakages\n","metadata":{}},{"cell_type":"code","source":"#basics\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n#preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.preprocessing import QuantileTransformer, quantile_transform\n\n\n#statistics\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import randint\n\n#feature engineering\nfrom sklearn.feature_selection import mutual_info_regression\n\n\n#transformers and pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn import set_config\n\n\n#algorithms\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom sklearn.svm import SVR\n\n\n#model evaluation\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice\n\n\n#stacking\nfrom sklearn.ensemble import StackingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:37.548569Z","iopub.execute_input":"2022-12-15T16:20:37.548989Z","iopub.status.idle":"2022-12-15T16:20:37.561981Z","shell.execute_reply.started":"2022-12-15T16:20:37.548958Z","shell.execute_reply":"2022-12-15T16:20:37.561102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n#  First look to data","metadata":{}},{"cell_type":"code","source":"# Read the data\noriginal_train_df = pd.read_csv('../input/home-data-for-ml-course/train.csv', index_col='Id')\noriginal_test_df = pd.read_csv('../input/home-data-for-ml-course/test.csv', index_col='Id')\n\n# reserved for pipeline\npipe_data = original_train_df.copy()\npipe_test = original_test_df.copy()\n\n# use for preliminary analysis\ntrain_df = original_train_df.copy()\ntest_df = original_test_df.copy()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:37.563534Z","iopub.execute_input":"2022-12-15T16:20:37.564507Z","iopub.status.idle":"2022-12-15T16:20:37.647322Z","shell.execute_reply.started":"2022-12-15T16:20:37.56447Z","shell.execute_reply":"2022-12-15T16:20:37.646059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descpriptive statistics for numerical and categorical data","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:55:35.751133Z","iopub.execute_input":"2022-12-13T13:55:35.752032Z","iopub.status.idle":"2022-12-13T13:55:35.75685Z","shell.execute_reply.started":"2022-12-13T13:55:35.751966Z","shell.execute_reply":"2022-12-13T13:55:35.75563Z"}}},{"cell_type":"code","source":"#numerical feature descriptive statistics\ntrain_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:37.648785Z","iopub.execute_input":"2022-12-15T16:20:37.649112Z","iopub.status.idle":"2022-12-15T16:20:37.766773Z","shell.execute_reply.started":"2022-12-15T16:20:37.649076Z","shell.execute_reply":"2022-12-15T16:20:37.765859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#categorical feature descriptive statistics\n\ntrain_df.describe(include='object').T.sort_values(by=['unique'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:37.767883Z","iopub.execute_input":"2022-12-15T16:20:37.768431Z","iopub.status.idle":"2022-12-15T16:20:37.848712Z","shell.execute_reply.started":"2022-12-15T16:20:37.768397Z","shell.execute_reply":"2022-12-15T16:20:37.84758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing values","metadata":{}},{"cell_type":"raw","source":"There are many missing values let's count and visualize them...","metadata":{}},{"cell_type":"code","source":"missing = pd.DataFrame(train_df.isnull().sum().sort_values(ascending=False))\nmissing.columns = [\"count\"]\nmissing = missing.loc[(missing!=0).any(axis=1)]\nmissing[\"percent\"] = missing[0:] / 1460\nmissing.style.background_gradient('viridis')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:37.852272Z","iopub.execute_input":"2022-12-15T16:20:37.852751Z","iopub.status.idle":"2022-12-15T16:20:37.877595Z","shell.execute_reply.started":"2022-12-15T16:20:37.852714Z","shell.execute_reply":"2022-12-15T16:20:37.876207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you see from the above summary, we have many missing entries. If you read the data summary (https://www.kaggle.com/competitions/home-data-for-ml-course/data)what you will see that all the missing values are on purpose. These houses just don't have stated features.\n\nWe will take care of missing values as following:\n* Fill all missing entries in numerical features with 0\n* Fill all missing entries in categorical features with 'Do_not_have_this_feature'\n\nWe will do this imputations in our pipeline, for now let's just keep going...","metadata":{}},{"cell_type":"markdown","source":"## Grouping features for preprocessing purposes","metadata":{"execution":{"iopub.status.busy":"2022-12-13T14:25:35.119692Z","iopub.execute_input":"2022-12-13T14:25:35.120147Z","iopub.status.idle":"2022-12-13T14:25:35.125564Z","shell.execute_reply.started":"2022-12-13T14:25:35.120096Z","shell.execute_reply":"2022-12-13T14:25:35.124281Z"}}},{"cell_type":"markdown","source":"Different data types will require different preprocessing techniques. Here, I will group features for bookkeeping and preprocessing purposes.","metadata":{"execution":{"iopub.status.busy":"2022-12-13T14:26:57.309727Z","iopub.execute_input":"2022-12-13T14:26:57.310159Z","iopub.status.idle":"2022-12-13T14:26:57.317197Z","shell.execute_reply.started":"2022-12-13T14:26:57.310125Z","shell.execute_reply":"2022-12-13T14:26:57.315555Z"}}},{"cell_type":"code","source":"#Group features for preprocessing purpose\ncategorical_features = [feature for feature in train_df.columns if\n                    train_df[feature].dtype == \"object\"] \n\n#from data set description\nnominal_features = [\"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \n                    \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\",\n                    \"CentralAir\", 'Electrical',\"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\nordinal_features = [ 'LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n                    'BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual',\n                    'GarageCond','PavedDrive','PoolQC','Fence']\n\n\n\nnumerical_features = [feature for feature in train_df.columns if feature not in categorical_features + ['SalePrice']]\n\ndiscrete_numerical_features = [ 'OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath',\n 'FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','MoSold', \"MSSubClass\"] \n\ncontinuous_numerical_features = ['LotFrontage','LotArea','YearBuilt','YearRemodAdd','MasVnrArea','BsmtFinSF1','BsmtFinSF2',\n                                 'BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageYrBlt',\n                                 'GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea',\n                                 'MiscVal','YrSold']\n\n#Just checking if any little feature left behind\nassert categorical_features.sort() == (nominal_features + ordinal_features).sort()\nassert numerical_features.sort() == (discrete_numerical_features + continuous_numerical_features).sort()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:37.879451Z","iopub.execute_input":"2022-12-15T16:20:37.880095Z","iopub.status.idle":"2022-12-15T16:20:37.894882Z","shell.execute_reply.started":"2022-12-15T16:20:37.880059Z","shell.execute_reply":"2022-12-15T16:20:37.893358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n# Exploratory Data Analysis","metadata":{"execution":{"iopub.status.busy":"2022-12-13T14:37:01.932505Z","iopub.execute_input":"2022-12-13T14:37:01.9333Z","iopub.status.idle":"2022-12-13T14:37:01.938752Z","shell.execute_reply.started":"2022-12-13T14:37:01.933252Z","shell.execute_reply":"2022-12-13T14:37:01.93745Z"}}},{"cell_type":"markdown","source":"Let's obsorve how target variable changes with features.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(12, 3, figsize=(23, 60))\nfor var, subplot in zip(numerical_features, ax.flatten()):\n    sns.scatterplot(x=var, y='SalePrice',  data=train_df, ax=subplot, hue = 'SalePrice' )\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:37.896435Z","iopub.execute_input":"2022-12-15T16:20:37.896832Z","iopub.status.idle":"2022-12-15T16:20:55.306873Z","shell.execute_reply.started":"2022-12-15T16:20:37.896797Z","shell.execute_reply":"2022-12-15T16:20:55.305888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: \n* Basement, first floor, and second floor areas have a strong correlation with the target\n* Overall quality also has a strong correlation\n* There are some outliers in the data set. Let's drop them.","metadata":{}},{"cell_type":"code","source":"\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea'] > 4000)\n                                  & (train_df['SalePrice'] < 200000)].index)\ntrain_df = train_df.drop(train_df[(train_df['GarageArea'] > 1200)\n                                  & (train_df['SalePrice'] < 300000)].index)\ntrain_df = train_df.drop(train_df[(train_df['TotalBsmtSF'] > 4000)\n                                  & (train_df['SalePrice'] < 200000)].index)\ntrain_df = train_df.drop(train_df[(train_df['1stFlrSF'] > 4000)\n                                  & (train_df['SalePrice'] < 200000)].index)\n\ntrain_df = train_df.drop(train_df[(train_df['TotRmsAbvGrd'] > 12)\n                                  & (train_df['SalePrice'] < 230000)].index)\ny = train_df.SalePrice","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:55.308401Z","iopub.execute_input":"2022-12-15T16:20:55.308915Z","iopub.status.idle":"2022-12-15T16:20:55.333127Z","shell.execute_reply.started":"2022-12-15T16:20:55.30888Z","shell.execute_reply":"2022-12-15T16:20:55.331493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at correlations between features and the target with a more quantitative way..","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:47:38.591649Z","iopub.execute_input":"2022-12-13T15:47:38.592089Z","iopub.status.idle":"2022-12-13T15:47:38.599697Z","shell.execute_reply.started":"2022-12-13T15:47:38.592052Z","shell.execute_reply":"2022-12-13T15:47:38.597884Z"}}},{"cell_type":"code","source":"# Display correlations between numerical features and saleprice on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = train_df.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(17, 17))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False);","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:55.334585Z","iopub.execute_input":"2022-12-15T16:20:55.334978Z","iopub.status.idle":"2022-12-15T16:20:58.970545Z","shell.execute_reply.started":"2022-12-15T16:20:55.334946Z","shell.execute_reply":"2022-12-15T16:20:58.969567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are many correlated features with the target. This is good news...\n\nCorrelation is a great measure to understand the relations between features and target variable. But it only measures linear relationships.\n\nMutual information is another measure, which also capable to measure more diverse relationships.\n\nNote that these two measures only good at univariate analysis and they can not detect multivariate relationships.\n\nLet's check mutual information for numerical features...\n","metadata":{}},{"cell_type":"code","source":"\n# determine the mutual information for numerical features\n#You need to fillna to get results from mutual_info_regression function\nmutual_df = train_df[numerical_features]\n\nmutual_info = mutual_info_regression(mutual_df.fillna(0), y, random_state=1)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = mutual_df.columns\npd.DataFrame(mutual_info.sort_values(ascending=False), columns = [\"Numerical_Feature_MI\"] ).style.background_gradient(\"cool\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:58.97207Z","iopub.execute_input":"2022-12-15T16:20:58.972455Z","iopub.status.idle":"2022-12-15T16:20:59.315312Z","shell.execute_reply.started":"2022-12-15T16:20:58.97242Z","shell.execute_reply":"2022-12-15T16:20:59.314099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mutual information also work with preprocessed categorical variables. Let's see which categorical features has strong relationship with the SalePrice..","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:59:19.753603Z","iopub.execute_input":"2022-12-13T15:59:19.754054Z","iopub.status.idle":"2022-12-13T15:59:19.762166Z","shell.execute_reply.started":"2022-12-13T15:59:19.753997Z","shell.execute_reply":"2022-12-13T15:59:19.760715Z"}}},{"cell_type":"code","source":"mutual_df_categorical = train_df[categorical_features]\n#categorical features must be encoded to get mutual information\nfor colname in mutual_df_categorical:\n    mutual_df_categorical[colname], _ = mutual_df_categorical[colname].factorize()\nmutual_info = mutual_info_regression(mutual_df_categorical.fillna(\"Do_not_have_feature\"), y, random_state=1)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = mutual_df_categorical.columns\npd.DataFrame(mutual_info.sort_values(ascending=False), columns = [\"Categorical_Feature_MI\"] ).style.background_gradient(\"cool\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:59.317735Z","iopub.execute_input":"2022-12-15T16:20:59.318191Z","iopub.status.idle":"2022-12-15T16:20:59.765391Z","shell.execute_reply.started":"2022-12-15T16:20:59.318149Z","shell.execute_reply":"2022-12-15T16:20:59.764666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's observe relationship between categorical variables with most mutual information scores and SalePrice. It could be seen that mean SalePrice is quite different for cotegories included in these features.","metadata":{"execution":{"iopub.status.busy":"2022-12-13T16:03:56.999761Z","iopub.execute_input":"2022-12-13T16:03:57.000452Z","iopub.status.idle":"2022-12-13T16:03:57.009424Z","shell.execute_reply.started":"2022-12-13T16:03:57.000412Z","shell.execute_reply":"2022-12-13T16:03:57.007678Z"}}},{"cell_type":"code","source":"categorical_champions = [\"ExterQual\", \"BsmtQual\", \"KitchenQual\", \"GarageFinish\", \"GarageType\",\"Foundation\"]    \n\n\n\nfig, ax = plt.subplots(3, 2, figsize=(22, 18))\nfor var, subplot in zip(categorical_champions, ax.flatten()):    \n    sns.swarmplot(x=var, y='SalePrice', data=train_df, ax=subplot, palette='Set3')\n    for ax in fig.axes:\n        plt.sca(ax)\n        plt.xticks(rotation=60)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:20:59.766392Z","iopub.execute_input":"2022-12-15T16:20:59.767053Z","iopub.status.idle":"2022-12-15T16:21:15.458994Z","shell.execute_reply.started":"2022-12-15T16:20:59.767007Z","shell.execute_reply":"2022-12-15T16:21:15.457699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also observe numerical features with high mutual information scores.","metadata":{}},{"cell_type":"code","source":"numerical_champions = [\"OverallQual\", \"GrLivArea\", \"YearBuilt\",  \"TotalBsmtSF\",\"GarageArea\", \"GarageCars\"]\n\nfig, ax = plt.subplots(2, 3, figsize=(22, 12))\nfor var, subplot in zip(numerical_champions, ax.flatten()):\n    sns.scatterplot(x=var, y='SalePrice',  data=train_df, ax=subplot, hue = 'SalePrice')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:15.464933Z","iopub.execute_input":"2022-12-15T16:21:15.465397Z","iopub.status.idle":"2022-12-15T16:21:18.335336Z","shell.execute_reply.started":"2022-12-15T16:21:15.465356Z","shell.execute_reply":"2022-12-15T16:21:18.333972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"ML algorithms have their own approaches to exploring multivariate relations within the data. But they are not capable to explore every relation by default. Think about regression models, they are just trying to optimize the coefficients of the features, but not interested in other arithmetical operations between features. Or tree-based models just focus to identify the best splitting points but are not interested in other things. So explicitly defining new features derived from the original features helps a lot. \n\nThere are many additional features suggested by the Kaggle community. I add most of them and also add some new ones. They are listed below.","metadata":{}},{"cell_type":"code","source":"train_df[\"Lack_of_feature_index\"] = train_df[[\"Street\", \"Alley\", \"MasVnrType\", \"GarageType\", \"MiscFeature\",  'BsmtQual',\n                                              'FireplaceQu','PoolQC','Fence']].isnull().sum(axis=1) + (train_df[\"MasVnrType\"] == 'None')+ (train_df[\"CentralAir\"] == 'No')\ntrain_df [\"MiscFeatureExtended\"] = (train_df[\"PoolQC\"].notnull()*1 + train_df[\"MiscFeature\"].notnull()*1+ train_df[\"Fence\"].notnull()*1).astype('int64')\ntrain_df[\"Has_Alley\"] = train_df[\"Alley\"].notnull().astype('int64')\ntrain_df[\"Lot_occupation\"] = train_df[\"GrLivArea\"]  / train_df[\"LotArea\"]\ntrain_df[\"Number_of_floors\"] = (train_df[\"TotalBsmtSF\"] != 0).astype('int64') + (train_df[\"1stFlrSF\"] != 0).astype('int64') + (train_df[\"2ndFlrSF\"] != 0).astype('int64')\ntrain_df['Total_Close_Live_Area'] = train_df['GrLivArea'] + train_df['TotalBsmtSF'] \ntrain_df['Outside_live_area'] =  train_df['WoodDeckSF'] + train_df['OpenPorchSF'] + train_df['EnclosedPorch']+ train_df['3SsnPorch'] + train_df['ScreenPorch']\ntrain_df['Total_usable_area'] = train_df['Total_Close_Live_Area'] + train_df['Outside_live_area']\ntrain_df['Area_Quality_Indicator'] = train_df['Total_usable_area'] * train_df['OverallQual']\ntrain_df['Area_Qual_Cond_Indicator'] = train_df['Total_usable_area'] * train_df['OverallQual']* train_df['OverallCond']\ntrain_df['TotalBath'] = (train_df['FullBath'] + (0.5 * train_df['HalfBath']) + train_df['BsmtFullBath'] + (0.5 * train_df['BsmtHalfBath']))\ntrain_df[\"Has_garage\"] = train_df[\"GarageYrBlt\"].notnull().astype('int64')\ntrain_df['House_Age'] = train_df['YrSold'] - train_df['YearBuilt']\ntrain_df[\"Is_Remodeled\"] = (train_df[\"YearBuilt\"] != train_df[\"YearRemodAdd\"]).astype('int64')\ntrain_df['HasBsmt'] = train_df['BsmtQual'].notnull().astype('int64')\ntrain_df['Quality_conditition'] = train_df['OverallQual']* train_df['OverallCond']\ntrain_df['Quality_conditition_2'] = train_df['OverallQual'] + train_df['OverallCond']\ntrain_df['House_Age2'] = train_df['YrSold'] - train_df['YearRemodAdd']\n\n\nnew_features = list(set(train_df.columns) - set(categorical_features+numerical_features+['SalePrice']))\nnew_continuous_features= [\"Lot_occupation\", 'Total_Close_Live_Area', 'Outside_live_area', 'Total_usable_area','Area_Quality_Indicator', 'House_Age',  'Area_Qual_Cond_Indicator', 'Quality_conditition', 'House_Age2']\nnew_discrete_features = [\"Lack_of_feature_index\", \"MiscFeatureExtended\", \"Has_Alley\", \"Number_of_floors\", \"Has_garage\", \"Is_Remodeled\", 'TotalBath', 'HasBsmt', 'Quality_conditition_2'] \n\nassert new_features.sort() == (new_discrete_features + new_continuous_features).sort()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:18.337101Z","iopub.execute_input":"2022-12-15T16:21:18.337798Z","iopub.status.idle":"2022-12-15T16:21:18.37511Z","shell.execute_reply.started":"2022-12-15T16:21:18.337762Z","shell.execute_reply":"2022-12-15T16:21:18.373962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check new features mutual information scores...","metadata":{}},{"cell_type":"code","source":"mutual_df = train_df[new_features]\n\nmutual_info = mutual_info_regression(mutual_df.fillna(0), y, random_state=1)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = mutual_df.columns\npd.DataFrame(mutual_info.sort_values(ascending=False), columns = [\"New_Feature_MI\"] ).style.background_gradient(\"cool\")","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:18.377468Z","iopub.execute_input":"2022-12-15T16:21:18.377826Z","iopub.status.idle":"2022-12-15T16:21:18.557755Z","shell.execute_reply.started":"2022-12-15T16:21:18.377788Z","shell.execute_reply":"2022-12-15T16:21:18.556467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yeah... Some new features have higher mutual information scores than the original ones. This is definitely good news.","metadata":{}},{"cell_type":"markdown","source":"Let's also observe scatterplots. You will see high correlation between some of the new features and target varaible.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(20, 20))\nfor var, subplot in zip(new_continuous_features, ax.flatten()):\n    sns.scatterplot(x=var, y='SalePrice',  data=train_df, ax=subplot, hue = 'SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:18.55976Z","iopub.execute_input":"2022-12-15T16:21:18.560219Z","iopub.status.idle":"2022-12-15T16:21:23.402253Z","shell.execute_reply.started":"2022-12-15T16:21:18.560174Z","shell.execute_reply":"2022-12-15T16:21:23.400733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also some new categorical features work well. Mean of SalePrice is different for different cateories..","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(20, 15))\nfor var, subplot in zip(new_discrete_features, ax.flatten()):\n    sns.boxplot(x=var, y='SalePrice',  data=train_df, ax=subplot, palette='Set3')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:23.40416Z","iopub.execute_input":"2022-12-15T16:21:23.40475Z","iopub.status.idle":"2022-12-15T16:21:25.498763Z","shell.execute_reply.started":"2022-12-15T16:21:23.404706Z","shell.execute_reply":"2022-12-15T16:21:25.497673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Update feature lists for bookkeeping\ncategorical_features = [feature for feature in train_df.columns if\n                    train_df[feature].dtype == \"object\"]\n\n\nordinal_features = [ 'LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n                    'BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual',\n                    'GarageCond','PavedDrive','PoolQC','Fence']\n\n\n\n\nnominal_features = list(set(categorical_features) - set(ordinal_features)) \n\n\nnumerical_features = list(set(train_df.columns) - set(categorical_features)) ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.500096Z","iopub.execute_input":"2022-12-15T16:21:25.500433Z","iopub.status.idle":"2022-12-15T16:21:25.511131Z","shell.execute_reply.started":"2022-12-15T16:21:25.500403Z","shell.execute_reply":"2022-12-15T16:21:25.50966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Columns will be dropped","metadata":{"execution":{"iopub.status.busy":"2022-11-05T16:49:14.317724Z","iopub.execute_input":"2022-11-05T16:49:14.318276Z","iopub.status.idle":"2022-11-05T16:49:14.350338Z","shell.execute_reply.started":"2022-11-05T16:49:14.318227Z","shell.execute_reply":"2022-11-05T16:49:14.348893Z"}}},{"cell_type":"markdown","source":"In preprocessing stage, I first decided to drop the following features. But it caused a slight score degradation. I decided to keep them. But note that dropping these columns could lead to a slight speed gain.\n\n* No info columns: MoSold, BsmtFinSF2, 3SsnPorch, YrSold, Street, Condition2, PoolQC, Utilities\n\n* Missing columns: PoolQC, MiscFeature, Alley, Fence\n\n* Preprocessing decisions: FireplaceQu, GarageYrBlt, YearBuilt, YearRemodAdd","metadata":{}},{"cell_type":"code","source":"#columns_dropped = [\"MoSold\",\"BsmtFinSF2\",\"3SsnPorch\",\"YrSold\",\"Street\",\"Condition2\",\"PoolQC\",\"Utilities\",\n                   #\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"GarageYrBlt\",\"YearBuilt\",\"YearRemodAdd\"]","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.512699Z","iopub.execute_input":"2022-12-15T16:21:25.513427Z","iopub.status.idle":"2022-12-15T16:21:25.521762Z","shell.execute_reply.started":"2022-12-15T16:21:25.513394Z","shell.execute_reply":"2022-12-15T16:21:25.520816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n# Preprocesing","metadata":{}},{"cell_type":"markdown","source":"Up to now we try to understand the data set and decided to create some new features. Remember that one of the primary goals of this notebook is to handle all preprocessing steps in the pipeline. Pipelines are great for the following reasons:\n\n* They help to avoid any data leakage in stateful transformations which could eventually lead to overfitting. \n* There is no need for manual tracking of preprocessing steps in train and test sets\n* You end up with a ready to the deployment model\n\nHere, I will construct building blocks of preprocessing steps in a pipeline according to the following decisions:\n\n#### Missing data handling:\n\n* Impute missing values according to data description (i.e. fill all missing entries in numerical features with 0, in categorical features with 'Do_not_have_this_feature')\n\n#### Categorical data processing:\n\n* Use ordinal transformation for ordinal categorical features, use one-hot encoding for others\n\n#### Numerical data processing:\n* For linear models and support vector machines scale the features, and apply additional power transformation for skewed features. (power transformation is kept limited with continuous features)\n\n* For tree-based models do not use any scaling since they do not require\n\n\n#### Target varaible processing:\n* For linear models and support vector machines use log transformation for target varaible. (TransformedTargetRegressor is very helpful)\n\n#### Creating new features:\n\n* Add a custom transformer in the pipeline to create new features\n","metadata":{}},{"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n## Tree preprocessor","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:36:05.415069Z","iopub.execute_input":"2022-12-14T06:36:05.415572Z","iopub.status.idle":"2022-12-14T06:36:05.42105Z","shell.execute_reply.started":"2022-12-14T06:36:05.415538Z","shell.execute_reply":"2022-12-14T06:36:05.419704Z"}}},{"cell_type":"markdown","source":"Sklearn has many usefull functions for preprocessing. For missing value imputation and one-hot encoding I will use these built-in functions.\nFor ordinal encoding a little extra work required. ","metadata":{}},{"cell_type":"code","source":"\n# Preprocessing for numerical data\n\nnumerical_transformer = Pipeline(steps=[\n    \n    ('imputer', SimpleImputer(strategy='constant', fill_value = 0))\n])\n# Preprocessing for categorical data\nnominal_transformer = Pipeline(steps=[\n    \n    ('imputer', SimpleImputer(strategy='constant', fill_value = 'Do_not_have_this_feature')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.522973Z","iopub.execute_input":"2022-12-15T16:21:25.523283Z","iopub.status.idle":"2022-12-15T16:21:25.533279Z","shell.execute_reply.started":"2022-12-15T16:21:25.523256Z","shell.execute_reply":"2022-12-15T16:21:25.531907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GarageQual_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nFence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}\nGarageFinish_map = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\nKitchenQual_map = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0}\nGarageCond_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nHeatingQC_map = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0}\nExterQual_map = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0}\nBsmtCond_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nLandSlope_map = {'Gtl': 2, 'Mod': 1, 'Sev': 0}\nExterCond_map = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0}\nBsmtExposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nPavedDrive_map = {'Y': 2, 'P': 1, 'N': 0}\nBsmtQual_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nLotShape_map = {'Reg': 3, 'IR1': 2, 'IR2': 1, 'IR3': 0}\nBsmtFinType2_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nBsmtFinType1_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nFireplaceQu_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nUtilities_map = {\"AllPub\":3, \"NoSewr\":2, \"NoSeWa\":1,  \"ELO\":0}\nFunctional_map = {'Typ': 7,'Min1': 6,'Min2': 5,'Mod': 4,'Maj1': 3,'Maj2': 2, 'Sev': 1 , 'Sal': 0}\nPoolQC_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.534757Z","iopub.execute_input":"2022-12-15T16:21:25.535395Z","iopub.status.idle":"2022-12-15T16:21:25.551068Z","shell.execute_reply.started":"2022-12-15T16:21:25.535363Z","shell.execute_reply":"2022-12-15T16:21:25.549676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_mapping = [{'col': col, 'mapping': globals()[col + '_map']} \n                    for col in ordinal_features]","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.552737Z","iopub.execute_input":"2022-12-15T16:21:25.553261Z","iopub.status.idle":"2022-12-15T16:21:25.563778Z","shell.execute_reply.started":"2022-12-15T16:21:25.553216Z","shell.execute_reply":"2022-12-15T16:21:25.562699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_transformer = ce.OrdinalEncoder(mapping = ordinal_mapping)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.565298Z","iopub.execute_input":"2022-12-15T16:21:25.56566Z","iopub.status.idle":"2022-12-15T16:21:25.574672Z","shell.execute_reply.started":"2022-12-15T16:21:25.565601Z","shell.execute_reply":"2022-12-15T16:21:25.573673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_transformer = Pipeline(steps=[\n    \n\n    ('ordinal_encoder', ce.OrdinalEncoder(mapping = ordinal_mapping))\n])\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.576339Z","iopub.execute_input":"2022-12-15T16:21:25.57688Z","iopub.status.idle":"2022-12-15T16:21:25.586799Z","shell.execute_reply.started":"2022-12-15T16:21:25.576838Z","shell.execute_reply":"2022-12-15T16:21:25.585876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay our building blocks are ready...Let's put them together. ","metadata":{}},{"cell_type":"code","source":"# Bundle preprocessing for tree-based algorithms\ntree_preprocessor = ColumnTransformer(remainder=numerical_transformer,\n    transformers=[\n\n        \n        ('nominal_transformer', nominal_transformer, nominal_features),\n        ('ordinal_transformer', ordinal_transformer, ordinal_features),\n        \n            \n    ])\n\n\nset_config(display=\"diagram\")\ntree_preprocessor","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.588185Z","iopub.execute_input":"2022-12-15T16:21:25.588595Z","iopub.status.idle":"2022-12-15T16:21:25.695975Z","shell.execute_reply.started":"2022-12-15T16:21:25.588554Z","shell.execute_reply":"2022-12-15T16:21:25.694846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n## Linear preprocessor","metadata":{}},{"cell_type":"markdown","source":"For linear models and support vector machines, we need additional preprocessing for numerical data. Scaling numerical features and handling skewness helps both for performance and faster convergence. \n\nAlso, target transformation helps to improve performance. We will do that in modeling by encapsulating our models by TransformedTargetRegressor.","metadata":{}},{"cell_type":"code","source":"# Preprocessing for numerical data\nnumerical_transformer2 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value = 0)),\n    ('Scaller', StandardScaler()),    \n     \n    \n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    \n    ('imputer', SimpleImputer(strategy='constant', fill_value = 'Do_not_have_this_feature')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.69995Z","iopub.execute_input":"2022-12-15T16:21:25.7003Z","iopub.status.idle":"2022-12-15T16:21:25.709235Z","shell.execute_reply.started":"2022-12-15T16:21:25.70027Z","shell.execute_reply":"2022-12-15T16:21:25.708156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since linear models have normality assumptions skewed features hurt their performance(at least in theory). So, we need to handle them. Let's observe the skewness of features.","metadata":{}},{"cell_type":"code","source":"skew_features = train_df.select_dtypes(exclude=['object']).skew().sort_values(ascending=False)\nskew_features = pd.DataFrame({'Skew' : skew_features})\nskew_features.style.background_gradient('rocket')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.710886Z","iopub.execute_input":"2022-12-15T16:21:25.711237Z","iopub.status.idle":"2022-12-15T16:21:25.737284Z","shell.execute_reply.started":"2022-12-15T16:21:25.711208Z","shell.execute_reply":"2022-12-15T16:21:25.736362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Kept limited with continuous features\nskewed_features = [\n'MiscVal',  'PoolArea', 'LotArea', '3SsnPorch', 'LowQualFinSF', 'BsmtFinSF2', 'ScreenPorch',\n 'EnclosedPorch','Lot_occupation','MasVnrArea','OpenPorchSF',\n    'Area_Qual_Cond_Indicator','LotFrontage','WoodDeckSF','Area_Quality_Indicator','Outside_live_area']\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.738334Z","iopub.execute_input":"2022-12-15T16:21:25.738858Z","iopub.status.idle":"2022-12-15T16:21:25.743389Z","shell.execute_reply.started":"2022-12-15T16:21:25.738825Z","shell.execute_reply":"2022-12-15T16:21:25.742664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewness_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value = 0)),\n    ('PowerTransformer', PowerTransformer( method='yeo-johnson', standardize=True)),      \n])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.744722Z","iopub.execute_input":"2022-12-15T16:21:25.745272Z","iopub.status.idle":"2022-12-15T16:21:25.756437Z","shell.execute_reply.started":"2022-12-15T16:21:25.74524Z","shell.execute_reply":"2022-12-15T16:21:25.755268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bundle preprocessing for linear algorithms and SVR\nlinear_preprocessor = ColumnTransformer(remainder=numerical_transformer2,\n    transformers=[\n        ('skewness_transformer', skewness_transformer, skewed_features),\n        ('nominal_transformer', nominal_transformer, nominal_features),\n        ('ordinal_transformer', ordinal_transformer, ordinal_features),\n\n    ])\n\nlinear_preprocessor","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.757656Z","iopub.execute_input":"2022-12-15T16:21:25.758513Z","iopub.status.idle":"2022-12-15T16:21:25.88095Z","shell.execute_reply.started":"2022-12-15T16:21:25.758467Z","shell.execute_reply":"2022-12-15T16:21:25.879809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n## A custom pipeline for Feature Engineering","metadata":{"execution":{"iopub.status.busy":"2022-12-14T07:25:16.287318Z","iopub.execute_input":"2022-12-14T07:25:16.287761Z","iopub.status.idle":"2022-12-14T07:25:16.293027Z","shell.execute_reply.started":"2022-12-14T07:25:16.287726Z","shell.execute_reply":"2022-12-14T07:25:16.291833Z"}}},{"cell_type":"markdown","source":"Sklearn has many built-in functions to create pipelines. However, for problem-specific needs, you may need to create custom ones. Sklearn allows this by inheriting base classes (BaseEstimator, TransformerMixin) or using FunctionTransformer. FunctionTransformer is a more convenient way however it is only suitable for stateless transformations(i.e. transformations that do not depend on any distribution in the data set). \n\nHere, I used the first method. If you are not familiar with custom pipelines please read (https://www.kaggle.com/code/lucabasa/understand-and-use-a-pipeline/notebook)","metadata":{}},{"cell_type":"code","source":"class FeatureCreator1(BaseEstimator, TransformerMixin):\n    def __init__(self, add_attributes=True):\n        \n        self.add_attributes = add_attributes\n        \n    def fit(self, X, y=None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        if self.add_attributes:\n            X_copy = X.copy()\n            X_copy[\"Lack_of_feature_index\"] = X_copy[[\"Street\", \"Alley\", \"MasVnrType\", \"GarageType\", \"MiscFeature\",  'BsmtQual',\n                                              'FireplaceQu','PoolQC','Fence']].isnull().sum(axis=1) + (X_copy[\"MasVnrType\"] == 'None')+ (X_copy[\"CentralAir\"] == 'No')\n            X_copy[\"MiscFeatureExtended\"] = (X_copy[\"PoolQC\"].notnull()*1 + X_copy[\"MiscFeature\"].notnull()*1+ X_copy[\"Fence\"].notnull()*1).astype('int64')\n            X_copy[\"Has_Alley\"] = X_copy[\"Alley\"].notnull().astype('int64')\n            X_copy[\"Lot_occupation\"] = X_copy[\"GrLivArea\"]  / X_copy[\"LotArea\"]\n            X_copy[\"Number_of_floors\"] = (X_copy[\"TotalBsmtSF\"] != 0).astype('int64') + (X_copy[\"1stFlrSF\"] != 0).astype('int64') + (X_copy[\"2ndFlrSF\"] != 0).astype('int64')\n            X_copy['Total_Close_Live_Area'] = X_copy['GrLivArea'] + X_copy['TotalBsmtSF'] \n            X_copy['Outside_live_area'] =  X_copy['WoodDeckSF'] + X_copy['OpenPorchSF'] + X_copy['EnclosedPorch']+ X_copy['3SsnPorch'] + X_copy['ScreenPorch']\n            X_copy['Total_usable_area'] = X_copy['Total_Close_Live_Area'] + X_copy['Outside_live_area']\n            X_copy['Area_Quality_Indicator'] = X_copy['Total_usable_area'] * X_copy['OverallQual']\n            X_copy['Area_Qual_Cond_Indicator'] = X_copy['Total_usable_area'] * X_copy['OverallQual']* X_copy['OverallCond']\n            X_copy['TotalBath'] = (X_copy['FullBath'] + (0.5 * X_copy['HalfBath']) + X_copy['BsmtFullBath'] + (0.5 * X_copy['BsmtHalfBath']))\n            X_copy[\"Has_garage\"] = X_copy[\"GarageYrBlt\"].notnull().astype('int64')\n            X_copy['House_Age'] = X_copy['YrSold'] - X_copy['YearBuilt']\n            X_copy[\"Is_Remodeled\"] = (X_copy[\"YearBuilt\"] != X_copy[\"YearRemodAdd\"]).astype('int64')\n            X_copy['HasBsmt'] = X_copy['BsmtQual'].notnull().astype('int64')\n            X_copy['Quality_conditition'] = X_copy['OverallQual']* X_copy['OverallCond']\n            X_copy['Quality_conditition_2'] = X_copy['OverallQual'] + X_copy['OverallCond']\n            X_copy['House_Age2'] = X_copy['YrSold'] - X_copy['YearRemodAdd']\n            X_copy['Quality_conditition'] = X_copy['OverallQual']* X_copy['OverallCond']\n            X_copy['Quality_conditition_2'] = X_copy['OverallQual'] + X_copy['OverallCond']\n            X_copy['House_Age2'] = X_copy['YrSold'] - X_copy['YearRemodAdd']\n            return X_copy\n        else:\n            return X_copy","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.882477Z","iopub.execute_input":"2022-12-15T16:21:25.882835Z","iopub.status.idle":"2022-12-15T16:21:25.899039Z","shell.execute_reply.started":"2022-12-15T16:21:25.882804Z","shell.execute_reply":"2022-12-15T16:21:25.897825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Creator1 = FeatureCreator1(add_attributes = True)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.900511Z","iopub.execute_input":"2022-12-15T16:21:25.901577Z","iopub.status.idle":"2022-12-15T16:21:25.914838Z","shell.execute_reply.started":"2022-12-15T16:21:25.901528Z","shell.execute_reply":"2022-12-15T16:21:25.913712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"10\"></a><br>\n# Putting pieces together","metadata":{"execution":{"iopub.status.busy":"2022-12-14T07:46:19.09666Z","iopub.execute_input":"2022-12-14T07:46:19.097697Z","iopub.status.idle":"2022-12-14T07:46:19.103035Z","shell.execute_reply.started":"2022-12-14T07:46:19.097646Z","shell.execute_reply":"2022-12-14T07:46:19.101654Z"}}},{"cell_type":"markdown","source":"Okay... We created the necessary pieces for a pipeline. Let's put everything together.\n\n* Some new features specifically count 'NA's. That is why I will put the feature creator on the top of everything(before any imputation).\n* It will be followed by a preprocessor block\n* Finally, we will attach an regression algorithm at the end\n\nLet's create a pipeline and visualize it.","metadata":{"execution":{"iopub.status.busy":"2022-12-14T07:55:16.066063Z","iopub.execute_input":"2022-12-14T07:55:16.06667Z","iopub.status.idle":"2022-12-14T07:55:16.077168Z","shell.execute_reply.started":"2022-12-14T07:55:16.06663Z","shell.execute_reply":"2022-12-14T07:55:16.075429Z"}}},{"cell_type":"code","source":"pipe_xgb = Pipeline(steps=[\n                        ('Creator1', Creator1),\n                       ('tree_preprocessor', tree_preprocessor),\n                      ('regressor1', XGBRegressor(random_state =1)),\n                     ])\npipe_xgb","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:25.916521Z","iopub.execute_input":"2022-12-15T16:21:25.917307Z","iopub.status.idle":"2022-12-15T16:21:26.180731Z","shell.execute_reply.started":"2022-12-15T16:21:25.917261Z","shell.execute_reply":"2022-12-15T16:21:26.179673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For linear models we will have an additional target transformation. I will use TransformedTargetRegressor to encapsulate our pipeline.\nLet's observe an example. ","metadata":{}},{"cell_type":"code","source":"pipe_Lasso = Pipeline(steps=[\n                       ('Creator1', Creator1),\n                       ('linear_preprocessor', linear_preprocessor),\n                      ('regressor2', Lasso()),\n                     ])\n\nTargetTransformedLasso = TransformedTargetRegressor(regressor=pipe_Lasso, func=np.log1p, inverse_func=np.expm1)\nTargetTransformedLasso","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:26.18231Z","iopub.execute_input":"2022-12-15T16:21:26.182835Z","iopub.status.idle":"2022-12-15T16:21:26.709133Z","shell.execute_reply.started":"2022-12-15T16:21:26.182796Z","shell.execute_reply":"2022-12-15T16:21:26.70803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay...We are almost ready to start the modeling. Before moving on we will make first touch with the data that we reserved for the pipeline. I will drop outliers and update the feature list. Any preprocessing step, excluding outlier removal, will be taken care of in pipeline.","metadata":{}},{"cell_type":"code","source":"#Drop outliers\npipe_data = pipe_data.drop(pipe_data[(pipe_data['GrLivArea'] > 4000)\n                                 & (pipe_data['SalePrice'] < 200000)].index)\npipe_data = pipe_data.drop(pipe_data[(pipe_data['GarageArea'] > 1200)\n                                & (pipe_data['SalePrice'] < 300000)].index)\npipe_data = pipe_data.drop(pipe_data[(pipe_data['TotalBsmtSF'] > 4000)\n                               & (pipe_data['SalePrice'] < 200000)].index)\npipe_data = pipe_data.drop(pipe_data[(pipe_data['1stFlrSF'] > 4000)\n                                  & (pipe_data['SalePrice'] < 200000)].index)\n\npipe_data = pipe_data.drop(pipe_data[(pipe_data['TotRmsAbvGrd'] > 12)\n                                  & (pipe_data['SalePrice'] < 230000)].index)\n\ny = pipe_data.SalePrice\npipe_data = pipe_data.drop(\"SalePrice\", axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:26.710349Z","iopub.execute_input":"2022-12-15T16:21:26.710812Z","iopub.status.idle":"2022-12-15T16:21:26.734946Z","shell.execute_reply.started":"2022-12-15T16:21:26.710769Z","shell.execute_reply":"2022-12-15T16:21:26.733968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Update feature lists\ncategorical_features = [feature for feature in pipe_data.columns if\n                    pipe_data[feature].dtype == \"object\"]\n\n\nordinal_features = [ 'LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n                    'BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual',\n                    'GarageCond','PavedDrive','PoolQC','Fence']\n\n\n\n\nnominal_features = list(set(categorical_features) - set(ordinal_features)) \n\n\nnumerical_features = list(set(pipe_data.columns) - set(categorical_features)) ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:26.736233Z","iopub.execute_input":"2022-12-15T16:21:26.736563Z","iopub.status.idle":"2022-12-15T16:21:26.747783Z","shell.execute_reply.started":"2022-12-15T16:21:26.736534Z","shell.execute_reply":"2022-12-15T16:21:26.746474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"11\"></a><br>\n# Modeling and hyperparameter tuning","metadata":{}},{"cell_type":"markdown","source":"I used the Optuna package for hyperparameter tuning. I use single-stage tuning for linear models. For the ensembles, I made some iterations by enlarging or shrinking the hyperparameter space according to findings in previous iterations.\n\nI will just demonstrate one example with elasticnet regressor here. I put the code blocks in the appendix for other algorithms for those who may need them.","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:21:26.757145Z","iopub.execute_input":"2022-12-15T16:21:26.757489Z","iopub.status.idle":"2022-12-15T16:21:26.76367Z","shell.execute_reply.started":"2022-12-15T16:21:26.757458Z","shell.execute_reply":"2022-12-15T16:21:26.762501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"12\"></a><br>\n## Elasticnet example","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:17:43.181378Z","iopub.execute_input":"2022-12-15T12:17:43.181786Z","iopub.status.idle":"2022-12-15T12:17:43.186404Z","shell.execute_reply.started":"2022-12-15T12:17:43.181755Z","shell.execute_reply":"2022-12-15T12:17:43.185252Z"}}},{"cell_type":"code","source":"def objective(trial):\n\n    max_iter = trial.suggest_int(\"max_iter\", 1000, 4000)\n    alpha =  trial.suggest_float(\"alpha\", 1e-4, 1000, log=True) \n    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0, step=0.05)\n    tol =  trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n\n    \n    \n    ElasticNet_regressor = ElasticNet(max_iter=max_iter, alpha=alpha,tol=tol, l1_ratio=l1_ratio, random_state =1)\n   \n    # -- Make a pipeline\n    ElasticNet_pipeline = make_pipeline(Creator1,linear_preprocessor, ElasticNet_regressor)\n    \n    ElasticNet_model = TransformedTargetRegressor(regressor=ElasticNet_pipeline, func=np.log1p, inverse_func=np.expm1)\n    \n    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n    score = cross_val_score(ElasticNet_model, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n    score = score.mean()\n    return score\n\n\nsampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler)\nstudy.optimize(objective, n_trials=100)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-15T16:21:26.765404Z","iopub.execute_input":"2022-12-15T16:21:26.765846Z","iopub.status.idle":"2022-12-15T16:28:45.817156Z","shell.execute_reply.started":"2022-12-15T16:21:26.765804Z","shell.execute_reply":"2022-12-15T16:28:45.81563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I wanted to add some visualizations here.  They could easily generated by Optuna's visualization functions. They are very helpful in understanding the tuning process. ","metadata":{}},{"cell_type":"code","source":"plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:45.819053Z","iopub.execute_input":"2022-12-15T16:28:45.822279Z","iopub.status.idle":"2022-12-15T16:28:46.047671Z","shell.execute_reply.started":"2022-12-15T16:28:45.82222Z","shell.execute_reply":"2022-12-15T16:28:46.046671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"History shows that at the begging of the tunning process there were some oscillations in objective value. But, finally, Optuna succeeded to find a good hyperparameter range, and oscillations settled down. We can conclude that any further iretations will not help a lot.","metadata":{}},{"cell_type":"code","source":"plot_slice(study)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:46.048803Z","iopub.execute_input":"2022-12-15T16:28:46.049118Z","iopub.status.idle":"2022-12-15T16:28:46.345198Z","shell.execute_reply.started":"2022-12-15T16:28:46.04909Z","shell.execute_reply":"2022-12-15T16:28:46.34403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_contour(study)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:46.346517Z","iopub.execute_input":"2022-12-15T16:28:46.346888Z","iopub.status.idle":"2022-12-15T16:28:47.527181Z","shell.execute_reply.started":"2022-12-15T16:28:46.346857Z","shell.execute_reply":"2022-12-15T16:28:47.525952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Slice and contour plots are very useful to understand which parameter works well in which range. It could be observed that lower alpha and tol generally work fine.","metadata":{}},{"cell_type":"code","source":"plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:47.528959Z","iopub.execute_input":"2022-12-15T16:28:47.529369Z","iopub.status.idle":"2022-12-15T16:28:49.935963Z","shell.execute_reply.started":"2022-12-15T16:28:47.529334Z","shell.execute_reply":"2022-12-15T16:28:49.934818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think the most useful plot is the parameter importance plot and its quite self-explanatory.","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:03:52.958811Z","iopub.execute_input":"2022-12-15T12:03:52.959764Z","iopub.status.idle":"2022-12-15T12:03:52.966983Z","shell.execute_reply.started":"2022-12-15T12:03:52.959723Z","shell.execute_reply":"2022-12-15T12:03:52.965483Z"}}},{"cell_type":"markdown","source":"<a id = \"13\"></a><br>\n## All tunned regressors","metadata":{}},{"cell_type":"markdown","source":"Well! I tunned all regressors similar to elasticnet. Let's list tunned versions of them with found hyperparameters. Note that I only tunned the algorithm specific parameters you could also tune other parameters in pipeline. (ex. type of scalling or different imputation methods)\n\nI dropped the feature creator part here. I will add it back in later. (I'm not sure but I feel like feeding regressors from a single featurecreator could help for speed increase. If you have a comment please do not hesitate to share)","metadata":{}},{"cell_type":"code","source":"xgb_tunned = XGBRegressor(n_estimators = 6500,\n                          alpha = 1.7938525031017074e-09,\n                          subsample = 0.3231512729662032,\n                          colsample_bytree = 0.25528017285233484,\n                          max_depth = 5, \n                          min_child_weight = 2, \n                          learning_rate = 0.004828231865923587, \n                          gamma = 0.0026151163125498213,\n                          random_state =1)\n\npipe_xgb = Pipeline(steps=[\n                        \n                       ('tree_preprocessor', tree_preprocessor),\n                       ('regressor1', xgb_tunned),\n                     ])\n\n\n\ngbm_tunned = GradientBoostingRegressor(n_estimators= 5500,\n                                       max_depth=5,\n                                       min_samples_leaf=14,\n                                       learning_rate=0.006328507206504974, \n                                       subsample=0.9170443266552768,\n                                       max_features='sqrt', \n                                       random_state=1)\n\n\npipe_gbm = Pipeline(steps=[\n                       \n                       ('tree_preprocessor', tree_preprocessor),\n                      ('regressor2', gbm_tunned),\n                     ])\n\nlgbm_tunned = LGBMRegressor(n_estimators =7000,\n                            max_depth = 7,\n                            learning_rate =0.002536841439596437,\n                            min_data_in_leaf =22, \n                            subsample= 0.7207500503954922, \n                            max_bin =210 ,\n                            feature_fraction = 0.30010067215105635, \n                            random_state =1,\n                            verbosity= -1)\n\npipe_lgbm = Pipeline(steps=[\n                       \n                       ('tree_preprocessor', tree_preprocessor),\n                      ('regressor3', lgbm_tunned),\n                     ])\n\ncatboost_tunned = CatBoostRegressor(iterations = 4500,\n                                    colsample_bylevel =0.05367479984702603,\n                                    learning_rate = 0.018477566955501026,random_strength = 0.1321272840705348,\n                                    depth = 6,\n                                    l2_leaf_reg = 4,\n                                    boosting_type = 'Plain',\n                                    bootstrap_type = 'Bernoulli', \n                                    subsample = 0.7629052520889268, \n                                    logging_level = 'Silent', \n                                    random_state =1)\n\npipe_catboost = Pipeline(steps=[\n                       ('tree_preprocessor', tree_preprocessor),\n                      ('regressor4', catboost_tunned),\n                     ])\n\n\n\nelasticnet_tunned = ElasticNet(max_iter= 3993,\n                               alpha = 0.0007824887724782356,\n                               l1_ratio= 0.25, \n                               tol = 3.78681184748232e-06, \n                               random_state= 1)\n\npipe_Elasticnet = Pipeline(steps=[\n                       \n                       ('linear_preprocessor', linear_preprocessor),\n                      ('regressor5', elasticnet_tunned),\n                     ])\n\nTargetTransformedElasticnet = TransformedTargetRegressor(regressor=pipe_Elasticnet, func=np.log1p, inverse_func=np.expm1)\n\n\nlasso_tunned = Lasso(max_iter= 2345,\n                     alpha =0.00019885959230548468,\n                     tol = 2.955506894549702e-05, \n                     random_state= 1)\n\npipe_Lasso = Pipeline(steps=[\n                       \n                       ('linear_preprocessor', linear_preprocessor),\n                      ('regressor6', lasso_tunned),\n                     ])\n\n\nTargetTransformedLasso = TransformedTargetRegressor(regressor=pipe_Lasso, func=np.log1p, inverse_func=np.expm1)\n\n\nridge_tunned = Ridge(max_iter= 1537, \n                     alpha = 6.654338887411367,\n                     tol = 8.936831872581897e-05, \n                     random_state= 1)\n\npipe_Ridge = Pipeline(steps=[\n                       \n                   ('linear_preprocessor', linear_preprocessor),\n                      ('regressor7', ridge_tunned),\n                     ])\n\nTargetTransformedRidge = TransformedTargetRegressor(regressor=pipe_Ridge, func=np.log1p, inverse_func=np.expm1)\n\n\nsvr_tunned = SVR(kernel = 'linear',\n                 C = 0.019257948556667938,\n                 epsilon = 0.016935170969518305,\n                 tol = 0.0006210492106739069)\n\npipe_SVR = Pipeline(steps=[\n                       \n                       ('linear_preprocessor', linear_preprocessor),\n                      ('regressor8', svr_tunned),\n                     ])\n\nTargetTransformedSVR = TransformedTargetRegressor(regressor=pipe_SVR, func=np.log1p, inverse_func=np.expm1)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:49.937679Z","iopub.execute_input":"2022-12-15T16:28:49.938291Z","iopub.status.idle":"2022-12-15T16:28:49.95765Z","shell.execute_reply.started":"2022-12-15T16:28:49.938256Z","shell.execute_reply":"2022-12-15T16:28:49.956397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"14\"></a><br>\n# Stacking","metadata":{}},{"cell_type":"markdown","source":"We got our trained and tunned models now. I will stack them and hope for improvement. (I did not include individual regressors' performances here. Stacking has provided about an additional %5 increase with respect to the best performing regressor (XGboost))\n\nI will not go into details about the stacking process since there are plenty of good notebooks about it. I just put a reference picture for those who may be not familiar with it. It just takes out-of-fold predictions from base learners (regressors in our case) and feet it to a meta-learner to make a final prediction. \n\nWhat I have done here is clearly explained in this tutorial  https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html","metadata":{}},{"cell_type":"markdown","source":"![ ]( https://miro.medium.com/max/1400/1*XsTcX5N6FXQW1bGhS9hxJQ.png )\nreference- https://towardsdatascience.com/stacking-made-easy-with-sklearn-e27a0793c92b","metadata":{}},{"cell_type":"markdown","source":"Okay, Let's construct our final pipeline. I tried different algorithms as meta regressor, Lasso was the best. \n\nI used grid search to tune the meta regressor. It takes a reasonable time. So I kept the alpha values quite limited.\n","metadata":{}},{"cell_type":"code","source":"estimators = [\n    (\"pipe_xgb\", pipe_xgb),\n    (\"pipe_gbm\", pipe_gbm),\n    (\"pipe_lgbm\", pipe_lgbm),\n    (\"pipe_catboost\", pipe_catboost),\n    (\"TargetTransformedElasticnet\", TargetTransformedElasticnet),\n    (\"TargetTransformedLasso\", TargetTransformedLasso),\n    (\"TargetTransformedRidge\", TargetTransformedRidge),\n    (\"TargetTransformedSVR\", TargetTransformedSVR)\n]\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:49.959267Z","iopub.execute_input":"2022-12-15T16:28:49.959666Z","iopub.status.idle":"2022-12-15T16:28:49.974465Z","shell.execute_reply.started":"2022-12-15T16:28:49.959575Z","shell.execute_reply":"2022-12-15T16:28:49.973225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=Lasso(alpha = 0.01, random_state =1))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:49.976067Z","iopub.execute_input":"2022-12-15T16:28:49.976446Z","iopub.status.idle":"2022-12-15T16:28:49.9842Z","shell.execute_reply.started":"2022-12-15T16:28:49.976405Z","shell.execute_reply":"2022-12-15T16:28:49.983327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grid_params = {\n\n#'stacking_regressor__final_estimator__alpha': [0.0001, 0.01, 1, 10]\n\n#}\n\n\n#ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar   \n    \n#stack_search = GridSearchCV(final_pipe, param_grid = grid_params,scoring= make_scorer(mean_absolute_error), cv = ss, n_jobs = -1)\n        \n#stack_search.fit(pipe_data, y)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:49.986965Z","iopub.execute_input":"2022-12-15T16:28:49.98772Z","iopub.status.idle":"2022-12-15T16:28:49.994419Z","shell.execute_reply.started":"2022-12-15T16:28:49.987684Z","shell.execute_reply":"2022-12-15T16:28:49.99355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_pipe = Pipeline(steps=[\n                       ('Creator1', Creator1),\n                      ('stacking_regressor', stacking_regressor),\n                     ])\nfinal_pipe","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:28:49.995686Z","iopub.execute_input":"2022-12-15T16:28:49.996852Z","iopub.status.idle":"2022-12-15T16:29:04.119742Z","shell.execute_reply.started":"2022-12-15T16:28:49.996817Z","shell.execute_reply":"2022-12-15T16:29:04.118642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well... Finally, we have our tunned pipeline which is capable of feature engineering, preprocessing and making predictions. \n\nLet's use it to make our submission...","metadata":{}},{"cell_type":"code","source":"stacked_regressor =  final_pipe.fit(pipe_data, y)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:29:04.121278Z","iopub.execute_input":"2022-12-15T16:29:04.121608Z","iopub.status.idle":"2022-12-15T16:35:42.230636Z","shell.execute_reply.started":"2022-12-15T16:29:04.12158Z","shell.execute_reply":"2022-12-15T16:35:42.22892Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test =  stacked_regressor.predict(pipe_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:42.23284Z","iopub.execute_input":"2022-12-15T16:35:42.233627Z","iopub.status.idle":"2022-12-15T16:35:43.988417Z","shell.execute_reply.started":"2022-12-15T16:35:42.233566Z","shell.execute_reply":"2022-12-15T16:35:43.986602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"15\"></a><br>\n# Submission","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'Id': pipe_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:43.990763Z","iopub.execute_input":"2022-12-15T16:35:43.991478Z","iopub.status.idle":"2022-12-15T16:35:44.014253Z","shell.execute_reply.started":"2022-12-15T16:35:43.991422Z","shell.execute_reply":"2022-12-15T16:35:44.012687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.01656Z","iopub.execute_input":"2022-12-15T16:35:44.017572Z","iopub.status.idle":"2022-12-15T16:35:44.037231Z","shell.execute_reply.started":"2022-12-15T16:35:44.017511Z","shell.execute_reply":"2022-12-15T16:35:44.035709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"16\"></a><br>\n# Appendix","metadata":{"execution":{"iopub.status.busy":"2022-12-15T14:43:01.748187Z","iopub.execute_input":"2022-12-15T14:43:01.74859Z","iopub.status.idle":"2022-12-15T14:43:01.753726Z","shell.execute_reply.started":"2022-12-15T14:43:01.748556Z","shell.execute_reply":"2022-12-15T14:43:01.752451Z"}}},{"cell_type":"markdown","source":"<a id = \"17\"></a><br>\n### Notes","metadata":{"execution":{"iopub.status.busy":"2022-12-15T15:51:42.824737Z","iopub.execute_input":"2022-12-15T15:51:42.825928Z","iopub.status.idle":"2022-12-15T15:51:42.829715Z","shell.execute_reply.started":"2022-12-15T15:51:42.825887Z","shell.execute_reply":"2022-12-15T15:51:42.828918Z"}}},{"cell_type":"markdown","source":"Some notes:\n* There is a little reproducibility issue which results in a Â±30 difference in public score. I tried many things to overcome this and made many submissions to identify the issue but fail to manage. I first suspected stackingregressor and then find out that problem is related to ensemble models. Although random_state is assigned they do not reproduce the same results. What I concluded is they could be using secondary seeds.  Any comment would be nice.\n\n* Use power transformations wisely. Unnecessary transformations may lead to the problem stated on this link https://github.com/scikit-learn/scikit-learn/issues/14959\n\n* Approach Kaggle test set as production! Since the data set is relatively small, I decided not to use any test set. It resulted in making many submissions.\n\n* Be satisfied with the performance of base models before moving on stacking phase.\n\n* Optuna failed to work on stacking regressor or I couldn't manage. That is why I decided to use gridsearch. But this may be a wiser approach since iterations take reasonable time for full pipeline in tunning processes.\n","metadata":{}},{"cell_type":"markdown","source":"<a id = \"18\"></a><br>\n### Optuna tuning settings for different models","metadata":{"execution":{"iopub.status.busy":"2022-12-15T15:51:09.222542Z","iopub.status.idle":"2022-12-15T15:51:09.222932Z","shell.execute_reply.started":"2022-12-15T15:51:09.222755Z","shell.execute_reply":"2022-12-15T15:51:09.222773Z"}}},{"cell_type":"code","source":"#def objective(trial):\n\n#    n_estimators1 = trial.suggest_int(\"n_estimators\", 2000, 5000, step=500)\n    # L1 regularization weight.\n#    alpha1 = trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True)\n    # sampling ratio for training data.\n#    subsample1 = trial.suggest_float(\"subsample\", 0.2, 1.0)\n    # sampling according to each tree.\n#    colsample_bytree1 = trial.suggest_float(\"colsample_bytree\", 0.4, 0.6)\n    # maximum depth of the tree, signifies complexity of the tree.\n#    max_depth1 = trial.suggest_int(\"max_depth\", 3, 10, step=2)\n    # minimum child weight, larger the term more conservative the tree.\n#    min_child_weight1 = trial.suggest_int(\"min_child_weight\", 1, 3)\n    # learning rate\n#    learning_rate1 =  trial.suggest_float(\"learning_rate\", 1e-6, 1, log=True)\n    # defines how selective algorithm is.\n#    gamma1 = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n       \n\n#    xgb_regressor = XGBRegressor(n_estimators = n_estimators1,alpha=alpha1,subsample=subsample1,colsample_bytree=colsample_bytree1,\n#                           max_depth=max_depth1,min_child_weight =min_child_weight1,learning_rate=learning_rate1,gamma=gamma1,eval_metric = 'mae',\n#                           random_state =1)\n   \n        \n\n    # -- Make a pipeline\n#    xgb_pipeline = make_pipeline(Creator1,tree_preprocessor, xgb_regressor)\n\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(xgb_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.039735Z","iopub.execute_input":"2022-12-15T16:35:44.040746Z","iopub.status.idle":"2022-12-15T16:35:44.051022Z","shell.execute_reply.started":"2022-12-15T16:35:44.040684Z","shell.execute_reply":"2022-12-15T16:35:44.049524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#second level xgboost\n\n#def objective(trial):\n\n#    n_estimators1 = trial.suggest_int(\"n_estimators\", 4000, 6000, step=500)\n    # L1 regularization weight.\n#    alpha1 = trial.suggest_float(\"alpha\", 1e-8, 1e-5, log=True)\n    # sampling ratio for training data.\n#    subsample1 = trial.suggest_float(\"subsample\", 0.3, 0.5)\n    # sampling according to each tree.\n#    colsample_bytree1 = trial.suggest_float(\"colsample_bytree\", 0.35, 0.56)\n    # maximum depth of the tree, signifies complexity of the tree.\n#    max_depth1 = trial.suggest_int(\"max_depth\", 3, 7, step=2)\n    # minimum child weight, larger the term more conservative the tree.\n#    min_child_weight1 = trial.suggest_int(\"min_child_weight\", 1, 3)\n    # learning rate\n#    learning_rate1 =  trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n    # defines how selective algorithm is.\n#    gamma1 = trial.suggest_float(\"gamma\", 1e-3, 1e-1, log=True)\n       \n\n        \n    \n\n#    xgb_regressor = XGBRegressor(n_estimators = n_estimators1,alpha=alpha1,subsample=subsample1,colsample_bytree=colsample_bytree1,\n#                           max_depth=max_depth1,min_child_weight =min_child_weight1,learning_rate=learning_rate1,gamma=gamma1,eval_metric = 'mae',\n#                           random_state =1)\n   \n        \n\n    # -- Make a pipeline\n#    xgb_pipeline = make_pipeline(Creator1,tree_preprocessor, xgb_regressor)\n\n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(xgb_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.054475Z","iopub.execute_input":"2022-12-15T16:35:44.055639Z","iopub.status.idle":"2022-12-15T16:35:44.068206Z","shell.execute_reply.started":"2022-12-15T16:35:44.055552Z","shell.execute_reply":"2022-12-15T16:35:44.06614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#third level xgboost\n\n#def objective(trial):\n\n#    n_estimators1 = trial.suggest_int(\"n_estimators\", 6000, 7000, step=500)\n    # L1 regularization weight.\n#    alpha1 = trial.suggest_float(\"alpha\", 1e-9, 1e-7, log=True)\n    # sampling ratio for training data.\n#    subsample1 = trial.suggest_float(\"subsample\", 0.3, 0.4)\n    # sampling according to each tree.\n#    colsample_bytree1 = trial.suggest_float(\"colsample_bytree\", 0.25, 0.45)\n    # maximum depth of the tree, signifies complexity of the tree.\n#    max_depth1 = trial.suggest_int(\"max_depth\", 3, 7, step=2)\n    # minimum child weight, larger the term more conservative the tree.\n#    min_child_weight1 = trial.suggest_int(\"min_child_weight\", 1, 3)\n    # learning rate\n#    learning_rate1 =  trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n    # defines how selective algorithm is.\n#    gamma1 = trial.suggest_float(\"gamma\", 1e-3, 1e-1, log=True)\n       \n\n    \n\n#    xgb_regressor = XGBRegressor(n_estimators = n_estimators1,alpha=alpha1,subsample=subsample1,colsample_bytree=colsample_bytree1,\n#                           max_depth=max_depth1,min_child_weight =min_child_weight1,learning_rate=learning_rate1,gamma=gamma1,eval_metric = 'mae',\n#                           random_state =1)\n   \n        \n\n    # -- Make a pipeline\n#    xgb_pipeline = make_pipeline(Creator1,tree_preprocessor, xgb_regressor)\n\n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(xgb_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.071176Z","iopub.execute_input":"2022-12-15T16:35:44.072512Z","iopub.status.idle":"2022-12-15T16:35:44.084589Z","shell.execute_reply.started":"2022-12-15T16:35:44.072415Z","shell.execute_reply":"2022-12-15T16:35:44.082851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first level gbm\n\n#def objective(trial):\n\n\n#    n_estimators = trial.suggest_int(\"n_estimators\", 1500, 5000, step=500)\n#    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 40, step=3)\n#    max_depth = trial.suggest_int(\"max_depth\", 2, 16, step=2)\n#    learning_rate =  trial.suggest_float(\"learning_rate\", 1e-4, 0.1, log=True)\n#    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.05)\n#    max_features = trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])        \n    \n\n#    gbm_regressor = GradientBoostingRegressor(n_estimators = n_estimators,min_samples_leaf=min_samples_leaf,max_depth=max_depth,\n#                                        learning_rate=learning_rate, subsample=subsample, max_features = max_features, random_state =1)\n   \n    # -- Make a pipeline\n#    gbm_pipeline = make_pipeline(Creator1,tree_preprocessor, gbm_regressor)\n\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(gbm_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.087859Z","iopub.execute_input":"2022-12-15T16:35:44.089663Z","iopub.status.idle":"2022-12-15T16:35:44.101793Z","shell.execute_reply.started":"2022-12-15T16:35:44.089567Z","shell.execute_reply":"2022-12-15T16:35:44.099794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#second level gbm\n\n#def objective(trial):\n\n\n#    n_estimators = trial.suggest_int(\"n_estimators\", 5000, 6000, step=500)\n#    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 10, 22, step=2)\n#    max_depth = trial.suggest_int(\"max_depth\", 2, 7, step=1)\n#    learning_rate =  trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n#    subsample = trial.suggest_float(\"subsample\", 0.85, 0.95)\n#    max_features = trial.suggest_categorical(\"max_features\", [ \"sqrt\", \"log2\"])        \n    \n    \n\n#    gbm_regressor = GradientBoostingRegressor(n_estimators = n_estimators,min_samples_leaf=min_samples_leaf,max_depth=max_depth,\n#                                        learning_rate=learning_rate, subsample=subsample, max_features = max_features, random_state =1)\n   \n    # -- Make a pipeline\n#    gbm_pipeline = make_pipeline(Creator1,tree_preprocessor, gbm_regressor)\n\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(gbm_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.105526Z","iopub.execute_input":"2022-12-15T16:35:44.106186Z","iopub.status.idle":"2022-12-15T16:35:44.117026Z","shell.execute_reply.started":"2022-12-15T16:35:44.106149Z","shell.execute_reply":"2022-12-15T16:35:44.115895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#catboost\n\n#def objective(trial):\n\n#    cat_param = {\n#        \"iterations\" : trial.suggest_int(\"iterations\", 4000, 6500, step=500),\n#        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\",  0.02, 0.5),\n#        \"learning_rate\": trial.suggest_float(\"learning_rate\",1e-3, 1e-2, log=True),\n#        \"random_strength\": trial.suggest_float(\"random_strength\",1e-2, 1, log=True),\n        \n#        \"depth\": trial.suggest_int(\"depth\", 2, 12),\n#        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 1, 8),\n\n#        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Plain\"]),\n#        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bernoulli\"])\n#    }\n\n#    if cat_param[\"bootstrap_type\"] == \"Bayesian\":\n#        cat_param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n#    elif cat_param[\"bootstrap_type\"] == \"Bernoulli\":\n#        cat_param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.6, 1)\n    \n\n    \n\n#    catboost_regressor = CatBoostRegressor(**cat_param,random_state =1, logging_level='Silent')\n\n    # -- Make a pipeline\n#    catboost_pipeline = make_pipeline(Creator1,tree_preprocessor, catboost_regressor)\n\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(catboost_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.118354Z","iopub.execute_input":"2022-12-15T16:35:44.119256Z","iopub.status.idle":"2022-12-15T16:35:44.132468Z","shell.execute_reply.started":"2022-12-15T16:35:44.11922Z","shell.execute_reply":"2022-12-15T16:35:44.131401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first level lightgbm\n\n#def objective(trial):\n\n\n#    n_estimators = trial.suggest_int(\"n_estimators\", 1500, 5000, step=500)\n#    max_depth = trial.suggest_int(\"max_depth\", 2, 14, step=2)\n#    learning_rate =  trial.suggest_float(\"learning_rate\", 1e-4, 0.1, log=True)\n#    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 1, 40, step=3)\n#    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.05)        \n#    max_bin = trial.suggest_int(\"max_bin\", 200, 350, step=10),\n#    feature_fraction = trial.suggest_float(\"feature_fraction\", 0.3, 1.0, step=0.1)\n\n\n    \n    \n\n#    lgbm_regressor = LGBMRegressor(n_estimators = n_estimators,max_depth=max_depth,learning_rate=learning_rate, min_data_in_leaf=min_data_in_leaf,\n#                                         subsample=subsample,max_bin=max_bin,feature_fraction=feature_fraction, random_state =1)\n   \n    # -- Make a pipeline\n#    lgbm_pipeline = make_pipeline(Creator1,tree_preprocessor, lgbm_regressor)\n\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(lgbm_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.134049Z","iopub.execute_input":"2022-12-15T16:35:44.134737Z","iopub.status.idle":"2022-12-15T16:35:44.148744Z","shell.execute_reply.started":"2022-12-15T16:35:44.134699Z","shell.execute_reply":"2022-12-15T16:35:44.147656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#second level lightgbm\n\n#def objective(trial):\n\n\n#    n_estimators = trial.suggest_int(\"n_estimators\", 5500, 7000, step=500)\n#    max_depth = trial.suggest_int(\"max_depth\", 4, 12)\n#    learning_rate =  trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n#    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 21, 30)\n#    subsample = trial.suggest_float(\"subsample\", 0.6, 0.88)        \n#    max_bin = trial.suggest_int(\"max_bin\", 190, 230, step=10),\n#    feature_fraction = trial.suggest_float(\"feature_fraction\", 0.3, 0.5)\n\n    \n    \n\n#    lgbm_regressor = LGBMRegressor(n_estimators = n_estimators,max_depth=max_depth,learning_rate=learning_rate, min_data_in_leaf=min_data_in_leaf,\n#                                         subsample=subsample,max_bin=max_bin,feature_fraction=feature_fraction, random_state =1)\n   \n    # -- Make a pipeline\n#    lgbm_pipeline = make_pipeline(Creator1,tree_preprocessor, lgbm_regressor)\n\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(lgbm_pipeline, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.150543Z","iopub.execute_input":"2022-12-15T16:35:44.150944Z","iopub.status.idle":"2022-12-15T16:35:44.165045Z","shell.execute_reply.started":"2022-12-15T16:35:44.150909Z","shell.execute_reply":"2022-12-15T16:35:44.163828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def objective(trial):\n\n#    max_iter = trial.suggest_int(\"max_iter\", 1000, 4000)\n#    alpha =  trial.suggest_float(\"alpha\", 1e-4, 1000, log=True) \n#    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0, step=0.05)\n#    tol =  trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n\n    \n    \n#    ElasticNet_regressor = ElasticNet(max_iter=max_iter, alpha=alpha,tol=tol, l1_ratio=l1_ratio, random_state =1)\n   \n    # -- Make a pipeline\n#    ElasticNet_pipeline = make_pipeline(Creator1,linear_preprocessor, ElasticNet_regressor)\n    \n#    ElasticNet_model = TransformedTargetRegressor(regressor=ElasticNet_pipeline, func=np.log1p, inverse_func=np.expm1)\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(ElasticNet_model, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.166717Z","iopub.execute_input":"2022-12-15T16:35:44.167072Z","iopub.status.idle":"2022-12-15T16:35:44.180928Z","shell.execute_reply.started":"2022-12-15T16:35:44.167039Z","shell.execute_reply":"2022-12-15T16:35:44.179811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def objective(trial):\n\n\n#    max_iter = trial.suggest_int(\"max_iter\", 1000, 4000)\n#    alpha =  trial.suggest_float(\"alpha\", 1e-4, 1000, log=True) \n#    tol =  trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n     \n\n    \n#    Lasso_regressor = Lasso(max_iter=max_iter, alpha=alpha,tol=tol, random_state =1)\n   \n#    # -- Make a pipeline\n#    Lasso_pipeline = make_pipeline(Creator1,linear_preprocessor, Lasso_regressor)\n    \n#    Lasso_model = TransformedTargetRegressor(regressor=Lasso_pipeline, func=np.log1p, inverse_func=np.expm1)\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(Lasso_model, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.182565Z","iopub.execute_input":"2022-12-15T16:35:44.182955Z","iopub.status.idle":"2022-12-15T16:35:44.196553Z","shell.execute_reply.started":"2022-12-15T16:35:44.182922Z","shell.execute_reply":"2022-12-15T16:35:44.195525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def objective(trial):\n\n\n#    max_iter = trial.suggest_int(\"max_iter\", 1000, 4000)\n#    alpha =  trial.suggest_float(\"alpha\", 1e-4, 1000, log=True) \n#    tol =  trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n\n    \n\n    \n    \n#    Ridge_regressor = Ridge(max_iter=max_iter, alpha=alpha, tol= tol, random_state =1)\n   \n    # -- Make a pipeline\n#    Ridge_pipeline = make_pipeline(Creator1,linear_preprocessor, Ridge_regressor)\n    \n#    Ridge_model = TransformedTargetRegressor(regressor=Ridge_pipeline, func=np.log1p, inverse_func=np.expm1)\n    \n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(Ridge_model, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)\n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.198038Z","iopub.execute_input":"2022-12-15T16:35:44.198792Z","iopub.status.idle":"2022-12-15T16:35:44.21289Z","shell.execute_reply.started":"2022-12-15T16:35:44.198754Z","shell.execute_reply":"2022-12-15T16:35:44.211863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first level SVR\n#Be careful higher C values takes too long or fails to converge\n\n#def objective(trial):\n    \n#    param = {\n\n#         \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\",  \"rbf\"]),\n#         \"C\": trial.suggest_float(\"C\", 1, 1000, log=True),\n#         \"epsilon\": trial.suggest_float(\"epsilon\", 1e-3, 0.1, log=True),\n        #\"tol\": trial.suggest_float(\"tol\", 1e-4, 1e-3, log=True)\n        \n#     }\n\n\n#    if param[\"kernel\"] ==  \"rbf\" :\n#         gamma = trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n    \n    \n#    SVR_regressor = SVR(**param, cache_size=800)\n   \n    # -- Make a pipeline\n#    SVR_pipeline = make_pipeline(Creator1,linear_preprocessor, SVR_regressor)\n    \n#    SVR_model = TransformedTargetRegressor(regressor=SVR_pipeline, func=np.log1p, inverse_func=np.expm1)\n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(SVR_model, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)     \n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.214412Z","iopub.execute_input":"2022-12-15T16:35:44.215158Z","iopub.status.idle":"2022-12-15T16:35:44.229057Z","shell.execute_reply.started":"2022-12-15T16:35:44.215113Z","shell.execute_reply":"2022-12-15T16:35:44.22797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#second level SVR\n\n#def objective(trial):\n    \n#    param = {\n\n#         \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\"]),\n#         \"C\": trial.suggest_float(\"C\", 1e-3, 10, log=True),\n#         \"epsilon\": trial.suggest_float(\"epsilon\", 1e-4, 0.1, log=True),\n        #\"tol\": trial.suggest_float(\"tol\", 1e-4, 1e-3, log=True)\n        \n#     }\n\n\n\n\n\n#    SVR_regressor = SVR(**param, cache_size=800)\n   \n    # -- Make a pipeline\n#    SVR_pipeline = make_pipeline(Creator1,linear_preprocessor, SVR_regressor)\n    \n#    SVR_model = TransformedTargetRegressor(regressor=SVR_pipeline, func=np.log1p, inverse_func=np.expm1)\n#    ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #ShuffleSplit will help you to keep distributions of training data and folds similar)\n#    score = cross_val_score(SVR_model, pipe_data, y, scoring= make_scorer(mean_absolute_error),  cv=ss)     \n#    score = score.mean()\n#    return score\n\n\n#sampler = TPESampler(seed=42) # create a seed for the sampler for reproducibility\n#study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n#study.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T16:35:44.230668Z","iopub.execute_input":"2022-12-15T16:35:44.23129Z","iopub.status.idle":"2022-12-15T16:35:44.24578Z","shell.execute_reply.started":"2022-12-15T16:35:44.231244Z","shell.execute_reply":"2022-12-15T16:35:44.244753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### ","metadata":{}}]}