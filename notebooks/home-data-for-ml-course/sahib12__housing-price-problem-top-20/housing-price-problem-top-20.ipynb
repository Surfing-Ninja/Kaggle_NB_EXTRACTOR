{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <div style=\"text-align: center\">LEARN FEATURE ENGINEERING AND FEATURE SELECTION TECHNIQUES </div>\n<div style=\"text-align:center\"><img src=\"https://brainstation-23.com/wp-content/uploads/2018/12/ML-real-state.png\"></div>"},{"metadata":{},"cell_type":"markdown","source":"PC - BRAIN STATION 23"},{"metadata":{},"cell_type":"markdown","source":"## I hope this kernel helpful and some <font color='red'><b>UPVOTES</b></font> would be very much appreciated"},{"metadata":{},"cell_type":"markdown","source":"<a id='top'></a> <br>\n## NOTEBOOK CONTENT\n1. [IMPORTS](#1)\n1. [LOAD DATA](#2)\n1. [DATA SNEAK PEAK](#3)\n    1. [UNIQUE WAY TO SEE MISSING VALUES](#3-1)\n1. [DATA SCIENCE WORKFLOW](#4)\n1. [PROFILE REPORT](#5)\n1. [FEATURE SELECTION TECHNIQUES FOR NUMERICAL VARIABLES](#6)\n1. [DATA CLEANING](#7)\n1. [UNIVARIATE SELECTION](#8)\n1. [FEATURE IMPORTANCE](#9)\n1. [RANDOM FOREST](#10)\n1. [WORKING WITH TEST DATA](#11)\n1. [TO MAKE CSV FILE FOR SUBMISSION](#12)\n1. [CATEGORICAL DATA](#13)\n    1. [HANDLING MISSING CATEGORICAL DATA](#13-1)\n1. [DATA VISUALISATION FOR CATEGORICAL VARIABLES](#14)\n1. [One Hot Encoding Categorical Columns](#15)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## 1-IMPORTS"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# for data visualzation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pandas_profiling # LIBRARY TO see all the details of data\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 2-LOAD DATA"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\ntest_data=pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\n# test_id=test_data.pop('Id')\ntest_data.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3></a><br>\n## 3-DATA SNEAK PEAK"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.dtypes.unique())\nprint(len(list(train_data.columns))) # we have total 81 columns with 1 target column and 80 variables\n# train_data.columns\n# train_id=train_data.pop('Id') # since Id iss not going to be used in Prediction so etter to pop it\n\nnum_col=train_data.select_dtypes(exclude='object')\ncat_col=train_data.select_dtypes(exclude=['int64','float64'])\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 100)\nnum_col.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3-1></a> <br>\n## 3.1-UNIQUE TECHNIQUE TO SEE MISSING VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# HEATMAP TO SEE MISSING VALUES\nplt.figure(figsize=(15,5))\nsns.heatmap(num_col.isnull(),yticklabels=0,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the heatmap shows **LotFrontage**,**MasVnrArea**,**GarageYrBlt** have the missing values"},{"metadata":{},"cell_type":"markdown","source":"<a id=4></a> <br>\n## 4-DATA SCIENCE WORKFLOW\n\nThere is no template for solving a data science problem. The roadmap changes with every new dataset and new problem. But we do see similar steps in many different projects. I wanted to make a clean workflow to serve as an example to aspiring data scientists. \n\n<img src=\"https://miro.medium.com/max/2000/1*3FQbrDoP1w1oibNPj9YeDw.png\"> \n\n### Overview:\n\n- Source the Data \n- Data Processing\n- Modeling\n- Model Deployment\n- Model Monitoring \n- Exploration and reporting\n\n## In this notebook we will focus mainly on <font color='red'><b>STEP-2(DATA CLEANING)</b></font>\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=5></a> <br>\n## 5-PROFILE REPORT  (PERSONAL FAV.)"},{"metadata":{},"cell_type":"markdown","source":"<font color='red'>WARNING</font> The Implementation of this cell takes resources"},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_col.profile_report() # this will show profile report only for numerical variables because we are using dataframe having only numerical variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=6></a> <br>\n# 6-FEATURE SELECTION TECHNIQUES FOR NUMERICAL VARIABLES\n### 1-UNIVARIATE SELECTION\n### 2-FEATURE IMPORTANCE\n### 3-CORRELATION MATRIX\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=7></a> <br>\n## 7-DATA CLEANING"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=num_col.copy() #  all numerical variables\ny=X.pop('SalePrice') # storing target variable in y\nX.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isna().sum().reset_index() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very Important Link\n[replace pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html)"},{"metadata":{},"cell_type":"markdown","source":"1.**MasVnrArea**"},{"metadata":{},"cell_type":"markdown","source":"Delaing with **MasVnrArea**\n    So MasVnrArea is  **Masonry veneer area** of house so we can take mean of **MasVnrArea** and replace all nun values of column\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(20,2))\nsns.heatmap(X.corr().iloc[7:8,:], annot=True, linewidths=.8, fmt= '.1f',ax=ax)\n# this shows that MasVnrArea is not highly corelated to any other feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(X.MasVnrArea,Label='MasVnrArea',color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This and profilereport above shows that most of the values (nearly 60%) values of MasVnrArea have **zero** value so replace nan values here with **ZERO**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.MasVnrArea.replace({np.nan:0},inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. **GarageYrBlt**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,2))\nsns.heatmap(X.corr().iloc[24:25,:], annot=True, linewidths=.8, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that **GarageYrBlt** is highly corelated to **YearBuilt**\n\nAnd One Important Reason that we cant drop **GarageYrBlt** because it has significant corelation with our predicctor variable **SalePrice** "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(X.GarageYrBlt,Label='GarageYrBlt',color='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.GarageYrBlt.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the Kdeplot for **GarageYrBlt** and Description we find that data in this column is not spread enough so we can use **mean** of this column to fill its Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['GarageYrBlt'].replace({np.nan:X.GarageYrBlt.mean()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. **LotFrontage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,2))\nsns.heatmap(X.corr().iloc[1:2,:], annot=True, linewidths=.8, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LotFrontage**: Linear feet of street connected to property\n\nAnd One Important Reason that we cant drop **LotFrontage** because it has significant corelation with our predicctor variable **SalePrice** \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(X.LotFrontage,Label='LotFrontage',color='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.LotFrontage.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the Kde Plot and Description  of **LotFrontage** we can replace Nan Values of this column either by **Mean** or **Median** Because data is almost Normal distribution\n\nI would Choose Mean to replace NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['LotFrontage'].replace({np.nan:X.LotFrontage.mean()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we are done with **data cleaning** part "},{"metadata":{},"cell_type":"markdown","source":"<a id=8></a> <br>\n## 8-UNIVARIATE SELECTION\n**CHI2** Test for Univariate Selection\n### 1. CHI2 Test only applies for Positive Value\n### 2. CHI2 should only be applied to columns that do not have any NAN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest # SELECT K  BEST  is used to first top k features from variables list\nfrom sklearn.feature_selection import chi2 # import chi1 function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # apply SelectKBest class to extract top 30 best features\nbestfeatures = SelectKBest(score_func=chi2, k=30)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(30,'Score'))  #print  TOP 30 best features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=9></a> <br>\n## 9-FEATURE IMPORTANCE\n### Extra Tree Classifier also uses positive values and is not applicable for NaN,Infinite values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='barh',figsize=(30,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfeatures_tree=set(list(feat_importances.nlargest(30).index)) # top 35 features by tree classifier\nfeatures_chi=set(list(featureScores.Specs[:30]))# top 30 features by chi square test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selcting Union of Features from both the ways"},{"metadata":{"trusted":true},"cell_type":"code","source":"union_fe=features_chi.union(features_tree)\nunion_fe=list(union_fe)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=X[union_fe] # SELCTING TOP fEATURES FROM FEATURE SET\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=10></a> <br>\n## 10-RANDOM FOREST\nRandom Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.model_selection import train_test_split as tt\nfrom sklearn.ensemble import RandomForestRegressor as rr\ntrain_X,val_X,train_Y,val_Y=tt(X,y,random_state=23)\nforest_model=rr(random_state=12,max_depth=9,n_estimators=200)\nforest_model.fit(X,y)\n\nprediction=forest_model.predict(val_X)\nprint(mae(val_Y,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=11></a> <br>\n## 11-WORKING WITH TEST DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X=test_data.select_dtypes(exclude=['object']) # way to select all numerical variables\ntest_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## HANDLE MISSING VALUES OF <font color='red'>Test Data</font>"},{"metadata":{},"cell_type":"markdown","source":"1. <font color='red'>**LotFrontage**</font>\n\nUse the same criteria that we used to handle missing values of <font color='red'>LotFrontage</font> in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['LotFrontage'].replace({np.nan:test_X.LotFrontage.mean()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. <font color='red'>**MasVnrArea**</font>\n\nUse the same criteria that we used to handle missing values of <font color='red'>MasVnrArea</font> in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.MasVnrArea.replace({np.nan:0},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. <font color='red'>**GarageYrBlt**</font>\n\nUse the same criteria that we used to handle missing values of <font color='red'>GarageYrBlt</font> in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['GarageYrBlt'].replace({np.nan:test_X.GarageYrBlt.mean()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. <font color='red'>**TotalBsmtSF**</font>\n\nWe have only 1 missing values in this column so just replace it with mean of the column\nAlthough data is varying alot but still the median and mean of data are nearly same so i chosse mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(test_X.TotalBsmtSF,label='TotalBsmtSF')\nprint(test_X.TotalBsmtSF.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['TotalBsmtSF'].replace({np.nan:test_X.TotalBsmtSF.mean()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n5. <font color='red'> **BsmtFinSF1**</font>\n\nWe can see from description of this column that there is lot of gap between 75th percentile and max value so that is the reason why mean is so high even 25 percentile is equal to zero.\nSo better to use median to replace NaN values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(test_X.BsmtFinSF1,label='BsmtFinSF1')\nprint(test_X.BsmtFinSF1.describe())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['BsmtFinSF1'].replace({np.nan:test_X.BsmtFinSF1.median()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. <font color='red'>**BsmtFinSF2**</font>\n\nAs we see 75 percent of the value are zero so better to replace missing value with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(test_X.BsmtFinSF2,label='BsmtFinSF2')\nprint(test_X.BsmtFinSF2.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['BsmtFinSF2'].replace({np.nan:0},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. <font color='red'>**GarageArea**</font>\n\nWe have only 1 missing values in this column so just replace it with mean of the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(test_X.GarageArea,label='GarageArea')\nprint(test_X.GarageArea.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['GarageArea'].replace({np.nan:test_X.GarageArea.mean()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8. <font color='red'>**BsmtUnfSF**</font>\n\nAs it is highly varied data so to use median to replace NaN value"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(test_X.BsmtUnfSF,label='BsmtUnfSF')\nprint(test_X.BsmtUnfSF.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BsmtUnfSF\ntest_X['BsmtUnfSF'].replace({np.nan:test_X.BsmtUnfSF.median()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"9. <font color='red'>**BsmtFullBath**</font>\n\n75 percentile of values are 0 so replace NaN with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(test_X.BsmtFullBath,label='BsmtUnfSF')\nprint(test_X.BsmtFullBath.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['BsmtFullBath'].replace({np.nan:0},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"10. <font color='red'>**BsmtHalfBath**</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# BsmtHalfBath\nsns.kdeplot(test_X.BsmtHalfBath,label='BsmtHalfBath')\nprint(test_X.BsmtHalfBath.describe())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['BsmtHalfBath'].replace({np.nan:0},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"11. <font color='red'>**GarageCars**</font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(test_X.GarageCars,label='BsmtHalfBath')\nprint(test_X.GarageCars.describe())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X['GarageCars'].replace({np.nan:test_X.GarageCars.median()},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_X.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning for test data done"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(test_X.corr(), annot=True, linewidths=.8, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_X=test_X[union_fe]\n# make predictions which we will submit. \ntest_preds = forest_model.predict(test_X)\ntest_preds.shape\n# The lines below shows how to save predictions in format used for competition scoring\n# Just uncomment them.\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                      'SalePrice': test_preds})\noutput.to_csv('submission12.csv', index=False)\n# test_X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=12></a> <br>\n## 12-TO MAKE CSV FILE FOR SUBMISSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = output\n\n# create a link to download the dataframe\ncreate_download_link(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=13></a> <br>\n## 13-CATEGORICAL DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n\ncat_col_name=[cname for cname in cat_col.columns if cat_col[cname].nunique() < 10 and \n                        cat_col[cname].dtype == \"object\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_col_name=list(X.columns)\nnum_col_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=cat_col_name+num_col_name\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train_data[features]\ntrain_data=pd.get_dummies(train_data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=13-1></a> <br>\n## 13.1-Handling Missing  Values in Categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#idxmax() function returns index of first occurrence of maximum over requested axis.\n#While finding the index of the maximum value across any index, all NA/null values are excluded.\n\n#These columns has only few value missing as compared to total number so i choose to replace the NaN values with the most frequent values\n\n#MasVnrType\ncat_col['MasVnrType'].replace({np.nan:cat_col.MasVnrType.value_counts().idxmax()},inplace=True)\n\n#BsmtQual\ncat_col['BsmtQual'].replace({np.nan:cat_col.BsmtQual.value_counts().idxmax()},inplace=True)\n\n#BsmtCond\ncat_col['BsmtCond'].replace({np.nan:cat_col.BsmtCond.value_counts().idxmax()},inplace=True)\n\n#BsmtExposure\ncat_col['BsmtExposure'].replace({np.nan:cat_col.BsmtExposure.value_counts().idxmax()},inplace=True)\n\n#BsmtFinType1\ncat_col['BsmtFinType1'].replace({np.nan:cat_col.BsmtFinType1.value_counts().idxmax()},inplace=True)\n\n#BsmtFinType2\ncat_col['BsmtFinType2'].replace({np.nan:cat_col.BsmtFinType2.value_counts().idxmax()},inplace=True)\n\n#Electrical\ncat_col['Electrical'].replace({np.nan:cat_col.Electrical.value_counts().idxmax()},inplace=True)\n\n#GarageType\ncat_col['GarageType'].replace({np.nan:cat_col.GarageType.value_counts().idxmax()},inplace=True)\n\n#GarageFinish\ncat_col['GarageFinish'].replace({np.nan:cat_col.GarageFinish.value_counts().idxmax()},inplace=True)\n\n#GarageQual\ncat_col['GarageQual'].replace({np.nan:cat_col.GarageQual.value_counts().idxmax()},inplace=True)\n\n#GarageCond\ncat_col['GarageCond'].replace({np.nan:cat_col.GarageCond.value_counts().idxmax()},inplace=True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now filling Missing Value for FireplaceQu column\nprint(cat_col.FireplaceQu.value_counts())\n# and Missing Values in this column are 690 so we will Replace nan with 'Unknown'\ncat_col.FireplaceQu.replace({np.nan:'Unknown'},inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So we have finally filled all the Missing Values in Categorical columns"},{"metadata":{},"cell_type":"markdown","source":"<a id=14></a> <br>\n## 14-DATA VISUALISATION FOR CATEGORICAL VARIABLES"},{"metadata":{},"cell_type":"markdown","source":"## Street vs SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2,figsize=(12, 8))\ng = sns.swarmplot(x=cat_col.Street,y=y,ax=axes[0]) # y is Saleprice\ng = g.set_ylabel(\"Sale Price for Diffrent Streets\")\n\nlabels=['Pave','Grvl']\nslices=[cat_col.loc[cat_col.Street==\"Pave\"].shape[0],cat_col.loc[cat_col.Street==\"Grvl\"].shape[0]]\nplt.pie(slices,labels=labels,startangle=90,shadow=1,explode=(0.5,0.7),autopct='%1.2f%%',colors=['#99ff99','#ffcc99'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This clearly shows that **Pave** street has more Saleprices as compared to **Grvl**\nand very Interesting thing \nMost of the people(99.59%) prefer **Pave** Street Access as compared to **Grvl**\nand in **Pave** section also most of the Houses cost under **400,000 $**"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_col=list(cat_col.columns)\ndict_of_col={i:list_of_col[i] for i in range(len(list_of_col))}\ndict_of_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation of First 5 Variables with **SalePrice**"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_list=list(i for i in range(5))\nf,axes=plt.subplots(1, 5,figsize=(20,8))\nf.subplots_adjust(hspace=0.5)\n\nfor j in temp_list:\n        g = sns.swarmplot(x=cat_col[dict_of_col[j]],y=y,ax=axes[j]) # y is Saleprice\n        g.set_title(label=dict_of_col[j].upper(),fontsize=20)\n        g.set_xlabel(str(dict_of_col[j]),fontsize=25)\n        g.set_ylabel('SalePrice',fontsize=25)\n        plt.tight_layout() # to increase gapping between subplots\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. RL is the most choosen street zone from all 5 zones and has  the maximum prices\n1. Pave is Highly Preferred Street access than Grvl\n1. IR1 and Reg are highly preferred LotShape"},{"metadata":{},"cell_type":"markdown","source":"Observing next 2 **variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes=plt.subplots(1, 2,figsize=(15,8))\nf.subplots_adjust(hspace=0.5)\nfor j,i in zip(temp_list,range(5,7)):\n        g = sns.swarmplot(x=cat_col[dict_of_col[i]],y=y,ax=axes[j]) # y is Saleprice\n        g.set_title(label=dict_of_col[i].upper(),fontsize=20)\n        g.set_xlabel(str(dict_of_col[i]),fontsize=25)\n        g.set_ylabel('SalePrice',fontsize=25)\n        plt.tight_layout() # to increase gapping between subplots\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly we can observe each **Categorical variables**"},{"metadata":{},"cell_type":"markdown","source":"## Next we will look how to Handle and use these categorical variables in our model"},{"metadata":{},"cell_type":"markdown","source":"Looking at all the **diffrent** values in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat_col.columns:\n    print(i,'\\t',cat_col[str(i)].unique(),'\\n')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder as le,OneHotEncoder as oe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding one Column with Sklearn "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nst_le=le()\nstreet_labels=st_le.fit_transform(cat_col.Street) # meaning of fit_transform is that it will asign appropriate numbers for each categorical variables\nstreet_mapping={index:label for index,label in enumerate(st_le.classes_)}\nstreet_mapping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_col=cat_col.copy() # doing our work on a copy of datframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_col['Street_Labels']=street_labels\ntemp_col.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Hot encoding a Column ith Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"st_oe=oe(handle_unknown='ignore')\nst_feature_arr=st_oe.fit_transform(temp_col[['Street_Labels']]).toarray()\nst_feature_labels=list(st_le.classes_)\nst_features=pd.DataFrame(st_feature_arr,columns=st_feature_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_X_train=pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\ntemp_X_test=pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. Notice that the 'Condition2' column in the validation data contains the values 'RRAn' and 'RRNn', but these don't appear in the training data -- thus, if we try to use a label encoder with scikit-learn, the code will throw an error.\n\nThis is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue. For instance, you can write a custom label encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns.\n\nRun the code cell below to save the problematic columns to a Python list bad_label_cols. Likewise, columns that can be safely label encoded are stored in good_label_cols.\n\n**Bad_Label_cols** are those columns that the values are not the same between the 2 dataset. In this case the **training** and **testing**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All categorical columns\nobject_cols = [col for col in temp_X_train.columns if temp_X_train[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(temp_X_train[col]) == set(temp_X_test[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So finally we will use **cat_col** dataframe and **good_label_cols** for one hot encoding and later will be used for prediction\nFirst of all we will remove **Alley,PoolQC,Fence,MiscFeature** from good labels list because these columns have more than 80% values as **NaN**\nand then we will use **good_label_cols** as list of features  for cat_col dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"good_label_cols=list(set(good_label_cols)-set(['Alley','PoolQC','Fence','MiscFeature']))\nprint(len(cat_col.columns))\ncat_col=cat_col[good_label_cols]\nprint(len(cat_col.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n[GO to top](#top)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}