{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"As mentioned by **@ambrosm** in this [discussion thread](https://www.kaggle.com/competitions/hull-tactical-market-prediction/discussion/608088) (a must-read), I also experimented with cross-validation on these models. The R² scores I got were:  \n\n- **CV R²:** -0.1777 , -0.002568\n\nYou may want to try running CV on the notebooks you’ve been submitting to the leaderboard.  \n\nThere’s a reason the hosts specifically noted that **leaderboard standings are not meaningful at this stage**. Don’t fall for it : focus instead on building models that generalize well.  \n","metadata":{}},{"cell_type":"markdown","source":"This is the highest scoring / voted notebook : https://www.kaggle.com/code/noorchauhan/hull-market-prediction-just-improved","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport polars as pl\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom dataclasses import dataclass, field\nimport kaggle_evaluation.default_inference_server\nimport time\n\n# ============ PATHS ============\nDATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n# ============ MODEL CONFIGS ============\n@dataclass\nclass ModelParameters:\n    enet_alpha: float = 0.01\n    enet_l1_ratio: float = 0.5\n    xgb_n_estimators: int = 350\n    xgb_max_depth: int = 8\n    xgb_learning_rate: float = 0.05\n    lgb_n_estimators: int = 200\n    lgb_max_depth: int = 8\n    lgb_learning_rate: float = 0.05\n    ensemble_weights: dict = field(default_factory=lambda: {'enet': 0.45, 'xgb': 0.55, 'lgb': 0.45})\n    vol_window: int = 20\n    signal_multiplier_low_vol: float = 600.0  # For low V1\n    signal_multiplier_high_vol: float = 400.0  # For high V1\n    min_signal: float = 0.0\n    max_signal: float = 2.0\n    vol_scaling: float = 1.2\n    retrain_freq: int = 1  # Retrain every row\n    missing_threshold: float = 0.5\n    max_train_rows: int = 800  # Reduced\n\n# Initialize parameters\nparams = ModelParameters()\n\n# ============ DATA LOADING AND PREPROCESSING ============\ndef load_trainset() -> pl.DataFrame:\n    df = (\n        pl.read_csv(DATA_PATH / \"train.csv\")\n        .rename({'market_forward_excess_returns': 'target'})\n        .with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False))\n        .filter(pl.col('date_id') >= 37)\n        .tail(params.max_train_rows)\n    )\n    missing_counts = {col: df[col].is_null().mean() for col in df.columns}\n    feature_cols = [\n        col for col, miss_rate in missing_counts.items() \n        if miss_rate <= params.missing_threshold and col not in ['date_id', 'target']\n    ]\n    keep_cols = ['date_id', 'target'] + feature_cols\n    if len(keep_cols) != len(set(keep_cols)):\n        raise ValueError(f\"Duplicate columns in keep_cols: {keep_cols}\")\n    return df.select(keep_cols)\n\ndef load_testset() -> pl.DataFrame:\n    df = (\n        pl.read_csv(DATA_PATH / \"test.csv\")\n        .with_columns(pl.exclude('date_id', 'is_scored').cast(pl.Float64, strict=False))\n    )\n    train_cols = load_trainset().columns\n    feature_cols = [col for col in train_cols if col not in ['date_id', 'target']]\n    return df.select(['date_id', 'is_scored', 'lagged_market_forward_excess_returns'] + feature_cols)\n\ndef create_features(df: pl.DataFrame, is_train: bool = False, median_values: dict = None) -> pl.DataFrame:\n    feature_prefixes = ['D', 'E', 'I', 'M', 'P', 'S', 'V']\n    base_features = [col for col in df.columns if any(col.startswith(prefix) for prefix in feature_prefixes)]\n    \n    # Derived features\n    required_cols = ['I1', 'I2', 'I7', 'I9', 'M11']\n    if all(col in base_features for col in required_cols):\n        df = df.with_columns(\n            (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n            (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n        )\n    else:\n        df = df.with_columns(\n            pl.lit(0.0).alias(\"U1\"),\n            pl.lit(0.0).alias(\"U2\")\n        )\n    \n    # Interaction features\n    if 'V1' in base_features and 'S1' in base_features:\n        df = df.with_columns((pl.col(\"V1\") * pl.col(\"S1\")).alias(\"V1_S1\"))\n    if 'M11' in base_features and 'V1' in base_features:\n        df = df.with_columns((pl.col(\"M11\") * pl.col(\"V1\")).alias(\"M11_V1\"))\n    if 'I9' in base_features and 'S1' in base_features:\n        df = df.with_columns((pl.col(\"I9\") * pl.col(\"S1\")).alias(\"I9_S1\"))\n    \n    # Training-only features\n    if is_train:\n        if 'S1' in base_features:\n            df = df.with_columns(pl.col(\"S1\").shift(1).alias(\"S1_lag1\"))\n        if 'P1' in base_features:\n            df = df.with_columns(pl.col(\"P1\").shift(1).alias(\"P1_lag1\"))\n        if 'I9' in base_features:\n            df = df.with_columns(pl.col(\"I9\").shift(1).alias(\"I9_lag1\"))\n        if 'V1' in base_features:\n            df = df.with_columns(pl.col(\"V1\").rolling_mean(window_size=5).alias(\"V1_roll_mean_5\"))\n        if 'target' in df.columns:\n            df = df.with_columns(pl.col(\"target\").rolling_std(window_size=5).alias(\"target_roll_std_5\"))\n    \n    # Impute missing values\n    for col in base_features:\n        if col.startswith('I'):\n            df = df.with_columns(pl.col(col).fill_null(pl.col(col).forward_fill()).fill_null(pl.col(col).backward_fill()))\n        median = median_values.get(col, df[col].median()) if median_values else df[col].median()\n        df = df.with_columns(pl.col(col).fill_null(median if median is not None else 0.0))\n    \n    # Impute derived and additional features\n    derived_features = [\"U1\", \"U2\", \"V1_S1\", \"M11_V1\", \"I9_S1\"]\n    additional_features = [\"S1_lag1\", \"P1_lag1\", \"I9_lag1\", \"V1_roll_mean_5\", \"target_roll_std_5\"] if is_train else []\n    for col in derived_features + additional_features:\n        if col in df.columns:\n            median = median_values.get(col, df[col].median()) if median_values else df[col].median()\n            df = df.with_columns(pl.col(col).fill_null(median if median is not None else 0.0))\n    \n    # Feature list (exclude training-only features)\n    features = base_features + derived_features\n    select_cols = [\"date_id\"] + features + ([\"target\"] if is_train else [])\n    return df.select(select_cols)\n\n# ============ MODEL TRAINING ============\nstart_time = time.time()\ntrain = load_trainset()\ntrain = create_features(train, is_train=True)\nfeatures = [col for col in train.columns if col not in ['date_id', 'target', 'S1_lag1', 'P1_lag1', 'I9_lag1', 'V1_roll_mean_5', 'target_roll_std_5']]\n\n# Cache median values for imputation (only for features used in model)\nmedian_values = {col: train[col].median() if col in train.columns and train[col].is_null().mean() < 1.0 else 0.0 for col in features}\n\n# Check for NaNs\nX_train = train.select(features).to_pandas()\nif X_train.isna().any().any():\n    raise ValueError(f\"NaNs found in X_train for columns: {X_train.columns[X_train.isna().any()].tolist()}\")\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\ny_train = train['target'].to_pandas()\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nenet_model = ElasticNet(alpha=params.enet_alpha, l1_ratio=params.enet_l1_ratio, max_iter=1000000)\nenet_score = cross_val_score(enet_model, X_train, y_train).mean() # R2 score\nprint('CV R2 score:', enet_score)\nenet_model.fit(X_train, y_train)\nprint(enet_model.coef_)\n\n\n# Train XGBoost\nxgb_model = xgb.XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=params.xgb_n_estimators,\n    max_depth=params.xgb_max_depth,\n    learning_rate=params.xgb_learning_rate,\n    random_state=42\n)\n\nxgb_score = cross_val_score(xgb_model, X_train, y_train).mean() # R2 score\nprint('CV R2 score:', xgb_score)\nxgb_model.fit(X_train, y_train)\nxgb_model.fit(X_train, y_train)\n\n# Train LightGBM\nlgb_model = lgb.LGBMRegressor(\n    objective='regression',\n    n_estimators=params.lgb_n_estimators,\n    max_depth=params.lgb_max_depth,\n    learning_rate=params.lgb_learning_rate,\n    random_state=42\n)\n\nlgb_score = cross_val_score(lgb_model, X_train, y_train).mean() # R2 score\nprint('CV R2 score:', lgb_score)\nlgb_model.fit(X_train, y_train)\nlgb_model.fit(X_train, y_train)\n\n# Check startup time\nif time.time() - start_time > 900:\n    raise RuntimeError(\"Startup time exceeded 900 seconds\")\n\n# State for online learning\nprevious_lagged = None\ntest_row_count = 0\nlast_allocation = 0.0\nv1_median = train['V1'].median() if 'V1' in train.columns else 0.0\n\n# ============ VOLATILITY ESTIMATION ============\ndef estimate_volatility(test: pl.DataFrame, train: pl.DataFrame) -> float:\n    vol = test['V1'][0] if 'V1' in test.columns else (train['target'].tail(params.vol_window).std() or 0.01)\n    recent_returns = train['target'].tail(params.vol_window).to_numpy()\n    if len(recent_returns) > 1:\n        garch_vol = np.sqrt(0.3 * np.var(recent_returns) + 0.7 * vol**2)\n        return max(garch_vol, 0.01)\n    return max(vol, 0.01)\n\n# ============ PREDICTION FUNCTION ============\ndef predict(test: pl.DataFrame) -> float:\n    global previous_lagged, train, enet_model, xgb_model, lgb_model, scaler, test_row_count, last_allocation, v1_median\n    \n    # Online learning: Update training data\n    if previous_lagged is not None and 'lagged_market_forward_excess_returns' in previous_lagged.columns:\n        append_row = previous_lagged.with_columns(\n            pl.col('lagged_market_forward_excess_returns').alias('target')\n        )\n        append_row = create_features(append_row, is_train=False, median_values=median_values)\n        if append_row.height > 0:\n            append_row = append_row.with_columns(pl.lit(None).cast(pl.Float64).alias('target'))\n            train = train.vstack(append_row.select(['date_id', 'target'] + features))\n            if train.height > params.max_train_rows:\n                train = train.tail(params.max_train_rows)\n        \n        # Retrain every `retrain_freq` rows\n        if test_row_count % params.retrain_freq == 0:\n            X_train = scaler.fit_transform(train.select(features).to_pandas())\n            y_train = train['target'].to_pandas()\n            if y_train.isna().any():\n                raise ValueError(\"NaNs found in y_train during retraining\")\n            enet_model.fit(X_train, y_train)\n            xgb_model.fit(X_train, y_train)\n            lgb_model.fit(X_train, y_train)\n    \n    # Preprocess test data\n    test = create_features(test, is_train=False, median_values=median_values)\n    \n    # Ensure no NaNs in test data\n    X_test = test.select(features).to_pandas()\n    if X_test.isna().any().any():\n        raise ValueError(f\"NaNs found in X_test for columns: {X_test.columns[X_test.isna().any()].tolist()}\")\n    \n    X_test = scaler.transform(X_test)\n    \n    # Ensemble prediction\n    enet_pred = enet_model.predict(X_test)[0]\n    xgb_pred = xgb_model.predict(X_test)[0]\n    lgb_pred = lgb_model.predict(X_test)[0]\n    raw_pred = (params.ensemble_weights['enet'] * enet_pred +\n                params.ensemble_weights['xgb'] * xgb_pred +\n                params.ensemble_weights['lgb'] * lgb_pred)\n    \n    # Estimate volatility and dynamic signal multiplier\n    vol = estimate_volatility(test, train)\n    signal_multiplier = params.signal_multiplier_low_vol if ('V1' in test.columns and test['V1'][0] < v1_median) else params.signal_multiplier_high_vol\n    \n    # Convert to signal\n    signal = np.clip(\n        raw_pred * signal_multiplier,\n        params.min_signal,\n        params.max_signal\n    )\n    \n    # Volatility-adjusted allocation\n    allocation = min(params.max_signal, max(params.min_signal, signal / (vol * params.vol_scaling)))\n    \n    # Smooth allocation\n    transaction_cost = 0.00003  # Reduced to 0.003%\n    allocation = (0.75 * allocation + 0.25 * last_allocation) * (1 - transaction_cost)  # Adjusted smoothing\n    last_allocation = allocation\n    \n    # Update state\n    previous_lagged = test\n    test_row_count += 1\n    \n    return float(allocation)\n\n# ============ LAUNCH SERVER ============\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway((str(DATA_PATH),))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T06:34:52.861392Z","iopub.execute_input":"2025-09-18T06:34:52.861747Z","iopub.status.idle":"2025-09-18T06:35:22.291816Z","shell.execute_reply.started":"2025-09-18T06:34:52.86172Z","shell.execute_reply":"2025-09-18T06:35:22.290614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}