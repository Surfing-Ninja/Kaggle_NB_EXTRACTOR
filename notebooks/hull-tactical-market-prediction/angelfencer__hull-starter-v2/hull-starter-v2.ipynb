{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"markdown","source":"#  Modified based on example made to be faster!","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport datetime\n\nfrom tqdm import tqdm\nfrom dataclasses import dataclass, asdict\n\nimport polars as pl \nimport numpy as np\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nimport kaggle_evaluation.default_inference_server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.446273Z","iopub.execute_input":"2025-11-09T15:38:19.446556Z","iopub.status.idle":"2025-11-09T15:38:19.451367Z","shell.execute_reply.started":"2025-11-09T15:38:19.446538Z","shell.execute_reply":"2025-11-09T15:38:19.450511Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Project Directory Structure","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.452473Z","iopub.execute_input":"2025-11-09T15:38:19.452703Z","iopub.status.idle":"2025-11-09T15:38:19.486451Z","shell.execute_reply.started":"2025-11-09T15:38:19.452685Z","shell.execute_reply":"2025-11-09T15:38:19.485477Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"# ============ PATHS ============\nDATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n# ============ RETURNS TO SIGNAL CONFIGS ============\nMIN_SIGNAL: float = -1.0                      # Allow for short signals (-1.0 to 1.0 range)\nMAX_SIGNAL: float = 1.0                       # Standard range for normalized signals\nSIGNAL_MULTIPLIER: float = 200.0              # Reduced from 400 to prevent extreme positions\n\n# ============ MODEL CONFIGS ============\nCV: int = 5                                   # Reduced from 10 for faster iteration\nL1_RATIO: float = 0.8                         # Increased L1 ratio for more sparsity\nALPHAS: np.ndarray = np.logspace(-5, 1, 50)   # Wider range with fewer points for faster tuning\nMAX_ITER: int = 10_000                        # Reduced from 1M, typically sufficient for convergence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.487318Z","iopub.execute_input":"2025-11-09T15:38:19.487566Z","iopub.status.idle":"2025-11-09T15:38:19.502891Z","shell.execute_reply.started":"2025-11-09T15:38:19.487545Z","shell.execute_reply":"2025-11-09T15:38:19.502093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataclasses Helpers","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass DatasetOutput:\n    X_train : pl.DataFrame \n    X_test: pl.DataFrame\n    y_train: pl.Series\n    y_test: pl.Series\n    scaler: StandardScaler\n\n@dataclass \nclass ElasticNetParameters:\n    l1_ratio : float \n    cv: int\n    alphas: np.ndarray \n    max_iter: int \n    \n    def __post_init__(self): \n        if self.l1_ratio < 0 or self.l1_ratio > 1: \n            raise ValueError(\"Wrong initializing value for ElasticNet l1_ratio\")\n        \n@dataclass(frozen=True)\nclass RetToSignalParameters:\n    signal_multiplier: float \n    min_signal : float = MIN_SIGNAL\n    max_signal : float = MAX_SIGNAL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.50445Z","iopub.execute_input":"2025-11-09T15:38:19.504708Z","iopub.status.idle":"2025-11-09T15:38:19.526414Z","shell.execute_reply.started":"2025-11-09T15:38:19.504687Z","shell.execute_reply":"2025-11-09T15:38:19.525476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Set the Parameters","metadata":{}},{"cell_type":"code","source":"ret_signal_params = RetToSignalParameters(\n    signal_multiplier= SIGNAL_MULTIPLIER\n)\n\nenet_params = ElasticNetParameters(\n    l1_ratio = L1_RATIO, \n    cv = CV, \n    alphas = ALPHAS, \n    max_iter = MAX_ITER\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.527303Z","iopub.execute_input":"2025-11-09T15:38:19.527563Z","iopub.status.idle":"2025-11-09T15:38:19.552914Z","shell.execute_reply.started":"2025-11-09T15:38:19.527542Z","shell.execute_reply":"2025-11-09T15:38:19.551975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Loading/Creating Helper Functions (This is the part I modded)","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom dataclasses import dataclass\nimport polars as pl\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Constants as tuples for faster access\nFEATURES_TO_KEEP = (\n    \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n    \"P10\", \"P12\", \"P13\", \"U1\", \"U2\"\n)\nTARGET_COL = \"target\"\nDATE_COL = \"date_id\"\nDEFAULT_SCALER = StandardScaler\n\n@dataclass(frozen=True)  # Immutable for better performance\nclass DatasetOutput:\n    X_train: pl.DataFrame\n    y_train: pl.Series\n    X_test: pl.DataFrame\n    y_test: pl.Series\n    scaler: StandardScaler\n\ndef load_dataset(file_path: Path, target_col: str, drop_last_n: Optional[int] = None) -> pl.DataFrame:\n    \"\"\"Optimized dataset loading with lazy evaluation and streaming.\"\"\"\n    df = (\n        pl.scan_csv(\n            str(file_path),\n            dtypes={DATE_COL: pl.Int32},  # Specify known dtypes\n            infer_schema_length=10000,  # Adjust based on your data\n            low_memory=True\n        )\n        .rename({target_col: TARGET_COL})\n        .with_columns(pl.exclude(DATE_COL).cast(pl.Float64, strict=False))\n    )\n    if drop_last_n:\n        df = df.head(-drop_last_n)\n    return df.collect(streaming=True)\n\ndef create_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Optimized feature creation with lazy evaluation.\"\"\"\n    return df.with_columns([\n        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n        (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n    ])\n\ndef preprocess_dataframe(\n    df: pl.DataFrame, \n    features: Tuple[str, ...] = FEATURES_TO_KEEP,\n    fill_na: bool = True\n) -> pl.DataFrame:\n    \"\"\"Optimized preprocessing with minimal memory usage.\"\"\"\n    required_columns = (DATE_COL, TARGET_COL) + features\n    missing = [col for col in required_columns if col not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing required columns: {missing}\")\n        \n    result = df.select(required_columns)\n    \n    if fill_na:\n        result = result.with_columns([\n            pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n            for col in features\n        ])\n    \n    return result.drop_nulls()\n\ndef scale_features(\n    X_train: pl.DataFrame, \n    X_test: pl.DataFrame, \n    scaler: Optional[Any] = None\n) -> Tuple[pl.DataFrame, pl.DataFrame, Any]:\n    \"\"\"Optimized feature scaling with minimal conversions.\"\"\"\n    features = X_train.columns\n    scaler = DEFAULT_SCALER() if scaler is None else scaler\n    \n    # Convert to numpy arrays once\n    X_train_np = X_train.to_numpy()\n    X_test_np = X_test.to_numpy()\n    \n    X_train_scaled = scaler.fit_transform(X_train_np)\n    X_test_scaled = scaler.transform(X_test_np)\n    \n    return (\n        pl.from_numpy(X_train_scaled, schema=features),\n        pl.from_numpy(X_test_scaled, schema=features),\n        scaler\n    )\n\ndef prepare_datasets(\n    train_path: Path,\n    test_path: Path,\n    features: Tuple[str, ...] = FEATURES_TO_KEEP,\n    drop_last_n_train: Optional[int] = 10,\n    scale: bool = True\n) -> DatasetOutput:\n    \"\"\"Optimized dataset preparation with minimal memory usage.\"\"\"\n    # Process training data\n    train = load_dataset(train_path, \"market_forward_excess_returns\", drop_last_n_train)\n    train = create_features(train)\n    train = preprocess_dataframe(train, features)\n    \n    # Process test data\n    test = load_dataset(test_path, \"lagged_forward_returns\")\n    test = create_features(test)\n    test = preprocess_dataframe(test, features)\n    \n    # Prepare features and targets\n    X_train = train.drop([DATE_COL, TARGET_COL])\n    y_train = train[TARGET_COL]\n    X_test = test.drop([DATE_COL, TARGET_COL])\n    y_test = test[TARGET_COL]\n    \n    # Scale if needed\n    scaler = None\n    if scale:\n        X_train, X_test, scaler = scale_features(X_train, X_test)\n    \n    return DatasetOutput(\n        X_train=X_train,\n        y_train=y_train,\n        X_test=X_test,\n        y_test=y_test,\n        scaler=scaler\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.660425Z","iopub.execute_input":"2025-11-09T15:38:19.660746Z","iopub.status.idle":"2025-11-09T15:38:19.674196Z","shell.execute_reply.started":"2025-11-09T15:38:19.660726Z","shell.execute_reply":"2025-11-09T15:38:19.673305Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Converting Return Prediction to Signal\n\nHere is an example of a potential function used to convert a prediction based on the market forward excess return to a daily signal position. ","metadata":{}},{"cell_type":"code","source":"def convert_ret_to_signal(\n    ret_arr: np.ndarray,\n    params: RetToSignalParameters\n) -> np.ndarray:\n    \"\"\"\n    Converts raw model predictions (expected returns) into a trading signal.\n\n    Args:\n        ret_arr (np.ndarray): The array of predicted returns.\n        params (RetToSignalParameters): Parameters for scaling and clipping the signal.\n\n    Returns:\n        np.ndarray: The resulting trading signal, clipped between min and max values.\n    \"\"\"\n    return np.clip(\n        ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.676043Z","iopub.execute_input":"2025-11-09T15:38:19.676403Z","iopub.status.idle":"2025-11-09T15:38:19.703914Z","shell.execute_reply.started":"2025-11-09T15:38:19.67638Z","shell.execute_reply":"2025-11-09T15:38:19.702898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_trainset() -> pl.DataFrame:\n    \"\"\"\n    Loads and preprocesses the training dataset.\n\n    Returns:\n        pl.DataFrame: The preprocessed training DataFrame.\n    \"\"\"\n    return (\n        pl.read_csv(DATA_PATH / \"train.csv\")\n        .rename({'market_forward_excess_returns':'target'})\n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n        .head(-10)\n    )\n\ndef load_testset() -> pl.DataFrame:\n    \"\"\"\n    Loads and preprocesses the testing dataset.\n\n    Returns:\n        pl.DataFrame: The preprocessed testing DataFrame.\n    \"\"\"\n    return (\n        pl.read_csv(DATA_PATH / \"test.csv\")\n        .rename({'lagged_forward_returns':'target'})\n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n    )\n\ndef create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Creates new features and cleans a DataFrame.\n\n    Args:\n        df (pl.DataFrame): The input Polars DataFrame.\n\n    Returns:\n        pl.DataFrame: The DataFrame with new features, selected columns, and no null values.\n    \"\"\"\n    vars_to_keep: List[str] = [\n        \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n        \"P10\", \"P12\", \"P13\", \"U1\", \"U2\"\n    ]\n\n    return (\n        df.with_columns(\n            (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n            (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n        )\n        .select([\"date_id\", \"target\"] + vars_to_keep)\n        .with_columns([\n            pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n            for col in vars_to_keep\n        ])\n        .drop_nulls()\n    )\n    \ndef join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Joins two dataframes by common columns and concatenates them vertically.\n\n    Args:\n        train (pl.DataFrame): The training DataFrame.\n        test (pl.DataFrame): The testing DataFrame.\n\n    Returns:\n        pl.DataFrame: A single DataFrame with vertically stacked data from common columns.\n    \"\"\"\n    common_columns: list[str] = [col for col in train.columns if col in test.columns]\n    \n    return pl.concat([train.select(common_columns), test.select(common_columns)], how=\"vertical\")\n\ndef split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput: \n    \"\"\"\n    Splits the data into features (X) and target (y), and scales the features.\n\n    Args:\n        train (pl.DataFrame): The processed training DataFrame.\n        test (pl.DataFrame): The processed testing DataFrame.\n        features (list[str]): List of features to used in model. \n\n    Returns:\n        DatasetOutput: A dataclass containing the scaled feature sets, target series, and the fitted scaler.\n    \"\"\"\n    X_train = train.drop(['date_id','target']) \n    y_train = train.get_column('target')\n    X_test = test.drop(['date_id','target']) \n    y_test = test.get_column('target')\n    \n    scaler = StandardScaler() \n    \n    X_train_scaled_np = scaler.fit_transform(X_train)\n    X_train = pl.from_numpy(X_train_scaled_np, schema=features)\n    \n    X_test_scaled_np = scaler.transform(X_test)\n    X_test = pl.from_numpy(X_test_scaled_np, schema=features)\n    \n    \n    return DatasetOutput(\n        X_train = X_train,\n        y_train = y_train, \n        X_test = X_test, \n        y_test = y_test,\n        scaler = scaler\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.704972Z","iopub.execute_input":"2025-11-09T15:38:19.70554Z","iopub.status.idle":"2025-11-09T15:38:19.73645Z","shell.execute_reply.started":"2025-11-09T15:38:19.705502Z","shell.execute_reply":"2025-11-09T15:38:19.735193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Looking at the Data","metadata":{}},{"cell_type":"code","source":"train: pl.DataFrame = load_trainset()\ntest: pl.DataFrame = load_testset() \nprint(train.tail(3)) \nprint(test.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.738459Z","iopub.execute_input":"2025-11-09T15:38:19.738732Z","iopub.status.idle":"2025-11-09T15:38:19.816432Z","shell.execute_reply.started":"2025-11-09T15:38:19.738715Z","shell.execute_reply":"2025-11-09T15:38:19.815277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generating the Train and Test","metadata":{}},{"cell_type":"code","source":"df: pl.DataFrame = join_train_test_dataframes(train, test)\ndf = create_example_dataset(df=df) \ntrain: pl.DataFrame = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\ntest: pl.DataFrame = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n\nFEATURES: list[str] = [col for col in test.columns if col not in ['date_id', 'target']]\n\ndataset: DatasetOutput = split_dataset(train=train, test=test, features=FEATURES) \n\nX_train: pl.DataFrame = dataset.X_train\nX_test: pl.DataFrame = dataset.X_test\ny_train: pl.DataFrame = dataset.y_train\ny_test: pl.DataFrame = dataset.y_test\nscaler: StandardScaler = dataset.scaler ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.817153Z","iopub.execute_input":"2025-11-09T15:38:19.8174Z","iopub.status.idle":"2025-11-09T15:38:19.85014Z","shell.execute_reply.started":"2025-11-09T15:38:19.817378Z","shell.execute_reply":"2025-11-09T15:38:19.849318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fitting the Model ","metadata":{}},{"cell_type":"code","source":"model_cv: ElasticNetCV = ElasticNetCV(\n    **asdict(enet_params)\n)\nmodel_cv.fit(X_train, y_train) \n        \n# Fit the final model using the best alpha found by cross-validation\nmodel: ElasticNet = ElasticNet(alpha=model_cv.alpha_, l1_ratio=enet_params.l1_ratio) \nmodel.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.851271Z","iopub.execute_input":"2025-11-09T15:38:19.851581Z","iopub.status.idle":"2025-11-09T15:38:19.90806Z","shell.execute_reply.started":"2025-11-09T15:38:19.851559Z","shell.execute_reply":"2025-11-09T15:38:19.907463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction Function via Kaggle Server","metadata":{}},{"cell_type":"code","source":"def predict(test: pl.DataFrame) -> float:\n    test = test.rename({'lagged_forward_returns':'target'})\n    df: pl.DataFrame = create_example_dataset(test)\n    X_test: pl.DataFrame = df.select(FEATURES)\n    X_test_scaled_np: np.ndarray = scaler.transform(X_test)\n    X_test: pl.DataFrame = pl.from_numpy(X_test_scaled_np, schema=FEATURES)\n    raw_pred: float = model.predict(X_test)[0]\n    return convert_ret_to_signal(raw_pred, ret_signal_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.9086Z","iopub.execute_input":"2025-11-09T15:38:19.908811Z","iopub.status.idle":"2025-11-09T15:38:19.913979Z","shell.execute_reply.started":"2025-11-09T15:38:19.908792Z","shell.execute_reply":"2025-11-09T15:38:19.913129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Launch Server","metadata":{}},{"cell_type":"code","source":"inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:38:19.914778Z","iopub.execute_input":"2025-11-09T15:38:19.91497Z","iopub.status.idle":"2025-11-09T15:38:20.045732Z","shell.execute_reply.started":"2025-11-09T15:38:19.914956Z","shell.execute_reply":"2025-11-09T15:38:20.044778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}