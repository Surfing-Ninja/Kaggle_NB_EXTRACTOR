{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import standard libraries\nimport numpy as np\nimport pandas as pd\n\n# Import modeling libraries\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\n# Import Optuna for hyperparameter optimization\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.pruners import MedianPruner\n\n# Set numpy print options for readability\nnp.set_printoptions(suppress=True, linewidth=120)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to load the datasets\ndef load_data(train_path, test_path):\n    # Read CSV files\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    return train, test","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to identify target and feature columns\ndef get_columns(train_df, target_pref=\"market_forward_excess_returns\"):\n    # Determine target column\n    target = target_pref if target_pref in train_df.columns else (\n        \"forward_returns\" if \"forward_returns\" in train_df.columns else None\n    )\n    \n    # Validate presence of target\n    if target is None:\n        raise ValueError(\"No suitable target found in train.csv\")\n\n    # Build exclusion set\n    exclude = {target, \"risk_free_rate\", \"market_forward_excess_returns\", \"forward_returns\"}\n    \n    # Collect id-like columns\n    id_cols = []\n    if \"date_id\" in train_df.columns:\n        id_cols.append(\"date_id\")\n        exclude.add(\"date_id\")\n\n    # Perform prefix-based feature selection\n    candidates = [c for c in train_df.columns if c not in exclude]\n    prefixes = (\"M\", \"E\", \"I\", \"P\", \"V\", \"S\", \"MOM\", \"D\")\n    feats = [c for c in candidates if c.startswith(prefixes)]\n\n    # Fallback to numeric columns\n    if len(feats) == 0:\n        feats = train_df.drop(columns=list(exclude), errors=\"ignore\") \\\n                        .select_dtypes(include=[np.number]) \\\n                        .columns.tolist()\n\n    # Validate features\n    if len(feats) == 0:\n        raise ValueError(\"No usable features found in training data. Check your CSV header.\")\n\n    return target, feats, id_cols","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to create contiguous time-based CV folds\ndef make_time_folds(df, n_splits=10):\n    # Check required column\n    if \"date_id\" not in df.columns:\n        raise ValueError(\"date_id column required for time-based CV.\")\n\n    # Sort by time\n    df_sorted = df.sort_values(\"date_id\").reset_index(drop=True)\n    \n    # Determine sizes\n    n = len(df_sorted)\n    fold_sizes = [n // n_splits + (1 if i < n % n_splits else 0) for i in range(n_splits)]\n    indices = np.cumsum([0] + fold_sizes)\n\n    # Build expanding-train, next-block-validate folds\n    folds = []\n    for i in range(1, n_splits):\n        start = indices[i]\n        end = indices[i + 1]\n        val_idx = np.arange(start, end)\n        train_idx = np.arange(0, start)\n        folds.append((train_idx, val_idx))\n\n    return folds, df_sorted.index.values","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to evaluate parameters via time-aware CV\ndef cv_rmse_for_params(train_df, features, target, params, n_splits=10, random_state=42, early_stopping=200):\n    # Prepare arrays\n    X = train_df[features].copy()\n    y = train_df[target].copy()\n    \n    # Identify categorical columns\n    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    \n    # Cast to category dtype\n    for c in cat_cols:\n        X[c] = X[c].astype(\"category\")\n    \n    # Build time folds\n    folds, _ = make_time_folds(train_df, n_splits=n_splits)\n    \n    # Initialize OOF predictions\n    oof_pred = np.zeros(len(train_df), dtype=float)\n    \n    # Iterate folds\n    for tr_idx, va_idx in folds:\n        X_tr = X.iloc[tr_idx]\n        y_tr = y.iloc[tr_idx]\n        X_va = X.iloc[va_idx]\n        y_va = y.iloc[va_idx]\n        \n        # Initialize model\n        model = lgb.LGBMRegressor(**params, random_state=random_state, verbose=-1)\n        \n        # Fit with early stopping\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_cols if len(cat_cols) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=early_stopping, verbose=False)],\n        )\n        \n        # Predict validation\n        pred_va = model.predict(X_va, num_iteration=model.best_iteration_)\n        \n        # Store OOF predictions\n        oof_pred[va_idx] = pred_va\n    \n    # Compute OOF RMSE\n    rmse = np.sqrt(mean_squared_error(y, oof_pred))\n    \n    return rmse","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to run Optuna optimization\ndef optimize_lgb_params(train_df, features, target, n_splits=10, n_trials=50, random_state=42):\n    # Create sampler\n    sampler = TPESampler(seed=random_state)\n    \n    # Create pruner\n    pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=0)\n    \n    # Define objective\n    def objective(trial):\n        # Suggest parameters\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True)\n        n_estimators = trial.suggest_int(\"n_estimators\", 500, 8000, step=250)\n        num_leaves = trial.suggest_int(\"num_leaves\", 16, 512, log=True)\n        max_depth = trial.suggest_int(\"max_depth\", -1, 16)\n        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n        reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0, 20.0)\n        reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0, 30.0)\n        min_child_samples = trial.suggest_int(\"min_child_samples\", 10, 200)\n        min_child_weight = trial.suggest_float(\"min_child_weight\", 1e-4, 1.0, log=True)\n        \n        # Build parameter dict\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": learning_rate,\n            \"n_estimators\": n_estimators,\n            \"num_leaves\": num_leaves,\n            \"max_depth\": max_depth,\n            \"subsample\": subsample,\n            \"colsample_bytree\": colsample_bytree,\n            \"reg_alpha\": reg_alpha,\n            \"reg_lambda\": reg_lambda,\n            \"min_child_samples\": min_child_samples,\n            \"min_child_weight\": min_child_weight,\n        }\n        \n        # Compute CV score\n        rmse = cv_rmse_for_params(\n            train_df=train_df,\n            features=features,\n            target=target,\n            params=params,\n            n_splits=n_splits,\n            random_state=random_state,\n            early_stopping=200,\n        )\n        \n        # Report to Optuna\n        trial.report(rmse, step=0)\n        \n        return rmse\n    \n    # Create study\n    study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n    \n    # Optimize\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n    \n    # Extract best parameters\n    best_params = study.best_params\n    \n    # Ensure fixed parameters are present\n    best_params[\"objective\"] = \"regression\"\n    best_params[\"metric\"] = \"rmse\"\n    \n    return best_params, study.best_value","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to train LightGBM with time-aware CV using chosen parameters\ndef train_with_cv(train_df, features, target, params, n_splits=10, random_state=42):\n    # Prepare arrays\n    X = train_df[features].copy()\n    y = train_df[target].copy()\n    \n    # Identify categorical columns for LightGBM\n    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    \n    # Cast to category dtype\n    for c in cat_cols:\n        X[c] = X[c].astype(\"category\")\n    \n    # Build time folds\n    folds, _ = make_time_folds(train_df, n_splits=n_splits)\n    \n    # Initialize out-of-fold predictions\n    oof_pred = np.zeros(len(train_df), dtype=float)\n    \n    # Initialize model container\n    models = []\n    \n    # Iterate folds in time order\n    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n        X_tr = X.iloc[tr_idx]\n        y_tr = y.iloc[tr_idx]\n        X_va = X.iloc[va_idx]\n        y_va = y.iloc[va_idx]\n        \n        # Create model\n        model = lgb.LGBMRegressor(**params, random_state=random_state, verbose=-1)\n        \n        # Fit with early stopping\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_cols if len(cat_cols) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        \n        # Predict validation\n        pred_va = model.predict(X_va, num_iteration=model.best_iteration_)\n        \n        # Store OOF predictions\n        oof_pred[va_idx] = pred_va\n        \n        # Append model\n        models.append(model)\n        \n        # Print fold RMSE\n        rmse = np.sqrt(mean_squared_error(y_va, pred_va))\n        print(f\"Fold {i} RMSE: {rmse:.6f}\")\n    \n    # Compute overall OOF RMSE\n    oof_rmse = np.sqrt(mean_squared_error(y, oof_pred))\n    \n    # Print OOF RMSE\n    print(f\"OOF RMSE: {oof_rmse:.6f}\")\n    \n    return models, oof_pred","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to ensemble predictions across models\ndef predict_ensemble(models, X):\n    # Initialize accumulator\n    preds = np.zeros(len(X), dtype=float)\n    \n    # Sum predictions\n    for m in models:\n        preds += m.predict(X, num_iteration=m.best_iteration_)\n    \n    # Average predictions\n    preds /= max(1, len(models))\n    \n    return preds","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to compute simple market and strategy stats\ndef strategy_eval(train_df, target_col, pred_col, vol_cap_ratio=1.2):\n    # Extract realized forward returns\n    realized = train_df[target_col].values\n    \n    # Choose market proxy\n    if \"forward_returns\" in train_df.columns:\n        market = train_df[\"forward_returns\"].values\n    else:\n        market = realized\n    \n    # Compute market volatility\n    market_vol = np.std(market)\n    \n    # Prepare signal\n    raw_sig = train_df[pred_col].values\n    sig = (raw_sig - np.median(raw_sig))\n    \n    # Compute scaling\n    if np.std(sig) < 1e-12:\n        alpha = 0.0\n    else:\n        strat_vol_per_unit = np.std(sig * realized)\n        target_vol = vol_cap_ratio * market_vol\n        alpha = 0.0 if strat_vol_per_unit == 0 else min(1.0, target_vol / strat_vol_per_unit)\n    \n    # Compute strategy returns\n    strat = alpha * sig * realized\n    \n    # Compute summary stats\n    strat_mean = np.mean(strat)\n    strat_vol = np.std(strat)\n    market_mean = np.mean(market)\n    \n    # Compute Sharpe-like ratios\n    strat_sharpe = 0.0 if strat_vol == 0 else strat_mean / strat_vol\n    market_sharpe = 0.0 if market_vol == 0 else market_mean / market_vol\n    \n    # Print summary\n    print(f\"Strategy mean: {strat_mean:.6f}\")\n    print(f\"Strategy vol:  {strat_vol:.6f}\")\n    print(f\"Market vol:    {market_vol:.6f}\")\n    print(f\"Alpha scale:   {alpha:.6f}\")\n    print(f\"Sharpe (strat): {strat_sharpe:.4f}\")\n    print(f\"Sharpe (mkt):   {market_sharpe:.4f}\")\n    \n    return strat, alpha","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to prepare test features safely\ndef build_test_matrix(test_df, features):\n    # Enumerate potential leak columns\n    leak_like = [\n        \"forward_returns\",\n        \"risk_free_rate\",\n        \"market_forward_excess_returns\",\n    ]\n    \n    # Build feature matrix\n    X_test = test_df[features].copy()\n    \n    # Ensure categorical dtype where applicable\n    cat_cols = X_test.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    for c in cat_cols:\n        X_test[c] = X_test[c].astype(\"category\")\n    \n    return X_test","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the main function\ndef main():\n    # Set file paths\n    train_path = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\n    test_path = \"/kaggle/input/hull-tactical-market-prediction/test.csv\"\n    \n    # Load data\n    train_df, test_df = load_data(train_path, test_path)\n    \n    # Identify target and features\n    target, features, id_cols = get_columns(train_df, target_pref=\"market_forward_excess_returns\")\n    \n    # Print dataset diagnostics\n    print(\"Target column:\", target)\n    print(\"Number of features:\", len(features))\n    print(\"Some features:\", features[:10])\n    print(\"Train shape:\", train_df.shape)\n    print(\"Columns:\", train_df.columns.tolist()[:20])\n    \n    # Determine optimization settings\n    n_trials = 768\n    \n    # Run Optuna hyperparameter search\n    best_params, best_cv_rmse = optimize_lgb_params(\n        train_df=train_df,\n        features=features,\n        target=target,\n        n_splits=10,\n        n_trials=n_trials,\n        random_state=42,\n    )\n    \n    # Print best parameters\n    print(\"Best CV RMSE:\", f\"{best_cv_rmse:.6f}\")\n    print(\"Best params:\", best_params)\n    \n    # Train final models using best parameters\n    models, oof_pred = train_with_cv(\n        train_df=train_df,\n        features=features,\n        target=target,\n        params=best_params,\n        n_splits=10,\n        random_state=42,\n    )\n    \n    # Attach OOF predictions\n    train_df[\"oof_pred\"] = oof_pred\n    \n    # Run simple betting strategy diagnostic\n    _strat, _alpha = strategy_eval(\n        train_df=train_df,\n        target_col=target,\n        pred_col=\"oof_pred\",\n        vol_cap_ratio=1.2,\n    )\n    \n    # Build test matrix\n    X_test = build_test_matrix(test_df, features)\n    \n    # Predict test\n    test_pred = predict_ensemble(models, X_test)\n    \n    # Build submission DataFrame\n    sub = pd.DataFrame({\"prediction\": test_pred})\n    if \"date_id\" in test_df.columns:\n        sub.insert(0, \"date_id\", test_df[\"date_id\"].values)\n    \n    # Save submission\n    out_path = \"submission.csv\"\n    sub.to_csv(out_path, index=False)\n    \n    # Print confirmation\n    print(f\"Saved submission to {out_path}\")\n    print(f\"Columns: {list(sub.columns)}\")\n    print(sub.head().to_string(index=False))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the main function\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}