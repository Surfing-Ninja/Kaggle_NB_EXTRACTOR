{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nimport gc\nimport time\nimport pickle\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Optional, Dict, Any\nfrom datetime import datetime\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.feature_selection import SelectKBest, f_regression, VarianceThreshold\nfrom sklearn.linear_model import ElasticNetCV, RidgeCV, LassoCV, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nimport kaggle_evaluation.default_inference_server\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n@dataclass\nclass Config:\n    # Paths\n    data_path: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n    output_path: Path = Path('/kaggle/working/')\n    train_file: str = 'train.csv'\n    test_file: str = 'test.csv'\n    \n    # Data parameters\n    random_seed: int = 42\n    max_train_rows: int = 800\n    missing_threshold: float = 0.5\n    min_date_id: int = 37\n    \n    # Feature engineering\n    use_rolling_features: bool = True\n    rolling_windows: List[int] = field(default_factory=lambda: [5, 10, 20, 50])\n    use_lag_features: bool = True\n    lag_periods: List[int] = field(default_factory=lambda: [1, 2, 3, 5, 10])\n    use_technical_indicators: bool = True\n    use_interaction_features: bool = True\n    max_features_for_interactions: int = 10\n    use_derived_features: bool = True\n    \n    # Preprocessing\n    handle_outliers: str = 'clip'\n    outlier_threshold: float = 3.0\n    scaling_method: str = 'robust'\n    dim_reduction_method: str = 'select_k_best'\n    n_components: int = 150\n    variance_threshold: float = 0.001\n    use_nan_imputation: bool = True\n    imputation_strategy: str = 'median'\n    \n    # Model parameters\n    model_type: str = 'ensemble'\n    cv_folds: int = 3\n    validation_size: float = 0.1\n    early_stopping_rounds: int = 50\n    \n    # Ensemble specific\n    ensemble_weights: Dict[str, float] = field(default_factory=lambda: {\n        'elastic': 0.25, 'xgboost': 0.35, 'lightgbm': 0.30, 'ridge': 0.10\n    })\n    \n    # Signal generation\n    signal_multiplier: float = 100.0\n    signal_multiplier_low_vol: float = 600.0\n    signal_multiplier_high_vol: float = 400.0\n    min_signal: float = 0.0\n    max_signal: float = 2.0\n    use_volatility_scaling: bool = True\n    volatility_window: int = 20\n    target_volatility: float = 0.12\n    vol_scaling: float = 1.2\n    \n    # Online learning\n    use_online_learning: bool = True\n    retrain_frequency: int = 1\n    transaction_cost: float = 0.00003\n    smoothing_weight: float = 0.75\n    \n    # Grid search\n    enable_grid_search: bool = True\n    max_configurations: int = 5  # Limited for speed\n    time_limit_minutes: int = 10\n    verbose: bool = True\n\n# ============================================================================\n# DATA LOADING UTILITIES\n# ============================================================================\ndef load_and_clean_data(file_path: Path, target_col: str = None) -> pd.DataFrame:\n    \"\"\"Load CSV and ensure all columns are numeric\"\"\"\n    df = pd.read_csv(file_path)\n    \n    exclude_cols = ['date_id', 'is_scored']\n    numeric_cols = [col for col in df.columns if col not in exclude_cols]\n    \n    for col in numeric_cols:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    if target_col and target_col in df.columns:\n        df = df.rename(columns={target_col: 'target'})\n    \n    return df\n\n# ============================================================================\n# FEATURE ENGINEERING\n# ============================================================================\nclass FeatureEngineer:\n    def __init__(self, config: Config):\n        self.config = config\n        self.base_features = None\n        self.feature_names = None\n        self.median_values = {}\n        \n    def fit(self, df: pd.DataFrame):\n        \"\"\"Fit feature engineer on training data\"\"\"\n        exclude_cols = ['date_id', 'target', 'forward_returns', 'is_scored', \n                       'risk_free_rate', 'market_forward_excess_returns',\n                       'lagged_forward_returns', 'lagged_risk_free_rate', \n                       'lagged_market_forward_excess_returns']\n        \n        self.base_features = [col for col in df.columns if col not in exclude_cols]\n        \n        missing_rates = df[self.base_features].isnull().mean()\n        self.base_features = [col for col in self.base_features \n                             if missing_rates[col] <= self.config.missing_threshold]\n        \n        for col in self.base_features:\n            median_val = df[col].median()\n            self.median_values[col] = median_val if not pd.isna(median_val) else 0.0\n        \n        return self\n    \n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Transform dataframe with feature engineering\"\"\"\n        result = pd.DataFrame(index=df.index)\n        \n        for col in self.base_features:\n            if col in df.columns:\n                result[col] = df[col].fillna(self.median_values.get(col, 0))\n            else:\n                result[col] = 0.0\n        \n        if self.config.use_derived_features:\n            if all(col in result.columns for col in ['I1', 'I2', 'I7', 'I9', 'M11']):\n                result['U1'] = result['I2'] - result['I1']\n                denominator = (result['I2'] + result['I9'] + result['I7']) / 3 + 1e-10\n                result['U2'] = result['M11'] / denominator\n        \n        if self.config.use_interaction_features:\n            interactions = [\n                ('V1', 'S1', 'V1_S1'),\n                ('M11', 'V1', 'M11_V1'),\n                ('I9', 'S1', 'I9_S1'),\n                ('P1', 'V1', 'P1_V1'),\n                ('E1', 'S1', 'E1_S1')\n            ]\n            for col1, col2, new_col in interactions:\n                if col1 in result.columns and col2 in result.columns:\n                    result[new_col] = result[col1] * result[col2]\n        \n        if self.config.use_technical_indicators:\n            for col in ['V1', 'S1', 'M11', 'P1']:\n                if col in result.columns:\n                    result = self._add_technical_indicators(result, col)\n        \n        if self.config.use_rolling_features:\n            for col in ['V1', 'S1', 'M11', 'I9', 'P1']:\n                if col in result.columns:\n                    for window in self.config.rolling_windows:\n                        result[f'{col}_roll_mean_{window}'] = result[col].rolling(\n                            window=window, min_periods=1).mean()\n                        result[f'{col}_roll_std_{window}'] = result[col].rolling(\n                            window=window, min_periods=1).std().fillna(0)\n        \n        if self.config.use_lag_features:\n            for col in ['V1', 'S1', 'I9', 'P1']:\n                if col in result.columns:\n                    for lag in self.config.lag_periods:\n                        result[f'{col}_lag_{lag}'] = result[col].shift(lag).fillna(\n                            self.median_values.get(col, 0))\n                        result[f'{col}_diff_{lag}'] = (result[col] - result[col].shift(lag)).fillna(0)\n        \n        result = result.fillna(0)\n        result = result.replace([np.inf, -np.inf], 0)\n        \n        self.feature_names = result.columns.tolist()\n        \n        return result\n    \n    def _add_technical_indicators(self, df: pd.DataFrame, col: str) -> pd.DataFrame:\n        \"\"\"Add RSI, Bollinger Bands position, and rate of change\"\"\"\n        delta = df[col].diff()\n        gain = delta.where(delta > 0, 0).rolling(window=14, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()\n        rs = gain / (loss + 1e-10)\n        df[f'{col}_rsi'] = 100 - (100 / (1 + rs))\n        \n        rolling_mean = df[col].rolling(window=20, min_periods=1).mean()\n        rolling_std = df[col].rolling(window=20, min_periods=1).std().fillna(0)\n        upper = rolling_mean + (2 * rolling_std)\n        lower = rolling_mean - (2 * rolling_std)\n        band_width = upper - lower + 1e-10\n        df[f'{col}_bb_position'] = (df[col] - lower) / band_width\n        \n        df[f'{col}_roc'] = df[col].pct_change(periods=10).fillna(0)\n        \n        return df\n\n# ============================================================================\n# DATA PROCESSOR\n# ============================================================================\nclass DataProcessor:\n    def __init__(self, config: Config):\n        self.config = config\n        self.feature_engineer = FeatureEngineer(config)\n        self.scaler = None\n        self.feature_selector = None\n        self.imputer = None\n        self.final_feature_names = None\n        \n    def fit(self, df: pd.DataFrame) -> 'DataProcessor':\n        \"\"\"Fit data processor on training data\"\"\"\n        self.feature_engineer.fit(df)\n        features_df = self.feature_engineer.transform(df)\n        X = features_df.values\n        \n        if self.config.handle_outliers == 'clip':\n            X = self._clip_outliers(X)\n        \n        if self.config.use_nan_imputation:\n            self.imputer = SimpleImputer(strategy=self.config.imputation_strategy)\n            X = self.imputer.fit_transform(X)\n        \n        if self.config.scaling_method == 'standard':\n            self.scaler = StandardScaler()\n        elif self.config.scaling_method == 'robust':\n            self.scaler = RobustScaler()\n        elif self.config.scaling_method == 'minmax':\n            self.scaler = MinMaxScaler()\n        else:\n            self.scaler = None\n        \n        if self.scaler:\n            X = self.scaler.fit_transform(X)\n        \n        if self.config.dim_reduction_method != 'none' and 'target' in df.columns:\n            y = df['target'].values\n            y = np.nan_to_num(y, nan=0.0)\n            \n            if self.config.dim_reduction_method == 'pca':\n                n_components = min(self.config.n_components, min(X.shape) - 1)\n                self.feature_selector = PCA(n_components=n_components, random_state=self.config.random_seed)\n                self.feature_selector.fit(X)\n            elif self.config.dim_reduction_method == 'select_k_best':\n                k = min(self.config.n_components, X.shape[1])\n                self.feature_selector = SelectKBest(score_func=f_regression, k=k)\n                self.feature_selector.fit(X, y)\n            elif self.config.dim_reduction_method == 'variance':\n                self.feature_selector = VarianceThreshold(threshold=self.config.variance_threshold)\n                self.feature_selector.fit(X)\n        \n        self._update_final_feature_names()\n        return self\n    \n    def transform(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Transform dataframe to model-ready features\"\"\"\n        features_df = self.feature_engineer.transform(df)\n        X = features_df.values\n        \n        if self.config.handle_outliers == 'clip':\n            X = self._clip_outliers(X)\n        \n        if self.imputer:\n            X = self.imputer.transform(X)\n        \n        if self.scaler:\n            X = self.scaler.transform(X)\n        \n        if self.feature_selector:\n            X = self.feature_selector.transform(X)\n        \n        X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n        \n        return X\n    \n    def _clip_outliers(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Clip outliers using z-score threshold\"\"\"\n        X_clipped = X.copy()\n        for i in range(X.shape[1]):\n            col_data = X[:, i]\n            valid_data = col_data[~np.isnan(col_data)]\n            if len(valid_data) > 0:\n                mean = np.mean(valid_data)\n                std = np.std(valid_data)\n                if std > 0:\n                    lower = mean - self.config.outlier_threshold * std\n                    upper = mean + self.config.outlier_threshold * std\n                    X_clipped[:, i] = np.clip(X[:, i], lower, upper)\n        return X_clipped\n    \n    def _update_final_feature_names(self):\n        \"\"\"Update final feature names after selection\"\"\"\n        if self.feature_selector and hasattr(self.feature_selector, 'get_support'):\n            mask = self.feature_selector.get_support()\n            self.final_feature_names = [f for f, m in zip(self.feature_engineer.feature_names, mask) if m]\n        else:\n            self.final_feature_names = self.feature_engineer.feature_names\n\n# ============================================================================\n# MODEL FACTORY\n# ============================================================================\nclass ModelFactory:\n    @staticmethod\n    def create_model(model_type: str, config: Config):\n        \"\"\"Create model based on type\"\"\"\n        if model_type == 'elastic':\n            return ElasticNetCV(\n                l1_ratio=[0.1, 0.5, 0.7, 0.9],\n                alphas=np.logspace(-4, 1, 20),\n                cv=config.cv_folds,\n                max_iter=10000,\n                random_state=config.random_seed\n            )\n        elif model_type == 'xgboost':\n            return xgb.XGBRegressor(\n                n_estimators=300,\n                max_depth=6,\n                learning_rate=0.03,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                min_child_weight=3,\n                gamma=0.1,\n                reg_alpha=0.1,\n                reg_lambda=1.0,\n                random_state=config.random_seed,\n                n_jobs=-1,\n                objective='reg:squarederror',\n                verbosity=0\n            )\n        elif model_type == 'lightgbm':\n            return lgb.LGBMRegressor(\n                n_estimators=300,\n                max_depth=6,\n                learning_rate=0.03,\n                num_leaves=31,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                min_child_samples=20,\n                reg_alpha=0.1,\n                reg_lambda=1.0,\n                random_state=config.random_seed,\n                n_jobs=-1,\n                verbosity=-1\n            )\n        elif model_type == 'ridge':\n            return RidgeCV(\n                alphas=np.logspace(-4, 2, 50),\n                cv=config.cv_folds\n            )\n        elif model_type == 'gradient_boost':\n            return GradientBoostingRegressor(\n                n_estimators=200,\n                max_depth=5,\n                learning_rate=0.05,\n                subsample=0.8,\n                min_samples_split=5,\n                min_samples_leaf=3,\n                loss='huber',\n                random_state=config.random_seed\n            )\n        else:\n            return RidgeCV(alphas=np.logspace(-4, 2, 50))\n\n# ============================================================================\n# ENSEMBLE MODEL\n# ============================================================================\nclass EnsembleModel:\n    def __init__(self, config: Config):\n        self.config = config\n        self.models = {}\n        self.weights = config.ensemble_weights.copy()\n        \n    def fit(self, X: np.ndarray, y: np.ndarray, X_val: Optional[np.ndarray] = None, \n            y_val: Optional[np.ndarray] = None):\n        \"\"\"Fit ensemble models\"\"\"\n        X_clean = np.nan_to_num(X, nan=0.0)\n        y_clean = np.nan_to_num(y, nan=0.0)\n        \n        if X_val is not None:\n            X_val_clean = np.nan_to_num(X_val, nan=0.0)\n            y_val_clean = np.nan_to_num(y_val, nan=0.0)\n        \n        for name, weight in self.weights.items():\n            if weight > 0:\n                try:\n                    model = ModelFactory.create_model(name, self.config)\n                    \n                    if name in ['xgboost', 'lightgbm'] and X_val is not None:\n                        if name == 'xgboost':\n                            model.fit(\n                                X_clean, y_clean,\n                                eval_set=[(X_val_clean, y_val_clean)],\n                                early_stopping_rounds=self.config.early_stopping_rounds,\n                                verbose=False\n                            )\n                        else:\n                            model.fit(\n                                X_clean, y_clean,\n                                eval_set=[(X_val_clean, y_val_clean)],\n                                callbacks=[\n                                    lgb.early_stopping(self.config.early_stopping_rounds),\n                                    lgb.log_evaluation(0)\n                                ]\n                            )\n                    else:\n                        model.fit(X_clean, y_clean)\n                    \n                    self.models[name] = model\n                    \n                except Exception as e:\n                    if self.config.verbose:\n                        print(f\"Failed to train {name}: {str(e)[:100]}\")\n                    self.weights[name] = 0\n        \n        total_weight = sum(self.weights.values())\n        if total_weight > 0:\n            for name in self.weights:\n                self.weights[name] /= total_weight\n    \n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Generate ensemble predictions\"\"\"\n        X_clean = np.nan_to_num(X, nan=0.0)\n        predictions = np.zeros(len(X_clean))\n        \n        for name, model in self.models.items():\n            if name in self.weights and self.weights[name] > 0:\n                try:\n                    pred = model.predict(X_clean)\n                    predictions += pred * self.weights[name]\n                except Exception as e:\n                    if self.config.verbose:\n                        print(f\"Prediction failed for {name}: {str(e)[:100]}\")\n        \n        return predictions\n\n# ============================================================================\n# GRID SEARCH OPTIMIZER\n# ============================================================================\nclass GridSearchOptimizer:\n    def __init__(self, config: Config):\n        self.config = config\n        self.results = []\n        self.best_config = None\n        self.best_score = float('inf')\n        \n    def generate_configurations(self) -> List[Config]:\n        \"\"\"Generate limited set of high-quality configurations\"\"\"\n        configs = []\n        \n        best_combinations = [\n            {\n                'scaling_method': 'robust',\n                'dim_reduction_method': 'select_k_best',\n                'n_components': 150,\n                'handle_outliers': 'clip',\n                'model_type': 'ensemble',\n                'use_rolling_features': True,\n                'use_technical_indicators': True,\n                'use_interaction_features': True,\n                'signal_multiplier_low_vol': 600.0,\n                'signal_multiplier_high_vol': 400.0,\n                'ensemble_weights': {'elastic': 0.25, 'xgboost': 0.35, 'lightgbm': 0.30, 'ridge': 0.10}\n            },\n            {\n                'scaling_method': 'standard',\n                'dim_reduction_method': 'pca',\n                'n_components': 100,\n                'handle_outliers': 'clip',\n                'model_type': 'ensemble',\n                'use_rolling_features': True,\n                'use_technical_indicators': False,\n                'use_interaction_features': True,\n                'signal_multiplier_low_vol': 500.0,\n                'signal_multiplier_high_vol': 350.0,\n                'ensemble_weights': {'elastic': 0.20, 'xgboost': 0.40, 'lightgbm': 0.40, 'ridge': 0.0}\n            },\n            {\n                'scaling_method': 'robust',\n                'dim_reduction_method': 'none',\n                'n_components': 200,\n                'handle_outliers': 'clip',\n                'model_type': 'lightgbm',\n                'use_rolling_features': False,\n                'use_technical_indicators': True,\n                'use_interaction_features': True,\n                'signal_multiplier_low_vol': 700.0,\n                'signal_multiplier_high_vol': 450.0,\n                'ensemble_weights': {'elastic': 0.0, 'xgboost': 0.0, 'lightgbm': 1.0, 'ridge': 0.0}\n            }\n        ]\n        \n        for params in best_combinations[:self.config.max_configurations]:\n            config = deepcopy(self.config)\n            for key, value in params.items():\n                setattr(config, key, value)\n            configs.append(config)\n        \n        return configs\n    \n    def evaluate_configuration(self, config: Config, train_df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Evaluate a single configuration\"\"\"\n        try:\n            processor = DataProcessor(config)\n            processor.fit(train_df)\n            \n            X = processor.transform(train_df)\n            y = train_df['target'].values if 'target' in train_df.columns else np.zeros(len(train_df))\n            y = np.nan_to_num(y, nan=0.0)\n            \n            split_idx = int(len(X) * (1 - config.validation_size))\n            X_train, X_val = X[:split_idx], X[split_idx:]\n            y_train, y_val = y[:split_idx], y[split_idx:]\n            \n            if config.model_type == 'ensemble':\n                model = EnsembleModel(config)\n                model.fit(X_train, y_train, X_val, y_val)\n            else:\n                model = ModelFactory.create_model(config.model_type, config)\n                if config.model_type in ['xgboost', 'lightgbm']:\n                    if config.model_type == 'xgboost':\n                        model.fit(\n                            X_train, y_train,\n                            eval_set=[(X_val, y_val)],\n                            early_stopping_rounds=config.early_stopping_rounds,\n                            verbose=False\n                        )\n                    else:\n                        model.fit(\n                            X_train, y_train,\n                            eval_set=[(X_val, y_val)],\n                            callbacks=[\n                                lgb.early_stopping(config.early_stopping_rounds),\n                                lgb.log_evaluation(0)\n                            ]\n                        )\n                else:\n                    model.fit(X_train, y_train)\n            \n            y_pred = model.predict(X_val) if hasattr(model, 'predict') else np.zeros(len(X_val))\n            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n            mae = mean_absolute_error(y_val, y_pred)\n            \n            if len(y_pred) > 1:\n                returns = y_pred[:-1] * y_val[1:]\n                if np.std(returns) > 0:\n                    sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n                else:\n                    sharpe = 0\n            else:\n                sharpe = 0\n            \n            return {\n                'config': config,\n                'model': model,\n                'processor': processor,\n                'rmse': rmse,\n                'mae': mae,\n                'sharpe': sharpe,\n                'score': rmse\n            }\n            \n        except Exception as e:\n            if config.verbose:\n                print(f\"Configuration failed: {str(e)[:200]}\")\n            return None\n    \n    def run(self, train_df: pd.DataFrame) -> Tuple[Config, Any, Any]:\n        \"\"\"Run grid search\"\"\"\n        configs = self.generate_configurations()\n        start_time = time.time()\n        \n        if self.config.verbose:\n            print(f\"Testing {len(configs)} configurations...\")\n        \n        best_result = None\n        for i, config in enumerate(configs):\n            if (time.time() - start_time) / 60 > self.config.time_limit_minutes:\n                if self.config.verbose:\n                    print(\"Time limit reached\")\n                break\n            \n            if self.config.verbose:\n                print(f\"Configuration {i+1}/{len(configs)}...\")\n            \n            result = self.evaluate_configuration(config, train_df)\n            \n            if result is not None:\n                self.results.append(result)\n                \n                if result['score'] < self.best_score:\n                    self.best_score = result['score']\n                    self.best_config = config\n                    best_result = result\n                    \n                    if self.config.verbose:\n                        print(f\"  New best score: {self.best_score:.6f}\")\n        \n        if best_result is not None:\n            return best_result['config'], best_result['model'], best_result['processor']\n        \n        return self.config, None, None\n\n# ============================================================================\n# MAIN PIPELINE\n# ============================================================================\nclass HullTacticalPipeline:\n    def __init__(self, config: Optional[Config] = None):\n        self.config = config or Config()\n        self.processor = None\n        self.model = None\n        self.train_df = None\n        self.last_allocation = 0.0\n        self.v1_median = None\n        self.test_row_count = 0\n        \n    def train(self, train_df: pd.DataFrame) -> 'HullTacticalPipeline':\n        \"\"\"Train the pipeline\"\"\"\n        self.train_df = train_df.copy()\n        \n        if 'V1' in train_df.columns:\n            v1_values = pd.to_numeric(train_df['V1'], errors='coerce')\n            self.v1_median = v1_values.median() if not v1_values.isna().all() else 0.01\n        else:\n            self.v1_median = 0.01\n        \n        if self.config.enable_grid_search:\n            if self.config.verbose:\n                print(\"Running grid search...\")\n            \n            optimizer = GridSearchOptimizer(self.config)\n            best_config, best_model, best_processor = optimizer.run(train_df)\n            \n            if best_model is not None:\n                self.config = best_config\n                self.model = best_model\n                self.processor = best_processor\n                \n                if self.config.verbose:\n                    print(f\"Grid search complete. Best RMSE: {optimizer.best_score:.6f}\")\n            else:\n                if self.config.verbose:\n                    print(\"Grid search failed, using default configuration\")\n        \n        if self.processor is None:\n            self.processor = DataProcessor(self.config)\n            self.processor.fit(train_df)\n        \n        if self.model is None:\n            X = self.processor.transform(train_df)\n            y = train_df['target'].values if 'target' in train_df.columns else np.zeros(len(train_df))\n            y = np.nan_to_num(y, nan=0.0)\n            \n            split_idx = int(len(X) * (1 - self.config.validation_size))\n            X_train, X_val = X[:split_idx], X[split_idx:]\n            y_train, y_val = y[:split_idx], y[split_idx:]\n            \n            if self.config.model_type == 'ensemble':\n                self.model = EnsembleModel(self.config)\n                self.model.fit(X_train, y_train, X_val, y_val)\n            else:\n                self.model = ModelFactory.create_model(self.config.model_type, self.config)\n                if self.config.model_type in ['xgboost', 'lightgbm']:\n                    if self.config.model_type == 'xgboost':\n                        self.model.fit(\n                            X_train, y_train,\n                            eval_set=[(X_val, y_val)],\n                            early_stopping_rounds=self.config.early_stopping_rounds,\n                            verbose=False\n                        )\n                    else:\n                        self.model.fit(\n                            X_train, y_train,\n                            eval_set=[(X_val, y_val)],\n                            callbacks=[\n                                lgb.early_stopping(self.config.early_stopping_rounds),\n                                lgb.log_evaluation(0)\n                            ]\n                        )\n                else:\n                    self.model.fit(X_train, y_train)\n        \n        return self\n    \n    def update_with_new_data(self, new_row: pd.DataFrame):\n        \"\"\"Update training data with new observation for online learning\"\"\"\n        if self.config.use_online_learning and self.train_df is not None:\n            self.train_df = pd.concat([self.train_df, new_row], ignore_index=True)\n            \n            if len(self.train_df) > self.config.max_train_rows:\n                self.train_df = self.train_df.tail(self.config.max_train_rows).reset_index(drop=True)\n            \n            if self.test_row_count % self.config.retrain_frequency == 0 and self.test_row_count > 0:\n                if self.config.verbose and self.test_row_count % 50 == 0:\n                    print(f\"Retraining at row {self.test_row_count}...\")\n                \n                try:\n                    self.processor.fit(self.train_df)\n                    \n                    X = self.processor.transform(self.train_df)\n                    y = self.train_df['target'].values\n                    y = np.nan_to_num(y, nan=0.0)\n                    \n                    if self.config.model_type == 'ensemble':\n                        self.model = EnsembleModel(self.config)\n                        self.model.fit(X, y)\n                    else:\n                        self.model = ModelFactory.create_model(self.config.model_type, self.config)\n                        self.model.fit(X, y)\n                except Exception as e:\n                    if self.config.verbose:\n                        print(f\"Retraining failed: {str(e)[:100]}\")\n    \n    def predict(self, test_df: pd.DataFrame) -> float:\n        \"\"\"Generate prediction for single row\"\"\"\n        if self.processor is None or self.model is None:\n            return 1.0\n        \n        try:\n            X_test = self.processor.transform(test_df)\n            \n            if hasattr(self.model, 'predict'):\n                raw_pred = self.model.predict(X_test)\n                if isinstance(raw_pred, np.ndarray) and len(raw_pred) > 0:\n                    raw_pred = raw_pred[0]\n                else:\n                    raw_pred = 0.0\n            else:\n                raw_pred = 0.0\n            \n            volatility = 0.01\n            if 'V1' in test_df.columns:\n                v1_val = pd.to_numeric(test_df['V1'].iloc[0], errors='coerce')\n                if not pd.isna(v1_val):\n                    volatility = max(v1_val, 0.001)\n            elif self.train_df is not None and 'target' in self.train_df.columns:\n                recent_returns = self.train_df['target'].tail(self.config.volatility_window).values\n                if len(recent_returns) > 1:\n                    vol_std = np.std(recent_returns)\n                    if vol_std > 0:\n                        volatility = vol_std\n            \n            use_low_vol_multiplier = False\n            if self.v1_median is not None and 'V1' in test_df.columns:\n                v1_val = pd.to_numeric(test_df['V1'].iloc[0], errors='coerce')\n                if not pd.isna(v1_val):\n                    use_low_vol_multiplier = v1_val < self.v1_median\n            \n            if use_low_vol_multiplier:\n                signal_mult = self.config.signal_multiplier_low_vol\n            else:\n                signal_mult = self.config.signal_multiplier_high_vol\n            \n            signal = raw_pred * signal_mult\n            \n            if self.config.use_volatility_scaling:\n                signal = signal / (volatility * self.config.vol_scaling)\n            \n            signal = np.clip(signal, self.config.min_signal, self.config.max_signal)\n            \n            allocation = (self.config.smoothing_weight * signal + \n                         (1 - self.config.smoothing_weight) * self.last_allocation)\n            \n            allocation *= (1 - self.config.transaction_cost)\n            \n            self.last_allocation = allocation\n            self.test_row_count += 1\n            \n            return float(allocation)\n            \n        except Exception as e:\n            if self.config.verbose:\n                print(f\"Prediction error: {str(e)[:100]}\")\n            return float(self.last_allocation) if self.last_allocation > 0 else 1.0\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\ndef main():\n    \"\"\"Main execution function\"\"\"\n    print(\"=\"*60)\n    print(\"Hull Tactical Market Prediction - Enhanced Pipeline\")\n    print(\"=\"*60)\n    \n    config = Config(\n        enable_grid_search=True,\n        max_configurations=3,\n        time_limit_minutes=5,\n        use_rolling_features=True,\n        use_technical_indicators=True,\n        use_interaction_features=True,\n        use_derived_features=True,\n        use_online_learning=True,\n        retrain_frequency=1,\n        signal_multiplier_low_vol=600.0,\n        signal_multiplier_high_vol=400.0,\n        max_train_rows=800,\n        verbose=True\n    )\n    \n    print(\"Loading training data...\")\n    train_df = load_and_clean_data(\n        config.data_path / config.train_file,\n        target_col='market_forward_excess_returns'\n    )\n    \n    train_df = train_df[train_df['date_id'] >= config.min_date_id].copy()\n    train_df = train_df.tail(config.max_train_rows).reset_index(drop=True)\n    \n    print(f\"Loaded {len(train_df)} training samples\")\n    \n    pipeline = HullTacticalPipeline(config)\n    \n    print(\"\\nTraining pipeline...\")\n    pipeline.train(train_df)\n    \n    print(\"Training complete!\")\n    \n    previous_lagged = None\n    \n    def predict(test: pl.DataFrame) -> float:\n        \"\"\"Prediction function for Kaggle API\"\"\"\n        nonlocal previous_lagged\n        \n        test_pd = test.to_pandas()\n        \n        exclude_cols = ['date_id', 'is_scored']\n        for col in test_pd.columns:\n            if col not in exclude_cols:\n                test_pd[col] = pd.to_numeric(test_pd[col], errors='coerce')\n        \n        if previous_lagged is not None and 'lagged_market_forward_excess_returns' in previous_lagged.columns:\n            update_row = previous_lagged.copy()\n            if 'lagged_market_forward_excess_returns' in update_row.columns:\n                update_row['target'] = update_row['lagged_market_forward_excess_returns']\n                update_row = update_row.drop(columns=['lagged_market_forward_excess_returns'], errors='ignore')\n            pipeline.update_with_new_data(update_row)\n        \n        allocation = pipeline.predict(test_pd)\n        \n        previous_lagged = test_pd.copy()\n        \n        return allocation\n    \n    print(\"\\nStarting inference server...\")\n    \n    try:\n        inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n        \n        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n            inference_server.serve()\n        else:\n            inference_server.run_local_gateway((str(config.data_path),))\n    except Exception as e:\n        print(f\"Server error: {str(e)}\")\n        print(\"Testing prediction function locally...\")\n        \n        test_sample = train_df.iloc[[0]].copy()\n        test_pl = pl.from_pandas(test_sample)\n        result = predict(test_pl)\n        print(f\"Test prediction: {result}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}