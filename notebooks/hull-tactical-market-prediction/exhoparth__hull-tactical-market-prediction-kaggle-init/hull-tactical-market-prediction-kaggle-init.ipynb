{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:30.223076Z","iopub.execute_input":"2025-11-06T20:34:30.223353Z","iopub.status.idle":"2025-11-06T20:34:31.401904Z","shell.execute_reply.started":"2025-11-06T20:34:30.223327Z","shell.execute_reply":"2025-11-06T20:34:31.400911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\n\ntrain_df = pl.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\")\ntest_df = pl.read_csv(\"/kaggle/input/hull-tactical-market-prediction/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:39:21.368161Z","iopub.execute_input":"2025-11-06T20:39:21.369449Z","iopub.status.idle":"2025-11-06T20:39:21.430688Z","shell.execute_reply.started":"2025-11-06T20:39:21.369416Z","shell.execute_reply":"2025-11-06T20:39:21.429957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Features\n* **date_id** - An identifier for a single trading day.\n* **M*** - Market Dynamics/Technical features.\n* **E*** - Macro Economic features.\n* **I*** - Interest Rate features.\n* **P*** - Price/Valuation features.\n* **V*** - Volatility features.\n* **S*** - Sentiment features.\n* **MOM*** - Momentum features.\n* **D*** - Dummy/Binary features.\n* ***forward_returns*** - The returns from buying the S&P 500 and selling it a day later. Train set only.\n* **risk_free_rate** - The federal funds rate. Train set only.\n* **market_forward_excess_returns** - Forward returns relative to expectations. Computed by subtracting the rolling five-year mean forward returns and winsorizing the result using a median absolute deviation (MAD) with a criterion of 4. Train set only.\n\nTarget: **forward_returns**\n","metadata":{}},{"cell_type":"code","source":"print(\"Total Number of Features [Train Set]\",len(train_df.columns))\nprint(\"Total Number of Features [Test Set]\",len(test_df.columns)) # +1 for `is_scored`","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:32.923192Z","iopub.execute_input":"2025-11-06T20:34:32.923519Z","iopub.status.idle":"2025-11-06T20:34:32.934312Z","shell.execute_reply.started":"2025-11-06T20:34:32.923491Z","shell.execute_reply":"2025-11-06T20:34:32.932997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:32.935437Z","iopub.execute_input":"2025-11-06T20:34:32.935809Z","iopub.status.idle":"2025-11-06T20:34:33.085174Z","shell.execute_reply.started":"2025-11-06T20:34:32.935773Z","shell.execute_reply":"2025-11-06T20:34:33.084149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Converting `String` -> `Float`","metadata":{}},{"cell_type":"code","source":"train_df = train_df.with_columns([\n    pl.col(c).cast(pl.Float64) for c, t in train_df.schema.items() if t == pl.Utf8\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:33.086269Z","iopub.execute_input":"2025-11-06T20:34:33.086577Z","iopub.status.idle":"2025-11-06T20:34:33.141832Z","shell.execute_reply.started":"2025-11-06T20:34:33.086546Z","shell.execute_reply":"2025-11-06T20:34:33.140875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fill `null` -> 0","metadata":{}},{"cell_type":"code","source":"train_df = train_df.fill_null(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:33.142894Z","iopub.execute_input":"2025-11-06T20:34:33.143232Z","iopub.status.idle":"2025-11-06T20:34:33.172909Z","shell.execute_reply.started":"2025-11-06T20:34:33.143198Z","shell.execute_reply":"2025-11-06T20:34:33.171783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:33.17623Z","iopub.execute_input":"2025-11-06T20:34:33.176501Z","iopub.status.idle":"2025-11-06T20:34:33.227646Z","shell.execute_reply.started":"2025-11-06T20:34:33.176479Z","shell.execute_reply":"2025-11-06T20:34:33.22662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"### Mini Histograms of Numeric Features","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport math\nnumeric_cols = [c for c, t in train_df.schema.items() if t in (pl.Int64, pl.Float64)]\n\ndata = {}\nfor c in numeric_cols:\n    arr = train_df[c].drop_nulls().to_numpy()\n    if arr.size and not np.all(arr == arr[0]):\n        data[c] = arr\n\nn = len(data)\ncols = 5\nrows = math.ceil(n / cols)\n\nplt.rcParams.update({\n    \"figure.dpi\": 160,\n    \"axes.titlesize\": 8,\n    \"xtick.labelsize\": 7,\n    \"ytick.labelsize\": 7\n})\n\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 2.2, rows * 1.8), constrained_layout=True)\naxes = axes.ravel()\n\nfor i, (col, arr) in enumerate(data.items()):\n    ax = axes[i]\n    bins = int(min(40, max(8, math.sqrt(arr.size))))\n    ax.hist(arr, bins=bins, color=\"#4682B4\", edgecolor=\"white\", alpha=0.9)\n    ax.set_title(col, fontsize=8, pad=2)\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=6)\n    ax.grid(alpha=0.2, linewidth=0.3)\n\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Mini Histograms — Numeric Columns\", fontsize=10, weight=\"bold\", y=1.02)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:33.228598Z","iopub.execute_input":"2025-11-06T20:34:33.228924Z","iopub.status.idle":"2025-11-06T20:34:56.155812Z","shell.execute_reply.started":"2025-11-06T20:34:33.228895Z","shell.execute_reply":"2025-11-06T20:34:56.154555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Correlation Heatmap of Numeric Features","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nnum_cols = [c for c, t in train_df.schema.items() if t in (pl.Int64, pl.Float64) and c != \"date_id\"]\ndf = train_df.select([pl.col(c).cast(pl.Float64) for c in num_cols]).to_pandas()\n\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf = df.loc[:, df.std(skipna=True) != 0]\n\nif df.shape[1] == 0:\n    raise SystemExit(\"no usable numeric columns for correlation after filtering\")\n\ncorr = df.corr().fillna(0)\ncorr = corr.replace([np.inf, -np.inf], 0)\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\nn = len(corr.columns)\n\nplt.rcParams.update({\"figure.dpi\": 350})\nfigsize = (max(3, min(12, n * 0.18)), max(3, min(12, n * 0.18)))\ncmap = \"Spectral_r\"\nnorm = mpl.colors.Normalize(vmin=-1, vmax=1)\n\nplt.figure(figsize=figsize)\nsns.heatmap(\n    corr,\n    mask=mask,\n    cmap=cmap,\n    norm=norm,\n    vmin=-1,\n    vmax=1,\n    square=True,\n    linewidths=0.3,\n    cbar_kws={\"shrink\": 0.65, \"aspect\": 20},\n    annot=(n <= 20),\n    fmt=\".2f\",\n    annot_kws={\"size\": 6}\n)\n\nplt.xticks(rotation=90, fontsize=max(5, 9 - n // 10))\nplt.yticks(rotation=0, fontsize=max(5, 9 - n // 10))\nplt.title(\"Correlation Heatmap\", fontsize=10, weight=\"bold\", pad=6)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:56.157018Z","iopub.execute_input":"2025-11-06T20:34:56.15734Z","iopub.status.idle":"2025-11-06T20:34:59.239938Z","shell.execute_reply.started":"2025-11-06T20:34:56.157314Z","shell.execute_reply":"2025-11-06T20:34:59.238853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Although we could select features based on their correlation with the `forward_return` to focus on the most predictive variables, in this experiment we will retain all features to allow the neural network to learn underlying relationships directly from the data.","metadata":{}},{"cell_type":"code","source":"import math\nimport warnings\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress only the specific FutureWarning from seaborn/pandas\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n\nsns.set_theme(style=\"whitegrid\", context=\"notebook\")\n\nplt.rcParams.update({\n    \"figure.dpi\": 300,\n    \"axes.titlesize\": 9,\n    \"xtick.labelsize\": 7,\n    \"ytick.labelsize\": 7\n})\n\nnumeric_cols = [c for c, t in train_df.schema.items() if t in (pl.Int64, pl.Float64) and c != \"date_id\"]\ndf = train_df.select([pl.col(c).cast(pl.Float64) for c in numeric_cols]).to_pandas()\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\n\nrows, cols = 4, 2\nfig, axes = plt.subplots(rows, cols, figsize=(11, 11), constrained_layout=True)\naxes = axes.ravel()\n\nsns.histplot(df[\"forward_returns\"].dropna(), bins=50, kde=True, color=\"#2E86AB\", ax=axes[0])\naxes[0].set_title(\"Forward Return Distribution\")\naxes[0].set_xlabel(\"\")\naxes[0].set_ylabel(\"\")\n\nsns.scatterplot(\n    x=df[\"risk_free_rate\"],\n    y=df[\"market_forward_excess_returns\"],\n    alpha=0.45, s=12, color=\"#1f77b4\", ax=axes[1]\n)\naxes[1].set_title(\"Risk-Free vs Market Excess\")\naxes[1].set_xlabel(\"\")\naxes[1].set_ylabel(\"\")\n\nif \"date_id\" in train_df.columns:\n    time_x = train_df[\"date_id\"].to_numpy()\n    time_y = train_df[\"forward_returns\"].fill_null(0).to_numpy()\n    axes[2].plot(time_x, time_y, linewidth=0.7, color=\"#4C72B0\")\nelse:\n    axes[2].plot(df.index, df[\"forward_returns\"].fillna(0), linewidth=0.7, color=\"#4C72B0\")\naxes[2].set_title(\"Forward Returns Over Time\")\naxes[2].set_xlabel(\"\")\naxes[2].set_ylabel(\"\")\n\nif \"forward_returns\" in df.columns:\n    corrs = df.corr()[\"forward_returns\"].drop(\"forward_returns\") \\\n        .sort_values(key=lambda x: x.abs(), ascending=False).head(10)\nelse:\n    corrs = pd.Series(dtype=float)\n\nsns.barplot(x=corrs.values, y=corrs.index, palette=\"Spectral\", ax=axes[3])\naxes[3].set_title(\"Top 10 Features by Corr with Forward Return\")\naxes[3].set_xlabel(\"Correlation\")\naxes[3].set_ylabel(\"\")\n\nnulls = [\n    train_df.select(pl.col(c).null_count()).item() / train_df.height * 100\n    for c in train_df.columns\n]\nnull_df = pl.DataFrame({\"column\": train_df.columns, \"null_pct\": nulls}) \\\n    .sort(\"null_pct\", descending=True).to_pandas()\nsns.barplot(data=null_df.head(10), x=\"null_pct\", y=\"column\", palette=\"rocket\", ax=axes[4])\naxes[4].set_title(\"Top 10 Features by Missing %\")\naxes[4].set_xlabel(\"Missing (%)\")\naxes[4].set_ylabel(\"\")\n\ntop_features = corrs.index[:6].tolist()\nif top_features:\n    sns.boxplot(data=df[top_features], orient=\"h\", fliersize=1, palette=\"vlag\", ax=axes[5])\naxes[5].set_title(\"Outlier Check — Top Features\")\naxes[5].set_xlabel(\"\")\naxes[5].set_ylabel(\"\")\n\nif top_features:\n    sns.violinplot(data=df[top_features], orient=\"h\", scale=\"width\",\n                   inner=\"quartile\", palette=\"muted\", ax=axes[6])\naxes[6].set_title(\"Violin — Top Feature Distributions\")\naxes[6].set_xlabel(\"\")\naxes[6].set_ylabel(\"\")\n\ntop_corr_features = corrs.index[:3].tolist()\nif top_corr_features:\n    df_time = train_df.select([\"date_id\"] + [pl.col(c).cast(pl.Float64) for c in top_corr_features]).to_pandas()\n    df_time = df_time.groupby(\"date_id\").mean().rolling(window=10).std()\n    df_time.plot(ax=axes[7], linewidth=0.9)\n    axes[7].legend(top_corr_features, fontsize=6, loc=\"upper right\")\naxes[7].set_title(\"Rolling Volatility (10-Day)\")\naxes[7].set_xlabel(\"\")\naxes[7].set_ylabel(\"\")\n\nfig.suptitle(\"Key Insights for Feature Engineering\", fontsize=9, weight=\"bold\", y=1.02)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:34:59.241267Z","iopub.execute_input":"2025-11-06T20:34:59.241755Z","iopub.status.idle":"2025-11-06T20:35:03.546842Z","shell.execute_reply.started":"2025-11-06T20:34:59.241717Z","shell.execute_reply":"2025-11-06T20:35:03.545227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modeling using Simple FF-NN Approach","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nclass TorchNN(torch.nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super().__init__()\n        self.linear = torch.nn.Linear(num_inputs, num_outputs)\n\n    def forward(self, x):\n        logits = self.linear(x)\n        return logits * 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:13.368785Z","iopub.execute_input":"2025-11-06T21:12:13.370005Z","iopub.status.idle":"2025-11-06T21:12:13.375979Z","shell.execute_reply.started":"2025-11-06T21:12:13.36996Z","shell.execute_reply":"2025-11-06T21:12:13.374854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Preparation","metadata":{}},{"cell_type":"code","source":"train_df = train_df.with_columns([\n    pl.col(\"forward_returns\").shift(1).alias(\"lagged_forward_returns\")\n])\ntrain_df = train_df.with_columns([\n    pl.col(\"risk_free_rate\").shift(1).alias(\"lagged_risk_free_rate\")\n])\ntrain_df = train_df.with_columns([\n    pl.col(\"market_forward_excess_returns\").shift(1).alias(\"lagged_market_forward_excess_returns\")\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:57:59.188076Z","iopub.execute_input":"2025-11-06T20:57:59.188357Z","iopub.status.idle":"2025-11-06T20:57:59.19554Z","shell.execute_reply.started":"2025-11-06T20:57:59.18834Z","shell.execute_reply":"2025-11-06T20:57:59.194486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_df = train_df.drop(\"risk_free_rate\")\ntrain_df = train_df.drop(\"market_forward_excess_returns\")\ntrain_df = train_df.rename({\"forward_returns\": \"label\"})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:35:08.731966Z","iopub.execute_input":"2025-11-06T20:35:08.732212Z","iopub.status.idle":"2025-11-06T20:35:08.793805Z","shell.execute_reply.started":"2025-11-06T20:35:08.732193Z","shell.execute_reply":"2025-11-06T20:35:08.792823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train_df.slice(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:35:08.794687Z","iopub.execute_input":"2025-11-06T20:35:08.795037Z","iopub.status.idle":"2025-11-06T20:35:08.819399Z","shell.execute_reply.started":"2025-11-06T20:35:08.795007Z","shell.execute_reply":"2025-11-06T20:35:08.818202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"non_feature_columns = [\"date_id\", \"label\"]\nfeature_columns = [c for c in train_df.columns if c not in set(non_feature_columns)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:35:08.820489Z","iopub.execute_input":"2025-11-06T20:35:08.820837Z","iopub.status.idle":"2025-11-06T20:35:08.831311Z","shell.execute_reply.started":"2025-11-06T20:35:08.820812Z","shell.execute_reply":"2025-11-06T20:35:08.830154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_fraction = 0.2\ntrain_size = train_df.height\nsplit_index= int(train_size * (1 - validation_fraction))\n\ntrain_slice = train_df.slice(0, split_index)\nvalidation_slice = train_df.slice(split_index, train_size - split_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:35:08.832226Z","iopub.execute_input":"2025-11-06T20:35:08.832466Z","iopub.status.idle":"2025-11-06T20:35:08.853805Z","shell.execute_reply.started":"2025-11-06T20:35:08.832447Z","shell.execute_reply":"2025-11-06T20:35:08.852824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nX_train = train_slice.select(feature_columns).to_torch()\ny_train = train_slice.select(\"label\").to_torch()\nX_val = validation_slice.select(feature_columns).to_torch()\ny_val = validation_slice.select(\"label\").to_torch()\n\nX_train = X_train.to(torch.float32).to(device)\ny_train = y_train.to(torch.float32).unsqueeze(1).to(device)\nX_val = X_val.to(torch.float32).to(device)\ny_val = y_val.to(torch.float32).unsqueeze(1).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:35:08.85488Z","iopub.execute_input":"2025-11-06T20:35:08.855193Z","iopub.status.idle":"2025-11-06T20:35:08.935422Z","shell.execute_reply.started":"2025-11-06T20:35:08.855172Z","shell.execute_reply":"2025-11-06T20:35:08.934596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nbatch_size=8\ntrain_ds = TensorDataset(X_train, y_train)\nval_ds   = TensorDataset(X_val, y_val)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalidation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:35:08.936318Z","iopub.execute_input":"2025-11-06T20:35:08.936665Z","iopub.status.idle":"2025-11-06T20:35:08.942678Z","shell.execute_reply.started":"2025-11-06T20:35:08.936633Z","shell.execute_reply":"2025-11-06T20:35:08.941736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn.functional as F\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# model = TorchNN(num_inputs=97, num_outputs=1).to(device)\n# model = model.double()\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # no weight decay for now\n\n# n_epochs = 25\n# best_val_mse = float(\"inf\")\n\n# for epoch in range(1, n_epochs + 1):\n#     model.train()\n#     train_losses = []\n    \n#     for xb, yb in train_loader:\n#         xb = xb.to(device).double()\n#         yb = yb.to(device).double().view(-1, 1)\n\n#         optimizer.zero_grad()\n#         preds = model(xb)\n#         loss = F.mse_loss(preds, yb)\n#         loss.backward()\n#         optimizer.step()\n\n#         train_losses.append(loss.item())\n\n#     avg_train = sum(train_losses) / len(train_losses)\n\n#     model.eval()\n#     val_losses = []\n#     with torch.no_grad():\n#         for xb, yb in validation_loader:\n#             xb = xb.to(device).double()\n#             yb = yb.to(device).double().view(-1, 1)\n#             preds = model(xb)\n#             val_losses.append(F.mse_loss(preds, yb).item())\n\n#     avg_val = sum(val_losses) / len(val_losses)\n\n#     print(f\"Epoch {epoch:02d} | train_mse: {avg_train:.6f} | val_mse: {avg_val:.10f}\")\n\n#     if avg_val < best_val_mse:\n#         best_val_mse = avg_val\n#         torch.save(model.state_dict(), \"hull-tactical-market-prediction-model-v1.pth\")\n#         print(f\"  -> saved best model (val_mse={best_val_mse:.10f})\")\n\n# print(f\"\\nTraining complete. Best val_mse: {best_val_mse:.10f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:35:08.943973Z","iopub.execute_input":"2025-11-06T20:35:08.944307Z","iopub.status.idle":"2025-11-06T20:35:08.967793Z","shell.execute_reply.started":"2025-11-06T20:35:08.944277Z","shell.execute_reply":"2025-11-06T20:35:08.966511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = TorchNN(num_inputs=97, num_outputs=1).to(device).double()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\nbest_val_sharpe = -1e9\npatience = 6\nno_improve = 0\ntrans_cost_coeff = 0.02\nn_epochs = 1\n\nfor epoch in range(1, n_epochs+1):\n    model.train()\n    train_losses = []\n    # optionally set a larger batch_size in your DataLoader\n    for xb, yb in train_loader:\n        xb = xb.to(device).double()\n        yb = yb.to(device).double().view(-1,1)\n        preds = model(xb)\n        # simpler, more stable base loss: maximize expected return\n        base_loss = -torch.mean(preds * yb)\n        # penalize large signals / turnover (approx)\n        loss = base_loss + trans_cost_coeff * torch.mean(torch.abs(preds))\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        train_losses.append(loss.item())\n\n    # validation: compute val_sharpe across all batches (no grad)\n    model.eval()\n    all_val_rets = []\n    with torch.no_grad():\n        for xb_v, yb_v in validation_loader:\n            xb_v = xb_v.to(device).double()\n            yb_v = yb_v.to(device).double().view(-1,1)\n            p = model(xb_v)\n            all_val_rets.append((p * yb_v).cpu())\n    all_val_rets = torch.cat(all_val_rets, dim=0)\n    val_mean = float(torch.mean(all_val_rets))\n    val_std = float(torch.std(all_val_rets) + 1e-8)\n    val_sharpe = val_mean / val_std\n\n    avg_train_loss = sum(train_losses) / len(train_losses)\n    print(f\"Epoch {epoch:02d} | train_loss: {avg_train_loss:.6f} | val_sharpe: {val_sharpe:.6f}\")\n\n    if val_sharpe > best_val_sharpe:\n        best_val_sharpe = val_sharpe\n        no_improve = 0\n        torch.save(model.state_dict(), \"hull-tactical-market-prediction-model-v1.pth\")\n        print(f\" -> saved best model (val_sharpe={best_val_sharpe:.6f})\")\n    else:\n        no_improve += 1\n        if no_improve >= patience:\n            print(\"Early stopping (no improvement).\")\n            break\n\nprint(\"Done. Best val_sharpe:\", best_val_sharpe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:19.169044Z","iopub.execute_input":"2025-11-06T21:12:19.169419Z","iopub.status.idle":"2025-11-06T21:12:27.474233Z","shell.execute_reply.started":"2025-11-06T21:12:19.169395Z","shell.execute_reply":"2025-11-06T21:12:27.473255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission Inference","metadata":{}},{"cell_type":"code","source":"import os\nimport kaggle_evaluation.default_inference_server\n\nimport pandas as pd\nimport polars as pl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:11:46.808644Z","iopub.execute_input":"2025-11-06T21:11:46.80905Z","iopub.status.idle":"2025-11-06T21:11:46.814893Z","shell.execute_reply.started":"2025-11-06T21:11:46.809025Z","shell.execute_reply":"2025-11-06T21:11:46.813457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport polars as pl\nimport numpy as np\n\ncheckpoint = torch.load(\"hull-tactical-market-prediction-model-v1.pth\", map_location=device)\nmodel_local = TorchNN(num_inputs=97, num_outputs=1).to(device)\nmodel_local = model_local.double()     \nmodel_local.load_state_dict(checkpoint)\nmodel_local.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:35.188653Z","iopub.execute_input":"2025-11-06T21:12:35.189012Z","iopub.status.idle":"2025-11-06T21:12:35.198205Z","shell.execute_reply.started":"2025-11-06T21:12:35.188984Z","shell.execute_reply":"2025-11-06T21:12:35.19743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    Inference function for Hull Tactical Market Prediction.\n    - Safely drops 'date_id' and 'is_scored' columns if present.\n    - Converts Polars DataFrame to torch tensor.\n    - Returns a float signal in range [0, 2].\n    \"\"\"\n    # Drop unwanted columns safely\n    drop_cols = [c for c in (\"date_id\", \"is_scored\") if c in test.columns]\n    if drop_cols:\n        test = test.drop(drop_cols)\n\n    # Convert features to numpy -> tensor\n    X = test.to_numpy()\n    X = torch.from_numpy(X).double().to(device)\n\n    # Run model inference\n    model_local.eval()\n    with torch.no_grad():\n        pred = model_local(X)\n\n    # Convert tensor -> float\n    signal = float(pred.cpu().numpy().squeeze())\n    \n    signal = np.clip(signal, 0.0, 2.0)\n\n    # Prediction Debug\n    print(\"DEBUG SIGNAL: \", signal)\n    return float(signal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:35.947574Z","iopub.execute_input":"2025-11-06T21:12:35.947905Z","iopub.status.idle":"2025-11-06T21:12:35.954477Z","shell.execute_reply.started":"2025-11-06T21:12:35.947884Z","shell.execute_reply":"2025-11-06T21:12:35.953498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:37.767596Z","iopub.execute_input":"2025-11-06T21:12:37.768606Z","iopub.status.idle":"2025-11-06T21:12:37.910039Z","shell.execute_reply.started":"2025-11-06T21:12:37.768579Z","shell.execute_reply":"2025-11-06T21:12:37.909065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"read_submission = pd.read_parquet('/kaggle/working/submission.parquet')\nread_submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:39.727423Z","iopub.execute_input":"2025-11-06T21:12:39.728208Z","iopub.status.idle":"2025-11-06T21:12:39.74149Z","shell.execute_reply.started":"2025-11-06T21:12:39.728182Z","shell.execute_reply":"2025-11-06T21:12:39.740499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas.api.types\n\nMIN_INVESTMENT = 0\nMAX_INVESTMENT = 2\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n\n    This metric penalizes strategies that take on significantly more volatility\n    than the underlying market.\n\n    Returns:\n        float: The calculated adjusted Sharpe ratio.\n    \"\"\"\n\n    if not pandas.api.types.is_numeric_dtype(submission['prediction']):\n        raise ParticipantVisibleError('Predictions must be numeric')\n\n    solution = solution\n    solution['position'] = submission['prediction']\n\n    if solution['position'].max() > MAX_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n    if solution['position'].min() < MIN_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n\n    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n\n    # Calculate strategy's Sharpe ratio\n    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n    strategy_std = solution['strategy_returns'].std()\n\n    trading_days_per_yr = 252\n    if strategy_std == 0:\n        raise ParticipantVisibleError('Division by zero, strategy std is zero')\n    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n\n    # Calculate market return and volatility\n    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n    market_std = solution['forward_returns'].std()\n\n    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n\n    if market_volatility == 0:\n        raise ParticipantVisibleError('Division by zero, market std is zero')\n\n    # Calculate the volatility penalty\n    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n    vol_penalty = 1 + excess_vol\n\n    # Calculate the return penalty\n    return_gap = max(\n        0,\n        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n    )\n    return_penalty = 1 + (return_gap**2) / 100\n\n    # Adjust the Sharpe ratio by the volatility and return penalty\n    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n    return min(float(adjusted_sharpe), 1_000_000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:41.148573Z","iopub.execute_input":"2025-11-06T21:12:41.149561Z","iopub.status.idle":"2025-11-06T21:12:41.161084Z","shell.execute_reply.started":"2025-11-06T21:12:41.149531Z","shell.execute_reply":"2025-11-06T21:12:41.159921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 1) Load your submission\nsubmission = pd.read_parquet('/kaggle/working/submission.parquet')  # has: date_id, prediction\n\n# 2) Build the \"solution\" for the same dates (from train.csv here)\n#    If your ground-truth lives elsewhere, load it from there instead.\nsolution_raw = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv',\n                           usecols=['date_id', 'forward_returns', 'risk_free_rate'])\n\n# 3) Keep only the rows that appear in the submission and align order\nsolution = (solution_raw\n            .merge(submission[['date_id']], on='date_id', how='inner')\n            .sort_values('date_id')\n            .reset_index(drop=True))\n\nsubmission = submission.sort_values('date_id').reset_index(drop=True)\n\n# 4) (Optional) sanity checks\nassert len(solution) == len(submission), \"Solution and submission lengths differ.\"\nassert solution['date_id'].equals(submission['date_id']), \"date_id order mismatch.\"\nassert pd.api.types.is_numeric_dtype(submission['prediction']), \"prediction must be numeric.\"\n\n# 5) Call the scorer. Note: row_id_column_name isn't used inside the function,\n#    but pass 'date_id' to match the interface.\nscore_value = score(solution[['forward_returns', 'risk_free_rate']].copy(),\n                    submission[['prediction']].copy(),\n                    row_id_column_name='date_id')\n\nprint(\"Adjusted Sharpe (custom metric):\", score_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T21:12:42.967846Z","iopub.execute_input":"2025-11-06T21:12:42.968759Z","iopub.status.idle":"2025-11-06T21:12:43.11568Z","shell.execute_reply.started":"2025-11-06T21:12:42.968723Z","shell.execute_reply":"2025-11-06T21:12:43.114469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}