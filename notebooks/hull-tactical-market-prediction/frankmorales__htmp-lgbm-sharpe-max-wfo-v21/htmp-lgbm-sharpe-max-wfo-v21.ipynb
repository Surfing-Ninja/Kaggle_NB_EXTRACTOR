{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Rationale for the Final Notebook Name\n\nThis name captures all the successful elements and strategic decisions made during your optimization process:\n\n* **`HTMP`**: The required acronym for the **Hull Tactical Market Prediction** competition.\n* **`LGBM`**: Indicates the high-performing **LightGBM** algorithm, which proved superior to hybrid models.\n* **`Sharpe_Max`**: Highlights the core objective of maximizing the **volatility-adjusted Sharpe Ratio** through the carefully chosen $0.55$ allocation factor.\n* **`WFO`**: Acknowledges the underlying **Walk Forward Optimization principle** you adhered to: using the most recent 6,000 days of data and tuning for generalization, which is the necessary philosophy for time-series forecasting.\n* **`V21`**: Denotes the **final, stable version** that incorporates all successful hyperparameter tuning for safety ($\\text{n\\_estimators}=150$) and performance ($\\text{LR}=0.07, \\text{MD}=9$).","metadata":{}},{"cell_type":"markdown","source":"A single, simple mathematical formula cannot represent the time required to train your LightGBM model. Instead, it's determined by a practical rule-of-thumb involving the model's complexity and your hardware's speed.\n\nIn the context of our Kaggle submission, the critical \"formula\" must follow is the **Training Time vs. Estimator Count Rule**:\n\n## â° Time Optimization Formula\n\nTraining time for boosting models like LightGBM scales roughly linearly with the number of estimators ($\\text{n\\_estimators}$) and the amount of data.\n\n$$\\text{Time}_{\\text{Training}} \\approx k \\times \\text{n\\_estimators} \\times N_{\\text{samples}}$$\n\nWhere:\n* **$\\text{Time}_{\\text{Training}}$:** The time taken for Phase I (must be $\\mathbf{\\leq 900 \\text{ seconds}}$).\n* **$k$**: A constant related to your hardware speed and the complexity of your features and depth.\n* **$\\text{n\\_estimators}$**: The number of trees (your tuning parameter).\n* **$N_{\\text{samples}}$**: The number of training rows (fixed at 6,000 in your code).\n\n### Your Safety Rule\n\nBased on your executions, the constraint is:\n\n$$\\text{n\\_estimators} \\le 300$$\n\n* **$\\text{n\\_estimators} = 300$**: Consumed $\\approx 900$ seconds, putting you right at the edge of the time limit.\n* **$\\text{n\\_estimators} = 990$**: Caused the time limit to be exceeded, leading to submission failure.\n\nTo ensure a valid submission, you should stick to $\\text{n\\_estimators} = 300$ as the maximum stable value.","metadata":{}},{"cell_type":"markdown","source":"Not exactly. The model was trained on $\\mathbf{6,000}$ rows of data, which corresponds to approximately $\\mathbf{16.43}$ calendar years, but represents close to **24 years of trading data**.\n\nHere's the breakdown of why the larger number of years is more relevant for the trading competition:\n\n---\n\n## ðŸ“… Trading vs. Calendar Years\n\nWhen modelling financial data, the number of **trading days** is the standard measure, as it represents the number of independent data points (daily decisions) the model learns from.\n\n* **Calendar Years:** $\\frac{6,000 \\text{ days}}{365.25 \\text{ days/year}} \\approx \\mathbf{16.43 \\text{ calendar years}}$\n* **Trading Years:** $\\frac{6,000 \\text{ days}}{252 \\text{ trading days/year}} \\approx \\mathbf{23.81 \\text{ trading years}}$\n\nThe `df_train.tail(6000)` command selects the last 6,000 **trading days** present in the file. Therefore, the LightGBM model was effectively trained on a dataset spanning nearly **24 years of market history**.\n\nThis large window (24 years) is used to capture sufficient **market regime diversity** (including multiple economic cycles and recessions) to ensure the model generalizes well to unseen data during the forecasting phase.","metadata":{}},{"cell_type":"markdown","source":"Complete final solution, which is split into two cells: the **Exploratory Data Analysis (EDA) Cell** and the **Main Code Cell (LGBM-Dominant Ensemble)**. This structure maximizes code stability and time efficiency for your final submission.\n\n---\n\n## 1. Code Cell 1: Exploratory Data Analysis (EDA)\n\nThis cell performs initial data investigation and generates visualizations critical for understanding the nature of the target variable ($\\text{market\\_forward\\_excess\\_returns}$).\n\n* **Purpose:** To visually confirm that the data is prepared correctly, identify key distributional properties, and spot highly correlated features before training the model.\n* **Charts Generated:**\n    1.  **Distribution of Returns:** A histogram with a Kernel Density Estimate ($\\text{KDE}$) shows how often different return values occur. In finance, this typically confirms that returns are centered near zero but have \"fat tails,\" indicating rare, extreme events (volatility).\n    2.  **Time Series of Returns:** Plots the last 2000 days of returns against time. This helps identify market regimes, trends, and periods of high/low volatility clustering, which is essential context for the $\\text{LGBM}$ model.\n    3.  **Top Correlated Features:** A bar plot shows the top 10 features with the highest **absolute Spearman correlation** to the target. This identifies features most linearly and non-linearly related to the prediction goal, informing why the $\\text{LGBM}$ model might find a strong signal.\n\n---\n\n## 2. Code Cell 2: Main Model Training and Inference\n\nThis cell contains the **LGBM-Dominant Ensemble**, which is the final, time-optimized prediction system.\n\n### Phase I: Model Training (LGBM)\n\nThe goal here is to train the most stable model possible within the strict 900-second time limit.\n\n* **Data Prep and Feature Set (Lines 63-75):** The script loads data, identifies $\\mathbf{86}$ available features, converts columns, and focuses training on the most recent $\\mathbf{6,000}$ rows for relevance.\n* **Time Optimization (WFO Principle) (Lines 80-92):** The hyperparameters are manually tuned for generalization, mimicking the stability required by Walk Forward Optimization (WFO), which is crucial for a financial model:\n    * **$\\text{n\\_estimators}=300$**: This is the **maximum safe value** that fits within the $\\approx 900$-second time limit.\n    * **$\\text{learning\\_rate}=0.03$ and $\\text{max\\_depth}=7$**: These conservative settings prevent the model from overfitting the training data's noise, which is necessary to achieve a high score on unseen test data.\n* **Metric Report (Lines 95-99):** The script reports its training success:\n    * $\\text{Training R}^2$: **$0.5146$**. The $\\text{LGBM}$ successfully explains over half the variance in the training data, confirming its strong predictive power.\n* **Asset Management (Lines 101-104):** The trained $\\text{LGBM}$ model is saved to disk. All logic related to the unstable $\\text{CNN-LSTM}$ model is deliberately excluded, ensuring the script avoids the fatal $\\text{IO}$ error during inference.\n\n### Phase II: Submission Inference (The Prediction Loop)\n\nThe `predict(test: pl.DataFrame)` function executes every time the competition server requests a prediction.\n\n| Code Section | Model Component | Weights | Rationale |\n| :--- | :--- | :--- | :--- |\n| **Model Loading** | **LGBM** | N/A | Uses a **lazy loading** block to load the $\\text{LGBM}$ model *only* on the first call, avoiding the startup timeout risk. |\n| **Component A** | $\\text{LGBM Prediction}$ | $\\mathbf{0.8}$ | The primary prediction is generated by the highly optimized $\\text{LGBM}$ model, based on your historical best scores. |\n| **Component C** | $\\text{Linear Proxy (M1)}$ | $\\mathbf{0.2}$ | A simple prediction based on the $\\text{M1}$ momentum feature provides a stable baseline floor to the ensemble. |\n| **Blending** | $\\text{Raw Prediction}$ | $1.0$ | The predictions are summed by their weights. The $\\text{LSTM}$ component's weight is $\\mathbf{0.0}$ (disabled). |\n| **Final Allocation** | $\\text{Scaling/Clipping}$ | $\\mathbf{0.55}$ | The raw prediction is multiplied by $\\mathbf{0.55}$. This **Sharpe-optimizing scaling factor** controls the portfolio's exposure to volatility (risk) while maximizing the potential for a high return. The output is clipped to the competition's range of $[0.0, 2.0]$. |","metadata":{}},{"cell_type":"code","source":"# --- EDA CELL: Exploratory Data Analysis for Financial Time Series ---\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom pathlib import Path\nimport warnings\nimport pandas as pd # Ensure pandas is imported if not already\n\n# Suppress pandas warnings in this block\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n\nKAGGLE_INPUT_PATH = '/kaggle/input/hull-tactical-market-prediction/'\nTARGET_COL = 'market_forward_excess_returns'\n\nprint(\"--- EDA PHASE: Analyzing Market Returns ---\")\ntry:\n    df_eda = pd.read_csv(Path(KAGGLE_INPUT_PATH) / 'train.csv')\n    \n    # 1. Convert Target to Numeric\n    df_eda[TARGET_COL] = pd.to_numeric(df_eda[TARGET_COL], errors='coerce')\n    \n    # --- CHART 1: Distribution of Returns ---\n    # Shows how centred returns are around zero (Volatility vs. Drift)\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df_eda[TARGET_COL].dropna(), kde=True, bins=100)\n    plt.title('1. Distribution of Market Forward Excess Returns (Target)')\n    plt.show()\n\n    # --- CHART 2: Time Series of Returns ---\n    # Shows market regimes, volatility clustering, and stability\n    plt.figure(figsize=(16, 6))\n    # Using 'date_id' as a proxy for time index\n    df_eda.set_index('date_id')[TARGET_COL].tail(2000).plot(linewidth=0.5) \n    plt.title('2. Time Series of Excess Returns (Last 2000 Days)')\n    plt.ylabel(TARGET_COL)\n    plt.show()\n\n    # --- CHART 3: Top Correlated Features ---\n    # Identifies the features most related to the target variable\n    correlation_data = df_eda.corr(method='spearman')[TARGET_COL].abs().sort_values(ascending=False).head(11)\n    # Remove the target column itself from the list\n    correlation_data = correlation_data.drop(TARGET_COL, errors='ignore') \n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=correlation_data.index, y=correlation_data.values)\n    plt.title('3. Top 10 Features by Absolute Spearman Correlation with Target')\n    plt.ylabel('Absolute Spearman Correlation')\n    plt.show()\n    \nexcept Exception as e:\n    print(f\"EDA ERROR: Could not complete EDA visuals. {e}\")\n\n# Clean up the dataframe to save memory before model training starts\ndel df_eda \n\nprint(\"--- EDA Complete. Proceeding to Model Training ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T00:25:52.818251Z","iopub.execute_input":"2025-10-30T00:25:52.818545Z","iopub.status.idle":"2025-10-30T00:25:58.976397Z","shell.execute_reply.started":"2025-10-30T00:25:52.81852Z","shell.execute_reply":"2025-10-30T00:25:58.975159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I'll update the table, adding the clarification that the aggressive tuning for $\\mathbf{R^2}$ was reversed to implement necessary regularization, which now means **overfitting is under control**.\n\n---\n\n## ðŸ† Final MVP Hyperparameter Rationale (Generalization Focus)\n\n| Hyperparameter | Value | Role in the Strategy |\n| :--- | :--- | :--- |\n| **$\\mathbf{n\\_estimators}$** | **150** | **Safety/Execution MVP:** This is the most critical parameter, as its low value ensures the training time stays well under the $\\mathbf{900\\text{-second}}$ limit, preventing submission failure. **This parameter remains a non-negotiable safety lock.** |\n| **$\\mathbf{\\text{learning\\_rate}}$** | **0.07** | **Generalization MVP:** This value was dialed back from aggressive levels (0.18) to **control overfitting**. It introduces necessary regularization (bias) to prevent the model from memorizing noise, maximizing performance on the **unseen private test set**. |\n| **$\\mathbf{\\text{max\\_depth}}$** | **9** | **Complexity MVP:** This value was reverted from 10 to reduce model complexity, further **controlling overfitting**. It allows each tree to capture robust, generalized feature interactions efficiently. |\n\nThis final configuration ensures the model is both **compliant** (it runs and is scored) and **generalized** (overfitting is controlled, maximizing the chance of a high score on future data).","metadata":{}},{"cell_type":"code","source":"# HULL TACTICAL - FINAL v21: LGBM OPTIMIZATION (GENERALIZATION & STABILITY)\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport polars as pl\nimport joblib \nimport lightgbm as lgb\nfrom pathlib import Path\nfrom kaggle_evaluation.default_inference_server import DefaultInferenceServer\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport warnings\nimport logging\n\n# --- TENSORFLOW/WARNING SUPPRESSION ---\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n# ------------------------------------------------------------------------------------------------\n\n# --- ðŸŽ¯ CUSTOMIZABLE CONFIGURATION ---\nKAGGLE_INPUT_PATH = '/kaggle/input/hull-tactical-market-prediction/'\n#MODEL_SAVE_PATH = '/kaggle/working/lgbm_ensemble_model.pkl' \nMODEL_SAVE_PATH = '/tmp/lgbm_ensemble_model.pkl'\nTARGET_COL = 'market_forward_excess_returns'\n\n# 1. TIME-CONSTRAINT PARAMETER (MAX SAFE VALUE)\nN_ESTIMATORS = 150 \n# 2. TRAINING DATA WINDOW SIZE\nTRAINING_WINDOW_SIZE = 6000\n# 3. SHARPE-OPTIMIZED SCALING FACTOR \nSCALING_FACTOR = 0.55 \n# 4. PERFORMANCE TUNING: REVERTED TO STABLE/GENERALIZED VALUE\nLEARNING_RATE = 0.7 \n# 5. PERFORMANCE TUNING: REVERTED TO STABLE/GENERALIZED VALUE\nMAX_DEPTH = 9  \n# 6. VOLATILITY FEATURE NAME\nVOLATILITY_FEATURE = 'VOL_20D'\n# --------------------------------------\n\n# List all possible feature columns \nALL_POSSIBLE_FEATURE_NAMES = [\n    'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E20',\n    'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15',\n    'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', \n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M19', 'M20',\n    'I1', 'I2', 'I3', 'I4', 'I5', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', \n    'MOM1', 'MOM2', 'MOM3', 'MOM4', 'MOM5', 'MOM6', 'MOM7', 'MOM8', 'MOM9', 'MOM10', \n    'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15',\n    VOLATILITY_FEATURE \n]\nEXCLUDE_COLS = [\n    'date_id', TARGET_COL, 'forward_returns', 'risk_free_rate', 'lagged_forward_returns', 'is_scored' \n]\nGLOBAL_TRAINING_FEATURE_COLS = []\n\n# --- GLOBAL INFERENCE STATE ---\nGLOBAL_MODEL = None \nGLOBAL_INFERENCE_FEATURE_COLS = [] \nMODEL_LOADED = False\nPREDICTION_COUNT = 0\n\n\n# --- TRAINING PHASE (LGBM Component) ---\nprint(\"--- PHASE I: MODEL TRAINING (LGBM Component) ---\")\ntry:\n    df_train = pd.read_csv(Path(KAGGLE_INPUT_PATH) / 'train.csv')\n    \n    # --- VOLATILITY FEATURE ENGINEERING ---\n    df_train[VOLATILITY_FEATURE] = df_train[TARGET_COL].rolling(window=20).std()\n    # --------------------------------------\n\n    current_feature_cols = [col for col in ALL_POSSIBLE_FEATURE_NAMES if col in df_train.columns and col not in EXCLUDE_COLS]\n    GLOBAL_TRAINING_FEATURE_COLS = current_feature_cols\n    \n    for col in GLOBAL_TRAINING_FEATURE_COLS:\n        df_train[col] = pd.to_numeric(df_train[col], errors='coerce')\n    \n    df_train.dropna(subset=[TARGET_COL, VOLATILITY_FEATURE], inplace=True) \n    df_train = df_train.tail(TRAINING_WINDOW_SIZE).copy() \n\n    X = df_train[GLOBAL_TRAINING_FEATURE_COLS]\n    y = df_train[TARGET_COL]\n\n    # --- HYPERPARAMETER ADJUSTMENT FOR GENERALIZATION (WFO Principle) ---\n    lgbm = lgb.LGBMRegressor(\n        objective='regression', \n        metric='rmse', \n        n_estimators=N_ESTIMATORS,        \n        learning_rate=LEARNING_RATE,      # GENERALIZATION VALUE\n        max_depth=MAX_DEPTH,             # GENERALIZATION VALUE\n        n_jobs=-1, \n        random_state=42\n    )\n    \n    lgbm.set_params(verbose=-1) \n    print(f\"Training LightGBM model on {len(X.columns)} features...\") \n    \n    # --- FIT WITHOUT EARLY STOPPING ---\n    lgbm.fit(X, y) \n    # ----------------------------------\n    \n    # --- METRIC CALCULATION AND DISPLAY (Use full data for final report) ---\n    y_pred = lgbm.predict(X) \n    train_rmse = np.sqrt(mean_squared_error(y, y_pred))\n    train_r2 = r2_score(y, y_pred)\n    print(f\"   LGBM Training RMSE (Full Data): {train_rmse:.4f}\")\n    print(f\"   LGBM Training RÂ² (Full Data): {train_r2:.4f}\")\n    # ----------------------------------------------------------------------\n\n    joblib.dump({\n        'model': lgbm, \n        'features': GLOBAL_TRAINING_FEATURE_COLS \n    }, MODEL_SAVE_PATH)\n    print(f\"Trained LGBM model saved to: {MODEL_SAVE_PATH}\")\n\nexcept Exception as e:\n    print(f\"FATAL ERROR during TRAINING PHASE: {e}\")\n    # Save a safe fallback model if training fails\n    class DummyModel:\n        def predict(self, X): \n            return np.array([0.0] * len(X))\n            \n    joblib.dump({'model': DummyModel(), 'features': []}, MODEL_SAVE_PATH)\n    \n# --- END TRAINING PHASE ---\n\n\n# --- INFERENCE PHASE (LGBM-ONLY) ---\n\nprint(\"\\n--- PHASE II: SUBMISSION INFERENCE SETUP (LGBM-ONLY FAILSATE) ---\")\n\ndef predict(test: pl.DataFrame) -> float:\n    \n    global MODEL_LOADED, GLOBAL_MODEL, GLOBAL_INFERENCE_FEATURE_COLS, PREDICTION_COUNT\n    \n    if not MODEL_LOADED:\n        try:\n            loaded_lgbm = joblib.load(MODEL_SAVE_PATH) \n            GLOBAL_MODEL = loaded_lgbm['model']\n            GLOBAL_INFERENCE_FEATURE_COLS = loaded_lgbm['features']\n            \n            MODEL_LOADED = True\n        except Exception as e:\n            print(f\"FATAL ERROR during model loading: {e}. Returning constant 1.0 (Safe Failsafe).\")\n            return 1.0 \n\n    # --- INFERENCE DATA PREP ---\n    df_test = test.to_pandas().iloc[0:1].copy()\n    \n    # 1. Prepare LGBM Input (Point-in-time)\n    X_lgbm = df_test.reindex(columns=GLOBAL_INFERENCE_FEATURE_COLS, fill_value=np.nan)\n    for col in X_lgbm.columns:\n        X_lgbm[col] = pd.to_numeric(X_lgbm[col], errors='coerce').fillna(0.0) \n\n    # --- PREDICTION (LGBM-ONLY) ---\n    \n    if X_lgbm.shape[1] > 0:\n        pred_lgbm = GLOBAL_MODEL.predict(X_lgbm)[0]\n    else:\n        pred_lgbm = 0.0\n\n    # Component C: Simple Linear Prediction (0.2 Weight)\n    pred_linear = df_test['M1'].iloc[0] * 0.0001 if 'M1' in df_test.columns and not pd.isna(df_test['M1'].iloc[0]) else 0.0\n    \n    # 4. Blending (Weighted Average: 0.8 LGBM, 0.0 LSTM, 0.2 Linear)\n    WEIGHTS = [0.8, 0.0, 0.2] \n    raw_prediction = (pred_lgbm * WEIGHTS[0]) + (0.0 * WEIGHTS[1]) + (pred_linear * WEIGHTS[2])\n\n    \n    # --- Allocation Sizing (Using SCALING_FACTOR = 0.55) ---\n    predicted_allocation = 1.0 + (raw_prediction * SCALING_FACTOR)\n    final_allocation = np.clip(predicted_allocation, 0.0, 2.0)\n\n    PREDICTION_COUNT += 1\n    if PREDICTION_COUNT % 50 == 0:\n        print(f\"--- LGBM Progress: Step {PREDICTION_COUNT} ---\")\n\n    return float(final_allocation)\n\n# --- 3. RUN THE INFERENCE SERVER ---\n\ninference_server = DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(\"Running in RERUN mode...\")\n    inference_server.serve()\nelse:\n    print(\"Running local gateway for testing...\")\n    inference_server.run_local_gateway((KAGGLE_INPUT_PATH,))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T00:51:34.534268Z","iopub.execute_input":"2025-10-30T00:51:34.534603Z","iopub.status.idle":"2025-10-30T00:51:36.959831Z","shell.execute_reply.started":"2025-10-30T00:51:34.534579Z","shell.execute_reply":"2025-10-30T00:51:36.9588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LR 0.07 SCORE 0.484\n\nLR 0.18 SCORE 0.502 \n\nLR 0.58 SCORE 0.527 LGBM Training RÂ² (Full Data): 0.9908\n\nLR 0.7 SCORE        LGBM Training RÂ² (Full Data): 0.9952","metadata":{}}]}