{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport polars as pl\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport os,gc\nfrom tqdm.auto import tqdm\nimport pickle\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport missingno as msno\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.stats.multitest import multipletests\nfrom statsmodels.tools.sm_exceptions import PerfectSeparationError","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:47:54.906313Z","iopub.execute_input":"2025-10-06T20:47:54.906912Z","iopub.status.idle":"2025-10-06T20:47:56.081523Z","shell.execute_reply.started":"2025-10-06T20:47:54.906884Z","shell.execute_reply":"2025-10-06T20:47:56.080812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\npath=\"/kaggle/input/hull-tactical-market-prediction\"\nsamples=[]\nfile_path=f\"{path}/train.csv\"\npart=pd.read_csv(file_path)\nsamples.append(part)\nsample_df=pd.concat(samples,ignore_index=True)\nsample_df.round(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:01.448492Z","iopub.execute_input":"2025-10-06T20:48:01.449147Z","iopub.status.idle":"2025-10-06T20:48:01.692503Z","shell.execute_reply.started":"2025-10-06T20:48:01.449122Z","shell.execute_reply":"2025-10-06T20:48:01.69177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:08.694531Z","iopub.execute_input":"2025-10-06T20:48:08.69484Z","iopub.status.idle":"2025-10-06T20:48:08.700612Z","shell.execute_reply.started":"2025-10-06T20:48:08.694817Z","shell.execute_reply":"2025-10-06T20:48:08.6999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:11.310429Z","iopub.execute_input":"2025-10-06T20:48:11.311228Z","iopub.status.idle":"2025-10-06T20:48:11.326217Z","shell.execute_reply.started":"2025-10-06T20:48:11.311199Z","shell.execute_reply":"2025-10-06T20:48:11.325367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df[\"forward_returns\"].mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:14.055459Z","iopub.execute_input":"2025-10-06T20:48:14.055747Z","iopub.status.idle":"2025-10-06T20:48:14.061568Z","shell.execute_reply.started":"2025-10-06T20:48:14.055725Z","shell.execute_reply":"2025-10-06T20:48:14.060921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rolling_var = sample_df[\"forward_returns\"].rolling(window=5).var()\nplt.figure(figsize=(15,8))\nplt.plot(rolling_var, label=f'Rolling Variance forward returns',color=\"black\")\nplt.title(f'Rolling Variance of forward_returns')\nplt.xlabel('Time')\nplt.ylabel('Variance')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:16.275681Z","iopub.execute_input":"2025-10-06T20:48:16.276274Z","iopub.status.idle":"2025-10-06T20:48:16.528843Z","shell.execute_reply.started":"2025-10-06T20:48:16.276249Z","shell.execute_reply":"2025-10-06T20:48:16.528165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nsample_df['date_id'] = pd.to_datetime(sample_df['date_id'])\nsample_df = sample_df.set_index('date_id', drop=False)\n\nplt.figure(figsize=(12,6))\nplt.plot(sample_df.index, sample_df['forward_returns'], label='Forward Returns',color=\"black\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Forward Returns\")\nplt.title(\"Time Series Plot\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:19.654731Z","iopub.execute_input":"2025-10-06T20:48:19.655453Z","iopub.status.idle":"2025-10-06T20:48:19.972589Z","shell.execute_reply.started":"2025-10-06T20:48:19.655427Z","shell.execute_reply":"2025-10-06T20:48:19.971863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot(sample_df.index,sample_df[\"risk_free_rate\"],label=\"Risk Free Rate\",color=\"black\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Risk Free Rate\")\nplt.title(\"Time series plot\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:22.746378Z","iopub.execute_input":"2025-10-06T20:48:22.746642Z","iopub.status.idle":"2025-10-06T20:48:22.945077Z","shell.execute_reply.started":"2025-10-06T20:48:22.746623Z","shell.execute_reply":"2025-10-06T20:48:22.944398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot(sample_df.index,sample_df[\"market_forward_excess_returns\"],label=\"market_forward_excess_returns\",color=\"black\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Market Forward access returns\")\nplt.title(\"Time series plot\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:27.205611Z","iopub.execute_input":"2025-10-06T20:48:27.205904Z","iopub.status.idle":"2025-10-06T20:48:27.606141Z","shell.execute_reply.started":"2025-10-06T20:48:27.205883Z","shell.execute_reply":"2025-10-06T20:48:27.605328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.infer_freq(sample_df[\"date_id\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:30.585325Z","iopub.execute_input":"2025-10-06T20:48:30.585712Z","iopub.status.idle":"2025-10-06T20:48:30.591854Z","shell.execute_reply.started":"2025-10-06T20:48:30.585683Z","shell.execute_reply":"2025-10-06T20:48:30.591196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.hist(sample_df[\"forward_returns\"],bins=30,color=\"black\",edgecolor=\"lightgrey\")\nplt.title(\"Forward Returns Distribution\")\nplt.xlabel(\"Forward Returns\")\nplt.ylabel(\"frequency\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:38.725629Z","iopub.execute_input":"2025-10-06T20:48:38.725934Z","iopub.status.idle":"2025-10-06T20:48:38.904943Z","shell.execute_reply.started":"2025-10-06T20:48:38.725913Z","shell.execute_reply":"2025-10-06T20:48:38.904337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(sample_df[\"date_id\"],sample_df[\"forward_returns\"],color=\"black\",s=5)\nplt.title(\"Time Series Scatter Plot\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Forward Returns\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:41.675023Z","iopub.execute_input":"2025-10-06T20:48:41.67527Z","iopub.status.idle":"2025-10-06T20:48:41.922604Z","shell.execute_reply.started":"2025-10-06T20:48:41.675254Z","shell.execute_reply":"2025-10-06T20:48:41.921701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(sample_df[\"date_id\"],sample_df[\"risk_free_rate\"],color=\"black\",s=5)\nplt.title(\"Risk Free Rate\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Risk Free Rate\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:46.360641Z","iopub.execute_input":"2025-10-06T20:48:46.361307Z","iopub.status.idle":"2025-10-06T20:48:46.578221Z","shell.execute_reply.started":"2025-10-06T20:48:46.361283Z","shell.execute_reply":"2025-10-06T20:48:46.577572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(sample_df[\"date_id\"],sample_df[\"market_forward_excess_returns\"],color=\"black\",s=5)\nplt.title(\"market_forward_excess_returns\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"market_forward_excess_returns\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:52.991523Z","iopub.execute_input":"2025-10-06T20:48:52.992059Z","iopub.status.idle":"2025-10-06T20:48:53.307524Z","shell.execute_reply.started":"2025-10-06T20:48:52.992034Z","shell.execute_reply":"2025-10-06T20:48:53.306816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df[\"diff\"]=sample_df[\"forward_returns\"].diff()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:57.05777Z","iopub.execute_input":"2025-10-06T20:48:57.058065Z","iopub.status.idle":"2025-10-06T20:48:57.063117Z","shell.execute_reply.started":"2025-10-06T20:48:57.058046Z","shell.execute_reply":"2025-10-06T20:48:57.062482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(sample_df['date_id'], sample_df['diff'], color='black', s=5)\nplt.title(\"Scatter Plot of Differences in Time Series\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Difference in Value\")\nplt.axhline(0, color='lightgrey', linestyle='--') \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:48:59.955984Z","iopub.execute_input":"2025-10-06T20:48:59.956245Z","iopub.status.idle":"2025-10-06T20:49:00.188026Z","shell.execute_reply.started":"2025-10-06T20:48:59.956227Z","shell.execute_reply":"2025-10-06T20:49:00.187299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(sample_df['forward_returns'].shift(1), sample_df['diff'],color=\"black\", s=10)\nplt.xlabel(\"Previous Value\")\nplt.ylabel(\"Difference\")\nplt.title(\"Difference vs Previous Value\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:03.100667Z","iopub.execute_input":"2025-10-06T20:49:03.101528Z","iopub.status.idle":"2025-10-06T20:49:03.304813Z","shell.execute_reply.started":"2025-10-06T20:49:03.101497Z","shell.execute_reply":"2025-10-06T20:49:03.304109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df['date_id'] = sample_df['date_id'].astype('int64') // 10**9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:06.425568Z","iopub.execute_input":"2025-10-06T20:49:06.426069Z","iopub.status.idle":"2025-10-06T20:49:06.430461Z","shell.execute_reply.started":"2025-10-06T20:49:06.426046Z","shell.execute_reply":"2025-10-06T20:49:06.429655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"na_counts = sample_df.isnull().sum()\nna_counts = na_counts[na_counts > 0].sort_values(ascending=False)\nplt.figure(figsize=(24,9))\nna_counts.plot(kind=\"bar\",color=\"darkred\")\nplt.title(\"Missing Values per Feature\")\nplt.ylabel(\"Count\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:09.360459Z","iopub.execute_input":"2025-10-06T20:49:09.360728Z","iopub.status.idle":"2025-10-06T20:49:09.927363Z","shell.execute_reply.started":"2025-10-06T20:49:09.360707Z","shell.execute_reply":"2025-10-06T20:49:09.926668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(18,9))\nsns.heatmap(sample_df.isnull(),cbar=False,cmap=\"viridis\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:12.891023Z","iopub.execute_input":"2025-10-06T20:49:12.891339Z","iopub.status.idle":"2025-10-06T20:49:14.472822Z","shell.execute_reply.started":"2025-10-06T20:49:12.891294Z","shell.execute_reply":"2025-10-06T20:49:14.472127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_cols = sample_df.columns[sample_df.isnull().any()]\nmsno.bar(sample_df[missing_cols])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:18.923245Z","iopub.execute_input":"2025-10-06T20:49:18.923789Z","iopub.status.idle":"2025-10-06T20:49:20.643761Z","shell.execute_reply.started":"2025-10-06T20:49:18.923762Z","shell.execute_reply":"2025-10-06T20:49:20.643006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_percent = sample_df.isnull().mean().sort_values(ascending=False) * 100\nplt.figure(figsize=(12,6))\nsns.barplot(x=missing_percent.index[:20], y=missing_percent.values[:20])\nplt.xticks(rotation=90)\nplt.ylabel(\"Percentage Missing\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:22.325926Z","iopub.execute_input":"2025-10-06T20:49:22.326635Z","iopub.status.idle":"2025-10-06T20:49:22.534372Z","shell.execute_reply.started":"2025-10-06T20:49:22.326609Z","shell.execute_reply":"2025-10-06T20:49:22.533716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# VISUALIZING SUCH A BIG DATASET DOES NOT GIVE MUCH INFORMATION!lets perform LITTLE'S MCAR Test\n","metadata":{}},{"cell_type":"code","source":"sample_df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:25.449393Z","iopub.execute_input":"2025-10-06T20:49:25.450021Z","iopub.status.idle":"2025-10-06T20:49:25.456146Z","shell.execute_reply.started":"2025-10-06T20:49:25.449995Z","shell.execute_reply":"2025-10-06T20:49:25.455296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cohens_d(a, b):\n    a, b = np.array(a), np.array(b)\n    n1, n2 = len(a), len(b)\n    s1, s2 = a.std(ddof=1), b.std(ddof=1)\n    pooled = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2)) if (n1 + n2 - 2) > 0 else np.nan\n    return (a.mean() - b.mean()) / pooled if pooled and not np.isnan(pooled) else np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:29.330511Z","iopub.execute_input":"2025-10-06T20:49:29.330994Z","iopub.status.idle":"2025-10-06T20:49:29.33592Z","shell.execute_reply.started":"2025-10-06T20:49:29.330971Z","shell.execute_reply":"2025-10-06T20:49:29.335056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef numeric_mcar_tests(df, alpha=0.05, max_predictors=20, min_group_n=4, fdr_correct=True):\n    df = df.copy()\n    n = df.shape[0]\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    results = []\n    details = {}\n\n    for target in numeric_cols:\n        miss_mask = df[target].isnull()\n        n_miss = int(miss_mask.sum())\n        if n_miss == 0:\n            continue\n        pct_miss = n_miss / n\n\n        pvals_pearson = {}\n        pearson_r = {}\n        pvals_t = {}\n        t_stats = {}\n        cohens = {}\n        pvals_mw = {}\n\n        for col in numeric_cols:\n            if col == target:\n                continue\n            gx = df.loc[~miss_mask, col].dropna()\n            hx = df.loc[ miss_mask, col].dropna()\n\n            if len(gx) < min_group_n or len(hx) < min_group_n:\n                continue\n\n            # Welch t-test\n            try:\n                tstat, p_t = stats.ttest_ind(gx, hx, equal_var=False, nan_policy='omit')\n            except Exception:\n                tstat, p_t = np.nan, np.nan\n\n            # Mann-Whitney U (nonparametric)\n            try:\n                u_stat, p_mw = stats.mannwhitneyu(gx, hx, alternative='two-sided')\n            except Exception:\n                p_mw = np.nan\n\n            # point-biserial ~ Pearson correlation between numeric col and missing indicator \n            valid_idx = df[col].notnull()\n            if valid_idx.sum() >= min_group_n:\n                try:\n                    r, p_r = stats.pearsonr(df.loc[valid_idx, col], df.loc[valid_idx, target].isnull().astype(int))\n                except Exception:\n                    r, p_r = np.nan, np.nan\n            else:\n                r, p_r = np.nan, np.nan\n\n            d = cohens_d(gx, hx)\n\n            pvals_pearson[col] = p_r\n            pearson_r[col] = r\n            pvals_t[col] = p_t\n            t_stats[col] = tstat\n            cohens[col] = d\n            pvals_mw[col] = p_mw\n\n        pvals_arr = np.array([v for v in pvals_pearson.values() if not pd.isna(v)])\n        cols_arr = [k for k, v in pvals_pearson.items() if not pd.isna(v)]\n        reject = np.array([])\n        pvals_adj = {}\n        if fdr_correct and len(pvals_arr) > 0:\n            rej, p_adj_arr, _, _ = multipletests(pvals_arr, alpha=alpha, method='fdr_bh')\n            reject = rej\n            pvals_adj = dict(zip(cols_arr, p_adj_arr))\n        else:\n            pvals_adj = {k: p for k, p in pvals_pearson.items()}\n\n        sig_after = [col for col in cols_arr if (pvals_adj.get(col, 1.0) < alpha)]\n        sig_count = len(sig_after)\n\n        logit_info = {}\n        try:\n            sort_r = sorted(pearson_r.items(), key=lambda x: 0 if pd.isna(x[1]) else abs(x[1]), reverse=True)\n            top_feats = [c for c, _ in sort_r if not pd.isna(pearson_r.get(c))][:max_predictors]\n            if len(top_feats) > 0:\n                X = df[top_feats].copy().fillna(df[top_feats].mean())\n                X = sm.add_constant(X, has_constant='add')\n                y = miss_mask.astype(int)\n                if y.nunique() == 2 and X.shape[1] >= 1:\n                    model = sm.Logit(y, X).fit(disp=0, maxiter=200)\n                    pvals_logit = model.pvalues.drop('const', errors='ignore').to_dict()\n                    sig_logit = {k: v for k, v in pvals_logit.items() if v < alpha}\n                    logit_info = {\n                        \"n_features_tested\": X.shape[1] - 1,\n                        \"n_significant\": len(sig_logit),\n                        \"significant_predictors\": sig_logit\n                    }\n        except PerfectSeparationError as e:\n            logit_info = {\"error\": f\"Perfect separation: {e}\"}\n        except Exception as e:\n            logit_info = {\"error\": str(e)}\n\n        details[target] = {\n            \"pearson_pvals\": pvals_pearson,\n            \"pearson_r\": pearson_r,\n            \"pearson_pvals_adj\": pvals_adj,\n            \"t_pvals\": pvals_t,\n            \"t_stats\": t_stats,\n            \"cohens_d\": cohens,\n            \"mannwhitney_pvals\": pvals_mw,\n            \"logit\": logit_info,\n            \"sig_after_fdr\": sig_after\n        }\n\n        verdict = \"evidence_against_MCAR\" if sig_count > 0 else \"no_strong_evidence_against_MCAR\"\n        reasons = f\"{sig_count} predictors significant after FDR\" if sig_count>0 else \"no predictors significant after FDR\"\n\n        results.append({\n            \"target\": target,\n            \"n_missing\": n_miss,\n            \"pct_missing\": pct_miss,\n            \"n_predictors_tested\": len(pvals_pearson),\n            \"n_significant_after_fdr\": sig_count,\n            \"verdict\": verdict,\n            \"reasons\": reasons\n        })\n\n    summary_df = pd.DataFrame(results).sort_values(by=\"pct_missing\", ascending=False).reset_index(drop=True)\n    return summary_df, details","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:31.726585Z","iopub.execute_input":"2025-10-06T20:49:31.727167Z","iopub.status.idle":"2025-10-06T20:49:31.742962Z","shell.execute_reply.started":"2025-10-06T20:49:31.727144Z","shell.execute_reply":"2025-10-06T20:49:31.742202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary, details = numeric_mcar_tests(sample_df)\nsummary.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:49:36.988872Z","iopub.execute_input":"2025-10-06T20:49:36.989152Z","iopub.status.idle":"2025-10-06T20:50:31.36549Z","shell.execute_reply.started":"2025-10-06T20:49:36.989133Z","shell.execute_reply":"2025-10-06T20:50:31.364721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count = (summary['verdict'] == 'evidence_against_MCAR').sum()\nprint(count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:31.366557Z","iopub.execute_input":"2025-10-06T20:50:31.36692Z","iopub.status.idle":"2025-10-06T20:50:31.371836Z","shell.execute_reply.started":"2025-10-06T20:50:31.366889Z","shell.execute_reply":"2025-10-06T20:50:31.371135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary[\"verdict\"].count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:31.372445Z","iopub.execute_input":"2025-10-06T20:50:31.372701Z","iopub.status.idle":"2025-10-06T20:50:31.389741Z","shell.execute_reply.started":"2025-10-06T20:50:31.37268Z","shell.execute_reply":"2025-10-06T20:50:31.389124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# THE DATA IS MISSING AT RANDOM, IT DEPENDS ON OTHER VARIABLES\n","metadata":{}},{"cell_type":"code","source":"miss_indicators=pd.DataFrame({\n    f\"{col} missing\":sample_df[col].isnull().astype(int)\n    for col in sample_df.columns if sample_df[col].isnull().any()\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:31.391607Z","iopub.execute_input":"2025-10-06T20:50:31.391889Z","iopub.status.idle":"2025-10-06T20:50:31.429545Z","shell.execute_reply.started":"2025-10-06T20:50:31.391872Z","shell.execute_reply":"2025-10-06T20:50:31.428837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"miss_indicators.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:31.430445Z","iopub.execute_input":"2025-10-06T20:50:31.430686Z","iopub.status.idle":"2025-10-06T20:50:31.454342Z","shell.execute_reply.started":"2025-10-06T20:50:31.430664Z","shell.execute_reply":"2025-10-06T20:50:31.453576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import pearsonr\nresults=[]\nfor i in miss_indicators.columns:\n    miss_col=miss_indicators[i]\n    for col in sample_df.select_dtypes(include=['int64','float64']).columns:\n        if col.replace(\"_missing\", \"\") in i: \n            continue\n        valid_idx=sample_df[col].notnull()\n        if valid_idx.sum()>5:\n            r, p = pearsonr(sample_df.loc[valid_idx, col], miss_col[valid_idx])\n            results.append((i, col, r, p))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:31.455165Z","iopub.execute_input":"2025-10-06T20:50:31.455399Z","iopub.status.idle":"2025-10-06T20:50:40.251472Z","shell.execute_reply.started":"2025-10-06T20:50:31.455377Z","shell.execute_reply":"2025-10-06T20:50:40.250846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df = pd.DataFrame(results, columns=['Missing_in', 'Feature', 'Correlation', 'p_value'])\nresults_df = results_df.sort_values(['Missing_in','p_value'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:40.252252Z","iopub.execute_input":"2025-10-06T20:50:40.252497Z","iopub.status.idle":"2025-10-06T20:50:40.262336Z","shell.execute_reply.started":"2025-10-06T20:50:40.252478Z","shell.execute_reply":"2025-10-06T20:50:40.261715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CORRELATION TELLS THE CORRELATION AND P VALUES TELLS HOW STRONG IS THE CORRELATIO (IF P<0.05)","metadata":{}},{"cell_type":"code","source":"results_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:40.263334Z","iopub.execute_input":"2025-10-06T20:50:40.263606Z","iopub.status.idle":"2025-10-06T20:50:40.282583Z","shell.execute_reply.started":"2025-10-06T20:50:40.263579Z","shell.execute_reply":"2025-10-06T20:50:40.281932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_cols=sample_df.select_dtypes(include=[np.number]).columns.tolist()\nmiss_indictors=sample_df[numeric_cols].isna().astype(int).add_suffix(\"missing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:50:40.283337Z","iopub.execute_input":"2025-10-06T20:50:40.284332Z","iopub.status.idle":"2025-10-06T20:50:40.313538Z","shell.execute_reply.started":"2025-10-06T20:50:40.284314Z","shell.execute_reply":"2025-10-06T20:50:40.31234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_columns = sample_df.shape[1]\nprint(num_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:55:49.691934Z","iopub.execute_input":"2025-10-06T20:55:49.692585Z","iopub.status.idle":"2025-10-06T20:55:49.696778Z","shell.execute_reply.started":"2025-10-06T20:55:49.692558Z","shell.execute_reply":"2025-10-06T20:55:49.695736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom xgboost import XGBRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def get_top_predictors(df_combined, miss_col, candidate_cols, top_k=5, min_valid=5):\n#     scores = {}\n#     target_col = miss_col\n#     if target_col not in df_combined.columns:\n#         df_combined[target_col] = df_combined[target_col.replace('_missing','')].isnull().astype(int)\n#     for col in candidate_cols:\n#         if col == target_col.replace('_missing',''):\n#             continue\n#         valid = df_combined[col].notnull()\n#         if valid.sum() < min_valid:\n#             continue\n#         try:\n#             r = df_combined.loc[valid, col].corr(df_combined.loc[valid, target_col])\n#         except Exception:\n#             r = np.nan\n#         if pd.isna(r):\n#             continue\n#         scores[col] = abs(r)\n#     top = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n#     return [c for c,_ in top]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def create_lag_features(df, predictors, n_lags=3):\n#     lagged = pd.DataFrame(index=df.index)\n#     for p in predictors:\n#         for lag in range(1, n_lags+1):\n#             lagged[f\"{p}_lag{lag}\"] = df[p].shift(lag)\n#     return lagged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def causal_impute_column(df, target, predictors, n_lags=3, min_train=30, model=None):\n#     df_new = df.copy()\n#     if model is None:\n#         model = RandomForestRegressor(n_estimators=100, random_state=0, n_jobs=-1)\n#     preds = [p for p in predictors if p in df_new.columns and p != target]\n#     if len(preds) == 0:\n#         return df_new, False\n\n#     lagged = create_lag_features(df_new, preds, n_lags)\n#     complete_lag_mask = lagged.notnull().all(axis=1)\n#     train_mask = complete_lag_mask & df_new[target].notnull()\n#     if train_mask.sum() < min_train:\n#         return df_new, False\n\n#     X_train = lagged.loc[train_mask]\n#     y_train = df_new.loc[train_mask, target].astype(float)\n\n#     model.fit(X_train, y_train)\n#     impute_mask = complete_lag_mask & df_new[target].isnull()\n#     if impute_mask.sum() == 0:\n#         return df_new, False\n\n#     X_pred = lagged.loc[impute_mask]\n#     preds_values = model.predict(X_pred)\n\n#     df_new.loc[impute_mask, target] = preds_values\n#     changed = df_new[target].notnull().sum() > df[target].notnull().sum()\n#     return df_new, changed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def iterative_causal_impute(df, n_lags=3, top_k=5, min_train=30, max_passes=5, min_valid=5):\n#     df_imp = df.copy()\n#     numeric_cols = df_imp.select_dtypes(include=[np.number]).columns.tolist()\n#     miss_inds = pd.DataFrame({f\"{c}_missing\": df_imp[c].isnull().astype(int) for c in numeric_cols}, index=df_imp.index)\n\n#     pass_no = 0\n#     overall_changed = True\n#     while pass_no < max_passes and overall_changed:\n#         pass_no += 1\n#         overall_changed = False\n#         cols_with_missing = [c for c in numeric_cols if df_imp[c].isnull().any()]\n#         if not cols_with_missing:\n#             break\n\n#         for target in cols_with_missing:\n#             miss_col = f\"{target}_missing\"\n#             combined = pd.concat([df_imp, miss_inds], axis=1)\n#             top_preds = get_top_predictors(combined, miss_col, numeric_cols, top_k=top_k, min_valid=min_valid)\n#             if not top_preds:\n#                 continue\n#             df_before_missing = df_imp[target].isnull().sum()\n#             df_imp, changed = causal_impute_column(df_imp, target, top_preds, n_lags=n_lags, min_train=min_train)\n#             if changed:\n#                 overall_changed = True\n#                 miss_inds[f\"{target}_missing\"] = df_imp[target].isnull().astype(int)\n\n#     remaining = df_imp.isnull().sum()\n#     fallback_cols = remaining[remaining > 0].index.tolist()\n#     for c in fallback_cols:\n#         df_imp[c] = df_imp[c].fillna(method='ffill').fillna(method='bfill')  \n\n#     final_missing = df_imp.isnull().sum().sum()\n#     return df_imp, final_missing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_imputed, missing_left = iterative_causal_impute(sample_df,\n#                                                     n_lags=3, top_k=5, min_train=30,\n#                                                     max_passes=6, min_valid=6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nimport kaggle_evaluation.default_inference_server\nimport pickle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FINAL_MODEL=None\nFEATURE_COLUMNS=[]\nHISTORY_BUFFER=None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_signal(predictions:np.ndarray,multiplier:float=400.0)->np.ndarray:\n    signals=predictions*multiplier+1\n    return np.clip(signals,0.0,2.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mice_impute_fast(df, max_iter=3):\n    df_imp = df.copy()\n    \n    numeric_cols = df_imp.select_dtypes(include=[np.number]).columns.tolist()\n    \n    if not numeric_cols or df_imp[numeric_cols].isnull().sum().sum() == 0:\n        return df_imp\n    \n    lgbm_estimator = LGBMRegressor(\n        n_estimators=50,   \n        learning_rate=0.1,\n        max_depth=4,          \n        num_leaves=15,\n        random_state=42,\n        n_jobs=-1,\n        verbose=-1,\n        force_col_wise=True   \n    )\n    \n    mice_imputer = IterativeImputer(\n        estimator=lgbm_estimator,\n        max_iter=max_iter,\n        random_state=42,\n        initial_strategy='mean',\n        verbose=0\n    )\n    \n    try:\n        imputed_values = mice_imputer.fit_transform(df_imp[numeric_cols])\n        df_imp[numeric_cols] = imputed_values\n    except Exception as e:\n        print(f\"MICE imputation failed: {e}, using simple imputation\")\n        for col in numeric_cols:\n            df_imp[col] = df_imp[col].fillna(df_imp[col].mean()).fillna(0)\n    \n    remaining = df_imp.isnull().sum()\n    if remaining.sum() > 0:\n        for col in remaining[remaining > 0].index:\n            df_imp[col] = df_imp[col].fillna(method='ffill').fillna(method='bfill').fillna(0)\n    return df_imp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import r2_score, mean_squared_error","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport numpy as np\n\ndef train_model():\n    global FINAL_MODEL, FEATURE_COLUMNS, HISTORY_BUFFER\n    print(\"Training model...\")\n    start_time = pd.Timestamp.now()    \n    path = \"/kaggle/input/hull-tactical-market-prediction\"\n    print(f\"Loading data from {path}\")\n    training_df = pd.read_csv(f\"{path}/train.csv\")\n    print(f\"Loaded {len(training_df)} rows\")\n    print(f\"Missing values before imputation: {training_df.isnull().sum().sum()}\")\n    \n    print(\"Applying MICE imputation with LightGBM...\")\n    training_df = mice_impute_fast(training_df, max_iter=3)\n    print(f\"Imputation done. Remaining NaN: {training_df.isnull().sum().sum()}\")\n    \n    if isinstance(training_df, pd.DataFrame):\n        training_df = pl.from_pandas(training_df)\n    \n    training_df = training_df.rename({'market_forward_excess_returns': 'target'})\n    \n    feature_cols = [col for col in training_df.columns if col != 'date_id']\n    training_df = training_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n    \n    training_df = training_df.drop_nulls()\n    print(f\"After dropping nulls: {len(training_df)} rows\")\n    \n    FEATURE_COLUMNS = [col for col in training_df.columns \n                       if col not in ['date_id', 'target', 'forward_returns', 'risk_free_rate']]\n    \n    print(f\"Training with {len(FEATURE_COLUMNS)} features\")\n    \n    X = training_df.select(FEATURE_COLUMNS).to_numpy()\n    y = training_df.select('target').to_numpy().ravel()\n    \n    n_splits = 5  \n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    cv_scores = []\n    cv_rmse = []\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        print(f\"\\nFold {fold}/{n_splits}\")\n        print(f\"  Train size: {len(train_idx):,} | Val size: {len(val_idx):,}\")\n        \n        model = XGBRegressor(\n            n_estimators=200,       \n            learning_rate=0.05,   \n            max_depth=6,\n            num_leaves=31,\n            min_child_samples=20,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            n_jobs=-1,\n            verbose=-1,\n            force_col_wise=True\n        )\n        \n        model.fit(X_train, y_train)\n        \n        y_pred = model.predict(X_val)\n        r2 = r2_score(y_val, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n        \n        cv_scores.append(r2)\n        cv_rmse.append(rmse)\n        \n        print(f\"  R² score: {r2:.4f} | RMSE: {rmse:.6f}\")\n \n    FINAL_MODEL = XGBRegressor(\n        n_estimators=200,       \n        learning_rate=0.05,   \n        max_depth=6,\n        num_leaves=31,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,\n        verbose=-1,\n        force_col_wise=True\n    )\n    \n    FINAL_MODEL.fit(X, y)\n    \n    y_pred = FINAL_MODEL.predict(X)\n    r2 = r2_score(y, y_pred)\n    print(f\"Final model trained. R² score: {r2:.4f}\")\n    \n    HISTORY_BUFFER = training_df.select(\n        ['date_id'] + FEATURE_COLUMNS + ['target', 'forward_returns', 'risk_free_rate']\n    ).tail(50)\n    \n    elapsed = (pd.Timestamp.now() - start_time).total_seconds()\n    print(f\"Model training complete in {elapsed:.1f} seconds. Ready for predictions.\")\n    \n    return {\n        'cv_r2_mean': np.mean(cv_scores),\n        'cv_r2_std': np.std(cv_scores),\n        'cv_rmse_mean': np.mean(cv_rmse),\n        'cv_scores': cv_scores\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    Make a prediction for a single test row.\n    \n    Args:\n        test: A polars DataFrame with one row containing the test features\n        \n    Returns:\n        A float signal value between 0 and 2\n    \"\"\"\n    global HISTORY_BUFFER, FINAL_MODEL, FEATURE_COLUMNS\n    \n    try:\n        for col in test.columns:\n            if col != 'date_id':\n                try:\n                    test = test.with_columns(pl.col(col).cast(pl.Float64))\n                except:\n                    test = test.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Float64))\n        \n        rename_mapping = {}\n        if 'lagged_forward_returns' in test.columns:\n            rename_mapping['lagged_forward_returns'] = 'forward_returns'\n        if 'lagged_risk_free_rate' in test.columns:\n            rename_mapping['lagged_risk_free_rate'] = 'risk_free_rate'\n        if 'lagged_market_forward_excess_returns' in test.columns:\n            rename_mapping['lagged_market_forward_excess_returns'] = 'target'\n        \n        if rename_mapping:\n            test = test.rename(rename_mapping)        \n        if 'is_scored' in test.columns:\n            test = test.drop('is_scored')\n        \n        for col in HISTORY_BUFFER.columns:\n            if col not in test.columns:\n                col_dtype = HISTORY_BUFFER[col].dtype\n                test = test.with_columns(pl.lit(0.0).cast(col_dtype).alias(col))\n        \n        for col in test.columns:\n            if col in HISTORY_BUFFER.columns:\n                if test[col].dtype != HISTORY_BUFFER[col].dtype:\n                    test = test.with_columns(pl.col(col).cast(HISTORY_BUFFER[col].dtype))\n        \n        test = test.select(HISTORY_BUFFER.columns)        \n        HISTORY_BUFFER = pl.concat([HISTORY_BUFFER, test], how=\"vertical\")        \n        if len(HISTORY_BUFFER) > 100:\n            HISTORY_BUFFER = HISTORY_BUFFER.tail(100)        \n        latest_row = HISTORY_BUFFER.tail(1)\n        X_test = latest_row.select(FEATURE_COLUMNS).to_numpy()        \n        X_test = np.nan_to_num(X_test, nan=0.0)        \n        raw_prediction = FINAL_MODEL.predict(X_test)[0]        \n        signal = convert_to_signal(np.array([raw_prediction]))[0]\n        return float(signal)\n    except Exception as e:\n        print(f\"Error in prediction: {e}\")\n        return 1.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(\"Running in competition mode\")\n    inference_server.serve()\nelse:\n    print(\"Running in local gateway mode\")\n    inference_server.run_local_gateway(\n        ('/kaggle/input/hull-tactical-market-prediction/',)\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}