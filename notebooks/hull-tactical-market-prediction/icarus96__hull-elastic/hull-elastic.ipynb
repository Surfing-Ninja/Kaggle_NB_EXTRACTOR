{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:10:47.859431Z","iopub.execute_input":"2025-11-04T19:10:47.859585Z","iopub.status.idle":"2025-11-04T19:10:48.940262Z","shell.execute_reply.started":"2025-11-04T19:10:47.85957Z","shell.execute_reply":"2025-11-04T19:10:48.939393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport datetime\n\n# --- NEW IMPORTS ---\nfrom dataclasses import dataclass, asdict, field \nfrom typing import List, Tuple\nimport lightgbm as lgb\n# -------------------\n\nimport polars as pl \nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nimport kaggle_evaluation.default_inference_server\n\n\n# ============ PATHS ============\nDATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n# ============ RETURNS TO SIGNAL CONFIGS ============\nMIN_SIGNAL: float = 0.0                          \nMAX_SIGNAL: float = 2.0                          \nSIGNAL_MULTIPLIER: float = 400.0                 \n\n# ============ MODEL CONFIGS (Used to initialize CONFIG object) ============\nCV: int = 10                                     \nL1_RATIO: float = 0.5                            \nALPHAS: np.ndarray = field(default_factory=lambda: np.logspace(-4, 2, 100)) # Safe initialization\nMAX_ITER: int = 1000000                          \n\n# --- CONFIGS for LGBM and Lag Features (New) ---\nLAG_PERIODS: Tuple[int, ...] = (1, 5)\nLAG_CANDIDATES: Tuple[str, ...] = (\"S2\", \"E2\", \"I2\", \"P9\", \"U1\", \"U2\")\n# Features used in original ElasticNet model structure\nVARS_TO_KEEP_BASE: List[str] = [\n    \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n    \"P10\", \"P12\", \"P13\"\n]\n\n@dataclass\nclass DatasetOutput:\n    X_train : pl.DataFrame \n    X_test: pl.DataFrame\n    y_train: pl.Series\n    y_test: pl.Series\n    scaler: StandardScaler\n\n@dataclass \nclass ElasticNetParameters:\n    l1_ratio : float \n    cv: int\n    alphas: np.ndarray \n    max_iter: int \n    \n    def __post_init__(self): \n        if self.l1_ratio < 0 or self.l1_ratio > 1: \n            raise ValueError(\"Wrong initializing value for ElasticNet l1_ratio\")\n        \n@dataclass(frozen=True)\nclass RetToSignalParameters:\n    signal_multiplier: float \n    min_signal : float = MIN_SIGNAL\n    max_signal : float = MAX_SIGNAL\n\n\nret_signal_params = RetToSignalParameters(\n    signal_multiplier= SIGNAL_MULTIPLIER\n)\n\nenet_params = ElasticNetParameters(\n    l1_ratio = L1_RATIO, \n    cv = CV, \n    alphas = ALPHAS, \n    max_iter = MAX_ITER\n)\n\n# --- Feature Engineering Helper ---\n\ndef add_lag_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Adds 1-day and 5-day lag features for specified columns.\"\"\"\n    \n    expressions = []\n    available_lag_candidates = [col for col in LAG_CANDIDATES if col in df.columns]\n\n    for col in available_lag_candidates:\n        for lag in LAG_PERIODS:\n            expressions.append(\n                pl.col(col).shift(lag).over('date_id').alias(f\"{col}_lag{lag}\")\n            )\n    \n    if expressions:\n        df = df.with_columns(expressions)\n        \n    return df\n    \ndef load_trainset() -> pl.DataFrame:\n    \"\"\"\n    Loads and preprocesses the training dataset.\n    \"\"\"\n    return (\n        pl.read_csv(DATA_PATH / \"train.csv\")\n        .rename({'market_forward_excess_returns':'target'})\n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n        .head(-10)\n    )\n\ndef load_testset() -> pl.DataFrame:\n    \"\"\"\n    Loads and preprocesses the testing dataset.\n    \"\"\"\n    # FIX: Rename the existing 'lagged_forward_returns' to 'target' for placeholder consistency\n    return (\n        pl.read_csv(DATA_PATH / \"test.csv\")\n        .rename({'lagged_forward_returns':'target'}) \n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n    )\n\ndef create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Creates new features, adds lags, and cleans a DataFrame.\n    \"\"\"\n    \n    # 1. Engineer U1 and U2\n    df_with_u = df.with_columns(\n        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n        (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n    )\n\n    # 2. Add Lag Features\n    df_lagged = add_lag_features(df_with_u)\n\n    # 3. Dynamically define ALL feature columns\n    feature_candidates = VARS_TO_KEEP_BASE + [\"U1\", \"U2\"]\n    lag_cols = [col for col in df_lagged.columns if any(col.endswith(f\"_lag{p}\") for p in LAG_PERIODS)]\n    all_features = [col for col in feature_candidates + lag_cols if col in df_lagged.columns]\n    \n    # 4. Initial selection of all necessary columns\n    selection_cols = [\"date_id\"]\n    if 'target' in df_lagged.columns:\n        selection_cols.append(\"target\")\n    selection_cols.extend(all_features)\n    \n    df_selected = df_lagged.select(selection_cols)\n    \n    # 5. Impute Nulls (Robust Fix: EWMA + Fallback to Zero)\n    \n    # Impute primary nulls using EWMA\n    df_imputed = df_selected.with_columns([\n        pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n        for col in all_features if col in df_selected.columns\n    ])\n    \n    # Fallback Imputation: Fill any remaining nulls (e.g., first few rows) with 0.0\n    df_imputed = df_imputed.with_columns([\n        pl.col(col).fill_null(0.0) \n        for col in all_features if col in df_imputed.columns\n    ])\n    \n    # Final selection step\n    return df_imputed.select(selection_cols)\n    \ndef join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Joins two dataframes by common columns and concatenates them vertically.\n    \"\"\"\n    common_columns: list[str] = [col for col in train.columns if col in test.columns]\n    \n    return pl.concat([train.select(common_columns), test.select(common_columns)], how=\"vertical\")\n\ndef split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput: \n    \"\"\"\n    Splits the data into features (X) and target (y), and scales the features.\n    \"\"\"\n    X_train = train.select(features)\n    y_train = train.get_column('target')\n    X_test = test.select(features)\n    y_test = test.get_column('target') \n    \n    scaler = StandardScaler() \n    \n    # Convert Polars DF to NumPy array for scikit-learn\n    X_train_scaled_np = scaler.fit_transform(X_train.to_numpy())\n    X_train = pl.from_numpy(X_train_scaled_np, schema=features)\n    \n    X_test_scaled_np = scaler.transform(X_test.to_numpy())\n    X_test = pl.from_numpy(X_test_scaled_np, schema=features)\n    \n    return DatasetOutput(\n        X_train = X_train,\n        y_train = y_train, \n        X_test = X_test, \n        y_test = y_test,\n        scaler = scaler\n    )\n\ndef convert_ret_to_signal(\n    ret_arr: np.ndarray,\n    params: RetToSignalParameters\n) -> np.ndarray:\n    \"\"\"\n    Converts raw model predictions (expected returns) into a trading signal.\n    \"\"\"\n    return np.clip(\n        ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal\n    )\n\n\ntrain: pl.DataFrame = load_trainset()\ntest: pl.DataFrame = load_testset() \nprint(train.tail(3)) \nprint(test.head(3))\n\n# --- Data Preparation ---\ndf: pl.DataFrame = join_train_test_dataframes(train, test)\ndf = create_example_dataset(df=df) \ntrain: pl.DataFrame = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\ntest: pl.DataFrame = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n\n# Define final feature list (now includes lags)\nFEATURES: list[str] = [col for col in test.columns if col not in ['date_id', 'target']]\n\ndataset: DatasetOutput = split_dataset(train=train, test=test, features=FEATURES) \n\nX_train: pl.DataFrame = dataset.X_train\nX_test: pl.DataFrame = dataset.X_test\ny_train: pl.DataFrame = dataset.y_train\ny_test: pl.DataFrame = dataset.y_test\nscaler: StandardScaler = dataset.scaler \n\n# Convert to NumPy for LGBM training\nX_train_np = X_train.to_numpy()\ny_train_np = y_train.to_numpy()\n\n\n# --- MODEL TRAINING (LightGBM) ---\nprint(\"\\n--- Starting LightGBM Model Training ---\")\n\nlgbm_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'n_estimators': 500, \n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'verbose': -1, \n    'n_jobs': -1,\n    'seed': 42\n}\n\n# 1. Initialize and Fit the LightGBM model\nmodel = lgb.LGBMRegressor(**lgbm_params)\nmodel.fit(X_train_np, y_train_np)\n\nprint(f\"--- LightGBM Model Trained with {model.n_estimators_} Estimators ---\")\n# --------------------------------------------------------------------------\n\ndef predict(test_chunk: pl.DataFrame) -> float:\n    # 1. Preprocess: Engineer Features, Select Features\n    # NOTE: The redundant rename has been removed. test_chunk is processed as is.\n    df_processed = create_example_dataset(test_chunk)\n    \n    # 2. Select final features (matching the columns the model was trained on)\n    X_test_chunk: pl.DataFrame = df_processed.select(FEATURES)\n    \n    # 3. Scale using the fitted scaler\n    X_test_scaled_np: np.ndarray = scaler.transform(X_test_chunk.to_numpy())\n    \n    # 4. Predict\n    raw_pred: float = model.predict(X_test_scaled_np)[0]\n    \n    # 5. Convert to signal\n    return convert_ret_to_signal(np.array([raw_pred]), ret_signal_params).item()\n\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    # Running a small local test\n    local_test_chunk = load_testset().head(1)\n    signal = predict(local_test_chunk)\n    print(f\"\\nLocal Test Prediction Signal: {signal:.4f}\")\n    # inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:12:11.798045Z","iopub.execute_input":"2025-11-04T19:12:11.79849Z","iopub.status.idle":"2025-11-04T19:12:12.425421Z","shell.execute_reply.started":"2025-11-04T19:12:11.798472Z","shell.execute_reply":"2025-11-04T19:12:12.424721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}