{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# ðŸ§  Hull Tactical Market Prediction â€” Stacked Ensemble v5\n# Compatible with Kaggle Inference Server\n# ============================================================\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport kaggle_evaluation.default_inference_server\n\n# ============================================================\n# CONFIG\n# ============================================================\nDATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\nMIN_SIGNAL, MAX_SIGNAL = 0.0, 2.0\nSIGNAL_MULTIPLIER = 400.0\n\nLAG_PERIODS = (1, 5)\nLAG_CANDIDATES = (\"S2\", \"E2\", \"I2\", \"P9\", \"U1\", \"U2\")\nVARS_TO_KEEP_BASE = [\n    \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n    \"P10\", \"P12\", \"P13\"\n]\n\n# ============================================================\n# HELPERS\n# ============================================================\n\ndef add_lag_features(df: pl.DataFrame) -> pl.DataFrame:\n    exprs = []\n    available = [col for col in LAG_CANDIDATES if col in df.columns]\n    for col in available:\n        for lag in LAG_PERIODS:\n            exprs.append(pl.col(col).shift(lag).over('date_id').alias(f\"{col}_lag{lag}\"))\n    if exprs:\n        df = df.with_columns(exprs)\n    return df\n\ndef load_train() -> pl.DataFrame:\n    return (\n        pl.read_csv(DATA_PATH / \"train.csv\")\n        .rename({'market_forward_excess_returns':'target'})\n        .with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False))\n        .head(-10)\n    )\n\ndef load_test() -> pl.DataFrame:\n    return (\n        pl.read_csv(DATA_PATH / \"test.csv\")\n        .rename({'lagged_forward_returns':'target'})\n        .with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False))\n    )\n\ndef create_features(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.with_columns(\n        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n        (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n    )\n    df = add_lag_features(df)\n\n    feature_candidates = VARS_TO_KEEP_BASE + [\"U1\", \"U2\"]\n    lag_cols = [c for c in df.columns if any(c.endswith(f\"_lag{p}\") for p in LAG_PERIODS)]\n    all_features = [c for c in feature_candidates + lag_cols if c in df.columns]\n\n    selection_cols = [\"date_id\"]\n    if 'target' in df.columns:\n        selection_cols.append(\"target\")\n    selection_cols.extend(all_features)\n\n    df = df.select(selection_cols)\n\n    # Impute nulls with EWMA then fill 0\n    df = df.with_columns([\n        pl.col(c).fill_null(pl.col(c).ewm_mean(com=0.5))\n        for c in all_features if c in df.columns\n    ])\n    df = df.with_columns([\n        pl.col(c).fill_null(0.0)\n        for c in all_features if c in df.columns\n    ])\n    return df.select(selection_cols)\n\ndef join_train_test(train, test):\n    common_cols = [c for c in train.columns if c in test.columns]\n    return pl.concat([train.select(common_cols), test.select(common_cols)], how=\"vertical\")\n\ndef convert_ret_to_signal(arr: np.ndarray) -> np.ndarray:\n    return np.clip(arr * SIGNAL_MULTIPLIER + 1, MIN_SIGNAL, MAX_SIGNAL)\n\n# ============================================================\n# LOAD + PROCESS\n# ============================================================\ntrain = load_train()\ntest = load_test()\ndf = join_train_test(train, test)\ndf = create_features(df)\n\ntrain = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\ntest  = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n\nFEATURES = [c for c in test.columns if c not in ['date_id', 'target']]\n\n# ============================================================\n# SCALE + CONVERT\n# ============================================================\nscaler = StandardScaler()\nX_train = scaler.fit_transform(train.select(FEATURES).to_numpy())\ny_train = train['target'].to_numpy()\nX_test  = scaler.transform(test.select(FEATURES).to_numpy())\n\n# ============================================================\n# STACKED ENSEMBLE\n# ============================================================\nbase_models = [\n    (\"ridge\", Ridge(alpha=0.5, random_state=42)),\n    (\"lgbm\", LGBMRegressor(\n        n_estimators=600, learning_rate=0.03, num_leaves=64,\n        subsample=0.8, colsample_bytree=0.8, random_state=42\n    )),\n    (\"xgb\", XGBRegressor(\n        n_estimators=700, learning_rate=0.03, max_depth=7,\n        subsample=0.8, colsample_bytree=0.8, random_state=42\n    )),\n    (\"cat\", CatBoostRegressor(\n        iterations=700, learning_rate=0.03, depth=7,\n        verbose=0, random_seed=42\n    ))\n]\n\nmeta_model = Ridge(alpha=1.0, random_state=42)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nmeta_train = np.zeros((X_train.shape[0], len(base_models)))\nmeta_test  = np.zeros((X_test.shape[0], len(base_models)))\n\nprint(\"\\n--- Starting 5-Fold Stacked Training ---\")\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train), 1):\n    print(f\"\\nFold {fold}\")\n    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n    y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n\n    fold_test_preds = np.zeros((X_test.shape[0], len(base_models)))\n\n    for j, (name, model) in enumerate(base_models):\n        model.fit(X_tr, y_tr)\n        val_pred = model.predict(X_val)\n        meta_train[val_idx, j] = val_pred\n        fold_test_preds[:, j] = model.predict(X_test)\n        rmse = mean_squared_error(y_val, val_pred, squared=False)\n        print(f\"{name:<6} RMSE: {rmse:.5f}\")\n\n    meta_test += fold_test_preds / kf.n_splits\n\nprint(\"\\nTraining meta Ridge model...\")\nmeta_model.fit(meta_train, y_train)\nfinal_preds = meta_model.predict(meta_test)\n\n# ============================================================\n# FINAL PREDICTION FUNCTION\n# ============================================================\ndef predict(test_chunk: pl.DataFrame) -> float:\n    df_processed = create_features(test_chunk)\n    X_t = df_processed.select(FEATURES).to_numpy()\n    X_t = scaler.transform(X_t)\n\n    base_preds = np.column_stack([\n        model.predict(X_t) for _, model in base_models\n    ])\n    meta_pred = meta_model.predict(base_preds)\n    return convert_ret_to_signal(meta_pred).item()\n\n# ============================================================\n# LOCAL TEST\n# ============================================================\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    local_test_chunk = load_test().head(1)\n    signal = predict(local_test_chunk)\n    print(f\"\\nLocal Test Prediction Signal: {signal:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T12:57:30.489911Z","iopub.execute_input":"2025-11-05T12:57:30.490831Z","iopub.status.idle":"2025-11-05T12:58:18.95352Z","shell.execute_reply.started":"2025-11-05T12:57:30.490796Z","shell.execute_reply":"2025-11-05T12:58:18.952532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}