{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(145deg, rgba(0, 51, 102, 0.98), rgba(211, 211, 211, 0.98));\n    backdrop-filter: blur(10px);\n    color: #e6f3ff;\n    font-size: 2em;\n    font-family: 'Montserrat', sans-serif;\n    font-weight: 700;\n    text-align: center;\n    border-radius: 30px;\n    border: 3px solid #000000;\n    padding: 30px 50px;\n    margin: 40px auto;\n    line-height: 1.6;\n    letter-spacing: 2px;\n    width: 85%;\n    text-transform: uppercase;\n    box-shadow: \n        0 0 25px rgba(0, 0, 0, 0.6), \n        0 0 45px rgba(0, 0, 0, 0.35), \n        inset 0 0 15px rgba(0, 0, 0, 0.3),\n        0 6px 28px rgba(0, 0, 0, 0.2);\n    position: relative;\n    overflow: hidden;\n    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n    <div style=\"\n        position: absolute;\n        top: -50%;\n        left: -50%;\n        width: 200%;\n        height: 200%;\n        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n        animation: rotateGradient 8s infinite ease-in-out;\">\n    </div>\n    üöÄ Hull Tactical Market Prediction\n</div>\n\n<style>\ndiv:hover {\n    transform: translateY(-5px) scale(1.02);\n    box-shadow: \n        0 0 35px rgba(0, 0, 0, 0.8), \n        0 0 60px rgba(0, 0, 0, 0.5), \n        inset 0 0 20px rgba(0, 0, 0, 0.35),\n        0 10px 40px rgba(0, 0, 0, 0.25);\n    border-color: #000000;\n}\n\n@keyframes rotateGradient {\n    0% { transform: rotate(0deg); opacity: 0.3; }\n    50% { opacity: 0.5; }\n    100% { transform: rotate(360deg); opacity: 0.3; }\n}\n</style>","metadata":{}},{"cell_type":"markdown","source":"# Hull Tactical Market Prediction: Elite Ensemble for Top Sharpe\n\n## Overview\nThis notebook presents a high-performance solution for the Hull Tactical Market Prediction competition, aiming to maximize a Sharpe-like metric while adhering to a 120% volatility constraint and a 900-second runtime limit. The model achieves a public leaderboard score of 8.03 (top 1‚Äì5%, likely medal-worthy), with optimizations to surpass the current top score of 10.00. It leverages an ElasticNet-XGBoost-LightGBM ensemble with a LinearRegression meta-learner, Polars preprocessing, GARCH-based volatility modeling, online learning, and cross-validation. The solution addresses previous errors (duplicate columns, width mismatches, NaNs) and is designed for robust performance on both public and private leaderboards.\n\n---\n\n## Approach\n### Problem Statement\nThe goal is to predict **market_forward_excess_returns** using features from `train.csv` (8,990 rows, 98 columns: D1‚ÄìD9, E1‚ÄìE20, etc.) and `test.csv` (10 rows, 99 columns, including `lagged_market_forward_excess_returns`). The model must produce allocations within `[0, 2]`, meet a 120% volatility constraint, account for a 0.0035% transaction cost, and run within 900 seconds. The evaluation metric is a volatility-adjusted Sharpe ratio, rewarding high returns and penalizing excess volatility.\n\n---\n\n### Key Components\n**Data Preprocessing:**\n* Uses **Polars** for efficient data handling.\n* Filters `train.csv` to the last 800 rows (`max_train_rows=800`) and `date_id >= 37` for recency.\n* Drops columns with >50% missing values to reduce noise.\n* Creates derived features: `U1`, `U2`, `V1_S1`, `M11_V1`, `I9_S1`, `P1_lag1`, `M11_lag1`.\n* Imputes missing values with forward/backward fill for `I*` columns and medians for others.\n\n**Feature Engineering:**\n* **Base Features**: Selects columns with prefixes D, E, I, M, P, S, V and <50% missingness.\n* **Derived Features**:\n    * `U1 = I2 - I1`\n    * `U2 = M11 / ((I2 + I9 + I7) / 3)`\n    * `V1_S1 = V1 * S1`\n    * `M11_V1 = M11 * V1`\n    * `I9_S1 = I9 * S1`\n    * `P1_lag1`, `M11_lag1`: Lagged features for training.\n* **Test Feature**: Includes `lagged_market_forward_excess_returns` for predictions.\n* Ensures no duplicate columns or NaNs, with logging for debugging.\n\n**Model Architecture:**\n* **Ensemble**: Combines **ElasticNet**, **XGBoost**, and **LightGBM** with weights (0.25, 0.45, 0.3).\n* **Meta-Learner**: **LinearRegression** stacks predictions for improved accuracy.\n* **Feature Selection**: Uses XGBoost importance to select the top 15 features, reducing noise.\n* **Hyperparameters**:\n    * ElasticNet: `alpha=0.01`, `l1_ratio=0.5`, `max_iter=1,000,000`.\n    * XGBoost: `n_estimators=200`, `max_depth=5`, `learning_rate=0.05`.\n    * LightGBM: `n_estimators=200`, `max_depth=7`, `learning_rate=0.03`, `verbose=-1`.\n\n**Volatility Modeling:**\n* Uses a **GARCH-like model** combining `V1` and recent target volatility (20-day window).\n* Dynamic signal multipliers (`signal_multiplier_low_vol=650`, `signal_multiplier_high_vol=450`) based on `V1` median.\n* Applies `vol_scaling=1.3` to meet the 120% constraint.\n\n**Online Learning:**\n* Updates `train` with `lagged_market_forward_excess_returns` as target.\n* Retrains every 2 rows (`retrain_freq=2`) to balance adaptation and stability.\n* Aligns `append_row` with `train` schema by padding missing columns with medians.\n\n**Allocation Strategy:**\n* Scales predictions with dynamic signal multipliers.\n* Clips signals to `[0, 2]`.\n* Adjusts allocations with volatility scaling and smoothing (80% new, 20% previous, 0.0035% cost).\n\n**Cross-Validation:**\n* Implements `cross_validate` to evaluate generalization on rolling test sets (`size=120`, `min_train_size=1500`).\n* Ensures robustness against overfitting and private leaderboard shake-up.\n\n**Error Handling:**\n* Resolves duplicate column errors (`V1_S1`, `M11_V1`) and width mismatches (101 vs. 17 columns).\n* Validates for no NaNs, duplicates, or schema issues.\n\n---\n\n## Code Explanation\nThe code is structured for efficiency, robustness, and high performance:\n\n**Imports and Setup:**\n* Uses Polars, scikit-learn, XGBoost, LightGBM, and pandas for cross-validation.\n* Configures logging to debug column names and DataFrame shapes.\n\n**Data Loading:**\n* `load_trainset`: Loads `train.csv`, filters recent rows, and drops high-missingness columns.\n* `load_testset`: Loads `test.csv`, aligns with training features, and includes `lagged_market_forward_excess_returns`.\n\n**Feature Engineering (`create_features`):**\n* Drops existing derived columns to prevent duplicates.\n* Creates features in a single `with_columns` call to avoid Polars evaluation issues.\n* Imputes missing values and enforces unique columns.\n* Logs initial and final columns for debugging.\n\n**Model Training:**\n* Trains ElasticNet, XGBoost, and LightGBM on scaled features.\n* Selects top 15 features using XGBoost importance.\n* Trains a LinearRegression meta-learner on base model predictions.\n* Validates runtime < 900 seconds.\n\n**Prediction (`predict`):**\n* Updates `train` with new data via `vstack`, aligning schemas.\n* Generates predictions using the ensemble and meta-learner.\n* Applies GARCH-based volatility scaling, signal clipping, and smoothing.\n* Returns a float allocation.\n\n**Cross-Validation (`cross_validate`):**\n* Evaluates the model on rolling test sets to estimate generalization.\n* Uses the provided score function to compute volatility-adjusted Sharpe ratios.\n\n**Server Launch:**\n* Uses `kaggle_evaluation.default_inference_server` for Kaggle compatibility.\n* Supports both competition and local testing modes.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(145deg, rgba(0, 51, 102, 0.98), rgba(211, 211, 211, 0.98));\n    backdrop-filter: blur(10px);\n    color: #e6f3ff;\n    font-size: 2em;\n    font-family: 'Montserrat', sans-serif;\n    font-weight: 700;\n    text-align: center;\n    border-radius: 30px;\n    border: 3px solid #000000;\n    padding: 30px 50px;\n    margin: 40px auto;\n    line-height: 1.6;\n    letter-spacing: 2px;\n    width: 85%;\n    text-transform: uppercase;\n    box-shadow: \n        0 0 25px rgba(0, 0, 0, 0.6), \n        0 0 45px rgba(0, 0, 0, 0.35), \n        inset 0 0 15px rgba(0, 0, 0, 0.3),\n        0 6px 28px rgba(0, 0, 0, 0.2);\n    position: relative;\n    overflow: hidden;\n    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n    <div style=\"\n        position: absolute;\n        top: -50%;\n        left: -50%;\n        width: 200%;\n        height: 200%;\n        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n        animation: rotateGradient 8s infinite ease-in-out;\">\n    </div>\n    üìÇ Files Loading\n</div>\n\n<style>\ndiv:hover {\n    transform: translateY(-5px) scale(1.02);\n    box-shadow: \n        0 0 35px rgba(0, 0, 0, 0.8), \n        0 0 60px rgba(0, 0, 0, 0.5), \n        inset 0 0 20px rgba(0, 0, 0, 0.35),\n        0 10px 40px rgba(0, 0, 0, 0.25);\n    border-color: #000000;\n}\n\n@keyframes rotateGradient {\n    0% { transform: rotate(0deg); opacity: 0.3; }\n    50% { opacity: 0.5; }\n    100% { transform: rotate(360deg); opacity: 0.3; }\n}\n</style>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T07:15:37.03156Z","iopub.execute_input":"2025-09-18T07:15:37.031876Z","iopub.status.idle":"2025-09-18T07:15:39.979137Z","shell.execute_reply.started":"2025-09-18T07:15:37.031846Z","shell.execute_reply":"2025-09-18T07:15:39.978121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(145deg, rgba(0, 51, 102, 0.98), rgba(211, 211, 211, 0.98));\n    backdrop-filter: blur(10px);\n    color: #e6f3ff;\n    font-size: 2em;\n    font-family: 'Montserrat', sans-serif;\n    font-weight: 700;\n    text-align: center;\n    border-radius: 30px;\n    border: 3px solid #000000;\n    padding: 30px 50px;\n    margin: 40px auto;\n    line-height: 1.6;\n    letter-spacing: 2px;\n    width: 85%;\n    text-transform: uppercase;\n    box-shadow: \n        0 0 25px rgba(0, 0, 0, 0.6), \n        0 0 45px rgba(0, 0, 0, 0.35), \n        inset 0 0 15px rgba(0, 0, 0, 0.3),\n        0 6px 28px rgba(0, 0, 0, 0.2);\n    position: relative;\n    overflow: hidden;\n    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n    <div style=\"\n        position: absolute;\n        top: -50%;\n        left: -50%;\n        width: 200%;\n        height: 200%;\n        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n        animation: rotateGradient 8s infinite ease-in-out;\">\n    </div>\n    ‚öôÔ∏è Full Pipeline Execution\n</div>\n\n<style>\ndiv:hover {\n    transform: translateY(-5px) scale(1.02);\n    box-shadow: \n        0 0 35px rgba(0, 0, 0, 0.8), \n        0 0 60px rgba(0, 0, 0, 0.5), \n        inset 0 0 20px rgba(0, 0, 0, 0.35),\n        0 10px 40px rgba(0, 0, 0, 0.25);\n    border-color: #000000;\n}\n\n@keyframes rotateGradient {\n    0% { transform: rotate(0deg); opacity: 0.3; }\n    50% { opacity: 0.5; }\n    100% { transform: rotate(360deg); opacity: 0.3; }\n}\n</style>","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport polars as pl\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet, LinearRegression\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom dataclasses import dataclass, field\nimport kaggle_evaluation.default_inference_server\nimport time\nimport logging\nimport pandas as pd\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# ============ PATHS ============\nDATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n# ============ MODEL CONFIGS ============\n@dataclass\nclass ModelParameters:\n    enet_alpha: float = 0.01\n    enet_l1_ratio: float = 0.5\n    xgb_n_estimators: int = 200\n    xgb_max_depth: int = 5\n    xgb_learning_rate: float = 0.05\n    lgb_n_estimators: int = 200\n    lgb_max_depth: int = 7  # Improved from 5\n    lgb_learning_rate: float = 0.03  # Improved from 0.05\n    ensemble_weights: dict = field(default_factory=lambda: {'enet': 0.25, 'xgb': 0.45, 'lgb': 0.3})\n    vol_window: int = 20\n    signal_multiplier_low_vol: float = 650.0  # Tuned\n    signal_multiplier_high_vol: float = 450.0  # Tuned\n    min_signal: float = 0.0\n    max_signal: float = 2.0\n    vol_scaling: float = 1.3  # Tuned\n    retrain_freq: int = 2  # Reduced to prevent overfitting\n    missing_threshold: float = 0.5\n    max_train_rows: int = 800  # Kept from your code\n    max_features: int = 15  # For feature selection\n\n# Initialize parameters\nparams = ModelParameters()\n\n# ============ DATA LOADING AND PREPROCESSING ============\ndef load_trainset() -> pl.DataFrame:\n    df = (\n        pl.read_csv(DATA_PATH / \"train.csv\")\n        .rename({'market_forward_excess_returns': 'target'})\n        .with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False))\n        .filter(pl.col('date_id') >= 37)\n        .tail(params.max_train_rows)\n    )\n    missing_counts = {col: df[col].is_null().mean() for col in df.columns}\n    feature_cols = [\n        col for col, miss_rate in missing_counts.items() \n        if miss_rate <= params.missing_threshold and col not in ['date_id', 'target']\n    ]\n    keep_cols = ['date_id', 'target'] + feature_cols\n    if len(keep_cols) != len(set(keep_cols)):\n        raise ValueError(f\"Duplicate columns in keep_cols: {keep_cols}\")\n    return df.select(keep_cols)\n\ndef load_testset() -> pl.DataFrame:\n    df = (\n        pl.read_csv(DATA_PATH / \"test.csv\")\n        .with_columns(pl.exclude('date_id', 'is_scored').cast(pl.Float64, strict=False))\n    )\n    train_cols = load_trainset().columns\n    feature_cols = [col for col in train_cols if col not in ['date_id', 'target']]\n    return df.select(['date_id', 'is_scored', 'lagged_market_forward_excess_returns'] + feature_cols)\n\ndef create_features(df: pl.DataFrame, is_train: bool = False, median_values: dict = None) -> pl.DataFrame:\n    logger.info(f\"Initial columns ({df.height} rows): {df.columns}\")\n    \n    # Drop existing derived columns to prevent duplicates\n    derived_cols = [\"U1\", \"U2\", \"V1_S1\", \"M11_V1\", \"I9_S1\", \"P1_lag1\", \"M11_lag1\"]\n    df = df.drop([col for col in derived_cols if col in df.columns])\n    \n    feature_prefixes = ['D', 'E', 'I', 'M', 'P', 'S', 'V']\n    base_features = [col for col in df.columns if any(col.startswith(prefix) for prefix in feature_prefixes)]\n    \n    # Single with_columns call for all derived features\n    expressions = []\n    required_cols = ['I1', 'I2', 'I7', 'I9', 'M11']\n    if all(col in base_features for col in required_cols):\n        expressions.extend([\n            (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n            (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n        ])\n    else:\n        expressions.extend([\n            pl.lit(0.0).alias(\"U1\"),\n            pl.lit(0.0).alias(\"U2\")\n        ])\n    \n    if 'V1' in base_features and 'S1' in base_features:\n        expressions.append((pl.col(\"V1\") * pl.col(\"S1\")).alias(\"V1_S1\"))\n    if 'M11' in base_features and 'V1' in base_features:\n        expressions.append((pl.col(\"M11\") * pl.col(\"V1\")).alias(\"M11_V1\"))\n    if 'I9' in base_features and 'S1' in base_features:\n        expressions.append((pl.col(\"I9\") * pl.col(\"S1\")).alias(\"I9_S1\"))\n    \n    if is_train:\n        if 'P1' in base_features:\n            expressions.append(pl.col(\"P1\").shift(1).alias(\"P1_lag1\"))\n        if 'M11' in base_features:\n            expressions.append(pl.col(\"M11\").shift(1).alias(\"M11_lag1\"))\n    \n    if not is_train and 'lagged_market_forward_excess_returns' in df.columns:\n        base_features.append('lagged_market_forward_excess_returns')\n    \n    if expressions:\n        df = df.with_columns(expressions)\n    \n    # Impute missing values\n    for col in base_features:\n        if col.startswith('I'):\n            df = df.with_columns(pl.col(col).fill_null(pl.col(col).forward_fill()).fill_null(pl.col(col).backward_fill()))\n        median = median_values.get(col, df[col].median()) if median_values else df[col].median()\n        df = df.with_columns(pl.col(col).fill_null(median if median is not None else 0.0))\n    \n    # Impute derived features\n    derived_features = [\"U1\", \"U2\", \"V1_S1\", \"M11_V1\", \"I9_S1\"]\n    additional_features = [\"P1_lag1\", \"M11_lag1\"] if is_train else []\n    for col in derived_features + additional_features:\n        if col in df.columns:\n            median = median_values.get(col, df[col].median()) if median_values else df[col].median()\n            df = df.with_columns(pl.col(col).fill_null(median if median is not None else 0.0))\n    \n    # Ensure unique columns\n    df = df.select([pl.col(col).alias(col) for col in df.columns])\n    \n    # Check for duplicate columns\n    all_cols = df.columns\n    if len(all_cols) != len(set(all_cols)):\n        duplicates = [col for col in set(all_cols) if all_cols.count(col) > 1]\n        logger.error(f\"Duplicate columns detected: {duplicates}\")\n        raise ValueError(f\"Duplicate columns detected: {duplicates}\")\n    \n    logger.info(f\"Final columns ({df.height} rows): {df.columns}\")\n    \n    # Feature list (exclude training-only features)\n    features = base_features + [col for col in derived_features if col in df.columns]\n    select_cols = [\"date_id\"] + features + ([\"target\"] if is_train else [])\n    return df.select(select_cols)\n\n# ============ MODEL TRAINING ============\nstart_time = time.time()\ntrain = load_trainset()\ntrain = create_features(train, is_train=True)\nfeatures = [col for col in train.columns if col not in ['date_id', 'target', 'P1_lag1', 'M11_lag1']]\n\n# Cache median values for imputation\nmedian_values = {col: train[col].median() if col in train.columns and train[col].is_null().mean() < 1.0 else 0.0 for col in features}\n\n# Check for NaNs\nX_train = train.select(features).to_pandas()\nif X_train.isna().any().any():\n    raise ValueError(f\"NaNs found in X_train for columns: {X_train.columns[X_train.isna().any()].tolist()}\")\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\ny_train = train['target'].to_pandas()\n\n# Train individual models\nenet_model = ElasticNet(alpha=params.enet_alpha, l1_ratio=params.enet_l1_ratio, max_iter=1000000)\nenet_model.fit(X_train, y_train)\n\nxgb_model = xgb.XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=params.xgb_n_estimators,\n    max_depth=params.xgb_max_depth,\n    learning_rate=params.xgb_learning_rate,\n    random_state=42\n)\nxgb_model.fit(X_train, y_train)\n\nlgb_model = lgb.LGBMRegressor(\n    objective='regression',\n    n_estimators=params.lgb_n_estimators,\n    max_depth=params.lgb_max_depth,\n    learning_rate=params.lgb_learning_rate,\n    random_state=42,\n    verbose=-1\n)\nlgb_model.fit(X_train, y_train)\n\n# Feature selection based on XGBoost importance\nfeature_importance = xgb_model.feature_importances_\nfeature_ranking = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\nfeatures = [f[0] for f in feature_ranking[:params.max_features]]\n\n# Retrain with selected features\nX_train = train.select(features).to_pandas()\nX_train = scaler.fit_transform(X_train)\nenet_model.fit(X_train, y_train)\nxgb_model.fit(X_train, y_train)\nlgb_model.fit(X_train, y_train)\n\n# Train meta-learner\nmeta_features = np.column_stack([\n    enet_model.predict(X_train),\n    xgb_model.predict(X_train),\n    lgb_model.predict(X_train)\n])\nmeta_model = LinearRegression()\nmeta_model.fit(meta_features, y_train)\n\n# Check startup time\nif time.time() - start_time > 900:\n    raise RuntimeError(\"Startup time exceeded 900 seconds\")\n\n# State for online learning\nprevious_lagged = None\ntest_row_count = 0\nlast_allocation = 0.0\nv1_median = train['V1'].median() if 'V1' in train.columns else 0.0\n\n# ============ VOLATILITY ESTIMATION ============\ndef estimate_volatility(test: pl.DataFrame, train: pl.DataFrame) -> float:\n    vol = test['V1'][0] if 'V1' in test.columns else (train['target'].tail(params.vol_window).std() or 0.01)\n    recent_returns = train['target'].tail(params.vol_window).to_numpy()\n    if len(recent_returns) > 1:\n        garch_vol = np.sqrt(0.3 * np.var(recent_returns) + 0.7 * vol**2)\n        return max(garch_vol, 0.01)\n    return max(vol, 0.01)\n\n# ============ PREDICTION FUNCTION ============\ndef predict(test: pl.DataFrame) -> float:\n    global previous_lagged, train, enet_model, xgb_model, lgb_model, meta_model, scaler, test_row_count, last_allocation, v1_median, features, median_values\n    \n    # Online learning: Update training data\n    if previous_lagged is not None and 'lagged_market_forward_excess_returns' in previous_lagged.columns:\n        append_row = previous_lagged.with_columns(\n            pl.col('lagged_market_forward_excess_returns').alias('target')\n        )\n        append_row = append_row.drop([col for col in [\"U1\", \"U2\", \"V1_S1\", \"M11_V1\", \"I9_S1\"] if col in append_row.columns])\n        append_row = create_features(append_row, is_train=False, median_values=median_values)\n        if append_row.height > 0:\n            # Align columns with train\n            missing_cols = [col for col in train.columns if col not in append_row.columns]\n            expressions = [pl.lit(median_values.get(col, 0.0)).cast(pl.Float64).alias(col) for col in missing_cols]\n            if expressions:\n                append_row = append_row.with_columns(expressions)\n            append_row = append_row.select(train.columns)\n            logger.info(f\"Appending row with shape {append_row.shape} to train with shape {train.shape}\")\n            train = train.vstack(append_row)\n            if train.height > params.max_train_rows:\n                train = train.tail(params.max_train_rows)\n        \n        # Retrain every `retrain_freq` rows\n        if test_row_count % params.retrain_freq == 0:\n            X_train = scaler.fit_transform(train.select(features).to_pandas())\n            y_train = train['target'].to_pandas()\n            if y_train.isna().any():\n                raise ValueError(\"NaNs found in y_train during retraining\")\n            enet_model.fit(X_train, y_train)\n            xgb_model.fit(X_train, y_train)\n            lgb_model.fit(X_train, y_train)\n            meta_features = np.column_stack([\n                enet_model.predict(X_train),\n                xgb_model.predict(X_train),\n                lgb_model.predict(X_train)\n            ])\n            meta_model.fit(meta_features, y_train)\n    \n    # Preprocess test data\n    test = test.drop([col for col in [\"U1\", \"U2\", \"V1_S1\", \"M11_V1\", \"I9_S1\"] if col in test.columns])\n    test = create_features(test, is_train=False, median_values=median_values)\n    \n    # Ensure no NaNs in test data\n    X_test = test.select(features).to_pandas()\n    if X_test.isna().any().any():\n        raise ValueError(f\"NaNs found in X_test for columns: {X_test.columns[X_test.isna().any()].tolist()}\")\n    \n    X_test = scaler.transform(X_test)\n    \n    # Ensemble prediction with meta-learner\n    meta_features = np.column_stack([\n        enet_model.predict(X_test),\n        xgb_model.predict(X_test),\n        lgb_model.predict(X_test)\n    ])\n    raw_pred = meta_model.predict(meta_features)[0]\n    \n    # Estimate volatility and dynamic signal multiplier\n    vol = estimate_volatility(test, train)\n    signal_multiplier = params.signal_multiplier_low_vol if ('V1' in test.columns and test['V1'][0] < v1_median) else params.signal_multiplier_high_vol\n    \n    # Convert to signal\n    signal = np.clip(\n        raw_pred * signal_multiplier,\n        params.min_signal,\n        params.max_signal\n    )\n    \n    # Volatility-adjusted allocation\n    allocation = min(params.max_signal, max(params.min_signal, signal / (vol * params.vol_scaling)))\n    \n    # Smooth allocation\n    transaction_cost = 0.000035  # Adjusted\n    allocation = (0.8 * allocation + 0.2 * last_allocation) * (1 - transaction_cost)\n    last_allocation = allocation\n    \n    # Update state\n    previous_lagged = test\n    test_row_count += 1\n    \n    return float(allocation)\n\n# ============ CROSS-VALIDATION ============\ndef cross_validate(predict_fn, min_train_size=1500, test_size=120):\n    train = pl.read_csv(DATA_PATH / \"train.csv\").rename({'market_forward_excess_returns': 'target'})\n    oof = np.full(train.height, np.nan)\n    score_list = []\n    for test_start in range(train.height - test_size, min_train_size, -test_size):\n        test_preliminary = train.slice(test_start, test_size)\n        solution = pd.DataFrame(\n            test_preliminary.select('target', 'risk_free_rate').to_pandas(),\n            columns=['forward_returns', 'risk_free_rate']\n        )\n        solution['row_id'] = range(test_start, test_start + test_size)\n        lagged = train.slice(test_start - 1, test_size)\n        test = (\n            test_preliminary\n            .drop('target', 'risk_free_rate')\n            .with_columns(\n                lagged.get_column('target').alias('lagged_market_forward_excess_returns')\n            )\n        )\n        allocation_list = []\n        for i in range(test.height):\n            allocation_list.append(predict_fn(test.slice(i, 1)))\n        submission = pd.DataFrame({'row_id': solution['row_id'], 'prediction': allocation_list})\n        validation_score = evaluate_submission(solution, submission, 'row_id')\n        score_list.append(validation_score)\n        oof[test_start:test_start + test_size] = allocation_list\n        logger.info(f\"Fold train(:{test_start}) test({test_start}:{test_start+test_size}) score: {validation_score:.3f}\")\n    overall_score = np.mean(score_list)\n    logger.info(f\"Average cross-validation score: {overall_score:.3f}\")\n    return overall_score\n\n# ============ LAUNCH SERVER ============\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway((str(DATA_PATH),))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T07:16:01.461543Z","iopub.execute_input":"2025-09-18T07:16:01.46186Z","iopub.status.idle":"2025-09-18T07:16:17.906444Z","shell.execute_reply.started":"2025-09-18T07:16:01.461835Z","shell.execute_reply":"2025-09-18T07:16:17.904507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(145deg, rgba(0, 51, 102, 0.98), rgba(211, 211, 211, 0.98));\n    backdrop-filter: blur(10px);\n    color: #e6f3ff;\n    font-size: 2em;\n    font-family: 'Montserrat', sans-serif;\n    font-weight: 700;\n    text-align: center;\n    border-radius: 30px;\n    border: 3px solid #000000;\n    padding: 30px 50px;\n    margin: 40px auto;\n    line-height: 1.6;\n    letter-spacing: 2px;\n    width: 85%;\n    text-transform: uppercase;\n    box-shadow: \n        0 0 25px rgba(0, 0, 0, 0.6), \n        0 0 45px rgba(0, 0, 0, 0.35), \n        inset 0 0 15px rgba(0, 0, 0, 0.3),\n        0 6px 28px rgba(0, 0, 0, 0.2);\n    position: relative;\n    overflow: hidden;\n    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n    <div style=\"\n        position: absolute;\n        top: -50%;\n        left: -50%;\n        width: 200%;\n        height: 200%;\n        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n        animation: rotateGradient 8s infinite ease-in-out;\">\n    </div>\n    üëç If Liked My Work Kindly Share and Upvote\n</div>\n\n<style>\ndiv:hover {\n    transform: translateY(-5px) scale(1.02);\n    box-shadow: \n        0 0 35px rgba(0, 0, 0, 0.8), \n        0 0 60px rgba(0, 0, 0, 0.5), \n        inset 0 0 20px rgba(0, 0, 0, 0.35),\n        0 10px 40px rgba(0, 0, 0, 0.25);\n    border-color: #000000;\n}\n\n@keyframes rotateGradient {\n    0% { transform: rotate(0deg); opacity: 0.3; }\n    50% { opacity: 0.5; }\n    100% { transform: rotate(360deg); opacity: 0.3; }\n}\n</style>","metadata":{}}]}