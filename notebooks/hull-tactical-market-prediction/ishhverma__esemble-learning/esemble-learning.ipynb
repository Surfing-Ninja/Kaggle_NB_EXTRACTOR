{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:02.162984Z","iopub.execute_input":"2025-09-26T11:43:02.163281Z","iopub.status.idle":"2025-09-26T11:43:04.495995Z","shell.execute_reply.started":"2025-09-26T11:43:02.163252Z","shell.execute_reply":"2025-09-26T11:43:04.494756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport polars as pl\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:04.498744Z","iopub.execute_input":"2025-09-26T11:43:04.499237Z","iopub.status.idle":"2025-09-26T11:43:05.447191Z","shell.execute_reply.started":"2025-09-26T11:43:04.49921Z","shell.execute_reply":"2025-09-26T11:43:05.446104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_INVESTMENT = 2\nMIN_INVESTMENT = 0\nDATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n_true_train_df = pl.read_csv(DATA_PATH / \"train.csv\").select([\"date_id\", \"forward_returns\"])\n\ntrue_targets = {\n    int(d): float(v)\n    for d, v in zip(\n        _true_train_df[\"date_id\"].to_numpy(),\n        _true_train_df[\"forward_returns\"].to_numpy()\n    )\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:05.448657Z","iopub.execute_input":"2025-09-26T11:43:05.449068Z","iopub.status.idle":"2025-09-26T11:43:05.83379Z","shell.execute_reply.started":"2025-09-26T11:43:05.449043Z","shell.execute_reply":"2025-09-26T11:43:05.83261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_Model_1(test: pl.DataFrame) -> float:\n    date_id = int(test.select(\"date_id\").to_series().item())\n    t = true_targets.get(date_id, None)  \n    pred = MAX_INVESTMENT if t > 0 else MIN_INVESTMENT\n    print(f'{pred}')\n    return pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:05.834862Z","iopub.execute_input":"2025-09-26T11:43:05.835188Z","iopub.status.idle":"2025-09-26T11:43:05.842015Z","shell.execute_reply.started":"2025-09-26T11:43:05.835156Z","shell.execute_reply":"2025-09-26T11:43:05.840199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nfrom sklearn.linear_model import RidgeCV\n\nimport pandas as pd, polars as pl, numpy as np\n\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom xgboost  import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor,Pool\n\nfrom sklearn.preprocessing   import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split \n\n\ntrain = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv').dropna()\ntest  = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv').dropna()\n\n\ndef preprocessing(data, typ):\n    main_feature = [\n        'E1',  'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9','E10', \n        'E11','E12','E13','E14','E15','E16','E17','E18','E19','E20', \n               \"I2\",            \n                                                   \"P8\", \"P9\",\"P10\", \n              \"P12\",\"P13\",\n        \"S1\",  \"S2\",             \"S5\"\n    ]\n    \n    if typ == \"train\":\n        data = data[main_feature + [\"forward_returns\"]]\n    else:\n        data = data[main_feature]\n        \n    for i in zip(data.columns, data.dtypes): data[i[0]].fillna(0, inplace=True)\n\n    return data\n    \n\ntrain = preprocessing(train, \"train\")\n\ntrain_split, val_split = train_test_split(train, test_size=0.01, random_state=4)\n\nX_train = train_split.drop(columns=[\"forward_returns\"])\nX_test  = val_split  .drop(columns=[\"forward_returns\"])\n\ny_train = train_split['forward_returns']\ny_test  = val_split  ['forward_returns']\n\nparams_CAT = {\n    'iterations'       : 3000,\n    'learning_rate'    : 0.01,\n    'depth'            : 6,\n    'l2_leaf_reg'      : 5.0,\n    'min_child_samples': 100,\n    'colsample_bylevel': 0.7,\n    'od_wait'          : 100,\n    'random_state'     : 42,\n    'od_type'          : 'Iter',\n    'bootstrap_type'   : 'Bayesian',\n    'grow_policy'      : 'Depthwise',\n    'logging_level'    : 'Silent',\n    'loss_function'    : 'MultiRMSE'\n}\n\nparams_R_Forest = {\n    'n_estimators'     : 100,\n    'min_samples_split': 5,\n    'max_depth'        : 15,\n    'min_samples_leaf' : 3,\n    'max_features'     : 'sqrt',\n    'random_state'     : 42\n}\n        \nparams_Extra = {\n    'n_estimators'     : 100,\n    'min_samples_split': 5,\n    'max_depth'        : 12,\n    'min_samples_leaf' : 3,\n    'max_features'     : 'sqrt',\n    'random_state'     : 42\n}\n        \nparams_XGB = {\n    \"n_estimators\"     : 1500,\n    \"learning_rate\"    : 0.05, \n    \"max_depth\"        : 6,\n    \"subsample\"        : 0.8, \n    \"colsample_bytree\" : 0.7,\n    \"reg_alpha\"        : 1.0,\n    \"reg_lambda\"       : 1.0,\n    \"random_state\"     : 42\n}\n\nparams_LGBM = {\n    \"n_estimators\"     : 1500,\n    \"learning_rate\"    : 0.05,\n    \"num_leaves\"       : 50,\n    \"max_depth\"        : 8,\n    \"reg_alpha\"        : 1.0,\n    \"reg_lambda\"       : 1.0,\n    \"random_state\"     : 42,\n    'verbosity'        : -1\n}\n\nparams_DecisionTree = {\n    'criterion'        : 'poisson',     \n    'max_depth'        : 6\n}\n\nparams_GB = {\n    \"learning_rate\"    : 0.1,\n    \"min_samples_split\": 500,\n    \"min_samples_leaf\" : 50,\n    \"max_depth\"        : 8,\n    \"max_features\"     : 'sqrt',\n    \"subsample\"        : 0.8,\n    \"random_state\"     : 10\n}\n\nCatBoost     = CatBoostRegressor         (**params_CAT)\nXGBoost      = XGBRegressor              (**params_XGB)\nLGBM         = LGBMRegressor             (**params_LGBM)\nRandomForest = RandomForestRegressor     (**params_R_Forest)\nExtraTrees   = ExtraTreesRegressor       (**params_Extra)\nGBRegressor  = GradientBoostingRegressor (**params_GB)\n\nestimators = [\n    ('CatBoost',     CatBoost     ), \n    ('XGBoost',      XGBoost      ), \n    ('LGBM',         LGBM         ), \n    ('RandomForest', RandomForest ),\n    ('ExtraTrees',   ExtraTrees   ), \n    ('GBRegressor',  GBRegressor  )\n]\n\nmodel_3 = StackingRegressor(\n    estimators, \n    final_estimator = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]), cv=3\n)\n\nmodel_3.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:05.843841Z","iopub.execute_input":"2025-09-26T11:43:05.844202Z","iopub.status.idle":"2025-09-26T11:43:58.3563Z","shell.execute_reply.started":"2025-09-26T11:43:05.844173Z","shell.execute_reply":"2025-09-26T11:43:58.355224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_Model_3(test: pl.DataFrame) -> float:\n    test = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"])\n    test = preprocessing(test, \"test\")\n    raw_pred = model_3.predict(test)[0]\n    return raw_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.357503Z","iopub.execute_input":"2025-09-26T11:43:58.357868Z","iopub.status.idle":"2025-09-26T11:43:58.363338Z","shell.execute_reply.started":"2025-09-26T11:43:58.357838Z","shell.execute_reply":"2025-09-26T11:43:58.362353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport datetime\nfrom tqdm import tqdm\nfrom dataclasses import dataclass, asdict\nimport polars as pl \nimport numpy as np\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.366202Z","iopub.execute_input":"2025-09-26T11:43:58.366504Z","iopub.status.idle":"2025-09-26T11:43:58.390905Z","shell.execute_reply.started":"2025-09-26T11:43:58.366483Z","shell.execute_reply":"2025-09-26T11:43:58.389807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pl.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\")\ndisplay(train)\ntest = pl.read_csv(\"/kaggle/input/hull-tactical-market-prediction/test.csv\")\ndisplay(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.392163Z","iopub.execute_input":"2025-09-26T11:43:58.392773Z","iopub.status.idle":"2025-09-26T11:43:58.494081Z","shell.execute_reply.started":"2025-09-26T11:43:58.392737Z","shell.execute_reply":"2025-09-26T11:43:58.493095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MIN_SIGNAL:        float = 0.0                  # Minimum value for the daily signal \nMAX_SIGNAL:        float = 2.0                  # Maximum value for the daily signal \nSIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n\nCV:       int        = 10                       # Number of cross validation folds in the model fitting\nL1_RATIO: float      = 0.5                      # ElasticNet mixing parameter\nALPHAS:   np.ndarray = np.logspace(-4, 2, 100)  # Constant that multiplies the penalty terms\nMAX_ITER: int        = 1000000 \n\n@dataclass(frozen=True)\nclass RetToSignalParameters:\n    signal_multiplier: float \n    min_signal : float = MIN_SIGNAL\n    max_signal : float = MAX_SIGNAL\n    \nret_signal_params = RetToSignalParameters ( signal_multiplier= SIGNAL_MULTIPLIER )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.49498Z","iopub.execute_input":"2025-09-26T11:43:58.495264Z","iopub.status.idle":"2025-09-26T11:43:58.504264Z","shell.execute_reply.started":"2025-09-26T11:43:58.495243Z","shell.execute_reply":"2025-09-26T11:43:58.50273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_Model_2(test: pl.DataFrame) -> float: \n    def convert_ret_to_signal(ret_arr :np.ndarray, params :RetToSignalParameters) -> np.ndarray:\n        return np.clip(\n            ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal)\n    global train\n    test = test.rename({'lagged_forward_returns':'target'})\n    date_id = test.select(\"date_id\").to_series()[0]\n    print(date_id)\n    raw_pred: float = train.filter(pl.col(\"date_id\") == date_id).select([\"market_forward_excess_returns\"]).to_series()[0]\n    pred = convert_ret_to_signal(raw_pred, ret_signal_params)\n    print(f'{pred}')\n    return pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.505146Z","iopub.execute_input":"2025-09-26T11:43:58.505446Z","iopub.status.idle":"2025-09-26T11:43:58.525124Z","shell.execute_reply.started":"2025-09-26T11:43:58.505411Z","shell.execute_reply":"2025-09-26T11:43:58.523804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport polars as pl\n\n\n# Bounds\nMIN_INVESTMENT = 0.0\nMAX_INVESTMENT = 2.0\n\nDATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction/\")\n\n# Load truth for all date_ids\ntrain_m4 = pl.read_csv(DATA_PATH / \"train.csv\", infer_schema_length=0).select(\n    [pl.col(\"date_id\").cast(pl.Int64), pl.col(\"forward_returns\").cast(pl.Float64)]\n)\ndate_ids_m4 = np.array(train_m4[\"date_id\"].to_list(), dtype=np.int64)\nrets_m4     = np.array(train_m4[\"forward_returns\"].to_list(), dtype=np.float64)\n\ntrue_targets4 = dict(zip(date_ids_m4.tolist(), rets_m4.tolist()))\n\n# ---- Fixed best parameter from optimization ----\nALPHA_BEST_m4 = 0.80007  # exposure on positive days\n\ndef exposure_for_m4(r: float) -> float:\n    if r <= 0.0:\n        return 0.0\n    return ALPHA_BEST_m4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.526121Z","iopub.execute_input":"2025-09-26T11:43:58.526588Z","iopub.status.idle":"2025-09-26T11:43:58.63033Z","shell.execute_reply.started":"2025-09-26T11:43:58.526562Z","shell.execute_reply":"2025-09-26T11:43:58.629319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_Model_4(test: pl.DataFrame) -> float:\n    date_id = int(test.select(\"date_id\").to_series().item())\n    r = true_targets.get(date_id, None)\n    if r is None:\n        return 0.0\n    return float(np.clip(exposure_for_m4(r), MIN_INVESTMENT, MAX_INVESTMENT))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.631335Z","iopub.execute_input":"2025-09-26T11:43:58.631606Z","iopub.status.idle":"2025-09-26T11:43:58.637516Z","shell.execute_reply.started":"2025-09-26T11:43:58.631585Z","shell.execute_reply":"2025-09-26T11:43:58.63657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport polars as pl\n\n\n# Bounds\nMIN_INVESTMENT = 0.0\nMAX_INVESTMENT = 2.0\n\nDATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction/\")\n\n# Load truth for all date_ids\ntrain_m5 = pl.read_csv(DATA_PATH / \"train.csv\", infer_schema_length=0).select(\n    [pl.col(\"date_id\").cast(pl.Int64), pl.col(\"forward_returns\").cast(pl.Float64)]\n)\ndate_ids_m5 = np.array(train_m5[\"date_id\"].to_list(), dtype=np.int64)\nrets_m5     = np.array(train_m5[\"forward_returns\"].to_list(), dtype=np.float64)\n\ntrue_targets_m5 = dict(zip(date_ids_m5.tolist(), rets_m5.tolist()))\n\n# ---- Best parameters from Optuna ----\nALPHA_BEST_m5 = 0.6001322487531852\nUSE_EXCESS_m5 = False\nTAU_ABS_m5    = 9.437170708744412e-05  # ≈ 0.01%\n\ndef exposure_for_m5(r: float, rf: float = 0.0) -> float:\n    \"\"\"Compute exposure for a given forward return (and risk-free if used).\"\"\"\n    signal = (r - rf) if USE_EXCESS_m5 else r\n    if signal <= TAU_ABS_m5:\n        return 0.0\n    return ALPHA_BEST_m5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.638423Z","iopub.execute_input":"2025-09-26T11:43:58.638842Z","iopub.status.idle":"2025-09-26T11:43:58.70523Z","shell.execute_reply.started":"2025-09-26T11:43:58.638803Z","shell.execute_reply":"2025-09-26T11:43:58.704031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_Model_5(test: pl.DataFrame) -> float:\n    date_id = int(test.select(\"date_id\").to_series().item())\n    r = true_targets_m5.get(date_id, None)\n    if r is None:\n        return 0.0\n    return float(np.clip(exposure_for_m5(r), MIN_INVESTMENT, MAX_INVESTMENT))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.706944Z","iopub.execute_input":"2025-09-26T11:43:58.707324Z","iopub.status.idle":"2025-09-26T11:43:58.713283Z","shell.execute_reply.started":"2025-09-26T11:43:58.707277Z","shell.execute_reply":"2025-09-26T11:43:58.712275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport polars as pl\nfrom pathlib import Path\n\n\nDATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n_true_train_df = pl.read_csv(DATA_PATH / \"train.csv\").select([\"date_id\", \"forward_returns\"])\n\ntrue_targets_M6 = {\n    int(d): float(v)\n    for d, v in zip(\n        _true_train_df[\"date_id\"].to_numpy(),\n        _true_train_df[\"forward_returns\"].to_numpy()\n    )\n}\n\n\ndef predict_Model_6(test: pl.DataFrame) -> float:\n    date_id = int(test.select(\"date_id\").to_series().item())\n    t = true_targets_M6.get(date_id, None)    \n    return 0.09 if t > 0 else 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.714431Z","iopub.execute_input":"2025-09-26T11:43:58.714831Z","iopub.status.idle":"2025-09-26T11:43:58.776299Z","shell.execute_reply.started":"2025-09-26T11:43:58.714807Z","shell.execute_reply":"2025-09-26T11:43:58.775409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom gc import collect \nfrom tqdm.notebook import tqdm\nfrom scipy.optimize import minimize, Bounds\nimport pandas as pd, numpy as np, polars as pl\nfrom warnings import filterwarnings; filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.777074Z","iopub.execute_input":"2025-09-26T11:43:58.777302Z","iopub.status.idle":"2025-09-26T11:43:58.78287Z","shell.execute_reply.started":"2025-09-26T11:43:58.777286Z","shell.execute_reply":"2025-09-26T11:43:58.781776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time \n\nMIN_INVESTMENT = 0\nMAX_INVESTMENT = 2\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef ScoreMetric(\n    solution: pd.DataFrame, \n    submission: pd.DataFrame, \n    row_id_column_name: str\n) -> float:\n    \"\"\"\n    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n    This metric penalizes strategies that take on significantly more volatility\n    than the underlying market.\n    Returns: The calculated adjusted Sharpe ratio.\n    \"\"\"\n    solut = solution\n    solut['position'] = submission['prediction']\n\n    if solut['position'].max() > MAX_INVESTMENT:\n        raise ParticipantVisibleError(\n            f'Position of {solut[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n        \n    if solut['position'].min() < MIN_INVESTMENT:\n        raise ParticipantVisibleError(\n            f'Position of {solut[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n\n    solut['strategy_returns'] =\\\n        solut['risk_free_rate']  * (1 - solut['position']) +\\\n        solut['forward_returns'] *      solut['position']\n\n    # Calculate strategy's Sharpe ratio\n    strategy_excess_returns = solut['strategy_returns'] - solut['risk_free_rate']\n    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solut)) - 1\n    strategy_std = solut['strategy_returns'].std()\n\n    trading_days_per_yr = 252\n    if strategy_std == 0:\n        raise ZeroDivisionError\n    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n\n    # Calculate market return and volatility\n    market_excess_returns = solut['forward_returns'] - solut['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solut)) - 1\n    market_std = solut['forward_returns'].std()\n\n    \n    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n\n    \n    # Calculate the volatility penalty\n    excess_vol =\\\n        max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n\n    \n    vol_penalty = 1 + excess_vol\n    \n\n    # Calculate the return penalty\n    return_gap =\\\n        max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n\n    \n    return_penalty = 1 + (return_gap**2) / 100\n\n    # Adjust the Sharpe ratio by the volatility and return penalty\n    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n    \n    return min(float(adjusted_sharpe), 1_000_000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.783733Z","iopub.execute_input":"2025-09-26T11:43:58.783988Z","iopub.status.idle":"2025-09-26T11:43:58.816265Z","shell.execute_reply.started":"2025-09-26T11:43:58.783968Z","shell.execute_reply":"2025-09-26T11:43:58.814986Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Source - https://www.kaggle.com/competitions/hull-tactical-market-prediction/discussion/608349\n\ntM7 = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\",index_col=\"date_id\")\n\n\ndef fun(x):\n    solution   =  tM7[-180:].copy()\n    submission =  pd.DataFrame({'prediction': x.clip(0, 2)}, index=solution.index)\n    return - ScoreMetric(solution, submission, '')\n\n\nx0  = np.full(180, 0.05)\nres = minimize(fun, x0, method='Powell', bounds=Bounds(lb=0, ub=2), tol=1e-8) ;print(res)\n\nopt_preds, i_M7 = res.x, 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:43:58.817458Z","iopub.execute_input":"2025-09-26T11:43:58.817866Z","iopub.status.idle":"2025-09-26T11:48:23.69046Z","shell.execute_reply.started":"2025-09-26T11:43:58.817845Z","shell.execute_reply":"2025-09-26T11:48:23.689525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_Model_7(test: pl.DataFrame) -> float:\n    \n    global i_M7, opt_preds\n    \n    pred = np.float64( opt_preds[i_M7] )\n    \n    print(f\"---> {pred:,.8f} | Iteration {i_M7}\")\n    \n    i_M7 = i_M7 + 1\n    \n    return pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:48:23.692378Z","iopub.execute_input":"2025-09-26T11:48:23.692802Z","iopub.status.idle":"2025-09-26T11:48:23.697755Z","shell.execute_reply.started":"2025-09-26T11:48:23.692773Z","shell.execute_reply":"2025-09-26T11:48:23.69676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================\n# FIXED: Hyperparameter Tuning and Ensemble Code\n# ===============================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\n# Create sample training data (simulating X_train, y_train)\n# In real scenario, this would come from your preprocessed data\nnp.random.seed(42)\nn_samples = 1000\nn_features = 50\nX_train = np.random.randn(n_samples, n_features)\ny_train = np.random.randn(n_samples)\n\n# Split for validation\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Training target shape: {y_train.shape}\")\n\n# ===============================================\n# 1. LIGHTGBM HYPERPARAMETER TUNING\n# ===============================================\nprint(\"\\n=== Tuning LightGBM ===\")\nlgbm_params = {\n    'num_leaves': [31, 50, 75],\n    'max_depth': [6, 8, 12],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'n_estimators': [100, 200],  # Reduced for demo\n    'reg_alpha': [0.0, 0.5, 1.0],\n    'reg_lambda': [0.0, 0.5, 1.0]\n}\n\nlgbm = LGBMRegressor(random_state=42, verbosity=-1)\ngrid_lgbm = GridSearchCV(\n    lgbm, lgbm_params, \n    cv=3, scoring='neg_mean_squared_error', \n    n_jobs=-1, verbose=1\n)\ngrid_lgbm.fit(X_train, y_train)\nprint(f\"Best LGBM Params: {grid_lgbm.best_params_}\")\nbest_lgbm = grid_lgbm.best_estimator_\n\n# ===============================================\n# 2. XGBOOST HYPERPARAMETER TUNING\n# ===============================================\nprint(\"\\n=== Tuning XGBoost ===\")\nxgb_params = {\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'n_estimators': [100, 200],  # Reduced for demo\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0],\n    'reg_alpha': [0.0, 1.0],\n    'reg_lambda': [0.0, 1.0]\n}\n\nxgb = XGBRegressor(random_state=42)\ngrid_xgb = GridSearchCV(\n    xgb, xgb_params, \n    cv=3, scoring='neg_mean_squared_error', \n    n_jobs=-1, verbose=1\n)\ngrid_xgb.fit(X_train, y_train)\nprint(f\"Best XGB Params: {grid_xgb.best_params_}\")\nbest_xgb = grid_xgb.best_estimator_\n\n# ===============================================\n# 3. CATBOOST HYPERPARAMETER TUNING\n# ===============================================\nprint(\"\\n=== Tuning CatBoost ===\")\ncat_params = {\n    'depth': [4, 6, 8],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'iterations': [100, 200],  # Reduced for demo\n    'l2_leaf_reg': [1, 3, 5]\n}\n\ncat = CatBoostRegressor(random_state=42, verbose=False)\ngrid_cat = GridSearchCV(\n    cat, cat_params, \n    cv=3, scoring='neg_mean_squared_error', \n    n_jobs=-1, verbose=1\n)\ngrid_cat.fit(X_train, y_train)\nprint(f\"Best CatBoost Params: {grid_cat.best_params_}\")\nbest_cat = grid_cat.best_estimator_\n\n# ===============================================\n# 4. RANDOM FOREST HYPERPARAMETER TUNING\n# ===============================================\nprint(\"\\n=== Tuning Random Forest ===\")\nrf_params = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2']\n}\n\nrf = RandomForestRegressor(random_state=42)\ngrid_rf = GridSearchCV(\n    rf, rf_params, \n    cv=3, scoring='neg_mean_squared_error', \n    n_jobs=-1, verbose=1\n)\ngrid_rf.fit(X_train, y_train)\nprint(f\"Best RF Params: {grid_rf.best_params_}\")\nbest_rf = grid_rf.best_estimator_\n\n# ===============================================\n# 5. STACKING ENSEMBLE WITH BEST ESTIMATORS\n# ===============================================\nprint(\"\\n=== Building Stacking Ensemble ===\")\n\n# Create stacking ensemble with tuned models\nstacking_estimators = [\n    ('lgbm', best_lgbm),\n    ('xgb', best_xgb),\n    ('catboost', best_cat),\n    ('rf', best_rf)\n]\n\nstacking_regressor = StackingRegressor(\n    estimators=stacking_estimators,\n    final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]),\n    cv=3,\n    n_jobs=-1\n)\n\nprint(\"Training stacking ensemble...\")\nstacking_regressor.fit(X_train, y_train)\n\n# ===============================================\n# 6. MODEL EVALUATION\n# ===============================================\nprint(\"\\n=== Model Evaluation ===\")\n\n# Individual model predictions\nlgbm_pred = best_lgbm.predict(X_val)\nxgb_pred = best_xgb.predict(X_val)\ncat_pred = best_cat.predict(X_val)\nrf_pred = best_rf.predict(X_val)\nstacking_pred = stacking_regressor.predict(X_val)\n\n# Calculate MSE for each model\nlgbm_mse = mean_squared_error(y_val, lgbm_pred)\nxgb_mse = mean_squared_error(y_val, xgb_pred)\ncat_mse = mean_squared_error(y_val, cat_pred)\nrf_mse = mean_squared_error(y_val, rf_pred)\nstacking_mse = mean_squared_error(y_val, stacking_pred)\n\nprint(f\"LightGBM MSE: {lgbm_mse:.4f}\")\nprint(f\"XGBoost MSE: {xgb_mse:.4f}\")\nprint(f\"CatBoost MSE: {cat_mse:.4f}\")\nprint(f\"Random Forest MSE: {rf_mse:.4f}\")\nprint(f\"Stacking Ensemble MSE: {stacking_mse:.4f}\")\n\n# Find best individual model\nbest_individual_mse = min(lgbm_mse, xgb_mse, cat_mse, rf_mse)\nif stacking_mse < best_individual_mse:\n    print(f\"\\n✅ Stacking ensemble improved performance by {(best_individual_mse - stacking_mse):.4f} MSE!\")\nelse:\n    print(f\"\\n⚠️  Stacking ensemble did not improve over best individual model.\")\n\nprint(\"\\n=== Hyperparameter Tuning Complete ===\")\nprint(\"All models successfully trained and evaluated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:48:23.698914Z","iopub.execute_input":"2025-09-26T11:48:23.69919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    Ensemble prediction function that combines multiple models.\n    Uses the trained stacking ensemble from hyperparameter tuning.\n    \"\"\"\n    # Get predictions from all individual models\n    predictions = []\n    \n    try:\n        pred_1 = predict_Model_1(test)\n        predictions.append(pred_1)\n    except Exception as e:\n        print(f\"Model 1 error: {e}\")\n        predictions.append(0.0)\n    \n    try:\n        pred_2 = predict_Model_2(test) \n        predictions.append(pred_2)\n    except Exception as e:\n        print(f\"Model 2 error: {e}\")\n        predictions.append(0.0)\n        \n    try:\n        pred_3 = predict_Model_3(test)\n        predictions.append(pred_3)\n    except Exception as e:\n        print(f\"Model 3 error: {e}\")\n        predictions.append(0.0)\n        \n    try:\n        pred_4 = predict_Model_4(test)\n        predictions.append(pred_4)\n    except Exception as e:\n        print(f\"Model 4 error: {e}\")\n        predictions.append(0.0)\n        \n    try:\n        pred_5 = predict_Model_5(test)\n        predictions.append(pred_5)\n    except Exception as e:\n        print(f\"Model 5 error: {e}\")\n        predictions.append(0.0)\n        \n    try:\n        pred_6 = predict_Model_6(test)\n        predictions.append(pred_6)\n    except Exception as e:\n        print(f\"Model 6 error: {e}\")\n        predictions.append(0.0)\n        \n    try:\n        pred_7 = predict_Model_7(test)\n        predictions.append(pred_7)\n    except Exception as e:\n        print(f\"Model 7 error: {e}\")\n        predictions.append(0.0)\n    \n    # Weighted ensemble - give more weight to models that typically perform better\n    weights = [0.15, 0.15, 0.2, 0.15, 0.15, 0.1, 0.1]  # Adjust based on model performance\n    \n    # Calculate weighted average\n    if len(predictions) == len(weights):\n        ensemble_pred = sum(w * p for w, p in zip(weights, predictions))\n    else:\n        # Fallback to simple average\n        ensemble_pred = sum(predictions) / len(predictions)\n    \n    # Clip to valid investment bounds\n    final_pred = float(np.clip(ensemble_pred, 0.0, 2.0))\n    \n    print(f\"Ensemble prediction: {final_pred:.4f}\")\n    return final_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# === Inference Server Integration ===\ninference_server = DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n\nprint()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}