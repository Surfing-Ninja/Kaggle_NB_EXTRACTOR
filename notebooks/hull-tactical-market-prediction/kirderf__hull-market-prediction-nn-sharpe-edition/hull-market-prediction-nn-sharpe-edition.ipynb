{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# |-------|-------------|\n# | **Vol-Constrained Sharpe Loss** | `-mean_ret / std_ret + λ × (std_ret - 0.4)²` |\n# | **EMA (Exponential Moving Average)** | Starts at epoch 5 → **smooths weights**|\n# | **Sharpe-Aware AdamW** | LR boosted by recent Sharpe improvement  |\n# | **Online Vol Scaling (inference)** | 20-day rolling std |\n# | **10-fold Walk-Forward CV** | 180-day validation windows |\n# | **Model Averaging** | Mean of 10 EMA models |\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os, math, random, numpy as np, pandas as pd, polars as pl\nfrom pathlib import Path\nimport torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport kaggle_evaluation.default_inference_server\n\n# ==============================================================\n#  1. DETERMINISTIC\n# ==============================================================\ndef set_deterministic(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.use_deterministic_algorithms(True, warn_only=True)\n\nset_deterministic(42)\n\n# --------------------------------------------------------------\n#  CONFIG\n# --------------------------------------------------------------\nDATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\nLB_RUN = True\nN_FOLDS = 10\nTEST_SIZE = 180\nTRAIN_EPOCHS = 60\nBATCH_SIZE = 1024\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_WORKERS = 0\nEMA_DECAY = 0.995\nEMA_START = 5\nTARGET_VOL = 0.4\nWARMUP = 5\nVOL_PENALTY_LAMBDA = 0.01  # From NeuriPS 2024\nLABEL_SMOOTHING = 0.1\n\n# --------------------------------------------------------------\n#  METRIC\n# --------------------------------------------------------------\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    sol = solution.copy()\n    sol['position'] = submission['prediction'].clip(0, 2)\n    sol['strategy_returns'] = sol['risk_free_rate'] * (1 - sol['position']) + sol['position'] * sol['forward_returns']\n    excess = sol['strategy_returns'] - sol['risk_free_rate']\n    cum = (1 + excess).prod()\n    ann_ret = cum ** (252 / len(sol)) - 1\n    ann_vol = sol['strategy_returns'].std() * np.sqrt(252) * 100\n    if ann_vol == 0: return 0\n    sharpe = ann_ret / (sol['strategy_returns'].std() * np.sqrt(252))\n    m_excess = sol['forward_returns'] - sol['risk_free_rate']\n    m_cum = (1 + m_excess).prod()\n    m_ann_ret = m_cum ** (252 / len(sol)) - 1\n    m_ann_vol = sol['forward_returns'].std() * np.sqrt(252) * 100\n    if m_ann_vol == 0: return 0\n    vol_penalty = 1 + max(0, ann_vol / m_ann_vol - 1.2)\n    ret_gap = max(0, (m_ann_ret - ann_ret) * 252 * 100)\n    ret_penalty = 1 + (ret_gap**2) / 100\n    return min(sharpe / (vol_penalty * ret_penalty), 1_000_000)\n\n# --------------------------------------------------------------\n#  DATA\n# --------------------------------------------------------------\ndef load_full_train() -> pl.DataFrame:\n    df = pl.read_csv(DATA_PATH / \"train.csv\")\n    return (df\n            .rename({'market_forward_excess_returns': 'target'})\n            .with_columns(pl.exclude('date_id').cast(pl.Float32))\n            .sort('date_id'))\n\ndef create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.with_columns(pl.exclude(['date_id','target']).cast(pl.Float32))\n    df = df.with_columns([pl.col(c).fill_null(0.0) for c in df.columns if c not in ['date_id','target']])\n    return df\n\ndef get_features(train_df, test_df) -> list[str]:\n    train_set = set(train_df.columns) - {'date_id','target','forward_returns','risk_free_rate'}\n    test_set  = set(test_df.columns) - {'date_id','target','is_scored','lagged_risk_free_rate','lagged_market_forward_excess_returns'}\n    return list(train_set & test_set)\n\ndef prepare_training_data(df: pl.DataFrame, features: list[str]):\n    df = df.with_columns(pl.col(features).cast(pl.Float32))\n    X = df.select(features).to_numpy().astype(np.float32)\n    y = df['target'].to_numpy().astype(np.float32)\n    fr = df.get_column('forward_returns').to_numpy().astype(np.float32) if 'forward_returns' in df.columns else np.zeros(len(y), dtype=np.float32)\n    return X, y, fr\n\n# --------------------------------------------------------------\n#  WALK-FORWARD\n# --------------------------------------------------------------\ndef walk_forward_split(n_samples: int):\n    need = N_FOLDS * TEST_SIZE\n    if n_samples < need: raise ValueError('Not enough data')\n    start = n_samples - need\n    splits = []\n    for i in range(N_FOLDS):\n        val_start = start + i*TEST_SIZE\n        val_end   = val_start + TEST_SIZE\n        train_idx = np.arange(0, val_start)\n        val_idx   = np.arange(val_start, val_end)\n        splits.append((train_idx, val_idx))\n    return splits\n\n# --------------------------------------------------------------\n#  LOSS – VOL CONSTRAINT\n# --------------------------------------------------------------\ndef sharpe_loss(y_pred, y_true, eps=1e-6):\n    y_pred = y_pred.clamp(-3, 3)\n    position = torch.tanh(y_pred) * 2.0\n    position = position.clamp(0, 2)\n    strategy_ret = position * y_true\n    mean_ret = strategy_ret.mean()\n    std_ret = strategy_ret.std() + eps\n    sharpe = -mean_ret / std_ret\n    # Vol constraint\n    vol_penalty = VOL_PENALTY_LAMBDA * (std_ret - TARGET_VOL) ** 2\n    return sharpe + vol_penalty\n\n# --------------------------------------------------------------\n#  EMA\n# --------------------------------------------------------------\nclass EMA:\n    def __init__(self, model, decay=0.995):\n        self.decay = decay\n        self.shadow = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n    def update(self, model):\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[n] = self.decay * self.shadow[n] + (1.0 - self.decay) * p.detach()\n    def apply(self, model):\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                p.data.copy_(self.shadow[n])\n\n# --------------------------------------------------------------\n#  OPTIMIZER\n# --------------------------------------------------------------\nclass SharpeAwareAdamW(optim.Optimizer):\n    def __init__(self, params, lr=3e-3, warmup=5, T_max=60, weight_decay=1e-5):\n        defaults = dict(lr=lr, warmup=warmup, T_max=T_max, weight_decay=weight_decay, t=0, sharpe_hist=[])\n        super().__init__(params, defaults)\n\n    def set_epoch_sharpe(self, sharpe):\n        for group in self.param_groups:\n            group['sharpe_hist'].append(sharpe)\n            if len(group['sharpe_hist']) > 5: group['sharpe_hist'].pop(0)\n\n    def get_lr(self, epoch):\n        warmup = self.param_groups[0]['warmup']\n        T_max = self.param_groups[0]['T_max']\n        base_lr = self.param_groups[0]['lr']\n        if epoch < warmup:\n            return base_lr * (epoch + 1) / warmup\n        else:\n            progress = (epoch - warmup) / (T_max - warmup)\n            return base_lr * 0.5 * (1 + math.cos(math.pi * progress))\n\n    def _sharpe_boost(self):\n        hist = self.param_groups[0]['sharpe_hist']\n        if len(hist) < 2: return 1.0\n        improvement = (hist[-1] - hist[-2]) / (abs(hist[-2]) + 1e-8)\n        return max(0.5, min(3.0, 1.0 + 3*improvement))\n\n    @torch.no_grad()\n    def step(self, closure=None, epoch=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            lr = self.get_lr(epoch)\n            boost = self._sharpe_boost()\n            lr *= boost\n\n            for p in group['params']:\n                if p.grad is None: continue\n                grad = p.grad\n                state = self.state[p]\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                state['step'] += 1\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(p, alpha=group['weight_decay'])\n\n                beta1, beta2 = 0.9, 0.999\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                bias1 = 1 - beta1 ** state['step']\n                bias2 = 1 - beta2 ** state['step']\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias2)).add_(1e-8)\n                step_size = lr / bias1\n                p.addcdiv_(exp_avg, denom, value=-step_size)\n        return loss\n\n# --------------------------------------------------------------\n#  MODEL\n# --------------------------------------------------------------\nclass TradingNN(nn.Module):\n    def __init__(self, input_dim, dropout=0.3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256), nn.BatchNorm1d(256), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(128, 64),  nn.BatchNorm1d(64),  nn.GELU(),\n            nn.Linear(64, 1)\n        )\n    def forward(self, x):\n        return self.net(x).squeeze(-1)\n\n# --------------------------------------------------------------\n#  DATASET\n# --------------------------------------------------------------\nclass TradingDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ndef get_loader(X, y, shuffle=True):\n    return DataLoader(TradingDataset(X, y), batch_size=BATCH_SIZE,\n                      shuffle=shuffle, num_workers=NUM_WORKERS, pin_memory=True)\n\n# --------------------------------------------------------------\n#  CV WITH FULL LOGGING\n# --------------------------------------------------------------\ndef compute_cv_with_logging(X, y, fr, dates, splits):\n    fold_scores = []\n    all_models = []\n    all_vols = []\n\n    for fold, (tr_idx, val_idx) in enumerate(splits):\n        print(f\"\\n{'='*80}\")\n        print(f\"FOLD {fold+1:02d} | Val: {dates[val_idx.min()]}→{dates[val_idx.max()]} | Train: {len(tr_idx):,} | Val: {len(val_idx):,}\")\n\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n        fr_val = fr[val_idx]\n\n        set_deterministic(42 + fold)\n        model = TradingNN(X.shape[1]).to(DEVICE)\n        optimizer = SharpeAwareAdamW(model.parameters(), lr=3e-3, warmup=5, T_max=60, weight_decay=1e-5)\n        ema = EMA(model, decay=EMA_DECAY)\n        train_loader = get_loader(X_tr, y_tr)\n        val_loader = get_loader(X_val, y_val, shuffle=False)\n\n        print(f\"Training {TRAIN_EPOCHS} epochs | EMA from epoch {EMA_START}\")\n        for epoch in range(TRAIN_EPOCHS):\n            model.train()\n            epoch_loss = 0.0\n            n_batches = 0\n            grad_norm = 0.0\n            for x, yt in train_loader:\n                x, yt = x.to(DEVICE), yt.to(DEVICE)\n                pred = model(x)\n                loss = sharpe_loss(pred, yt)\n                optimizer.zero_grad()\n                loss.backward()\n                grad_norm += torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0).item()\n                optimizer.step(epoch=epoch)\n                epoch_loss -= loss.item()\n                n_batches += 1\n\n            epoch_sharpe = epoch_loss / n_batches\n            optimizer.set_epoch_sharpe(epoch_sharpe)\n            grad_norm /= n_batches\n\n            if epoch >= EMA_START:\n                ema.update(model)\n\n            if epoch < 5 or (epoch + 1) % 10 == 0:\n                lr = optimizer.get_lr(epoch)\n                print(f\"  E{epoch+1:02d} | Sharpe: {epoch_sharpe:.6f} | LR: {lr:.2e} | Grad: {grad_norm:.4f}\")\n\n        ema.apply(model)\n\n        # Val\n        model.eval()\n        with torch.inference_mode():\n            val_pred = np.concatenate([model(x.to(DEVICE)).cpu().numpy() for x, _ in val_loader])\n        pred_std = np.std(val_pred)\n        scale = TARGET_VOL / (pred_std + 1e-6)\n        val_pred_scaled = val_pred * scale\n        position = np.clip(np.tanh(val_pred_scaled) * 2, 0, 2)\n\n        sol = pd.DataFrame({'risk_free_rate': np.zeros_like(fr_val), 'forward_returns': fr_val})\n        sub = pd.DataFrame({'prediction': position})\n        fold_score = score(sol, sub, '')\n\n        fold_scores.append(fold_score)\n        all_models.append({k: v.cpu().clone() for k, v in model.state_dict().items()})\n        all_vols.append(pred_std)\n\n        print(f\"  Val Pred: μ={val_pred.mean():.6f} σ={pred_std:.6f} → scale={scale:.3f}\")\n        print(f\"  Position: μ={position.mean():.6f} σ={position.std():.6f}\")\n        print(f\"  FOLD SCORE: {fold_score:.6f}\")\n\n    print(f\"\\nCV SCORES: {[f'{s:.6f}' for s in fold_scores]}\")\n    print(f\"CV MEAN: {np.mean(fold_scores):.6f} | STD: {np.std(fold_scores):.6f}\")\n    return fold_scores, all_models, all_vols\n\n# --------------------------------------------------------------\n#  MAIN\n# --------------------------------------------------------------\ntrain_raw = load_full_train()\ntest_raw  = pl.read_csv(DATA_PATH / \"test.csv\")\nFEATURES  = get_features(train_raw, test_raw)\nprint(f\"Common features: {len(FEATURES)}\")\n\ndf_all = create_example_dataset(train_raw)\n\nheld_out_df = None\nif LB_RUN:\n    train_df = df_all.filter(pl.col('date_id') < 8810)\n    held_out_df = df_all.filter((pl.col('date_id') >= 8810) & (pl.col('date_id') <= 8989))\nelse:\n    train_df = df_all\n\nX, y, fr_arr = prepare_training_data(train_df, FEATURES)\ndate_arr = train_df['date_id'].to_numpy()\nsplits = walk_forward_split(len(X))\n\n# CV\nprint(\"\\n=== SOTA CV ===\")\ncv_scores, fold_models, fold_vols = compute_cv_with_logging(X, y, fr_arr, date_arr, splits)\n\n# Final Model\nprint(\"\\nAveraging all 10 EMA models...\")\nfinal_state = {}\nfirst = fold_models[0]\nfor key in first.keys():\n    if torch.is_floating_point(first[key]):\n        stacked = torch.stack([m[key] for m in fold_models])\n        final_state[key] = stacked.mean(0)\n\nnp.savez('final_model.npz', **{k: v.cpu().numpy() for k, v in final_state.items()})\n\n# Local LB\nif held_out_df is not None:\n    print(\"\\n=== LOCAL LB ===\")\n    X_ho = held_out_df.select(FEATURES).to_numpy().astype(np.float32)\n    fr_ho = held_out_df.get_column('forward_returns').to_numpy().astype(np.float32)\n    ho_loader = get_loader(X_ho, np.zeros(len(X_ho), dtype=np.float32), shuffle=False)\n\n    model = TradingNN(X.shape[1]).to(DEVICE)\n    model.load_state_dict(final_state)\n    model.eval()\n\n    with torch.inference_mode():\n        pred_ho = np.concatenate([model(x.to(DEVICE)).cpu().numpy() for x, _ in ho_loader])\n    pred_std = np.std(pred_ho)\n    scale = TARGET_VOL / (pred_std + 1e-6)\n    pred_ho_scaled = pred_ho * scale\n    position = np.clip(np.tanh(pred_ho_scaled) * 2, 0, 2)\n\n    sol = pd.DataFrame({'risk_free_rate': np.zeros_like(fr_ho), 'forward_returns': fr_ho})\n    sub = pd.DataFrame({'prediction': position})\n    lb = score(sol, sub, '')\n    print(f\"LOCAL LB PRED STD: {pred_std:.6f} → scale={scale:.3f}\")\n    print(f\"LOCAL LB SCORE = {lb:.6f}\")\n\n# --------------------------------------------------------------\n#  INFERENCE – ONLINE VOL\n# --------------------------------------------------------------\nfinal_state_np = np.load('final_model.npz')\n\nclass InferenceState:\n    def __init__(self):\n        self.buffer = []\n    def update_vol(self, pred):\n        self.buffer.append(pred)\n        if len(self.buffer) > 50: self.buffer.pop(0)\n        return np.std(self.buffer[-20:]) if len(self.buffer) >= 20 else 1.0\n\nstate = InferenceState()\n\ndef predict(test: pl.DataFrame) -> float:\n    df = create_example_dataset(test)\n    X_test = df.select(FEATURES).to_numpy().astype(np.float32)\n    x_t = torch.from_numpy(X_test[:1]).to(DEVICE)\n\n    model = TradingNN(X_test.shape[1]).to(DEVICE)\n    state_dict = {k: torch.from_numpy(v).to(DEVICE) for k, v in final_state_np.items()}\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    with torch.inference_mode():\n        p = model(x_t).cpu().numpy()[0]\n\n    vol = state.update_vol(p)\n    p_scaled = p * (TARGET_VOL / (vol + 1e-6))\n    return float(np.clip(np.tanh(p_scaled) * 2, 0, 2))\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T13:34:36.846129Z","iopub.execute_input":"2025-11-05T13:34:36.846741Z","iopub.status.idle":"2025-11-05T13:36:13.343506Z","shell.execute_reply.started":"2025-11-05T13:34:36.846706Z","shell.execute_reply":"2025-11-05T13:36:13.342535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}