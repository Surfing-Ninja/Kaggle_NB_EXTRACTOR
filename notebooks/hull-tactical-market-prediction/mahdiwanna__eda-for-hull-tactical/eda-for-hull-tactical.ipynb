{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  The Hull Tactical competition exploratory data analysis (EDA)\n\n\nThis notebook performs:\n- A detailed **exploratory data analysis (EDA)** of Hull Tactical‚Äôs feature families\n- Assessment of **data quality, correlations, and statistical properties**\n- Baseline **linear and simple rule-based models** for allocation sizing\n- Preliminary **Sharpe and volatility evaluation**\n\nMy goal: derive insights guiding future model iterations while staying aligned with the fund‚Äôs risk-managed approach.\n","metadata":{}},{"cell_type":"markdown","source":"First our target variable is:\n\n>  market_forward_excess_returns\n = (S&P 500 forward return) ‚àí (rolling 5-year mean forward return), winsorized by a MAD criterion.\n\nIt represents the risk-adjusted, forward-looking excess return of the market ‚Äî i.e., how much above/below ‚Äúnormal‚Äù the next-day market return is expected to be.\n\nIn plain terms:\n‚Üí Positive = market expected to outperform normal levels.\n‚Üí Negative = market expected to underperform.\n\nOur model‚Äôs goal is to predict this value, then convert that into an allocation (0‚Äì2) for exposure.","metadata":{}},{"cell_type":"markdown","source":"## Dataset Structure & Overview","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom io import StringIO\nfrom scipy import stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.impute import KNNImputer\nfrom statsmodels.tsa.stattools import adfuller\n\n# Read train/test from input (replace below with full CSVs in competition)\ntrain_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv')\n\ntrain_df.set_index('date_id', inplace=True)\ntest_df.set_index('date_id', inplace=True)\n\n# Identify features\nall_cols = train_df.columns\nfeature_cols = [col for col in all_cols if col[0] in ['D','E','I','M','P','S','V']]\ntarget = 'market_forward_excess_returns'\nreturns_col = 'forward_returns'\n\ntrain_df.shape, test_df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T02:51:50.000758Z","iopub.execute_input":"2025-10-07T02:51:50.001082Z","iopub.status.idle":"2025-10-07T02:51:50.22119Z","shell.execute_reply.started":"2025-10-07T02:51:50.00106Z","shell.execute_reply":"2025-10-07T02:51:50.220016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Hull Tactical competition provides a robust financial time series dataset with **8,990 training samples and 98 features across 7 distinct categories** :\n\n* D Features (D1-D9): 9 technical indicators\n\n* E Features (E1-E20): 20 economic indicators\n\n* V Features (V1-V15): 15 volatility/variance signals\n\n* S Features (S1-S15): 15 sentiment signals\n\n* M Features (M1-M20): 20 market/momentum indicators\n\n* T Features (T1-T10): 10 trend/timing signals\n\n* P Features (P1-P9): 9 proprietary Hull Tactical signals\n\nLet‚Äôs inspect their structure and missingness next.\n\n\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.heatmap(train_df[feature_cols].isna(), cbar=False, cmap='viridis')\nplt.title('Missing Values Heatmap ‚Äì Training Data')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T02:56:18.864124Z","iopub.execute_input":"2025-10-07T02:56:18.864616Z","iopub.status.idle":"2025-10-07T02:56:20.379896Z","shell.execute_reply.started":"2025-10-07T02:56:18.864585Z","shell.execute_reply":"2025-10-07T02:56:20.378622Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Early periods in the dataset contain significant missingness, especially in **macro** and **sentiment** features.  \nLater data (recent decades) is considerably more complete ‚Äî suggesting that **truncating or imputing selectively** may be more robust.\n","metadata":{}},{"cell_type":"code","source":"train_df[returns_col].describe(), train_df[target].describe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T03:32:22.89105Z","iopub.execute_input":"2025-10-07T03:32:22.896315Z","iopub.status.idle":"2025-10-07T03:32:22.963683Z","shell.execute_reply.started":"2025-10-07T03:32:22.896206Z","shell.execute_reply":"2025-10-07T03:32:22.961967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Target Variable Exploration","metadata":{}},{"cell_type":"code","source":"# Clean infinite or NaN values before plotting\ntrain_df[target] = train_df[target].replace([np.inf, -np.inf], np.nan)\ntrain_df = train_df.dropna(subset=[target])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T03:36:55.829635Z","iopub.execute_input":"2025-10-07T03:36:55.830078Z","iopub.status.idle":"2025-10-07T03:36:55.9221Z","shell.execute_reply.started":"2025-10-07T03:36:55.830051Z","shell.execute_reply":"2025-10-07T03:36:55.920606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"use_inf_as_na option is deprecated\")\n\nplt.figure(figsize=(10,5))\nsns.histplot(train_df[target], kde=True)\nplt.title('Distribution of Target (Excess Returns)')\nplt.show()\n\nplt.figure(figsize=(15,5))\nplt.plot(train_df[target].values)\nplt.title('Target Over Time')\nplt.xlabel('Date ID')\nplt.ylabel('Market Forward Excess Return')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T03:37:28.80477Z","iopub.execute_input":"2025-10-07T03:37:28.805111Z","iopub.status.idle":"2025-10-07T03:37:29.566506Z","shell.execute_reply.started":"2025-10-07T03:37:28.805088Z","shell.execute_reply":"2025-10-07T03:37:29.565462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution of Target (Histogram + KDE) \n\n* The distribution of S&P 500 forward excess returns is roughly bell-shaped but exhibits pronounced fat tails (leptokurtic), indicating that extreme positive and negative returns occur much more frequently than a normal distribution would predict. \n* While the central peak suggests most returns cluster near zero, the heavy tails reflect heightened risk of outsized market moves. There is a slight rightward skew, meaning large positive excess returns may be somewhat more probable or larger than negative ones, although the overall asymmetry is modest. \n* This shape is consistent with well-documented characteristics of financial returns: non-normality, excess kurtosis, and the presence of tail risk.\n\n## Target Over Time (Time Series Plot)\n* The time series of excess returns displays clear episodes of high and low volatility, demonstrating classic volatility clustering‚Äîwhere tranquil periods are punctuated by bursts of intense fluctuation. \n* This heteroskedastic behavior is widely observed in financial markets and motivates the use of models like GARCH or various volatility-adjusted forecasting techniques that dynamically respond to changing risk conditions. Notably, there‚Äôs no sustained long-term drift in the series; excess returns oscillate around zero, suggesting the process is stationary and in line with the efficient market hypothesis, where no persistent predictability or bias in returns is evident.\n\nThese features, fat tails, volatility clustering, and mean-reversion‚Äîshould be carefully considered when selecting and validating predictive models for financial time series. They imply that traditional models assuming normality or constant volatility may underestimate both the likelihood of extreme outcomes and the need for robust risk management.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Spearman Correlations","metadata":{}},{"cell_type":"code","source":"import polars as pl\n\nnull_features = [c for c in feature_cols if train_df[c].isna().all()]\nconst_features = [c for c in feature_cols if train_df[c].nunique() <= 1]\n\nprint(f\"All-NaN features: {len(null_features)}\")\nprint(f\"Constant features: {len(const_features)}\")\n\n\npl_train = pl.from_pandas(train_df)\n\ntarget_corrs = []\n\nfor col in feature_cols:\n    if col not in pl_train.columns:\n        print(f\"‚ö†Ô∏è Skipping {col}: not in dataset\")\n        continue\n    try:\n        corr_value = pl_train.select(pl.corr(col, target, method=\"spearman\")).item()\n        target_corrs.append((col, corr_value))\n    except Exception as e:\n        print(f\"‚ö†Ô∏è {col} failed: {e}\")\n        target_corrs.append((col, np.nan))\n\ntarget_corr_df = (\n    pd.DataFrame(target_corrs, columns=[\"feature\", \"corr_with_target\"])\n    .dropna()\n    .sort_values(\"corr_with_target\", ascending=False)\n)\n\nprint(f\"\\n‚úÖ Computed correlations for {len(target_corr_df)} features.\\n\")\nprint(\"üîù Top 10 correlations:\\n\", target_corr_df.head(10))\nprint(\"\\nüîª Bottom 10 correlations:\\n\", target_corr_df.tail(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T15:08:30.249317Z","iopub.execute_input":"2025-10-07T15:08:30.249636Z","iopub.status.idle":"2025-10-07T15:08:30.393643Z","shell.execute_reply.started":"2025-10-07T15:08:30.249617Z","shell.execute_reply":"2025-10-07T15:08:30.392577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,6))\nplt.barh(target_corr_df['feature'].tail(10), target_corr_df['corr_with_target'].tail(10), color='red')\nplt.barh(target_corr_df['feature'].head(10), target_corr_df['corr_with_target'].head(10), color='green')\nplt.title('Top & Bottom Feature Correlations with Target')\nplt.xlabel('Correlation')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T15:08:48.958272Z","iopub.execute_input":"2025-10-07T15:08:48.958516Z","iopub.status.idle":"2025-10-07T15:08:49.150136Z","shell.execute_reply.started":"2025-10-07T15:08:48.958502Z","shell.execute_reply":"2025-10-07T15:08:49.149359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get top correlated features (from your earlier Spearman computation)\ntop_features = target_corr_df.head(10)['feature'].tolist()\n\n# Compute and visualize Spearman correlations among them\ncorr_matrix = train_df[top_features + [target]].corr(method=\"spearman\")\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=True, fmt=\".2f\")\nplt.title(\"Spearman Correlation Heatmap (Top 10 Features vs Target)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T15:17:37.737704Z","iopub.execute_input":"2025-10-07T15:17:37.737944Z","iopub.status.idle":"2025-10-07T15:17:38.209699Z","shell.execute_reply.started":"2025-10-07T15:17:37.737931Z","shell.execute_reply":"2025-10-07T15:17:38.208986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the Spearman correlations computed between 94 features and the target variable (market_forward_excess_returns), the following key insights emerge:\n\n**1. Magnitude and Direction**\n\nAll correlations are very weak in absolute value. The strongest positive correlation is approximately 0.05 (M1), and the strongest negative is around -0.05 (M4).\n\nValues near zero indicate almost no monotonic relationship with the target variable.\n\nBoth positive and negative correlations are small, suggesting that no single feature independently predicts returns.\n\n**2. Implications of Weak Correlations**\n\nNo dominant predictors: Excess returns likely depend on many subtle signals rather than strong individual effects.\n\nFeature interactions matter: Since monotonic associations are weak, modeling nonlinear relationships or feature combinations will be key.\n\nConsistent with EMH: The weak correlations align with the Efficient Market Hypothesis, implying limited predictable structure in isolated features.","metadata":{}},{"cell_type":"markdown","source":"##   Collinearity Analysis","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n\n# Select numeric features with more than 2 unique values\nnum_features = [col for col in feature_cols if train_df[col].nunique() > 2]\n\n# Impute missing values with mean (or median)\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(train_df[num_features])\n\n# Calculate VIF\nvif_data = pd.DataFrame()\nvif_data['feature'] = num_features\nvif_data['VIF'] = [variance_inflation_factor(X_imputed, i) for i in range(len(num_features))]\n\n# Show top 10 features with highest VIF\nprint(vif_data.sort_values('VIF', ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T15:33:44.498196Z","iopub.execute_input":"2025-10-07T15:33:44.498438Z","iopub.status.idle":"2025-10-07T15:33:48.206016Z","shell.execute_reply.started":"2025-10-07T15:33:44.498424Z","shell.execute_reply":"2025-10-07T15:33:48.205466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We performed a Variance Inflation Factor (**VIF**) analysis on 94 numeric features to assess multicollinearity within the dataset. The VIF quantifies how much the variance of a regression coefficient is inflated due to linear dependencies with other features. Values exceeding 10 indicate severe multicollinearity.\n\nOur results revealed several features with **extremely high VIF values**, with the top 10 reaching from **~100 up to over 1700**, signaling **near-perfect linear dependencies** among certain variables. This suggests that some features are redundant or represent linear combinations of others, potentially causing instability in linear models.\n\nHigh multicollinearity can **distort model interpretations, inflate standard errors, and degrade prediction robustness**, particularly in parametric models. To address this, we recommend dimensionality reduction techniques such as Principal Component Analysis (PCA), careful feature selection to remove redundant variables, and leveraging regularized or tree-based models that are more resilient to such correlations.\n","metadata":{}},{"cell_type":"markdown","source":"## Augmented Dickey-Fuller Test","metadata":{}},{"cell_type":"code","source":"adf_result = adfuller(train_df[returns_col].dropna())\nprint(f\"ADF p-value: {adf_result[1]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T16:52:25.267273Z","iopub.execute_input":"2025-10-07T16:52:25.267674Z","iopub.status.idle":"2025-10-07T16:52:25.850934Z","shell.execute_reply.started":"2025-10-07T16:52:25.267643Z","shell.execute_reply":"2025-10-07T16:52:25.849115Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Augmented Dickey-Fuller (ADF) test checks the null hypothesis:\n\n$$\nH_0: \\text{The series has a unit root (non-stationary)}\n$$\n\nagainst the alternative hypothesis:\n\n$$\nH_1: \\text{The series is stationary (no unit root)}\n$$\n\nIn our analysis, the ADF test returned a **p-value of 0.0000**, providing strong evidence *against* the null hypothesis. This indicates that the series is likely **stationary**.  \n\nStationarity implies that the mean, variance, and autocorrelation structure of the series remain roughly constant over time.\n","metadata":{}},{"cell_type":"code","source":"window = 50  # rolling window size\n\nplt.figure(figsize=(14,6))\nplt.plot(train_df[target], color='blue', label='Excess Returns')\nplt.plot(train_df[target].rolling(window).mean(), color='red', linestyle='--', label='Rolling Mean')\nplt.plot(train_df[target].rolling(window).std(), color='green', linestyle='--', label='Rolling Std')\nplt.legend()\nplt.title('Excess Returns: Rolling Mean and Standard Deviation')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T16:34:28.166295Z","iopub.execute_input":"2025-10-07T16:34:28.166586Z","iopub.status.idle":"2025-10-07T16:34:28.675384Z","shell.execute_reply.started":"2025-10-07T16:34:28.166566Z","shell.execute_reply":"2025-10-07T16:34:28.674294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visual inspection further supports this conclusion:\n\nThe rolling mean and standard deviation remain roughly constant, indicating that the series does not exhibit systematic trends or changing volatility.","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(train_df[target].dropna(), lags=50)\nplt.title('ACF of Excess Returns')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T16:39:55.961577Z","iopub.execute_input":"2025-10-07T16:39:55.962306Z","iopub.status.idle":"2025-10-07T16:39:56.215262Z","shell.execute_reply.started":"2025-10-07T16:39:55.962278Z","shell.execute_reply":"2025-10-07T16:39:56.214069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The autocorrelation function (ACF) decays rapidly, suggesting the absence of long-term dependencies typical of non-stationary processes.\nOverall, despite the stochastic and noisy nature of financial returns, our analysis confirms that excess forward returns are weakly stationary, meaning their statistical properties (mean, variance) remain stable over time.","metadata":{}},{"cell_type":"markdown","source":"## Why this is useful ?\n\nKnowing that your target is stationary is very important for modeling:\n\n* Model choice: Stationarity of returns enables many statistical/machine learning models to be valid. Many time series models assume stationarity.\n  \n* Feature engineering: You can create lag features or rolling statistics (mean, std) directly on the series without worrying about introducing spurious correlations.\n\n* Forecasting reliability: Stable mean and variance improve the reliability of statistical models, non-stationary data can produce misleading predictions, so confirming stationarity reduces that risk.\n\n* Interpretation of results: Coefficients or relationships learned by your model are more stable over time because the underlying series does not drift.","metadata":{}},{"cell_type":"markdown","source":"## Outlier & Distribution Tests (QQ Plot, Skewness, and Kurtosis)","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\nstats.probplot(train_df[target], dist=\"norm\", plot=ax)\nplt.title(\"QQ Plot for Target\")\nplt.show()\n\nprint(\"Skew:\", stats.skew(train_df[target]))\nprint(\"Kurtosis:\", stats.kurtosis(train_df[target]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T17:44:42.956707Z","iopub.execute_input":"2025-10-07T17:44:42.959074Z","iopub.status.idle":"2025-10-07T17:44:43.705659Z","shell.execute_reply.started":"2025-10-07T17:44:42.958984Z","shell.execute_reply":"2025-10-07T17:44:43.70393Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The QQ plot visually compares the empirically observed distribution of excess returns against a theoretical normal distribution. Data points hugging the diagonal line indicate normality, while systematic deviations reveal departures such as skewness or heavy tails. In our case, the plot showed moderate deviations from normality, especially in the tails, signaling that extreme positive or negative returns occur more frequently than a Gaussian would predict.\n\nThe skewness value of approximately -0.18 indicates a slight asymmetry with a longer left tail, implying that negative returns, though infrequent, can be more extreme than positive returns. This aligns with the commonly observed phenomenon of downside risk in financial markets.\n\nThe kurtosis of about 2.24, marginally below the normal value of 3, suggests the distribution is relatively platykurtic ‚Äî somewhat flatter and thinner-tailed than normal. This unique characteristic might reflect the specific dataset or time period analyzed, but still highlights that the simple normality assumption does not hold perfectly.\n\n**Why is this important?**\n\n* **Risk Assessment**: Understanding the presence of skewness and kurtosis helps quantify the likelihood of extreme losses or gains (tail risk), which is critical for setting stop-loss limits, stress testing, and capital allocation.\n\n* **Modeling Strategy**: Non-normality means classical models that assume Gaussian errors (e.g., OLS regression) may yield misleading inference and predictions. Robust or distribution-aware methods such as quantile regression, heavy-tailed distributions, or non-parametric approaches are recommended.\n\n* **Portfolio Optimization**: Incorporating skewness and kurtosis metrics can improve portfolio construction rules by accounting for asymmetric risk preferences and tail dependence.\n\n* **Performance Evaluation**: Return distributions influence the reliability of standard metrics like Sharpe ratio; alternative measures that consider higher moments (e.g., Sortino ratio) become more meaningful.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Tail Risk Measures: Value at Risk (VaR) and Conditional Value at Risk (CVaR)","metadata":{}},{"cell_type":"code","source":"sorted_returns = np.sort(train_df[returns_col])\nvar_95 = np.percentile(sorted_returns, 5)\ncvar_95 = sorted_returns[sorted_returns <= var_95].mean()\nvar_95, cvar_95\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T18:19:17.653926Z","iopub.execute_input":"2025-10-07T18:19:17.654463Z","iopub.status.idle":"2025-10-07T18:19:17.672079Z","shell.execute_reply.started":"2025-10-07T18:19:17.654439Z","shell.execute_reply":"2025-10-07T18:19:17.670607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To quantify the risk of extreme negative returns, we computed two key tail risk metrics at the 95% confidence level:\n\n* Value at Risk (VaR 95%): -1.77%\nThis means that, based on historical data, 95% of daily returns are better than -1.77%. Conversely, there is a 5% chance on any given day that returns will be worse than -1.77%. VaR provides a threshold for the worst expected loss under normal market conditions.\n\n* Conditional Value at Risk (CVaR 95%): -2.54%\nCVaR, also known as Expected Shortfall, measures the average loss on the worst 5% of days. In this dataset, if a loss exceeds the VaR threshold, the average loss is -2.54%. CVaR is a more comprehensive measure of tail risk, as it accounts for the magnitude of extreme losses beyond the VaR cutoff.\n\n\n## Why These Measures Matter\n* Risk Management: VaR and CVaR are widely used in finance to set risk limits, allocate capital, and design hedging strategies. They help quantify the potential for rare but severe losses that can threaten portfolio stability.\n\n* Modeling Implications: The presence of significant tail risk (as shown by the gap between VaR and CVaR) highlights the limitations of models that assume normality or focus only on volatility. Models and strategies should be robust to rare, extreme events.\n\n* Portfolio Construction: Understanding tail risk is crucial for stress testing, scenario analysis, and for investors with capital preservation mandates. It informs decisions on leverage, stop-losses, and diversification.\n\n## Practical Use\n* Set risk limits: Use VaR and CVaR to define maximum acceptable daily losses.\n  \n* Stress test strategies: Simulate how portfolios would perform under repeated tail events.\n\n  \n* Communicate risk: These measures provide clear, quantitative risk metrics for stakeholders and regulators.","metadata":{}},{"cell_type":"markdown","source":"## Baseline Linear Models","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Impute missing values with the mean of each column\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(train_df[feature_cols])\n\n# Proceed as before\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, train_df[target], test_size=0.2, random_state=42)\n\nlr_multi = LinearRegression().fit(X_train, y_train)\ny_pred = lr_multi.predict(X_test)\nprint(\"R¬≤:\", r2_score(y_test, y_pred))\nprint(\"MSE:\", mean_squared_error(y_test, y_pred))\nprint(\"MAE:\", mean_absolute_error(y_test, y_pred))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T18:39:46.547866Z","iopub.execute_input":"2025-10-07T18:39:46.54821Z","iopub.status.idle":"2025-10-07T18:39:46.690986Z","shell.execute_reply.started":"2025-10-07T18:39:46.548188Z","shell.execute_reply":"2025-10-07T18:39:46.689745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The baseline linear regression model yielded a negative R¬≤ (-0.013), indicating that the model's predictions are less accurate than simply using the mean of the target variable. This result is not unexpected in financial return prediction, where individual features typically have very weak linear relationships with the target and the data is dominated by noise. These findings highlight the need for more sophisticated modeling approaches and advanced feature engineering to extract any predictive signal from the data.","metadata":{}},{"cell_type":"markdown","source":"## Random Allocation Model","metadata":{}},{"cell_type":"code","source":"n_sim = 1000\nsharpe_list = []\nvol_list = []\n\nfor i in range(n_sim):\n    rand_alloc = np.random.uniform(0, 2, len(train_df))\n    strategy = rand_alloc * train_df[returns_col] - train_df['risk_free_rate']\n    sharpe = strategy.mean() / strategy.std() * np.sqrt(252)\n    vol = strategy.std() * np.sqrt(252)\n    sharpe_list.append(sharpe)\n    vol_list.append(vol)\n\nprint(\"Random Sharpe (mean):\", np.mean(sharpe_list))\nprint(\"Random Sharpe (std):\", np.std(sharpe_list))\nprint(\"Random Sharpe (95% CI):\", np.percentile(sharpe_list, [2.5, 97.5]))\nprint(\"Annualized Vol (mean):\", np.mean(vol_list))\nplt.figure(figsize=(8, 5))\nplt.hist(sharpe_list, bins=30, color='skyblue', edgecolor='black')\nplt.xlabel('Sharpe Ratio')\nplt.ylabel('Count')\nplt.title('Distribution of Sharpe Ratios Across 1000 Simulations')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T21:33:38.895813Z","iopub.execute_input":"2025-10-07T21:33:38.896114Z","iopub.status.idle":"2025-10-07T21:33:39.605739Z","shell.execute_reply.started":"2025-10-07T21:33:38.896092Z","shell.execute_reply":"2025-10-07T21:33:39.60457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As a baseline, we simulated a random allocation strategy, drawing daily weights uniformly between 0 and 2. Over 1000 Monte Carlo simulations, this approach yielded a **mean Sharpe ratio of 0.47** (standard deviation 0.09, 95% confidence interval [0.30, 0.64]) and **an annualized volatility of 19.3%**. These results highlight the importance of benchmarking any predictive or rule-based strategy against random allocation. In financial markets, it is not uncommon for random or naive strategies to achieve positive Sharpe ratios due to the inherent noise and volatility in returns. Therefore, any model or strategy must demonstrate consistent and significant outperformance over this random baseline to be considered robust and practically useful.","metadata":{}},{"cell_type":"markdown","source":"## SMA Crossover Model Example ","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nn_sim = 1000\nsharpe_list = []\nvol_list = []\n\nreturns = train_df[returns_col].values\nrisk_free = train_df['risk_free_rate'].values\n\nfor i in range(n_sim):\n    # Bootstrap resample the returns (with replacement)\n    idx = np.random.choice(len(returns), size=len(returns), replace=True)\n    returns_sample = returns[idx]\n    risk_free_sample = risk_free[idx]\n    \n    # Compute rolling mean on the resampled returns\n    rolling_20 = pd.Series(returns_sample).rolling(20).mean()\n    alloc_sma = np.where(rolling_20 > 0, 1.5, 0.5)\n    basic_ret = alloc_sma * returns_sample - risk_free_sample\n    \n    # Calculate Sharpe and volatility\n    sharpe = basic_ret.mean() / basic_ret.std() * np.sqrt(252)\n    vol = basic_ret.std() * np.sqrt(252)\n    sharpe_list.append(sharpe)\n    vol_list.append(vol)\n\nprint(\"SMA Sharpe (mean):\", np.mean(sharpe_list))\nprint(\"SMA Sharpe (std):\", np.std(sharpe_list))\nprint(\"SMA Sharpe (95% CI):\", np.percentile(sharpe_list, [2.5, 97.5]))\nprint(\"Annualized Vol (mean):\", np.mean(vol_list))\nplt.figure(figsize=(8, 5))\nplt.hist(sharpe_list, bins=30, color='skyblue', edgecolor='black')\nplt.xlabel('Sharpe Ratio')\nplt.ylabel('Count')\nplt.title('Distribution of Sharpe Ratios Across 1000 Simulations')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T21:33:24.540492Z","iopub.execute_input":"2025-10-07T21:33:24.540841Z","iopub.status.idle":"2025-10-07T21:33:25.811693Z","shell.execute_reply.started":"2025-10-07T21:33:24.54081Z","shell.execute_reply":"2025-10-07T21:33:25.810925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We implemented a simple moving average (SMA) crossover strategy, allocating 1.5x leverage when the 20-day rolling mean of returns was positive and 0.5x when negative. To robustly assess its performance, we ran 1000 Monte Carlo simulations using bootstrapped samples of the returns. This rule-based approach achieved a **mean Sharpe ratio of 1.70** (standard deviation 0.14, 95% confidence interval [1.41, 1.98]) and an **annualized volatility of 19.6%**. These results substantially outperform both the random allocation and linear regression baselines. The findings demonstrate that even simple trend-following rules can extract meaningful signal from financial time series, providing a strong benchmark for more sophisticated models. This also highlights the importance of including robust, interpretable baselines in any systematic trading research.","metadata":{}},{"cell_type":"markdown","source":"## Regression-Based Allocation Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import Ridge\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# --- Simulation parameters ---\nn_sim = 1000\nsharpe_list_reg = []\nvol_list_reg = []\n\n# --- Ensure all arrays are NumPy arrays ---\nreturns = np.array(train_df[returns_col])\nrisk_free = np.array(train_df['risk_free_rate'])\n\nfeatures = top_features\nX = np.array(train_df[features])\ny = np.array(target)  # ensure target is a NumPy array\n\n# Define the pipeline (imputation + model)\nmodel = make_pipeline(SimpleImputer(strategy='mean'), Ridge(alpha=1.0))\n\n# --- Monte Carlo Simulation ---\nfor i in range(n_sim):\n    # Bootstrap resampling\n    idx = np.random.choice(len(X), size=len(X), replace=True)\n    \n    X_sample = X[idx]\n    y_sample = y[idx]\n    returns_sample = returns[idx]\n    risk_free_sample = risk_free[idx]\n\n    # Fit model and predict\n    model.fit(X_sample, y_sample)\n    preds = model.predict(X_sample)\n\n    # Allocation logic\n    alloc_reg = np.where(preds > 0, 1.5, 0.5)\n    strat_ret = alloc_reg * returns_sample - risk_free_sample\n\n    # Compute Sharpe and volatility\n    sharpe = strat_ret.mean() / strat_ret.std() * np.sqrt(252)\n    vol = strat_ret.std() * np.sqrt(252)\n    sharpe_list_reg.append(sharpe)\n    vol_list_reg.append(vol)\n\n# --- Summary statistics ---\nprint(\"Regression Sharpe (mean):\", np.mean(sharpe_list_reg))\nprint(\"Regression Sharpe (std):\", np.std(sharpe_list_reg))\nprint(\"Regression Sharpe (95% CI):\", np.percentile(sharpe_list_reg, [2.5, 97.5]))\nprint(\"Regression Annualized Vol (mean):\", np.mean(vol_list_reg))\n\n# --- Visualization ---\nplt.figure(figsize=(8, 5))\nplt.hist(sharpe_list_reg, bins=30, color='skyblue', edgecolor='black')\nplt.xlabel('Sharpe Ratio')\nplt.ylabel('Count')\nplt.title('Distribution of Sharpe Ratios (Regression Strategy)')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T22:08:31.660926Z","iopub.execute_input":"2025-10-07T22:08:31.661274Z","iopub.status.idle":"2025-10-07T22:08:41.728025Z","shell.execute_reply.started":"2025-10-07T22:08:31.661248Z","shell.execute_reply":"2025-10-07T22:08:41.727148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We implemented a regression-based allocation strategy, using the model‚Äôs predicted returns to dynamically adjust portfolio exposure. To robustly assess its performance, we ran 1000 Monte Carlo simulations with bootstrapped samples. This approach achieved a **mean Sharpe ratio of 0.76** (standard deviation 0.17, 95% confidence interval [0.42, 1.07]) and an **annualized volatility of 19.2%**. While this outperforms the random allocation baseline, it falls short of the SMA crossover model‚Äôs performance. These results demonstrate that even a simple linear model can extract useful predictive signals from the feature set, but also highlight the need for more advanced modeling and risk management techniques to achieve stronger, more consistent results in financial time series.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## EDA Summary\n* The dataset exhibits strong time-dependence, missing value patterns, and evidence of structural regime shifts.\n\n* Momentum, valuation, and volatility feature groups show mild predictive value for excess returns.\n\n* Missing data and multicollinearity are present and require careful preprocessing and feature selection.\n\n* The target (excess returns) is nearly symmetric, slightly platykurtic, and stationary, with moderate tail risk.\n\n* Linear models provide interpretable but modest baselines, with weak individual feature correlations.\n\n* Random allocation yields a Sharpe ratio of ~0.4, setting a realistic benchmark for naive strategies.\n\n* Simple rule-based strategies (e.g., SMA crossover) deliver strong risk-adjusted returns (Sharpe > 1), outperforming random and linear baselines.\n\nThese results highlight the importance of robust preprocessing, benchmarking, and risk-aware modeling in financial prediction.\n\n## Next Steps\n* Introduce feature lagging and scaling to capture temporal dependencies and normalize input distributions.\n\n* Implement regularized regressors (Ridge, Lasso) to address multicollinearity and improve generalization.\n\n* Develop volatility-adjusted Sharpe objectives to align model optimization with competition metrics.\n\n* Transition to a LightGBM or ensemble meta-model for final competition submission, leveraging advanced feature engineering and robust validation.\n","metadata":{}}]}