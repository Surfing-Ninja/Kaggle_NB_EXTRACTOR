{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T22:58:37.328664Z","iopub.execute_input":"2025-11-03T22:58:37.329122Z","iopub.status.idle":"2025-11-03T22:58:37.345338Z","shell.execute_reply.started":"2025-11-03T22:58:37.329098Z","shell.execute_reply":"2025-11-03T22:58:37.344122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport polars as pl\nimport numpy as np\nimport logging\nfrom typing import List\n\n# New Imports: Scikit-Learn RandomForestRegressor for stability\nimport pandas as pd \nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\n\n# --- Kaggle Evaluation API Interface (Mandatory) ---\ntry:\n    import kaggle_evaluation.default_inference_server as inference_server\nexcept ImportError:\n    class MockInferenceServer:\n        def __init__(self, predict_fn):\n            self.predict_fn = predict_fn\n        def serve(self):\n            logging.info(\"Mock Inference Server Running...\")\n        def run_local_gateway(self, path):\n            logging.info(f\"Mock Local Gateway Running with path: {path}\")\n    inference_server = MockInferenceServer\n\n# --- Global Model Variables ---\nMODEL = None\nTRAIN_COLS = None\nTRAIN_MEANS = None \nFEATURE_HISTORY = [] \nHISTORY_MAX_LENGTH = 100 # Max number of rows to keep for rolling feature calculation\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\n# --- Feature Engineering and Selection ---\n\ndef preprocess_data(df: pl.DataFrame, is_training: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Simplified preprocessing for maximum stability. Focuses on core momentum and volatility.\n    \"\"\"\n    \n    # --- Polars Feature Engineering (Core Signals) ---\n    \n    # 1. Rolling Mean and Standard Deviation \n    ROLLING_WINDOWS = [10, 20] # Only use medium and long windows\n    BASE_FEATURES = ['M1', 'E1', 'V1', 'S1', 'T1', 'P1', 'D1']\n\n    expressions = []\n    for window in ROLLING_WINDOWS:\n        for col in BASE_FEATURES:\n            if col in df.columns:\n                # Rolling Mean (Momentum) \n                expressions.append(\n                    pl.col(col).rolling_mean(window_size=window, min_samples=1).alias(f'{col}_roll_mean_{window}')\n                )\n                # Rolling Standard Deviation (Volatility)\n                expressions.append(\n                    pl.col(col).rolling_std(window_size=window, min_samples=1).alias(f'{col}_roll_std_{window}')\n                )\n    \n    if expressions:\n        df = df.with_columns(expressions)\n        \n    # --- Convert to Pandas for EMA ---\n    pdf = df.to_pandas()\n    \n    # 2. Exponential Moving Averages (EMAs)\n    EMA_WINDOWS = [10, 30] \n    for window in EMA_WINDOWS:\n        for col in BASE_FEATURES:\n            if col in pdf.columns:\n                pdf[f'{col}_ema_{window}'] = pdf[col].ewm(span=window, adjust=False).mean()\n    \n    # 3. Time Feature (Only the raw date_id)\n    if 'date_id' in pdf.columns:\n        pdf['date_id'] = pdf['date_id']\n        \n    # Drop non-feature columns\n    EXCLUDE_FINAL_COLS = ['forward_returns', 'risk_free_rate', \n                          'market_forward_excess_returns', 'is_scored', \n                          'lagged_forward_returns', 'lagged_risk_free_rate', \n                          'lagged_market_forward_excess_returns']\n                          \n    final_cols = [col for col in pdf.columns if col not in EXCLUDE_FINAL_COLS]\n    \n    if is_training:\n        return pdf[final_cols]\n    else:\n        # Return the last row only for inference\n        return pdf[final_cols].tail(1)\n\n\n# --- Core Prediction and Allocation Logic ---\n\ndef train_model(train_df: pl.DataFrame):\n    \"\"\"\n    Trains a Random Forest Regressor to predict the raw excess return.\n    \"\"\"\n    global MODEL, TRAIN_COLS, TRAIN_MEANS, FEATURE_HISTORY\n    \n    # 1. TARGET: Predict the raw excess return (Regression)\n    y_train = train_df['market_forward_excess_returns'].to_numpy()\n    \n    # 2. Prepare Features\n    X_train_pd = preprocess_data(train_df, is_training=True)\n    \n    TRAIN_COLS = list(X_train_pd.columns) \n    \n    # 3. Impute Data Manually\n    TRAIN_MEANS = X_train_pd.mean()\n    X_train_pd = X_train_pd.fillna(TRAIN_MEANS)\n    \n    # 4. Define and fit the Random Forest Regressor (Simpler, more robust)\n    regressor = RandomForestRegressor(\n        n_estimators=100,             # Fewer trees for faster and simpler model\n        max_depth=5,                  # Shallow trees to avoid deep overfitting\n        min_samples_leaf=20,          # High value for robust splits\n        random_state=42, \n        n_jobs=4,\n    )\n    \n    MODEL = regressor\n    \n    logging.info(f\"Starting Random Forest training on {len(X_train_pd)} samples with {len(TRAIN_COLS)} features...\")\n    MODEL.fit(X_train_pd, y_train)\n    logging.info(\"Model training complete.\")\n    \n    # Store the tail of the training data as history \n    FEATURE_HISTORY.extend(train_df.tail(HISTORY_MAX_LENGTH).rows(named=False))\n\n\ndef convert_prediction_to_allocation(predicted_return: float) -> float:\n    \"\"\"\n    Converts the model's raw predicted return into the required allocation size [0.0, 2.0].\n    \n    CRITICAL CHANGE: This uses the most conservative allocation multiplier yet,\n    prioritizing stability over aggressive returns.\n    \"\"\"\n    \n    # MAXIMUM RISK CONTROL: The multiplier is extremely low (0.5), meaning we need a very \n    # large predicted return for the bet size to move far from 1.0 (neutral).\n    CONFIDENCE_MULTIPLIER = 0.5 \n\n    # 1. Scale the raw predicted return (which is usually small, like -0.005 to 0.005)\n    # The result is our 'edge'. Note: Tanh is removed for simplicity and direct scaling.\n    scaled_edge = CONFIDENCE_MULTIPLIER * predicted_return * 1000.0 # Scale return magnitude up\n\n    # 2. Clamp the scaled edge to a safe range before adding to neutral\n    scaled_edge = np.clip(scaled_edge, -0.9, 0.9)\n\n    # 3. Map the scaled edge back to the final allocation range [0.0, 2.0]\n    final_allocation = 1.0 + scaled_edge\n    \n    # Clip the result to the required range [0.0, 2.0]\n    final_allocation = np.clip(final_allocation, 0.0, 2.0)\n    \n    return float(final_allocation)\n\n# --- CRITICAL FIX: Model Setup (to avoid Kaggle startup timeout) ---\ndef setup_model():\n    \"\"\"\n    Loads training data and trains the model. This is called once at script start \n    in the competition environment to run the time-consuming step before the server \n    startup timer begins.\n    \"\"\"\n    global MODEL\n    if MODEL is not None:\n        return\n        \n    train_path = os.path.join('/kaggle/input/hull-tactical-market-prediction/', 'train.csv')\n    \n    try:\n        logging.info(\"Attempting to load train.csv for pre-server setup...\")\n        train_df = pl.read_csv(train_path, try_parse_dates=True, infer_schema_length=100000)\n        train_model(train_df)\n    except Exception as e:\n        # If loading fails here, the model remains None, and predict will return neutral.\n        logging.error(f\"FATAL: Model setup failed. Could not load train.csv or train model: {e}\")\n        \n\n# --- The Required Kaggle Inference Function ---\n\ndef predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    The main inference function called by the Kaggle evaluation API for each timestep.\n    (Model training is now handled outside this function in setup_model.)\n    \"\"\"\n    global MODEL, TRAIN_COLS, TRAIN_MEANS, FEATURE_HISTORY\n    \n    is_mock_run = not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n    default_return = (1.0, 0.0) if is_mock_run else 1.0\n    \n    if MODEL is None:\n        # If model is None, it means the setup_model call failed or hasn't run yet.\n        logging.error(\"Model not initialized. Returning neutral allocation.\")\n        return default_return\n            \n    # --- CRITICAL: Update and Use Feature History for Rolling Features ---\n    FEATURE_HISTORY.extend(test.rows(named=False))\n    \n    if len(FEATURE_HISTORY) > HISTORY_MAX_LENGTH:\n        FEATURE_HISTORY = FEATURE_HISTORY[-HISTORY_MAX_LENGTH:]\n        \n    try:\n        current_data_for_features = pl.DataFrame(\n            FEATURE_HISTORY, \n            schema=test.schema \n        )\n    except Exception as e:\n        logging.error(f\"Failed to reconstruct Polars DataFrame from history: {e}\")\n        return default_return\n    \n    # Ensure we only use the features the model was trained on\n    try:\n        X_test_pd = preprocess_data(current_data_for_features, is_training=False)\n        \n        # Check and handle missing features from simplified set\n        missing_cols = set(TRAIN_COLS) - set(X_test_pd.columns)\n        for col in missing_cols:\n            X_test_pd[col] = np.nan\n            \n        X_test_pd = X_test_pd[TRAIN_COLS] # Reorder and select\n        \n        # IMPUTATION STEP: using TRAIN_MEANS\n        if TRAIN_MEANS is not None:\n             X_test_pd = X_test_pd.fillna(TRAIN_MEANS)\n        else:\n             X_test_pd = X_test_pd.fillna(X_test_pd.mean()) \n        \n    except Exception as e:\n        logging.error(f\"Feature selection failed in predict: {e}\")\n        return default_return\n\n    # Predict the raw market forward excess return\n    try:\n        predicted_return = MODEL.predict(X_test_pd)[-1]\n        \n        # Convert the predicted return into the final allocation\n        final_allocation = convert_prediction_to_allocation(predicted_return)\n        \n        if is_mock_run:\n             # Store both allocation and raw prediction for the mock test\n             return (final_allocation, predicted_return) \n        \n        return final_allocation\n\n    except Exception as e:\n        logging.error(f\"Inference failed: {e}. Returning neutral allocation.\")\n        return default_return\n\n\n# --- Visualization Helper ---\n\ndef plot_results(results_df: pd.DataFrame):\n    \"\"\"Generates a plot of the predicted returns and allocations.\"\"\"\n    \n    fig, ax1 = plt.subplots(figsize=(10, 5))\n\n    # Plot Predicted Return (left axis)\n    color = 'tab:blue'\n    ax1.set_xlabel('Simulated Day')\n    ax1.set_ylabel('Predicted Excess Return (Regression)', color=color) \n    ax1.plot(results_df['day'], results_df['predicted_return'], color=color, label='Predicted Return', alpha=0.6)\n    ax1.tick_params(axis='y', labelcolor=color)\n    ax1.grid(True, linestyle='--', alpha=0.5)\n    ax1.axhline(0.0, color='orange', linestyle='--', label='Neutral Return (0.0)')\n\n    # Plot Allocation (right axis)\n    ax2 = ax1.twinx()  \n    color = 'tab:red'\n    ax2.set_ylabel('Final Allocation (0.0 to 2.0)', color=color)  \n    ax2.plot(results_df['day'], results_df['allocation'], color=color, label='Final Allocation', linewidth=2)\n    ax2.tick_params(axis='y', labelcolor=color)\n    ax2.axhline(1.0, color='gray', linestyle=':', label='Neutral (1.0)')\n\n    # Title and Final Touches\n    fig.suptitle('Random Forest REGRESSION Mock Prediction and MAXIMAL RISK CONTROL')\n    fig.tight_layout()\n    plt.show()\n\n# --- Mock Test Runner ---\ndef run_mock_test_and_visualize():\n    \"\"\"Simulates 50 days of inference for visualization.\"\"\"\n    \n    MOCK_FEATURES = [\n        'D1', 'D2', 'E1', 'E2', 'V1', 'V2', 'S1', 'S2', 'M1', 'M2', 'T1', 'T2', 'P1', 'P2'\n    ]\n    \n    results = []\n    \n    # 1. Simulate training (Call setup_model manually for mock environment)\n    # The setup_model call is skipped in the mock runner below, so we manually simulate training\n    # for the mock environment by constructing a mock training set and calling predict once.\n    \n    # Re-simulate the required training history for feature engineering\n    mock_train_rows = []\n    for i in range(HISTORY_MAX_LENGTH + 10):\n        row = {col: [np.random.rand()] for col in MOCK_FEATURES}\n        row['market_forward_excess_returns'] = [np.random.uniform(-0.005, 0.005)] \n        row['date_id'] = [1000 + i]\n        mock_train_rows.append(pl.DataFrame(row))\n        \n    mock_train_df = pl.concat(mock_train_rows)\n    # Call the train_model directly since the mock environment doesn't have train.csv path\n    train_model(mock_train_df)\n    \n    logging.info(\"Model is now trained and history buffer initialized (Mock).\")\n\n    # 2. Simulate 50 days of real-time inference\n    NUM_SIMULATION_DAYS = 50\n    for day in range(NUM_SIMULATION_DAYS):\n        mock_day_data = {\n            c: [np.random.uniform(0.1, 0.9) if c == 'M1' else np.random.rand()] \n            for c in MOCK_FEATURES\n        }\n        mock_day_data['market_forward_excess_returns'] = [np.random.uniform(-0.005, 0.005)]\n        mock_day_data['date_id'] = [1000 + HISTORY_MAX_LENGTH + 10 + day]\n        mock_test_df_day = pl.DataFrame(mock_day_data)\n        \n        # Predict returns the tuple (allocation, raw_prediction) in mock mode\n        allocation, predicted_return = predict(mock_test_df_day)\n        \n        results.append({\n            'day': day + 1,\n            'predicted_return': predicted_return, # Storing raw predicted return\n            'allocation': allocation\n        })\n\n    # 3. Process and Plot Results\n    results_df = pd.DataFrame(results)\n    \n    logging.info(f\"\\n--- MOCK TEST SIMULATION SUMMARY (50 Days) ---\")\n    logging.info(f\"Mean Final Allocation: {results_df['allocation'].mean():.4f}\")\n    logging.info(f\"Min/Max Allocation: {results_df['allocation'].min():.4f} / {results_df['allocation'].max():.4f}\")\n    logging.info(\"-------------------------------------------------\")\n    \n    plot_results(results_df)\n\n# --- Main Execution Block for Kaggle ---\n\ninference_server_instance = inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    # CRITICAL FIX: Setup model BEFORE starting the inference server to avoid timeout.\n    setup_model() \n    inference_server_instance.serve()\nelse:\n    logging.info(\"Running local gateway for testing.\")\n    try:\n        local_input_path = os.path.join(os.getcwd(), 'kaggle_input/hull-tactical-market-prediction/')\n        inference_server_instance.run_local_gateway((local_input_path,))\n    except Exception as e:\n        logging.error(f\"Local gateway simulation failed. This is expected outside Kaggle: {e}\")\n        \n        run_mock_test_and_visualize()\n        \n        # --- Mandatory Dummy Submission File Generation for Kaggle System Check ---\n        logging.info(\"Generating dummy submission.parquet for Kaggle system check.\")\n        dummy_submission = pl.DataFrame({\n            'date_id': [999], \n            'allocation': [1.0] \n        })\n        dummy_submission.write_parquet('submission.parquet')\n        logging.info(\"submission.parquet created successfully.\")\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T23:04:28.143254Z","iopub.execute_input":"2025-11-03T23:04:28.143661Z","iopub.status.idle":"2025-11-03T23:04:31.822265Z","shell.execute_reply.started":"2025-11-03T23:04:28.143632Z","shell.execute_reply":"2025-11-03T23:04:31.820858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}