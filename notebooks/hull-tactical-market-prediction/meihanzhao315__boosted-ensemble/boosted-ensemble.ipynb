{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# 1. IMPORTS\n# =============================================================================\nimport os\nimport time\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nimport polars as pl\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet\nimport kaggle_evaluation.default_inference_server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T15:26:51.152358Z","iopub.execute_input":"2025-09-17T15:26:51.15261Z","iopub.status.idle":"2025-09-17T15:26:51.157088Z","shell.execute_reply.started":"2025-09-17T15:26:51.152591Z","shell.execute_reply":"2025-09-17T15:26:51.156361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# 2.CONFIGURATION\n# Centralizes all tunable parameters for easy experimentation.\n# =============================================================================\n@dataclass\nclass ModelConfig:\n    # --- Model Hyperparameters ---\n    enet_alpha: float = 0.001\n    enet_l1_ratio: float = 0.5\n    \n    xgb_n_estimators: int = 500\n    xgb_max_depth: int = 8\n    xgb_learning_rate: float = 0.04\n    xgb_subsample: float = 0.8\n    xgb_colsample_bytree: float = 0.8\n    \n    lgb_n_estimators: int = 500\n    lgb_max_depth: int = 8\n    lgb_learning_rate: float = 0.04\n    lgb_num_leaves: int = 20\n    lgb_subsample: float = 0.8\n    lgb_colsample_bytree: float = 0.8\n    \n    # --- Strategy Parameters ---\n    ensemble_weights: dict = field(default_factory=lambda: {'enet': 0.1, 'xgb': 0.5, 'lgb': 0.4})\n    vol_window: int = 20  # Window for calculating rolling volatility\n    signal_multiplier: float = 500.0  # Scales the raw prediction to a signal\n    min_signal: float = 0.0  # Minimum allocation allowed (no shorting)\n    max_signal: float = 2.0  # Maximum allocation allowed (up to 2x leverage)\n    vol_scaling: float = 1.0  # Factor to adjust volatility-based allocation\n    smoothing_factor: float = 0.8  # Smoothes allocation changes to reduce turnover (0.8*new + 0.2*old)\n    \n    # --- Training Control ---\n    retrain_freq: int = 20  # Retrain the models every 20 timesteps for efficiency\n    max_train_rows: int = 1000  # Use a rolling window of the most recent 1000 rows for training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T15:26:51.215838Z","iopub.execute_input":"2025-09-17T15:26:51.216048Z","iopub.status.idle":"2025-09-17T15:26:51.22312Z","shell.execute_reply.started":"2025-09-17T15:26:51.216032Z","shell.execute_reply":"2025-09-17T15:26:51.222369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# 3. MODEL CLASS\n# Encapsulates the entire modeling pipeline, including state management.\n# =============================================================================\nclass Model:\n    def __init__(self, config: ModelConfig):\n        \"\"\"\n        Constructor for the Model class.\n        Initializes models, scaler, and state-tracking variables.\n        \"\"\"\n        self.config = config\n        self.scaler = StandardScaler()\n        self.models = self._init_models()\n        \n        # State variables to be updated during online inference\n        self.training_data = None      # Stores the raw training data (features + target)\n        self.median_values = {}        # Caches median values for imputation\n        self.feature_cols = []         # List of feature names used for training\n        self.last_allocation = 0.0     # Stores the allocation from the previous timestep\n        self.test_row_count = 0        # Counter for incoming test rows\n        self.previous_test_df = None   # Stores the previous test row for online learning\n        \n        print(\"Model initialized.\")\n\n    def _init_models(self):\n        \"\"\"Initializes all model instances with hyperparameters from the config.\"\"\"\n        return {\n            'enet': ElasticNet(alpha=self.config.enet_alpha, l1_ratio=self.config.enet_l1_ratio, max_iter=10000),\n            \n            'xgb': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=self.config.xgb_n_estimators, max_depth=self.config.xgb_max_depth, \\\n                                    learning_rate=self.config.xgb_learning_rate, subsample=self.config.xgb_subsample, colsample_bytree=self.config.xgb_colsample_bytree, random_state=42, n_jobs=-1),\n            \n            'lgb': lgb.LGBMRegressor(objective='regression_l1', n_estimators=self.config.lgb_n_estimators, max_depth=self.config.lgb_max_depth, \\\n                                     learning_rate=self.config.lgb_learning_rate, num_leaves=self.config.lgb_num_leaves, subsample=self.config.lgb_subsample, \\\n                                     colsample_bytree=self.config.lgb_colsample_bytree, random_state=42, n_jobs=-1)\n        }\n\n    def _create_features(self, df: pl.DataFrame) -> pl.DataFrame:\n        \"\"\"\n        Generates time-series features for the input dataframe.\n        - Rolling Mean / Std: Captures recent trends and volatility.\n        - EWMA: Gives more weight to recent data, adapting faster to changes.\n        \"\"\"\n        key_features_for_rolling = ['V1', 'S1', 'P1', 'E1', 'M11'] # Key features for time-series analysis\n        windows = [5, 10, 20] # Short, medium, and long-term windows\n        spans = [5, 10, 20]   # Spans for Exponentially Weighted Moving Average\n        rolling_exprs = []\n\n        for col_name in key_features_for_rolling:\n            if col_name in df.columns:\n                for w in windows:\n                    rolling_exprs.append(pl.col(col_name).rolling_mean(w).alias(f'{col_name}_roll_mean_{w}'))\n                    rolling_exprs.append(pl.col(col_name).rolling_std(w).alias(f'{col_name}_roll_std_{w}'))\n                for s in spans:\n                    rolling_exprs.append(pl.col(col_name).ewm_mean(span=s).alias(f'{col_name}_ewm_mean_{s}'))\n        \n        return df.with_columns(rolling_exprs) if rolling_exprs else df\n\n    def _run_training_cycle(self):\n        \"\"\"\n        Runs a complete feature engineering and model training cycle on `self.training_data`.\n        This is a reusable function for both initial training and periodic retraining.\n        \"\"\"\n        print(\"Running a full training cycle...\")\n        \n        # Step 1: Generate features from the raw training data\n        processed_data = self._create_features(self.training_data)\n        \n        # Step 2: Identify feature columns and cache median values for imputation\n        all_features = [col for col in processed_data.columns if col not in ['date_id', 'target']]\n        self.feature_cols = all_features\n        self.median_values = {col: processed_data[col].median() for col in self.feature_cols if processed_data[col].is_not_null().any()}\n\n        # Step 3: Impute missing values\n        impute_exprs = [pl.col(c).fill_null(self.median_values.get(c, 0.0)) for c in self.feature_cols]\n        processed_data = processed_data.with_columns(impute_exprs)\n\n        # Step 4: Prepare data for Scikit-learn models (convert to Pandas, scale)\n        # Final fill_null in Polars before converting to Pandas to prevent errors\n        X_train_pd = processed_data.select(self.feature_cols).fill_null(0.0).to_pandas()\n        y_train_pd = processed_data['target'].to_pandas()\n        X_train_scaled = self.scaler.fit_transform(X_train_pd)\n\n        # Step 5: Train each model in the ensemble\n        for name, model in self.models.items():\n            model.fit(X_train_scaled, y_train_pd)\n        print(\"Training cycle completed.\")\n\n    def train(self, train_df: pl.DataFrame):\n        \"\"\"\n        Initializes the model by performing the first training cycle.\n        \"\"\"\n        print(\"Starting initialization and first training...\")\n        \n        # Drop columns that exist in train.csv but not in test.csv to ensure consistency\n        unused_cols = ['forward_returns', 'risk_free_rate']\n        train_df = train_df.drop([col for col in unused_cols if col in train_df.columns])\n        \n        # Prepare and store the initial raw training data\n        self.training_data = train_df.rename({'market_forward_excess_returns': 'target'}) \\\n                           .with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False)) \\\n                           .filter(pl.col('date_id') >= 37) \\\n                           .tail(self.config.max_train_rows)\n        \n        # Run the first training cycle\n        self._run_training_cycle()\n\n    def predict(self, test_df: pl.DataFrame) -> float:\n        \"\"\"\n        Generates a prediction and allocation for a single test data point.\n        This function is called iteratively by the Kaggle API.\n        \"\"\"\n        # Ensure incoming data has the correct float type to prevent errors\n        test_df = test_df.with_columns(pl.exclude(['date_id', 'is_scored']).cast(pl.Float64, strict=False))\n\n        # --- Online Learning Step ---\n        # If this is not the first prediction, update the training data with the previous step's observation.\n        if self.previous_test_df is not None:\n            # The lagged return from the current test_df is the true target for the previous test_df\n            target_value = test_df['lagged_market_forward_excess_returns'][0]\n            new_training_row = self.previous_test_df.with_columns(pl.lit(target_value).alias('target'))\n            \n            # Append the new row and keep the training data size fixed\n            self.training_data = pl.concat([\n                self.training_data,\n                new_training_row.select(self.training_data.columns)\n            ]).tail(self.config.max_train_rows)\n            \n            # Retrain periodically based on the configured frequency\n            if self.test_row_count % self.config.retrain_freq == 0:\n                self._run_training_cycle()\n\n        # --- Prediction Step for the Current Timestep ---\n        # Step 1: Create features for the current test row using historical context\n        history_for_features = self.training_data.drop('target')\n        current_test_with_history = pl.concat([\n            history_for_features.tail(self.config.vol_window * 2),\n            test_df.select(history_for_features.columns)\n        ])\n        featured_test_with_history = self._create_features(current_test_with_history)\n        featured_test = featured_test_with_history.tail(1)\n        \n        # Step 2: Impute and prepare the test data for prediction\n        impute_exprs = [pl.col(c).fill_null(self.median_values.get(c, 0.0)) for c in self.feature_cols]\n        featured_test = featured_test.with_columns(impute_exprs)\n        X_test_pd = featured_test.select(self.feature_cols).fill_null(0.0).to_pandas()\n        X_test_scaled = self.scaler.transform(X_test_pd)\n        \n        # Step 3: Get predictions from all models and create an ensemble prediction\n        predictions = {name: model.predict(X_test_scaled)[0] for name, model in self.models.items()}\n        raw_pred = sum(predictions[name] * self.config.ensemble_weights[name] for name in self.models)\n        \n        # --- Allocation Strategy ---\n        # Step 4: Estimate recent volatility from the training data returns\n        recent_returns = self.training_data['target'].tail(self.config.vol_window).to_numpy()\n        volatility = np.std(recent_returns) if len(recent_returns) > 1 else 0.01\n        volatility = max(volatility, 0.01) # Avoid division by zero\n        \n        # Step 5: Convert raw prediction to a signal and adjust for volatility\n        signal = np.clip(raw_pred * self.config.signal_multiplier, self.config.min_signal, self.config.max_signal)\n        allocation = np.clip(signal / (volatility * self.config.vol_scaling), self.config.min_signal, self.config.max_signal)\n        \n        # Step 6: Smooth the final allocation to reduce rapid changes\n        final_allocation = (self.config.smoothing_factor * allocation + (1 - self.config.smoothing_factor) * self.last_allocation)\n        \n        # --- State Update ---\n        # Update state variables for the next iteration\n        self.last_allocation = final_allocation\n        self.test_row_count += 1\n        self.previous_test_df = test_df # Store the current test data for the next learning step\n        \n        return float(final_allocation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T15:26:51.224286Z","iopub.execute_input":"2025-09-17T15:26:51.224534Z","iopub.status.idle":"2025-09-17T15:26:51.249198Z","shell.execute_reply.started":"2025-09-17T15:26:51.224518Z","shell.execute_reply":"2025-09-17T15:26:51.248519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# 4. MAIN EXECUTION BLOCK\n# =============================================================================\nif __name__ == '__main__':\n    # Setup data path and load initial training data\n    DATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\n    initial_train_df = pl.read_csv(DATA_PATH / \"train.csv\")\n    \n    # Initialize and train the model\n    start_time = time.time()\n    model_config = ModelConfig()\n    model = Model(model_config)\n    model.train(initial_train_df)\n    \n    # Check if the startup process is within the time limit\n    startup_time = time.time() - start_time\n    print(f\"Model startup and initial training time: {startup_time:.2f} seconds\")\n    if startup_time > 900:\n        raise RuntimeError(\"Startup time exceeded the 900-second limit.\")\n\n    # Create a wrapper function for the Kaggle API\n    # The API requires a plain 'predict' function, not a class method.\n    def predict(test_df: pl.DataFrame) -> float:\n        return model.predict(test_df)\n\n    # Initialize the inference server with the wrapper function\n    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n    \n    # Run the server based on the environment (Kaggle submission vs. local testing)\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        inference_server.serve()\n    else:\n        inference_server.run_local_gateway((str(DATA_PATH),))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T15:26:51.251756Z","iopub.execute_input":"2025-09-17T15:26:51.252123Z","iopub.status.idle":"2025-09-17T15:26:58.563227Z","shell.execute_reply.started":"2025-09-17T15:26:51.252106Z","shell.execute_reply":"2025-09-17T15:26:58.562691Z"}},"outputs":[],"execution_count":null}]}