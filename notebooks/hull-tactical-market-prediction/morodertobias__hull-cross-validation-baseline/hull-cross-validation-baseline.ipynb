{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"},{"sourceId":262840772,"sourceType":"kernelVersion"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hull - Cross Validation Baseline\n\n## Motivation\n- We need a good validation scheme, otherwise the competition is pure gambling.\n- Note, ultimately the validation scheme needs to show us good hyperparameters, not neccessarily that the CV score is exact. \n- As a model reference I use my leak-free baseline that directly targets the strategy as output, see [Hull - Leak Safe Baseline](https://www.kaggle.com/code/morodertobias/hull-leak-safe-baseline).\n- A single train-valid split is not sufficient; so a larger number of splits is required.\n- These splits should respect the time ordering that training comes before validation.\n- After having determined the best hyperparameters train the best model for the leak-free public test set.\n- The first results were disappointing:\n    - Lots of fluctuations in the scores.\n    - Unclear cross validation settings: number of splits, train/valid sizes or aggregate function\n    - Also, the simple model from the baseline seems to not learn anything. In order to enhace model capacity I switched to the elastic net using all features.\n\n## Current Cross Validation\n- Make several temporal time series splits over the training set.\n- Keep validation set sizes fixed. Initially I set it the size of test set (so 180), but the scores are strongly fluctuating. I think the score is also very unstable, maybe even too susceptible to outlier due to the mean the Sharpe ratio. Hence I was using a larger validation set sizes.\n- In order to remove effects on the initial splitting subsequent validations sets partially overlap.\n- Finally I look at median score to determine best hyperparameters.\n\n**What do you think about this cross validation strategy? How do you validate? All comments welcome!**","metadata":{}},{"cell_type":"markdown","source":"## Import & Settings","metadata":{}},{"cell_type":"code","source":"import os\nimport pathlib\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport polars as pl \nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nimport kaggle_evaluation.default_inference_server\nfrom metric import score as hull_score\nwarnings.simplefilter(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:16.686061Z","iopub.execute_input":"2025-10-30T20:11:16.686683Z","iopub.status.idle":"2025-10-30T20:11:18.955133Z","shell.execute_reply.started":"2025-10-30T20:11:16.686659Z","shell.execute_reply":"2025-10-30T20:11:18.954354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_DIR = pathlib.Path(\"/kaggle/input/hull-tactical-market-prediction\")\nTEST_SKIP = 180\nSKIP_ROWS = 2000\nINFO_COLS = [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"]\n# model as in hull starter but on all features\nL1_RATIO = 0.5\nALPHAS = np.logspace(-5, 0, 50)\nMAX_ITER = 1000000\n# cv \nTRAIN_SIZE = 4000\nVALID_SIZE = 2 * 180\nVALID_STRIDE = 90\nN_SPLITS = 28","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:18.956636Z","iopub.execute_input":"2025-10-30T20:11:18.957023Z","iopub.status.idle":"2025-10-30T20:11:18.962498Z","shell.execute_reply.started":"2025-10-30T20:11:18.957003Z","shell.execute_reply":"2025-10-30T20:11:18.961714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(BASE_DIR / \"train.csv\")\nFEATURES = data.filter(regex=r\"(M|E|I|P|V|S)\").columns.to_list()\n# skip first rows\ndata = data.iloc[SKIP_ROWS:]\ndata = data[FEATURES + INFO_COLS]\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:18.963139Z","iopub.execute_input":"2025-10-30T20:11:18.96337Z","iopub.status.idle":"2025-10-30T20:11:19.287517Z","shell.execute_reply.started":"2025-10-30T20:11:18.963354Z","shell.execute_reply":"2025-10-30T20:11:19.286597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_train_date = data[\"date_id\"].max() - TEST_SKIP\nprint(\"max_train_date:\", max_train_date)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.288319Z","iopub.execute_input":"2025-10-30T20:11:19.288598Z","iopub.status.idle":"2025-10-30T20:11:19.293448Z","shell.execute_reply.started":"2025-10-30T20:11:19.288572Z","shell.execute_reply":"2025-10-30T20:11:19.292584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = data.loc[data[\"date_id\"] <= max_train_date].copy()\ntest = data.loc[data[\"date_id\"] > max_train_date].copy()\nprint(\"train shape:\", train.shape)\nprint(\"test shape:\", test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.295475Z","iopub.execute_input":"2025-10-30T20:11:19.295686Z","iopub.status.idle":"2025-10-30T20:11:19.315648Z","shell.execute_reply.started":"2025-10-30T20:11:19.295664Z","shell.execute_reply":"2025-10-30T20:11:19.314978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build target\nSet target as best strategy on training dataset.","metadata":{}},{"cell_type":"code","source":"def compute_ideal_targets(df):\n    \"\"\"Computes ideal targets\"\"\"\n    solution = df.copy()\n    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n    c = (1 + market_mean_excess_return) ** (1 / (market_excess_returns > 0).mean()) - 1\n    submission = pd.DataFrame({'prediction': (c / market_excess_returns).clip(0, 2)})\n    score = hull_score(solution, submission, '')\n    return submission, score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.316231Z","iopub.execute_input":"2025-10-30T20:11:19.316449Z","iopub.status.idle":"2025-10-30T20:11:19.321421Z","shell.execute_reply.started":"2025-10-30T20:11:19.316433Z","shell.execute_reply":"2025-10-30T20:11:19.320732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"__, score = compute_ideal_targets(train)\nprint(f\"Hull score: {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.32207Z","iopub.execute_input":"2025-10-30T20:11:19.32226Z","iopub.status.idle":"2025-10-30T20:11:19.33998Z","shell.execute_reply.started":"2025-10-30T20:11:19.322246Z","shell.execute_reply":"2025-10-30T20:11:19.339345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"def generate_splits():\n    num_samples = len(train)\n    tv_size = TRAIN_SIZE + VALID_SIZE\n    start = num_samples - tv_size - (N_SPLITS - 1) * VALID_STRIDE\n    if start < 0:\n        raise ValueError(\"Too few samples!\")\n    for i in range(N_SPLITS):\n        i_start = start + i * VALID_STRIDE\n        i_loc_train = np.arange(i_start, i_start + TRAIN_SIZE)\n        i_loc_valid = np.arange(i_start + TRAIN_SIZE, i_start + tv_size)\n        yield (i_loc_train, i_loc_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.340653Z","iopub.execute_input":"2025-10-30T20:11:19.340934Z","iopub.status.idle":"2025-10-30T20:11:19.346073Z","shell.execute_reply.started":"2025-10-30T20:11:19.340907Z","shell.execute_reply":"2025-10-30T20:11:19.345269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_summary = []\nfor i, (train_iloc, valid_iloc) in enumerate(generate_splits()):\n    split_summary.append(\n        {\n            \"split\": i,\n            \"train_start\": train_iloc[0],\n            \"train_end\": train_iloc[-1],\n            \"train_size\": 1 + train_iloc[-1] - train_iloc[0],\n            \"valid_start\": valid_iloc[0],\n            \"valid_end\": valid_iloc[-1],\n            \"valid_size\": 1 + valid_iloc[-1] - valid_iloc[0],\n        }\n    )\nsplit_summary = pd.DataFrame(split_summary)\nsplit_summary[\"train_stride\"] = split_summary[\"train_start\"] - split_summary[\n    \"train_start\"\n].shift(1)\nsplit_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.346904Z","iopub.execute_input":"2025-10-30T20:11:19.347167Z","iopub.status.idle":"2025-10-30T20:11:19.372168Z","shell.execute_reply.started":"2025-10-30T20:11:19.347142Z","shell.execute_reply":"2025-10-30T20:11:19.371571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_df, test_df=None):\n    y_train, __ = compute_ideal_targets(train_df)\n    X_train = train_df[FEATURES]\n    model.fit(X_train, y_train)\n    score_train = model.score(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    sub_train = pd.DataFrame(\n        {\"prediction\": y_pred_train.clip(0.0, 2.0)}, index=train_df.index\n    )\n    hull_train = hull_score(train_df, sub_train, \"\")\n    scores = {\n        \"train_score\": score_train,\n        \"train_hull\": hull_train,\n    }\n    if test_df is not None:\n        X_test = test_df[FEATURES]\n        y_pred_test = model.predict(X_test)\n        sub_train = pd.DataFrame(\n            {\"prediction\": y_pred_test.clip(0.0, 2.0)}, index=test_df.index\n        )\n        hull_test = hull_score(test_df, sub_train, \"\")\n        scores[\"test_hull\"] = hull_test\n    return model, scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.372973Z","iopub.execute_input":"2025-10-30T20:11:19.373167Z","iopub.status.idle":"2025-10-30T20:11:19.378735Z","shell.execute_reply.started":"2025-10-30T20:11:19.373153Z","shell.execute_reply":"2025-10-30T20:11:19.377919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv_scores = {}\n\nfor fold, (train_iloc, valid_iloc) in \\\n    tqdm(enumerate(generate_splits()), total=N_SPLITS):\n\n    df_train = train.iloc[train_iloc].copy()\n    df_test = train.iloc[valid_iloc].copy()\n\n    fold_scores = {}\n    for alpha in ALPHAS:\n        model = Pipeline([\n            (\"im\", SimpleImputer(strategy='median')),\n            (\"sc\", StandardScaler()),\n            (\"reg\", ElasticNet(alpha=alpha, l1_ratio=L1_RATIO, max_iter=MAX_ITER)),\n        ])\n        model, scores = train_model(model, df_train, df_test)\n        fold_scores[alpha] = scores\n    fold_scores = pd.DataFrame(fold_scores).T\n    \n    cv_scores[fold] = fold_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:11:19.379324Z","iopub.execute_input":"2025-10-30T20:11:19.379557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv_scores = pd.concat(cv_scores).unstack(level=1)\ncv_scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Look at scores","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 5))\ncv_scores[\"test_hull\"].T.plot(\n    ax=ax,\n    logx=True,\n    legend=False,\n    xlabel=\"alpha\",\n    ylabel=\"test hull\",\n    title=\"hull score by split\",\n)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 5))\ncv_scores[\"test_hull\"].plot(\n    kind=\"box\", ax=ax, xlabel=\"alpha position\", ylabel=\"test hull\"\n)\nax.set_xticks(range(len(ALPHAS)))\nax.set_xticklabels(range(len(ALPHAS)), rotation=90, ha=\"right\")\nax.set_xlabel(\"ALPHA position\")\nax.set_ylabel(\"Hull  score\")\nax.set_ylim(-0.5, 1.8)\nax.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, sharex=\"all\", sharey=\"all\", gridspec_kw=dict(hspace=0), figsize=(8, 5))\nfig.suptitle(\"mean/median hull scores\")\nax = axs[0]\ncv_scores[\"train_hull\"].mean().plot(\n    ax=ax, xlabel=\"alpha\", marker=\"x\", label=\"train\", logx=True, ylabel=\"mean hull\"\n)\ncv_scores[\"test_hull\"].mean().plot(ax=ax, marker=\"x\", label=\"test\")\nax.legend(loc=\"upper right\")\nax = axs[1]\ncv_scores[\"train_hull\"].median().plot(\n    ax=ax,\n    marker=\"x\",\n    label=\"train\",\n    logx=True,\n    ylabel=\"mean hull\",\n    xlabel=\"alpha\",\n)\ncv_scores[\"test_hull\"].median().plot(ax=ax, marker=\"x\", label=\"test\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 5))\ncv_scores[\"test_hull\"][ALPHAS[::10]].plot(\n    kind=\"bar\",\n    ax=ax,\n    xlabel=\"split\",\n    ylabel=\"test hull\",\n    title=\"hull score by split for some variants\",\n)\nax.legend(title=\"alpha\")\nhandles, labels = ax.get_legend_handles_labels()\nfmt_labels = [f\"{float(l):.4e}\" for l in labels]\nax.legend(handles, fmt_labels, title=\"alpha\")\nax.set_ylim([-0.2, 1.7])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 5))\ncv_scores[\"train_score\"].median().plot(\n    ax=ax, marker=\"x\", label=\"train\", logx=True, xlabel=\"alpha\", ylabel=\"r2\"\n)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Determine best parameter and retrain","metadata":{}},{"cell_type":"code","source":"test_scores = cv_scores[\"test_hull\"].median()\nalpha0 = test_scores.idxmax()\nprint(f\"best median score: {test_scores[alpha0]:.4f} @ alpha={alpha0:.4E}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = train.iloc[-TRAIN_SIZE:].copy()\ndf_test = test\nmodel = Pipeline([\n    (\"im\", SimpleImputer(strategy='median')),\n    (\"sc\", StandardScaler()),\n    (\"reg\", ElasticNet(alpha=alpha0, l1_ratio=L1_RATIO, max_iter=MAX_ITER)),\n])\nmodel, scores = train_model(model, df_train, df_test)\npd.Series(scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = test[FEATURES]\ny_test_pred = model.predict(X_test)\ny_test_pred = np.clip(y_test_pred, 0.0, 2.0)\npd.Series(y_test_pred).describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(\n    model.named_steps[\"reg\"].intercept_,\n    model.named_steps[\"reg\"].coef_ \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solution = test.copy()\nsubmission = pd.DataFrame({'prediction': y_test_pred}, index=solution.index)\nhull_score(solution, submission, '')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Make submission","metadata":{}},{"cell_type":"code","source":"def predict(test: pl.DataFrame) -> float:\n    data = test.to_pandas()\n    X = data[FEATURES]\n    y = model.predict(X)\n    pred = np.clip(y, 0.0, 2.0)[0]\n    print(f\"date_id: {data['date_id'][0]} -> prediction: {pred:>.4f}\")\n    return pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway((BASE_DIR.as_posix(),))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}