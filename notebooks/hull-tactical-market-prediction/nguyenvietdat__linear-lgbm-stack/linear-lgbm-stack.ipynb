{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Tiny-data robust pipeline with linear stack (+ optional LightGBM) and learned exposure map\n# - Imputer -> PowerTransformer(Yeo–Johnson) -> StandardScaler (fit on train, reuse at test)\n# - Base learners: RidgeCV, LassoCV, ElasticNetCV, optional LightGBM (tightly regularized)\n# - StackingRegressor with RidgeCV meta for mu and var\n# - Score s = mu / sqrt(var + eps), monotone piecewise exposure map g(s) learned on time-ordered validation\n# - Global volatility cap <= 1.2x market vol on validation; predictions clipped to [0, 2]\n# - Works with Kaggle default inference server\n\nimport os, warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np, pandas as pd, polars as pl\nfrom pathlib import Path\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.ensemble import StackingRegressor\n\ntry:\n    from lightgbm import LGBMRegressor\n    HAS_LGBM = True\nexcept Exception:\n    HAS_LGBM = False\n\nimport kaggle_evaluation.default_inference_server \n\n# ---------------- Config ----------------\nDATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction/\")\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nMIN_INVESTMENT, MAX_INVESTMENT = 0.0, 2.0\nPREFIXES = (\"D\",\"E\",\"I\",\"M\",\"P\",\"S\",\"V\",\"MOM\")\nLAG_SRC_COLS = [\"E1\",\"V1\",\"E3\"]\n\nVAL_FRAC = 0.15\nN_BINS = 7\nEPS_VAR = 1e-6\nTARGET_VOL_MULT = 1.2  # keep strategy vol <= 1.2 * market vol on validation\n\nUSE_LGBM_BASE = True  # set False to keep strictly linear base models\n\n# ------------- Metric (official) -------------\nclass ParticipantVisibleError(Exception): pass\n\ndef score_metric(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n    sol = solution.copy()\n    sol[\"position\"] = submission[\"prediction\"].astype(float)\n    if sol[\"position\"].max() > MAX_INVESTMENT:\n        raise ParticipantVisibleError(f'Position {sol[\"position\"].max()} exceeds {MAX_INVESTMENT}')\n    if sol[\"position\"].min() < MIN_INVESTMENT:\n        raise ParticipantVisibleError(f'Position {sol[\"position\"].min()} below {MIN_INVESTMENT}')\n    sol[\"strategy_returns\"] = sol[\"risk_free_rate\"]*(1 - sol[\"position\"]) + sol[\"forward_returns\"]*sol[\"position\"]\n    strategy_excess = sol[\"strategy_returns\"] - sol[\"risk_free_rate\"]\n    strategy_excess_cum = (1.0 + strategy_excess).prod()\n    strategy_mean_excess = strategy_excess_cum**(1/len(sol)) - 1.0\n    strategy_std = sol[\"strategy_returns\"].std()\n    if strategy_std == 0:\n        raise ZeroDivisionError\n    trading_days_per_yr = 252\n    sharpe = strategy_mean_excess / strategy_std * np.sqrt(trading_days_per_yr)\n\n    market_excess = sol[\"forward_returns\"] - sol[\"risk_free_rate\"]\n    market_excess_cum = (1.0 + market_excess).prod()\n    market_mean_excess = market_excess_cum**(1/len(sol)) - 1.0\n    market_std = sol[\"forward_returns\"].std()\n\n    strategy_vol = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n    market_vol = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n    excess_vol = max(0, strategy_vol/market_vol - 1.2) if market_vol > 0 else 0\n    vol_penalty = 1 + excess_vol\n\n    return_gap = max(0, (market_mean_excess - strategy_mean_excess) * 100 * trading_days_per_yr)\n    return_penalty = 1 + (return_gap**2) / 100\n\n    return min(float(sharpe / (vol_penalty * return_penalty)), 1_000_000.0)\n\n# ------------- Globals for serving -------------\nfeature_cols_global = None\nimputer_global = None\npt_global = None\nscaler_global = None\nlag_state = {c: 0.0 for c in LAG_SRC_COLS}\nmu_model = None\nvar_model = None\nbin_edges_global = None\nbin_exposures_global = None\nvol_scale_global = 1.0\n\n# ------------- Helpers -------------\ndef is_feature(c): return c.startswith(PREFIXES)\n\ndef list_feature_cols(df):\n    cols = [c for c in df.columns if is_feature(c)]\n    cols.sort()\n    return cols\n\ndef clip_outliers(df, cols, ql=0.01, qh=0.99):\n    for c in cols:\n        low, high = df[c].quantile(ql), df[c].quantile(qh)\n        df[c] = df[c].clip(low, high)\n    return df\n\ndef add_lags(df, cols):\n    for c in cols:\n        if c not in df.columns: df[c] = 0.0\n        df[f\"{c}_lag1\"] = df[c].shift(1).fillna(0.0)\n        df[f\"{c}_lag2\"] = df[c].shift(2).fillna(0.0)\n    return df\n\n# ------------- Preprocessing (fit) -------------\ndef fit_preprocess(train):\n    global feature_cols_global, imputer_global, pt_global, scaler_global\n    train = train.sort_values(\"date_id\").reset_index(drop=True)\n    feature_cols_global = list_feature_cols(train)\n\n    train[\"risk_free_rate\"] = train[\"risk_free_rate\"].diff().fillna(0.0)\n    train[\"excess_returns\"] = train[\"forward_returns\"] - train[\"risk_free_rate\"]\n\n    train = clip_outliers(train, feature_cols_global)\n    train = add_lags(train, [c for c in LAG_SRC_COLS if c in train.columns])\n    lag_cols = [f\"{c}_lag1\" for c in LAG_SRC_COLS if f\"{c}_lag1\" in train.columns] + \\\n               [f\"{c}_lag2\" for c in LAG_SRC_COLS if f\"{c}_lag2\" in train.columns]\n    X_cols = feature_cols_global + lag_cols\n\n    imputer_global = SimpleImputer(strategy=\"median\")\n    X_imp = imputer_global.fit_transform(train[X_cols])\n\n    pt_global = PowerTransformer(method=\"yeo-johnson\", standardize=False)\n    X_pt = pt_global.fit_transform(X_imp)\n\n    scaler_global = StandardScaler()\n    X_all = scaler_global.fit_transform(X_pt)\n\n    y_mu = train[\"excess_returns\"].values.astype(float)\n    y_var = (train[\"excess_returns\"].values.astype(float))**2\n    return X_all, y_mu, y_var, X_cols, train\n\n# ------------- Preprocessing (transform) -------------\ndef transform_preprocess(test_df):\n    global lag_state\n    df = test_df.sort_values(\"date_id\").reset_index(drop=True)\n    for c in feature_cols_global:\n        if c not in df.columns: df[c] = 0.0\n    df = clip_outliers(df, feature_cols_global)\n    for c in LAG_SRC_COLS:\n        if c not in df.columns: df[c] = 0.0\n        df[f\"{c}_lag1\"] = df[c].shift(1)\n        df.loc[df.index.min(), f\"{c}_lag1\"] = lag_state.get(c, 0.0)\n        df[f\"{c}_lag1\"] = df[f\"{c}_lag1\"].fillna(0.0)\n        df[f\"{c}_lag2\"] = df[c].shift(2).fillna(0.0)\n    if len(df) > 0:\n        for c in LAG_SRC_COLS:\n            lag_state[c] = float(df[c].iloc[-1])\n    lag_cols = [f\"{c}_lag1\" for c in LAG_SRC_COLS] + [f\"{c}_lag2\" for c in LAG_SRC_COLS]\n    X_cols = feature_cols_global + lag_cols\n    for c in X_cols:\n        if c not in df.columns: df[c] = 0.0\n    X_imp = imputer_global.transform(df[X_cols])\n    X_pt = pt_global.transform(X_imp)\n    Xt = scaler_global.transform(X_pt)\n    if not np.isfinite(Xt).all():\n        Xt = np.nan_to_num(Xt, nan=0.0, posinf=0.0, neginf=0.0)\n    return Xt\n\n# ------------- Models -------------\ndef build_models():\n    ridge = RidgeCV(alphas=[1e-3,1e-2,1e-1,1,10])\n    lasso = LassoCV(alphas=[1e-3,1e-2,1e-1,1,10], max_iter=5000, random_state=RANDOM_STATE)\n    enet  = ElasticNetCV(alphas=[1e-3,1e-2,1e-1,1,10], l1_ratio=[0.2,0.5,0.8], max_iter=5000, random_state=RANDOM_STATE)\n    base = [(\"ridge\", ridge), (\"lasso\", lasso), (\"enet\", enet)]\n\n    # Optional: one tightly-regularized LightGBM to capture mild nonlinearity\n    if USE_LGBM_BASE and HAS_LGBM:\n        lgbm = LGBMRegressor(\n            n_estimators=300, learning_rate=0.03, num_leaves=15,\n            max_depth=4, reg_alpha=1.0, reg_lambda=2.0,\n            subsample=0.7, colsample_bytree=0.7, random_state=RANDOM_STATE, verbose=-1\n        )\n        base.append((\"lgbm\", lgbm))\n\n    meta = RidgeCV(alphas=[1e-3,1e-2,1e-1,1,10])\n    mu = StackingRegressor(estimators=base, final_estimator=meta, cv=5, n_jobs=-1)\n    var = StackingRegressor(estimators=base, final_estimator=meta, cv=5, n_jobs=-1)\n    return mu, var\n\n# ------------- Learn exposure map and vol cap -------------\ndef learn_exposure_and_vol(train_proc, X_all, mu, var):\n    n = len(train_proc)\n    split = int(n*(1.0 - VAL_FRAC))\n    val_idx = np.arange(split, n)\n    sol_val = train_proc.iloc[val_idx][[\"date_id\",\"forward_returns\",\"risk_free_rate\"]].reset_index(drop=True)\n\n    mu_val = mu.predict(X_all[val_idx])\n    var_val = var.predict(X_all[val_idx])\n    scores = mu_val / np.sqrt(np.maximum(var_val, EPS_VAR))\n\n    qs = np.linspace(0, 1, N_BINS+1)\n    edges = np.quantile(scores, qs)\n    for i in range(1, len(edges)):\n        if edges[i] <= edges[i-1]:\n            edges[i] = edges[i-1] + 1e-9\n\n    exps = np.linspace(0.0, 0.8, N_BINS)\n    best_score = -1e9\n\n    def assign(scores_, exps_, edges_):\n        idx = np.searchsorted(edges_[1:-1], scores_, side=\"right\")\n        return exps_[idx]\n\n    # Coordinate ascent + monotone projection\n    for _ in range(60):\n        improved = False\n        for k in range(N_BINS):\n            grid = np.linspace(max(0.0, exps[k]-0.25), min(2.0, exps[k]+0.25), 9)\n            local_best, local_best_score = exps[k], -1e9\n            for v in grid:\n                trial = exps.copy()\n                trial[k] = v\n                for i in range(1, N_BINS):\n                    if trial[i] < trial[i-1]:\n                        trial[i] = trial[i-1]\n                pos = np.clip(assign(scores, trial, edges), MIN_INVESTMENT, MAX_INVESTMENT)\n                s = score_metric(sol_val.copy(), pd.DataFrame({\"prediction\": pos}))\n                if s > local_best_score:\n                    local_best_score, local_best = s, trial[k]\n            if local_best_score > best_score:\n                best_score, exps[k] = local_best_score, local_best\n                improved = True\n        if not improved:\n            break\n\n    # Volatility cap to <= 1.2x market vol\n    pos_val = np.clip(assign(scores, exps, edges), MIN_INVESTMENT, MAX_INVESTMENT)\n    tmp = sol_val.copy()\n    tmp[\"position\"] = pos_val\n    strat = tmp[\"risk_free_rate\"]*(1 - tmp[\"position\"]) + tmp[\"forward_returns\"]*tmp[\"position\"]\n    strat_std_ann = strat.std() * np.sqrt(252)\n    mkt_std_ann = tmp[\"forward_returns\"].std() * np.sqrt(252)\n    vol_scale = 1.0\n    if mkt_std_ann > 0 and strat_std_ann > TARGET_VOL_MULT * mkt_std_ann:\n        vol_scale = (TARGET_VOL_MULT * mkt_std_ann) / strat_std_ann\n\n    return edges, exps, float(vol_scale), float(best_score)\n\n# ------------- Train all -------------\ndef train_all():\n    global mu_model, var_model, bin_edges_global, bin_exposures_global, vol_scale_global\n    train = pd.read_csv(DATA_PATH / \"train.csv\").dropna(subset=[\"forward_returns\",\"risk_free_rate\"])\n    train = train.sort_values(\"date_id\").reset_index(drop=True)\n    X_all, y_mu, y_var, X_cols, train_proc = fit_preprocess(train)\n\n    mu_model, var_model = build_models()\n    mu_model.fit(X_all, y_mu)\n    var_model.fit(X_all, y_var)\n\n    edges, exps, vol_scale, val_score = learn_exposure_and_vol(train_proc, X_all, mu_model, var_model)\n    bin_edges_global, bin_exposures_global, vol_scale_global = edges, exps, vol_scale\n    return float(val_score)\n\n# ------------- Predict callback -------------\ndef make_predict():\n    def predict(test_pl: pl.DataFrame) -> pl.DataFrame:\n        Xt = transform_preprocess(test_pl.to_pandas())\n        mu = mu_model.predict(Xt)\n        var = var_model.predict(Xt)\n        scores = mu / np.sqrt(np.maximum(var, EPS_VAR))\n        idx = np.searchsorted(bin_edges_global[1:-1], scores, side=\"right\")\n        pos = bin_exposures_global[idx] * vol_scale_global\n        pos = np.clip(pos, MIN_INVESTMENT, MAX_INVESTMENT)\n        if np.all(pos == pos[0]):\n            pos = np.clip(pos + 1e-6, MIN_INVESTMENT, MAX_INVESTMENT)\n        return pl.DataFrame({\"prediction\": pos})\n    return predict\n\n# ------------- Main -------------\ndef main():\n    val_score = train_all()\n    # Optional: print validation score for monitoring\n    print(f\"[Validation adjusted‑Sharpe (val slice)]: {val_score:.4f}\")\n    predict_fn = make_predict()\n    server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict_fn)\n    if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        server.serve()\n    else:\n        server.run_local_gateway((str(DATA_PATH),))\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:13:47.226631Z","iopub.execute_input":"2025-10-18T08:13:47.226942Z","iopub.status.idle":"2025-10-18T08:13:55.067745Z","shell.execute_reply.started":"2025-10-18T08:13:47.226921Z","shell.execute_reply":"2025-10-18T08:13:55.066614Z"}},"outputs":[],"execution_count":null}]}