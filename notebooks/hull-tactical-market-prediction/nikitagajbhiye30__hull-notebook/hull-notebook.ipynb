{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Legitimate, ready-to-run notebook for Hull Tactical Market Prediction\n# - Does NOT use test labels\n# - Handles common runtime errors (file paths, NaNs, polars->pandas, server registration)\n# - Uses time-aware CV and a robust mapping from predicted return to position [0,2]\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom typing import List, Tuple\nimport joblib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Kaggle evaluation server (use in Kaggle environment)\nimport kaggle_evaluation.default_inference_server\n\n# ----------------------------\n# Configuration / constants\n# ----------------------------\nTRAIN_PATH = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\n# If running locally outside Kaggle, change TRAIN_PATH to actual file path\nRANDOM_STATE = 42\nN_SPLITS = 4            # time-series splits for quick CV\nMODEL_FILE = \"gbm_model.joblib\"\nMEDIANS_FILE = \"feature_medians.joblib\"\nPRED_QUANT_FILE = \"pred_quantiles.joblib\"\n\n# ----------------------------\n# Utility: robust feature selection + numeric coercion\n# ----------------------------\ndef load_and_prepare_train(path: str) -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n    \"\"\"\n    Load CSV, identify numeric features, coerce to numeric safely,\n    return X (DataFrame), y (Series), feature_cols list.\n    \"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Train CSV not found at {path!r}. Update TRAIN_PATH.\")\n    df = pd.read_csv(path, index_col=\"date_id\")\n    # target\n    if \"forward_returns\" not in df.columns:\n        raise KeyError(\"Expected 'forward_returns' column in train.csv\")\n    y = df[\"forward_returns\"].copy()\n    # Known non-feature columns to drop\n    drop_cols = {\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"}\n    # Feature candidates: all columns except known non-features\n    feature_cols = [c for c in df.columns if c not in drop_cols]\n    # Coerce feature columns to numeric (safe): strings -> NaN\n    df_features = df[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n    # Replace purely-constant or all-NaN columns by dropping\n    valid_cols = [c for c in df_features.columns if not df_features[c].isna().all()]\n    X = df_features[valid_cols].copy()\n    # Note: risk_free_rate and others remain accessible in df if needed during scoring/analysis\n    return X, y.loc[X.index], valid_cols\n\n# ----------------------------\n# Simple preprocessing: compute medians for numeric imputation\n# ----------------------------\ndef compute_medians(X: pd.DataFrame) -> pd.Series:\n    # compute median per column (numeric)\n    medians = X.median(numeric_only=True)\n    return medians\n\n# ----------------------------\n# Model training (time-aware)\n# ----------------------------\ndef train_time_aware_model(X: pd.DataFrame, y: pd.Series, n_splits=4, random_state=42):\n    \"\"\"\n    Train HistGradientBoostingRegressor using time-series splits.\n    Returns fitted model on full data and out-of-fold metrics.\n    \"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    val_scores = []\n    # Use simple pipeline: scaler -> HGBRegressor (HGB handles NaNs but scaling helps)\n    model = HistGradientBoostingRegressor(random_state=random_state, max_iter=500, learning_rate=0.05)\n    # Track OOF predictions (optional)\n    oof_preds = np.zeros(len(X))\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        # Fill missing using medians computed on X_tr (avoid leakage)\n        med = X_tr.median(numeric_only=True)\n        X_tr_f = X_tr.fillna(med)\n        X_val_f = X_val.fillna(med)\n        # Standardize (fit on train)\n        scaler = StandardScaler()\n        X_tr_s = scaler.fit_transform(X_tr_f)\n        X_val_s = scaler.transform(X_val_f)\n        # Fit\n        model_fold = HistGradientBoostingRegressor(random_state=random_state, max_iter=500, learning_rate=0.05)\n        model_fold.fit(X_tr_s, y_tr)\n        y_val_pred = model_fold.predict(X_val_s)\n        oof_preds[val_idx] = y_val_pred\n        mse = mean_squared_error(y_val, y_val_pred)\n        val_scores.append(mse)\n        print(f\"[CV fold {fold}] val MSE = {mse:.6f}\")\n    # Fit final model on full dataset with median imputation and scaler\n    final_med = compute_medians(X)\n    X_full = X.fillna(final_med)\n    scaler_final = StandardScaler()\n    X_full_s = scaler_final.fit_transform(X_full)\n    final_model = HistGradientBoostingRegressor(random_state=random_state, max_iter=500, learning_rate=0.05)\n    final_model.fit(X_full_s, y)\n    # Save scaler/medians/model together\n    return final_model, scaler_final, final_med, oof_preds, np.mean(val_scores)\n\n# ----------------------------\n# Map predicted returns -> position in [0,2]\n# ----------------------------\ndef fit_return_to_position_map(preds_train: np.ndarray, low_q=2.0, high_q=98.0) -> Tuple[float, float]:\n    \"\"\"\n    Compute robust quantiles (low, high) used to linearly map predicted returns\n    to [0,2]. Values below low -> 0, above high -> 2.\n    \"\"\"\n    q_low = np.percentile(preds_train, low_q)\n    q_high = np.percentile(preds_train, high_q)\n    # Guard against equal quantiles (rare): expand slightly\n    if q_high <= q_low:\n        q_low = np.percentile(preds_train, 1.0)\n        q_high = np.percentile(preds_train, 99.0)\n        if q_high <= q_low:\n            q_low -= 1e-6\n            q_high += 1e-6\n    return float(q_low), float(q_high)\n\ndef predicted_return_to_position(pred: float, q_low: float, q_high: float) -> float:\n    \"\"\"\n    Convert a single predicted return into a position in [0,2] using linear scaling\n    between q_low and q_high, clipped to [0,2].\n    \"\"\"\n    pos = (pred - q_low) / (q_high - q_low) * 2.0\n    pos_clipped = float(np.clip(pos, 0.0, 2.0))\n    return pos_clipped\n\n# ----------------------------\n# Main training flow\n# ----------------------------\nprint(\"Loading training data...\")\nX_train, y_train, FEATURE_COLS = load_and_prepare_train(TRAIN_PATH)\nprint(f\"Train shape (features): {X_train.shape}, feature count: {len(FEATURE_COLS)}\")\n\nprint(\"Computing medians and training model (time-aware)...\")\nmodel, scaler_obj, medians, oof_preds, cv_mse = train_time_aware_model(X_train, y_train, n_splits=N_SPLITS, random_state=RANDOM_STATE)\nprint(f\"CV mean MSE: {cv_mse:.6f}\")\n\n# Save model + medians + scaler and compute quantiles for mapping\njoblib.dump(model, MODEL_FILE)\njoblib.dump(medians, MEDIANS_FILE)\njoblib.dump(scaler_obj, \"scaler.joblib\")\n\n# Convert training predictions -> compute mapping quantiles\nX_train_f = X_train.fillna(medians)\nX_train_s = scaler_obj.transform(X_train_f)\npreds_train = model.predict(X_train_s)\nq_low, q_high = fit_return_to_position_map(preds_train, low_q=3.0, high_q=97.0)\njoblib.dump((q_low, q_high), PRED_QUANT_FILE)\nprint(f\"Return->position mapping quantiles: q_low={q_low:.6e}, q_high={q_high:.6e}\")\n\n# ----------------------------\n# Inference: define predict(test: pl.DataFrame) -> float\n# ----------------------------\n# Load artifacts (ensures predict works even if notebook restarted)\n_model = joblib.load(MODEL_FILE)\n_medians = joblib.load(MEDIANS_FILE)\n_scaler = joblib.load(\"scaler.joblib\")\n_q_low, _q_high = joblib.load(PRED_QUANT_FILE)\n\ndef predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    This function will be called by the Kaggle evaluation server.\n    Input: a polars DataFrame with a single row (the test row).\n    Output: a float position in [0, 2].\n    \"\"\"\n    # Convert to pandas (safer to use pandas API here)\n    try:\n        row = test.to_pandas()\n    except Exception:\n        # If polars is already converted, ensure row is a pandas DataFrame\n        if isinstance(test, pd.DataFrame):\n            row = test.copy()\n        else:\n            # as a last resort, try to construct DataFrame\n            row = pd.DataFrame(test)\n    # Drop columns not used as features, if present\n    for c in [\"date_id\", \"is_scored\", \"lagged_forward_returns\", \"lagged_risk_free_rate\", \"lagged_market_forward_excess_returns\"]:\n        if c in row.columns:\n            row = row.drop(columns=[c])\n    # Ensure we have the feature columns expected (intersection)\n    available = [c for c in FEATURE_COLS if c in row.columns]\n    # If some feature cols are missing from row, add them with median values\n    missing = [c for c in FEATURE_COLS if c not in available]\n    for c in missing:\n        row[c] = _medians.get(c, 0.0)\n    # Select features in the same order as training\n    X_row = row[FEATURE_COLS].astype(float).fillna(_medians)\n    # Standardize\n    X_row_s = _scaler.transform(X_row)\n    # Predict return\n    pred_ret = _model.predict(X_row_s)[0]  # model returns array-like\n    # Map return -> position\n    pos = predicted_return_to_position(pred_ret, _q_low, _q_high)\n    # Debug print (safe for local runs)\n    print(f\"Predicted return: {pred_ret:.6e} -> Position: {pos:.6f}\")\n    return float(pos)\n\n# ----------------------------\n# Hook into evaluation server (Kaggle)\n# ----------------------------\nprint(\"Registering inference server with `predict` endpoint...\")\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    # In the real competition rerun path\n    inference_server.serve()\nelse:\n    # Local simulation (writes submission.parquet locally)\n    inference_server.run_local_gateway((\"/kaggle/input/hull-tactical-market-prediction/\",))\n\nprint(\"Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T20:30:04.821796Z","iopub.execute_input":"2025-09-29T20:30:04.822256Z","iopub.status.idle":"2025-09-29T20:30:29.241521Z","shell.execute_reply.started":"2025-09-29T20:30:04.822224Z","shell.execute_reply":"2025-09-29T20:30:29.240454Z"}},"outputs":[],"execution_count":null}]}