{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13411996,"sourceType":"datasetVersion","datasetId":8511937}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:52:01.118862Z","iopub.execute_input":"2025-10-28T05:52:01.119486Z","iopub.status.idle":"2025-10-28T05:52:01.132767Z","shell.execute_reply.started":"2025-10-28T05:52:01.119462Z","shell.execute_reply":"2025-10-28T05:52:01.132036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# üöÄ Hull Tactical Market Prediction - GPU Ensemble Enhanced\n# ============================================================\n\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import spearmanr\nimport optuna\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================\n# 1Ô∏è‚É£ Global Variables\n# ============================================================\nTARGET = 'market_forward_excess_returns'\nMODERN_ERA_START = 1000\nFEATURES_TO_ENGINEER = ['I1', 'P10', 'S1', 'V1']\nLAGS = [1, 3, 5]\nROLL_WINDOWS = [10, 20, 50]\n\n# ============================================================\n# 2Ô∏è‚É£ Feature Engineering\n# ============================================================\ndef preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.ffill().bfill()\n    for col in FEATURES_TO_ENGINEER:\n        for lag in LAGS:\n            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n        for window in ROLL_WINDOWS:\n            df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n            df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n    df = df.fillna(0)\n    return df\n\n# ============================================================\n# 3Ô∏è‚É£ Load & Prepare Data\n# ============================================================\nprint(\"üìÇ Loading training data...\")\ndf_train = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\")\ntrain_modern = df_train[df_train['date_id'] >= MODERN_ERA_START].copy()\ntrain_processed = preprocess(train_modern)\n\nfeatures_to_drop = ['forward_returns', 'risk_free_rate', 'date_id', TARGET]\nmodel_features = [col for col in train_processed.columns if col not in features_to_drop]\n\nX = train_processed[model_features]\ny = train_processed[TARGET]\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)\n\n# ============================================================\n# 4Ô∏è‚É£ Optuna Hyperparameter Tuning (LightGBM)\n# ============================================================\ndef objective_lgb(trial):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"device\": \"gpu\",\n        \"n_estimators\": 1000,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 127),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.9),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 0.3),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 0.3),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100)\n    }\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              callbacks=[lgb.early_stopping(stopping_rounds=100)])\n    preds = model.predict(X_valid)\n    corr, _ = spearmanr(y_valid, preds)\n    return -corr\n\nprint(\"üéØ Running Optuna tuning for LightGBM...\")\nstudy_lgb = optuna.create_study(direction=\"minimize\")\nstudy_lgb.optimize(objective_lgb, n_trials=25, show_progress_bar=True)\nbest_lgb_params = study_lgb.best_params\nprint(\"‚úÖ Best LightGBM params:\", best_lgb_params)\n\n# ============================================================\n# 5Ô∏è‚É£ Train Final LightGBM\n# ============================================================\nfinal_lgb = lgb.LGBMRegressor(**best_lgb_params, n_estimators=1500, device=\"gpu\")\nfinal_lgb.fit(X, y)\nprint(\"‚úÖ LightGBM trained.\")\n\n# ============================================================\n# 6Ô∏è‚É£ Train XGBoost (GPU)\n# ============================================================\nprint(\"‚ö° Training XGBoost on GPU...\")\nparams_xgb = {\n    \"tree_method\": \"gpu_hist\",\n    \"n_estimators\": 1200,\n    \"learning_rate\": 0.03,\n    \"max_depth\": 7,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"random_state\": 42\n}\nmodel_xgb = xgb.XGBRegressor(**params_xgb)\nmodel_xgb.fit(X, y, verbose=False)\nprint(\"‚úÖ XGBoost trained.\")\n\n# ============================================================\n# 7Ô∏è‚É£ Train CatBoost (GPU) with Fixed Bootstrap\n# ============================================================\nprint(\"ü¶ã Training CatBoost on GPU...\")\nbest_cat_params = {\n    \"iterations\": 1200,\n    \"learning_rate\": 0.03,\n    \"depth\": 7,\n    \"l2_leaf_reg\": 3,\n    \"bootstrap_type\": \"Poisson\",   # ‚úÖ Fixes subsample issue\n    \"task_type\": \"GPU\",\n    \"devices\": \"0:1\",\n    \"verbose\": 200\n}\nmodel_cat = CatBoostRegressor(**best_cat_params)\nmodel_cat.fit(X, y)\nprint(\"‚úÖ CatBoost trained.\")\n\n# ============================================================\n# 8Ô∏è‚É£ Ensemble Predictions\n# ============================================================\ndef ensemble_predict(df):\n    df_proc = preprocess(df)\n    X_test = df_proc[model_features].fillna(0)\n\n    preds_lgb = final_lgb.predict(X_test)\n    preds_xgb = model_xgb.predict(X_test)\n    preds_cat = model_cat.predict(X_test)\n\n    final_pred = (0.45 * preds_lgb) + (0.35 * preds_xgb) + (0.20 * preds_cat)\n    return final_pred\n\n# ============================================================\n# 9Ô∏è‚É£ Local Validation\n# ============================================================\nprint(\"üìà Evaluating ensemble locally...\")\nvalid_preds = ensemble_predict(X_valid)\ncorr, _ = spearmanr(y_valid, valid_preds)\nprint(f\"üìä Local validation Spearman correlation: {corr:.4f}\")\n\n# ============================================================\n# üîü Inference Server (for Kaggle)\n# ============================================================\nimport polars as pl\nimport kaggle_evaluation.default_inference_server as kes\n\ndef predict(test_df: pl.DataFrame) -> float:\n    try:\n        df = test_df.to_pandas()\n        preds = ensemble_predict(df)\n        allocation = (preds > 0).astype(int) * 2.0\n        return float(allocation[-1])\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Prediction error: {e}\")\n        return 0.0\n\nprint(\"üîß Starting inference server...\")\ninference_server = kes.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n\nprint(\"‚úÖ Final GPU Ensemble Submission Ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:52:03.864642Z","iopub.execute_input":"2025-10-28T05:52:03.864921Z","iopub.status.idle":"2025-10-28T05:55:30.198325Z","shell.execute_reply.started":"2025-10-28T05:52:03.864901Z","shell.execute_reply":"2025-10-28T05:55:30.197707Z"}},"outputs":[],"execution_count":null}]}