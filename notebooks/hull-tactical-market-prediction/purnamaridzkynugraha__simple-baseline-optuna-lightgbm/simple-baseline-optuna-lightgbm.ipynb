{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyNhuATthlT/ZXGv9jaabq/Z"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n\n# âœ… **Project Explanation (Baseline Model â€“ Early Stage)**\n\nThis model is only meant as an **initial baseline**, to make sure the pipeline works and to get a rough idea of performance. It is *not yet the final or optimized solution*. If this helps you, feel free to **upvote** :)\n\n---\n\n# **1. Importing Libraries & Loading Data**\n\n* The dataset is loaded from Kaggle (`train.csv` and `test.csv`) with `date_id` used as the index.\n* Unnecessary columns that do not exist in both train and test are removed to keep the same feature structure.\n* Missing values are checked to understand data quality.\n\n---\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\") \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:46.543338Z","iopub.execute_input":"2025-10-27T12:03:46.543635Z","iopub.status.idle":"2025-10-27T12:03:46.55221Z","shell.execute_reply.started":"2025-10-27T12:03:46.543608Z","shell.execute_reply":"2025-10-27T12:03:46.551008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv',index_col=\"date_id\")\ndf_test = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv',index_col=\"date_id\")","metadata":{"id":"pOMlA696jTyf","executionInfo":{"status":"ok","timestamp":1761550966850,"user_tz":-420,"elapsed":200,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:46.553729Z","iopub.execute_input":"2025-10-27T12:03:46.554022Z","iopub.status.idle":"2025-10-27T12:03:47.237928Z","shell.execute_reply.started":"2025-10-27T12:03:46.554002Z","shell.execute_reply":"2025-10-27T12:03:47.237075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **2. Handling Missing Values**\n\nBecause this is time-series data, we donâ€™t simply drop rows with missing values.\nInstead, we use a **three-step filling strategy**:\n\n1. `ffill` â€“ forward fill (use previous valid value)\n2. `bfill` â€“ backward fill (use next valid value)\n3. Remaining missing values â†’ filled with **median**\n\nThis keeps the time-series structure intact and avoids data leakage.\n\n---","metadata":{}},{"cell_type":"code","source":"df.isna().sum()[(df.nunique() > 1) & (df.isna().sum() > 0)]","metadata":{"id":"MeXTsfsgfPwl","executionInfo":{"status":"ok","timestamp":1761550886081,"user_tz":-420,"elapsed":272,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"outputId":"7792a74c-9273-4e88-9624-a293399b5caa","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:47.2387Z","iopub.execute_input":"2025-10-27T12:03:47.238969Z","iopub.status.idle":"2025-10-27T12:03:47.300506Z","shell.execute_reply.started":"2025-10-27T12:03:47.238947Z","shell.execute_reply":"2025-10-27T12:03:47.299673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_in_test = set(df_test.columns) - set(df.columns)\nprint(missing_in_test)\n","metadata":{"id":"9IkCGehBnXFy","executionInfo":{"status":"ok","timestamp":1761551016751,"user_tz":-420,"elapsed":387,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"outputId":"ad940d52-bd17-4af8-98d2-1cfe3eb47ee4","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:52.907841Z","iopub.execute_input":"2025-10-27T12:03:52.908251Z","iopub.status.idle":"2025-10-27T12:03:52.915166Z","shell.execute_reply.started":"2025-10-27T12:03:52.90821Z","shell.execute_reply":"2025-10-27T12:03:52.914136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop(columns=['market_forward_excess_returns', 'risk_free_rate'])\ndf_test = df_test.drop(columns=['lagged_forward_returns', 'is_scored', 'lagged_risk_free_rate', 'lagged_market_forward_excess_returns'])\n","metadata":{"id":"M7oUsheCnijR","executionInfo":{"status":"ok","timestamp":1761550970314,"user_tz":-420,"elapsed":10,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:52.916455Z","iopub.execute_input":"2025-10-27T12:03:52.916804Z","iopub.status.idle":"2025-10-27T12:03:52.941775Z","shell.execute_reply.started":"2025-10-27T12:03:52.916773Z","shell.execute_reply":"2025-10-27T12:03:52.940729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()[(df.nunique() > 1) & (df.isna().sum() > 0)]\n","metadata":{"id":"fz8E3rGynr4S","executionInfo":{"status":"ok","timestamp":1761551868825,"user_tz":-420,"elapsed":39,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"outputId":"db08b0ff-93b2-40f1-b60e-c973c95f8567","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:52.94284Z","iopub.execute_input":"2025-10-27T12:03:52.943191Z","iopub.status.idle":"2025-10-27T12:03:52.996461Z","shell.execute_reply.started":"2025-10-27T12:03:52.943169Z","shell.execute_reply":"2025-10-27T12:03:52.995468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_ffill = df.fillna(method='ffill')\ndf_bfill = df_ffill.fillna(method='bfill')\ndf_filled = df_bfill.fillna(df_bfill.median())\n","metadata":{"id":"SnUP7rXBo7HL","executionInfo":{"status":"ok","timestamp":1761551899496,"user_tz":-420,"elapsed":88,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"outputId":"422cbaeb-b878-41b8-8eb5-5bf019096ea8","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:52.997423Z","iopub.execute_input":"2025-10-27T12:03:52.997695Z","iopub.status.idle":"2025-10-27T12:03:53.069273Z","shell.execute_reply.started":"2025-10-27T12:03:52.997672Z","shell.execute_reply":"2025-10-27T12:03:53.06832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# **3. Splitting Features and Target**\n\n* The target variable we want to predict is: **`forward_returns`**\n* All other columns become features (`X`).\n\n```python\ntarget = \"forward_returns\"\ny = df_filled[target]\nX = df_filled.drop(columns=[target])\n```\n\n---\n# **4. Feature Scaling**\n\n* Features are standardized using **StandardScaler**.\n* The scaler is saved (`scaler.pkl`) so that the exact same scaling can be applied to the test set during prediction.\n* This is required because LightGBM + Optuna will use these scaled values.\n\n---","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\n\n# ================= 0. PREPROCESS =================\n# misal df_filled sudah siap\ntarget = \"forward_returns\"\ny = df_filled[target]\nX = df_filled.drop(columns=[target])\n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# simpan scaler\nimport joblib\njoblib.dump(scaler, \"scaler.pkl\")","metadata":{"id":"Yw0uwu2DnQxy","executionInfo":{"status":"ok","timestamp":1761552421703,"user_tz":-420,"elapsed":76,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:53.07031Z","iopub.execute_input":"2025-10-27T12:03:53.07058Z","iopub.status.idle":"2025-10-27T12:03:53.886574Z","shell.execute_reply.started":"2025-10-27T12:03:53.07056Z","shell.execute_reply":"2025-10-27T12:03:53.88557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **5. Hyperparameter Tuning with Optuna + TimeSeriesSplit**\n\nThis is the core idea of the baseline:\n\n* We use **Optuna** to automatically search for the best LightGBM hyperparameters.\n* **TimeSeriesSplit (5 folds)** is used for cross-validation to prevent data leakage in time-series.\n* Each trial trains a model on past data and validates on future data.\n* The average **RMSE (Root Mean Squared Error)** is returned as the objective metric.\n\nIf you find this useful â€” **upvote appreciated ğŸ™Œ**\n\n---\n\n# **6. Training the Final Model**\n\nAfter finding the best parameters, we:\n\n* Retrain LightGBM on **the entire dataset** using those best parameters.\n* This is still considered an **early-stage model**. Itâ€™s only for baseline purposes â€” not meant for final evaluation or competition submission.\n\n---","metadata":{}},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\nimport numpy as np\n\ntscv = TimeSeriesSplit(n_splits=5)\n# ================= 1. DEFINE OBJECTIVE =================\ndef objective(trial):\n\n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.6, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.6, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n    }\n\n    rmse_scores = []\n\n    # Cross-validation time series\n    for train_index, valid_index in tscv.split(X):\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n        train_set = lgb.Dataset(X_train, y_train)\n        valid_set = lgb.Dataset(X_valid, y_valid, reference=train_set)\n\n        model = lgb.train(\n            param,\n            train_set,\n            valid_sets=[valid_set],\n            num_boost_round=5000,\n            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],\n     \n        )\n\n        y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n        rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n        rmse_scores.append(rmse)\n\n    return np.mean(rmse_scores)  \n\n# ================= 2. RUN OPTUNA STUDY =================\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Best RMSE:\", study.best_value)\nprint(\"Best hyperparameters:\", study.best_params)\n\n# ================= 3. TRAIN FINAL MODEL DENGAN PARAM TERBAIK =================\nbest_params = study.best_params\nbest_params.update({\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"verbosity\": -1\n})\n\ntrain_set = lgb.Dataset(X, y)\nfinal_model = lgb.train(\n    best_params,\n    train_set,\n    num_boost_round=5000  \n)","metadata":{"id":"Jk9vRtQltB2y","executionInfo":{"status":"ok","timestamp":1761552795982,"user_tz":-420,"elapsed":244866,"user":{"displayName":"Purnama Ridzky Nugraha","userId":"11631294939034666647"}},"outputId":"b9f455d9-5ab3-44cc-f37d-1d456181168d","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:03:53.889716Z","iopub.execute_input":"2025-10-27T12:03:53.890146Z","iopub.status.idle":"2025-10-27T12:05:56.409676Z","shell.execute_reply.started":"2025-10-27T12:03:53.890113Z","shell.execute_reply":"2025-10-27T12:05:56.408885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **7. Prediction Function for the Test Set**\n\n* The `predict()` function is written as required by the competition API.\n* Steps inside the function:\n\n  1. Select the same feature columns.\n  2. Apply the saved scaler.\n  3. Use the trained LightGBM model to predict `forward_returns`.\n  4. Convert those continuous returns into **discrete trading signals (0, 1, 2)** using quantiles.\n\nSignal rules:\n\n| Signal | Meaning                                |\n| ------ | -------------------------------------- |\n| 0      | Return â‰¤ 0                             |\n| 1      | Return > 0 and â‰¤ 75th percentile (q75) |\n| 2      | Return > q75                           |\n\n---\n","metadata":{}},{"cell_type":"code","source":"def convert_ret_to_signal(ret_arr: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert an array of returns to discrete trading signals 0, 1, 2\n    \"\"\"\n    q75 = max(0, np.quantile(ret_arr, 0.75))\n\n    signal = np.zeros_like(ret_arr, dtype=int)\n\n    signal[(ret_arr > 0) & (ret_arr <= q75)] = 1\n\n    signal[ret_arr > q75] = 2\n\n    return signal\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:05:56.410598Z","iopub.execute_input":"2025-10-27T12:05:56.411168Z","iopub.status.idle":"2025-10-27T12:05:56.417255Z","shell.execute_reply.started":"2025-10-27T12:05:56.411137Z","shell.execute_reply":"2025-10-27T12:05:56.416165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nfeatures = list(X.columns)\n\ndef predict(test: pl.DataFrame) -> np.ndarray:\n    df_test = test.select(features)\n\n    X_test_np = df_test.to_numpy()\n\n    scaler = joblib.load(\"scaler.pkl\")  \n    X_test_scaled = scaler.transform(X_test_np)  \n    y_pred = final_model.predict(\n        X_test_scaled, \n        num_iteration=final_model.best_iteration\n    )\n    \n\n    signals = convert_ret_to_signal(y_pred)\n\n    return float(signals)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:09:27.845699Z","iopub.execute_input":"2025-10-27T12:09:27.846131Z","iopub.status.idle":"2025-10-27T12:09:27.853291Z","shell.execute_reply.started":"2025-10-27T12:09:27.846108Z","shell.execute_reply":"2025-10-27T12:09:27.852112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kaggle_evaluation.default_inference_server\nimport os\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:09:29.546848Z","iopub.execute_input":"2025-10-27T12:09:29.5478Z","iopub.status.idle":"2025-10-27T12:09:29.727655Z","shell.execute_reply.started":"2025-10-27T12:09:29.547769Z","shell.execute_reply":"2025-10-27T12:09:29.72682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### **Summary**\n\n> A minimal, functioning pipeline using LightGBM + Optuna for the Hull Tactical Market Prediction data.\n> It serves purely as a **foundation** to verify that preprocessing, tuning, and prediction work correctly before deeper modeling begins.\n>\n> If this baseline gives you a head start, an **upvote** would be appreciated ğŸ™\n","metadata":{}}]}