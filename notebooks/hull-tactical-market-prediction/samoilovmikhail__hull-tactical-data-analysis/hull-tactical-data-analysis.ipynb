{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Settings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom scipy.stats import jarque_bera, normaltest\nimport gc\nfrom IPython.display import display\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Visualisation settings\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\npd.options.display.float_format = '{:,.4f}'.format\nplt.rcParams['figure.figsize'] = (18, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.rcParams['axes.labelsize'] = 14\n\n# --- Function to reduce memory consumption ---\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print(f'Mem. usage decreased to {end_mem:5.2f} Mb ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n    return df\n\nprint(\"OK.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:47:46.765771Z","iopub.execute_input":"2025-09-29T14:47:46.766622Z","iopub.status.idle":"2025-09-29T14:47:46.779693Z","shell.execute_reply.started":"2025-09-29T14:47:46.766592Z","shell.execute_reply":"2025-09-29T14:47:46.778316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading and Initial Inspection of Data","metadata":{}},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/hull-tactical-market-prediction/train.csv'\n\ndf = pd.read_csv(TRAIN_PATH).set_index('date_id')\ndf = reduce_mem_usage(df)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Time range of data: from {df.index.min()} to {df.index.max()}\")\n\ndisplay(df.head())\ndisplay(df.tail())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:41:52.129072Z","iopub.execute_input":"2025-09-29T14:41:52.129437Z","iopub.status.idle":"2025-09-29T14:41:52.69226Z","shell.execute_reply.started":"2025-09-29T14:41:52.129416Z","shell.execute_reply":"2025-09-29T14:41:52.691423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:41:52.69313Z","iopub.execute_input":"2025-09-29T14:41:52.693454Z","iopub.status.idle":"2025-09-29T14:41:52.870062Z","shell.execute_reply.started":"2025-09-29T14:41:52.693427Z","shell.execute_reply":"2025-09-29T14:41:52.869212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysis of Missing Values","metadata":{}},{"cell_type":"code","source":"missing_ratio = df.isnull().sum() * 100 / df.shape[0]\n\nplt.figure(figsize=(10, 6))\nplt.hist(missing_ratio, bins=20, color='skyblue', edgecolor='steelblue', alpha=0.8)\n\nplt.title('Distribution of Missing Values Across Columns', fontsize=16, fontweight='bold', pad=15)\nplt.xlabel('Null Value Ratio, %', fontsize=12)\nplt.ylabel('Number of Columns', fontsize=12)\nplt.grid(axis='y', linestyle='--', alpha=0.7, linewidth=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:41:52.871031Z","iopub.execute_input":"2025-09-29T14:41:52.8719Z","iopub.status.idle":"2025-09-29T14:41:53.22068Z","shell.execute_reply.started":"2025-09-29T14:41:52.871867Z","shell.execute_reply":"2025-09-29T14:41:53.219925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n# Visualize gaps (yellow - there is data, purple - not)\n# Transpose so that the signs are on the Y axis and the time is on the X axis\nsns.heatmap(df.isnull().T, cmap='viridis', cbar=False)\nplt.title('Heat map of missed values by time')\nplt.xlabel('Date')\nplt.ylabel('Features')\nplt.show()\n\ncompleteness_by_date = df.notnull().mean(axis=1)\n\nplt.figure(figsize=(18, 6))\ncompleteness_by_date.plot()\nplt.title('Completeness of time data (percentage of available features)')\nplt.ylabel('Proportion of non-NaN features')\nplt.xlabel('Date')\nplt.axvline(pd.to_datetime('2000-01-01'), color='red', linestyle='--', label='2000 year')\nplt.axvline(pd.to_datetime('2008-01-01'), color='orange', linestyle='--', label='2008 year')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:41:53.22166Z","iopub.execute_input":"2025-09-29T14:41:53.222021Z","iopub.status.idle":"2025-09-29T14:41:56.20433Z","shell.execute_reply.started":"2025-09-29T14:41:53.221993Z","shell.execute_reply":"2025-09-29T14:41:56.203174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Target Variable Analysis\nOur goal is to predict excess returns. Let's create this variable and examine it.","metadata":{"execution":{"iopub.status.busy":"2025-09-24T21:36:59.770468Z","iopub.execute_input":"2025-09-24T21:36:59.771744Z","iopub.status.idle":"2025-09-24T21:36:59.778585Z","shell.execute_reply.started":"2025-09-24T21:36:59.771705Z","shell.execute_reply":"2025-09-24T21:36:59.777015Z"}}},{"cell_type":"code","source":"df['excess_return'] = df['forward_returns'] - df['risk_free_rate']\n\nTARGET_NAME = 'excess_return'\n\nplt.figure(figsize=(18, 6))\ndf[[TARGET_NAME, 'market_forward_excess_returns']].plot(alpha=0.7)\nplt.title('Comparison of \"Clean\" vs. Processed Excess Returns')\nplt.legend(['Our Calculated (forward_returns - risk_free_rate)', 'Provided (processed)'])\nplt.show()\nprint(\"The plots are very similar. The provided variable is likely a winsorized version of our calculated one.\")\nprint(\"For our analysis, we'll use our 'clean' version as it's more fundamental.\")\n\n# 1. Target Distribution\nplt.figure(figsize=(18, 6))\nsns.histplot(df[TARGET_NAME].dropna(), kde=True, bins=100)\nplt.title(f'Distribution of {TARGET_NAME}')\nplt.xlabel('Excess Return Value')\n\n# Normality Test\nstat, p_value = jarque_bera(df[TARGET_NAME].dropna())\nprint(f\"Jarque-Bera Test: Statistic={stat:.2f}, p-value={p_value:.3f}\")\nif p_value < 0.05:\n    print(\"The null hypothesis of normality is rejected. The distribution has 'heavy tails', which is typical for financial markets.\")\nplt.show()\n\n# 2. Behavior Over Time\nplt.figure(figsize=(18, 6))\ndf[TARGET_NAME].plot(alpha=0.8, style='-', lw=0.5)\nplt.title(f'Daily {TARGET_NAME}')\nplt.ylabel('Return')\nplt.show()\nprint(\"Volatility clustering is evident: periods of high and low variance (e.g., crises of 2000, 2008, 2020).\")\n\n# 3. Cumulative Return (Buy-and-Hold Strategy)\nplt.figure(figsize=(18, 6))\ndf[TARGET_NAME].cumsum().plot()\nplt.title('Cumulative Excess Return (Buy-and-Hold S&P500)')\nplt.ylabel('Total Return')\nplt.show()\n\n# 4. Autocorrelation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\nplot_acf(df[TARGET_NAME].dropna(), lags=40, ax=ax1, title='Autocorrelation (ACF)')\nplot_pacf(df[TARGET_NAME].dropna(), lags=40, ax=ax2, title='Partial Autocorrelation (PACF)')\nplt.show()\nprint(\"Autocorrelation is very weak and close to zero. This supports the efficient-market hypothesis at short lags.\")\nprint(\"This means that simply using yesterday's return to predict today's is a poor strategy.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:41:56.205326Z","iopub.execute_input":"2025-09-29T14:41:56.205567Z","iopub.status.idle":"2025-09-29T14:41:58.222059Z","shell.execute_reply.started":"2025-09-29T14:41:56.20554Z","shell.execute_reply":"2025-09-29T14:41:58.221077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Analysis by Group","metadata":{}},{"cell_type":"markdown","source":"## Time Series","metadata":{}},{"cell_type":"code","source":"completeness_by_date = df.notnull().mean(axis=1)\ntry:\n    start_id = (completeness_by_date >= 0.95).idxmax()\n    print(f\"Dynamically determined start_id: {start_id} (the first day with >= 95% data completeness)\")\nexcept ValueError:\n    start_id = df.index[int(len(df) * 0.5)]\n    print(f\"Warning: Could not find a 95% completeness point. Falling back to a default start_id: {start_id}\")\n    \ndf_filtered = df.loc[df.index >= start_id].copy()\nprint(f\"Filtered dataset shape: {df_filtered.shape}\")\n\nfeature_cols = [col for col in df.columns if '*' not in col and col not in ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'excess_return']]\n\ngroups = {\n    'M': [f for f in feature_cols if f.startswith('M')],\n    'E': [f for f in feature_cols if f.startswith('E')],  \n    'I': [f for f in feature_cols if f.startswith('I')],  \n    'P': [f for f in feature_cols if f.startswith('P')],\n    'V': [f for f in feature_cols if f.startswith('V')],   \n    'S': [f for f in feature_cols if f.startswith('S')],  \n    'MOM': [f for f in feature_cols if f.startswith('MOM')],\n    'D': [f for f in feature_cols if f.startswith('D')]   \n}\n\ngroups_filtered = {key: [col for col in item if col in df.columns] for key, item in groups.items()}\n\nn_groups = len(groups_filtered)\ncols = 3\nrows = math.ceil(n_groups / cols)\n\nfig, axes = plt.subplots(rows, cols, figsize=(16, 5 * rows), sharex=True)\naxes = axes.flatten() if n_groups > 1 else [axes]\n\nfor idx, (key, columns) in enumerate(groups_filtered.items()):\n    ax = axes[idx]\n    colors = sns.color_palette(\"husl\", len(columns))\n    \n    for i, col in enumerate(columns):\n        ax.plot(df.index, df[col], label=col, color=colors[i], linewidth=1.2)\n\n    ax.set_title(f'Group \"{key}\"', fontsize=14, fontweight='bold', pad=15)\n    ax.legend(loc='upper left', fontsize=9, frameon=True, fancybox=True, shadow=False)\n    ax.grid(True, alpha=0.5)\n    ax.set_ylabel(\"Values\", fontsize=10)\n\nfor idx in range(n_groups, len(axes)):\n    fig.delaxes(axes[idx])\n\nplt.suptitle(\"Time series by feature groups\", fontsize=18, fontweight='bold', y=0.98)\nplt.xlabel(\"Time\", fontsize=12)\nplt.tight_layout(rect=[0, 0, 1, 0.97])\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:49:05.892167Z","iopub.execute_input":"2025-09-29T14:49:05.892467Z","iopub.status.idle":"2025-09-29T14:49:09.218129Z","shell.execute_reply.started":"2025-09-29T14:49:05.892446Z","shell.execute_reply":"2025-09-29T14:49:09.217294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"code","source":"completeness_by_date = df.notnull().mean(axis=1)\ntry:\n    start_id = (completeness_by_date >= 0.95).idxmax()\n    print(f\"Dynamically determined start_id: {start_id} (the first day with >= 95% data completeness)\")\nexcept ValueError:\n    start_id = df.index[int(len(df) * 0.5)]\n    print(f\"Warning: Could not find a 95% completeness point. Falling back to a default start_id: {start_id}\")\n    \ndf_filtered = df.loc[df.index >= start_id].copy()\nprint(f\"Filtered dataset shape: {df_filtered.shape}\")\n\nfeature_cols = [col for col in df.columns if '*' not in col and col not in ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'excess_return']]\n\ngroups = {\n    'M': [f for f in feature_cols if f.startswith('M')],\n    'E': [f for f in feature_cols if f.startswith('E')],  \n    'I': [f for f in feature_cols if f.startswith('I')],  \n    'P': [f for f in feature_cols if f.startswith('P')],\n    'V': [f for f in feature_cols if f.startswith('V')],   \n    'S': [f for f in feature_cols if f.startswith('S')],  \n    'MOM': [f for f in feature_cols if f.startswith('MOM')],\n    'D': [f for f in feature_cols if f.startswith('D')]   \n}\n\nif TARGET_NAME in df_filtered.columns:\n    correlations = df_filtered[feature_cols + [TARGET_NAME]].corr(method='pearson')[TARGET_NAME].drop(TARGET_NAME)\n\n    print(\"\\nAnalyzing feature correlations with the target variable (excess_return):\")\n\n    for prefix, features in groups.items():\n        if not features: continue\n\n        group_corr_series = correlations.reindex(features).dropna()\n\n        if group_corr_series.empty:\n            print(f\"No correlations to plot for group '{prefix}'.\")\n            continue\n\n        plt.figure(figsize=(10, len(group_corr_series) * 0.3 + 2))\n        group_corr_series = group_corr_series.sort_values()\n\n        sns.barplot(x=group_corr_series.values, y=group_corr_series.index, orient='h')\n        plt.title(f'Correlation of \"{prefix}\" Group Features with {TARGET_NAME}')\n        plt.xlabel('Pearson Correlation Coefficient')\n        plt.show()\nelse:\n    print(f\"Target '{TARGET_NAME}' not found in df_filtered. Skipping correlation plots.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:41:58.22423Z","iopub.execute_input":"2025-09-29T14:41:58.224672Z","iopub.status.idle":"2025-09-29T14:41:59.99049Z","shell.execute_reply.started":"2025-09-29T14:41:58.224646Z","shell.execute_reply":"2025-09-29T14:41:59.989624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Correlation Heatmap","metadata":{}},{"cell_type":"code","source":"df_imputed = df.fillna(method='ffill').fillna(method='bfill')\n\nfeature_categories = ['D', 'E', 'I', 'M', 'P', 'S', 'V']\nfeature_groups = {cat: [col for col in df_imputed.columns if col.startswith(cat)] for cat in feature_categories}\n\nfor category, features in feature_groups.items():\n    if len(features) < 2:\n        print(f\"\\nGroup '{category}' has fewer than 2 features, skipping the heatmap.\")\n        continue\n\n    print(f\"\\nGenerating heatmap for group '{category}'...\")\n\n    corr_matrix = df_imputed[features].corr()\n\n    show_annotations = len(features) < 15\n\n    plt.figure(figsize=(12, 9))\n    sns.heatmap(\n        corr_matrix,\n        annot=show_annotations,\n        cmap='coolwarm',       \n        fmt='.2f',             \n        linewidths=.5\n    )\n    plt.title(f'Correlation Heatmap for {category} Group Features', fontsize=16)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:41:59.991368Z","iopub.execute_input":"2025-09-29T14:41:59.991688Z","iopub.status.idle":"2025-09-29T14:42:03.537431Z","shell.execute_reply.started":"2025-09-29T14:41:59.99166Z","shell.execute_reply":"2025-09-29T14:42:03.536513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Some Verdicts","metadata":{}},{"cell_type":"code","source":"print(f\"Dataset consist of {len(list(df))} columns: {list(df)}\")\nprint()\nprint(f\"Train df contains {df.shape[0]} lines and {df.shape[1]} columns\")\nprint()\nprint(f\"Top 5 signs by the number of passes:\\n{df.isnull().mean().sort_values(ascending=False).head()}\")\nprint()\ndf['forward_returns'].plot(title='Visualization of target value')\nprint(f\"Top 20 features by correlation: \\n{df.corrwith(df['market_forward_excess_returns']).abs().sort_values(ascending=False).head(20)}\")\n\nforward_returns_data = df['forward_returns'].dropna()\nstat, p_value = normaltest(forward_returns_data)\nprint(f\"Normality test (D'Agostino): p-value = {p_value:.4f}\")\nif p_value > 0.05:\n    print(\"The data is normally distributed.\")\nelse:\n    print(\"The data is not normally distributed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:42:03.538397Z","iopub.execute_input":"2025-09-29T14:42:03.538649Z","iopub.status.idle":"2025-09-29T14:42:04.140429Z","shell.execute_reply.started":"2025-09-29T14:42:03.53863Z","shell.execute_reply":"2025-09-29T14:42:04.139643Z"}},"outputs":[],"execution_count":null}]}