{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Data Preparation and Feature Engineering","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Imports and Config","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom collections import deque\nimport math\nimport warnings\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport kaggle_evaluation.default_inference_server\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:22:23.992629Z","iopub.execute_input":"2025-09-30T16:22:23.99334Z","iopub.status.idle":"2025-09-30T16:22:23.998004Z","shell.execute_reply.started":"2025-09-30T16:22:23.993311Z","shell.execute_reply":"2025-09-30T16:22:23.997324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    # Paths\n    MODEL_PATH = 'best_transformer_model.pth'\n    SCALER_PATH = 'scaler.joblib'\n    BEST_K_PATH = 'best_k.joblib'\n\n    # Features\n    BASE_FEATURE_PREFIXES = ('D', 'E', 'I', 'M', 'P', 'S', 'V')\n    KEY_FEATURES = ['M1', 'E1', 'I1', 'P1', 'S1', 'V1', 'M2', 'E2', 'I2', 'P2', 'S2', 'V2']\n    ROLLING_WINDOWS = [5, 21, 63]\n    LAG_PERIODS = [1, 5, 21]\n\n    # Model & Train\n    SEQUENCE_LENGTH = 60\n    TRAIN_SPLIT_RATIO = 0.85\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Transformer params\n    D_MODEL = 128\n    N_HEADS = 8\n    N_LAYERS = 4\n    DROPOUT = 0.2\n    \n    # Train params\n    N_EPOCHS = 20\n    BATCH_SIZE = 128\n    LEARNING_RATE = 1e-4\n    WEIGHT_DECAY = 1e-5\n    \n    # Post-processing\n    K_VALUES_SEARCH = [10, 50, 100, 200, 400, 600, 800]\n    \n    HISTORY_LEN_FOR_FEATURES = max(ROLLING_WINDOWS + LAG_PERIODS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:23:39.053341Z","iopub.execute_input":"2025-09-30T16:23:39.053591Z","iopub.status.idle":"2025-09-30T16:23:39.059217Z","shell.execute_reply.started":"2025-09-30T16:23:39.053573Z","shell.execute_reply":"2025-09-30T16:23:39.058476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Competition metric\nJust full copy from https://www.kaggle.com/code/metric/hull-competition-sharpe","metadata":{}},{"cell_type":"code","source":"MIN_INVESTMENT = 0\nMAX_INVESTMENT = 2\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef to_allocation(predictions, k):\n    \"\"\"Converts model predictions into allocations.\"\"\"\n    allocations = 2 * (1 / (1 + np.exp(-np.array(predictions) * k)))\n    return np.clip(allocations, 0, 2)\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n    \"\"\"A local copy of the competition metric.\"\"\"\n    if not pd.api.types.is_numeric_dtype(submission['prediction']):\n        raise ParticipantVisibleError('Predictions must be numeric')\n\n    solution = solution.copy()\n    solution['position'] = submission['prediction'].values\n\n    if solution['position'].max() > MAX_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n    if solution['position'].min() < MIN_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n\n    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n\n    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n    \n    if len(solution) == 0: return 0.0\n    \n    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n    strategy_std = solution['strategy_returns'].std()\n\n    trading_days_per_yr = 252\n    if strategy_std == 0: return 0.0\n    \n    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n\n    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n    market_std = solution['forward_returns'].std()\n    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n\n    if market_volatility == 0: return 0.0\n\n    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n    vol_penalty = 1 + excess_vol\n    return_gap = max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n    return_penalty = 1 + (return_gap**2) / 100\n\n    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n    return min(float(adjusted_sharpe), 1_000_000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:24:02.509123Z","iopub.execute_input":"2025-09-30T16:24:02.509422Z","iopub.status.idle":"2025-09-30T16:24:02.518513Z","shell.execute_reply.started":"2025-09-30T16:24:02.509401Z","shell.execute_reply":"2025-09-30T16:24:02.51775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Help-Class for model","metadata":{}},{"cell_type":"code","source":"class TimeSeriesDataset(Dataset):\n    def __init__(self, X, y, seq_len):\n        self.X = X\n        self.y = y\n        self.seq_len = seq_len\n    def __len__(self):\n        return len(self.X) - self.seq_len + 1\n    def __getitem__(self, idx):\n        sequence = self.X[idx : idx + self.seq_len]\n        target = self.y[idx + self.seq_len - 1]\n        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    def forward(self, x):\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim, d_model, n_heads, n_layers, dropout, sequence_length):\n        super().__init__()\n        self.d_model = d_model\n        self.input_embedding = nn.Linear(input_dim, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=sequence_length)\n        encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, d_model * 4, dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n        self.reg_head = nn.Sequential(nn.Linear(d_model, d_model // 2), nn.ReLU(), nn.Linear(d_model // 2, 1))\n\n    def forward(self, src):\n        src = self.input_embedding(src) * math.sqrt(self.d_model)\n        src = src.permute(1, 0, 2)\n        src = self.pos_encoder(src)\n        src = src.permute(1, 0, 2)\n        output = self.transformer_encoder(src)\n        return self.reg_head(output[:, -1, :]).squeeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:24:53.407412Z","iopub.execute_input":"2025-09-30T16:24:53.408004Z","iopub.status.idle":"2025-09-30T16:24:53.417243Z","shell.execute_reply.started":"2025-09-30T16:24:53.407979Z","shell.execute_reply":"2025-09-30T16:24:53.416407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Advances & Optimized feature engineering","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self, config: Config):\n        self.config = config\n        self.feature_cols = None\n        self.generated_feature_names = None\n\n    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Creates signs and remembers their names.\"\"\"\n        df_featured = self._generate(df)\n        self.feature_cols = [col for col in df.columns if col.startswith(self.config.BASE_FEATURE_PREFIXES)]\n        self.generated_feature_names = [c for c in df_featured.columns if c not in df.columns]\n        return df_featured\n\n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Applies feature generation to new data.\"\"\"\n        return self._generate(df)\n\n    def _generate(self, df_input: pd.DataFrame) -> pd.DataFrame:\n        df = df_input.copy()\n        \n        feature_cols = [col for col in df.columns if col.startswith(self.config.BASE_FEATURE_PREFIXES)]\n        key_features = [f for f in self.config.KEY_FEATURES if f in df.columns]\n\n        generated_features = {}\n        for col in key_features:\n            for lag in self.config.LAG_PERIODS:\n                generated_features[f'{col}_lag_{lag}'] = df[col].shift(lag)\n                generated_features[f'{col}_diff_{lag}'] = df[col].diff(lag)\n            for window in self.config.ROLLING_WINDOWS:\n                rolling = df[col].rolling(window=window, min_periods=max(1, window // 2))\n                generated_features[f'{col}_roll_mean_{window}'] = rolling.mean()\n                generated_features[f'{col}_roll_std_{window}'] = rolling.std()\n\n        feature_groups = {p: [c for c in feature_cols if c.startswith(p)] for p in set(c[0] for c in feature_cols)}\n        for group_prefix, group_cols in feature_groups.items():\n            if group_cols:\n                generated_features[f'group_{group_prefix}_mean'] = df[group_cols].mean(axis=1)\n                generated_features[f'group_{group_prefix}_std'] = df[group_cols].std(axis=1)\n        \n        generated_df = pd.DataFrame(generated_features)\n        return pd.concat([df, generated_df], axis=1)\n\n    def get_feature_names(self):\n        if self.feature_cols is None or self.generated_feature_names is None:\n            raise RuntimeError(\"FeatureEngineer must be fit first.\")\n        return self.feature_cols + self.generated_feature_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:25:08.154659Z","iopub.execute_input":"2025-09-30T16:25:08.154928Z","iopub.status.idle":"2025-09-30T16:25:08.164331Z","shell.execute_reply.started":"2025-09-30T16:25:08.154906Z","shell.execute_reply":"2025-09-30T16:25:08.163652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Main megaclass","metadata":{"execution":{"iopub.status.busy":"2025-09-28T22:47:31.915587Z","iopub.execute_input":"2025-09-28T22:47:31.915998Z","iopub.status.idle":"2025-09-28T22:47:31.923433Z","shell.execute_reply.started":"2025-09-28T22:47:31.915927Z","shell.execute_reply":"2025-09-28T22:47:31.922022Z"}}},{"cell_type":"code","source":"class MarketPredictor:\n    def __init__(self, config: Config):\n        self.config = config\n        self.feature_engineer = FeatureEngineer(config)\n        self.model = None\n        self.scaler = None\n        self.best_k = None\n        self.feature_names = None\n        \n        self.inference_history = pd.DataFrame()\n\n    def train(self, df_raw: pd.DataFrame):\n        \"\"\"The full training cycle of the model.\"\"\"\n        print(\"Starting training process...\")\n\n        df_raw[df_raw.select_dtypes(include=np.number).columns] = df_raw.select_dtypes(include=np.number).ffill().bfill()\n        df_featured = self.feature_engineer.fit_transform(df_raw)\n        self.feature_names = self.feature_engineer.get_feature_names()\n        \n        df_featured = df_featured.dropna(subset=self.feature_names).reset_index(drop=True)\n        print(f\"Total features: {len(self.feature_names)}\")\n        print(f\"Dataset size after cleaning: {df_featured.shape}\")\n\n        TARGET = 'forward_returns'\n        X = df_featured[self.feature_names]\n        y = df_featured[TARGET]\n\n        train_split_idx = int(len(df_featured) * self.config.TRAIN_SPLIT_RATIO)\n        val_df_for_metric = df_featured.iloc[train_split_idx:].reset_index(drop=True) # для скоринга\n\n        X_train, X_val = X.iloc[:train_split_idx], X.iloc[train_split_idx:]\n        y_train, y_val = y.iloc[:train_split_idx], y.iloc[train_split_idx:]\n\n        self.scaler = StandardScaler()\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        X_val_scaled = self.scaler.transform(X_val)\n\n        train_dataset = TimeSeriesDataset(X_train_scaled, y_train.values, self.config.SEQUENCE_LENGTH)\n        val_dataset = TimeSeriesDataset(X_val_scaled, y_val.values, self.config.SEQUENCE_LENGTH)\n        train_loader = DataLoader(train_dataset, batch_size=self.config.BATCH_SIZE, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=2)\n\n        self.model = TransformerModel(\n            input_dim=len(self.feature_names),\n            d_model=self.config.D_MODEL,\n            n_heads=self.config.N_HEADS,\n            n_layers=self.config.N_LAYERS,\n            dropout=self.config.DROPOUT,\n            sequence_length=self.config.SEQUENCE_LENGTH\n        ).to(self.config.DEVICE)\n        print(f\"Model created and moved to {self.config.DEVICE}\")\n\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config.LEARNING_RATE, weight_decay=self.config.WEIGHT_DECAY)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)\n\n        best_epoch_score = -np.inf\n\n        for epoch in range(self.config.N_EPOCHS):\n            self.model.train()\n            total_train_loss = 0\n            for seq, target in train_loader:\n                seq, target = seq.to(self.config.DEVICE), target.to(self.config.DEVICE)\n                optimizer.zero_grad()\n                prediction = self.model(seq)\n                loss = criterion(prediction, target)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.7)\n                optimizer.step()\n                total_train_loss += loss.item()\n            avg_train_loss = total_train_loss / len(train_loader)\n\n            current_best_score, current_best_k = self._validate(val_loader, val_df_for_metric)\n            scheduler.step(current_best_score)\n            \n            print(f\"Epoch {epoch+1}/{self.config.N_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Score: {current_best_score:.4f} at k={current_best_k}\")\n\n            if current_best_score > best_epoch_score:\n                best_epoch_score = current_best_score\n                self.best_k = current_best_k\n                print(f\"** New best score! Saving artifacts... Best k is {self.best_k} **\")\n                self._save_artifacts()\n\n        print(f\"\\nTraining complete. Best validation score: {best_epoch_score:.4f} with k={self.best_k}\")\n\n    def _validate(self, val_loader, val_df_for_metric):\n        \"\"\"Performs validation on a deferred sample and finds the best `k`.\"\"\"\n        self.model.eval()\n        val_preds = []\n        with torch.no_grad():\n            for seq, _ in val_loader:\n                seq = seq.to(self.config.DEVICE)\n                prediction = self.model(seq)\n                val_preds.extend(prediction.cpu().numpy())\n        \n        actuals_for_metric = val_df_for_metric.iloc[self.config.SEQUENCE_LENGTH - 1:].copy()\n        \n        current_best_score = -np.inf\n        current_best_k = None\n        \n        for k in self.config.K_VALUES_SEARCH: \n            allocations = to_allocation(val_preds, k=k)\n            submission_df = pd.DataFrame({'prediction': allocations[:len(actuals_for_metric)]})\n\n            current_score = score(actuals_for_metric, submission_df)\n            \n            if current_score > current_best_score:\n                current_best_score = current_score\n                current_best_k = k\n        \n        return current_best_score, current_best_k\n    \n    def _save_artifacts(self):\n        \"\"\"Saves the model, the scaler, and the best k.\"\"\"\n        torch.save(self.model.state_dict(), self.config.MODEL_PATH)\n        joblib.dump(self.scaler, self.config.SCALER_PATH)\n        joblib.dump(self.best_k, self.config.BEST_K_PATH)\n        joblib.dump(self.feature_names, 'feature_names.joblib')\n\n    def load_artifacts(self):\n        \"\"\"Uploads artifacts for the inference.\"\"\"\n        print(\"Loading artifacts for inference...\")\n        self.feature_names = joblib.load('feature_names.joblib')\n        self.model = TransformerModel(\n            input_dim=len(self.feature_names),\n            d_model=self.config.D_MODEL,\n            n_heads=self.config.N_HEADS,\n            n_layers=self.config.N_LAYERS,\n            dropout=self.config.DROPOUT,\n            sequence_length=self.config.SEQUENCE_LENGTH\n        )\n        self.model.load_state_dict(torch.load(self.config.MODEL_PATH, map_location=self.config.DEVICE))\n        self.model.to(self.config.DEVICE)\n        self.model.eval()\n        \n        self.scaler = joblib.load(self.config.SCALER_PATH)\n        self.best_k = joblib.load(self.config.BEST_K_PATH)\n        print(f\"Artifacts loaded. Model is on {self.config.DEVICE}. Best K is {self.best_k}.\")\n\n    def predict(self, test_df_polars: pl.DataFrame) -> pl.DataFrame:\n        \"\"\"\n        Makes a prediction for a new batch of data.\n        Manages the history for the correct calculation of features and sequences.\n        \"\"\"\n        test_df = test_df_polars.to_pandas()\n        \n        if self.inference_history.empty:\n            self.inference_history = test_df\n            return pl.DataFrame({'prediction': [1.0] * len(test_df)})\n\n        combined_df = pd.concat([self.inference_history, test_df], ignore_index=True)\n        \n        featured_df = self.feature_engineer.transform(combined_df)\n        \n        if len(featured_df) < self.config.SEQUENCE_LENGTH:\n             self.inference_history = combined_df\n             return pl.DataFrame({'prediction': [1.0] * len(test_df)})\n        \n        sequence_data = featured_df[self.feature_names].tail(self.config.SEQUENCE_LENGTH)\n        sequence_data = sequence_data.ffill().bfill()\n        scaled_sequence = self.scaler.transform(sequence_data)\n        \n        with torch.no_grad():\n            tensor_sequence = torch.tensor(scaled_sequence, dtype=torch.float32).unsqueeze(0).to(self.config.DEVICE)\n            raw_prediction = self.model(tensor_sequence).item()\n            \n        allocation = to_allocation([raw_prediction], k=self.best_k)[0]\n\n        self.inference_history = combined_df.tail(self.config.HISTORY_LEN_FOR_FEATURES)\n        \n        return pl.DataFrame({'prediction': [allocation]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:25:38.640791Z","iopub.execute_input":"2025-09-30T16:25:38.641331Z","iopub.status.idle":"2025-09-30T16:25:38.661305Z","shell.execute_reply.started":"2025-09-30T16:25:38.641308Z","shell.execute_reply":"2025-09-30T16:25:38.660706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Main (Train & Submission using classes)","metadata":{"execution":{"iopub.status.busy":"2025-09-28T22:48:06.841177Z","iopub.execute_input":"2025-09-28T22:48:06.841705Z","iopub.status.idle":"2025-09-28T22:48:06.849156Z","shell.execute_reply.started":"2025-09-28T22:48:06.84165Z","shell.execute_reply":"2025-09-28T22:48:06.84782Z"}}},{"cell_type":"code","source":"config = Config()\n\nprint(\"--- Start Model Training ---\")\npredictor = MarketPredictor(config)\ndf_train_raw = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\ndf_train_raw = df_train_raw.sort_values('date_id').reset_index(drop=True)\npredictor.train(df_train_raw)\nprint(\"--- Training finished. Artifacts saved. ---\")\n\npredictor = MarketPredictor(config)\n\n\npredictor.load_artifacts() \n\ndef predict(test_df: pl.DataFrame) -> pl.DataFrame:\n    return predictor.predict(test_df)\n\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:29:37.96962Z","iopub.execute_input":"2025-09-30T16:29:37.970353Z","iopub.status.idle":"2025-09-30T16:29:38.306843Z","shell.execute_reply.started":"2025-09-30T16:29:37.970326Z","shell.execute_reply":"2025-09-30T16:29:38.306105Z"}},"outputs":[],"execution_count":null}]}