{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nS&P 500 Market Timing Competition - Kaggle Notebook\nOptimized for Kaggle environment with proper API integration\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Kaggle-specific imports\nimport os\nimport gc\nfrom pathlib import Path\n\nclass KaggleSP500Pipeline:\n    def __init__(self):\n        # Kaggle paths\n        self.input_path = Path('/kaggle/input/hull-tactical-market-prediction')\n        self.working_path = Path('/kaggle/working')\n        \n        # Model components\n        self.scaler = RobustScaler()  # More robust to outliers\n        self.models = {}\n        self.feature_columns = []\n        self.target_column = 'market_forward_excess_returns'\n        \n        # Model weights (will be optimized during validation)\n        self.model_weights = {}\n        \n    def load_data(self):\n        \"\"\"Load data from multiple possible locations\"\"\"\n        print(\"Loading data...\")\n        \n        # Possible data locations (ordered by preference)\n        possible_paths = [\n            # Current directory (for uploaded files)\n            Path('.'),\n            # Kaggle competition paths  \n            Path('/kaggle/input/hull-tactical-market-prediction'),\n            Path('/kaggle/input/hull-tactical-market-prediction'),\n            # Local paths\n            Path('./input'),\n            Path('./data'), \n            # Current working directory\n            Path(os.getcwd())\n        ]\n        \n        self.train_df = None\n        self.test_df = None\n        \n        # Try to find the data files\n        for base_path in possible_paths:\n            print(f\"Checking path: {base_path}\")\n            \n            # Try different file patterns\n            train_files = [\n                base_path / 'train.csv',\n                base_path / 'sp-500-market-timing' / 'train.csv'\n            ]\n            \n            test_files = [\n                base_path / 'test.csv', \n                base_path / 'sp-500-market-timing' / 'test.csv'\n            ]\n            \n            # Look for train.csv\n            for train_path in train_files:\n                if train_path.exists():\n                    self.train_df = pd.read_csv(train_path)\n                    print(f\"‚úì Found training data at: {train_path}\")\n                    print(f\"  Shape: {self.train_df.shape}\")\n                    break\n            \n            # Look for test.csv\n            for test_path in test_files:\n                if test_path.exists():\n                    self.test_df = pd.read_csv(test_path)\n                    print(f\"‚úì Found test data at: {test_path}\")\n                    print(f\"  Shape: {self.test_df.shape}\")\n                    break\n            \n            # If both found, stop searching\n            if self.train_df is not None and self.test_df is not None:\n                break\n        \n        # Check if files were found\n        if self.train_df is None:\n            print(\"‚ùå Training data not found in any location!\")\n            # List available files for debugging\n            print(\"\\nAvailable files in current directory:\")\n            for item in Path('.').iterdir():\n                print(f\"  {item}\")\n            raise FileNotFoundError(\"Could not find train.csv\")\n            \n        if self.test_df is None:\n            print(\"‚ùå Test data not found in any location!\")\n            raise FileNotFoundError(\"Could not find test.csv\")\n            \n        return self.train_df, self.test_df\n    \n    def quick_eda(self):\n        \"\"\"Quick exploratory data analysis for Kaggle\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"QUICK EDA\")\n        print(\"=\"*50)\n        \n        # Basic info\n        print(f\"Training period: {self.train_df['date_id'].min()} to {self.train_df['date_id'].max()}\")\n        print(f\"Test period: {self.test_df['date_id'].min()} to {self.test_df['date_id'].max()}\")\n        \n        # Target statistics (recent data only to avoid early missing values)\n        recent_data = self.train_df.iloc[-1000:]  # Last 1000 rows\n        target_stats = recent_data[self.target_column].describe()\n        print(f\"\\nTarget ({self.target_column}) statistics (recent 1000 days):\")\n        print(target_stats)\n        \n        # Feature groups\n        feature_groups = {}\n        for col in self.train_df.columns:\n            if col not in ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']:\n                prefix = col[0] if col[0].isalpha() else 'Other'\n                feature_groups[prefix] = feature_groups.get(prefix, 0) + 1\n        \n        print(f\"\\nFeature groups: {feature_groups}\")\n        \n        # Missing data check (recent data)\n        missing_recent = recent_data.isnull().sum()\n        missing_features = missing_recent[missing_recent > 0]\n        print(f\"\\nFeatures with missing values (recent data): {len(missing_features)}\")\n        \n    def create_engineered_features(self, df, is_train=True):\n        \"\"\"Optimized feature engineering for Kaggle environment\"\"\"\n        print(f\"Creating features... (is_train={is_train})\")\n        \n        # Start with original features\n        df_features = df.copy()\n        \n        # Get base feature columns\n        exclude_cols = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n        if not is_train:\n            exclude_cols.extend(['is_scored', 'lagged_forward_returns', 'lagged_risk_free_rate', 'lagged_market_forward_excess_returns'])\n        \n        base_features = [col for col in df.columns if col not in exclude_cols]\n        numeric_features = [col for col in base_features if df[col].dtype in ['float64', 'int64']]\n        \n        print(f\"Base numeric features: {len(numeric_features)}\")\n        \n        # 1. Rolling features (trend indicators)\n        windows = [5, 20, 60]  # Short, medium, long term\n        for window in windows:\n            for i, feature in enumerate(numeric_features[:15]):  # Limit to avoid memory issues\n                if i % 5 == 0:  # Progress indicator\n                    print(f\"Processing rolling features: {i+1}/{min(15, len(numeric_features))}\")\n                \n                df_features[f'{feature}_ma{window}'] = df[feature].rolling(window=window, min_periods=1).mean()\n                \n                # Only add std for longer windows to reduce feature count\n                if window >= 20:\n                    df_features[f'{feature}_std{window}'] = df[feature].rolling(window=window, min_periods=1).std()\n        \n        # 2. Lag features (previous day information)\n        lags = [1, 2, 5]\n        for lag in lags:\n            for feature in numeric_features[:8]:  # Top features only\n                df_features[f'{feature}_lag{lag}'] = df[feature].shift(lag)\n        \n        # 3. Technical indicators - ALWAYS CREATE THESE FEATURES\n        # Volatility regime (always create, use defaults if not enough data)\n        if 'V1' in df.columns:\n            if df['V1'].notna().sum() > 50:  # Lowered threshold\n                vol_threshold = df['V1'].rolling(min(252, len(df)), min_periods=5).quantile(0.75)\n                df_features['high_vol_regime'] = (df['V1'] > vol_threshold).astype(int)\n            else:\n                # Default: neutral regime for small datasets\n                df_features['high_vol_regime'] = 0\n        else:\n            df_features['high_vol_regime'] = 0\n        \n        # Momentum signal (always create)\n        if 'M1' in df.columns:\n            if df['M1'].notna().sum() > 10:  # Lowered threshold\n                momentum_ma = df['M1'].rolling(min(20, len(df)), min_periods=2).mean()\n                df_features['momentum_signal'] = np.sign(df['M1'] - momentum_ma)\n            else:\n                # Default: neutral momentum for small datasets\n                df_features['momentum_signal'] = 0\n        else:\n            df_features['momentum_signal'] = 0\n        \n        # 4. Cross-feature interactions (limited to avoid explosion)\n        vol_features = [col for col in base_features if col.startswith('V')][:3]\n        sent_features = [col for col in base_features if col.startswith('S')][:3]\n        \n        for v_feat in vol_features:\n            for s_feat in sent_features:\n                if v_feat in df.columns and s_feat in df.columns:\n                    df_features[f'{v_feat}_{s_feat}_interaction'] = df[v_feat] * df[s_feat]\n        \n        # 5. Add lagged returns if available (test set)\n        if 'lagged_forward_returns' in df.columns:\n            df_features['lagged_return_sign'] = np.sign(df['lagged_forward_returns'])\n            df_features['lagged_return_magnitude'] = np.abs(df['lagged_forward_returns'])\n            \n            # Rolling averages of lagged returns\n            df_features['lagged_return_ma5'] = df['lagged_forward_returns'].rolling(5, min_periods=1).mean()\n            df_features['lagged_return_ma20'] = df['lagged_forward_returns'].rolling(min(20, len(df)), min_periods=1).mean()\n        \n        print(f\"Final feature count: {df_features.shape[1]}\")\n        \n        # Memory cleanup\n        gc.collect()\n        \n        return df_features\n    \n    def prepare_training_data(self):\n        \"\"\"Prepare clean training data\"\"\"\n        print(\"Preparing training data...\")\n        \n        # Create features\n        train_features = self.create_engineered_features(self.train_df, is_train=True)\n        \n        # Remove rows with missing target\n        train_clean = train_features.dropna(subset=[self.target_column])\n        print(f\"Training rows after target cleaning: {len(train_clean)}\")\n        \n        # Use more recent data for better model performance (last 3 years ‚âà 750 trading days)\n        if len(train_clean) > 1500:\n            train_clean = train_clean.iloc[-1500:]\n            print(f\"Using recent {len(train_clean)} rows for training\")\n        \n        # Get feature columns\n        exclude_cols = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n        self.feature_columns = [col for col in train_clean.columns if col not in exclude_cols]\n        \n        print(f\"Using {len(self.feature_columns)} features\")\n        \n        # Prepare X and y with proper missing value handling\n        X = train_clean[self.feature_columns]\n        \n        # Smart missing value imputation\n        X = X.fillna(X.median())  # Use median for robustness\n        \n        y = train_clean[self.target_column]\n        date_ids = train_clean['date_id']\n        \n        return X, y, date_ids\n    \n    def train_ensemble_models(self, X, y):\n        \"\"\"Train ensemble of models optimized for Kaggle\"\"\"\n        print(\"Training ensemble models...\")\n        \n        # 1. LightGBM (fast and effective)\n        self.models['lgb'] = lgb.LGBMRegressor(\n            n_estimators=200,\n            max_depth=8,\n            learning_rate=0.05,\n            num_leaves=31,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            verbose=-1\n        )\n        \n        # 2. XGBoost  \n        self.models['xgb'] = xgb.XGBRegressor(\n            n_estimators=200,\n            max_depth=6,\n            learning_rate=0.05,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            verbosity=0\n        )\n        \n        # 3. Linear model (regularized)\n        self.models['ridge'] = Ridge(alpha=10.0)\n        \n        # 4. Random Forest (for diversity)\n        self.models['rf'] = RandomForestRegressor(\n            n_estimators=100,\n            max_depth=12,\n            random_state=42,\n            n_jobs=-1\n        )\n        \n        # Train models\n        for name, model in self.models.items():\n            print(f\"Training {name}...\")\n            if name == 'ridge':\n                X_scaled = self.scaler.fit_transform(X)\n                model.fit(X_scaled, y)\n            else:\n                model.fit(X, y)\n        \n        print(\"‚úì All models trained!\")\n    \n    def validate_and_optimize_weights(self, X, y):\n        \"\"\"Validate models and optimize ensemble weights\"\"\"\n        print(\"Validating models with time series CV...\")\n        \n        tscv = TimeSeriesSplit(n_splits=3)\n        model_performance = {name: [] for name in self.models.keys()}\n        \n        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n            print(f\"Fold {fold + 1}/3\")\n            \n            X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n            y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n            \n            fold_predictions = {}\n            \n            for name, model in self.models.items():\n                if name == 'ridge':\n                    X_train_scaled = self.scaler.fit_transform(X_train_cv)\n                    X_val_scaled = self.scaler.transform(X_val_cv)\n                    \n                    model.fit(X_train_scaled, y_train_cv)\n                    pred = model.predict(X_val_scaled)\n                else:\n                    model.fit(X_train_cv, y_train_cv)\n                    pred = model.predict(X_val_cv)\n                \n                fold_predictions[name] = pred\n                mse = mean_squared_error(y_val_cv, pred)\n                model_performance[name].append(mse)\n        \n        # Calculate average performance and weights\n        avg_performance = {name: np.mean(scores) for name, scores in model_performance.items()}\n        \n        print(\"\\nModel Performance (MSE):\")\n        for name, score in avg_performance.items():\n            print(f\"{name}: {score:.6f}\")\n        \n        # Inverse MSE weighting (better models get higher weight)\n        total_inv_mse = sum(1/score for score in avg_performance.values())\n        self.model_weights = {name: (1/score)/total_inv_mse for name, score in avg_performance.items()}\n        \n        print(\"\\nOptimized Model Weights:\")\n        for name, weight in self.model_weights.items():\n            print(f\"{name}: {weight:.3f}\")\n    \n    def predict_allocations(self, test_df):\n        \"\"\"Generate allocation predictions for test data\"\"\"\n        print(\"Generating predictions...\")\n        \n        # Create test features\n        test_features = self.create_engineered_features(test_df, is_train=False)\n        \n        # Ensure test features match training features\n        # Add any missing features with default values\n        for col in self.feature_columns:\n            if col not in test_features.columns:\n                print(f\"Adding missing feature {col} with default value 0\")\n                test_features[col] = 0\n        \n        # Select only the features used during training\n        X_test = test_features[self.feature_columns]\n        \n        # Handle missing values\n        X_test = X_test.fillna(X_test.median())\n        \n        # If median results in NaN (all values are NaN), fill with 0\n        X_test = X_test.fillna(0)\n        \n        print(f\"Test feature matrix shape: {X_test.shape}\")\n        \n        # Get predictions from all models\n        predictions = {}\n        for name, model in self.models.items():\n            if name == 'ridge':\n                X_test_scaled = self.scaler.transform(X_test)\n                predictions[name] = model.predict(X_test_scaled)\n            else:\n                predictions[name] = model.predict(X_test)\n        \n        # Weighted ensemble prediction\n        ensemble_pred = np.zeros(len(X_test))\n        for name, pred in predictions.items():\n            ensemble_pred += self.model_weights[name] * pred\n        \n        # Convert predictions to allocations\n        allocations = self.convert_to_allocations(ensemble_pred)\n        \n        return allocations, ensemble_pred\n    \n    def convert_to_allocations(self, predictions):\n        \"\"\"Convert excess return predictions to allocation weights (0-2)\"\"\"\n        allocations = np.ones_like(predictions)  # Start with market weight\n        \n        # Dynamic thresholds based on prediction distribution\n        high_threshold = np.percentile(predictions, 70)\n        low_threshold = np.percentile(predictions, 30)\n        \n        # Conservative allocation strategy\n        for i, pred in enumerate(predictions):\n            if pred > high_threshold:\n                # Positive signal: increase allocation\n                confidence = min((pred - high_threshold) / (np.max(predictions) - high_threshold + 1e-8), 1.0)\n                allocations[i] = 1.0 + 1.0 * confidence  # Max 2x leverage\n            elif pred < low_threshold:\n                # Negative signal: reduce allocation\n                confidence = min((low_threshold - pred) / (low_threshold - np.min(predictions) + 1e-8), 1.0)\n                allocations[i] = 1.0 - 0.8 * confidence  # Min 0.2x (keep some market exposure)\n        \n        # Ensure bounds [0, 2]\n        allocations = np.clip(allocations, 0, 2)\n        \n        return allocations\n    \n    def analyze_results(self, allocations, predictions, test_df):\n        \"\"\"Analyze prediction results\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"RESULTS ANALYSIS\")  \n        print(\"=\"*50)\n        \n        print(f\"Allocation Statistics:\")\n        print(f\"  Mean: {np.mean(allocations):.3f}\")\n        print(f\"  Std:  {np.std(allocations):.3f}\")\n        print(f\"  Min:  {np.min(allocations):.3f}\")\n        print(f\"  Max:  {np.max(allocations):.3f}\")\n        \n        leverage_days = np.sum(allocations > 1.05)\n        conservative_days = np.sum(allocations < 0.95)\n        \n        print(f\"\\nStrategy Breakdown:\")\n        print(f\"  Leverage days (>1.05):     {leverage_days}/{len(allocations)} ({leverage_days/len(allocations)*100:.1f}%)\")\n        print(f\"  Conservative days (<0.95): {conservative_days}/{len(allocations)} ({conservative_days/len(allocations)*100:.1f}%)\")\n        print(f\"  Neutral days:              {len(allocations)-leverage_days-conservative_days}/{len(allocations)}\")\n        \n        # Quick visualization\n        plt.figure(figsize=(15, 5))\n        \n        plt.subplot(1, 3, 1)\n        plt.hist(allocations, bins=20, alpha=0.7, edgecolor='black')\n        plt.xlabel('Allocation Weight')\n        plt.ylabel('Frequency')\n        plt.title('Allocation Distribution')\n        plt.axvline(1.0, color='red', linestyle='--', alpha=0.7, label='Market Weight')\n        plt.legend()\n        \n        plt.subplot(1, 3, 2)\n        plt.plot(range(len(allocations)), allocations, alpha=0.8)\n        plt.xlabel('Test Day')\n        plt.ylabel('Allocation Weight')\n        plt.title('Allocations Over Time')\n        plt.axhline(1.0, color='red', linestyle='--', alpha=0.7)\n        \n        plt.subplot(1, 3, 3)\n        plt.scatter(predictions, allocations, alpha=0.6)\n        plt.xlabel('Excess Return Prediction')\n        plt.ylabel('Allocation Weight')\n        plt.title('Prediction vs Allocation')\n        plt.axhline(1.0, color='red', linestyle='--', alpha=0.7)\n        plt.axvline(0.0, color='red', linestyle='--', alpha=0.7)\n        \n        plt.tight_layout()\n        plt.show()\n\ndef main():\n    \"\"\"Main execution function for Kaggle\"\"\"\n    print(\"üöÄ Starting S&P 500 Market Timing Pipeline\")\n    print(\"=\"*60)\n    \n    # Initialize pipeline\n    pipeline = KaggleSP500Pipeline()\n    \n    try:\n        # Load data\n        train_df, test_df = pipeline.load_data()\n        \n        # Quick EDA\n        pipeline.quick_eda()\n        \n        # Prepare training data  \n        X, y, date_ids = pipeline.prepare_training_data()\n        \n        # Train models\n        pipeline.train_ensemble_models(X, y)\n        \n        # Validate and optimize\n        pipeline.validate_and_optimize_weights(X, y)\n        \n        # Generate predictions\n        allocations, predictions = pipeline.predict_allocations(test_df)\n        \n        # Analyze results\n        pipeline.analyze_results(allocations, predictions, test_df)\n        \n        # Create submission format\n        submission_df = pd.DataFrame({\n            'date_id': test_df['date_id'],\n            'allocation': allocations\n        })\n        \n        # Save results\n        submission_path = Path('/kaggle/working/submission.csv')\n        if not submission_path.parent.exists():\n            submission_path = Path('./submission.csv')\n        \n        submission_df.to_csv(submission_path, index=False)\n        \n        print(f\"\\n‚úÖ Pipeline completed successfully!\")\n        print(f\"üìä Submission saved to: {submission_path}\")\n        print(f\"üéØ Mean allocation: {np.mean(allocations):.3f}\")\n        print(f\"üìà Leverage usage: {np.sum(allocations > 1.05)/len(allocations)*100:.1f}%\")\n        \n        return pipeline, submission_df\n        \n    except FileNotFoundError as e:\n        print(f\"\\n‚ùå Data files not found: {e}\")\n        print(\"\\nüí° Solutions:\")\n        print(\"1. Make sure you're in a Kaggle notebook with the competition dataset attached\")\n        print(\"2. Or upload train.csv and test.csv to your current directory\")  \n        print(\"3. Or run this code in the competition environment\")\n        return None, None\n        \n    except Exception as e:\n        print(f\"‚ùå Error in pipeline: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\n# Execute pipeline\nif __name__ == \"__main__\":\n    pipeline, submission = main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T17:50:00.803216Z","iopub.execute_input":"2025-09-16T17:50:00.803596Z","iopub.status.idle":"2025-09-16T17:50:29.414507Z","shell.execute_reply.started":"2025-09-16T17:50:00.803568Z","shell.execute_reply":"2025-09-16T17:50:29.413371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}