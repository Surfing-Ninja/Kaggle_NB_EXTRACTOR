{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. EDA","metadata":{}},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\n\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf, kpss\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\nfrom statsmodels.tools.sm_exceptions import InterpolationWarning\nimport os\nimport math\nimport matplotlib.pyplot as plt\n\n\nwarnings.filterwarnings(\"ignore\", category=InterpolationWarning)\n\n\ndef ts_diagnostics(\n    df: pd.DataFrame,\n    time_col: str = \"date_id\",\n    max_lag: int = 60,\n    season_search_max_lag: int = 60,\n    season_min_acf: float = 0.30,\n    alpha: float = 0.05,\n    fill_gap: bool = True,\n    save_path: str | None = None,\n    # ‚ñº Ï∂îÍ∞Ä ÏòµÏÖò\n    contig_policy: str = \"none\",      # {\"none\",\"trim_ends\",\"longest\"}\n    min_contig_len: int = 20,         # Ïú†Ìö®Íµ¨Í∞Ñ ÏµúÏÜå Í∏∏Ïù¥\n) -> pd.DataFrame:\n\n    if time_col in df.columns:\n        df = df.sort_values(time_col).reset_index(drop=True)\n\n    value_cols = [c for c in df.columns if c != time_col]\n\n    rows = []\n\n    for col in value_cols:\n        x_raw = df[col].astype(float).values\n        n_tot = len(x_raw)\n\n        # --- Ïó∞ÏÜç Ïú†Ìö®Íµ¨Í∞Ñ ÏÑ†ÌÉù ---\n        eff_start, eff_end = 0, n_tot - 1\n        if contig_policy != \"none\":\n            valid = np.isfinite(x_raw)\n\n            if not valid.any():\n                rows.append({\"column\": col, \"length\": n_tot, \"effective_length\": 0,\n                             \"coverage_ratio\": 0.0, \"used_range\": None, \"note\": \"all_nan\"})\n                continue\n\n            if contig_policy == \"trim_ends\":\n                # Ïïû/Îí§ Ïó∞ÏÜç NaNÎßå Ï†úÍ±∞\n                i0 = int(np.argmax(valid))                           # Ï≤´ True\n                i1 = int(n_tot - 1 - np.argmax(valid[::-1]))         # ÎßàÏßÄÎßâ True\n            elif contig_policy == \"longest\":\n                # Í∞ÄÏû• Í∏¥ Ïó∞ÏÜç True Íµ¨Í∞Ñ\n                arr = valid.astype(int)\n                diff = np.diff(np.r_[0, arr, 0])\n                starts = np.where(diff == 1)[0]\n                ends   = np.where(diff == -1)[0]\n                lengths = ends - starts\n                k = int(np.argmax(lengths))\n                i0, i1 = int(starts[k]), int(ends[k] - 1)\n            else:\n                i0, i1 = 0, n_tot - 1\n\n            eff_start, eff_end = i0, i1\n            if i1 - i0 + 1 < min_contig_len:\n                rows.append({\"column\": col, \"length\": n_tot, \"effective_length\": i1 - i0 + 1,\n                             \"coverage_ratio\": (i1 - i0 + 1) / max(n_tot, 1),\n                             \"used_range\": f\"{i0}:{i1}\", \"note\": \"contig_too_short\"})\n                continue\n\n            x_work = x_raw[i0:i1+1]\n        else:\n            x_work = x_raw.copy()\n\n        # --- ÌïÑÏöî Ïãú Î≥¥Í∞Ñ/Ï±ÑÏõÄ(ÏÑ†ÌÉùÎêú Íµ¨Í∞Ñ ÏïàÏóêÏÑúÎßå) ---\n        if fill_gap:\n            s = pd.Series(x_work)\n            x = s.interpolate(method=\"linear\", limit_direction=\"both\").ffill().bfill().values\n        else:\n            x = x_work\n\n        if np.sum(np.isfinite(x)) < 20:\n            rows.append({\"column\": col, \"length\": n_tot, \"effective_length\": len(x),\n                         \"coverage_ratio\": len(x)/max(n_tot,1), \"used_range\": f\"{eff_start}:{eff_end}\",\n                         \"note\": \"too_short\"})\n            continue\n\n        # --- ÌÜµÍ≥Ñ Í≤ÄÏ†ï(Í∏∞Ï°¥ Î°úÏßÅ Í∑∏ÎåÄÎ°ú) ---\n        def safe_adf(v):\n            try:\n                st, pv, *_ = adfuller(v, autolag=\"AIC\")\n                return st, pv\n            except Exception:\n                return np.nan, np.nan\n\n        def safe_kpss_with_note(v):\n            try:\n                st, pv, _, _ = kpss(v, regression=\"c\", nlags=\"auto\")\n                note = \"p < 0.01 (beyond table range)\" if pv <= 0.01 else (\"p > 0.10 (beyond table range)\" if pv >= 0.10 else None)\n                return st, pv, note\n            except Exception:\n                return np.nan, np.nan, None\n\n        def safe_acf(v):\n            try:\n                return acf(v, nlags=min(max_lag, len(v) - 2), fft=False)\n            except Exception:\n                return np.array([np.nan])\n\n        def safe_pacf(v):\n            try:\n                return pacf(v, nlags=min(max_lag, len(v) - 2))\n            except Exception:\n                return np.array([np.nan])\n\n        def safe_dw(v):\n            try:\n                return durbin_watson(v - np.mean(v))\n            except Exception:\n                return np.nan\n\n        def safe_ljung_box(v):\n            try:\n                lags = min(max_lag, max(1, len(v) // 5))\n                lb = acorr_ljungbox(v, lags=[lags], return_df=True)\n                return float(lb[\"lb_stat\"].values[0]), float(lb[\"lb_pvalue\"].values[0])\n            except Exception:\n                return np.nan, np.nan\n\n        def safe_arch_lm(v):\n            try:\n                lags = min(max_lag, int(np.sqrt(len(v))))\n                st, pv, *_ = het_arch(v, nlags=lags)\n                return st, pv\n            except Exception:\n                return np.nan, np.nan\n\n        adf_stat, adf_p = safe_adf(x)\n        kpss_stat, kpss_p, kpss_note = safe_kpss_with_note(x)\n        acf_vals = safe_acf(x)\n        pacf_vals = safe_pacf(x)\n        dw = safe_dw(x)\n        lb_stat, lb_p = safe_ljung_box(x)\n        arch_stat, arch_p = safe_arch_lm(x)\n\n        def classify_stationarity(adf_p, kpss_p):\n            if np.isnan(adf_p) and np.isnan(kpss_p): return \"unknown\"\n            adf_ok = (not np.isnan(adf_p)) and (adf_p < alpha)\n            kpss_ok = (not np.isnan(kpss_p)) and (kpss_p >= alpha)\n            if adf_ok and kpss_ok: return \"stationary\"\n            if adf_ok or kpss_ok:  return \"likely_stationary\"\n            return \"non_stationary\"\n\n        def classify_autocorr(acf_vals, lb_p):\n            acf1 = acf_vals[1] if isinstance(acf_vals, np.ndarray) and len(acf_vals) > 1 else np.nan\n            if (not np.isnan(lb_p)) and (lb_p < alpha): return \"autocorrelated\"\n            if (not np.isnan(acf1)) and (abs(acf1) >= 0.20): return \"autocorrelated\"\n            return \"no_autocorr\"\n\n        def classify_white_noise(stationarity_label, lb_p):\n            if stationarity_label in (\"stationary\",\"likely_stationary\") and (not np.isnan(lb_p)) and (lb_p >= alpha):\n                return \"white_noise\"\n            return \"not_white_noise\"\n\n        def classify_heteroskedasticity(arch_p):\n            if (not np.isnan(arch_p)) and (arch_p < alpha): return \"heteroskedastic\"\n            if np.isnan(arch_p): return \"unknown\"\n            return \"homoskedastic\"\n\n        stationarity = classify_stationarity(adf_p, kpss_p)\n        autocorr_lbl = classify_autocorr(acf_vals, lb_p)\n        seasonal_flag = False; season_lag = np.nan; season_acf = np.nan\n        if acf_vals is not None and len(acf_vals) > 2:\n            seg = acf_vals[2:min(season_search_max_lag, len(acf_vals)-1)+1]\n            if len(seg) > 0:\n                k = np.nanargmax(seg)\n                season_lag = int(2 + k); season_acf = float(seg[k])\n                seasonal_flag = season_acf >= season_min_acf\n\n        acf1 = float(acf_vals[1]) if isinstance(acf_vals, np.ndarray) and len(acf_vals) > 1 else np.nan\n        pacf1 = float(pacf_vals[1]) if isinstance(pacf_vals, np.ndarray) and len(pacf_vals) > 1 else np.nan\n        acf10 = float(acf_vals[min(10, len(acf_vals)-1)]) if isinstance(acf_vals, np.ndarray) and len(acf_vals) > 10 else np.nan\n\n        rows.append({\n            \"column\": col,\n            \"length\": n_tot,\n            \"effective_length\": len(x),\n            \"coverage_ratio\": len(x) / max(n_tot, 1),\n            \"used_range\": f\"{eff_start}:{eff_end}\",\n            \"ADF_stat\": adf_stat, \"ADF_p\": adf_p,\n            \"KPSS_stat\": kpss_stat, \"KPSS_p\": kpss_p, \"KPSS_note\": kpss_note,\n            \"stationarity_label\": stationarity,\n            \"DW_stat\": dw,\n            \"LB_stat\": lb_stat, \"LB_p\": lb_p,\n            \"autocorr_label\": autocorr_lbl,\n            \"ACF_lag1\": acf1, \"ACF_lag10\": acf10, \"PACF_lag1\": pacf1,\n            \"seasonal_flag\": bool(seasonal_flag),\n            \"season_period_est\": season_lag,\n            \"season_acf_at_period\": season_acf,\n            \"white_noise_label\": classify_white_noise(stationarity, lb_p),\n            \"ARCH_stat\": arch_stat, \"ARCH_p\": arch_p, \"variance_label\": classify_heteroskedasticity(arch_p),\n        })\n\n    results_df = pd.DataFrame(rows).set_index(\"column\").sort_index()\n    if save_path is not None:\n        results_df.to_csv(save_path, index=True)\n        print(f\"Saved diagnostics to: {save_path}\")\n    return results_df\n\n\n\ndef ts_eda_overview(\n    df: pd.DataFrame,\n    time_col: str = \"date_id\",\n    save_dir: str | None = \"./eda_plots\",\n    max_hist_cols: int = 24,\n    max_box_cols: int = 24,\n    corr_method: str = \"pearson\",\n    sample_rows_for_plots: int | None = None,\n    nan_heatmap_row_sample: int | None = None,  # NoneÏù¥Î©¥ Ï†ÑÏ≤¥ Ìñâ\n    show: bool = False,\n    label_on: bool = True,\n    chunk_size: int = 50,\n    # ‚ñ∂ Ï∂îÍ∞Ä: NaN ÌûàÌä∏Îßµ ÎÜíÏù¥ Ï†úÏñ¥\n    nan_heatmap_height_per_row: float = 0.02,\n    nan_heatmap_base_height: float = 3.0,\n    nan_heatmap_max_height: float = 20.0,   # ‚Üê ÏµúÎåÄ ÎÜíÏù¥(Ïù∏Ïπò)\n):\n    \"\"\"\n    ÏãúÍ≥ÑÏó¥/ÌëúÌòï Îç∞Ïù¥ÌÑ∞Ïùò Ï†ÑÏ≤¥Ï†Å Î∂ÑÌè¨/Í≤∞Ï∏°/ÏÉÅÍ¥ÄÏùÑ Îπ†Î•¥Í≤å ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§.\n\n    Î≥ÄÍ≤ΩÏÇ¨Ìï≠\n    - Î∞ïÏä§ÌîåÎ°Ø: q95-q5 Ïä§ÏºÄÏùºÏùÑ Î°úÍ∑∏ Ïä§ÏºÄÏùº Î≤ÑÌÇ∑(10Ïùò ÏßÄÏàò)ÏúºÎ°ú Î¨∂Ïñ¥, ÎπÑÏä∑Ìïú Îã®ÏúÑÎÅºÎ¶¨ ÌëúÏãú\n    - NaN ÎπÑÏú® Î∞îÏ∞®Ìä∏: 50Í∞ú Îã®ÏúÑÎ°ú Î∂ÑÌï† Ï†ÄÏû•/ÌëúÏãú\n    - NaN ÌûàÌä∏Îßµ: Ïó¥ Í∏∞Ï§Ä 50Í∞ú Îã®ÏúÑÎ°ú Î∂ÑÌï† Ï†ÄÏû•/ÌëúÏãú\n    - ÏÉÅÍ¥Ä ÌûàÌä∏Îßµ: Ïó¥ Í∏∞Ï§Ä 50Í∞ú Îã®ÏúÑÎ°ú Î∂ÑÌï† Ï†ÄÏû•/ÌëúÏãú\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    time_col : str\n    save_dir : str|None\n    max_hist_cols : int\n    max_box_cols : int\n    corr_method : {\"pearson\",\"spearman\"}\n    sample_rows_for_plots : int|None\n    nan_heatmap_row_sample : int\n    show : bool\n    label_on : bool\n    chunk_size : int\n    \"\"\"\n    # ----------------------------\n    # Ï§ÄÎπÑ\n    # ----------------------------\n    if save_dir is not None:\n        os.makedirs(save_dir, exist_ok=True)\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    if time_col in numeric_cols:\n        numeric_cols.remove(time_col)\n\n    plot_df = df.copy()\n    if sample_rows_for_plots is not None and len(plot_df) > sample_rows_for_plots:\n        plot_df = plot_df.sample(sample_rows_for_plots, random_state=42).sort_index()\n\n    # Ìó¨Ìçº: Ïª¨Îüº Î¶¨Ïä§Ìä∏Î•º Í≥†Ï†ï Í∏∏Ïù¥Î°ú ÏûêÎ•¥Í∏∞\n    def chunks(lst, n):\n        for i in range(0, len(lst), n):\n            yield lst[i:i+n]\n\n    # ----------------------------\n    # 1) Í≤∞Ï∏° ÎπÑÏú® ÏöîÏïΩ ÌÖåÏù¥Î∏î + Î∞îÏ∞®Ìä∏(Ï≤≠ÌÅ¨)\n    # ----------------------------\n    na_ratio = df.isna().mean().rename(\"nan_ratio\")\n    desc = df.describe(include=\"all\").T\n    summary_df = desc.join(na_ratio, how=\"outer\")\n    summary_df = summary_df.sort_values(\"nan_ratio\", ascending=False)\n    if save_dir is not None:\n        summary_df.to_csv(os.path.join(save_dir, \"summary_numeric_and_nan.csv\"))\n\n    # NaN bar: 50Í∞úÏî© ÎÅäÏñ¥ÏÑú\n    ordered_cols = na_ratio.sort_values(ascending=False).index.tolist()\n    for j, cols_chunk in enumerate(chunks(ordered_cols, chunk_size), start=1):\n        plt.figure(figsize=(12, 5))\n        na_ratio.loc[cols_chunk].plot(kind=\"bar\")\n        if label_on:\n            plt.title(f\"NaN ratio per column (chunk {j})\")\n            plt.ylabel(\"ratio\")\n            plt.xlabel(\"columns\")\n            plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n        else:\n            plt.title(\"\"); plt.ylabel(\"\"); plt.xlabel(\"\")\n            plt.xticks([]); plt.yticks([])\n        plt.tight_layout()\n        if save_dir is not None:\n            plt.savefig(os.path.join(save_dir, f\"nan_ratio_bar_chunk{j}.png\"), dpi=150)\n        if show: plt.show()\n        plt.close()\n\n    # ----------------------------\n    # 2) ÌûàÏä§ÌÜ†Í∑∏Îû® (ÏÉÅÏúÑ max_hist_colsÍ∞ú)\n    # ----------------------------\n    hist_cols = numeric_cols[:max_hist_cols]\n    if len(hist_cols) > 0:\n        n = len(hist_cols)\n        ncols = min(6, max(1, int(math.ceil(n ** 0.5))))\n        nrows = int(math.ceil(n / ncols))\n        plt.figure(figsize=(3.2 * ncols, 2.6 * nrows))\n        for i, c in enumerate(hist_cols, 1):\n            ax = plt.subplot(nrows, ncols, i)\n            plot_df[c].dropna().hist(bins=40, ax=ax)\n            if label_on:\n                ax.set_title(c, fontsize=9)\n                ax.set_ylabel(\"freq\")\n            else:\n                ax.set_title(\"\"); ax.set_xticks([]); ax.set_yticks([])\n        plt.tight_layout()\n        if save_dir is not None:\n            plt.savefig(os.path.join(save_dir, \"histograms.png\"), dpi=150)\n        if show: plt.show()\n        plt.close()\n\n    # ----------------------------\n    # 3) Î∞ïÏä§ÌîåÎ°Ø (Ïä§ÏºÄÏùº Î≤ÑÌÇ∑ÏúºÎ°ú Í∑∏Î£π)\n    # ----------------------------\n    # Ïä§ÏºÄÏùº = q95 - q5 (robust range). 0 ÎòêÎäî NaNÏù¥Î©¥ 'const/zero' Î≤ÑÌÇ∑.\n    scale_info = {}\n    for c in numeric_cols:\n        x = plot_df[c].astype(float).values\n        x = x[~np.isnan(x)]\n        if x.size == 0:\n            rng = np.nan\n        else:\n            q5, q95 = np.nanpercentile(x, [5, 95])\n            rng = float(q95 - q5)\n        scale_info[c] = rng\n\n    def bucket_of(rng):\n        if rng is None or np.isnan(rng) or rng <= 0:\n            return \"const/zero\"\n        exp = int(np.floor(np.log10(rng)))\n        # Ïòà: 10^-3 ~ 10^-2 Îì±\n        return f\"1e{exp}~1e{exp+1}\"\n\n    buckets = {}\n    for c, rng in scale_info.items():\n        b = bucket_of(rng)\n        buckets.setdefault(b, []).append(c)\n\n    # Î≤ÑÌÇ∑Î≥ÑÎ°ú Ï†ïÎ†¨(ÏÉÅÏàò/Ï†úÎ°ú Î®ºÏ†Ä), Í∑∏ Îã§Ïùå ÏßÄÏàò Ïò§Î¶ÑÏ∞®Ïàú\n    def bucket_key(b):\n        if b == \"const/zero\": return -999\n        # \"1e-3~1e-2\" ÌòïÌÉúÏóêÏÑú -3 Ï∂îÏ∂ú\n        try:\n            exp = int(b.split(\"~\")[0].replace(\"1e\", \"\"))\n        except:\n            exp = 0\n        return exp\n\n    for b in sorted(buckets.keys(), key=bucket_key):\n        cols_in_bucket = buckets[b]\n        if len(cols_in_bucket) == 0:\n            continue\n        # ÌéòÏù¥ÏßÄ Îã®ÏúÑÎ°ú ÎÇòÎà† Í∑∏Î¶¨Í∏∞\n        for j, cols_chunk in enumerate(chunks(cols_in_bucket, max_box_cols), start=1):\n            plt.figure(figsize=(max(10, 0.4 * len(cols_chunk)), 5))\n            plot_df[cols_chunk].boxplot(rot=45 if label_on else 0)\n            if label_on:\n                plt.title(f\"Boxplots (bucket={b}, chunk={j})\")\n            else:\n                plt.title(\"\"); plt.xlabel(\"\"); plt.ylabel(\"\")\n                plt.xticks([]); plt.yticks([])\n            plt.tight_layout()\n            if save_dir is not None:\n                fn = f\"boxplots_bucket_{b.replace('/', '_').replace('~','to').replace('-','m')}_chunk{j}.png\"\n                plt.savefig(os.path.join(save_dir, fn), dpi=150)\n            if show: plt.show()\n            plt.close()\n\n    # ----------------------------\n    # 4) ÏÉÅÍ¥Ä ÌûàÌä∏Îßµ (Ïó¥ 50Í∞úÏî© ÎÅäÏñ¥ÏÑú)\n    # ----------------------------\n    corr_cols_all = numeric_cols[:]\n    if len(corr_cols_all) >= 2:\n        for j, cols_chunk in enumerate(chunks(corr_cols_all, chunk_size), start=1):\n            corr = plot_df[cols_chunk].corr(method=corr_method)\n            plt.figure(figsize=(min(12, 0.25*len(cols_chunk)+3), min(12, 0.25*len(cols_chunk)+3)))\n            im = plt.imshow(corr.values, aspect=\"auto\")\n            plt.colorbar(im, fraction=0.045, pad=0.04)\n            if label_on:\n                plt.xticks(range(len(cols_chunk)), cols_chunk, rotation=90, fontsize=7)\n                plt.yticks(range(len(cols_chunk)), cols_chunk, fontsize=7)\n                plt.title(f\"Correlation heatmap ({corr_method}) - chunk {j}\")\n            else:\n                plt.xticks([]); plt.yticks([]); plt.title(\"\")\n            plt.tight_layout()\n            if save_dir is not None:\n                plt.savefig(os.path.join(save_dir, f\"correlation_heatmap_chunk{j}.png\"), dpi=150)\n            if show: plt.show()\n            plt.close()\n\n    # ----------------------------\n    # 5) NaN ÌûàÌä∏Îßµ (Ìñâ Ï†ÑÏ≤¥ or ÏÉòÌîå)\n    # ----------------------------\n    nan_mat = df.isna()\n    if isinstance(nan_heatmap_row_sample, int) and nan_heatmap_row_sample > 0:\n        if len(nan_mat) > nan_heatmap_row_sample:\n            nan_mat = nan_mat.head(nan_heatmap_row_sample)\n    \n    all_cols = df.columns.tolist()\n    \n    for j, cols_chunk in enumerate(chunks(all_cols, chunk_size), start=1):\n        sub = nan_mat[cols_chunk]\n        n_rows, n_cols = sub.shape\n    \n        # üîß ÎÜíÏù¥ Í≥ÑÏÇ∞ + ÏÉÅÌïú Ï†ÅÏö© (Ïó¨Í∏∞Îßå ÏÇ¨Ïö©)\n        height = nan_heatmap_height_per_row * n_rows + nan_heatmap_base_height\n        height = min(height, nan_heatmap_max_height)\n    \n        width = min(12, 0.25 * n_cols + 3)\n    \n        fig, ax = plt.subplots(figsize=(width, height))\n        im = ax.imshow(sub.values, aspect=\"auto\")\n        if label_on:\n            ax.set_title(f\"NaN heatmap (rows={n_rows}) - chunk {j}\")\n            ax.set_xlabel(\"columns\")\n            ax.set_ylabel(\"rows\")\n            ax.set_xticks(range(n_cols))\n            ax.set_xticklabels(cols_chunk, rotation=90, fontsize=7)\n            ax.set_yticks([])  # Ìñâ ÎùºÎ≤®ÏùÄ ÏÉùÎûµ\n        else:\n            ax.set_title(\"\"); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n            ax.set_xticks([]); ax.set_yticks([])\n    \n        plt.tight_layout()\n        if save_dir is not None:\n            plt.savefig(os.path.join(save_dir, f\"nan_heatmap_chunk{j}.png\"), dpi=150)\n        if show:\n            plt.show()\n        plt.close(fig)\n\n\n    return summary_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:01:47.209865Z","iopub.execute_input":"2025-10-03T19:01:47.210718Z","iopub.status.idle":"2025-10-03T19:01:52.177633Z","shell.execute_reply.started":"2025-10-03T19:01:47.210662Z","shell.execute_reply":"2025-10-03T19:01:52.176577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Show ALL columns with your EDA helpers (no file saving, inline plots) ===\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------\n# 0) Load data\n# -------------------------------------------------\nPATH = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\ntime_col = \"date_id\"\ndf = pd.read_csv(PATH)\n\n# -------------------------------------------------\n# 1) Determine numeric columns (excluding time_col)\n# -------------------------------------------------\nnum_cols = df.select_dtypes(include=np.number).columns.tolist()\nif time_col in num_cols:\n    num_cols.remove(time_col)\nN = len(num_cols)\nprint(f\"Numeric columns (excluding '{time_col}'): {N}\")\n\n# -------------------------------------------------\n# 2) Run EDA plots inline (nothing saved)\n#    - Uses your ts_eda_overview() already defined above\n# -------------------------------------------------\n_ = ts_eda_overview(\n    df=df,\n    time_col=\"date_id\",\n    save_dir=None,\n    show=True,\n    chunk_size=len(df.columns)  # ‚úÖ Ï†ÑÏ≤¥ Ìïú Î≤àÏóê\n)\n\n# -------------------------------------------------\n# 3) Time-series diagnostics for ALL columns\n#    - Uses your ts_diagnostics() already defined above\n# -------------------------------------------------\ndiag_df = ts_diagnostics(\n    df,\n    time_col=\"date_id\",\n    fill_gap=False,\n    contig_policy=\"longest\",\n    min_contig_len=180\n)\n\n# -------------------------------------------------\n# 4) Summary table (describe + NaN ratio) for ALL columns\n#    - ts_eda_overview returns this; re-run once with no plotting to get table\n# -------------------------------------------------\nsummary_df = df.describe(include=\"all\").T\nsummary_df[\"nan_ratio\"] = df.isna().mean()\n\n# -------------------------------------------------\n# 5) Untruncate display to see ALL rows/columns\n# -------------------------------------------------\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\n\nprint(\"\\n=== Summary (describe + NaN ratio) ===\")\ndisplay(summary_df)\n\nprint(\"\\n=== Time-series diagnostics (all columns) ===\")\ndisplay(diag_df)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:01:52.179187Z","iopub.execute_input":"2025-10-03T19:01:52.179643Z","iopub.status.idle":"2025-10-03T19:02:44.42878Z","shell.execute_reply.started":"2025-10-03T19:01:52.17961Z","shell.execute_reply":"2025-10-03T19:02:44.427762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"need_diff = diag_df.query(\"stationarity_label == 'non_stationary'\").index.tolist()\nhas_season = diag_df.query(\"seasonal_flag == True\").assign(period=lambda d: d['season_period_est'])[[\"period\"]]\nhas_autocorr = diag_df.query(\"autocorr_label == 'autocorrelated' or LB_p < 0.05\").index.tolist()\nhetero = diag_df.query(\"variance_label == 'heteroskedastic'\").index.tolist()\nmaybe_noise = diag_df.query(\"white_noise_label == 'white_noise'\").index.tolist()\n\nprint(\"Ï∞®Î∂Ñ/Î≥ÄÌôò ÌõÑÎ≥¥:\", need_diff[:10], \"‚Ä¶\")\nprint(\"Í≥ÑÏ†àÏÑ± ÌõÑÎ≥¥(+Ï£ºÍ∏∞):\\n\", has_season.head())\nprint(\"ÏûêÍ∏∞ÏÉÅÍ¥Ä ÏûàÏùå:\", has_autocorr[:10], \"‚Ä¶\")\nprint(\"Ïù¥Î∂ÑÏÇ∞ ÏûàÏùå:\", hetero[:10], \"‚Ä¶\")\nprint(\"ÌôîÏù¥Ìä∏ÎÖ∏Ïù¥Ï¶à(Ïã†Ìò∏ ÏïΩÌï®):\", maybe_noise[:10], \"‚Ä¶\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:44.42966Z","iopub.execute_input":"2025-10-03T19:02:44.429902Z","iopub.status.idle":"2025-10-03T19:02:44.479956Z","shell.execute_reply.started":"2025-10-03T19:02:44.429883Z","shell.execute_reply":"2025-10-03T19:02:44.479168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. preprocessing","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Hull Tactical ‚Äî Processing Utilities (functions only)\n#  - NaN 50%‚Üë Ïª¨Îüº ÎìúÎ°≠ Î™©Î°ù Í≥ÑÏÇ∞\n#  - Î≥ÄÌôò ÎåÄÏÉÅ ÍµêÏßëÌï© Ï†ïÎ¶¨\n#  - Ï∞®Î∂Ñ/Í≥ÑÏ†àÏ∞®Î∂Ñ/ÎûòÍπÖ/Î°§ÎßÅ z-score Ï†ÅÏö© ÌååÏù¥ÌîÑÎùºÏù∏\n#  - ÌååÏùº I/O / ÌîÑÎ¶∞Ìä∏ ÏóÜÏùå (Ìï®ÏàòÎßå Ï†úÍ≥µ)\n# =========================================================\nfrom __future__ import annotations\nfrom typing import List, Dict, Iterable, Tuple\nimport numpy as np\nimport pandas as pd\n\n\n__all__ = [\n    \"rolling_zscore\",\n    \"compute_nan_drop_columns\",\n    \"resolve_transform_sets\",\n    \"apply_pipeline\",\n]\n\n\n# ---------- 0) Î°§ÎßÅ z-score (Ïù¥Î∂ÑÏÇ∞ ÏôÑÌôî) ----------\ndef rolling_zscore(\n    s: pd.Series,\n    win: int = 63,\n    min_frac: float = 0.34,\n    eps: float = 1e-9,\n) -> pd.Series:\n    \"\"\"\n    Î°§ÎßÅ ÌèâÍ∑†/ÌëúÏ§ÄÌé∏Ï∞®Î°ú z-score ÌëúÏ§ÄÌôî.\n    - min_periods = max(10, win * min_frac)\n    \"\"\"\n    min_periods = max(10, int(round(win * float(min_frac))))\n    m = s.rolling(win, min_periods=min_periods).mean()\n    v = s.rolling(win, min_periods=min_periods).std()\n    return (s - m) / (v + eps)\n\n\n# ---------- 1) NaN 50%‚Üë ÎìúÎ°≠ Î™©Î°ù Í≥ÑÏÇ∞ ----------\ndef compute_nan_drop_columns(\n    train_df: pd.DataFrame,\n    *,\n    time_col: str = \"date_id\",\n    nan_thresh: float = 0.50,\n    train_only_targets: Iterable[str] = (\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"),\n    test_only_cols: Iterable[str] = (\"is_scored\", \"lagged_forward_returns\", \"lagged_risk_free_rate\", \"lagged_market_forward_excess_returns\"),\n    extra_exempt: Iterable[str] | None = None,\n) -> List[str]:\n    \"\"\"\n    Train Í∏∞Ï§Ä Í≤∞Ï∏° ÎπÑÏú® >= nan_thresh Ïù∏ Ïª¨Îüº Î™©Î°ùÏùÑ Î∞òÌôò.\n    - ÏãúÍ∞Ñ/ÌÉÄÍπÉ/ÌÖåÏä§Ìä∏ Ï†ÑÏö©/extra_exempt Ïª¨ÎüºÏùÄ ÎìúÎ°≠ ÎåÄÏÉÅÏóêÏÑú Ï†úÏô∏.\n    \"\"\"\n    exempt = set([time_col, *train_only_targets, *test_only_cols])\n    if extra_exempt:\n        exempt |= set(extra_exempt)\n\n    nan_ratio = train_df.isna().mean()\n    to_drop = [c for c, r in nan_ratio.items() if (r >= float(nan_thresh)) and (c not in exempt)]\n    return to_drop\n\n\n# ---------- 2) Î≥ÄÌôò ÎåÄÏÉÅ ÍµêÏßëÌï© Ï†ïÎ¶¨ ----------\ndef resolve_transform_sets(\n    df: pd.DataFrame,\n    *,\n    diff_cols_given: Iterable[str],\n    seasonal_periods: Dict[str, int],\n    autocorr_cols_given: Iterable[str],\n    hetero_cols_given: Iterable[str],\n) -> Tuple[List[str], Dict[str, int], List[str], List[str]]:\n    \"\"\"\n    Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Ïã§Ï†ú Ï°¥Ïû¨ÌïòÎäî Ïª¨ÎüºÎßå ÎÇ®Í≤® Î≥ÄÌôò Í≥ÑÌöçÏùÑ Ï†ïÏ†ú.\n    Î∞òÌôò: (diff_cols, seasonal_info, lag_cols, hetero_cols)\n    \"\"\"\n    cols = set(df.columns)\n\n    def _intersect_list(cands: Iterable[str]) -> List[str]:\n        return [c for c in cands if c in cols]\n\n    diff_cols = _intersect_list(diff_cols_given)\n    lag_cols = _intersect_list(autocorr_cols_given)\n    hetero_cols = _intersect_list(hetero_cols_given)\n    seasonal_info = {c: int(p) for c, p in seasonal_periods.items() if c in cols}\n\n    return diff_cols, seasonal_info, lag_cols, hetero_cols\n\n\n# ---------- 3) Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏ ----------\ndef apply_pipeline(\n    df: pd.DataFrame,\n    *,\n    time_col: str = \"date_id\",\n    cols_to_drop: Iterable[str] = (),\n    diff_cols: Iterable[str] = (),\n    seasonal_info: Dict[str, int] = (),\n    lag_cols: Iterable[str] = (),\n    hetero_cols: Iterable[str] = (),\n    white_noise_drop: Iterable[str] = (),\n    z_win: int = 63,\n    protect_train_only: Iterable[str] = (\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"),\n    protect_test_only: Iterable[str] = (\"is_scored\", \"lagged_forward_returns\", \"lagged_risk_free_rate\", \"lagged_market_forward_excess_returns\"),\n    fill_eps: float = 0.0,\n) -> pd.DataFrame:\n    \"\"\"\n    Ï∞®Î∂Ñ/Í≥ÑÏ†àÏ∞®Î∂Ñ/ÎûòÍ∑∏/Î°§ÎßÅ z-scoreÎ•º Ï†ÅÏö©ÌïòÍ≥† Í≤∞Ï∏°ÏùÑ ÏãúÍ≥ÑÏó¥ Î∞©ÏãùÏúºÎ°ú Ï≤òÎ¶¨.\n    - drop: white_noise_drop ‚à™ cols_to_drop\n    - diff: ÏßÄÏ†ï Ïª¨Îüº ÏπòÌôò(ÏõêÎ≥∏ ÎåÄÏã† 1Ï∞® Ï∞®Î∂Ñ)\n    - seasonal: ÏßÄÏ†ï Ïª¨Îüº ÏπòÌôò(x_t - x_{t-p})\n    - lag: _lag1 Ïª¨Îüº Ï∂îÍ∞Ä(ÏõêÎ≥∏ Ïú†ÏßÄ)\n    - hetero: _z{z_win} Ïª¨Îüº Ï∂îÍ∞Ä(ÏõêÎ≥∏ Ïú†ÏßÄ)\n    - Í≤∞Ï∏°: (ÌîºÏ≤ò) ffill ‚Üí bfill ‚Üí fillna(fill_eps), (Î≥¥Ìò∏ Ïª¨Îüº) bfill\n\n    Î∞òÌôò: Ï≤òÎ¶¨Îêú DataFrame (ÏûÖÎ†•Í≥º Í∞ôÏùÄ Ïª¨Îüº ÏàúÏÑú/Ï†ïÎ†¨ÏùÄ Î≥¥Ïû•ÌïòÏßÄ ÏïäÏùå)\n    \"\"\"\n    out = df.sort_values(time_col).reset_index(drop=True).copy()\n\n    # 0) ÎìúÎ°≠\n    drop_now = list({*list(cols_to_drop), *list(white_noise_drop)} & set(out.columns))\n    if drop_now:\n        out = out.drop(columns=drop_now, errors=\"ignore\")\n\n    # 1) Ï∞®Î∂Ñ(ÏπòÌôò)\n    for c in diff_cols:\n        if c in out.columns:\n            out[c] = out[c].diff()\n\n    # 2) Í≥ÑÏ†àÏ∞®Î∂Ñ(ÏπòÌôò)\n    for c, p in seasonal_info.items():\n        if c in out.columns:\n            out[c] = out[c] - out[c].shift(int(p))\n\n    # 3) ÎûòÍπÖ Ï∂îÍ∞Ä(ÏõêÎ≥∏ Ïú†ÏßÄ)\n    for c in lag_cols:\n        if c in out.columns:\n            out[f\"{c}_lag1\"] = out[c].shift(1)\n\n    # 4) Ïù¥Î∂ÑÏÇ∞ ÏôÑÌôî(z-score Ï∂îÍ∞Ä, ÏõêÎ≥∏ Ïú†ÏßÄ)\n    for c in hetero_cols:\n        if c in out.columns:\n            out[f\"{c}_z{int(z_win)}\"] = rolling_zscore(out[c], win=int(z_win))\n\n    # 5) Í≤∞Ï∏° Ï≤òÎ¶¨\n    protect_cols = {time_col, *protect_train_only, *protect_test_only}\n    feat_cols = [c for c in out.columns if c not in protect_cols]\n\n    if feat_cols:\n        out[feat_cols] = out[feat_cols].ffill().bfill().fillna(fill_eps)\n\n    for c in list(protect_cols & set(out.columns)):\n        if out[c].dtype.kind in \"fc\":  # ÏàòÏπòÌòïÎßå\n            out[c] = out[c].bfill()\n\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:44.481049Z","iopub.execute_input":"2025-10-03T19:02:44.481373Z","iopub.status.idle":"2025-10-03T19:02:44.501588Z","shell.execute_reply.started":"2025-10-03T19:02:44.481349Z","shell.execute_reply":"2025-10-03T19:02:44.500481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. train","metadata":{}},{"cell_type":"code","source":"# ===== Ïó¨Í∏∞Îäî ÏúÑÏóêÏÑú Ï†úÍ≥µÎêú ÌèâÍ∞ÄÏãù Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© =====\nMIN_INVESTMENT = 0\nMAX_INVESTMENT = 2\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    solution = solution.copy()\n    solution['position'] = submission['prediction']\n\n    if solution['position'].max() > MAX_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n    if solution['position'].min() < MIN_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n\n    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n\n    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n    strategy_std = solution['strategy_returns'].std()\n\n    trading_days_per_yr = 252\n    if strategy_std == 0:\n        raise ZeroDivisionError\n    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n\n    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n    market_std = solution['forward_returns'].std()\n\n    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n\n    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n    vol_penalty = 1 + excess_vol\n\n    return_gap = max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n    return_penalty = 1 + (return_gap**2) / 100\n\n    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n    return min(float(adjusted_sharpe), 1_000_000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:44.503962Z","iopub.execute_input":"2025-10-03T19:02:44.504256Z","iopub.status.idle":"2025-10-03T19:02:44.530508Z","shell.execute_reply.started":"2025-10-03T19:02:44.504233Z","shell.execute_reply":"2025-10-03T19:02:44.529405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# Hull Tactical ‚Äî LGBM Training Utilities (functions only)\n#  - ÌååÏùº I/O / Ï†úÏ∂ú Ï†ÄÏû• / Îç∞Ïù¥ÌÑ∞ Î°úÎìúÎäî Ìè¨Ìï®ÌïòÏßÄ ÏïäÏùå\n#  - ÌïôÏäµÏóê ÌïÑÏöîÌïú Ìï®ÏàòÎßå Ï†úÍ≥µÌï©ÎãàÎã§.\n# =========================================================\n# ===== Ïú†Ìã∏ Î≥ÄÍ≤Ω: risk_free_rate Î∞òÌôò + CVÏóêÏÑú score() ÏÇ¨Ïö© =====\nfrom typing import Dict, List, Tuple, Iterable, Optional\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\ndef default_lgbm_params(seed: int = 42) -> Dict:\n    return dict(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.019018772389315385,\n        n_estimators=200,\n        num_leaves=128,\n        max_depth=6,\n        min_child_samples=20,\n        subsample=0.683544047260533,\n        colsample_bytree=0.9617596652953478,\n        reg_alpha=0.006879339704011885,\n        reg_lambda=6.791029724043033,\n        random_state=seed,\n        verbose=-1,\n        n_jobs=-1,\n        # ÌïÑÏöîÌïòÎ©¥: subsample_freq=1,\n    )\n\ndef select_feature_columns(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    *,\n    time_col: str = \"date_id\",\n    train_only: Iterable[str] = (\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"),\n    test_only: Iterable[str] = (\"is_scored\",\"lagged_forward_returns\",\"lagged_risk_free_rate\",\"lagged_market_forward_excess_returns\"),\n) -> List[str]:\n    ban = set([time_col, *train_only, *test_only])\n    tr_feats = [c for c in train_df.columns if c not in ban]\n    te_feats = [c for c in test_df.columns  if c not in ban]\n    feats = sorted(list(set(tr_feats).intersection(te_feats)))\n    return feats\n\ndef prepare_train_test_matrices(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    *,\n    target_col: str = \"forward_returns\",\n    time_col: str = \"date_id\",\n    train_only: Iterable[str] = (\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"),\n    test_only:  Iterable[str] = (\"is_scored\",\"lagged_forward_returns\",\"lagged_risk_free_rate\",\"lagged_market_forward_excess_returns\"),\n) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, List[str], pd.Series]:\n    tr = train_df.sort_values(time_col).reset_index(drop=True)\n    te = test_df.sort_values(time_col).reset_index(drop=True)\n\n    feat_cols = select_feature_columns(tr, te, time_col=time_col, train_only=train_only, test_only=test_only)\n\n    X = tr[feat_cols]\n    y = tr[target_col].astype(float)\n    X_test = te[feat_cols]\n    # ‚¨áÔ∏è Ï∂îÍ∞Ä: Í≤ÄÏ¶ù Îïå score() Í≥ÑÏÇ∞Ïö© risk-free\n    rf = tr[\"risk_free_rate\"].astype(float).reset_index(drop=True)\n\n    return X, y, X_test, feat_cols, rf\n\ndef _pred_to_position(pred: np.ndarray) -> np.ndarray:\n    \"\"\"Î™®Îç∏ ÏòàÏ∏°ÏùÑ [0,2] Ìè¨ÏßÄÏÖòÏúºÎ°ú Îß§Ìïë (Í∏∞Î≥∏: Îã®Ïàú clip).\"\"\"\n    return np.clip(pred, MIN_INVESTMENT, MAX_INVESTMENT)\n\ndef time_series_cv_fit(\n    X, y, X_test=None, *,\n    risk_free_series=None,\n    params=None,\n    n_splits=5,\n    early_stopping_rounds=500,   # ‚Üê Ïó¨Í∏∞Î•º 0Ïù¥ÎÇò NoneÏúºÎ°ú Ï£ºÎ©¥ ES ÎπÑÌôúÏÑ±Ìôî\n    log_every_n=200,\n    return_models=True,\n    seed=42,\n) -> Dict:\n    if params is None:\n        params = default_lgbm_params(seed=seed)\n\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    oof = np.zeros(len(X), dtype=float)\n    preds_test = np.zeros(len(X_test), dtype=float) if X_test is not None else None\n\n    models, imps, adj_scores = [], [], []\n\n    for fold, (tr_idx, va_idx) in enumerate(tscv.split(X, y), 1):\n        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]\n        rf_va = None if risk_free_series is None else risk_free_series.iloc[va_idx].values\n\n        model = lgb.LGBMRegressor(**params)\n\n        # ‚úÖ callbacksÎ•º Ï°∞Í±¥Î∂ÄÎ°ú Íµ¨ÏÑ±\n        cbs = [lgb.log_evaluation(log_every_n)]\n        use_es = bool(early_stopping_rounds) and early_stopping_rounds > 0\n        if use_es:\n            cbs.insert(0, lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False))\n\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=cbs,\n        )\n\n        # ‚úÖ ESÍ∞Ä Í∫ºÏ†∏ ÏûàÏúºÎ©¥ Ï†ÑÏ≤¥ Ìä∏Î¶¨Î°ú ÏòàÏ∏°(num_iteration=None)\n        num_it = model.best_iteration_ if use_es else None\n\n        pred_va = model.predict(X_va, num_iteration=num_it)\n        oof[va_idx] = pred_va\n\n        if rf_va is not None:\n            sol = pd.DataFrame({\"forward_returns\": y_va.values, \"risk_free_rate\": rf_va})\n            sub = pd.DataFrame({\"prediction\": _pred_to_position(pred_va)})\n            try:\n                adj = score(sol, sub, row_id_column_name=\"row_id\")\n            except Exception:\n                adj = -np.inf\n            adj_scores.append(adj)\n            print(f\"[Fold {fold}] Adjusted Sharpe (metric): {adj:.6f}\")\n\n        if X_test is not None:\n            preds_test += model.predict(X_test, num_iteration=num_it) / n_splits\n\n        if return_models:\n            models.append(model)\n            try:\n                imps.append(pd.Series(model.feature_importances_, index=X.columns, name=f\"fold{fold}\"))\n            except Exception:\n                pass\n\n    rmse = float(mean_squared_error(y, oof, squared=False))\n    out = {\"oof\": oof, \"rmse\": rmse, \"preds_test\": preds_test}\n    if adj_scores:\n        out[\"adj_sharpe_per_fold\"] = adj_scores\n        out[\"adj_sharpe_mean\"] = float(np.mean(adj_scores))\n    if return_models:\n        out[\"models\"] = models\n        if imps:\n            imp_df = pd.concat(imps, axis=1); imp_df[\"mean\"] = imp_df.mean(axis=1)\n            out[\"importances\"] = imp_df\n    return out\n\ndef fit_full_and_predict(\n    X: pd.DataFrame,\n    y: pd.Series,\n    X_test: pd.DataFrame,\n    *,\n    params: Optional[Dict] = None,\n    num_iteration: Optional[int] = None,\n    seed: int = 42,\n) -> Tuple[lgb.LGBMRegressor, np.ndarray]:\n    if params is None:\n        params = default_lgbm_params(seed=seed)\n    model = lgb.LGBMRegressor(**params)\n    model.fit(X, y)\n    preds = model.predict(X_test, num_iteration=num_iteration)\n    return model, preds\n\ndef feature_importance_df(models: List[lgb.LGBMRegressor], feature_names: Iterable[str]) -> pd.DataFrame:\n    rows = []\n    for i, m in enumerate(models, 1):\n        try:\n            rows.append(pd.Series(m.feature_importances_, index=list(feature_names), name=f\"fold{i}\"))\n        except Exception:\n            continue\n    if not rows:\n        return pd.DataFrame(index=list(feature_names))\n    imp = pd.concat(rows, axis=1)\n    imp[\"mean\"] = imp.mean(axis=1)\n    return imp.sort_values(\"mean\", ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:44.531585Z","iopub.execute_input":"2025-10-03T19:02:44.531987Z","iopub.status.idle":"2025-10-03T19:02:51.174351Z","shell.execute_reply.started":"2025-10-03T19:02:44.531962Z","shell.execute_reply":"2025-10-03T19:02:51.173281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================\n# Optuna tuner for LightGBM (Adjusted Sharpe)\n# =========================================\nimport json, os\nimport numpy as np\nimport pandas as pd\n\n# Optuna import (ÏóÜÏúºÎ©¥ ÏÑ§Ïπò)\ntry:\n    import optuna\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"optuna\"])\n    import optuna\n\n# ÏïàÏ†Ñ: Ìè¨ÏßÄÏÖò ÌÅ¥Î¶Ω ÏÉÅÏàò (ÏóÜÏúºÎ©¥ NameError)\nMIN_INVESTMENT = 0.0\nMAX_INVESTMENT = 2.0\n\n# ÎåÄÌöå Ï†êÏàò Ìï®ÏàòÍ∞Ä ÏóÜÏúºÎ©¥ RMSEÎ°ú Ìè¥Î∞±\ntry:\n    _HAS_SCORE = callable(score)\nexcept Exception:\n    _HAS_SCORE = False\n\ndef _objective_factory(X, y, rf, n_splits=5, seed=42, early_stopping_rounds=500, log_every_n=200):\n    def objective(trial: optuna.Trial) -> float:\n        # ---- search space ----\n        params = dict(\n            objective=\"regression\",\n            metric=\"rmse\",\n            # ÌïôÏäµÎ•†/Ìä∏Î¶¨Í∑úÎ™®\n            learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 0.05, log=True),\n            n_estimators=trial.suggest_int(\"n_estimators\", 5_000, 40_000, step=1_000),\n            num_leaves=trial.suggest_int(\"num_leaves\", 31, 511, step=8),\n            max_depth=trial.suggest_int(\"max_depth\", 3, 12),\n            min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 200),\n            # Í∑úÏ†ú/ÏÉòÌîåÎßÅ\n            subsample=trial.suggest_float(\"subsample\", 0.4, 1.0),\n            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n            reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-3, 100.0, log=True),\n            reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-3, 100.0, log=True),\n            random_state=seed,\n            verbose=-1,\n            n_jobs=-1,\n        )\n\n        # CV ÌïôÏäµ (ÎÑ§Í∞Ä ÎßåÎì† Ìï®Ïàò ÏÇ¨Ïö©; Î©îÌä∏Î¶≠ÏùÄ ÎÇ¥Î∂ÄÏóêÏÑú Í≥ÑÏÇ∞)\n        out = time_series_cv_fit(\n            X, y, X_test=None,\n            risk_free_series=rf,\n            params=params,\n            n_splits=n_splits,\n            early_stopping_rounds=early_stopping_rounds,\n            log_every_n=log_every_n,\n            return_models=False,\n            seed=seed,\n        )\n\n        # OptunaÏùò Î™©Ìëú: ÎåÄÌöå Î©îÌä∏Î¶≠ ÏµúÎåÄÌôî (ÏóÜÏúºÎ©¥ -RMSE ÏµúÏÜåÌôîÏôÄ ÎèôÏπò)\n        if _HAS_SCORE and (\"adj_sharpe_mean\" in out):\n            return float(out[\"adj_sharpe_mean\"])   # ‚Üë maximize\n        else:\n            return -float(out[\"rmse\"])             # ‚Üë maximize(ÏùåÏàò RMSE)\n\n    return objective\n\n\ndef run_optuna_tuning(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    *,\n    target_col=\"forward_returns\",\n    time_col=\"date_id\",\n    n_trials: int = 50,\n    n_splits: int = 5,\n    seed: int = 42,\n    study_name: str = \"lgbm_adj_sharpe\",\n    save_path: str = \"/kaggle/working/best_lgbm_params.json\",\n):\n    # ÌïôÏäµ/Ï∂îÎ°† ÌñâÎ†¨ Ï§ÄÎπÑ (+ risk_free series)\n    X, y, X_test, feat_cols, rf = prepare_train_test_matrices(\n        train_df, test_df,\n        target_col=target_col,\n        time_col=time_col\n    )\n\n    # Î™©Ìëú Ìï®Ïàò\n    objective = _objective_factory(X, y, rf, n_splits=n_splits, seed=seed)\n\n    # Pruner/ÏÉòÌîåÎü¨(Ïû¨ÌòÑÏÑ± Ìè¨Ìï®)\n    sampler = optuna.samplers.TPESampler(seed=seed)\n    pruner  = optuna.pruners.MedianPruner(n_warmup_steps=10)\n\n    study = optuna.create_study(direction=\"maximize\", study_name=study_name, sampler=sampler, pruner=pruner)\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n\n    print(f\"[Optuna] Best value={study.best_value:.6f}\")\n    print(\"[Optuna] Best params:\", study.best_params)\n\n    # Ï†ÄÏû• (ÏÑúÎ≤ÑÍ∞Ä ÏùΩÏùÑ Ïàò ÏûàÍ≤å)\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, \"w\") as f:\n        json.dump(study.best_params, f)\n    print(f\"[Optuna] Saved best params ‚Üí {save_path}\")\n\n    return study\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:51.175377Z","iopub.execute_input":"2025-10-03T19:02:51.176017Z","iopub.status.idle":"2025-10-03T19:02:51.191528Z","shell.execute_reply.started":"2025-10-03T19:02:51.175984Z","shell.execute_reply":"2025-10-03T19:02:51.190664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nstudy = run_optuna_tuning(\n    train_df, test_df,\n    n_trials=60,       # Ïòà: 60 Ìä∏ÎùºÏù¥Ïñº\n    n_splits=5,        # TSCV Ìè¥Îìú Ïàò\n    seed=42,\n    save_path=\"/kaggle/working/best_lgbm_params.json\"\n)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:51.192662Z","iopub.execute_input":"2025-10-03T19:02:51.193026Z","iopub.status.idle":"2025-10-03T19:02:51.231012Z","shell.execute_reply.started":"2025-10-03T19:02:51.192995Z","shell.execute_reply":"2025-10-03T19:02:51.229604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. submission","metadata":{}},{"cell_type":"code","source":"# ----- Kelly & Volatility guard (add this near the other globals) -----\nfrom collections import deque\n\n# Kelly config\nKELLY_WINDOW = 20       # ÏµúÍ∑º ÏòÅÏóÖÏùº Ï†ïÎãµÎ•†\nKELLY_SHRINK = 1      # Ï∂ïÏÜåÍ≥ÑÏàò: Î©ÄÌã∞Ìîå Î≤îÏúÑ [0.5, 1.5]\n\n# Kelly state\n_kelly_hits = deque(maxlen=KELLY_WINDOW)   # 1=Ï†ïÎãµ, 0=Ïò§Îãµ\n_prev_raw_pred_sign = None                 # ÏßÅÏ†Ñ ÏòàÏ∏° Î∂ÄÌò∏ (-1/0/+1)\n\ndef _kelly_multiplier_from_hits() -> float:\n    \"\"\"ÏµúÍ∑º Ï†ïÎãµÎ•† pÎ°ú Î©ÄÌã∞Ìîå Í≥ÑÏÇ∞: mult = 1 + KELLY_SHRINK*(2p-1) ‚àà [1-KS, 1+KS].\"\"\"\n    n = len(_kelly_hits)\n    if n == 0:\n        return 1.0\n    p = (sum(_kelly_hits) + 1) / (n + 2)  # ÎùºÌîåÎùºÏä§ Ïä§Î¨¥Îî©\n    f = 2.0 * p - 1.0\n    mult = 1.0 + KELLY_SHRINK * f\n    return float(np.clip(mult, 1.0 - KELLY_SHRINK, 1.0 + KELLY_SHRINK))\n\n# ---- Simple volatility guard (safe defaults) ----\n_VOL_EWMA = None       # EWMA of strategy daily variance\n_VOL_ALPHA = 0.1       # smoothing for variance\n_VOL_SOFT_CAP = 0.24   # Ïó∞ÌôòÏÇ∞ 24% Ïù¥ÏÉÅÏù¥Î©¥ Í∞êÏá† ÏãúÏûë (ÏõêÌïòÎ©¥ Ï°∞Ï†ï)\n\ndef _update_vol_guard(latest_df: pd.DataFrame, alloc: float) -> None:\n    \"\"\"Ïñ¥Ï†ú Í≤∞Í≥ºÏôÄ ÌòÑÏû¨ Î∞∞Î∂ÑÏúºÎ°ú Ï†ÑÎûµ ÏùºÎ≥ÄÎèô(Ï†úÍ≥±ÏàòÏùµ) EWMA ÏóÖÎç∞Ïù¥Ìä∏.\"\"\"\n    global _VOL_EWMA\n    col = None\n    if \"lagged_forward_returns\" in latest_df.columns:\n        col = \"lagged_forward_returns\"\n    elif \"lagged_market_forward_excess_returns\" in latest_df.columns:\n        col = \"lagged_market_forward_excess_returns\"\n    if col is None:\n        return\n    r = latest_df[col].iloc[0]\n    if pd.isna(r):\n        return\n    strat_ret = float(alloc) * float(r)\n    var = strat_ret * strat_ret\n    _VOL_EWMA = var if _VOL_EWMA is None else (1 - _VOL_ALPHA) * _VOL_EWMA + _VOL_ALPHA * var\n\ndef _vol_damper() -> float:\n    \"\"\"Ïó∞ÌôòÏÇ∞ Î≥ÄÎèôÏÑ±Ïù¥ ÏÜåÌîÑÌä∏ Ï∫°ÏùÑ ÎÑòÏúºÎ©¥ 1.0‚Üí0.6ÏúºÎ°ú ÏÑ†Ìòï Í∞êÏá†.\"\"\"\n    if _VOL_EWMA is None:\n        return 1.0\n    ann_vol = np.sqrt(_VOL_EWMA) * np.sqrt(252.0)\n    if ann_vol <= _VOL_SOFT_CAP:\n        return 1.0\n    # Ï∫° ÎåÄÎπÑ ÏµúÎåÄ 2Î∞∞ÏóêÏÑú 0.6ÍπåÏßÄ Í∞êÏá†\n    excess = np.clip((ann_vol - _VOL_SOFT_CAP) / _VOL_SOFT_CAP, 0.0, 1.0)\n    return float(1.0 - 0.4 * excess)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:51.232074Z","iopub.execute_input":"2025-10-03T19:02:51.232552Z","iopub.status.idle":"2025-10-03T19:02:51.252413Z","shell.execute_reply.started":"2025-10-03T19:02:51.232521Z","shell.execute_reply":"2025-10-03T19:02:51.251259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# Hull Tactical ‚Äî Online Inference Server\n#  - LightGBM (Optuna best params) + Í∞ÑÏÜå ÌååÏù¥ÌîÑÎùºÏù∏ Ïú†Ìã∏(Ìè¥Î∞±)\n#  - A) Base ÌîºÏ≤ò ÏÑ∏Ìä∏\n#  - B) Core(+exp) ÌååÏÉù ÌîºÏ≤ò Top-K (ÏÑ†ÌÉùÏ†Å, Í∏∞Î≥∏ÏùÄ A ÏÇ¨Ïö©)\n#  - OoF Calibration: P(up) (Logit), Œº (Linear)\n#  - Kelly Î†àÎ≤ÑÎ¶¨ÏßÄ ÏµúÎåÄ 2x + Vol-guard + Ïä§Î¨¥Îî©\n# =========================================================\nimport os\nfrom pathlib import Path\nimport time\nfrom itertools import combinations\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport kaggle_evaluation.default_inference_server\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\n# ---------------- Optional: Polars ----------------\ntry:\n    import polars as pl\n    _POLARS_AVAILABLE = True\nexcept Exception as e:\n    pl = None\n    _POLARS_AVAILABLE = False\n    print(f\"[WARN] Polars unavailable; falling back to Pandas-only mode: {e}\")\n\n# =================================================\n# Í≤ΩÎ°ú & ÏÉÅÏàò\n# =================================================\nDATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction\")\nTRAIN_CSV = DATA_PATH / \"train.csv\"\nTEST_CSV  = DATA_PATH / \"test.csv\"  # local gateway Ïö©\nTIME_COL  = \"date_id\"\nTARGET    = \"forward_returns\"\n\nTRAIN_ONLY  = (\"forward_returns\",\"risk_free_rate\",\"market_forward_excess_returns\")\nTEST_ONLY   = (\"is_scored\",\"lagged_forward_returns\",\"lagged_risk_free_rate\",\"lagged_market_forward_excess_returns\")\n\n# ==== Transform plan (same as diagnostics) ====\nDIFF_COLS_GIVEN     = ['E10','E11','E12','E2','E20','E3','E5','E6','E9','I4']\nSEASONAL_PERIODS    = {'D4':2,'D5':42,'D6':42,'D7':44,'D8':42}\nAUTOCORR_COLS_GIVEN = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','E1']\nHETERO_COLS_GIVEN   = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','E1']\nWHITE_NOISE_DROP    = ['P3']  # ÏôÑÏ†ÑÎ∞±ÏÉâÏû°ÏùåÏúºÎ°ú Í∞ÑÏ£ºÌïòÏó¨ Ï†úÍ±∞Ìï† ÌõÑÎ≥¥\n\n# NaN-drop threshold & z-score window\nNAN_THRESH = 0.50\nZ_WIN      = 63\nFILL_EPS   = 0.0\n\n# CV & model params\nSEED         = 42\nEARLY_STOP   = 300\nLOG_EVERY    = 200\nVAL_HORIZON  = 180     # ÏµúÍ∑º 180ÏòÅÏóÖÏùº Í≤ÄÏ¶ù\n\n# Allocation post-processing\nCLIP_MIN = 0.0\nCLIP_MAX = 2.0              # Kelly Ìè¨Ìï® ÏµúÏ¢Ö Î†àÎ≤ÑÎ¶¨ÏßÄ ÏÉÅÌïú\nSCALE_LOW_VOL  = 700.0\nSCALE_HIGH_VOL = 500.0\nSMOOTH_ALPHA   = 0.25\nVOL_COL        = \"V1\"\n\n# Vol guard (Ïã§ÌòÑÎ≥ÄÎèôÏÑ± ÌÉÄÍ≤ü)\nTARGET_DAILY_VOL = float(os.getenv(\"TARGET_DAILY_VOL\", \"0.012\"))  # 1.2%/day\nVOL_GUARD_WIN    = int(os.getenv(\"VOL_GUARD_WIN\", \"90\"))\n\n# ====== Global state (online) ======\n_models: list[lgb.LGBMRegressor] = []                  # A) base tuned models\n_feat_cols: list[str] = []\n_train_like_schema: list[str] = []\n_drop_cols: list[str] = []\n_diff_cols: list[str] = []\n_seasonal_info: dict[str,int] = {}\n_lag_cols: list[tuple[str,int]] = []     # (col, lag)\n_hetero_cols: list[str] = []\n_last_allocation: float = 0.0\n_v1_median: float = 0.0\n_const_cols: list[str] = []\n_prev_raw_pred_sign = None\n_kelly_hits = []\n_test_buffer: pd.DataFrame = pd.DataFrame()\n\n# For B) core+exp pruned tuned models (ÏÑ†ÌÉù ÏÇ¨Ïö© Í∞ÄÎä•)\nmodels_coreexp_sel: list[lgb.LGBMRegressor] = []\n_feat_cols_coreexp_sel: list[str] = []\n\n# ---- Optuna Best Params (provided) ----\n_LGBM_TUNED = {\n    \"boosting_type\": \"gbdt\",\n    \"max_depth\": 6,\n    \"num_leaves\": 97,\n    \"learning_rate\": 0.04765455492382819,\n    \"min_child_samples\": 606,\n    \"feature_fraction\": 0.9267638600380922,\n    \"bagging_fraction\": 0.5538379982286759,\n    \"bagging_freq\": 7,\n    \"lambda_l1\": 0.025572847585528788,\n    \"lambda_l2\": 0.008280168455497349,\n    \"min_gain_to_split\": 1.2392218344254143e-07,\n}\n\n# =================================================\n# Ïú†Ìã∏(Ìè¥Î∞±) ‚Äî ÌîÑÎ°úÏ†ùÌä∏ ÎÇ¥ Ï†ÑÏö© Ïú†Ìã∏Ïù¥ ÏûàÏúºÎ©¥ import ÌïòÏÑ∏Ïöî.\n# =================================================\ndef default_lgbm_params(seed: int = 42) -> dict:\n    \"\"\"ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í.\"\"\"\n    return dict(\n        objective=\"regression\",\n        metric=\"rmse\",\n        max_depth=-1,\n        num_leaves=63,\n        learning_rate=0.05,\n        min_child_samples=20,\n        feature_fraction=0.9,\n        bagging_fraction=0.9,\n        bagging_freq=1,\n        lambda_l1=0.0,\n        lambda_l2=0.0,\n        min_gain_to_split=0.0,\n    )\n\ndef compute_nan_drop_columns(\n    df: pd.DataFrame,\n    time_col: str,\n    nan_thresh: float = 0.5,\n    train_only_targets=(),\n    test_only_cols=(),\n    extra_exempt=None,\n):\n    exempt = set([time_col, *train_only_targets, *test_only_cols])\n    if extra_exempt:\n        exempt|= set(extra_exempt)\n    drop = []\n    for c in df.columns:\n        if c in exempt: \n            continue\n        frac = df[c].isna().mean()\n        if frac >= nan_thresh:\n            drop.append(c)\n    # white-noise ÌõÑÎ≥¥ Ï†úÍ±∞\n    for c in WHITE_NOISE_DROP:\n        if c in df.columns and c not in exempt and c not in drop:\n            drop.append(c)\n    return drop\n\ndef resolve_transform_sets(\n    df: pd.DataFrame,\n    diff_cols_given=None,\n    seasonal_periods=None,\n    autocorr_cols_given=None,\n    hetero_cols_given=None,\n):\n    diff_cols = [c for c in (diff_cols_given or []) if c in df.columns]\n    seasonal_info = {c:p for c,p in (seasonal_periods or {}).items() if c in df.columns}\n    # Í∞ÑÎã®Ìûà 1,5,21 ÎûòÍ∑∏ ÏÉùÏÑ±\n    lag_cols = []\n    for c in (autocorr_cols_given or []):\n        if c in df.columns:\n            for L in (1,5,21):\n                lag_cols.append((c, L))\n    hetero_cols = [c for c in (hetero_cols_given or []) if c in df.columns]\n    return diff_cols, seasonal_info, lag_cols, hetero_cols\n\ndef _zscore(x: pd.Series, win: int) -> pd.Series:\n    r = x.rolling(win, min_periods=max(5, win//5))\n    m = r.mean(); s = r.std(ddof=0)\n    z = (x - m) / (s.replace(0.0, np.nan))\n    return z\n\ndef apply_pipeline(\n    df_in: pd.DataFrame,\n    time_col: str,\n    cols_to_drop=None,\n    diff_cols=None,\n    seasonal_info=None,\n    lag_cols=None,\n    hetero_cols=None,\n    white_noise_drop=None,\n    z_win=63,\n    protect_train_only=(),\n    protect_test_only=(),\n    fill_eps=0.0,\n):\n    df = df_in.sort_values(time_col).reset_index(drop=True).copy()\n    cols_to_drop = set(cols_to_drop or [])\n    keep_all = set([time_col, *protect_train_only, *protect_test_only])\n\n    # 1) Drop NaN-heavy & white noise\n    extra_drop = set(white_noise_drop or [])\n    drop_now = [c for c in df.columns if (c in cols_to_drop or c in extra_drop) and c not in keep_all]\n    if drop_now:\n        df = df.drop(columns=drop_now, errors=\"ignore\")\n\n    # 2) Diffs\n    for c in (diff_cols or []):\n        if c in df.columns:\n            df[f\"{c}_diff1\"] = df[c].diff()\n\n    # 3) Seasonality (rolling mean)\n    for c, p in (seasonal_info or {}).items():\n        if c in df.columns:\n            df[f\"{c}_ma{p}\"] = pd.to_numeric(df[c], errors=\"coerce\").rolling(p, min_periods=max(2, p//2)).mean()\n\n    # 4) Lags\n    for c, L in (lag_cols or []):\n        if c in df.columns:\n            df[f\"{c}_lag{L}\"] = df[c].shift(L)\n\n    # 5) Heteroskedastic proxy (rolling std)\n    for c in (hetero_cols or []):\n        if c in df.columns:\n            df[f\"{c}_std{z_win}\"] = pd.to_numeric(df[c], errors=\"coerce\").rolling(z_win, min_periods=max(5, z_win//5)).std(ddof=0)\n\n    # 6) Rolling z-score (Î≥¥Ìò∏ Ïª¨Îüº Ï†úÏô∏)\n    num_cols = [c for c in df.columns if c not in keep_all]\n    for c in num_cols:\n        if c == time_col: \n            continue\n        s = pd.to_numeric(df[c], errors=\"coerce\")\n        df[c] = _zscore(s, z_win)\n\n    # ÎßàÎ¨¥Î¶¨: Í≤∞Ï∏°/Î¨¥ÌïúÏπò Ï±ÑÏö∞Í∏∞\n    df = df.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    return df\n\n# =================================================\n# Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ ÎπåÎçî\n# =================================================\ndef build_lgbm_params(seed: int) -> dict:\n    base = {}\n    try:\n        base = default_lgbm_params(seed=seed)\n    except Exception:\n        base = {}\n    base.update(_LGBM_TUNED)\n    # Î™®Îç∏ ÏÉùÏÑ±ÏûêÏóêÏÑú ÏßÅÏ†ë ÎÑ£ÏùÑ Í∞í Ï†úÍ±∞ (Ï§ëÎ≥µ/alias Î∞©ÏßÄ)\n    for k in [\"random_state\", \"n_estimators\", \"n_jobs\", \"verbosity\", \"subsample\", \"colsample_bytree\"]:\n        base.pop(k, None)\n\n    # (ÏÑ†ÌÉù) num_leaves ‚â§ 2^max_depth - 1 Í∞ïÏ†ú\n    ENFORCE_LEAF_DEPTH = False\n    if ENFORCE_LEAF_DEPTH:\n        md = base.get(\"max_depth\", -1)\n        if isinstance(md, int) and md > 0 and \"num_leaves\" in base:\n            base[\"num_leaves\"] = min(base[\"num_leaves\"], (1 << md) - 1)\n    return base\n\n# =================================================\n# Ìó¨Ìçº\n# =================================================\ndef _to_pandas(df_any) -> pd.DataFrame:\n    if isinstance(df_any, pd.DataFrame):\n        return df_any.copy()\n    if _POLARS_AVAILABLE and isinstance(df_any, pl.DataFrame):\n        return df_any.to_pandas()\n    if isinstance(df_any, dict):\n        return pd.DataFrame([df_any])\n    try:\n        return pd.DataFrame(df_any)\n    except Exception:\n        return pd.DataFrame([df_any])\n\ndef _online_pipeline_apply(df_in: pd.DataFrame) -> pd.DataFrame:\n    out = apply_pipeline(\n        df_in,\n        time_col=TIME_COL,\n        cols_to_drop=_drop_cols,\n        diff_cols=_diff_cols,\n        seasonal_info=_seasonal_info,\n        lag_cols=_lag_cols,\n        hetero_cols=_hetero_cols,\n        white_noise_drop=WHITE_NOISE_DROP,\n        z_win=Z_WIN,\n        protect_train_only=TRAIN_ONLY,\n        protect_test_only=TEST_ONLY,\n        fill_eps=FILL_EPS,\n    )\n    if _const_cols:\n        out = out.drop(columns=[c for c in _const_cols if c in out.columns], errors=\"ignore\")\n    return out\n\ndef _align_features(df_proc: pd.DataFrame) -> pd.DataFrame:\n    for c in _feat_cols:\n        if c not in df_proc.columns:\n            df_proc[c] = 0.0\n    return df_proc.reindex(columns=_feat_cols, fill_value=0.0)\n\ndef _choose_scale(row: pd.Series) -> float:\n    try:\n        v1 = float(row.get(VOL_COL, np.nan))\n    except Exception:\n        v1 = np.nan\n    if np.isnan(v1):\n        return SCALE_HIGH_VOL\n    return SCALE_LOW_VOL if v1 < _v1_median else SCALE_HIGH_VOL\n\n# =================================================\n# Start-up: Train once\n# =================================================\nt0 = time.time()\ntrain_raw = pd.read_csv(TRAIN_CSV).sort_values(TIME_COL).reset_index(drop=True)\n\n# (1) Drop columns by NaN>=50% on TRAIN\n_fast_drop = compute_nan_drop_columns(\n    train_raw,\n    time_col=TIME_COL,\n    nan_thresh=NAN_THRESH,\n    train_only_targets=TRAIN_ONLY,\n    test_only_cols=TEST_ONLY,\n    extra_exempt=None,\n)\n\n# (2) Resolve transform sets from TRAIN schema\n_diff_cols, _seasonal_info, _lag_cols, _hetero_cols = resolve_transform_sets(\n    train_raw,\n    diff_cols_given=DIFF_COLS_GIVEN,\n    seasonal_periods=SEASONAL_PERIODS,\n    autocorr_cols_given=AUTOCORR_COLS_GIVEN,\n    hetero_cols_given=HETERO_COLS_GIVEN,\n)\n\n_drop_cols = _fast_drop\n\n# (3) Apply pipeline to TRAIN\ntrain_proc = _online_pipeline_apply(train_raw)\n\n# (4) Choose base features\nban = set([TIME_COL, *TRAIN_ONLY, *TEST_ONLY])\n_feat_cols = sorted([c for c in train_proc.columns if c not in ban])\n_train_like_schema = train_proc.columns.tolist()\n\n# Compute V1 median for scaling (if available)\n_v1_median = float(pd.to_numeric(train_proc[VOL_COL], errors=\"coerce\").median()) if (VOL_COL in train_proc.columns) else 0.0\n\n# (5) Build folds: 5 expanding folds, 180-day validation windows\nX_base = train_proc[_feat_cols]\ny      = train_proc[TARGET].astype(float)\n\n# remove constant features in Base\n_const_cols = [c for c in _feat_cols if X_base[c].nunique(dropna=False) <= 1]\nif _const_cols:\n    X_base = X_base.drop(columns=_const_cols)\n    _feat_cols = [c for c in _feat_cols if c not in _const_cols]\n\nK = 5\nH = VAL_HORIZON\nN = len(X_base)\nfolds = []\nfor k in range(K):\n    val_start = max(0, N - H * (K - k))\n    val_end   = min(N, val_start + H)\n    train_end = val_start\n    if train_end > 0 and (val_end - val_start) > 0:\n        folds.append((np.arange(0, train_end), np.arange(val_start, val_end)))\nif not folds:\n    val_size = min(H, max(30, int(N * 0.15)))\n    folds = [(np.arange(0, N - val_size), np.arange(N - val_size, N))]\n\n# =========================================================\n#  FAST CORE(+exp) ÌååÏÉùÌîºÏ≤ò (ÏÑ†ÌÉù ÏÇ¨Ïö©)\n# =========================================================\nEPS_DIV   = 1e-3\nCORE_COLS = [\"M4\", \"V7\", \"E20\", \"E9\", \"P5\", \"E19\"]\nEXP_SCALE = 1.0\nEXP_CLIP  = (-8.0, 8.0)  # exp ÏïàÏ†ïÌôî\n\ndef build_coreexp_pairwise_features_fast(\n    df: pd.DataFrame,\n    core_cols=CORE_COLS,\n    eps: float = EPS_DIV,\n    exp_scale: float = EXP_SCALE,\n    exp_clip: tuple[float, float] = EXP_CLIP,\n    dtype=np.float32\n) -> pd.DataFrame:\n    present = [c for c in core_cols if c in df.columns]\n    if not present:\n        return pd.DataFrame(index=df.index)\n    vals = {c: pd.to_numeric(df[c], errors=\"coerce\").astype(dtype).to_numpy() for c in present}\n    for c in present:\n        v = vals[c]\n        v_scaled = np.clip(v * exp_scale, exp_clip[0], exp_clip[1])\n        vals[f\"exp_{c}\"] = np.exp(v_scaled).astype(dtype)\n    base = list(vals.keys())\n    cols = {name: vals[name] for name in base}\n    for a, b in combinations(base, 2):\n        av, bv = vals[a], vals[b]\n        cols[f\"{a}_plus_{b}\"]   = (av + bv).astype(dtype)\n        cols[f\"{a}_minus_{b}\"]  = (av - bv).astype(dtype)\n        cols[f\"{b}_minus_{a}\"]  = (bv - av).astype(dtype)\n        cols[f\"{a}_times_{b}\"]  = (av * bv).astype(dtype)\n        cols[f\"{a}_div_{b}\"]    = (av / (np.abs(bv) + eps)).astype(dtype)\n        cols[f\"{b}_div_{a}\"]    = (bv / (np.abs(av) + eps)).astype(dtype)\n    out = pd.DataFrame(cols, index=df.index).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    return out\n\nX_coreexp = build_coreexp_pairwise_features_fast(train_proc, CORE_COLS, EPS_DIV, EXP_SCALE, EXP_CLIP)\nconst_cols_coreexp = [c for c in X_coreexp.columns if X_coreexp[c].nunique(dropna=False) <= 1]\nif const_cols_coreexp:\n    X_coreexp = X_coreexp.drop(columns=const_cols_coreexp)\ny_coreexp = y\n\n# Similarity-pruned selection\nSIM_THRESH = 0.85\nTOPK       = 200\nif X_coreexp.shape[1] > 0:\n    corr_to_y   = X_coreexp.corrwith(y_coreexp).abs().fillna(0.0)\n    corr_ranked = corr_to_y.sort_values(ascending=False)\n    corr_mat    = X_coreexp.corr().abs().fillna(0.0)\n    selected = []\n    for feat in corr_ranked.index:\n        if len(selected) >= TOPK:\n            break\n        if not selected:\n            selected.append(feat); continue\n        if corr_mat.loc[feat, selected].max() >= SIM_THRESH:\n            continue\n        selected.append(feat)\n    X_coreexp_sel = X_coreexp[selected].copy()\n    _feat_cols_coreexp_sel = list(X_coreexp_sel.columns)\n    print(f\"[CORE+EXP] raw={X_coreexp.shape[1]}, selected={X_coreexp_sel.shape[1]} (sim_thresh={SIM_THRESH}, topk={TOPK})\")\nelse:\n    X_coreexp_sel = pd.DataFrame(index=train_proc.index)\n    _feat_cols_coreexp_sel = []\n\n# =========================================================\n#  A) Base set ‚Äî train with best params\n# =========================================================\n_models = []\ncv_r2_list = []\nfor f, (tr_idx, va_idx) in enumerate(folds, 1):\n    m = lgb.LGBMRegressor(\n        **build_lgbm_params(SEED),\n        n_estimators=5000,\n        random_state=SEED,\n        n_jobs=-1,\n        verbosity=-1,\n    )\n    m.fit(\n        X_base.iloc[tr_idx], y.iloc[tr_idx],\n        eval_set=[(X_base.iloc[va_idx], y.iloc[va_idx])],\n        eval_metric=\"rmse\",\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=EARLY_STOP, verbose=False),\n            lgb.log_evaluation(LOG_EVERY),\n        ],\n    )\n    best_it = getattr(m, \"best_iteration_\", None)\n    y_va_pred = m.predict(X_base.iloc[va_idx], num_iteration=best_it)\n    r2 = float(r2_score(y.iloc[va_idx], y_va_pred)) if y.iloc[va_idx].var() > 0 else float(\"nan\")\n    print(f\"[A-Base][Fold {f}] best_it={best_it}, R2_valid={r2:.6f}\")\n    cv_r2_list.append(r2)\n    _models.append(m)\nvalsA = [v for v in cv_r2_list if np.isfinite(v)]\nprint(f\"[A-Base][CV] R2 mean={np.mean(valsA):.6f}, median={np.median(valsA):.6f}\" if valsA else \"[A-Base][CV] all NaN\")\n\n# =========================================================\n#  B) Core+Exp pruned ‚Äî train (ÏÑ†ÌÉùÏ†Å)\n# =========================================================\nmodels_coreexp_sel = []\ncv_r2_coreexp_sel = []\nif _feat_cols_coreexp_sel:\n    for f, (tr_idx, va_idx) in enumerate(folds, 1):\n        m = lgb.LGBMRegressor(\n            **build_lgbm_params(SEED),\n            n_estimators=5000,\n            random_state=SEED,\n            n_jobs=-1,\n            verbosity=-1,\n        )\n        m.fit(\n            X_coreexp_sel.iloc[tr_idx], y_coreexp.iloc[tr_idx],\n            eval_set=[(X_coreexp_sel.iloc[va_idx], y_coreexp.iloc[va_idx])],\n            eval_metric=\"rmse\",\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=EARLY_STOP, verbose=False),\n                lgb.log_evaluation(LOG_EVERY),\n            ],\n        )\n        best_it = getattr(m, \"best_iteration_\", None)\n        yhat = m.predict(X_coreexp_sel.iloc[va_idx], num_iteration=best_it)\n        r2 = float(r2_score(y_coreexp.iloc[va_idx], yhat)) if y_coreexp.iloc[va_idx].var() > 0 else float(\"nan\")\n        cv_r2_coreexp_sel.append(r2)\n        models_coreexp_sel.append(m)\n        print(f\"[B-Core+Exp][Fold {f}] best_it={best_it}, R2_valid={r2:.6f}\")\n    valsB = [v for v in cv_r2_coreexp_sel if np.isfinite(v)]\n    print(f\"[B-Core+Exp][CV] R2 mean={np.mean(valsB):.6f}, median={np.median(valsB):.6f}\" if valsB else \"[B-Core+Exp][CV] all NaN\")\n\n# =========================================================\n#  OoF Calibration (ÌôïÎ•† p, Í∏∞ÎåÄÏàòÏùµ Œº) + payoff Ï∂îÏ†ï(g_up, g_down)\n# =========================================================\n_y_oof = np.full(len(X_base), np.nan, dtype=float)\n_yhat_oof = np.full(len(X_base), np.nan, dtype=float)\n\nfor (tr_idx, va_idx), m in zip(folds, _models):\n    best_it = getattr(m, \"best_iteration_\", None)\n    _y_oof[va_idx] = y.iloc[va_idx].to_numpy()\n    _yhat_oof[va_idx] = m.predict(X_base.iloc[va_idx], num_iteration=best_it)\n\n_mask = np.isfinite(_y_oof) & np.isfinite(_yhat_oof)\ny_oof = _y_oof[_mask]\nyhat_oof = _yhat_oof[_mask]\n\n_cal_logit = None\n_cal_lin   = None\n_g_up = 0.001\n_g_down = 0.001\n\nif y_oof.size >= 200:\n    lbl = (y_oof > 0).astype(int)\n    _cal_logit = LogisticRegression(max_iter=2000)\n    _cal_logit.fit(yhat_oof.reshape(-1,1), lbl)\n\n    _cal_lin = LinearRegression()\n    _cal_lin.fit(yhat_oof.reshape(-1,1), y_oof)\n\n    # Ï†àÎã®ÌèâÍ∑†ÏúºÎ°ú Ïñë/Ïùå ÌèâÍ∑† ÌÅ¨Í∏∞ Ï∂îÏ†ï\n    def _trimmed_mean(v):\n        lo, hi = np.nanpercentile(v, [5,95])\n        vv = v[(v>=lo)&(v<=hi)]\n        return float(np.nanmean(vv)) if vv.size>0 else float(np.nanmean(v))\n    pos = y_oof[y_oof>0]\n    neg = -y_oof[y_oof<0]\n    _g_up   = max(1e-4, _trimmed_mean(pos) if pos.size>0 else 0.001)\n    _g_down = max(1e-4, _trimmed_mean(neg) if neg.size>0 else 0.001)\nelse:\n    print(\"[cal] OoF insufficient -> use conservative defaults.\")\n\ndef _prob_up_from_yhat(yhat: float) -> float:\n    if _cal_logit is None:\n        # fallback: ÏôÑÎßåÌïú sigmoid\n        return float(1.0/(1.0+np.exp(-yhat)))\n    return float(_cal_logit.predict_proba([[yhat]])[0,1])\n\ndef _mu_from_yhat(yhat: float) -> float:\n    if _cal_lin is None:\n        return float(yhat)\n    return float(_cal_lin.predict([[yhat]])[0])\n\ndef _kelly_multiplier_from_proba(p: float, g_up: float = None, g_down: float = None, cap: float = 2.0) -> float:\n    \"\"\"Kelly for asymmetric payoffs:\n       f* = (p*g_up - (1-p)*g_down) / (g_up * g_down), multiplier = 1 + f*\n       (long-only ‚Üí ÏùåÏàòÏùº Îïê 0ÏúºÎ°ú ÌÅ¥Î¶Ω)\"\"\"\n    gu = _g_up if g_up is None else g_up\n    gd = _g_down if g_down is None else g_down\n    gu = max(gu, 1e-8); gd = max(gd, 1e-8)\n    f_star = (p*gu - (1.0-p)*gd) / (gu*gd)\n    mult = 1.0 + f_star\n    return float(np.clip(mult, 0.0, cap))\n\n# =================================================\n# Vol guard helpers\n# =================================================\n_realized_window = []  # ÏµúÍ∑º ÏùºÏùº Ïã§ÌòÑ ÏàòÏùµÎ•†\ndef _update_vol_guard(df_row: pd.DataFrame, current_alloc: float):\n    global _realized_window\n    # lagged_forward_returns ÎòêÎäî market_forward_excess_returns ÏÇ¨Ïö©\n    val = None\n    for c in [\"lagged_forward_returns\", \"lagged_market_forward_excess_returns\"]:\n        if c in df_row.columns:\n            try:\n                val = float(df_row.iloc[0][c])\n                break\n            except Exception:\n                pass\n    if val is None or not np.isfinite(val):\n        return\n    _realized_window.append(val)\n    if len(_realized_window) > VOL_GUARD_WIN:\n        _realized_window = _realized_window[-VOL_GUARD_WIN:]\n\ndef _vol_damper():\n    if TARGET_DAILY_VOL <= 0 or len(_realized_window) < max(10, VOL_GUARD_WIN//5):\n        return 1.0\n    vol = float(np.std(_realized_window, ddof=0))\n    if vol <= 1e-8:\n        return 1.0\n    return float(np.clip(TARGET_DAILY_VOL / vol, 0.0, 1.0))\n\n# =================================================\n# Predict (called once per new row)\n# =================================================\ndef predict(test_row):\n    global _test_buffer, _last_allocation, _prev_raw_pred_sign\n\n    # (0) to pandas & append to buffer\n    test_pd = _to_pandas(test_row)\n    test_pd.columns = [str(c) for c in test_pd.columns]\n\n    # (0-a) yesterday realized return vs previous sign (ÌûàÌä∏Ïú® Ï∂îÏ†Å ‚Äî ÏÑ†ÌÉù)\n    realized_prev = None\n    if \"lagged_forward_returns\" in test_pd.columns:\n        realized_prev = test_pd.loc[0, \"lagged_forward_returns\"]\n    elif \"lagged_market_forward_excess_returns\" in test_pd.columns:\n        realized_prev = test_pd.loc[0, \"lagged_market_forward_excess_returns\"]\n    if _prev_raw_pred_sign is not None and pd.notna(realized_prev):\n        r = float(realized_prev)\n        if r != 0.0:\n            _kelly_hits.append(1 if np.sign(r) == _prev_raw_pred_sign else 0)\n\n    # update buffer & re-run SAME pipeline\n    _test_buffer = pd.concat([_test_buffer, test_pd], axis=0, ignore_index=True)\n    buf_proc = _online_pipeline_apply(_test_buffer)\n    last_proc = buf_proc.tail(1).copy()\n    X_last = _align_features(last_proc)\n\n    # --- A) Base tuned ensemble ---\n    preds = []\n    for m in _models:\n        best_it = getattr(m, \"best_iteration_\", None)\n        preds.append(float(m.predict(X_last, num_iteration=best_it)[0]))\n    raw_pred = float(np.mean(preds)) if preds else 0.0\n\n    # --- (ÏÑ†ÌÉù) B) Core+Exp tunedÎ°ú ÎåÄÏ≤¥ÌïòÎ†§Î©¥ ÏïÑÎûò Ï£ºÏÑù Ìï¥Ï†ú ---\n    # if _feat_cols_coreexp_sel:\n    #     last_B = build_coreexp_pairwise_features_fast(last_proc, CORE_COLS, EPS_DIV, EXP_SCALE, EXP_CLIP)\n    #     for c in _feat_cols_coreexp_sel:\n    #         if c not in last_B.columns: last_B[c] = 0.0\n    #     last_B = last_B.reindex(columns=_feat_cols_coreexp_sel, fill_value=0.0)\n    #     predsB = [float(m.predict(last_B, num_iteration=getattr(m,\"best_iteration_\",None))[0]) for m in models_coreexp_sel]\n    #     raw_pred = float(np.mean(predsB)) if predsB else raw_pred\n\n    _prev_raw_pred_sign = 0 if raw_pred == 0 else (1 if raw_pred > 0 else -1)\n\n    # ====== CalibrationÏúºÎ°ú P(up), Œº ÏÉùÏÑ± ======\n    p_up = _prob_up_from_yhat(raw_pred)   # ÏùºÏùº ÏÉÅÏäπÌôïÎ•†\n    mu   = _mu_from_yhat(raw_pred)        # ÏùºÏùº Í∏∞ÎåÄ ÏàòÏùµÎ•†\n\n    # ====== Í∏∞Î≥∏ Ïä§ÏºÄÏùº √ó Kelly(ÏµúÎåÄ 2x) √ó Vol guard ======\n    scale = _choose_scale(last_proc.iloc[0])\n    base_alloc = np.clip(mu * scale, CLIP_MIN, CLIP_MAX)     # Œº Í∏∞Î∞ò Î≤†Ïù¥Ïä§ Ìè¨ÏßÄÏÖò\n    kelly_mult = _kelly_multiplier_from_proba(p_up, cap=2.0) # 0~2 Î∞∞\n    vol_mult   = _vol_damper()                               # 0~1 Î∞∞\n    alloc = np.clip(base_alloc * kelly_mult * vol_mult, CLIP_MIN, CLIP_MAX)\n\n    # Ïä§Î¨¥Îî©\n    alloc = (1.0 - SMOOTH_ALPHA) * alloc + SMOOTH_ALPHA * _last_allocation\n    _last_allocation = float(alloc)\n\n    # vol guard ÏóÖÎç∞Ïù¥Ìä∏\n    _update_vol_guard(test_pd, _last_allocation)\n\n    return float(alloc)\n\n# ============== Launch server ==============\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    # Î°úÏª¨ÏóêÏÑú submission.parquet ÏÉùÏÑ±\n    import pyarrow  # parquet ÏóîÏßÑ ÌôïÏù∏ (KaggleÏóêÎäî Í∏∞Î≥∏ ÏÑ§ÏπòÎê®)\n\n    test = pd.read_csv(TEST_CSV).sort_values(TIME_COL).reset_index(drop=True)\n\n    # ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî(ÏÑ†ÌÉù): Ïò®ÎùºÏù∏ Î≤ÑÌçº/Í∞ÄÎìú Î¶¨ÏÖã\n    _test_buffer = pd.DataFrame()\n    _realized_window = []\n    _last_allocation = 0.0\n    _prev_raw_pred_sign = None\n\n    preds = []\n    for _, row in test.iterrows():\n        # predictÎäî 1-Ìñâ dict/Series/DataFrame Î™®Îëê ÏßÄÏõêÌïòÎèÑÎ°ù ÏúÑÏóêÏÑú wrapper Íµ¨ÏÑ±Îê®\n        preds.append(predict(row.to_dict()))\n\n    sub = pd.DataFrame({\"allocation\": preds})\n    # ID Ïª¨Îüº Î∂ôÏù¥Í∏∞(ÎåÄÌöå Ïä§ÌÇ§ÎßàÏóê ÎßûÏ∂∞ ÏûêÎèô ÏÑ†ÌÉù)\n    if \"row_id\" in test.columns:\n        sub.insert(0, \"row_id\", test[\"row_id\"].values)\n    elif \"prediction_id\" in test.columns:\n        sub.insert(0, \"prediction_id\", test[\"prediction_id\"].values)\n    elif \"date_id\" in test.columns:\n        sub.insert(0, \"date_id\", test[\"date_id\"].values)\n\n    sub.to_parquet(\"submission.parquet\", index=False)\n    print(\"[Saved] submission.parquet\", sub.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:02:55.836138Z","iopub.execute_input":"2025-10-03T19:02:55.836459Z","iopub.status.idle":"2025-10-03T19:03:13.579341Z","shell.execute_reply.started":"2025-10-03T19:02:55.836424Z","shell.execute_reply":"2025-10-03T19:03:13.578284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def exp_saturation_report(df_full: pd.DataFrame, folds, tol=0.1):\n    # exp_* ÌîºÏ≤òÏùò logÍ∞Ä clip Í≤ΩÍ≥Ñ(¬±8)Ïóê ÏñºÎßàÎÇò Î∂ôÎäîÏßÄ ÎπÑÏú®\n    cols = [c for c in df_full.columns if c.startswith(\"exp_\")]\n    if not cols: \n        print(\"[SAT] no exp_* columns found\"); \n        return\n    for f, (_, va_idx) in enumerate(folds, 1):\n        sub = df_full.iloc[va_idx][cols]\n        # log(exp_x) = x_scaled (clipÎêú Í∞íÏóê Í∑ºÏÇ¨)\n        logv = np.log(np.clip(sub, 1e-12, None))\n        lower = (logv <= (EXP_CLIP[0] + tol)).mean().mean()\n        upper = (logv >= (EXP_CLIP[1] - tol)).mean().mean()\n        print(f\"[SAT][Fold {f}] near-lower‚âà{lower:.3f}, near-upper‚âà{upper:.3f}, total‚âà{(lower+upper):.3f}\")\n\n# ÏõêÎ≥∏(exp Ìè¨Ìï®) Ï†ÑÏ≤¥ ÌååÏÉù DataFrameÏùÄ X_coreexp Ïù¥Í≥†,\n# prunedÎäî Í∑∏ Ï§ë 30Í∞ú ÏÑ†ÌÉù Ïó¥ÎßåÏù¥ÏßÄÎßå Ìè¨ÌôîÏú®ÏùÄ Ï†ÑÏ≤¥ exp_*ÏóêÏÑú Î≥¥Îäî Í≤å Ï¢ãÏïÑÏöî.\nexp_cols = [c for c in X_coreexp.columns if c.startswith(\"exp_\")]\nif exp_cols:\n    exp_saturation_report(X_coreexp[exp_cols], folds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:03:13.580494Z","iopub.execute_input":"2025-10-03T19:03:13.58083Z","iopub.status.idle":"2025-10-03T19:03:13.778028Z","shell.execute_reply.started":"2025-10-03T19:03:13.580807Z","shell.execute_reply":"2025-10-03T19:03:13.776934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Extra diagnostics for the pruned model =====\nimport numpy as np, pandas as pd\n\ndef prune_fold_diagnostics(X_sel: pd.DataFrame, y: pd.Series, models):\n    rows = []\n    for f, (tr_idx, va_idx) in enumerate(folds, 1):\n        X_va, y_va = X_sel.iloc[va_idx], y.iloc[va_idx]\n        m = models[f-1]\n        bi = getattr(m, \"best_iteration_\", None)\n        yhat = m.predict(X_va, num_iteration=bi)\n\n        y_std    = float(y_va.std())\n        yhat_std = float(np.std(yhat))\n        rmse     = float(np.sqrt(np.mean((y_va - yhat)**2)))\n        used_cnt = int((m.booster_.feature_importance(importance_type=\"split\") > 0).sum())\n\n        rows.append(dict(fold=f, best_it=bi, y_std=y_std, yhat_std=yhat_std,\n                         rmse=rmse, used_feats=used_cnt))\n    df = pd.DataFrame(rows)\n    print(\"\\n[PRUNE][DIAG]\\n\", df.to_string(index=False))\n    df.to_csv(\"/kaggle/working/prune_fold_diagnostics.csv\", index=False)\n\nprune_fold_diagnostics(X_coreexp_sel, y_coreexp, models_coreexp_sel)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T19:03:13.779034Z","iopub.execute_input":"2025-10-03T19:03:13.779733Z","iopub.status.idle":"2025-10-03T19:03:13.817882Z","shell.execute_reply.started":"2025-10-03T19:03:13.779678Z","shell.execute_reply":"2025-10-03T19:03:13.816908Z"}},"outputs":[],"execution_count":null}]}