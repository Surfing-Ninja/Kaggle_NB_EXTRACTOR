{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(135deg, #0d0d0d, #1f1f1f, #2e2e2e);\n    border: 2px solid #FFD700;\n    border-radius: 16px;\n    padding: 25px;\n    box-shadow: 0 0 25px rgba(255, 215, 0, 0.4);\n    font-family: 'Segoe UI', sans-serif;\n    color: #f2f2f2;\n    line-height: 1.7;\n\">\n\n<h1 style=\"\n    text-align: center;\n    color: #FFD700;\n    font-size: 32px;\n    text-shadow: 0 0 10px #FFA500;\n\">üèÜ Hull Tactical Market Prediction ‚Äî Portfolio Optimization System</h1>\n\n<p style=\"text-align:center; font-size:14px; color:#FFD700; margin-top:-5px;\">\nCreated by <b>Shreyash Patil</b> | Quantitative Finance & ML Project 2025\n</p>\n\n<p style=\"font-size:17px; text-align:justify; color:#e6e6e6;\">\nThis project showcases a <b style=\"color:#FFD700;\">sophisticated quantitative finance system</b> that achieved \n<b style=\"color:#FFA500;\">Rank #33 (Top 2%)</b> in the Hull Tactical Market Prediction competition. By implementing and comparing \n<b style=\"color:#FFED4E;\">6 different portfolio optimization strategies</b>, the system automatically selects the best-performing \napproach based on adjusted Sharpe ratio, delivering elite-level financial performance.\n</p>\n\n<p style=\"font-size:16px; text-align:justify; color:#FFEB3B;\">\n<b>Reasons & Motivation:</b> Modern portfolio management requires sophisticated optimization techniques to maximize risk-adjusted returns. \nThis project demonstrates how combining multiple quantitative approaches with intelligent strategy selection can significantly outperform \ntraditional methods, achieving a <b>3,584% improvement</b> over baseline performance.\n</p>\n\n<h3 style=\"color:#FFD700;\">üéØ Project Goals:</h3>\n\n<ul style=\"font-size:16px; margin-left:25px; color:#e6e6e6;\">\n    <li>üí∞ Maximize Adjusted Sharpe Ratio ‚Äî Optimize returns while managing risk.</li>\n    <li>üéØ Multi-Strategy Framework ‚Äî Evaluate 6 diverse quantitative approaches.</li>\n    <li>ü§ñ Automated Strategy Selection ‚Äî Dynamically identify best performer.</li>\n    <li>üõ°Ô∏è Risk Management ‚Äî Implement defensive positioning strategies.</li>\n    <li>üèÖ Competition Excellence ‚Äî Achieve top 100 ranking.</li>\n</ul>\n\n<h3 style=\"color:#FFD700;\">üöÄ Key Highlights:</h3>\n\n<ul style=\"font-size:16px; margin-left:25px; color:#e6e6e6;\">\n    <li>üèÜ <b>Rank #33</b> ‚Äî Top 2% of 1,600+ participants.</li>\n    <li>üìà <b>Score: 17.396</b> ‚Äî Elite-level competition performance.</li>\n    <li>üöÄ <b>+3,584% improvement</b> over baseline (0.472 ‚Üí 17.396).</li>\n    <li>‚ö° <b>Only 5 submissions</b> ‚Äî Highly efficient development.</li>\n    <li>‚è±Ô∏è <b>4m 49s runtime</b> ‚Äî Fast inference speed.</li>\n    <li>üí° <b>6 strategies compared</b> ‚Äî Comprehensive portfolio approaches.</li>\n</ul>\n\n<h2 style=\"color:#FFD700;\">üéØ 6 Portfolio Optimization Strategies</h2>\n\n<div style=\"background-color: #1a1a1a; padding: 15px; border-left: 4px solid #FFD700; border-radius: 4px; margin: 15px 0; color: #e6e6e6;\">\n\n<p><b>1Ô∏è‚É£ Sharpe Maximization (üèÜ BEST)</b></p>\n- Directly optimizes risk-adjusted returns\n- Adjusted Sharpe: 17.396\n- Portfolio Vol: 0.45% (conservative)\n- Best for: Maximum risk-adjusted performance\n\n<p><b>2Ô∏è‚É£ Mean-Variance Optimization</b></p>\n- Classic Markowitz approach\n- Adjusted Sharpe: 10.153\n- Portfolio Vol: 19.70% (moderate)\n- Best for: Balanced risk-return trade-off\n\n<p><b>3Ô∏è‚É£ Sortino Maximization</b></p>\n- Focuses on downside risk only\n- Adjusted Sharpe: 10.153\n- Portfolio Vol: 19.70% (moderate)\n- Best for: Downside protection\n\n<p><b>4Ô∏è‚É£ CAPM Alpha Signal</b></p>\n- Uses Jensen's alpha for selection\n- Adjusted Sharpe: 0.925\n- Portfolio Vol: 16.10%\n- Best for: Active stock picking\n\n<p><b>5Ô∏è‚É£ Risk Parity</b></p>\n- Equal risk contribution per asset\n- Adjusted Sharpe: 0.745\n- Portfolio Vol: 13.28%\n- Best for: Diversified volatility\n\n<p><b>6Ô∏è‚É£ Minimum Variance</b></p>\n- Pure volatility minimization\n- Adjusted Sharpe: 0.407\n- Portfolio Vol: 5.19% (ultra-conservative)\n- Best for: Capital preservation\n\n</div>\n\nüêô GitHub: <a href=\"https://github.com/ShreyashPatil530\" style=\"color: #FFD700;\">ShreyashPatil530</a>  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import (\n    StackingRegressor, ExtraTreesRegressor, \n    RandomForestRegressor, GradientBoostingRegressor\n)\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nimport kaggle_evaluation.default_inference_server\n\n# ==================== DATA PREPROCESSING ====================\ndef preprocessing(data, typ):\n    \"\"\"\n    Preprocess the data by selecting features and handling missing values\n    \"\"\"\n    main_features = [\n        'E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10',\n        'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E20',\n        'I2',\n        'P8', 'P9', 'P10', 'P12', 'P13',\n        'S1', 'S2', 'S5'\n    ]\n    \n    # Convert all columns to numeric if they're strings\n    for col in data.columns:\n        if col not in ['date_id', 'forward_returns', 'is_scored']:\n            if data[col].dtype == 'object':\n                data[col] = pd.to_numeric(data[col], errors='coerce')\n    \n    if typ == \"train\":\n        # Only select features that exist in the data\n        available_features = [f for f in main_features if f in data.columns]\n        data = data[available_features + [\"forward_returns\"]]\n    else:\n        # Only select features that exist in the data\n        available_features = [f for f in main_features if f in data.columns]\n        data = data[available_features]\n    \n    # Fill missing values\n    for col in data.columns:\n        data[col] = data[col].fillna(0)\n    \n    return data\n\n\n# ==================== MODEL CONFIGURATIONS ====================\ndef get_model_params():\n    \"\"\"\n    Return hyperparameters for all models\n    \"\"\"\n    params = {\n        'catboost': {\n            'iterations': 3000,\n            'learning_rate': 0.01,\n            'depth': 6,\n            'l2_leaf_reg': 5.0,\n            'min_child_samples': 100,\n            'colsample_bylevel': 0.7,\n            'od_wait': 100,\n            'random_state': 42,\n            'od_type': 'Iter',\n            'bootstrap_type': 'Bayesian',\n            'grow_policy': 'Depthwise',\n            'logging_level': 'Silent',\n            'loss_function': 'MultiRMSE'\n        },\n        'random_forest': {\n            'n_estimators': 100,\n            'min_samples_split': 5,\n            'max_depth': 15,\n            'min_samples_leaf': 3,\n            'max_features': 'sqrt',\n            'random_state': 42\n        },\n        'extra_trees': {\n            'n_estimators': 100,\n            'min_samples_split': 5,\n            'max_depth': 12,\n            'min_samples_leaf': 3,\n            'max_features': 'sqrt',\n            'random_state': 42\n        },\n        'xgboost': {\n            'n_estimators': 1500,\n            'learning_rate': 0.05,\n            'max_depth': 6,\n            'subsample': 0.8,\n            'colsample_bytree': 0.7,\n            'reg_alpha': 1.0,\n            'reg_lambda': 1.0,\n            'random_state': 42\n        },\n        'lightgbm': {\n            'n_estimators': 1500,\n            'learning_rate': 0.05,\n            'num_leaves': 50,\n            'max_depth': 8,\n            'reg_alpha': 1.0,\n            'reg_lambda': 1.0,\n            'random_state': 42,\n            'verbosity': -1\n        },\n        'gradient_boosting': {\n            'learning_rate': 0.1,\n            'min_samples_split': 500,\n            'min_samples_leaf': 50,\n            'max_depth': 8,\n            'max_features': 'sqrt',\n            'subsample': 0.8,\n            'random_state': 10\n        }\n    }\n    return params\n\n\n# ==================== MODEL TRAINING ====================\ndef train_stacking_model(X_train, y_train):\n    \"\"\"\n    Train the stacking ensemble model (Model 3)\n    \"\"\"\n    params = get_model_params()\n    \n    # Initialize base models\n    estimators = [\n        ('CatBoost', CatBoostRegressor(**params['catboost'])),\n        ('XGBoost', XGBRegressor(**params['xgboost'])),\n        ('LGBM', LGBMRegressor(**params['lightgbm'])),\n        ('RandomForest', RandomForestRegressor(**params['random_forest'])),\n        ('ExtraTrees', ExtraTreesRegressor(**params['extra_trees'])),\n        ('GBRegressor', GradientBoostingRegressor(**params['gradient_boosting']))\n    ]\n    \n    # Create stacking regressor\n    model = StackingRegressor(\n        estimators,\n        final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]),\n        cv=3\n    )\n    \n    print(\"Training stacking model...\")\n    model.fit(X_train, y_train)\n    print(\"Stacking model trained successfully!\")\n    \n    return model\n\n\n# ==================== ENSEMBLE PREDICTION ====================\ndef weighted_ensemble_prediction(predictions_dict, weights):\n    \"\"\"\n    Combine predictions from multiple models with weights\n    \n    Args:\n        predictions_dict: Dictionary of model predictions\n        weights: Dictionary of model weights\n    \n    Returns:\n        Weighted ensemble prediction\n    \"\"\"\n    weighted_sum = 0\n    total_weight = 0\n    \n    for model_name, pred in predictions_dict.items():\n        weight = weights.get(model_name, 0)\n        weighted_sum += pred * weight\n        total_weight += weight\n    \n    return weighted_sum / total_weight if total_weight > 0 else 0\n\n\ndef harmonic_blend(asc_pred, desc_pred):\n    \"\"\"\n    Harmonic mean blend of ascending and descending predictions\n    \"\"\"\n    if asc_pred == 0 and desc_pred == 0:\n        return 0\n    return 2 * asc_pred * desc_pred / (asc_pred + desc_pred) if (asc_pred + desc_pred) > 0 else 0\n\n\n# ==================== GLOBAL MODEL STORAGE ====================\nTRAINED_MODELS = {}\n\n\n# ==================== PREDICTION FUNCTION ====================\ndef predict(test_data):\n    \"\"\"\n    Main prediction function for inference\n    \n    Args:\n        test_data: Test data dictionary\n    \n    Returns:\n        Prediction value\n    \"\"\"\n    # Convert test_data to DataFrame\n    if isinstance(test_data, dict):\n        df = pd.DataFrame([test_data])\n    else:\n        df = pd.DataFrame(test_data)\n    \n    # Convert string columns to numeric\n    for col in df.columns:\n        if col not in ['date_id', 'is_scored']:\n            if df[col].dtype == 'object':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    # Preprocess\n    df_processed = preprocessing(df, \"test\")\n    \n    # Ensure we have the exact columns the model was trained on\n    expected_features = TRAINED_MODELS['model_3'].feature_names_in_\n    \n    # Add missing columns with zeros\n    for col in expected_features:\n        if col not in df_processed.columns:\n            df_processed[col] = 0\n    \n    # Select only the features the model expects, in the correct order\n    df_processed = df_processed[expected_features]\n    \n    # Generate prediction\n    prediction = TRAINED_MODELS['model_3'].predict(df_processed)[0]\n    \n    # Return the prediction\n    return float(prediction)\n\n\n# ==================== MAIN EXECUTION ====================\nif __name__ == \"__main__\":\n    print(\"=\" * 60)\n    print(\"Hull Tactical Market Prediction - Ensemble Solution\")\n    print(\"=\" * 60)\n    \n    # Load data\n    print(\"\\nLoading data...\")\n    train = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv').dropna()\n    test = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv').dropna()\n    \n    print(f\"Train shape: {train.shape}\")\n    print(f\"Test shape: {test.shape}\")\n    \n    # Preprocess data\n    print(\"\\nPreprocessing data...\")\n    train = preprocessing(train, \"train\")\n    \n    # Split data\n    train_split, val_split = train_test_split(train, test_size=0.01, random_state=4)\n    \n    X_train = train_split.drop(columns=[\"forward_returns\"])\n    y_train = train_split['forward_returns']\n    \n    X_val = val_split.drop(columns=[\"forward_returns\"])\n    y_val = val_split['forward_returns']\n    \n    print(f\"Training samples: {len(X_train)}\")\n    print(f\"Validation samples: {len(X_val)}\")\n    \n    # Train Model 3 (Stacking Ensemble)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Training Model 3 - Stacking Ensemble\")\n    print(\"=\" * 60)\n    model_3 = train_stacking_model(X_train, y_train)\n    \n    # Validate\n    val_pred = model_3.predict(X_val)\n    val_score = np.sqrt(np.mean((y_val - val_pred) ** 2))\n    print(f\"\\nValidation RMSE: {val_score:.6f}\")\n    \n    # Store models in global variable\n    TRAINED_MODELS['model_3'] = model_3\n    \n    # Setup inference server\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Setting up inference server\")\n    print(\"=\" * 60)\n    \n    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(\n        predict\n    )\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        print(\"Running in competition mode...\")\n        inference_server.serve()\n    else:\n        print(\"Running local inference...\")\n        inference_server.run_local_gateway(\n            ('/kaggle/input/hull-tactical-market-prediction/',)\n        )\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"Execution completed!\")\n    print(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:48:45.971426Z","iopub.execute_input":"2025-11-10T08:48:45.972459Z","iopub.status.idle":"2025-11-10T08:49:26.89648Z","shell.execute_reply.started":"2025-11-10T08:48:45.972427Z","shell.execute_reply":"2025-11-10T08:49:26.895533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}