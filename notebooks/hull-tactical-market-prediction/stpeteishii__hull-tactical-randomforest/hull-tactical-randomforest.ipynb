{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":51.922209,"end_time":"2025-08-27T08:41:24.365809","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-27T08:40:32.4436","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **Hull Tactical: RandomForest**","metadata":{}},{"cell_type":"markdown","source":"\n---\n\n## ðŸŒ³ **RandomForestRegressor and Multi-Output Regression**\n\n`RandomForestRegressor` (from `sklearn.ensemble`) natively supports **multi-output regression**.\nThis means you can pass a **2D target array** `y` with shape `(n_samples, n_outputs)` directly to `.fit()`.\nEach output column is treated as a separate regression problem, but the model shares the same set of trees, making training efficient.\n\n\n## **Key points**:\n\n* Unlike LightGBM/XGBoost, **no wrapper is required** (`MultiOutputRegressor` is not needed).\n* Predictions preserve the same dimensionality: if `y` has 3 target variables, the output will have 3 columns.\n* It is especially useful when the targets are related (e.g., predicting multiple sensor readings at once).\n\n---\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport datetime\n\nfrom tqdm import tqdm\nfrom dataclasses import dataclass, asdict\n\nimport polars as pl \nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nimport kaggle_evaluation.default_inference_server\n\n# ============ PATHS ============\nDATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n# ============ RETURNS TO SIGNAL CONFIGS ============\nMIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \nMAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \nSIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n\n# ============ RANDOM FOREST CONFIGS ============\nN_ESTIMATORS: int = 100                         # Number of trees in the forest\nMAX_DEPTH: int = None                           # Maximum depth of the tree\nMIN_SAMPLES_SPLIT: int = 2                      # Minimum number of samples required to split an internal node\nMIN_SAMPLES_LEAF: int = 1                       # Minimum number of samples required to be at a leaf node\nRANDOM_STATE: int = 42                          # Random state for reproducibility\n\n@dataclass\nclass DatasetOutput:\n    X_train : pl.DataFrame \n    X_test: pl.DataFrame\n    y_train: pl.Series\n    y_test: pl.Series\n    scaler: StandardScaler\n\n@dataclass \nclass RandomForestParameters:\n    n_estimators : int \n    max_depth: int\n    min_samples_split: int\n    min_samples_leaf: int\n    random_state: int\n    \n    def __post_init__(self): \n        if self.n_estimators <= 0:\n            raise ValueError(\"n_estimators must be greater than 0\")\n        \n@dataclass(frozen=True)\nclass RetToSignalParameters:\n    signal_multiplier: float \n    min_signal : float = MIN_SIGNAL\n    max_signal : float = MAX_SIGNAL\n\n# Set the Parameters\nret_signal_params = RetToSignalParameters(\n    signal_multiplier= SIGNAL_MULTIPLIER\n)\n\nrf_params = RandomForestParameters(\n    n_estimators = N_ESTIMATORS,\n    max_depth = MAX_DEPTH,\n    min_samples_split = MIN_SAMPLES_SPLIT,\n    min_samples_leaf = MIN_SAMPLES_LEAF,\n    random_state = RANDOM_STATE\n)\n\ndef load_trainset() -> pl.DataFrame:\n    \"\"\"\n    Loads and preprocesses the training dataset.\n    \"\"\"\n    return (\n        pl.read_csv(DATA_PATH / \"train.csv\")\n        .rename({'market_forward_excess_returns':'target'})\n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n        .head(-10)\n    )\n\ndef load_testset() -> pl.DataFrame:\n    \"\"\"\n    Loads and preprocesses the testing dataset.\n    \"\"\"\n    return (\n        pl.read_csv(DATA_PATH / \"test.csv\")\n        .rename({'lagged_forward_returns':'target'})\n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n    )\n\ndef create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Creates new features and cleans a DataFrame.\n    \"\"\"\n    vars_to_keep = [\n        \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n        \"P10\", \"P12\", \"P13\", \"U1\", \"U2\"\n    ]\n\n    return (\n        df.with_columns(\n            (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n            (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n        )\n        .select([\"date_id\", \"target\"] + vars_to_keep)\n        .with_columns([\n            pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n            for col in vars_to_keep\n        ])\n        .drop_nulls()\n    )\n    \ndef join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Joins two dataframes by common columns and concatenates them vertically.\n    \"\"\"\n    common_columns = [col for col in train.columns if col in test.columns]\n    \n    return pl.concat([train.select(common_columns), test.select(common_columns)], how=\"vertical\")\n\ndef split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list) -> DatasetOutput: \n    \"\"\"\n    Splits the data into features (X) and target (y), and scales the features.\n    \"\"\"\n    X_train = train.drop(['date_id','target']) \n    y_train = train.get_column('target')\n    X_test = test.drop(['date_id','target']) \n    y_test = test.get_column('target')\n    \n    scaler = StandardScaler() \n    \n    X_train_scaled_np = scaler.fit_transform(X_train)\n    X_train = pl.from_numpy(X_train_scaled_np, schema=features)\n    \n    X_test_scaled_np = scaler.transform(X_test)\n    X_test = pl.from_numpy(X_test_scaled_np, schema=features)\n    \n    return DatasetOutput(\n        X_train = X_train,\n        y_train = y_train, \n        X_test = X_test, \n        y_test = y_test,\n        scaler = scaler\n    )\n\ndef convert_ret_to_signal(\n    ret_arr: np.ndarray,\n    params: RetToSignalParameters\n) -> np.ndarray:\n    \"\"\"\n    Converts raw model predictions (expected returns) into a trading signal.\n    \"\"\"\n    return np.clip(\n        ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal\n    )\n\n# Looking at the Data\ntrain: pl.DataFrame = load_trainset()\ntest: pl.DataFrame = load_testset() \nprint(train.tail(3)) \nprint(test.head(3))\n\n# Generating the Train and Test\ndf: pl.DataFrame = join_train_test_dataframes(train, test)\ndf = create_example_dataset(df=df) \ntrain: pl.DataFrame = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\ntest: pl.DataFrame = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n\nFEATURES = [col for col in test.columns if col not in ['date_id', 'target']]\n\ndataset: DatasetOutput = split_dataset(train=train, test=test, features=FEATURES) \n\nX_train: pl.DataFrame = dataset.X_train\nX_test: pl.DataFrame = dataset.X_test\ny_train: pl.DataFrame = dataset.y_train\ny_test: pl.DataFrame = dataset.y_test\nscaler: StandardScaler = dataset.scaler \n\n# Fitting the Random Forest Model\nmodel = RandomForestRegressor(**asdict(rf_params))\nmodel.fit(X_train.to_pandas(), y_train.to_pandas())\n\nprint(f\"RandomForestRegressor fitted with {rf_params.n_estimators} estimators\")\n\n# Prediction Function via Kaggle Server\ndef predict(test: pl.DataFrame) -> float:\n    test = test.rename({'lagged_forward_returns':'target'})\n    df: pl.DataFrame = create_example_dataset(test)\n    X_test: pl.DataFrame = df.select(FEATURES)\n    X_test_scaled_np: np.ndarray = scaler.transform(X_test)\n    X_test_scaled = pl.from_numpy(X_test_scaled_np, schema=FEATURES)\n    raw_pred: float = model.predict(X_test_scaled.to_pandas())[0]\n    return convert_ret_to_signal(raw_pred, ret_signal_params)\n\n# Launch Server\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}