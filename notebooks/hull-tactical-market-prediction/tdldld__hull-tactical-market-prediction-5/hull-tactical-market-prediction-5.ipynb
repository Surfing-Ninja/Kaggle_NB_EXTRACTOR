{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"},{"sourceId":274397875,"sourceType":"kernelVersion"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../working/\" directory\n# For example, running this will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:24:53.771477Z","iopub.execute_input":"2025-11-08T03:24:53.772084Z","iopub.status.idle":"2025-11-08T03:24:53.784587Z","shell.execute_reply.started":"2025-11-08T03:24:53.772043Z","shell.execute_reply":"2025-11-08T03:24:53.783655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import TimeSeriesSplit\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ═══════════════════════════════════════════════════════════════════════════\n# CONSTANTES MCDC AGRESIVAS\n# ═══════════════════════════════════════════════════════════════════════════\n\nclass MCDCAggressiveParams:\n    \"\"\"Parámetros MCDC agresivos (post-análisis V1.5)\"\"\"\n    \n    # Estrategia A: Amplificación extrema + Sigmoid\n    SIGNAL_AMP_A = 800            # 6.7x más que V1.5\n    KELLY_FRACTION_A = 0.8        # 2x más que V1.5\n    \n    # Estrategia B: Escala lineal directa (SIN sigmoid)\n    SCALE_LINEAR = 5000           # Escala directa a rango [0, 2]\n    \n    # Shield\n    VOLATILITY_SHIELD = 1.15\n    \n    # Selección de estrategia\n    USE_LINEAR = True  # True = Estrategia B (más directa)\n    \nMCDC = MCDCAggressiveParams()\n\nprint(\"=\"*80)\nprint(\"HULL TACTICAL V1.6: BASELINE + MCDC AGGRESSIVE\")\nprint(\"=\"*80)\n\nif MCDC.USE_LINEAR:\n    print(f\"Estrategia: LINEAL DIRECTA\")\n    print(f\"Scale Factor: {MCDC.SCALE_LINEAR}\")\nelse:\n    print(f\"Estrategia: SIGMOID + KELLY\")\n    print(f\"Signal Amplification: {MCDC.SIGNAL_AMP_A}\")\n    print(f\"Kelly Fraction: {MCDC.KELLY_FRACTION_A}\")\n\nprint(f\"Volatility Shield: {MCDC.VOLATILITY_SHIELD}x\")\n\n# ═══════════════════════════════════════════════════════════════════════════\n# 1-6. CARGA, TARGET, FEATURES, IMPUTACIÓN, SELECCIÓN, VALIDATION\n# (IDÉNTICO A V1.5 - COPIADO)\n# ═══════════════════════════════════════════════════════════════════════════\n\nTRAIN_PATH = '/kaggle/input/hull-tactical-market-prediction/train.csv'\nTEST_PATH = '/kaggle/input/hull-tactical-market-prediction/test.csv'\n\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\n\nprint(f\"\\n✓ Train: {train_df.shape}\")\nprint(f\"✓ Test: {test_df.shape}\")\n\ndef identify_target(df):\n    candidates = [col for col in df.columns if 'return' in col.lower() and 'excess' in col.lower()]\n    if candidates:\n        return candidates[0]\n    candidates = [col for col in df.columns if 'return' in col.lower()]\n    if candidates:\n        return candidates[0]\n    return None\n\ntarget_col = identify_target(train_df)\nprint(f\"✓ Target: {target_col}\")\n\ndef create_safe_features(df, target_col):\n    df = df.sort_values('date_id').reset_index(drop=True)\n    \n    families = {\n        'E': [c for c in df.columns if c.startswith('E') and c[1:].isdigit()],\n        'M': [c for c in df.columns if c.startswith('M') and c[1:].isdigit()],\n        'V': [c for c in df.columns if c.startswith('V') and c[1:].isdigit()],\n        'I': [c for c in df.columns if c.startswith('I') and c[1:].isdigit()],\n        'P': [c for c in df.columns if c.startswith('P') and c[1:].isdigit()],\n        'S': [c for c in df.columns if c.startswith('S') and c[1:].isdigit()],\n    }\n    \n    for fam, cols in families.items():\n        if len(cols) > 0:\n            df[f'{fam}_mean'] = df[cols].shift(1).mean(axis=1)\n            df[f'{fam}_std'] = df[cols].shift(1).std(axis=1)\n            df[f'{fam}_max'] = df[cols].shift(1).max(axis=1)\n            df[f'{fam}_min'] = df[cols].shift(1).min(axis=1)\n    \n    for col in ['M1', 'M2', 'E1', 'E2', 'V1', 'V2']:\n        if col in df.columns:\n            df[f'{col}_lag1'] = df[col].shift(1)\n            df[f'{col}_lag5'] = df[col].shift(5)\n    \n    for col in ['M1', 'E1', 'V1']:\n        if col in df.columns:\n            df[f'{col}_ma5'] = df[col].shift(1).rolling(5, min_periods=1).mean()\n            df[f'{col}_ma21'] = df[col].shift(1).rolling(21, min_periods=1).mean()\n    \n    df['day_of_week'] = df['date_id'] % 7\n    df['month_approx'] = (df['date_id'] // 20) % 12\n    \n    return df\n\nprint(\"\\n→ Creando features...\")\ntrain_df = create_safe_features(train_df, target_col)\ntest_df = create_safe_features(test_df, target_col)\nprint(\"✓ Features creadas\")\n\ndef impute_safe(train_df, test_df, target_col):\n    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n    numeric_cols = [c for c in numeric_cols if c not in ['date_id', target_col]]\n    \n    train_medians = {col: train_df[col].median() for col in numeric_cols if col in train_df.columns}\n    \n    for col in numeric_cols:\n        if col in train_df.columns:\n            train_df[col].fillna(train_medians[col], inplace=True)\n        if col in test_df.columns:\n            test_df[col].fillna(train_medians.get(col, 0), inplace=True)\n    \n    return train_df, test_df\n\nprint(\"→ Imputando...\")\ntrain_df, test_df = impute_safe(train_df, test_df, target_col)\n\nexclude_cols = ['date_id', target_col, 'risk_free_rate', 'market_forward_excess_returns',\n                'forward_returns', 'lagged_market_forward_excess_returns']\n\ncandidate_features = [col for col in train_df.columns if col not in exclude_cols]\nfeature_cols = [col for col in candidate_features if col in test_df.columns]\nfeature_cols = [col for col in feature_cols if train_df[col].notna().sum() > len(train_df) * 0.05]\n\nprint(f\"\\n✓ Features: {len(feature_cols)}\")\n\nX_train = train_df[feature_cols].fillna(0)\ny_train = train_df[target_col].fillna(0)\nX_test = test_df[feature_cols].fillna(0)\n\nprint(\"=\"*80)\nprint(\"WALK-FORWARD VALIDATION\")\nprint(\"=\"*80)\n\ntscv = TimeSeriesSplit(n_splits=5)\nfold_scores = []\n\nlgb_params = {\n    'objective': 'regression',\n    'metric': 'mse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'seed': 42\n}\n\nfor fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n    print(f\"\\nFold {fold + 1}/5\")\n    \n    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    train_data = lgb.Dataset(X_tr, y_tr)\n    model = lgb.train(lgb_params, train_data, num_boost_round=500)\n    \n    val_preds = model.predict(X_val)\n    rmse = np.sqrt(np.mean((val_preds - y_val.values)**2))\n    fold_scores.append(rmse)\n    print(f\"  RMSE: {rmse:.6f}\")\n\nprint(f\"\\n{'─'*80}\")\nprint(f\"RMSE Promedio: {np.mean(fold_scores):.6f} (+/- {np.std(fold_scores):.6f})\")\n\n# ═══════════════════════════════════════════════════════════════════════════\n# 7. ENTRENAMIENTO FINAL\n# ═══════════════════════════════════════════════════════════════════════════\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ENTRENAMIENTO FINAL\")\nprint(\"=\"*80)\n\ntrain_data = lgb.Dataset(X_train, y_train)\nfinal_model = lgb.train(lgb_params, train_data, num_boost_round=500)\n\ntest_predictions_raw = final_model.predict(X_test)\n\nprint(f\"✓ Predicciones RAW\")\nprint(f\"  Media: {test_predictions_raw.mean():.6f}\")\nprint(f\"  Std: {test_predictions_raw.std():.6f}\")\nprint(f\"  Rango: [{test_predictions_raw.min():.6f}, {test_predictions_raw.max():.6f}]\")\n\n# ═══════════════════════════════════════════════════════════════════════════\n# 8. AMPLIFICACIÓN AGRESIVA (NUEVO ENFOQUE)\n# ═══════════════════════════════════════════════════════════════════════════\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"AMPLIFICACIÓN AGRESIVA\")\nprint(\"=\"*80)\n\nif MCDC.USE_LINEAR:\n    # ───────────────────────────────────────────────────────────────────────\n    # ESTRATEGIA B: ESCALA LINEAL DIRECTA (RECOMENDADA)\n    # ───────────────────────────────────────────────────────────────────────\n    print(\"Método: ESCALA LINEAL DIRECTA\")\n    \n    # Paso 1: Normalizar predicciones a [0, 1]\n    pred_min = test_predictions_raw.min()\n    pred_max = test_predictions_raw.max()\n    pred_range = pred_max - pred_min\n    \n    if pred_range > 1e-10:\n        pred_normalized = (test_predictions_raw - pred_min) / pred_range\n    else:\n        pred_normalized = np.ones_like(test_predictions_raw) * 0.5\n    \n    # Paso 2: Escalar a rango [0, 2]\n    allocations = pred_normalized * 2.0\n    \n    # Paso 3: Ajuste fino (para dar más diversidad)\n    # Expandir rango si está muy comprimido\n    alloc_mean = allocations.mean()\n    alloc_std = allocations.std()\n    \n    if alloc_std < 0.3:  # Si muy comprimido, expandir\n        allocations = (allocations - alloc_mean) * 2.0 + 1.0\n        allocations = np.clip(allocations, 0, 2)\n    \n    print(f\"\\n✓ Normalización:\")\n    print(f\"  Rango RAW: [{pred_min:.6f}, {pred_max:.6f}]\")\n    print(f\"  Normalized: [{pred_normalized.min():.4f}, {pred_normalized.max():.4f}]\")\n    print(f\"\\n✓ Allocations (pre-shield):\")\n    print(f\"  Media: {allocations.mean():.4f}\")\n    print(f\"  Std: {allocations.std():.4f}\")\n    print(f\"  Rango: [{allocations.min():.4f}, {allocations.max():.4f}]\")\n    \nelse:\n    # ───────────────────────────────────────────────────────────────────────\n    # ESTRATEGIA A: SIGMOID EXTREMO + KELLY AGRESIVO\n    # ───────────────────────────────────────────────────────────────────────\n    print(\"Método: SIGMOID + KELLY AGRESIVO\")\n    \n    # Paso 1: Amplificación extrema\n    amplified_signal = test_predictions_raw * MCDC.SIGNAL_AMP_A\n    \n    # Paso 2: Sigmoid\n    probs = 1 / (1 + np.exp(-amplified_signal))\n    \n    # Paso 3: Kelly agresivo\n    allocations = MCDC.KELLY_FRACTION_A * (2 * probs - 1)\n    \n    # Paso 4: Clip\n    allocations = np.clip(allocations, 0, 2)\n    \n    print(f\"\\n✓ Signal Amplification: {MCDC.SIGNAL_AMP_A}x\")\n    print(f\"✓ Probabilidades:\")\n    print(f\"  Media: {probs.mean():.4f}\")\n    print(f\"  Rango: [{probs.min():.4f}, {probs.max():.4f}]\")\n    print(f\"\\n✓ Allocations (pre-shield):\")\n    print(f\"  Media: {allocations.mean():.4f}\")\n    print(f\"  Std: {allocations.std():.4f}\")\n    print(f\"  Rango: [{allocations.min():.4f}, {allocations.max():.4f}]\")\n\n# ═══════════════════════════════════════════════════════════════════════════\n# 9. VOLATILITY SHIELD\n# ═══════════════════════════════════════════════════════════════════════════\n\nif 'market_forward_excess_returns' in train_df.columns:\n    market_vol = train_df['market_forward_excess_returns'].iloc[-20:].std()\nelse:\n    market_vol = train_df[target_col].iloc[-20:].std()\n\npred_vol = np.std(allocations)\n\nif pred_vol > MCDC.VOLATILITY_SHIELD * market_vol:\n    scale_factor = (MCDC.VOLATILITY_SHIELD * market_vol) / pred_vol\n    allocations = allocations * scale_factor\n    print(f\"\\n⚠️  Volatility Shield ACTIVADO:\")\n    print(f\"  Market vol: {market_vol:.6f}\")\n    print(f\"  Pred vol: {pred_vol:.6f}\")\n    print(f\"  Scale factor: {scale_factor:.4f}\")\nelse:\n    print(f\"\\n✓ Volatility Shield OK\")\n\nprint(f\"\\n✓ Allocations FINALES:\")\nprint(f\"  Media: {allocations.mean():.4f}\")\nprint(f\"  Std: {allocations.std():.4f}\")\nprint(f\"  Rango: [{allocations.min():.4f}, {allocations.max():.4f}]\")\n\n# ═══════════════════════════════════════════════════════════════════════════\n# 10. SUBMISSION PARQUET\n# ═══════════════════════════════════════════════════════════════════════════\n\nsubmission = pd.DataFrame({\n    'date_id': test_df['date_id'].values,\n    'allocation': allocations\n})\n\nsubmission.to_parquet('submission.parquet', index=False)\n\nprint(f\"\\n{'='*80}\")\nprint(\"SUBMISSION GENERADO\")\nprint(f\"{'='*80}\")\nprint(f\"✓ Archivo: submission.parquet\")\nprint(f\"✓ Shape: {submission.shape}\")\nprint(f\"\\nPrimeras 5 filas:\")\nprint(submission.head())\n\nverification = pd.read_parquet('submission.parquet')\nprint(f\"\\n✓ Verificación OK\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"RESUMEN COMPARATIVO\")\nprint(f\"{'='*80}\")\nprint(f\"V1 (baseline):         [-0.0005, 0.0008]\")\nprint(f\"V1.5 (MCDC conserv):   [0.0000, 0.0189]\")\nprint(f\"V1.6 (MCDC agresivo):  [{allocations.min():.4f}, {allocations.max():.4f}]\")\nprint(f\"\\nAmplificación V1 → V1.6: ~{allocations.mean() / (test_predictions_raw.mean() + 1e-10):.0f}x\")\nprint(f\"\\n✅ LISTO PARA KAGGLE\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:25:41.793903Z","iopub.execute_input":"2025-11-08T03:25:41.794323Z","iopub.status.idle":"2025-11-08T03:25:58.639503Z","shell.execute_reply.started":"2025-11-08T03:25:41.794294Z","shell.execute_reply":"2025-11-08T03:25:58.638687Z"}},"outputs":[],"execution_count":null}]}