{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Daily S&P 500 Tactical Allocation\n\nThis project develops and validates an algorithmic framework for allocating capital each trading day between the S&P 500 index and a risk-free asset. The goal is to maximize risk-adjusted return (Sharpe ratio) while respecting volatility limits and contest rules.\n\nKey components and features\n\nData preprocessing and EDA\n• Load and inspect train/test sets, analyze forward and excess returns distributions\n• Identify missing values and fill with medians or zeros\n• Correlation analysis highlights top predictive features (forward_returns, M4, V13, etc.)\n\nFeature engineering\n• Volatility and momentum indicators over multiple rolling windows (5, 10, 20, 30 days)\n• Lagged versions of core predictors (M4, V13, M1, S5, V3, V4) to simulate real-time inference\n• Interaction terms (e.g. M4_V13_ratio, V3_V4_spread)\n• Regime detection flags for high/low volatility environments\n• Momentum acceleration (difference of 5-day momentum)\n\nModel ensemble\n• LightGBM, RandomForest, XGBoost and GradientBoosting classifiers trained to predict one-day excess return > 0.001\n• Static base weights (40% LGB, 25% RF, 20% XGB, 15% GB) adjusted up to ±20% based on current market volatility\n\nRisk-adjusted allocation\n• S-shaped transformation of ensemble probability signal with steepness (γ)\n• Scaling by model confidence, volatility multiplier and regime multiplier\n• Neutral (1.0×) allocation whenever confidence < threshold for the detected regime\n• Final allocation clipped to [0, 2]\n\nValidation and metrics\n• Walk-forward and rolling validation ensure out-of-sample robustness\n• Comprehensive metrics: Sharpe, Sortino, Calmar, Profit Factor, Win Rate and Active Signals %\n• Comparison versus buy-and-hold benchmark\n\nSubmission\n• Clean, standalone Python script replicates feature engineering and allocation logic\n• Generates “submission.csv” compatible with the Kaggle inference gateway\n• Final allocation statistics printed (range, mean, active signals)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv')\n\nprint(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n\n# Basic characteristics\nprint(\"\\nTrain date_id range:\", train_df['date_id'].min(), \"-\", train_df['date_id'].max())\nprint(\"Test date_id range:\", test_df['date_id'].min(), \"-\", test_df['date_id'].max())\n\n# Target variable analysis\ntargets = ['forward_returns', 'market_forward_excess_returns', 'risk_free_rate']\nfor t in targets:\n    if t in train_df.columns:\n        s = train_df[t].dropna()\n        q1, q3 = s.quantile([0.25, 0.75])\n        iqr = q3 - q1\n        outliers = s[(s < q1 - 1.5 * iqr) | (s > q3 + 1.5 * iqr)]\n        print(f\"\\n{t}: mean={s.mean():.6f}, std={s.std():.6f}, \"\n              f\"min={s.min():.6f}, max={s.max():.6f}, \"\n              f\"outliers={len(outliers)} ({100*len(outliers)/len(s):.1f}%)\")\n\n# Visualize distributions and time series\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfor i, col in enumerate(['forward_returns', 'market_forward_excess_returns']):\n    data = train_df[col].dropna()\n    axes[0, i].hist(data, bins=100, color='steelblue', edgecolor='black')\n    axes[0, i].set_title(f'Distribution: {col}')\n    temp = train_df[['date_id', col]].dropna().sort_values('date_id')\n    axes[1, i].plot(temp['date_id'], temp[col], color='crimson', linewidth=0.8)\n    axes[1, i].set_title(f'Time series: {col}')\nplt.tight_layout()\nplt.show()\n\n# Missing values\nmissing = train_df.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\nprint(f\"\\nFeatures with missing values: {len(missing)}\")\nprint(\"Top 10 by count:\")\nprint(missing.head(10))\n\nprint(f\"\\nMissing values in test: {test_df.isnull().sum().sum()}\")\n\n# Feature categorization\nprefixes = ['D', 'E', 'M', 'P', 'V']\ncategories = {f\"{p}_features\": [c for c in train_df.columns if c.startswith(p)] for p in prefixes}\ncategories['targets'] = targets\ncategories['ids'] = ['date_id']\n\nprint(\"\\nFeature distribution by category:\")\nfor cat, feats in categories.items():\n    if feats and cat != 'ids':\n        total = len(train_df) * len(feats)\n        missing_cells = train_df[feats].isnull().sum().sum()\n        pct = 100 * missing_cells / total if total > 0 else 0\n        print(f\"{cat}: {len(feats)} features, missing: {pct:.1f}%\")\n\n# Correlations with target\nif 'market_forward_excess_returns' in train_df.columns:\n    numeric = train_df.select_dtypes(include=[np.number]).columns.tolist()\n    numeric = [c for c in numeric if c not in ['date_id', 'market_forward_excess_returns']]\n    \n    corrs = []\n    for col in numeric:\n        pair = train_df[[col, 'market_forward_excess_returns']].dropna()\n        if len(pair) > 100:\n            r = pair[col].corr(pair['market_forward_excess_returns'])\n            corrs.append((col, r, len(pair)))\n    \n    corrs = sorted(corrs, key=lambda x: abs(x[1]), reverse=True)\n    print(\"\\nTop 10 features by correlation with market_forward_excess_returns:\")\n    for col, r, n in corrs[:10]:\n        print(f\"{col:<20} {r:>8.4f} (n={n})\")\n\n    # Correlation matrix\n    top_features = [c[0] for c in corrs[:8]] + ['market_forward_excess_returns']\n    if len(top_features) > 1:\n        cm = train_df[top_features].corr()\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='.3f', cmap='RdBu_r', center=0)\n        plt.title('Correlations: Top 8 features')\n        plt.tight_layout()\n        plt.show()\n\n# Example distributions by category\nsample_feats = [feats[0] for feats in categories.values() if feats and feats[0] not in targets + ['date_id']]\nn = len(sample_feats)\ncols = 3\nrows = (n + cols - 1) // cols\nfig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\naxes = axes.flatten()\n\nfor i, feat in enumerate(sample_feats):\n    data = train_df[feat].dropna()\n    axes[i].hist(data, bins=50, color='seagreen', edgecolor='black')\n    axes[i].set_title(f'{feat}')\n    stats_text = f'n={len(data):,}\\nμ={data.mean():.3f}\\nσ={data.std():.3f}'\n    axes[i].text(0.05, 0.95, stats_text, transform=axes[i].transAxes,\n                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white'))\n\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()\n\n# Final Sharpe ratio on target\nif 'market_forward_excess_returns' in train_df.columns:\n    s = train_df['market_forward_excess_returns'].dropna()\n    sharpe = s.mean() / s.std() if s.std() != 0 else np.nan\n    print(f\"\\nSharpe ratio (train): {sharpe:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:48:37.937951Z","iopub.execute_input":"2025-11-06T00:48:37.938295Z","iopub.status.idle":"2025-11-06T00:48:41.080304Z","shell.execute_reply.started":"2025-11-06T00:48:37.938264Z","shell.execute_reply":"2025-11-06T00:48:41.079226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom scipy.special import expit\n\nclass SubmissionFeatureEngineer:\n    def __init__(self):\n        self.lag_periods = [1, 2, 3, 5, 10, 20]\n        self.vol_windows = [5, 10, 20, 30]\n\n    def create_features(self, df):\n        df = df.copy()\n        if 'lagged_market_forward_excess_returns' in df.columns:\n            for w in self.vol_windows:\n                df[f'market_vol_{w}'] = df['lagged_market_forward_excess_returns'].rolling(w).std()\n                df[f'market_mom_{w}'] = df['lagged_market_forward_excess_returns'].rolling(w).mean()\n                df[f'market_zscore_{w}'] = (df['lagged_market_forward_excess_returns'] - df[f'market_mom_{w}']) / (df[f'market_vol_{w}'] + 1e-8)\n\n        for f in ['M4', 'V13', 'M1', 'S5', 'V3', 'V4']:\n            if f in df.columns:\n                for lag in self.lag_periods:\n                    df[f'{f}_lag_{lag}'] = df[f].shift(lag)\n\n        if 'M4' in df.columns and 'V13' in df.columns:\n            df['M4_V13_ratio'] = df['M4'] / (df['V13'] + 1e-8)\n        if 'V3' in df.columns and 'V4' in df.columns:\n            df['V3_V4_spread'] = df['V3'] - df['V4']\n\n        if 'market_vol_10' in df.columns:\n            df['high_vol_regime'] = (df['market_vol_10'] > df['market_vol_10'].quantile(0.7)).astype(int)\n            df['low_vol_regime'] = (df['market_vol_10'] < df['market_vol_10'].quantile(0.3)).astype(int)\n\n        if 'market_mom_5' in df.columns:\n            df['mom_accel'] = df['market_mom_5'].diff(1)\n\n        return df\n\nclass SubmissionAllocationStrategy:\n    def __init__(self, alpha=1.0, gamma=10.0):\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def dynamic_allocation(self, proba, vol, regime):\n        signal = 2 * (proba - 0.5)\n        confidence = np.maximum(proba, 1 - proba)\n        adjusted_signal = signal * confidence\n\n        regime_mult = 1.0\n        if regime == 'high_vol':\n            regime_mult, conf_thresh = 0.7, 0.65\n        elif regime == 'low_vol':\n            regime_mult, conf_thresh = 1.3, 0.55\n        else:\n            regime_mult, conf_thresh = 1.0, 0.6\n\n        vol_mult = 1.0\n        if vol > 0.02:\n            vol_mult = 0.8\n        elif vol < 0.005:\n            vol_mult = 1.2\n\n        s = expit(adjusted_signal * self.gamma)\n        allocation = 1.0 + self.alpha * vol_mult * regime_mult * (2 * s - 1)\n        allocation[confidence < conf_thresh] = 1.0\n        return np.clip(allocation, 0, 2)\n\ndef prepare_features(df, feature_names=None):\n    exclude = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n    X = df.drop(columns=exclude, errors='ignore')\n    if feature_names:\n        X = X[[c for c in feature_names if c in X.columns]]\n    else:\n        X = X.select_dtypes(include=[np.number])\n    X = X.fillna(X.median())\n    return X\n\ntrain_df = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\ntest_df  = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv')\n\nfe = SubmissionFeatureEngineer()\ntrain_enh = fe.create_features(train_df)\ntest_enh = fe.create_features(test_df)\n\nX_train = prepare_features(train_enh)\nX_test = prepare_features(test_enh, X_train.columns.tolist())\n\ny_train = (train_df['market_forward_excess_returns'] > 0.001).astype(int)\n\nmodel = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=7, random_state=42, verbosity=-1)\nmodel.fit(X_train, y_train)\n\ntest_proba = model.predict_proba(X_test)[:, 1]\n\nstrategy = SubmissionAllocationStrategy(alpha=1.0, gamma=10.0)\ntest_vol = train_df['market_forward_excess_returns'].std()\nregime = 'normal'\nif test_vol > 0.02:\n    regime = 'high_vol'\nelif test_vol < 0.005:\n    regime = 'low_vol'\nfinal_alloc = strategy.dynamic_allocation(test_proba, test_vol, regime)\n\nprint('Allocation range: [{:.4f}, {:.4f}]'.format(final_alloc.min(), final_alloc.max()))\nprint('Mean allocation: {:.4f}'.format(final_alloc.mean()))\nprint('Active signals: {:.2%}'.format(np.mean(final_alloc != 1.0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:48:41.081893Z","iopub.execute_input":"2025-11-06T00:48:41.082206Z","iopub.status.idle":"2025-11-06T00:48:43.189646Z","shell.execute_reply.started":"2025-11-06T00:48:41.082176Z","shell.execute_reply":"2025-11-06T00:48:43.1883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom scipy.special import expit\nfrom sklearn.preprocessing import StandardScaler\n\n# Импорт API сервера\nimport kaggle_evaluation.default_inference_server\n\n# Ваши классы и функции (оригинальные, для pandas)\nclass SubmissionFeatureEngineer:\n    def __init__(self):\n        self.lag_periods = [1, 2, 3, 5, 10]\n        self.vol_windows = [5, 10, 20]\n\n    def create_features(self, df):\n        # Работает с pandas DataFrame\n        df = df.copy()\n        if 'lagged_market_forward_excess_returns' in df.columns:\n            for w in self.vol_windows:\n                df[f'market_vol_{w}'] = df['lagged_market_forward_excess_returns'].rolling(w).std()\n                df[f'market_mom_{w}'] = df['lagged_market_forward_excess_returns'].rolling(w).mean()\n\n        for f in ['M4', 'V13', 'M1', 'S5', 'V3', 'V4']:\n            if f in df.columns:\n                for lag in self.lag_periods:\n                    df[f'{f}_lag_{lag}'] = df[f].shift(lag)\n\n        if 'M4' in df.columns and 'V13' in df.columns:\n            df['M4_V13_ratio'] = df['M4'] / (df['V13'] + 1e-8)\n        if 'V3' in df.columns and 'V4' in df.columns:\n            df['V3_V4_spread'] = df['V3'] - df['V4']\n\n        if 'market_vol_10' in df.columns:\n            df['high_vol_regime'] = (df['market_vol_10'] > df['market_vol_10'].quantile(0.7)).astype(int)\n\n        return df\n\nclass SubmissionAllocationStrategy:\n    def __init__(self):\n        self.alpha = 0.95\n        self.gamma = 19.5\n\n    def dynamic_allocation(self, proba, vol):\n        signal = float(2 * (proba - 0.5))\n        confidence = float(np.maximum(proba, 1 - proba))\n        adjusted_signal = signal * confidence\n\n        if vol > 0.02:\n            vol_mult, conf_thresh = 0.45, 0.7\n        elif vol < 0.005:\n            vol_mult, conf_thresh = 1.25, 0.55\n        else:\n            vol_mult, conf_thresh = 1.0, 0.6\n\n        s = float(expit(adjusted_signal * self.gamma))\n        allocation = float(1.0 + self.alpha * vol_mult * (2 * s - 1))\n        if confidence < conf_thresh:\n            allocation = 1.0\n        return float(np.clip(allocation, 0, 2))\n\ndef prepare_features(df, feature_names=None):\n    # Работает с pandas DataFrame\n    exclude = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'target']\n    X = df.drop(columns=exclude, errors='ignore')\n    if feature_names:\n        X = X[[c for c in feature_names if c in X.columns]]\n    else:\n        X = X.select_dtypes(include=[np.number])\n    X = X.fillna(X.median())\n    return X\n\n# ============ PATHS ============\nDATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n# Загрузка и предобработка ТОЛЬКО для обучения модели (всё как было)\ntrain_df = pd.read_csv(DATA_PATH / 'train.csv')\n\nfe = SubmissionFeatureEngineer()\ntrain_enh = fe.create_features(train_df)\n\nX_train_raw = prepare_features(train_enh)\ny_train = (train_df['market_forward_excess_returns'] > 0.001).astype(int)\n\n# Модель\nmodel = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=7, random_state=42, verbosity=-1)\nmodel.fit(X_train_raw, y_train)\n\n# Сохраняем фичи и стратегию\nfeature_names = X_train_raw.columns.tolist()\nstrategy = SubmissionAllocationStrategy()\ntrain_vol = train_df['market_forward_excess_returns'].std()\n\n# Scaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_raw)\n\nprint(\"Model and scaler trained.\")\n\n# ============================== PREDICTION FUNCTION FOR KAGGLE API ==============================\n\ndef predict(test_row_df):\n    \"\"\"\n    Функция, которую вызывает Kaggle API для каждой строки test.csv\n    test_row_df: pd.DataFrame или pl.DataFrame с одной строкой из test.csv\n    \"\"\"\n    # 1. Конвертируем в pandas, если пришёл polars\n    if isinstance(test_row_df, pl.DataFrame):\n        test_row_pd = test_row_df.to_pandas()\n    else:\n        test_row_pd = test_row_df\n\n    # 2. Feature Engineering (на одной строке, используя оригинальный код для pandas)\n    test_enh_pd = fe.create_features(test_row_pd)\n\n    # 3. Prepare Features (тоже для pandas)\n    X_test_pd = prepare_features(test_enh_pd, feature_names)\n\n    # 4. Fill NaN if any (e.g. from rolling/lag on first rows)\n    # Заполняем NaN из трейна (X_train_raw)\n    for col in X_test_pd.columns:\n        if X_test_pd[col].isna().any():\n            X_test_pd[col] = X_test_pd[col].fillna(X_train_raw[col].median())\n\n    # 5. Scale\n    X_test_scaled = scaler.transform(X_test_pd)\n\n    # 6. Predict\n    proba = float(model.predict_proba(X_test_scaled)[0, 1])\n    allocation = strategy.dynamic_allocation(proba, train_vol)\n\n    return float(allocation)\n\n# ============================== LAUNCH SERVER ==============================\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    print(\"Running local gateway for testing...\")\n    inference_server.run_local_gateway((str(DATA_PATH),))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:48:43.19113Z","iopub.execute_input":"2025-11-06T00:48:43.191582Z","iopub.status.idle":"2025-11-06T00:48:46.46119Z","shell.execute_reply.started":"2025-11-06T00:48:43.191414Z","shell.execute_reply":"2025-11-06T00:48:46.460079Z"}},"outputs":[],"execution_count":null}]}