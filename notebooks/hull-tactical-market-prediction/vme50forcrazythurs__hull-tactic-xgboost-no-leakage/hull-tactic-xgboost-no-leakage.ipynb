{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Thanks [ahsuna123](https://www.kaggle.com/code/ahsuna123/hull-tactic-no-overfitting-leakage) and [morodertobias](https://www.kaggle.com/code/morodertobias/hull-leak-safe-baseline) for sharing\n\næ„Ÿè°¢[ahsuna123](https://www.kaggle.com/code/ahsuna123/hull-tactic-no-overfitting-leakage)å’Œ[morodertobias](https://www.kaggle.com/code/morodertobias/hull-leak-safe-baseline)çš„åˆ†äº«","metadata":{}},{"cell_type":"markdown","source":"Remove the last 180 data points of train.csv from the training set to make the scores on the leaderboard meaningful\n\nå°†train.csvçš„æœ€åŽ180æ¡æ•°æ®ä»Žè®­ç»ƒé›†ä¸­åˆ é™¤ï¼Œä»¥ä½¿å¾—leaderboardä¸Šçš„åˆ†æ•°å…·å¤‡å®žé™…æ„ä¹‰\n\nPredicting positions directly from features\n\nç›´æŽ¥ä»Žç‰¹å¾ä¸­é¢„æµ‹ä»“ä½","metadata":{}},{"cell_type":"markdown","source":"If features such as 3-day rolling average are manually added, it will cause the score given by the rating function called by the code to be inconsistent with the submitted result, because after submission, the data is passed in item by item, and features cannot be extracted based on adjacent data\n\nå¦‚æžœæ‰‹åŠ¨æ·»åŠ ä¾‹å¦‚3æ—¥æ»šåŠ¨å¹³å‡ä¹‹ç±»çš„ç‰¹å¾ï¼Œä¼šå¯¼è‡´ä»£ç è°ƒç”¨çš„è¯„åˆ†å‡½æ•°ç»™å‡ºçš„åˆ†æ•°ä¸Žæäº¤ç»“æžœä¸ä¸€è‡´ï¼Œå› ä¸ºæäº¤åŽï¼Œæ•°æ®æ˜¯é€æ¡ä¼ å…¥çš„ï¼Œæ— æ³•åŸºäºŽç›¸é‚»æ•°æ®æå–ç‰¹å¾","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas.api.types\n\nMIN_INVESTMENT = 0\nMAX_INVESTMENT = 2\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n\n    This metric penalizes strategies that take on significantly more volatility\n    than the underlying market.\n\n    Returns:\n        float: The calculated adjusted Sharpe ratio.\n    \"\"\"\n\n    if not pandas.api.types.is_numeric_dtype(submission['prediction']):\n        raise ParticipantVisibleError('Predictions must be numeric')\n\n    solution = solution\n    solution['position'] = submission['prediction']\n\n    if solution['position'].max() > MAX_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n    if solution['position'].min() < MIN_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n\n    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n\n    # Calculate strategy's Sharpe ratio\n    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n    strategy_std = solution['strategy_returns'].std()\n\n    trading_days_per_yr = 252\n    if strategy_std == 0:\n        raise ParticipantVisibleError('Division by zero, strategy std is zero')\n    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n\n    # Calculate market return and volatility\n    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n    market_std = solution['forward_returns'].std()\n\n    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n\n    if market_volatility == 0:\n        raise ParticipantVisibleError('Division by zero, market std is zero')\n\n    # Calculate the volatility penalty\n    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n    vol_penalty = 1 + excess_vol\n\n    # Calculate the return penalty\n    return_gap = max(\n        0,\n        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n    )\n    return_penalty = 1 + (return_gap**2) / 100\n\n    # Adjust the Sharpe ratio by the volatility and return penalty\n    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n    return min(float(adjusted_sharpe), 1_000_000)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-06T01:25:58.172474Z","iopub.execute_input":"2025-11-06T01:25:58.172734Z","iopub.status.idle":"2025-11-06T01:25:59.533068Z","shell.execute_reply.started":"2025-11-06T01:25:58.172712Z","shell.execute_reply":"2025-11-06T01:25:59.532152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nimport pickle\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')\nimport kaggle_evaluation.default_inference_server\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\n\nclass FixedSP500Pipeline:\n    \"\"\"Fixed end-to-end pipeline for S&P 500 returns prediction\"\"\"\n\n    def __init__(self, feature_selection_k=50, manual_features=None):\n        \"\"\"\n        Args:\n            feature_selection_k: è‡ªåŠ¨ç‰¹å¾é€‰æ‹©æ—¶ä¿ç•™çš„ç‰¹å¾æ•°é‡\n            manual_features: æ‰‹åŠ¨æŒ‡å®šç‰¹å¾ååˆ—è¡¨ã€‚å¦‚æžœä¼ å…¥æ­¤å‚æ•°ï¼Œåˆ™ç›´æŽ¥ä½¿ç”¨è¿™äº›ç‰¹å¾è€Œéžè‡ªåŠ¨ç­›é€‰ã€‚\n        \"\"\"\n        self.feature_selection_k = feature_selection_k\n        self.manual_features = manual_features\n\n        self.scaler = RobustScaler()\n        self.feature_selector = SelectKBest(\n            lambda X, y: mutual_info_regression(X, y, random_state=42),\n            k=feature_selection_k\n        )\n\n        self.models = {\n            'xgb': xgb.XGBRegressor(\n                n_estimators=100,\n                learning_rate=0.01,\n                verbosity=0,\n                random_state=42,\n            ),\n        }\n\n        self.model_weights = {'xgb': 1}\n        self.feature_names = None\n        self.all_feature_names = None\n        # ç”¨äºŽä¿å­˜æœ€åˆï¼ˆæœªç»ç‰¹å¾å·¥ç¨‹ï¼‰çš„åŽŸå§‹ç‰¹å¾åç§°\n        self.original_features = None  # Store original feature list\n        self.is_fitted = False\n\n    def create_features(self, df_batch):\n        \"\"\"Create additional features - CONSISTENT between training and prediction\"\"\"\n        # df_batchï¼šä¼ å…¥çš„ä¸€æ‰¹æ•°æ®ï¼ˆå¯ä»¥æ˜¯æ•´ä¸ªè®­ç»ƒé›†æˆ–å•æ¡æµ‹è¯•æ•°æ®ï¼‰\n        df_features = df_batch.copy()\n\n        # Get base feature columns\n        # è¿‡æ»¤å‡ºåŸºç¡€ç‰¹å¾åˆ—ï¼ˆæŽ’é™¤æ—¥æœŸã€ç›®æ ‡ç­‰ï¼‰\n        feature_cols = [col for col in df_batch.columns\n                        if not col.startswith(('date_id', 'forward_returns',\n                                               'risk_free_rate', 'market_forward_excess_returns',\n                                               'is_scored', 'lagged_'))]\n        # ç”¨äºŽåŽç»­é¢„æµ‹é˜¶æ®µå°±èƒ½æ£€æŸ¥ç‰¹å¾æ˜¯å¦ä¸€è‡´\n        if self.original_features is None:\n            self.original_features = feature_cols\n\n        # Simple interaction\n        # åœ¨é‡‘èžå»ºæ¨¡ä¸­ï¼šE1 é€šå¸¸ä»£è¡¨ Earningsï¼ˆç›ˆåˆ©ç±»æŒ‡æ ‡ï¼‰ï¼›M1 é€šå¸¸ä»£è¡¨ Market factorï¼ˆå¸‚åœºåŠ¨é‡æˆ–å¸‚å€¼ç±»æŒ‡æ ‡ï¼‰ã€‚\n        # ä¸¤è€…ç›¸ä¹˜å¯ä»¥åæ˜ ï¼šâ€œå½“ç›ˆåˆ©å¼ºåŠ²ä¸”å¸‚åœºåŠ¨é‡ä¹Ÿé«˜â€ çš„å…±æŒ¯æ•ˆåº”ï¼Œå³å¸‚åœºä¸­å¼ºè€…æ’å¼ºçš„å¤åˆä¿¡å·ã€‚\n        # äº¤äº’é¡¹èƒ½å¤Ÿè®©æ¨¡åž‹å­¦ä¹ ç‰¹å¾é—´çš„éžçº¿æ€§å…³ç³»ï¼šä¾‹å¦‚ï¼šå½“ E1 å¾ˆé«˜ä½† M1 å¾ˆä½Ž â†’ ä¹Ÿè®¸ä¸æ˜¯å¥½ä¿¡å·ï¼›å½“ä¸¤è€…éƒ½é«˜ â†’ æœªæ¥å›žæŠ¥çŽ‡å¯èƒ½æ›´é«˜ã€‚\n        if 'E1' in df_features.columns and 'M1' in df_features.columns:\n            df_features['E1_M1_interaction'] = df_features['E1'] * df_features['M1']\n\n        df_features[\"U1\"] = df_features[\"I2\"] - df_features[\"I1\"]\n        df_features[\"U2\"] = df_features[\"M11\"] / ((df_features[\"I2\"] + df_features[\"I9\"] + df_features[\"I7\"]) / 3)\n\n        return df_features\n\n    def prepare_features(self, df, is_training=False):\n        \"\"\"Prepare features for modeling - CONSISTENT approach\"\"\"\n        # is_training: æ˜¯å¦ä¸ºè®­ç»ƒé˜¶æ®µï¼ˆå¸ƒå°”å€¼ï¼‰ï¼›True â†’ ä¼š fit scalerï¼ˆæ ‡å‡†åŒ–å™¨ï¼‰ï¼›False â†’ åª transformï¼ˆä¿è¯ä¸€è‡´æ€§ï¼‰\n\n        # è°ƒç”¨ä¸Šä¸€å±‚çš„ç‰¹å¾ç”Ÿæˆ\n        df_enhanced = self.create_features(df)\n\n        feature_cols = [col for col in df_enhanced.columns\n                        if not col.startswith(('date_id', 'forward_returns',\n                                               'risk_free_rate', 'market_forward_excess_returns',\n                                               'is_scored', 'lagged_'))]\n\n        X = df_enhanced[feature_cols].copy()\n\n        # Clip outlierså¼‚å¸¸å€¼æˆªæ–­ã€‚æ‰€æœ‰å°äºŽ 1% åˆ†ä½æ•°çš„å€¼ â†’ è®¾ä¸º q01ï¼›æ‰€æœ‰å¤§äºŽ 99% åˆ†ä½æ•°çš„å€¼ â†’ è®¾ä¸º q99ã€‚\n        for col in X.columns:\n            q99 = X[col].quantile(0.99)\n            q01 = X[col].quantile(0.01)\n            X[col] = X[col].clip(q01, q99)\n\n        # Fill missing values å…ˆç”¨æ¯ä¸€åˆ—çš„å‡å€¼å¡«è¡¥ç¼ºå¤±ï¼›å¦‚æžœä»ç„¶æœ‰æ²¡å¡«å®Œçš„ï¼ˆä¾‹å¦‚æ•´åˆ—æ˜¯ NaNï¼‰ï¼Œåˆ™å¡« 0ã€‚\n        X = X.fillna(X.mean()).fillna(0)\n\n        # is_training = Trueï¼šâ†’ æœ‰å®Œæ•´çš„è®­ç»ƒæ•°æ®ï¼ˆå«ç›®æ ‡å˜é‡ï¼‰ï¼Œéœ€è¦æ‹Ÿåˆï¼ˆfitï¼‰æ ‡å‡†åŒ–å™¨ï¼Œç¡®å®šæ•°æ®çš„ç»Ÿè®¡ç‰¹å¾ï¼ˆå‡å€¼ã€åˆ†ä½æ•°ç­‰ï¼‰ã€‚\n        # is_training = Falseï¼šâ†’ é¢„æµ‹é˜¶æ®µï¼Œä½¿ç”¨å‰é¢â€œè®­ç»ƒé˜¶æ®µâ€ä¿å­˜çš„æ ‡å‡†åŒ–å™¨å‚æ•°è¿›è¡Œè½¬æ¢ï¼ˆtransformï¼‰ï¼Œä¿è¯ç¼©æ”¾æ–¹å¼ä¸€è‡´ã€‚\n        # å¦åˆ™ï¼Œå¦‚æžœåœ¨æµ‹è¯•æ•°æ®ä¸Šé‡æ–° fitï¼Œå°±ä¼šå¼•å…¥â€œæœªæ¥ä¿¡æ¯â€ï¼Œé€ æˆæ•°æ®æ³„éœ²ã€‚\n        if is_training:\n            # ä½¿ç”¨ RobustScaler å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œé²æ£’æ ‡å‡†åŒ–ï¼ˆrobust scalingï¼‰\n            X_scaled = self.scaler.fit_transform(X)\n            self.all_feature_names = list(X.columns)\n        else:\n            # é¢„æµ‹æ—¶ï¼Œå¦‚æžœå½“å‰è¾“å…¥ X ç¼ºå°‘æŸäº›ç‰¹å¾åˆ—ï¼Œåˆ™åœ¨è¿™é‡Œè‡ªåŠ¨è¡¥ä¸Šï¼Œé»˜è®¤å€¼ä¸º 0\n            for col in self.all_feature_names:\n                if col not in X.columns:\n                    X[col] = 0\n            # é‡æ–°æŽ’åˆ—åˆ—é¡ºåº\n            X = X[self.all_feature_names]\n            # åº”ç”¨å·²æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨\n            X_scaled = self.scaler.transform(X)\n        # å°†ç¼©æ”¾åŽçš„ NumPy æ•°ç»„é‡æ–°åŒ…è£…æˆ DataFrameï¼Œä¿æŒç»“æž„ä¿¡æ¯\n        return pd.DataFrame(X_scaled, columns=self.all_feature_names, index=df.index)\n\n    def select_features(self, X_full):\n        \"\"\"\n        æ ¹æ®å½“å‰é…ç½®æ‰§è¡Œç‰¹å¾é€‰æ‹©ï¼š\n        - å¦‚æžœè®¾ç½®äº† manual_featuresï¼Œåˆ™ç›´æŽ¥ä½¿ç”¨è¿™äº›åˆ—ï¼›\n        - å¦åˆ™ç”¨ SelectKBest(mutual_info_regression) è‡ªåŠ¨ç­›é€‰ã€‚\n        \"\"\"\n        # self.manual_features='E19', 'I2', 'I4', 'P10', 'P11', 'P5', 'S8', 'V13', 'V7', 'V9'\n        self.manual_features = 'E19','I4', 'P11','S8', 'V13','V9'\n        \n\n        # æ‰‹åŠ¨æ¨¡å¼\n        if self.manual_features is not None:\n            print(f\"âš™ï¸ Using {len(self.manual_features)} manually specified features\")\n            # æ£€æŸ¥ç¼ºå¤±åˆ—\n            missing = [f for f in self.manual_features if f not in X_full.columns]\n            if missing:\n                print(f\"âš ï¸ Warning: Missing features in data: {missing}\")\n            self.feature_names = [f for f in self.manual_features if f in X_full.columns]\n            X_selected = X_full[self.feature_names].copy()\n\n        # è‡ªåŠ¨æ¨¡å¼\n        else:\n            print(f\"ðŸ” Automatically selecting top {self.feature_selection_k} features...\")\n            X_selected = self.feature_selector.transform(X_full)\n            selected_indices = self.feature_selector.get_support(indices=True)\n            self.feature_names = [self.all_feature_names[i] for i in selected_indices]\n\n        X_selected = pd.DataFrame(X_selected, columns=self.feature_names)\n\n        print(\"Selected features:\", self.feature_names)\n        \n        return X_selected\n\n    def fit_from_file(self, train_path, target_col='forward_returns'):\n        \"\"\"Fit the model using fixed train/test split (8810 train, 180 test)\"\"\"\n        print(\"Loading training data...\")\n        train_df = pd.read_csv(train_path)\n\n        print(\"Preparing features...\")\n        X_full = self.prepare_features(train_df, is_training=True)\n\n        # è®¡ç®—æœ€ä¼˜ä»“ä½ä½œä¸ºç›®æ ‡å˜é‡\n        market_excess_returns = train_df['forward_returns'] - train_df['risk_free_rate']\n        market_excess_cumulative = (1 + market_excess_returns).prod()\n        market_mean_excess_return = (market_excess_cumulative) ** (1 / len(train_df)) - 1\n        c = (1 + market_mean_excess_return) ** (1 / (market_excess_returns > 0).mean()) - 1\n        train_df[\"target\"] = (c / market_excess_returns).clip(0, 2)\n        y = train_df[\"target\"].fillna(0)\n\n        print(\"best score (full data):\", score(train_df, pd.DataFrame({'prediction': y}), ''))\n        # ==========================================================\n\n        split_point = 8000\n        \n        # ===== è°ƒç”¨ç‰¹å¾é€‰æ‹©å‡½æ•° =====\n        self.feature_selector.fit(X_full[:split_point], y[:split_point])\n        X_final = self.select_features(X_full)\n\n        \n        X_train, X_test = X_final.iloc[:split_point], X_final.iloc[split_point:]\n        y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n        # èŽ·å–æµ‹è¯•é›†çš„solutionæ•°æ®å¹¶é‡ç½®ç´¢å¼•\n        test_solution = train_df.iloc[split_point:].copy().reset_index(drop=True)\n\n        results = {}\n        for name, model in self.models.items():\n            try:\n                print(f\"\\nTraining {name} model...\")\n                model.fit(X_train, y_train)\n\n                y_train_pred = model.predict(X_train).clip(0, 2)\n                y_test_pred = model.predict(X_test).clip(0, 2)\n\n                # è®¡ç®—æŒ‡æ ‡\n                rmse_train = np.sqrt(np.mean((y_train_pred - y_train) ** 2))\n                rmse_test = np.sqrt(np.mean((y_test_pred - y_test) ** 2))\n\n                # Sharpe-likeå¾—åˆ†ï¼ˆç”¨æ¯”èµ›è‡ªå¸¦scoreå‡½æ•°ï¼‰\n                score_train = score(\n                    train_df.iloc[:split_point],\n                    pd.DataFrame({'prediction': y_train_pred}),\n                    ''\n                )\n                score_test = score(\n                    test_solution,  # ä½¿ç”¨å·²ç»é‡ç½®ç´¢å¼•çš„æµ‹è¯•é›†solution\n                    pd.DataFrame({'prediction': y_test_pred}),\n                    ''\n                )\n\n                print(f\"Results for {name}:\")\n                print(f\"  RMSE_train={rmse_train:.6f}, RMSE_test={rmse_test:.6f}\")\n                print(f\"  Sharpe_train={score_train:.6f}, Sharpe_test={score_test:.6f}\")\n\n            except Exception as e:\n                print(f\"Error training {name}: {e}\")\n                self.model_weights[name] = 0\n\n        self.is_fitted = True\n\n        return self\n\n    def predict_batch(self, df_batch):\n        if not self.is_fitted:\n            raise ValueError(\"Model must be fitted before making predictions\")\n\n        X_full = self.prepare_features(df_batch, is_training=False)\n        X_final = self.select_features(X_full)\n\n        predictions = np.zeros(len(X_final))\n        for name, model in self.models.items():\n            # dict.get(key, default) defaultï¼šå¦‚æžœé”®ä¸å­˜åœ¨ï¼Œå°±è¿”å›žè¿™ä¸ªé»˜è®¤å€¼ï¼ˆè€Œä¸æ˜¯æŠ¥é”™ï¼‰\n            if self.model_weights.get(name, 0) > 0:\n                try:\n                    pred = model.predict(X_final)\n                    predictions += self.model_weights[name] * pred\n                except Exception as e:\n                    print(f\"Error in prediction with {name}: {e}\")\n\n        return predictions[0] if len(predictions) == 1 else predictions\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:25:59.534627Z","iopub.execute_input":"2025-11-06T01:25:59.535229Z","iopub.status.idle":"2025-11-06T01:26:07.596978Z","shell.execute_reply.started":"2025-11-06T01:25:59.535199Z","shell.execute_reply":"2025-11-06T01:26:07.596296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_prediction_function():\n    \"\"\"Create prediction function that captures trained model\"\"\"\n    pipeline = FixedSP500Pipeline(feature_selection_k=10)\n\n    train_path = '/kaggle/input/hull-tactical-market-prediction/train.csv'\n    if os.path.exists(train_path):\n        pipeline.fit_from_file(train_path)\n    else:\n        print(\"Warning: No training data found\")\n        return lambda test: 0.0\n\n    def predict(test: pl.DataFrame) -> float:\n        try:\n            test_df = test.to_pandas()\n            prediction = pipeline.predict_batch(test_df)\n            signal = np.clip(prediction, 0.0, 2.0)\n            return float(signal)\n        except Exception as e:\n            print(f\"Prediction error: {e}\")\n            return 1.0\n\n    return predict\n\n\npredict = create_prediction_function()\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif __name__ == \"__main__\":\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        inference_server.serve()\n    else:\n        inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:26:07.601104Z","iopub.execute_input":"2025-11-06T01:26:07.601312Z","iopub.status.idle":"2025-11-06T01:26:15.394847Z","shell.execute_reply.started":"2025-11-06T01:26:07.601294Z","shell.execute_reply":"2025-11-06T01:26:15.394133Z"}},"outputs":[],"execution_count":null}]}