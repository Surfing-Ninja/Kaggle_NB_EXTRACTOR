{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Description\nWisdom from most personal finance experts would suggest that it's irresponsible to try and time the market. The Efficient Market Hypothesis (EMH) would agree: everything knowable is already priced in, so don’t bother trying.\n\nBut in the age of machine learning, is it irresponsible to not try and time the market? Is the EMH an extreme oversimplification at best and possibly just…false?\n\n* This competition is about more than predictive modeling. Predicting market returns challenges the assumptions of market efficiency. Your work could help reshape how investors and academics understand financial markets. Participants could uncover signals others overlook, develop innovative strategies, and contribute to a deeper understanding of market behavior—potentially rewriting a fundamental principle of modern finance. Most investors don’t beat the S&P 500. That failure has been used for decades to prop up EMH: If even the professionals can’t win, it must be impossible. This observation has long been cited as evidence for the Efficient Market Hypothesis the idea that prices already reflect all available information and no persistent edge is possible. This story is tidy, but reality is less so. Markets are noisy, messy, and full of behavioral quirks that don’t vanish just because academic orthodoxy said they should.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------=-=-=-=-=-=-=-=-=-=-=\n# Market Prediction Notebook \n# -------------------------=-=-=-=-=-=-=-=-=-=-=\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n# 0. Import libraries\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\nimport os, sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import joblib   # ❌ commented out (no joblib save required)\n\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom xgboost import XGBRegressor\n\n# ---------- Safety: confirm network is off ----------\nimport socket\ndef can_connect(host=\"8.8.8.8\", port=53, timeout=1):\n    try:\n        socket.create_connection((host, port), timeout=timeout)\n        return True\n    except Exception:\n        return False\n\nprint(\"Network reachable?\", can_connect())   # Expect False\n\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n# ----------------------------\n# 1. Load Data\n# ----------------------------\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n\nTRAIN_PATH = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\nTEST_PATH  = \"/kaggle/input/hull-tactical-market-prediction/test.csv\"\n\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape: \", test.shape)\n\n# ----------------------------\n# 2. Quick Data Check\n# ----------------------------\nTARGET = \"market_forward_excess_returns\"\nif TARGET not in train.columns:\n    raise SystemExit(f\"Target column '{TARGET}' not found in train!\")\n\nprint(\"Train columns:\", train.columns.tolist())\nprint(\"Test columns:\", test.columns.tolist())\n\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n# ----------------------------\n# 3. Prepare features\n# ----------------------------\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n\nX = train.drop(columns=[TARGET]).copy()\ny = train[TARGET].copy()\nX_test = test.copy()\n\ncommon_cols = [c for c in X.columns if c in X_test.columns]\nprint(\"Number of common columns:\", len(common_cols))\n\nif len(common_cols) == 0:\n    raise SystemExit(\"No overlapping feature columns between train and test.\")\n\nX = X[common_cols].copy()\nX_test = X_test[common_cols].copy()\n\nfor col in common_cols:\n    X[col] = pd.to_numeric(X[col], errors='coerce')\n    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n\nmedians = X.median()\nX = X.fillna(medians)\nX_test = X_test.fillna(medians)\n\nprint(\"Final feature count (train):\", X.shape[1])\nprint(\"Any NaNs in X?\", X.isnull().sum().sum())\nprint(\"Any NaNs in X_test?\", X_test.isnull().sum().sum())\n\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n# ----------------------------\n# 4. Scaling\n# ----------------------------\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_test_scaled = scaler.transform(X_test)\n\n# joblib.dump(scaler, \"scaler.joblib\")   # ❌ commented out\n\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n# ----------------------------\n# 5. Model Training\n# ----------------------------\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\nparams = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    'random_state': 42,\n    'verbosity': 0\n}\n\nxgb_model = XGBRegressor(**params)\n\ngrid = {\n    'max_depth': [3, 5],\n    'learning_rate': [0.01, 0.05],\n    'n_estimators': [100, 200]\n}\n\nts_split = TimeSeriesSplit(n_splits=5)\n\nsearch = GridSearchCV(\n    xgb_model,\n    param_grid=grid,\n    cv=ts_split,\n    scoring='neg_mean_squared_error',\n    verbose=1,\n    n_jobs=1\n)\n\nsearch.fit(X_scaled, y)\n\nprint(\"Best params:\", search.best_params_)\nbest_model = search.best_estimator_\n\n# joblib.dump(best_model, \"best_model.joblib\")   # ❌ commented out\n\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n# ----------------------------\n# 6. Evaluate on Train\n# ----------------------------\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\npreds_train = best_model.predict(X_scaled)\nrmse = np.sqrt(mean_squared_error(y, preds_train))\nr2 = r2_score(y, preds_train)\nprint(\"Train RMSE:\", rmse)\nprint(\"Train R2:\", r2)\n\nplt.figure(figsize=(8,5))\nplt.scatter(y, preds_train, alpha=0.6, s=8)\nplt.xlabel(\"Actual Returns\")\nplt.ylabel(\"Predicted Returns\")\nplt.title(\"Actual vs Predicted (Train)\")\nplt.grid(alpha=0.3)\nplt.show()\n\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\n# ----------------------------\n# 7. Predict on Test & Save\n# ----------------------------\n# =-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=\ntest_predictions = best_model.predict(X_test_scaled)\n\nsubmission = pd.DataFrame({\n    'row_id': np.arange(len(test_predictions)),\n    'prediction': test_predictions\n})\n\nprint(submission.head())\n\n# ❌ Commented out CSV save\n# submission.to_csv(\"submission.csv\", index=False)\n\n# ✅ Keep only parquet for Kaggle submission\nsubmission.to_parquet(\"submission.parquet\", index=False)\n\nprint(\"Saved: submission.parquet\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}