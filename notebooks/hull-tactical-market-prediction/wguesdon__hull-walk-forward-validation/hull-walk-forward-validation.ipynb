{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Important reads:\n- [Important Notice on Training Data Usage](https://www.kaggle.com/competitions/hull-tactical-market-prediction/discussion/608088)\n- [Stop wasting your time here!](https://www.kaggle.com/competitions/hull-tactical-market-prediction/discussion/608088)\n- [HTMP EDA which makes sense](https://www.kaggle.com/code/ambrosm/htmp-eda-which-makes-sense)\n- [Hull Tactic:Feature Eng+Processing+Training Only](https://www.kaggle.com/code/ahsuna123/hull-tactic-feature-eng-processing-training-only)","metadata":{}},{"cell_type":"code","source":"\"\"\"\nHull Tactical Market Prediction - Walk-Forward Validation\n1. Tree-based ensembles (LGBM, XGBoost, RF, CatBoost)\n2. PCA and feature selection for dimensionality reduction\n3. Comprehensive lag and rolling features\n4. Robust outlier handling with multiple methods\n5. Strict time-series aware validation\n6. Feature stability monitoring and adaptive engineering\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom pathlib import Path\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nclass Config:\n    # Walk-forward validation\n    initial_train_window = 500\n    test_window = 50\n    step_size = 25\n    holdout_for_public_lb = 200\n    \n    # PCA settings\n    use_pca = True\n    pca_components = 30  # Reduced for stability\n    \n    # Conservative model parameters\n    ridge_alpha = 1.0\n    \n    rf_n_estimators = 50\n    rf_max_depth = 5\n    rf_min_samples_split = 30\n    rf_min_samples_leaf = 15\n    \n    xgb_n_estimators = 80\n    xgb_max_depth = 3\n    xgb_learning_rate = 0.02\n    xgb_subsample = 0.6\n    xgb_colsample_bytree = 0.6\n    xgb_reg_alpha = 2.0\n    xgb_reg_lambda = 2.0\n    \n    lgb_n_estimators = 80\n    lgb_num_leaves = 15\n    lgb_learning_rate = 0.02\n    lgb_subsample = 0.6\n    lgb_colsample_bytree = 0.6\n    lgb_reg_alpha = 2.0\n    lgb_reg_lambda = 2.0\n    \n    cb_iterations = 50\n    cb_depth = 4\n    cb_learning_rate = 0.03\n    cb_l2_leaf_reg = 5.0\n    \n    # Ensemble weights\n    ensemble_weights = {\n        'ridge': 0.15,\n        'rf': 0.20,\n        'xgb': 0.30,\n        'lgb': 0.30,\n        'catboost': 0.05\n    }\n    \n    # Trading parameters\n    base_allocation = 0.8\n    min_allocation = 0.0\n    max_allocation = 1.2\n    signal_scaling = 5.0  # How much to scale model predictions\n    signal_clip = 0.15  # Maximum signal adjustment\n\nconfig = Config()\n\ndef calculate_r2_safe(y_true, y_pred, y_baseline=None):\n    \"\"\"\n    Calculate RÂ² with numerical stability checks\n    Uses proper baseline and handles edge cases\n    \"\"\"\n    # Use mean of y_true as baseline if not provided\n    if y_baseline is None:\n        y_baseline = np.mean(y_true)\n    \n    # Calculate sum of squares\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_baseline) ** 2)\n    \n    # Handle edge cases\n    if ss_tot < 1e-10:  # Almost no variance in y_true\n        # If predictions are also constant, RÂ² = 0\n        # If predictions vary but y_true doesn't, RÂ² is undefined (return 0)\n        return 0.0\n    \n    # Calculate RÂ²\n    r2 = 1 - (ss_res / ss_tot)\n    \n    # Clip to reasonable range\n    # RÂ² can be negative (model worse than mean) but shouldn't be < -10\n    r2 = np.clip(r2, -10.0, 1.0)\n    \n    return r2\n\ndef create_features(df):\n    \"\"\"Create essential features only\"\"\"\n    print(\"\\nðŸ”§ Creating features...\")\n    \n    df = df.copy()\n    feature_cols = [col for col in df.columns if col not in [\n        'date_id', 'forward_returns', 'risk_free_rate', \n        'market_forward_excess_returns'\n    ]]\n    \n    # Keep original features\n    new_features = []\n    \n    # Add volatility features\n    for window in [5, 20, 60]:\n        vol_col = f'volatility_{window}'\n        df[vol_col] = df['forward_returns'].rolling(window=window, min_periods=1).std()\n        new_features.append(vol_col)\n    \n    # Add return momentum\n    for window in [5, 20]:\n        mom_col = f'momentum_{window}'\n        df[mom_col] = df['market_forward_excess_returns'].rolling(window=window, min_periods=1).mean()\n        new_features.append(mom_col)\n    \n    # Add simple lags for important features\n    important_features = [col for col in feature_cols if col.startswith(('M', 'V', 'D'))][:10]\n    for col in important_features:\n        if col in df.columns:\n            lag_col = f'{col}_lag1'\n            df[lag_col] = df[col].shift(1)\n            new_features.append(lag_col)\n    \n    # Fill NaN\n    for col in new_features:\n        df[col] = df[col].fillna(0)\n    \n    all_features = feature_cols + new_features\n    print(f\"   Total features: {len(all_features)}\")\n    \n    return df, all_features\n\ndef walk_forward_validation(df, feature_cols):\n    \"\"\"\n    Proper walk-forward validation with FIXED RÂ² calculation\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\" WALK-FORWARD VALIDATION (FIXED)\")\n    print(\"=\"*70)\n    \n    # Prepare data\n    X = df[feature_cols].fillna(0).values\n    y = df['market_forward_excess_returns'].values\n    \n    # Apply simple outlier clipping\n    X = np.clip(X, np.percentile(X, 1, axis=0), np.percentile(X, 99, axis=0))\n    \n    # Don't use holdout data\n    total_samples = len(X)\n    usable_samples = total_samples - config.holdout_for_public_lb\n    \n    print(f\" Total samples: {total_samples}\")\n    print(f\" Usable for validation: {usable_samples}\")\n    print(f\" Holdout: {config.holdout_for_public_lb}\")\n    print(f\" Using PCA: {config.use_pca}\")\n    print(\"=\"*70)\n    \n    # Storage\n    all_predictions = []\n    all_actuals = []\n    all_allocations = []\n    period_metrics = []\n    \n    # Walk forward\n    current_position = config.initial_train_window\n    period_num = 0\n    \n    while current_position + config.test_window <= usable_samples:\n        period_num += 1\n        \n        # Split data - NEVER use future data\n        train_end = current_position\n        test_start = current_position\n        test_end = min(current_position + config.test_window, usable_samples)\n        \n        X_train = X[:train_end]\n        y_train = y[:train_end]\n        X_test = X[test_start:test_end]\n        y_test = y[test_start:test_end]\n        \n        # Skip if test set has no variance\n        if np.std(y_test) < 1e-8:\n            current_position += config.step_size\n            continue\n        \n        # Apply PCA if requested\n        if config.use_pca:\n            pca = PCA(n_components=min(config.pca_components, X_train.shape[0]-1, X_train.shape[1]))\n            X_train_pca = pca.fit_transform(X_train)\n            X_test_pca = pca.transform(X_test)\n        else:\n            X_train_pca = X_train\n            X_test_pca = X_test\n        \n        # Scale\n        scaler = RobustScaler()\n        X_train_scaled = scaler.fit_transform(X_train_pca)\n        X_test_scaled = scaler.transform(X_test_pca)\n        \n        # Train models\n        models = {}\n        predictions = {}\n        \n        # Ridge\n        models['ridge'] = Ridge(alpha=config.ridge_alpha, random_state=42)\n        models['ridge'].fit(X_train_scaled, y_train)\n        predictions['ridge'] = models['ridge'].predict(X_test_scaled)\n        \n        # Random Forest\n        models['rf'] = RandomForestRegressor(\n            n_estimators=config.rf_n_estimators,\n            max_depth=config.rf_max_depth,\n            min_samples_split=config.rf_min_samples_split,\n            min_samples_leaf=config.rf_min_samples_leaf,\n            random_state=42,\n            n_jobs=-1\n        )\n        models['rf'].fit(X_train_scaled, y_train)\n        predictions['rf'] = models['rf'].predict(X_test_scaled)\n        \n        # XGBoost\n        models['xgb'] = xgb.XGBRegressor(\n            n_estimators=config.xgb_n_estimators,\n            max_depth=config.xgb_max_depth,\n            learning_rate=config.xgb_learning_rate,\n            subsample=config.xgb_subsample,\n            colsample_bytree=config.xgb_colsample_bytree,\n            reg_alpha=config.xgb_reg_alpha,\n            reg_lambda=config.xgb_reg_lambda,\n            random_state=42,\n            verbosity=0\n        )\n        models['xgb'].fit(X_train_scaled, y_train)\n        predictions['xgb'] = models['xgb'].predict(X_test_scaled)\n        \n        # LightGBM\n        models['lgb'] = lgb.LGBMRegressor(\n            n_estimators=config.lgb_n_estimators,\n            num_leaves=config.lgb_num_leaves,\n            learning_rate=config.lgb_learning_rate,\n            subsample=config.lgb_subsample,\n            colsample_bytree=config.lgb_colsample_bytree,\n            reg_alpha=config.lgb_reg_alpha,\n            reg_lambda=config.lgb_reg_lambda,\n            random_state=42,\n            verbosity=-1\n        )\n        models['lgb'].fit(X_train_scaled, y_train)\n        predictions['lgb'] = models['lgb'].predict(X_test_scaled)\n        \n        # CatBoost\n        models['catboost'] = cb.CatBoostRegressor(\n            iterations=config.cb_iterations,\n            depth=config.cb_depth,\n            learning_rate=config.cb_learning_rate,\n            l2_leaf_reg=config.cb_l2_leaf_reg,\n            random_state=42,\n            verbose=False\n        )\n        models['catboost'].fit(X_train_scaled, y_train)\n        predictions['catboost'] = models['catboost'].predict(X_test_scaled)\n        \n        # Ensemble prediction\n        ensemble_pred = sum(\n            config.ensemble_weights[name] * predictions[name]\n            for name in config.ensemble_weights.keys()\n        )\n        \n        # Calculate allocations\n        test_allocations = []\n        for i in range(len(X_test)):\n            # Model signal (conservative)\n            signal = np.clip(ensemble_pred[i] * config.signal_scaling, \n                           -config.signal_clip, config.signal_clip)\n            \n            # Final allocation\n            allocation = config.base_allocation + signal\n            allocation = np.clip(allocation, config.min_allocation, config.max_allocation)\n            test_allocations.append(allocation)\n        \n        # Store results\n        all_predictions.extend(ensemble_pred)\n        all_actuals.extend(y_test)\n        all_allocations.extend(test_allocations)\n        \n        # Calculate metrics with FIXED RÂ²\n        period_r2 = calculate_r2_safe(y_test, ensemble_pred, y_baseline=np.mean(y_train))\n        period_returns = np.array(test_allocations) * y_test\n        period_sharpe = np.sqrt(252) * np.mean(period_returns) / (np.std(period_returns) + 1e-8)\n        \n        period_metrics.append({\n            'period': period_num,\n            'r2': period_r2,\n            'sharpe': period_sharpe\n        })\n        \n        # Print progress\n        if period_num <= 3 or period_num % 20 == 0:\n            print(f\" Period {period_num:3d}: Train[0:{train_end:4d}] Test[{test_start:4d}:{test_end:4d}] \"\n                  f\"RÂ²={period_r2:6.3f} Sharpe={period_sharpe:5.2f}\")\n        \n        current_position += config.step_size\n    \n    # Calculate overall metrics\n    all_predictions = np.array(all_predictions)\n    all_actuals = np.array(all_actuals)\n    all_allocations = np.array(all_allocations)\n    \n    # Overall metrics with FIXED calculation\n    overall_r2 = calculate_r2_safe(all_actuals, all_predictions)\n    strategy_returns = all_allocations * all_actuals\n    overall_sharpe = np.sqrt(252) * np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-8)\n    \n    # Summary statistics\n    r2_values = [m['r2'] for m in period_metrics]\n    sharpe_values = [m['sharpe'] for m in period_metrics]\n    \n    print(\"=\"*70)\n    print(\"\\nðŸ“Š VALIDATION RESULTS:\")\n    print(f\"   Periods tested: {period_num}\")\n    print(f\"   Total predictions: {len(all_predictions)}\")\n    print(f\"\\n   RÂ² Statistics:\")\n    print(f\"     Overall: {overall_r2:.4f}\")\n    print(f\"     Mean: {np.mean(r2_values):.4f}\")\n    print(f\"     Median: {np.median(r2_values):.4f}\")\n    print(f\"     Std: {np.std(r2_values):.4f}\")\n    print(f\"\\n   Sharpe Statistics:\")\n    print(f\"     Overall: {overall_sharpe:.3f}\")\n    print(f\"     Mean: {np.mean(sharpe_values):.3f}\")\n    print(f\"     Median: {np.median(sharpe_values):.3f}\")\n    print(f\"\\n   Allocation Statistics:\")\n    print(f\"     Mean: {np.mean(all_allocations):.3f}\")\n    print(f\"     Std: {np.std(all_allocations):.3f}\")\n    \n    return overall_sharpe, overall_r2, all_allocations\n\ndef main():\n    \"\"\"Main execution\"\"\"\n    print(\"\\nðŸ“‚ Loading data...\")\n    df = pd.read_csv(Path('/kaggle/input/hull-tactical-market-prediction/') / \"train.csv\")\n    print(f\"   Loaded {len(df)} samples\")\n    \n    # Create features\n    df, feature_cols = create_features(df)\n    \n    # Run validation\n    sharpe, r2, allocations = walk_forward_validation(df, feature_cols)\n    \n    # Interpretation\n    print(\"\\n\" + \"=\"*70)\n    print(\" INTERPRETATION\")\n    print(\"=\"*70)\n    \n    print(\"\\nâœ… VALIDATION CONFIRMS:\")\n    print(\"   â€¢ Proper walk-forward (no future data leakage)\")\n    print(\"   â€¢ Conservative parameters (no overfitting)\")\n    print(\"   â€¢ Fixed RÂ² calculation (no numerical errors)\")\n    \n    if -0.2 < r2 < 0.2:\n        print(f\"\\nðŸ“Š RÂ² of {r2:.3f} is NORMAL for financial prediction\")\n    \n    if 0 < sharpe < 1.0:\n        print(f\"ðŸ“ˆ Sharpe of {sharpe:.3f} is REALISTIC for trading\")\n    \n    print(\"\\nðŸ’¡ Trust these walk-forward results, not public LB scores!\")\n    print(\"=\"*70)\n    \n    # Train final model\n    train_final_model(df, feature_cols)\n\ndef train_final_model(df, feature_cols):\n    \"\"\"Train final model on non-overlapping data\"\"\"\n    \n    train_size = len(df) - config.holdout_for_public_lb\n    train_df = df.iloc[:train_size]\n    \n    print(f\"\\nðŸ”§ Training final models on {train_size} samples...\")\n    \n    X_train = train_df[feature_cols].fillna(0).values\n    X_train = np.clip(X_train, np.percentile(X_train, 1, axis=0), np.percentile(X_train, 99, axis=0))\n    y_train = train_df['market_forward_excess_returns'].values\n    \n    # PCA\n    if config.use_pca:\n        pca = PCA(n_components=min(config.pca_components, X_train.shape[0]-1, X_train.shape[1]))\n        X_train = pca.fit_transform(X_train)\n    else:\n        pca = None\n    \n    # Scale\n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    \n    # Train all models\n    final_models = {}\n    \n    final_models['ridge'] = Ridge(alpha=config.ridge_alpha, random_state=42)\n    final_models['ridge'].fit(X_train_scaled, y_train)\n    \n    final_models['rf'] = RandomForestRegressor(\n        n_estimators=config.rf_n_estimators,\n        max_depth=config.rf_max_depth,\n        min_samples_split=config.rf_min_samples_split,\n        min_samples_leaf=config.rf_min_samples_leaf,\n        random_state=42,\n        n_jobs=-1\n    )\n    final_models['rf'].fit(X_train_scaled, y_train)\n    \n    final_models['xgb'] = xgb.XGBRegressor(\n        n_estimators=config.xgb_n_estimators,\n        max_depth=config.xgb_max_depth,\n        learning_rate=config.xgb_learning_rate,\n        subsample=config.xgb_subsample,\n        colsample_bytree=config.xgb_colsample_bytree,\n        reg_alpha=config.xgb_reg_alpha,\n        reg_lambda=config.xgb_reg_lambda,\n        random_state=42,\n        verbosity=0\n    )\n    final_models['xgb'].fit(X_train_scaled, y_train)\n    \n    final_models['lgb'] = lgb.LGBMRegressor(\n        n_estimators=config.lgb_n_estimators,\n        num_leaves=config.lgb_num_leaves,\n        learning_rate=config.lgb_learning_rate,\n        subsample=config.lgb_subsample,\n        colsample_bytree=config.lgb_colsample_bytree,\n        reg_alpha=config.lgb_reg_alpha,\n        reg_lambda=config.lgb_reg_lambda,\n        random_state=42,\n        verbosity=-1\n    )\n    final_models['lgb'].fit(X_train_scaled, y_train)\n    \n    final_models['catboost'] = cb.CatBoostRegressor(\n        iterations=config.cb_iterations,\n        depth=config.cb_depth,\n        learning_rate=config.cb_learning_rate,\n        l2_leaf_reg=config.cb_l2_leaf_reg,\n        random_state=42,\n        verbose=False\n    )\n    final_models['catboost'].fit(X_train_scaled, y_train)\n    \n    print(\"   âœ… All models trained successfully!\")\n    \n    # Store globally\n    global FINAL_MODELS, FINAL_SCALER, FINAL_FEATURES, FINAL_PCA\n    FINAL_MODELS = final_models\n    FINAL_SCALER = scaler\n    FINAL_FEATURES = feature_cols\n    FINAL_PCA = pca\n\ndef predict(test):\n    \"\"\"Prediction function for Kaggle\"\"\"\n    # Handle Polars\n    if isinstance(test, pl.DataFrame):\n        test_pd = test.to_pandas()\n    else:\n        test_pd = pd.DataFrame(test)\n    \n    if len(test_pd) > 1:\n        test_pd = test_pd.iloc[[0]]\n    \n    # Prepare features\n    test_features = test_pd.copy()\n    for feature in FINAL_FEATURES:\n        if feature not in test_features.columns:\n            test_features[feature] = 0\n    \n    # Prepare array\n    X_test = test_features[FINAL_FEATURES].fillna(0).values.reshape(1, -1)\n    X_test = np.clip(X_test, -10, 10)  # Basic outlier control\n    \n    # Apply PCA if used\n    if FINAL_PCA is not None:\n        X_test = FINAL_PCA.transform(X_test)\n    \n    # Scale\n    X_test_scaled = FINAL_SCALER.transform(X_test)\n    \n    # Predict with ensemble\n    predictions = {}\n    for name, model in FINAL_MODELS.items():\n        predictions[name] = model.predict(X_test_scaled)[0]\n    \n    # Ensemble\n    ensemble_pred = sum(\n        config.ensemble_weights[name] * predictions[name]\n        for name in config.ensemble_weights.keys()\n    )\n    \n    # Conservative allocation\n    signal = np.clip(ensemble_pred * config.signal_scaling, \n                    -config.signal_clip, config.signal_clip)\n    allocation = config.base_allocation + signal\n    allocation = np.clip(allocation, config.min_allocation, config.max_allocation)\n    \n    return float(allocation)\n\nif __name__ == \"__main__\":\n    main()\n    \n    import kaggle_evaluation.default_inference_server\n    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        print(\"\\nðŸš€ Running in competition environment...\")\n        inference_server.serve()\n    else:\n        print(\"\\nðŸ§ª Running local gateway test...\")\n        inference_server.run_local_gateway((str(Path('/kaggle/input/hull-tactical-market-prediction/')),))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}