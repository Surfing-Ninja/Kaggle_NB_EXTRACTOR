{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# S&P 500 Returns Prediction — Comprehensive Analysis & Pipeline\n\n\n**Note:**\nThis notebook is a clean version, free from any bugs. It is based on the original notebook: [Hull Tactic: No Overfitting / LEAKAGE](https://www.kaggle.com/code/ahsuna123/hull-tactic-no-overfitting-leakage?scriptVersionId=269940868).\n\n\n**Reference:** Insights and explorations are derived from the EDA notebook [Hull Tactical: Complete EDA Deep Dive](https://www.kaggle.com/code/ahsuna123/hull-tactical-complete-eda-deep-dive).\n\n---\n\n## 1. Dataset Overview\n- **Rows:** 8,990  \n- **Features:** 98 (94 numeric, 4 special)  \n- **Target:** `forward_returns`  \n- **Feature Categories:**\n  - **Market Dynamics:** M1–M13  \n  - **Macro Economic:** E1–E20  \n  - **Interest Rate:** I1–I9  \n  - **Price Valuation:** P1–P13  \n  - **Volatility:** V1–V13  \n  - **Sentiment:** S1–S12  \n  - **Dummy Binary:** D1–D9  \n  - **Special:** 4  \n\n✅ Rich in macroeconomic and market features; some categories show **high internal correlations**.\n\n---\n\n## 2. Target Variable Insights\n- `forward_returns`:\n  - Not normally distributed (Jarque-Bera p-value = 0)  \n  - Stationary (ADF test p-value = 0)  \n  - Weak autocorrelation: lag 1 ≈ -0.045, lag 5 ≈ -0.024  \n  - Skewness ≈ -0.176, Kurtosis ≈ 2.19 → slightly platykurtic  \n  - Maximum drawdown ≈ -0.492  \n\n**Implications:**  \n- Use robust methods (tree-based, quantile regression).  \n- Temporal features can be leveraged due to stationarity.\n\n---\n\n## 3. Feature Correlation & Stability\n- Most features weakly correlated with target (<0.07).  \n- High multicollinearity within:\n  - Macro_Economic, Interest_Rate, Price_Valuation, Volatility, Sentiment.  \n- Feature stability over time:\n  - **Stable:** Dummy_Binary, M1  \n  - **Unstable:** Macro_Economic (E11, E12), Price_Valuation (P12), Volatility & Sentiment (V10–V12, S12)  \n\n**Implications:**  \n- Non-linear models preferred.  \n- Unstable features should be transformed (lag/rolling means, smoothing).\n\n---\n\n## 4. Outlier Analysis\n- Significant outliers in Macro_Economic and Price_Valuation features.  \n- Recommendation: Winsorization, clipping, or robust scaling.\n\n---\n\n## 5. Feature Engineering Strategy\n- **Lag Features:** M1–M10, D1–D5, V1–V5 → lag1, lag3  \n- **Rolling Features:** Volatility features → rolling mean/std (window=5)  \n- **Interaction Features:** Selected pairs between Macro_Economic and Market_Dynamics features  \n- **Outlier Handling:** Clipping based on category-specific bounds  \n- **Dimensionality Reduction:** PCA (n_components=50) to reduce high correlation noise  \n- **Feature Selection:** SelectKBest (mutual information) when PCA is not used\n\n---\n\n## 6. Modeling Approach\n- **Ensemble:** LGBM + XGBoost + Random Forest  \n- **Hyperparameters:** Reduced for fast inference (n_estimators ≤ 300, max_depth limited)  \n- **Weighting:** LGBM 0.4, XGB 0.35, RF 0.25 (renormalized if a model fails)  \n- **Scaling:** RobustScaler applied to features  \n- **Training/Inference Flow:**\n  - Fit scalers and PCA during training  \n  - Handle outliers for each batch  \n  - Create lag/rolling/interaction features consistently for training & prediction  \n  - Maintain recent feature history for single-row lag computations  \n\n**Advantages:**\n- Robust to outliers  \n- Handles weakly correlated features and non-linear effects  \n- Maintains consistent feature structure between training and inference\n\n---\n\n## 7. Pipeline Usage\n- **Training:** `pipeline.fit_from_file(train_path)`  \n- **Prediction:** `predict(pl.DataFrame)` for batch inference  \n- **Saving/Loading:** `pipeline.save_model('sp500_model.pkl')` / `pipeline.load_model('sp500_model.pkl')`  \n- **Server Deployment:** `kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)`  \n\n**Note:** Polars is used for batch input; features are converted to Pandas internally.\n\n---\n\n## 8. Key Recommendations\n1. Start modeling with **tree-based ensembles** (LGBM, XGBoost, RF)  \n2. Use **PCA or feature selection** to reduce dimensionality and multicollinearity  \n3. Include **lag and rolling features** for unstable categories  \n4. Apply **robust scaling/clipping** to mitigate extreme outliers  \n5. Implement **time-series aware validation** (avoid random splits)  \n6. Monitor **feature stability** for future iterations and potential new engineered features\n\n---\n\n**Summary:**  \nThis pipeline integrates insights from EDA, feature stability analysis, and target characteristics into an end-to-end, deployable model for predicting S&P 500 forward returns.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nfrom typing import Dict, Tuple, List\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport kaggle_evaluation.default_inference_server\n\n\nclass LeakSafeSP500Pipeline:\n    \"\"\"\n    Leakage-free, regularized pipeline for time-series S&P500 forward_returns.\n    \"\"\"\n\n    def __init__(\n        self,\n        selected_lag_prefixes: Dict[str, Tuple[int]] = None,\n        rolling_window: int = 5,\n        use_pca: bool = True,\n        pca_components: int = 50,\n        var_threshold: float = 1e-5,\n        random_state: int = 42,\n        n_history: int = 10,\n    ):\n        self.random_state = random_state\n        self.rolling_window = rolling_window\n        self.use_pca = use_pca\n        self.pca_components = pca_components\n        self.var_threshold = var_threshold\n        self.n_history = n_history\n\n        if selected_lag_prefixes is None:\n            self.selected_lag_prefixes = {\n                'M': (1, 3),\n                'D': (1, 3),\n                'V': (1, 3),\n            }\n        else:\n            self.selected_lag_prefixes = selected_lag_prefixes\n\n        self.models = {\n            'lgb': lgb.LGBMRegressor(\n                objective='regression', metric='rmse', num_leaves=15,\n                learning_rate=0.01, n_estimators=200, min_child_samples=50,\n                subsample=0.7, colsample_bytree=0.7, reg_alpha=1.0, reg_lambda=1.0,\n                random_state=self.random_state,\n            ),\n            'xgb': xgb.XGBRegressor(\n                objective='reg:squarederror', n_estimators=200, max_depth=3,\n                learning_rate=0.01, min_child_weight=10, subsample=0.7,\n                colsample_bytree=0.7, reg_alpha=1.0, reg_lambda=1.0,\n                random_state=self.random_state, verbosity=0,\n            ),\n            'rf': RandomForestRegressor(\n                n_estimators=200, max_depth=6, min_samples_leaf=50,\n                max_features='sqrt', n_jobs=-1, random_state=self.random_state,\n            )\n        }\n\n        self.model_weights = {'lgb': 0.4, 'xgb': 0.35, 'rf': 0.25}\n\n        self.clip_bounds = None\n        self.medians = None\n        self.scaler = None\n        self.pca = None\n        self.var_selector = None\n        self.feature_names = None\n\n        self.is_fitted = False\n        self.history_buffer = None\n\n    # ---------------------------\n    # Feature engineering helpers\n    # ---------------------------\n    def _sorted_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if 'date_id' in df.columns:\n            return df.sort_values('date_id').reset_index(drop=True)\n        else:\n            return df.reset_index(drop=True)\n\n    def _select_lag_columns(self, df_cols: List[str]) -> Dict[str, List[str]]:\n        selected = {}\n        for prefix in self.selected_lag_prefixes.keys():\n            matched = [c for c in df_cols if c.startswith(prefix)]\n            if prefix == 'M':\n                matched = sorted(matched)[:10]\n            elif prefix == 'D':\n                matched = sorted(matched)[:5]\n            elif prefix == 'V':\n                matched = sorted(matched)[:5]\n            selected[prefix] = matched\n        return selected\n\n    def _create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Create lag and rolling features and drop raw columns to prevent mismatch.\n        \"\"\"\n        df = df.copy()\n        df = self._sorted_df(df)\n\n        # identify columns to lag (excluding target)\n        raw_cols = [c for c in df.columns if not c.startswith(('date_id', 'forward_returns', 'is_scored'))]\n        matched = self._select_lag_columns(raw_cols)\n\n        # Lags\n        for prefix, cols in matched.items():\n            lags = self.selected_lag_prefixes.get(prefix, ())\n            for col in cols:\n                for lag in lags:\n                    df[f'lagged_{col}_lag{lag}'] = df[col].shift(lag)\n            # drop original columns after lagging\n            df.drop(cols, axis=1, inplace=True)\n\n        # Rolling features for volatility\n        vol_cols = matched.get('V', [])\n        for col in vol_cols:\n            df[f'lagged_{col}_roll_mean_{self.rolling_window}'] = (\n                df[f'lagged_{col}_lag1'].rolling(self.rolling_window, min_periods=1).mean()\n            )\n            df[f'lagged_{col}_roll_std_{self.rolling_window}'] = (\n                df[f'lagged_{col}_lag1'].rolling(self.rolling_window, min_periods=1).std().fillna(0.0)\n            )\n\n        # Interaction features (using lagged only)\n        macro_cols = [c for c in df.columns if c.startswith('E')]\n        market_cols = [c for c in df.columns if c.startswith('lagged_M')]\n        for e in macro_cols[:3]:\n            for m in market_cols[:3]:\n                df[f'{e}_x_{m}'] = df[e].shift(1) * df[m].shift(1)\n\n        return df\n\n    # ---------------------------\n    # Preprocessing (train-only)\n    # ---------------------------\n    def _fit_preprocessors(self, train_df: pd.DataFrame) -> Dict:\n        X = train_df.copy()\n        feature_cols = [c for c in X.columns if not c.startswith(('date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'is_scored'))]\n        print(feature_cols)\n        X = X[feature_cols].copy()\n\n        clip_bounds = {col: X[col].quantile([0.05, 0.95]).values for col in X.columns}\n        for col in X.columns:\n            q05, q95 = clip_bounds[col]\n            X[col] = X[col].clip(q05, q95)\n\n        medians = X.median()\n        X = X.fillna(medians).fillna(0)\n\n        var_selector = VarianceThreshold(threshold=self.var_threshold)\n        X_var = var_selector.fit_transform(X)\n\n        scaler = RobustScaler()\n        X_scaled = scaler.fit_transform(X_var)\n\n        pca = None\n        if self.use_pca:\n            n_comp = min(self.pca_components, X_scaled.shape[1])\n            pca = PCA(n_components=n_comp, random_state=self.random_state)\n            X_reduced = pca.fit_transform(X_scaled)\n            final_feature_names = [f'pca_{i}' for i in range(X_reduced.shape[1])]\n        else:\n            X_reduced = X_scaled\n            sel_mask = var_selector.get_support()\n            final_feature_names = [c for c, m in zip(X.columns, sel_mask) if m]\n\n        preprocessors = {\n            'clip_bounds': clip_bounds,\n            'medians': medians,\n            'var_selector': var_selector,\n            'scaler': scaler,\n            'pca': pca,\n            'feature_names': final_feature_names,\n        }\n        return preprocessors\n\n    def _transform_with_preprocessors(self, df: pd.DataFrame, preprocessors: Dict) -> pd.DataFrame:\n        df = df.copy()\n        feature_cols = ['D6', 'D7', 'D8', 'D9', 'E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'lagged_M1_lag1', 'lagged_M1_lag3', 'lagged_M10_lag1', 'lagged_M10_lag3', 'lagged_M11_lag1', 'lagged_M11_lag3', 'lagged_M12_lag1', 'lagged_M12_lag3', 'lagged_M13_lag1', 'lagged_M13_lag3', 'lagged_M14_lag1', 'lagged_M14_lag3', 'lagged_M15_lag1', 'lagged_M15_lag3', 'lagged_M16_lag1', 'lagged_M16_lag3', 'lagged_M17_lag1', 'lagged_M17_lag3', 'lagged_M18_lag1', 'lagged_M18_lag3', 'lagged_D1_lag1', 'lagged_D1_lag3', 'lagged_D2_lag1', 'lagged_D2_lag3', 'lagged_D3_lag1', 'lagged_D3_lag3', 'lagged_D4_lag1', 'lagged_D4_lag3', 'lagged_D5_lag1', 'lagged_D5_lag3', 'lagged_V1_lag1', 'lagged_V1_lag3', 'lagged_V10_lag1', 'lagged_V10_lag3', 'lagged_V11_lag1', 'lagged_V11_lag3', 'lagged_V12_lag1', 'lagged_V12_lag3', 'lagged_V13_lag1', 'lagged_V13_lag3', 'lagged_V1_roll_mean_5', 'lagged_V1_roll_std_5', 'lagged_V10_roll_mean_5', 'lagged_V10_roll_std_5', 'lagged_V11_roll_mean_5', 'lagged_V11_roll_std_5', 'lagged_V12_roll_mean_5', 'lagged_V12_roll_std_5', 'lagged_V13_roll_mean_5', 'lagged_V13_roll_std_5', 'E1_x_lagged_M1_lag1', 'E1_x_lagged_M1_lag3', 'E1_x_lagged_M10_lag1', 'E10_x_lagged_M1_lag1', 'E10_x_lagged_M1_lag3', 'E10_x_lagged_M10_lag1', 'E11_x_lagged_M1_lag1', 'E11_x_lagged_M1_lag3', 'E11_x_lagged_M10_lag1']\n        \n        #print(feature_cols)\n        X = df[feature_cols].copy()\n\n        clip_bounds = preprocessors['clip_bounds']\n        for col in X.columns:\n            if col in clip_bounds:\n                q05, q95 = clip_bounds[col]\n                X[col] = X[col].clip(q05, q95)\n\n        medians = preprocessors['medians']\n        X = X.fillna(medians).fillna(0)\n\n        var_selector = preprocessors['var_selector']\n        X_var = var_selector.transform(X)\n\n        scaler = preprocessors['scaler']\n        X_scaled = scaler.transform(X_var)\n\n        pca = preprocessors.get('pca', None)\n        if pca is not None:\n            X_final = pca.transform(X_scaled)\n            col_names = preprocessors['feature_names']\n            X_df = pd.DataFrame(X_final, columns=col_names, index=df.index)\n        else:\n            col_mask = var_selector.get_support()\n            selected_cols = [c for c, m in zip(feature_cols, col_mask) if m]\n            X_df = pd.DataFrame(X_scaled, columns=selected_cols, index=df.index)\n\n        return X_df\n\n    # ---------------------------\n    # Fit / predict\n    # ---------------------------\n    def fit_from_file(self, train_path: str, target_col: str = 'forward_returns'):\n        df_raw = pd.read_csv(train_path)\n        df_raw = self._sorted_df(df_raw)\n\n        df_feats_full = self._create_time_features(df_raw)\n        final_preprocessors = self._fit_preprocessors(df_feats_full)\n\n        self.clip_bounds = final_preprocessors['clip_bounds']\n        self.medians = final_preprocessors['medians']\n        self.var_selector = final_preprocessors['var_selector']\n        self.scaler = final_preprocessors['scaler']\n        self.pca = final_preprocessors['pca']\n        self.feature_names = final_preprocessors['feature_names']\n\n        X_full = self._transform_with_preprocessors(df_feats_full, final_preprocessors)\n        y_full = df_feats_full[target_col].fillna(0).reset_index(drop=True)\n\n        for name, model in self.models.items():\n            model.fit(X_full, y_full)\n\n        if 'date_id' in df_raw.columns:\n            self.history_buffer = df_raw.sort_values('date_id').tail(self.n_history).reset_index(drop=True)\n        else:\n            self.history_buffer = df_raw.tail(self.n_history).reset_index(drop=True)\n\n        self.is_fitted = True\n        print(\"Training completed.\")\n        return self\n\n    def predict_batch(self, df_batch: pd.DataFrame) -> np.ndarray:\n        if not self.is_fitted:\n            raise ValueError(\"Pipeline not fitted. Call fit_from_file() first.\")\n        #print(\"df_batch\",df_batch)\n        df_input = df_batch.copy().reset_index(drop=True)\n        if len(df_input) < self.n_history and self.history_buffer is not None:\n            hist = self.history_buffer.copy()\n            #print(\"hist\",hist)\n            cols_inter = [c for c in hist.columns if c in df_input.columns or c in ['date_id']]\n            hist = hist[cols_inter]\n            df_input = pd.concat([hist, df_input], ignore_index=True)\n        #print(\"df_input\",df_input)\n        df_feats = self._create_time_features(df_input)\n        #print(\"df_feats\",df_feats)\n        preprocessors = {\n            'clip_bounds': self.clip_bounds,\n            'medians': self.medians,\n            'var_selector': self.var_selector,\n            'scaler': self.scaler,\n            'pca': self.pca,\n            'feature_names': self.feature_names,\n        }\n        X_all = self._transform_with_preprocessors(df_feats, preprocessors)\n        X_preds = X_all.iloc[-len(df_batch):].reset_index(drop=True)\n        #print(\"X_preds\",X_preds)\n        preds = np.zeros(len(X_preds))\n        for name, model in self.models.items():\n            w = self.model_weights.get(name, 0.0)\n            if w <= 0:\n                continue\n            preds += w * model.predict(X_preds)\n\n        return preds if len(preds) > 1 else preds.flatten()[0]\n\n    def save_model(self, filepath: str = 'leaksafe_sp500_pipeline.pkl'):\n        data = {\n            'models': self.models,\n            'model_weights': self.model_weights,\n            'clip_bounds': self.clip_bounds,\n            'medians': self.medians,\n            'var_selector': self.var_selector,\n            'scaler': self.scaler,\n            'pca': self.pca,\n            'feature_names': self.feature_names,\n            'is_fitted': self.is_fitted,\n            'history_buffer': self.history_buffer,\n        }\n        with open(filepath, 'wb') as f:\n            pickle.dump(data, f)\n\n    def load_model(self, filepath: str = 'leaksafe_sp500_pipeline.pkl'):\n        with open(filepath, 'rb') as f:\n            data = pickle.load(f)\n        self.models = data['models']\n        self.model_weights = data['model_weights']\n        self.clip_bounds = data['clip_bounds']\n        self.medians = data['medians']\n        self.var_selector = data['var_selector']\n        self.scaler = data['scaler']\n        self.pca = data['pca']\n        self.feature_names = data['feature_names']\n        self.is_fitted = data['is_fitted']\n        self.history_buffer = data.get('history_buffer', None)\n        return self   # ✅ critical fix\n\n\n# ---------------------------\n# Kaggle inference\n# ---------------------------\ndef create_prediction_function():\n    pipeline = LeakSafeSP500Pipeline()\n    train_path = '/kaggle/input/hull-tactical-market-prediction/train.csv'\n    if os.path.exists(train_path):\n        pipeline = pipeline.fit_from_file(train_path)\n        pipeline.save_model()\n        Model = pipeline.load_model()\n        \n    return Model\nModel = create_prediction_function()\ndef predict(test):\n        pipeline = LeakSafeSP500Pipeline()\n        Model = pipeline.load_model()\n        test_df = test.to_pandas() if hasattr(test, 'to_pandas') else test\n        preds = Model.predict_batch(test_df)\n        val = float(preds[0]) if hasattr(preds, '__iter__') else float(preds)\n        print(val)\n        signal = np.clip(val * 50.0 + 1.0, 0.0, 2.0)\n        return float(signal)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:36:55.431335Z","iopub.execute_input":"2025-10-23T12:36:55.432166Z","iopub.status.idle":"2025-10-23T12:37:11.43949Z","shell.execute_reply.started":"2025-10-23T12:36:55.432131Z","shell.execute_reply":"2025-10-23T12:37:11.438188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = LeakSafeSP500Pipeline()\nModel = pipeline.load_model(\"/kaggle/working/leaksafe_sp500_pipeline.pkl\")\nprint(\"Loaded model:\", Model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:37:11.441201Z","iopub.execute_input":"2025-10-23T12:37:11.441523Z","iopub.status.idle":"2025-10-23T12:37:11.471782Z","shell.execute_reply.started":"2025-10-23T12:37:11.441499Z","shell.execute_reply":"2025-10-23T12:37:11.470543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif __name__ == \"__main__\":\n    if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        inference_server.serve()\n    else:\n        inference_server.run_local_gateway((\"/kaggle/input/hull-tactical-market-prediction/\",))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:37:11.47252Z","iopub.execute_input":"2025-10-23T12:37:11.472832Z","iopub.status.idle":"2025-10-23T12:37:14.675962Z","shell.execute_reply.started":"2025-10-23T12:37:11.472801Z","shell.execute_reply":"2025-10-23T12:37:14.675035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}