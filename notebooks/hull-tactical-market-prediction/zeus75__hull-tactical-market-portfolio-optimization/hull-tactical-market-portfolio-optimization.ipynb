{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n**Hull Tactical Market Prediction Competition\nPortfolio Optimization Solution with Multiple Strategies**\n","metadata":{}},{"cell_type":"markdown","source":"The Hull Tactical Market Prediction competition challenges participants to predict S&P 500 excess returns while managing volatility constraints. The goal is to develop a trading strategy that outperforms buy-and-hold while maintaining volatility within 120% of the market's volatility.","metadata":{}},{"cell_type":"markdown","source":"## What is Portfolio Optimization?\n\n**Portfolio Optimization** is the process of mathematically determining the optimal **position** (or allocation) in the risky asset (the market, S&P 500 excess returns) versus the risk-free asset (cash). The goal is to maximize a specific utility function or financial metric, such as the **Adjusted Sharpe Ratio**, subject to predefined constraints on the allocation size and risk.\n\nThe notebook defines the position as the allocation to the risky asset, where:\n* 0.0 MIN POSITION represents 0% in the market (i.e., 100% in the risk-free asset).\n* 1.0 represents 100% in the market (market neutral).\n* 2.0 MAX POSITION represents 200% in the market (a leveraged, aggressive position).\n\nThe overall portfolio return is calculated based on this position:\nStrategy Return = (Risk-Free Rate * (1 - Position)) + (Position * Market Return)\n\nThe optimization itself is performed using numerical methods like **L-BFGS-B** (for Mean-Variance) and **SLSQP** (for constrained and Sharpe maximization problems) via the `scipy.optimize.minimize` function.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Role in the Hull Tactical Market Prediction Competition\n\nThe central role of portfolio optimization in this competition is to achieve a trading strategy that **outperforms a simple buy-and-hold strategy** while adhering to a critical risk constraint:\n\n### 1. Primary Objective: Maximizing the Adjusted Sharpe Ratio\nThe ultimate goal of the competition is to develop a strategy that yields the highest **Adjusted Sharpe Ratio**. The strategies (Mean-Variance, Minimum Variance, Sharpe Maximization, Risk Parity, CAPM Alpha Signal) all aim to find a series of daily positions that maximize this specific metric. The strategy that produces the highest Adjusted Sharpe Ratio on the historical data is declared the **Winner**.\n\n### 2. Managing Volatility and Returns (The Adjusted Sharpe Penalties)\nThe Adjusted Sharpe Ratio is the competition's core evaluation metric because it heavily **penalizes** strategies that violate two key constraints:\n\n* **Excess Volatility Penalty**: This penalty is applied if the strategy's volatility is greater than **120% of the market's volatility**. \n* **Return Penalty**: This penalty is applied if the strategy's geometric mean excess return **underperforms the market's return**. \nBy optimizing directly for this metric (as in the **Sharpe Maximization** strategy), the portfolio optimization implicitly manages these constraints to secure the highest possible rank.\n\nThe optimization process uses a **lookback window of 180 days** of historical data for all calculations, simulating the rolling window approach typically used in live trading. The final winning strategy is chosen as the one that yields the highest **Adjusted Sharpe Ratio** during this historical evaluation.","metadata":{}},{"cell_type":"markdown","source":"**Multiple Strategy Implementation**\n\nInstead of a single optimization approach, we implement 6 distinct portfolio optimization strategies:\n\n* Mean-Variance Optimization (Markowitz)\n* Minimum Variance Portfolio\n* Sharpe Ratio Maximization\n* Risk Parity (Inverse Volatility) Allocation\n* CAPM Alpha-based Signal Strategy\n* Sortino Ratio Maximization","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport hashlib\nfrom scipy.optimize import minimize, Bounds\nfrom scipy.stats import skew, kurtosis\nimport warnings\nimport os\nimport kaggle_evaluation.default_inference_server \nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:06:06.304204Z","iopub.execute_input":"2025-10-24T05:06:06.304592Z","iopub.status.idle":"2025-10-24T05:06:10.546345Z","shell.execute_reply.started":"2025-10-24T05:06:06.304564Z","shell.execute_reply":"2025-10-24T05:06:10.545027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CONSTANTS AND CONFIGURATION\n# ============================================================================\n\n# Position represents the allocation to the risky asset (the market)\nMIN_POSITION = 0.0  # Minimum investment (0% in market, 100% in risk-free) - Defensive/Conservative\nMAX_POSITION = 2.0  # Maximum investment (200% in market - leveraged) - Aggressive/Leveraged\nTRADING_DAYS_PER_YEAR = 252 # Standard assumption for annualized calculations\nLOOKBACK_WINDOW = 180  # Days of historical data to use for portfolio optimization (rolling window)\nEPS = 1e-10  # Small epsilon to avoid division by zero\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `calculate_adjusted_sharpe` function, is the core evaluation metric for the competition. It takes a series of portfolio positions and the corresponding historical market data, calculates the strategy's performance, and then adjusts the standard Sharpe Ratio by applying penalties for high volatility and underperforming the market.","metadata":{}},{"cell_type":"code","source":"# ===================================\n# LOADING DATA\n# ===================================\n\n# Load training data\nprint(\"Loading training data...\")\ntrain_ = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\ntest_ = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:08:54.834456Z","iopub.execute_input":"2025-10-24T05:08:54.834796Z","iopub.status.idle":"2025-10-24T05:08:55.036455Z","shell.execute_reply.started":"2025-10-24T05:08:54.834769Z","shell.execute_reply":"2025-10-24T05:08:55.035422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ðŸ’¡ðŸ’¡ðŸ’¡Dimistifying best score in leaderboard. ####\n\nIf you check your data, test set is included in the train set. So the best score in the public leaderboard (17.396) is coming from this effect. Version 2/4 of the notebook shows equal result between my private leaderboard and public leaderboard. Version 5 shows difference between them.\nCredits to Luca Massaron\n","metadata":{}},{"cell_type":"code","source":"# ===========================================\n# CHECK DATA\n# ===========================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use only common columns for comparison\ncommon_cols = list(set(train_.columns) & set(test_.columns))\ntrain_subset = train_[common_cols].copy()\ntest_subset = test_[common_cols].copy()\n\n# Create hash for each row for efficient comparison\ntrain_subset['_row_hash'] = pd.util.hash_pandas_object(train_subset, index=False)\ntest_subset['_row_hash'] = pd.util.hash_pandas_object(test_subset, index=False)\n    \n# Find overlapping rows\noverlapping_hashes = set(test_subset['_row_hash']) & set(train_subset['_row_hash'])\ntest_only_hashes = set(test_subset['_row_hash']) - set(train_subset['_row_hash'])\nresults = {}    \nresults['exact_row_match'] = {\n        'overlap_count': len(overlapping_hashes),\n        'test_only_count': len(test_only_hashes),\n        'overlap_percentage': (len(overlapping_hashes) / len(test_subset) * 100) if len(test_subset) > 0 else 0,\n        'is_test_subset': len(test_only_hashes) == 0\n}\n\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:09:47.746501Z","iopub.execute_input":"2025-10-24T05:09:47.747221Z","iopub.status.idle":"2025-10-24T05:09:47.78542Z","shell.execute_reply.started":"2025-10-24T05:09:47.747191Z","shell.execute_reply":"2025-10-24T05:09:47.784518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train set distilled from test set\ntrain=train_.iloc[:-10]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION METRIC (ADJUSTED SHARPE RATIO)\n# ============================================================================\n\ndef calculate_adjusted_sharpe(solution_df, predictions):\n    \"\"\"\n    Calculate the competition's adjusted Sharpe ratio.\n    \n    This metric is crucial as the strategies optimize directly for this score.\n    It penalizes:\n    1. Strategies with volatility > 120% of market volatility (Excess Volatility Penalty)\n    2. Strategies that underperform the market return (Return Penalty)\n    \n    Parameters:\n    -----------\n    solution_df : pd.DataFrame\n        Contains 'forward_returns' (market return) and 'risk_free_rate' (benchmark/cash return)\n    predictions : array-like\n        Position sizes (0 to 2) - the output of the optimization strategies\n    \n    Returns:\n    --------\n    float : Adjusted Sharpe ratio\n    \"\"\"\n    solution = solution_df.copy()\n    solution['position'] = predictions\n    \n    # Calculate the portfolio's realized daily returns\n    # Strategy Return = (Risk-Free Rate * Uninvested Portion) + (Position * Market Return)\n    solution['strategy_returns'] = (\n        solution['risk_free_rate'] * (1 - solution['position']) +\n        solution['position'] * solution['forward_returns']\n    )\n    \n    # Strategy performance metrics\n    strategy_excess = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_cum_excess = (1 + strategy_excess).prod()\n    \n    # Severe penalty for cumulative loss\n    if strategy_cum_excess <= 0:\n        return -1000.0\n    \n    # Calculate annualized geometric mean excess return\n    strategy_mean_excess = strategy_cum_excess ** (1 / len(solution)) - 1\n    strategy_std = solution['strategy_returns'].std()\n    \n    if strategy_std == 0:\n        return 0.0\n    \n    # Standard Sharpe ratio calculation (annualized)\n    sharpe = strategy_mean_excess / strategy_std * np.sqrt(TRADING_DAYS_PER_YEAR)\n    strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YEAR) * 100 # Annualized volatility in %\n    \n    # Market performance metrics for benchmarking\n    market_excess = solution['forward_returns'] - solution['risk_free_rate']\n    market_cum_excess = (1 + market_excess).prod()\n    \n    if market_cum_excess <= 0:\n        market_mean_excess = -1.0\n    else:\n        market_mean_excess = market_cum_excess ** (1 / len(solution)) - 1\n    \n    market_std = solution['forward_returns'].std()\n    market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YEAR) * 100 # Annualized market volatility in %\n    \n    # Apply penalties\n    # Volatility penalty: Kicks in when strategy vol > 120% of market vol\n    excess_vol_penalty = 1.0\n    if market_volatility > 0:\n        vol_ratio = strategy_volatility / market_volatility\n        if vol_ratio > 1.2:\n            # Multiplicative penalty based on how much the ratio exceeds 1.2\n            excess_vol_penalty = 1 + (vol_ratio - 1.2)\n    \n    # Return penalty: Penalizes underperformance vs market\n    return_gap = max(0, (market_mean_excess - strategy_mean_excess) * 100 * TRADING_DAYS_PER_YEAR) # Annualized return difference\n    # Quadratic penalty: gap^2 / 100\n    return_penalty = 1 + (return_gap ** 2) / 100\n    \n    # Final adjusted Sharpe: standard Sharpe / (Volatility Penalty * Return Penalty)\n    adjusted_sharpe = sharpe / (excess_vol_penalty * return_penalty)\n    \n    return float(min(adjusted_sharpe, 1_000_000)) # Clip maximum value\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SORTINO RATIO\n# ============================================================================\n\ndef calculate_sortino_ratio(solution_df, predictions):\n    \"\"\"\n    Calculate the Sortino Ratio.\n    MAR (Minimum Acceptable Return) is set to the risk-free rate.\n    \"\"\"\n    solution = solution_df.copy()\n    solution['position'] = predictions\n    \n    # Calculate the portfolio's realized daily returns\n    solution['strategy_returns'] = (\n        solution['risk_free_rate'] * (1 - solution['position']) +\n        solution['position'] * solution['forward_returns']\n    )\n    \n    # Excess return over MAR (Risk-Free Rate)\n    strategy_excess = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_cum_excess = (1 + strategy_excess).prod()\n\n    if strategy_cum_excess <= 0:\n        return -1000.0\n    \n    # Annualized geometric mean excess return (Numerator)\n    strategy_mean_excess = strategy_cum_excess ** (1 / len(solution)) - 1\n    \n    # Downside Deviation (Denominator)\n    # Only consider returns below the risk-free rate (i.e., excess returns < 0)\n    downside_returns = strategy_excess[strategy_excess < 0]\n    \n    # If there are no downside returns, downside deviation is 0.\n    if len(downside_returns) == 0:\n        # Return a large number to favor this position set\n        return 1000.0\n\n    # Downside deviation (annualized)\n    downside_std = np.std(downside_returns)\n    downside_deviation = downside_std * np.sqrt(TRADING_DAYS_PER_YEAR)\n    \n    if downside_deviation == 0:\n        return 1000.0\n    \n    # Sortino Ratio calculation (annualized)\n    # The numerator (strategy_mean_excess) is already annualized geometrically\n    sortino = strategy_mean_excess / (downside_deviation / np.sqrt(TRADING_DAYS_PER_YEAR)) # Annualized Excess Return / Downside Volatility\n    \n    return float(min(sortino, 1_000_000))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `create_features`, which performs feature engineering by generating time-series characteristics from the raw return data. These are typical technical analysis indicators that could be used as inputs for a more complex machine learning model.\n\nThe features are calculated using a rolling window approach to capture short-term, localized market dynamics.","metadata":{}},{"cell_type":"markdown","source":"**Feature Engineering**\n\nAdded market dynamics features:\n\n* Rolling statistics (mean, standard deviation, skewness, kurtosis)\n* Momentum indicators (5-day, 20-day)\n* Volatility regime identification","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# FEATURE ENGINEERING\n# ============================================================================\n\ndef create_features(data):\n    \"\"\"\n    Create additional features for portfolio optimization.\n    \n    These features capture time-series characteristics which could be used \n    by a more complex ML model to predict optimal positions, though they \n    are not explicitly used by the current simple optimization functions.\n    They represent typical technical analysis indicators.\n    \"\"\"\n    df = data.copy()\n    \n    # Basic returns if not present\n    if 'returns' not in df.columns and 'forward_returns' in df.columns:\n        df['returns'] = df['forward_returns']\n    \n    # Rolling window statistics (20-day = ~1 month)\n    window = min(20, len(df) // 4)  # Adaptive window size to ensure sufficient data\n    \n    if len(df) >= window:\n        # Volatility and central tendency\n        df['rolling_mean'] = df['returns'].rolling(window).mean()\n        df['rolling_std'] = df['returns'].rolling(window).std()\n        # Higher-order moments (risk-related)\n        df['rolling_skew'] = df['returns'].rolling(window).skew()\n        df['rolling_kurt'] = df['returns'].rolling(window).apply(lambda x: kurtosis(x))\n        \n        # Momentum indicators (short and medium term)\n        df['momentum_5'] = df['returns'].rolling(5).mean()\n        df['momentum_20'] = df['returns'].rolling(20).mean() if len(df) >= 20 else np.nan\n        \n        # Volatility regime indicator\n        df['vol_regime'] = df['rolling_std'] / df['rolling_std'].mean()\n    \n    # Fill NaN values (common in rolling calculations at the start of the series) with 0\n    df = df.fillna(0)\n    \n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# PORTFOLIO OPTIMIZATION STRATEGIES\n# ============================================================================\n# Each strategy uses scipy.optimize.minimize to find the optimal 'positions' array\n# for the given lookback window, based on a specific financial objective function.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Mean-Variance Optimization (Markowitz Portfolio Theory)**\n\nObjective Function:\n\nMaximize: E[R] - Î» * Var(R)\n\nWhere:\n\nE[R] = Expected return\nVar(R) = Variance of returns\nÎ» = Risk aversion parameter (set to 2.0)\n\n***Characteristics:***\n\n* Balances return and risk explicitly\n* Risk aversion parameter can be tuned\n* Classic approach with strong theoretical foundation\n* Works well in stable market conditions\n\n***When it works best:***\n\n* Markets with clear risk-return tradeoffs\n* When historical patterns are predictive\n* Moderate volatility environments","metadata":{}},{"cell_type":"code","source":"# Strategy 1: Mean-Variance Optimization (Markowitz)\ndef mean_variance_optimization(data, target_return=None):\n    \"\"\"\n    Classic Markowitz mean-variance optimization (MVO).\n    \n    The objective function maximizes a 'utility' function:\n    Utility = Expected Return - (Risk Aversion * Variance)\n    \"\"\"\n    n_days = len(data)\n    \n    def objective(positions):\n        # The positions array is the variable being optimized\n        pos = np.clip(positions, MIN_POSITION, MAX_POSITION)\n        \n        # Calculate portfolio returns based on positions\n        portfolio_returns = (\n            data['risk_free_rate'].values * (1 - pos) +\n            pos * data['forward_returns'].values\n        )\n        \n        # Mean and variance of the portfolio returns\n        mean_return = np.mean(portfolio_returns)\n        variance = np.var(portfolio_returns)\n        \n        # Risk aversion parameter (lambda). A higher value means the optimizer \n        # heavily penalizes variance. Set to 2.0 (arbitrary choice).\n        risk_aversion = 2.0\n        \n        # Maximize: mean_return - risk_aversion * variance. \n        # Since 'minimize' is used, we negate the objective function.\n        return -(mean_return - risk_aversion * variance) * TRADING_DAYS_PER_YEAR\n    \n    # Initial guess: equal weight (100% market allocation)\n    x0 = np.full(n_days, 1.0)\n    \n    # Bounds: ensures positions stay within [MIN_POSITION, MAX_POSITION]\n    bounds = Bounds(MIN_POSITION, MAX_POSITION)\n    \n    # Optimization using L-BFGS-B (a quasi-Newton method suitable for bounded problems)\n    result = minimize(\n        objective,\n        x0,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 500, 'ftol': 1e-8}\n    )\n    \n    return np.clip(result.x, MIN_POSITION, MAX_POSITION)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Minimum Variance Portfolio**\n\nObjective Function:\n\nMinimize: Var(R)\nSubject to: E[R] >= minimum_return\n\n***Characteristics:***\n\n* Defensive strategy focusing on risk minimization\n* Includes return constraint to avoid trivial solutions\n* Tends to produce lower but more stable returns\n* Good for risk-averse investors\n\n***When it works best:***\n\n* High volatility periods\n* Market downturns\n* When capital preservation is priority","metadata":{}},{"cell_type":"code","source":"# Strategy 2: Minimum Variance Portfolio\ndef minimum_variance_optimization(data):\n    \"\"\"\n    Minimize portfolio variance subject to a minimum return constraint.\n    \n    This strategy is defensive, focusing solely on risk reduction while \n    ensuring a basic level of performance above the risk-free rate.\n    \"\"\"\n    n_days = len(data)\n    \n    def objective(positions):\n        pos = np.clip(positions, MIN_POSITION, MAX_POSITION)\n        \n        portfolio_returns = (\n            data['risk_free_rate'].values * (1 - pos) +\n            pos * data['forward_returns'].values\n        )\n        \n        # Objective: Minimize variance\n        return np.var(portfolio_returns)\n    \n    # Start with a low-risk position guess (30% market allocation)\n    x0 = np.full(n_days, 0.3)\n    \n    bounds = Bounds(MIN_POSITION, MAX_POSITION)\n    \n    # Define the minimum return constraint\n    # Target: At least 50% above the historical mean risk-free rate\n    min_return = data['risk_free_rate'].mean() * 1.5\n    \n    def return_constraint(positions):\n        pos = np.clip(positions, MIN_POSITION, MAX_POSITION)\n        portfolio_returns = (\n            data['risk_free_rate'].values * (1 - pos) +\n            pos * data['forward_returns'].values\n        )\n        # Constraint: Mean Portfolio Return - Min Target Return >= 0\n        return np.mean(portfolio_returns) - min_return\n    \n    # Constraints list for SLSQP\n    constraints = [{'type': 'ineq', 'fun': return_constraint}] # 'ineq' means inequality constraint (fun(x) >= 0)\n    \n    # Optimization using SLSQP (Sequential Least Squares Programming, good for constraints)\n    result = minimize(\n        objective,\n        x0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=constraints,\n        options={'maxiter': 500, 'ftol': 1e-8}\n    )\n    \n    return np.clip(result.x, MIN_POSITION, MAX_POSITION)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sharpe Ratio Maximization**\n\nObjective Function:\n\nMaximize: (E[R] - Rf) / Ïƒ(R)\n\nWith penalties for:\n- Volatility > 120% of market\n- Underperformance vs market\n\n***Characteristics:***\n\n* Directly optimizes the competition metric\n* Accounts for both returns and risk\n* Includes competition-specific penalties\n* Most aligned with evaluation criteria\n\n***When it works best:***\n\n* When you need balanced risk-adjusted returns\n* Competition scenarios with specific metrics\n* Markets where relative performance matters","metadata":{}},{"cell_type":"code","source":"# Strategy 3: Sharpe Ratio Maximization\ndef sharpe_ratio_optimization(data):\n    \"\"\"\n    Directly maximize the Adjusted Sharpe Ratio metric defined by the competition.\n    \n    This is the most direct and aggressive strategy.\n    It uses multiple random starting points (x0) to increase the chance of \n    finding the global maximum for the non-linear objective function.\n    \"\"\"\n    n_days = len(data)\n    \n    def objective(positions):\n        return -calculate_adjusted_sharpe(data, positions)\n    \n    best_result = None\n    best_score = float('-inf')\n    \n    # Test multiple initial guesses to avoid getting stuck in local minima/maxima\n    initial_guesses = [\n        np.full(n_days, 0.5),  # Moderate\n        np.full(n_days, 1.0),  # Market neutral\n        np.random.uniform(0.3, 0.7, n_days),  # Random moderate\n        np.linspace(0.2, 0.8, n_days),  # Gradual increase\n    ]\n    \n    bounds = Bounds(MIN_POSITION, MAX_POSITION)\n    \n    for x0 in initial_guesses:\n        try:\n            # Optimization using SLSQP (suitable for non-smooth objectives like Sharpe)\n            result = minimize(\n                objective,\n                x0,\n                method='SLSQP',\n                bounds=bounds,\n                options={'maxiter': 1000, 'ftol': 1e-8}\n            )\n            \n            # Check if this run is successful and better than the current best\n            if result.success and -result.fun > best_score:\n                best_score = -result.fun\n                best_result = result\n        except:\n            continue\n    \n    if best_result is not None:\n        return np.clip(best_result.x, MIN_POSITION, MAX_POSITION)\n    else:\n        # Fallback to a conservative position if all optimizations fail\n        return np.full(n_days, 0.5)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n**Risk Parity (Inverse Volatility) Allocation**\n\nObjective Function:\n\nMinimize the variance contribution from each component to achieve equal risk contribution across all assets.\n\nâˆ‘(Ïƒiâˆ’Ïƒp/n)^2\n\nwhere Ïƒi is the risk contribution of asset, Ïƒp is the total portfolio risk, and n is the number of assets.\n\n\n***Characteristics:***\n* Focuses purely on risk rather than maximizing return or Sharpe Ratio.\n* The core principle is to maintain a constant level of risk (or equal risk contribution) over time.\n* Position size is inversely proportional to the asset's risk (volatility), meaning the allocation automatically decreases when risk spikes.\n* It does not require estimation of expected returns (unlike Mean-Variance), making it more robust against common estimation errors.\n* The allocation is calculated via a simple, explicit formula, not an iterative numerical solver.\n\n***When it works best:***\n* High Volatility Environments: The strategy is defensive, automatically reducing market exposure when volatility rises, leading to stability.\n* Regime Shifts and Market Crises: It is robust because it relies only on the relatively stable estimation of volatility, not the volatile estimation of mean returns.\n* Risk Management: When the primary goal is capital preservation and portfolio stability over chasing maximum returns.","metadata":{}},{"cell_type":"code","source":"# Strategy 4: Risk Parity (Inverse Volatility) Allocation \ndef risk_parity_optimization(data):\n    \"\"\"\n    Implements a simple single-asset Risk Parity (RP) allocation based on Inverse Volatility.\n    \n    This strategy sets the position inversely proportional to the asset's rolling volatility\n    to maintain a constant risk contribution over time. \n    \"\"\"\n    # Use a 20-day rolling window for volatility calculation\n    window = min(20, len(data) // 4)\n    \n    if len(data) < window or data['forward_returns'].std() == 0:\n        # Fallback for short data or zero volatility\n        return np.full(len(data), 1.0) \n\n    # Calculate rolling standard deviation (volatility)\n    volatility = data['forward_returns'].rolling(window).std().fillna(method='bfill').values\n    \n    # RP allocates weights inversely to volatility: P_i proportional to 1/sigma_i\n    # Calculate the inverse volatility weight, using EPS to prevent division by zero\n    inv_volatility = 1 / (volatility + EPS)\n    \n    # Normalize the positions such that their mean is 1.0 (to maintain scale with MVO/Sharpe)\n    # np.nanmean is used for robustness against NaNs in early rolling periods\n    normalized_weights = inv_volatility / np.nanmean(inv_volatility)\n    \n    # Apply to positions and clip to bounds [MIN_POSITION, MAX_POSITION]\n    positions = np.clip(normalized_weights, MIN_POSITION, MAX_POSITION)\n    \n    # Handle NaNs\n    positions = np.nan_to_num(positions, nan=1.0) \n    \n    return positions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CAPM Alpha-based Signal Strategy**\n\nObjective Function:\n\nThe strategy's objective is to set the market position proportional to the historically realized Alpha signal (i.e., the market's excess return over the risk-free rate).\n\nPosition t = 1.0 + (Rolling Excess Return t * Scaling Factor)\n\nWhere:\n* Position t = Allocation to the risky asset at time t.\n* Rolling Excess Return t = Rolling mean of (Market Return - Risk-Free Rate).\n* Scaling Factor = A constant used to translate the small daily excess return into a position size.\n* 1.0 = A base market-neutral position, which is adjusted up or down by the signal.\n\n\n\n***Characteristics:***\n* Factor-Based: This is a factor-based strategy where the factor is historical outperformance (Alpha).\n* Simple and Quick: It uses a straightforward rolling mean calculation, avoiding the complexity and time of numerical optimization.\n* Momentum/Trend-Following: The strategy implicitly acts as a medium-term momentum/trend-following signal by investing more when the market has recently delivered positive excess returns.\n* Risk-Free Benchmark: The signal is explicitly benchmarked against the risk-free rate, aligning with the core concept of CAPM.\n* Requires Tuning: The Scaling Factor is a critical hyperparameter that must be tuned to keep the position within sensible bounds.\n\n\n***When it works best:***\n* Persistent Trends: Markets where historical market outperformance (positive alpha) is likely to continue into the future (i.e., when momentum is strong).\n* Benchmark Exploitation: Environments where the market is consistently outperforming the risk-free rate (cash) and that trend is expected to persist.\n* Simplicity and Speed: When fast, low-latency strategy calculation is necessary, as it doesn't rely on iterative minimization solvers.\n* Outperforming the \"Hold 1.0\" Baseline: When there are clear periods of positive alpha and clear periods of negative alpha, allowing the strategy to scale up and down effectively.","metadata":{}},{"cell_type":"code","source":"# Strategy 5: CAPM Alpha-based Signal Strategy \ndef capm_alpha_optimization(data):\n    \"\"\"\n    Implements a simple time-series strategy based on a rolling Alpha signal.\n    \n    In this single-asset context, the 'Alpha' signal is approximated by the rolling \n    mean of the market's excess return over the risk-free rate. Positive alpha suggests\n    historical outperformance and leads to a higher position.\n    \"\"\"\n    window = min(20, len(data) // 4)\n    \n    # Calculate rolling mean of excess return (Alpha-like signal)\n    market_excess = data['forward_returns'] - data['risk_free_rate']\n    alpha_signal = market_excess.rolling(window).mean().fillna(method='bfill').values\n    \n    # Scale the position based on the Alpha signal\n    # Base position is 1.0 (market neutral), scaled by the alpha signal times a factor (e.g., 50.0)\n    positions = 1.0 + (alpha_signal * 50.0) \n    \n    # Clip to bounds\n    positions = np.clip(positions, MIN_POSITION, MAX_POSITION)\n    \n    return positions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sortino Ratio Maximization Strategy**\n\nObjective Function:\n\nMaximize: Sortino Ratio = (Portfolio Mean Return - Rf)/Downside Deviation}\n\nWhere:\n* Downside Deviation is the standard deviation of only the returns that fall below the target return.\n* Rf is the Ris Free-Rate\n\n***Characteristics:***\n\n* Asymmetric Risk Focus: The strategy explicitly distinguishes between \"good\" volatility (upside movement) and \"bad\" volatility (downside movement). It penalizes only losses.\n* Risk Metric: Replaces total volatility (used in the Sharpe Ratio and Mean-Variance) with Downside Deviation, leading to a more intuitive measure of \"bad\" risk.\n* Non-Linear Optimization: Requires a numerical solver (like the existing SLSQP used for Sharpe Maximization) to find the optimal time series of positions.\n* Allocation Goal: Seeks to capture upward market moves while strongly suppressing drawdowns.\n\n***When it works best:***\n\n* Risk-Averse Investors: Ideal for clients or competitions (like the one in your notebook) where drawdown control and capital preservation are paramount.\n* Non-Normal Distributions: When asset returns are negatively skewed (have frequent, large losses), the Sortino Ratio is a more robust performance measure than the Sharpe Ratio.\n* Long-Term Strategy: Suitable for strategies focusing on compound growth, as minimizing the depth and frequency of negative returns is key to long-term compounding.\n* Downside Protection: When the primary goal is to minimize the probability of returns falling below a set benchmark, such as the risk-free rate.","metadata":{}},{"cell_type":"code","source":"# Strategy 6: Sortino Ratio Maximization\ndef sortino_ratio_optimization(data):\n    \"\"\"\n    Directly maximize the Sortino Ratio metric, focusing on reducing downside risk.\n    \n    Uses multiple random starting points (x0) and a non-linear solver (SLSQP).\n    \"\"\"\n    n_days = len(data)\n    \n    def objective(positions):\n        # Objective: Maximize Sortino Ratio. Negate it because we use 'minimize'.\n        return -calculate_sortino_ratio(data, positions)\n    \n    best_result = None\n    best_score = float('-inf')\n    \n    # Test multiple initial guesses to avoid getting stuck in local minima/maxima\n    initial_guesses = [\n        np.full(n_days, 0.5),  # Moderate\n        np.full(n_days, 1.0),  # Market neutral\n        np.random.uniform(0.3, 0.7, n_days),  # Random moderate\n        np.linspace(0.2, 0.8, n_days),  # Gradual increase\n    ]\n    \n    bounds = Bounds(MIN_POSITION, MAX_POSITION)\n    \n    for x0 in initial_guesses:\n        try:\n            # Optimization using SLSQP\n            result = minimize(\n                objective,\n                x0,\n                method='SLSQP',\n                bounds=bounds,\n                options={'maxiter': 1000, 'ftol': 1e-8}\n            )\n            \n            # Check if this run is successful and better than the current best\n            if result.success and -result.fun > best_score:\n                best_score = -result.fun\n                best_result = result\n        except Exception as e:\n            # Fail silently on initial guess failure\n            continue\n    \n    if best_result is not None:\n        # The result 'x' array contains the optimized positions\n        return np.clip(best_result.x, MIN_POSITION, MAX_POSITION)\n    else:\n        # Fallback to a conservative position if all optimizations fail\n        return np.full(n_days, 0.5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `evaluate_strategies` function, which serves as the main engine for the backtesting and selection phase of the portfolio optimization system. Its primary role is to test five different trading strategies on historical data, measure their performance using the Adjusted Sharpe Ratio, and select the best one.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# STRATEGY EVALUATION AND SELECTION\n# ============================================================================\n\ndef evaluate_strategies(train_data):\n    \"\"\"\n    Run all defined optimization strategies on the historical lookback window \n    and select the best performing one based on the Adjusted Sharpe Ratio.\n    \"\"\"\n    # Use only the most recent data for optimization (rolling window approach)\n    lookback = min(LOOKBACK_WINDOW, len(train_data))\n    recent_data = train_data.iloc[-lookback:].copy()\n    \n    # Feature creation is run, though not directly used by the current optimization objectives\n    recent_data = create_features(recent_data)\n    \n    # Calculate market metrics for context/reporting\n    market_vol = recent_data['forward_returns'].std() * np.sqrt(252) * 100\n    market_return = recent_data['forward_returns'].mean() * 252 * 100\n    \n    print(\"=\" * 80)\n    print(\"PORTFOLIO OPTIMIZATION STRATEGY EVALUATION\")\n    print(\"=\" * 80)\n    print(f\"Using {lookback} days of historical data\")\n    print(f\"Market volatility: {market_vol:.2f}%\\nAverage market return: {market_return:.2f}%\")\n    print(\"=\" * 80)\n    \n    strategies = {\n        'Mean-Variance': mean_variance_optimization,\n        'Minimum Variance': minimum_variance_optimization,\n        'Sharpe Maximization': sharpe_ratio_optimization,\n        'Risk Parity': risk_parity_optimization, \n        'CAPM Alpha Signal': capm_alpha_optimization,\n        'Sortino Maximization': sortino_ratio_optimization\n    }\n    \n    results = []\n    \n    for name, strategy_func in strategies.items():\n        print(f\"\\nOptimizing {name} strategy...\")\n        \n        try:\n            # Execute the optimization\n            positions = strategy_func(recent_data)\n            \n            # Evaluate the optimized positions using the competition metric\n            sharpe = calculate_adjusted_sharpe(recent_data, positions)\n            \n            # Calculate additional performance metrics for the leaderboard\n            portfolio_returns = (\n                recent_data['risk_free_rate'].values * (1 - positions) +\n                positions * recent_data['forward_returns'].values\n            )\n            \n            avg_position = np.mean(positions)\n            portfolio_vol = np.std(portfolio_returns) * np.sqrt(252) * 100\n            \n            results.append({\n                'Strategy': name,\n                'Adjusted Sharpe': sharpe,\n                'Avg Position': avg_position,\n                'Portfolio Vol (%)': portfolio_vol,\n                'Positions': positions \n            })\n            \n            print(f\"  âœ“ Adjusted Sharpe: {sharpe:.4f}\")\n            print(f\"  âœ“ Average Position: {avg_position:.4f}\")\n            print(f\"  âœ“ Portfolio Volatility: {portfolio_vol:.2f}%\")\n            \n        except Exception as e:\n            print(f\"  âœ— Failed: {str(e)}\")\n            results.append({\n                'Strategy': name,\n                'Adjusted Sharpe': -999,\n                'Avg Position': 0,\n                'Portfolio Vol (%)': 0,\n                'Positions': np.full(lookback, 0.5)\n            })\n    \n    # Create and display the performance leaderboard\n    leaderboard = pd.DataFrame(results)\n    leaderboard = leaderboard.sort_values('Adjusted Sharpe', ascending=False)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"STRATEGY LEADERBOARD\")\n    print(\"=\" * 80)\n    print(leaderboard[['Strategy', 'Adjusted Sharpe', 'Avg Position', 'Portfolio Vol (%)']].to_string(index=False))\n    print(\"=\" * 80)\n    \n    # Select the overall best strategy\n    best_strategy = leaderboard.iloc[0]\n    print(f\"\\nðŸ† WINNER: {best_strategy['Strategy']} with Sharpe {best_strategy['Adjusted Sharpe']:.4f}\")\n    \n    # Return the optimal positions for the best strategy\n    return best_strategy['Positions'], leaderboard\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# INFERENCE SETUP\n# ============================================================================\n\n# Global variables to store the pre-calculated optimal positions and track progress\noptimal_positions = None # Will store the 'Positions' array from the winning strategy\nposition_counter = 0     # Tracks the current day in the test set\n\ndef predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    Prediction function for the competition inference server (daily prediction).\n    \n    This function simulates the deployment phase where the optimal positions \n    calculated from the historical lookback window are executed sequentially.\n    \"\"\"\n    global position_counter, optimal_positions\n    \n    # Check if there's a pre-calculated position for the current step\n    if position_counter < len(optimal_positions):\n        position = float(optimal_positions[position_counter])\n    else:\n        # If the test set is longer than the lookback window, use the last calculated position\n        position = float(optimal_positions[-1])\n    \n    # Ensure position is within the defined bounds [0.0, 2.0]\n    position = np.clip(position, MIN_POSITION, MAX_POSITION)\n    \n    print(f\"[Step {position_counter}] Position: {position:.6f}\")\n    position_counter += 1\n    \n    return position # The required output: the position size for the current day\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n    \n# Run strategy evaluation and get best positions\nprint(\"\\nRunning portfolio optimization strategies...\")\n# This call calculates the 'optimal_positions' which are used in the 'predict' function\noptimal_positions, leaderboard = evaluate_strategies(train)\n    \n    \nprint(\"\\nâœ… Execution complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup inference server\nprint(\"\\nInitializing inference server...\")\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n    \n# Run server\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n# Competition environment\n    inference_server.serve()\nelse:\n# Local testing\n    print(\"Running local gateway for testing...\")\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:22:14.69857Z","iopub.execute_input":"2025-10-23T10:22:14.698924Z","iopub.status.idle":"2025-10-23T10:22:15.092278Z","shell.execute_reply.started":"2025-10-23T10:22:14.698895Z","shell.execute_reply":"2025-10-23T10:22:15.091543Z"}},"outputs":[],"execution_count":null}]}