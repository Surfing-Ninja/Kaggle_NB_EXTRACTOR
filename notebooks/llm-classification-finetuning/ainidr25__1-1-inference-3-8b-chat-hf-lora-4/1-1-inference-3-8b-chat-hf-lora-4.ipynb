{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":8449074,"sourceType":"datasetVersion","datasetId":5034873},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":243353541,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libs","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:23.77556Z","iopub.execute_input":"2025-06-04T12:52:23.776235Z","iopub.status.idle":"2025-06-04T12:52:58.706368Z","shell.execute_reply.started":"2025-06-04T12:52:23.776205Z","shell.execute_reply":"2025-06-04T12:52:58.705292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport time\nimport psutil\nimport os\n\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:58.708584Z","iopub.execute_input":"2025-06-04T12:52:58.70908Z","iopub.status.idle":"2025-06-04T12:52:58.715821Z","shell.execute_reply.started":"2025-06-04T12:52:58.709035Z","shell.execute_reply":"2025-06-04T12:52:58.714947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_gpu_memory_usage(device_id=0, stage=\"\"):\n    if torch.cuda.is_available():\n        torch.cuda.synchronize(device=device_id) # Pastikan semua operasi CUDA selesai\n        allocated_mb = torch.cuda.memory_allocated(device=device_id) / (1024 * 1024)\n        reserved_mb = torch.cuda.memory_reserved(device=device_id) / (1024 * 1024)\n        total_mb = torch.cuda.get_device_properties(device=device_id).total_memory / (1024 * 1024)\n        print(f\"GPU {device_id} Memory ({stage}): Allocated={allocated_mb:.2f} MB, Reserved={reserved_mb:.2f} MB, Total={total_mb:.2f} MB\")\n        return allocated_mb, reserved_mb\n    else:\n        print(f\"CUDA not available ({stage}). Cannot print GPU memory usage.\")\n        return None, None\n\ndef get_system_memory_usage(stage=\"\"):\n    process = psutil.Process(os.getpid())\n    mem_info = process.memory_info()\n    rss_mb = mem_info.rss / (1024 * 1024) # Resident Set Size in MB\n    vms_mb = mem_info.vms / (1024 * 1024) # Virtual Memory Size in MB\n    print(f\"System RAM Usage ({stage}): RSS={rss_mb:.2f} MB, VMS={vms_mb:.2f} MB\")\n    return rss_mb, vms_mb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:58.716875Z","iopub.execute_input":"2025-06-04T12:52:58.717169Z","iopub.status.idle":"2025-06-04T12:52:58.729817Z","shell.execute_reply.started":"2025-06-04T12:52:58.71714Z","shell.execute_reply":"2025-06-04T12:52:58.729212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_system_memory_usage(\"Initial - Before everything\")\nget_gpu_memory_usage(device_id=0, stage=\"Initial - GPU 0\")\nif torch.cuda.device_count() > 1:\n    get_gpu_memory_usage(device_id=1, stage=\"Initial - GPU 1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:58.731359Z","iopub.execute_input":"2025-06-04T12:52:58.731582Z","iopub.status.idle":"2025-06-04T12:52:59.047535Z","shell.execute_reply.started":"2025-06-04T12:52:58.731563Z","shell.execute_reply":"2025-06-04T12:52:59.046648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\nWEIGHTS_PATH = '/kaggle/input/1-1-training-3-8b-chat-hf-lora-4/llama_3_finetuned_model.pth'\nMAX_LENGTH = 1024\nBATCH_SIZE = 8\nDEVICE = torch.device(\"cuda\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:59.04853Z","iopub.execute_input":"2025-06-04T12:52:59.048827Z","iopub.status.idle":"2025-06-04T12:52:59.065707Z","shell.execute_reply.started":"2025-06-04T12:52:59.048769Z","shell.execute_reply":"2025-06-04T12:52:59.064982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Data ","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\n# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:59.066728Z","iopub.execute_input":"2025-06-04T12:52:59.066982Z","iopub.status.idle":"2025-06-04T12:52:59.138176Z","shell.execute_reply.started":"2025-06-04T12:52:59.066963Z","shell.execute_reply":"2025-06-04T12:52:59.137358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\nprint(test['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:59.13924Z","iopub.execute_input":"2025-06-04T12:52:59.13955Z","iopub.status.idle":"2025-06-04T12:52:59.145618Z","shell.execute_reply.started":"2025-06-04T12:52:59.139523Z","shell.execute_reply":"2025-06-04T12:52:59.144763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"%%time\n\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n\ntokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\nATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n\n# Move tensors to CPU and convert them to lists\ninput_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\nattention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_cpu\ndata['ATTENTION_MASKS'] = attention_masks_cpu\ndata[:2]\nget_system_memory_usage(\"After tokenizing data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:59.14665Z","iopub.execute_input":"2025-06-04T12:52:59.146912Z","iopub.status.idle":"2025-06-04T12:52:59.816243Z","shell.execute_reply.started":"2025-06-04T12:52:59.146892Z","shell.execute_reply":"2025-06-04T12:52:59.815291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load model ","metadata":{}},{"cell_type":"code","source":"# BitsAndBytes configuration\nbnb_config =  BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16,\n    bnb_8bit_use_double_quant=False)\n\n# Load base model on GPU 0\ndevice0 = torch.device('cuda:0')\nget_system_memory_usage(\"Before loading base_model_0\")\nget_gpu_memory_usage(device_id=0, stage=\"Before loading base_model_0\")\n\nbase_model_0 = LlamaForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:0')\nbase_model_0.config.pad_token_id = tokenizer.pad_token_id\nget_system_memory_usage(\"After loading base_model_0\")\nget_gpu_memory_usage(device_id=0, stage=\"After loading base_model_0\")\n\n# Load base model on GPU 1\ndevice1 = torch.device('cuda:1')\nget_system_memory_usage(\"Before loading base_model_1\")\nget_gpu_memory_usage(device_id=1, stage=\"Before loading base_model_1\")\n\nbase_model_1 = LlamaForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:1')\nbase_model_1.config.pad_token_id = tokenizer.pad_token_id\nget_system_memory_usage(\"After loading base_model_1\")\nget_gpu_memory_usage(device_id=1, stage=\"After loading base_model_1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:52:59.817235Z","iopub.execute_input":"2025-06-04T12:52:59.817503Z","iopub.status.idle":"2025-06-04T12:55:19.106347Z","shell.execute_reply.started":"2025-06-04T12:52:59.817482Z","shell.execute_reply":"2025-06-04T12:55:19.105429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load weights ","metadata":{}},{"cell_type":"code","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:19.108553Z","iopub.execute_input":"2025-06-04T12:55:19.10881Z","iopub.status.idle":"2025-06-04T12:55:19.113007Z","shell.execute_reply.started":"2025-06-04T12:55:19.108763Z","shell.execute_reply":"2025-06-04T12:55:19.112056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get peft\nmodel_0 = get_peft_model(base_model_0, peft_config).to(device0) \n# Load weights\nmodel_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_0.eval()\nget_gpu_memory_usage(device_id=0, stage=\"After PEFT and loading weights for model_0\")\n\nmodel_1 = get_peft_model(base_model_1, peft_config).to(device1)\nmodel_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_1.eval()\nget_gpu_memory_usage(device_id=1, stage=\"After PEFT and loading weights for model_1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:19.113948Z","iopub.execute_input":"2025-06-04T12:55:19.114367Z","iopub.status.idle":"2025-06-04T12:55:30.194953Z","shell.execute_reply.started":"2025-06-04T12:55:19.114346Z","shell.execute_reply":"2025-06-04T12:55:30.194047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trainable Parameters\nmodel_0.print_trainable_parameters(), model_1.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:30.196109Z","iopub.execute_input":"2025-06-04T12:55:30.19648Z","iopub.status.idle":"2025-06-04T12:55:30.208677Z","shell.execute_reply.started":"2025-06-04T12:55:30.196449Z","shell.execute_reply":"2025-06-04T12:55:30.207822Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\n","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\nget_system_memory_usage(\"Before inference loop\")\nget_gpu_memory_usage(device_id=0, stage=\"Before inference - GPU 0\")\nif device1: get_gpu_memory_usage(device_id=1, stage=\"Before inference - GPU 1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:30.209659Z","iopub.execute_input":"2025-06-04T12:55:30.209925Z","iopub.status.idle":"2025-06-04T12:55:30.418209Z","shell.execute_reply.started":"2025-06-04T12:55:30.209902Z","shell.execute_reply":"2025-06-04T12:55:30.417209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(df, model, device, batch_size=BATCH_SIZE):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:30.419271Z","iopub.execute_input":"2025-06-04T12:55:30.419507Z","iopub.status.idle":"2025-06-04T12:55:30.432362Z","shell.execute_reply.started":"2025-06-04T12:55:30.419489Z","shell.execute_reply":"2025-06-04T12:55:30.431497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"st = time.time()\n\nN_SAMPLES = len(data)\n\n# Split the data into two subsets\nhalf = round(N_SAMPLES / 2)\nsub1 = data.iloc[0:half].copy()\nsub2 = data.iloc[half:N_SAMPLES].copy()\n\n# Function to run inference in a thread\ndef run_inference(df, model, device, results, index):\n    results[index] = inference(df, model, device)\n\n# Dictionary to store results from threads\nresults = {}\n\n# start threads\nt0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\nt1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n\nt0.start()\nt1.start()\n\n# Wait for all threads to finish\nt0.join()\nt1.join()\n\n# Combine results back into the original DataFrame\ndata = pd.concat([results[0], results[1]], axis=0)\n\nprint(f\"Processing complete. Total time: {time.time() - st}\")\nget_system_memory_usage(\"After inference loop\")\nget_gpu_memory_usage(device_id=0, stage=\"After inference - GPU 0\")\nif device1: get_gpu_memory_usage(device_id=1, stage=\"After inference - GPU 1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:30.433363Z","iopub.execute_input":"2025-06-04T12:55:30.433593Z","iopub.status.idle":"2025-06-04T12:55:49.763074Z","shell.execute_reply.started":"2025-06-04T12:55:30.433575Z","shell.execute_reply":"2025-06-04T12:55:49.762132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n\nsample_sub[TARGETS] = data[TARGETS]\ndisplay(sample_sub)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:49.76409Z","iopub.execute_input":"2025-06-04T12:55:49.764358Z","iopub.status.idle":"2025-06-04T12:55:49.784975Z","shell.execute_reply.started":"2025-06-04T12:55:49.764336Z","shell.execute_reply":"2025-06-04T12:55:49.784049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:55:49.786215Z","iopub.execute_input":"2025-06-04T12:55:49.786815Z","iopub.status.idle":"2025-06-04T12:55:49.801537Z","shell.execute_reply.started":"2025-06-04T12:55:49.786764Z","shell.execute_reply":"2025-06-04T12:55:49.800926Z"}},"outputs":[],"execution_count":null}]}