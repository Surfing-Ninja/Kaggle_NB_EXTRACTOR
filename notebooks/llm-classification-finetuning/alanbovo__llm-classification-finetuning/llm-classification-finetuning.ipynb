{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":207984,"sourceType":"modelInstanceVersion","modelInstanceId":177316,"modelId":199621},{"sourceId":208004,"sourceType":"modelInstanceVersion","modelInstanceId":177333,"modelId":199638},{"sourceId":210036,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":177262,"modelId":199568}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! ls /kaggle/input/dependencies/scikitlearn/default/1\n! pip install sentence_transformers --no-index --find-links=file:///kaggle/input/dependencies/scikitlearn/default/1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle, os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom textblob import TextBlob\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom lightgbm import LGBMClassifier, early_stopping, log_evaluation\nfrom torch.utils.data import DataLoader\n\nmodel = SentenceTransformer('/kaggle/input/all-minilm-l6-v2/scikitlearn/default/1/all-MiniLM-L6-v2')\n\ndef load_csv(train = True):\n    if train:\n        return pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n    return pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\nclass EmbeddingDataset:\n    def __init__(self, texts):\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return self.texts[idx]\n\ndef batch_encode_texts(texts, model, batch_size=32):\n    dataset = EmbeddingDataset(texts)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    embeddings = []\n\n    for batch in tqdm(dataloader, desc=\"Batch Encoding\", unit=\"batch\"):\n        batch_embeddings = model.encode(batch, convert_to_tensor=True)\n        embeddings.append(batch_embeddings.cpu().numpy())\n\n    return np.vstack(embeddings)\n\ndef extract_features(df, train=False):\n    prompts = df[\"prompt\"].tolist()\n    responses_a = df[\"response_a\"].tolist()\n    responses_b = df[\"response_b\"].tolist()\n\n    # Batch embedding extraction\n    prompt_embeddings = batch_encode_texts(prompts, model)\n    response_a_embeddings = batch_encode_texts(responses_a, model)\n    response_b_embeddings = batch_encode_texts(responses_b, model)\n\n    # Sentiment analysis and verbosity\n    sentiment_a = [TextBlob(text).sentiment.polarity for text in responses_a]\n    sentiment_b = [TextBlob(text).sentiment.polarity for text in responses_b]\n    verbosity_a = [len(text.split()) for text in responses_a]\n    verbosity_b = [len(text.split()) for text in responses_b]\n\n    # Compute cosine similarities\n    similarity_a = np.diag(cosine_similarity(prompt_embeddings, response_a_embeddings))\n    similarity_b = np.diag(cosine_similarity(prompt_embeddings, response_b_embeddings))\n    similarity_a_b = np.diag(cosine_similarity(response_a_embeddings, response_b_embeddings))\n\n    # Stack features\n    features = np.column_stack([\n        similarity_a, similarity_b, similarity_a_b,\n        sentiment_a, sentiment_b,\n        verbosity_a, verbosity_b\n    ])\n\n    return features\n\ndef test_model():\n    test = load_csv(train=False)\n    clf: LGBMClassifier = pickle.load(open(\"/kaggle/input/model/scikitlearn/default/3/model.pkl\", \"rb\"))\n    label_encoder: LabelEncoder = pickle.load(open(\"/kaggle/input/model/scikitlearn/default/3/label_encoder.pkl\", \"rb\"))\n    X_test = extract_features(test)\n    test_predictions = clf.predict_proba(X_test)\n\n    test[\"winner_model_a\"] = test_predictions[:, 0]\n    test[\"winner_model_b\"] = test_predictions[:, 1]\n    test[\"winner_tie\"] = test_predictions[:, 2]\n\n    submission = test[[\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]\n    submission.columns = [\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_model_tie\"]\n    submission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Starting\", flush=True)\ntest_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:22:00.38608Z","iopub.execute_input":"2024-12-25T21:22:00.386424Z","iopub.status.idle":"2024-12-25T21:22:03.130076Z","shell.execute_reply.started":"2024-12-25T21:22:00.386398Z","shell.execute_reply":"2024-12-25T21:22:03.128749Z"}},"outputs":[],"execution_count":null}]}