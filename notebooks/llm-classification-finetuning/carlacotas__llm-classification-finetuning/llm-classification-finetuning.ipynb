{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"aa804c37-91bc-45b6-a766-f69f181539f4","_cell_guid":"1ebfd922-b329-4e0b-bc9d-23e8cfc724de","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-19T15:37:27.645669Z","iopub.execute_input":"2024-12-19T15:37:27.646074Z","iopub.status.idle":"2024-12-19T15:37:28.066657Z","shell.execute_reply.started":"2024-12-19T15:37:27.646028Z","shell.execute_reply":"2024-12-19T15:37:28.065322Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries helpful for cleaning text data\n\nimport re     #regular expression operations\nimport string #simplifies working with strings\nimport nltk   #Natural Language Toolkit\nnltk.download('stopwords') #download stopwords list from NLTK\nfrom nltk.corpus import stopwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:37:28.068918Z","iopub.execute_input":"2024-12-19T15:37:28.069404Z","iopub.status.idle":"2024-12-19T15:38:01.453886Z","shell.execute_reply.started":"2024-12-19T15:37:28.069367Z","shell.execute_reply":"2024-12-19T15:38:01.452724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:01.45538Z","iopub.execute_input":"2024-12-19T15:38:01.455948Z","iopub.status.idle":"2024-12-19T15:38:03.012674Z","shell.execute_reply.started":"2024-12-19T15:38:01.455913Z","shell.execute_reply":"2024-12-19T15:38:03.011288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# kaggle 'Competitions': LLM Classification Finetuning\n***Finetune LLMs to Predict Human Preference using Chatbot Arena conversations***\n\nThis notebook contain a solution for the [LLM Classification Finetuning on Kaggle](http://www.kaggle.com/competitions/llm-classification-finetuning/overview)\n\n\n**Main objective:** Predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models(LLMs).","metadata":{}},{"cell_type":"markdown","source":"**Data**\n\ntrain.csv\n\n- id - A unique identifier for the row.\n- model_a/b - The identity of model_a/b. Included in train.csv but not test.csv.\n- prompt - The prompt that was given as an input (to both models).\n- response_a/b - The response from model_a/b to the given prompt.\n- winner_model_a/b/tie - Binary columns marking the judge's selection. The ground truth target column.\n\ntest.csv\n\n- id\n- prompt\n- response_a/b\n\nsample_submission.csv A submission file in the correct format.\n- id\n- winner_model_a/b/tie - This is what is predicted from the test set.","metadata":{}},{"cell_type":"markdown","source":"\n**Main steps**\n\n1. [Understand the original data](#section1) <a href='#section1'></a>\n2. [Data Cleaning](#section2) <a href='#section2'></a>\n3. [Data Exploration](#section3) <a href='#section3'></a>\n4. [Feature Engineering](#section4) <a href='#section4'></a>\n5. [Model Selection and Training](#section5) <a href='#section5'></a>\n6. [Model Evaluation](#section6) <a href='#section6'></a>\n7. [Submission File](#section7) <a href='#section7'></a>","metadata":{}},{"cell_type":"markdown","source":"<a id='section1'></a>","metadata":{}},{"cell_type":"markdown","source":"## **1. Understand the original data**","metadata":{}},{"cell_type":"markdown","source":"> The first analysis is always made working with the Training set. Only after this first analysis, it comes the final step which is to make predictions based on the test set.","metadata":{}},{"cell_type":"markdown","source":"The first thing to do is to import the raw data.","metadata":{}},{"cell_type":"code","source":"# import the data\ntraining = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\nall_data = pd.concat([training,test])\n\nprint(\"Import Data Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:03.014819Z","iopub.execute_input":"2024-12-19T15:38:03.015206Z","iopub.status.idle":"2024-12-19T15:38:06.954649Z","shell.execute_reply.started":"2024-12-19T15:38:03.01517Z","shell.execute_reply":"2024-12-19T15:38:06.953207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Then we should look at our data to get familiar with data and understand the data types, data consistency, null counts to think about how to manage that data/information","metadata":{}},{"cell_type":"code","source":"training.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:06.958217Z","iopub.execute_input":"2024-12-19T15:38:06.958966Z","iopub.status.idle":"2024-12-19T15:38:07.018311Z","shell.execute_reply.started":"2024-12-19T15:38:06.958929Z","shell.execute_reply":"2024-12-19T15:38:07.017086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:07.019642Z","iopub.execute_input":"2024-12-19T15:38:07.020087Z","iopub.status.idle":"2024-12-19T15:38:07.047963Z","shell.execute_reply.started":"2024-12-19T15:38:07.02004Z","shell.execute_reply":"2024-12-19T15:38:07.046846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training.tail(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:07.049145Z","iopub.execute_input":"2024-12-19T15:38:07.049628Z","iopub.status.idle":"2024-12-19T15:38:07.067278Z","shell.execute_reply.started":"2024-12-19T15:38:07.049578Z","shell.execute_reply":"2024-12-19T15:38:07.064305Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='section2'></a>","metadata":{}},{"cell_type":"markdown","source":"## **2. Data Cleaning**","metadata":{}},{"cell_type":"markdown","source":"After we get familiar with data and understand the data types, we should look at data consistency, null counts to think about how to manage that data/information.","metadata":{}},{"cell_type":"code","source":"#check for duplicate id's\ntotal_id = len(training[\"id\"])\ntotal_unique_id = len(training[\"id\"].unique())\n\nprint(\"Total number of 'id' duplicates:\")\nprint(total_id - total_unique_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:07.069182Z","iopub.execute_input":"2024-12-19T15:38:07.069725Z","iopub.status.idle":"2024-12-19T15:38:07.088591Z","shell.execute_reply.started":"2024-12-19T15:38:07.069661Z","shell.execute_reply":"2024-12-19T15:38:07.086847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for null or empty cells\nnan_count = training.isna().sum().sum()\nnull_count = training.isnull().sum().sum()\n\nprint('Number of NaN values:', nan_count)\nprint('Number of null values:', null_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:07.089894Z","iopub.execute_input":"2024-12-19T15:38:07.090389Z","iopub.status.idle":"2024-12-19T15:38:07.17551Z","shell.execute_reply.started":"2024-12-19T15:38:07.090338Z","shell.execute_reply":"2024-12-19T15:38:07.174315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for consistency of model_a, model_b and LLMs identification\ntotal_unique_model_a = len(training[\"model_a\"].unique())\ntotal_unique_model_b = len(training[\"model_b\"].unique())\n\nprint(\"Total number of 'model_a' and 'model_b' unique values:\")\nprint('model_a =', total_unique_model_a)\nprint('model_b =', total_unique_model_b)\n\nLLM_a = training[\"model_a\"].unique()\nLLM_b = training[\"model_b\"].unique()\nLLM = list(set(LLM_a).intersection(set(LLM_b)))\n\nprint(\"total number of LLMs =\", len(LLM))\nprint('LLMs utilized:', LLM)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:07.176943Z","iopub.execute_input":"2024-12-19T15:38:07.177314Z","iopub.status.idle":"2024-12-19T15:38:07.203276Z","shell.execute_reply.started":"2024-12-19T15:38:07.177272Z","shell.execute_reply":"2024-12-19T15:38:07.2019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for duplicate prompts -  the same prompt could have been given to more than two different LLMs\ntotal_prompt = len(training[\"prompt\"])\ntotal_unique_prompt = len(training[\"prompt\"].unique())\n\nprint(\"Total number of 'prompt' duplicates:\")\nprint(total_prompt - total_unique_prompt)\n\n#There are 57477 observations and 5743 prompt duplicates without id duplicates -> having prompt duplicates are ok to have \n#and no further data cleaning is needed to deal with prompt duplicates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:07.20481Z","iopub.execute_input":"2024-12-19T15:38:07.205302Z","iopub.status.idle":"2024-12-19T15:38:07.282091Z","shell.execute_reply.started":"2024-12-19T15:38:07.205219Z","shell.execute_reply":"2024-12-19T15:38:07.280838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that the data was checked for duplicates, NaN and null values, and consistency, it is essential to clean cells with text data (columns _prompt_, _response_a_ and _response_b_).","metadata":{}},{"cell_type":"code","source":"#clean cells with text data\ndef preprocess_text(text):\n    #convert text to lower case\n    text = text.lower()\n    #remove digits and special characters using regular expressions\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    #tokenize the text\n    text = nltk.word_tokenize(text)\n    \n    return text\n\ndef remove_stopwords(text):\n    #remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text_no_stopwords = [word for word in text if word not in stop_words]\n\n    return text_no_stopwords\n\n\ndef lemmatization(text):\n    lemmatizer = nltk.WordNetLemmatizer()\n    lemmatizer_text = [lemmatizer.lemmatize(text) for text in text]\n\n    return lemmatizer_text\n\n\ndef clean_text(text):\n    #convert text to lower case, remove digits and special characters using regular expressions and remove stopwords combined together\n    text = preprocess_text(text)\n    filtered_text = remove_stopwords(text)\n    lemmatizer_text = lemmatization(filtered_text)\n    clean_text = ' '.join(lemmatizer_text)\n\n    return clean_text\n\ntraining[\"prompt\"] = training[\"prompt\"].apply(clean_text)\ntraining[\"response_a\"] = training[\"response_a\"].apply(clean_text)\ntraining[\"response_b\"] = training[\"response_b\"].apply(clean_text)\n\n\nprint(\"Clean Cells with Text Data Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:38:07.283521Z","iopub.execute_input":"2024-12-19T15:38:07.283866Z","iopub.status.idle":"2024-12-19T15:41:59.516618Z","shell.execute_reply.started":"2024-12-19T15:38:07.283833Z","shell.execute_reply":"2024-12-19T15:41:59.515354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:41:59.518264Z","iopub.execute_input":"2024-12-19T15:41:59.518668Z","iopub.status.idle":"2024-12-19T15:41:59.533888Z","shell.execute_reply.started":"2024-12-19T15:41:59.518598Z","shell.execute_reply":"2024-12-19T15:41:59.532786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training.tail(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:41:59.539611Z","iopub.execute_input":"2024-12-19T15:41:59.540058Z","iopub.status.idle":"2024-12-19T15:41:59.5574Z","shell.execute_reply.started":"2024-12-19T15:41:59.54002Z","shell.execute_reply":"2024-12-19T15:41:59.556134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, the raw text in columns _prompt_, _response_a_ and _response_b_ are clean and ready for the next steps. ","metadata":{}},{"cell_type":"markdown","source":"<a id='section3'></a>","metadata":{}},{"cell_type":"markdown","source":"## **3. Data Exploration**","metadata":{}},{"cell_type":"markdown","source":"The next thing to do it to get to know data more detailed and examining it to find initial patterns and interesting points","metadata":{}},{"cell_type":"code","source":"#barchart - model a\nresult_model_a = training[\"model_a\"].value_counts()\nprint(\"model a:\", result_model_a)\n\n## Matplotlib barchart:\nprint(\"-----\")\nprint(\"Matplotlib barchart, model a:\")\n  \nbarWidth = 0.45\nplt.figure(figsize=(15, 7))\n\nplt.bar(result_model_a.index, result_model_a.values, barWidth, color='r')\n\nplt.ylabel('Counts', fontweight ='bold', fontsize = 15)\nplt.xlabel('LLMs', fontweight ='bold', fontsize = 15)\n\nplt.xticks(rotation=90)\nplt.title('LLMs Value Counts - model a', fontweight ='bold', fontsize = 15)\n\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:41:59.559068Z","iopub.execute_input":"2024-12-19T15:41:59.559563Z","iopub.status.idle":"2024-12-19T15:42:00.421303Z","shell.execute_reply.started":"2024-12-19T15:41:59.55951Z","shell.execute_reply":"2024-12-19T15:42:00.420041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#barchart - model b\nresult_model_b = training[\"model_b\"].value_counts()\nprint(\"model b:\", result_model_b)\n\n## Matplotlib barchart:\nprint(\"-----\")\nprint(\"Matplotlib barchart, model b:\")\n  \nbarWidth = 0.45\nplt.figure(figsize=(15, 7))\n\nplt.bar(result_model_b.index, result_model_b.values, barWidth, color='g')\n\nplt.ylabel('Counts', fontweight ='bold', fontsize = 15)\nplt.xlabel('LLMs', fontweight ='bold', fontsize = 15)\n\nplt.xticks(rotation=90)\nplt.title('LLMs Value Counts - model b', fontweight ='bold', fontsize = 15)\n\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:00.423064Z","iopub.execute_input":"2024-12-19T15:42:00.423429Z","iopub.status.idle":"2024-12-19T15:42:01.176929Z","shell.execute_reply.started":"2024-12-19T15:42:00.423393Z","shell.execute_reply":"2024-12-19T15:42:01.175862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#barchart - model winner: winner model a, winner model b or winner tie\ndef which_winner(value):\n    if  value[\"winner_model_a\"] == 1:\n         #winner model a\n         value[\"winner_model_b\"] = 0\n         value[\"winner_tie\"] = 0\n         return 0\n    elif value[\"winner_model_b\"] == 1:\n         #winner model b\n         return 1\n    elif value[\"winner_tie\"] == 1:\n         #winner tie\n         return 2\n    return None\n\ntraining[\"winner\"] = training.apply(which_winner, axis=1)\n\ntraining[\"winner_model\"] = training[\"winner\"].astype(str)\ntraining.loc[training[\"winner_model\"] == \"0\", \"winner_model\"] = \"model a\"\ntraining.loc[training[\"winner_model\"] == \"1\", \"winner_model\"] = \"model b\"\ntraining.loc[training[\"winner_model\"] == \"2\", \"winner_model\"] = \"winner tie\"\n\nresult_model_winner = training[\"winner_model\"].value_counts()\nprint(\"model winner:\", result_model_winner)\n\nprint(\"-----\")\nprint(\"Matplotlib barchart, model winner:\")\n\nbarWidth = 0.75\nplt.figure(figsize=(8, 7))\n\nplt.bar(result_model_winner.index, result_model_winner.values, barWidth, color='b')\n\nplt.ylabel('Counts', fontweight ='bold', fontsize = 15)\nplt.xlabel('Model winner', fontweight ='bold', fontsize = 15)\n\nplt.title('LLMs Value Counts - model winner', fontweight ='bold', fontsize = 15)\n\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:01.17849Z","iopub.execute_input":"2024-12-19T15:42:01.178954Z","iopub.status.idle":"2024-12-19T15:42:02.614762Z","shell.execute_reply.started":"2024-12-19T15:42:01.178904Z","shell.execute_reply":"2024-12-19T15:42:02.613509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='section4'></a>","metadata":{}},{"cell_type":"markdown","source":"## **4. Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"After cleaning the raw data and understanding it more cleary, the next step is extracting meaningful information from the data to make it usable for machine learning models.","metadata":{}},{"cell_type":"code","source":"training.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:02.616773Z","iopub.execute_input":"2024-12-19T15:42:02.617283Z","iopub.status.idle":"2024-12-19T15:42:02.662872Z","shell.execute_reply.started":"2024-12-19T15:42:02.617206Z","shell.execute_reply":"2024-12-19T15:42:02.661692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#transform text data into numerical form\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features = 150) #without max_features it crashes due to memory limit\nvectorizer_prompt = vectorizer.fit_transform(training[\"prompt\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_response_a = vectorizer.fit_transform(training[\"response_a\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_response_b = vectorizer.fit_transform(training[\"response_b\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\n\n\n\ntemp_prompt = vectorizer.transform(training[\"prompt\"])\ntemp_response_a = vectorizer.fit_transform(training[\"response_a\"])\ntemp_response_b = vectorizer.fit_transform(training[\"response_b\"])\n\nprint(\"vectorizer representation for 'prompt':\\n\", temp_prompt.toarray())\nprint(\"vectorizer representation for 'response a':\\n\", temp_response_a.toarray())\nprint(\"vectorizer representation for 'response b':\\n\", temp_response_b.toarray())\n\nprint(\"Number of elements for the vectorizer representation for 'prompt':\\n\", temp_prompt.shape)\nprint(\"Number of elements for the vectorizer representation for 'response a':\\n\", temp_response_a.shape)\nprint(\"Number of elements for the vectorizer representation for 'response b':\\n\", temp_response_b.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:02.664209Z","iopub.execute_input":"2024-12-19T15:42:02.664562Z","iopub.status.idle":"2024-12-19T15:42:36.527129Z","shell.execute_reply.started":"2024-12-19T15:42:02.664528Z","shell.execute_reply":"2024-12-19T15:42:36.525934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#selecting the prediction target\ntrain_y = training[\"winner\"].values\n\n#choosing \"features\"\ntrain_X = np.concatenate((temp_prompt.toarray(), temp_response_a.toarray(), temp_response_b.toarray()), axis=1)\n\nprint(\"Selecting The Prediction Target and Choosing Features Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:36.528775Z","iopub.execute_input":"2024-12-19T15:42:36.529136Z","iopub.status.idle":"2024-12-19T15:42:36.924488Z","shell.execute_reply.started":"2024-12-19T15:42:36.529101Z","shell.execute_reply":"2024-12-19T15:42:36.923267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='section5'></a>","metadata":{}},{"cell_type":"markdown","source":"## **5. Model Selection and Training**","metadata":{}},{"cell_type":"markdown","source":"The next step is the model selection and predictions. I will use data science model **Logistic Regression** to predict the outcome of the winner model, because as input features we have *temp_prompt*, *temp_response_a* and *temp_response_b* and corresponding labels _winner_ (0 for winner model a, 1 for winner model b, and 2 for winner tie). Moreover, Logistic Regression predicts the probability for each target class as requested for the Submission File.","metadata":{}},{"cell_type":"code","source":"#use Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom datetime import datetime\n\n#record start time to calculate the execution time\nstart = datetime.now()\n\n#Logistic Regression\nmodel = LogisticRegression(max_iter=500, multi_class='multinomial', solver='saga') #For large datasets the “saga” solver is usually faster [scikit-learn documentation]\nmodel.fit(train_X, train_y)\n\n#record end time\nend = datetime.now()\n \n#calculate the execution time\nexecution_time = (end - start).total_seconds() / 60\nprint(f\"The time of execution is: {execution_time} minutes\")\n\n\nprint(\"Model Training Complete\")\n\n \n\n#Note: model = LogisticRegression() without futher improvements is giving a ConvergenceWarning:\n#/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n#STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n#Increase the number of iterations (max_iter) or scale the data as shown in:\n#    https://scikit-learn.org/stable/modules/preprocessing.html\n#Please also refer to the documentation for alternative solver options:\n#    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:36.92646Z","iopub.execute_input":"2024-12-19T15:42:36.926831Z","iopub.status.idle":"2024-12-19T15:42:55.688945Z","shell.execute_reply.started":"2024-12-19T15:42:36.926795Z","shell.execute_reply":"2024-12-19T15:42:55.687789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='section6'></a>","metadata":{}},{"cell_type":"markdown","source":"## **6. Model Evaluation**","metadata":{}},{"cell_type":"markdown","source":"In this step, the Logistic Regression model trained is evaluated to check how it performs to estimate the winner model (winner model a, winner model b or winner tie).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n#split into validation and training data\ntrain_X_train, train_X_val, train_y_train, train_y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n\n#record start time to calculate the execution time\nstart = datetime.now()\n\n#think about results - comparing predictions (value_y_predict) to the actual winner model (train_y_val)\nvalue_y_predict = model.predict(train_X_val)\nprint('Model winner prediction', value_y_predict)\nprint('Model winner real value', train_y_val)\n\nvalue_y_probabilities = model.predict_proba(train_X_val)\nprint('Model winner prediction, probability', value_y_probabilities) #winner model a | winner model b | winner tie\n\n#confusion matrix\ncm = confusion_matrix(train_y_val, value_y_predict)\nprint(\"Confusion Matrix:\\n\", cm)\n\n#model accuracy\nscore = model.score(train_X_val, train_y_val)\nprint('Model Accuracy Score', score)\n\n#macro and micro averaged Precision and Recall\nmacro_precision = precision_score(train_y_val, value_y_predict, average='macro') #calculate precision for all classes individually and then average them\nmacro_recall = recall_score(train_y_val, value_y_predict, average='macro')\nmicro_precision = precision_score(train_y_val, value_y_predict, average='micro') #calculate class wise true positive and false positive and then use that to calculate overall precision\nmicro_recall = recall_score(train_y_val, value_y_predict, average='micro')\nprint(\"Macro Precision:\", macro_precision)\nprint(\"Macro Recall:\", macro_recall)\nprint(\"Micro Precision:\", micro_precision)\nprint(\"Micro Recall:\", micro_recall)\n\n#record end time\nend = datetime.now()\n \n#calculate the execution time\nexecution_time = (end - start).total_seconds()\nprint(f\"The time of execution is: {execution_time} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:55.690634Z","iopub.execute_input":"2024-12-19T15:42:55.69099Z","iopub.status.idle":"2024-12-19T15:42:55.892511Z","shell.execute_reply.started":"2024-12-19T15:42:55.690956Z","shell.execute_reply":"2024-12-19T15:42:55.889847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model log loss - https://www.kaggle.com/competitions/llm-classification-finetuning/discussion/552103\nfrom sklearn.metrics import log_loss\n\nmodel_log_loss = log_loss(train_y_val, value_y_probabilities)\n\nprint('Model Log loss:', model_log_loss) \n\n# Number of classes = 3 : Logloss = - log(1/3) = 1.10\n# Model Log loss: 1.05, model prediction is considered good for this project","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:50:23.110396Z","iopub.execute_input":"2024-12-19T15:50:23.110802Z","iopub.status.idle":"2024-12-19T15:50:23.122575Z","shell.execute_reply.started":"2024-12-19T15:50:23.110759Z","shell.execute_reply":"2024-12-19T15:50:23.121283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='section7'></a>","metadata":{}},{"cell_type":"markdown","source":"## **7. Submission File**","metadata":{}},{"cell_type":"markdown","source":"The last step is to generate a CSV file with model predictions on test data to submit to the competition.","metadata":{}},{"cell_type":"code","source":"#clean cells with text data\ntest[\"prompt\"] = test[\"prompt\"].apply(clean_text)\ntest[\"response_a\"] = test[\"response_a\"].apply(clean_text)\ntest[\"response_b\"] = test[\"response_b\"].apply(clean_text)\n\n\nprint(\"Clean Cells with Text Data Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:55.894373Z","iopub.execute_input":"2024-12-19T15:42:55.89835Z","iopub.status.idle":"2024-12-19T15:42:55.957074Z","shell.execute_reply.started":"2024-12-19T15:42:55.895214Z","shell.execute_reply":"2024-12-19T15:42:55.955825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:55.958706Z","iopub.execute_input":"2024-12-19T15:42:55.95905Z","iopub.status.idle":"2024-12-19T15:42:55.969497Z","shell.execute_reply.started":"2024-12-19T15:42:55.959018Z","shell.execute_reply":"2024-12-19T15:42:55.968147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#transform text data into numerical form\n\nvectorizer = TfidfVectorizer(max_features = 150) #without max_features it crashes due to memory limit\nvectorizer_prompt = vectorizer.fit_transform(test[\"prompt\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_test_response_a = vectorizer.fit_transform(test[\"response_a\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_test_response_b = vectorizer.fit_transform(test[\"response_b\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\n\n\n\ntemp_test_prompt = vectorizer.transform(test[\"prompt\"])\ntemp_test_response_a = vectorizer.fit_transform(test[\"response_a\"])\ntemp_test_response_b = vectorizer.fit_transform(test[\"response_b\"])\n\nprint(\"vectorizer representation for 'prompt':\\n\", temp_test_prompt.toarray())\nprint(\"vectorizer representation for 'response a':\\n\", temp_test_response_a.toarray())\nprint(\"vectorizer representation for 'response b':\\n\", temp_test_response_b.toarray())\n\nprint(\"Number of elements for the vectorizer representation for 'prompt':\\n\", temp_test_prompt.shape)\nprint(\"Number of elements for the vectorizer representation for 'response a':\\n\", temp_test_response_a.shape)\nprint(\"Number of elements for the vectorizer representation for 'response b':\\n\", temp_test_response_b.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:55.971752Z","iopub.execute_input":"2024-12-19T15:42:55.972302Z","iopub.status.idle":"2024-12-19T15:42:56.059028Z","shell.execute_reply.started":"2024-12-19T15:42:55.972221Z","shell.execute_reply":"2024-12-19T15:42:56.057555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model prediction\ntest_X = np.concatenate((temp_test_prompt.toarray(), temp_test_response_a.toarray(), temp_test_response_b.toarray()), axis=1)\nvalue_test_y_probabilities = model.predict_proba(test_X)\nprint('Model winner prediction, probability', value_test_y_probabilities) #winner model a | winner model b | winner tie","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:56.060101Z","iopub.execute_input":"2024-12-19T15:42:56.06047Z","iopub.status.idle":"2024-12-19T15:42:56.069268Z","shell.execute_reply.started":"2024-12-19T15:42:56.060434Z","shell.execute_reply":"2024-12-19T15:42:56.067924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = pd.DataFrame({'id': test.id,\n                        'winner_model_a': value_test_y_probabilities[:, 0],\n                        'winner_model_b': value_test_y_probabilities[:, 1],\n                        'winner_tie': value_test_y_probabilities[:, 2]})\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:42:56.070926Z","iopub.execute_input":"2024-12-19T15:42:56.071445Z","iopub.status.idle":"2024-12-19T15:42:56.089326Z","shell.execute_reply.started":"2024-12-19T15:42:56.071404Z","shell.execute_reply":"2024-12-19T15:42:56.088051Z"}},"outputs":[],"execution_count":null}]}