{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10214229,"sourceType":"datasetVersion","datasetId":6267026},{"sourceId":10302322,"sourceType":"datasetVersion","datasetId":6265823},{"sourceId":75103,"sourceType":"modelInstanceVersion","modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Student Information\n* Họ và tên: Nguyễn Văn Lê Bá Thành\n* MSSV: 22127390\n* Lớp: HP2-K34","metadata":{}},{"cell_type":"markdown","source":"# Installing Packages","metadata":{}},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-06-30T00:52:42.91816Z","iopub.execute_input":"2025-06-30T00:52:42.918396Z","iopub.status.idle":"2025-06-30T00:53:29.62771Z","shell.execute_reply.started":"2025-06-30T00:52:42.918366Z","shell.execute_reply":"2025-06-30T00:53:29.626735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-06-30T00:53:29.628873Z","iopub.execute_input":"2025-06-30T00:53:29.62914Z","iopub.status.idle":"2025-06-30T00:54:18.406365Z","shell.execute_reply.started":"2025-06-30T00:53:29.629118Z","shell.execute_reply":"2025-06-30T00:54:18.405274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T00:54:18.409221Z","iopub.execute_input":"2025-06-30T00:54:18.409603Z","iopub.status.idle":"2025-06-30T00:54:59.453037Z","shell.execute_reply.started":"2025-06-30T00:54:18.409571Z","shell.execute_reply":"2025-06-30T00:54:59.452015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install --no-index --find-links=/kaggle/input/some-pack/sentence_transformers_packages sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T00:54:59.454566Z","iopub.execute_input":"2025-06-30T00:54:59.454845Z","iopub.status.idle":"2025-06-30T00:55:08.358698Z","shell.execute_reply.started":"2025-06-30T00:54:59.454821Z","shell.execute_reply":"2025-06-30T00:55:08.357645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"execution":{"iopub.status.busy":"2025-06-30T00:55:08.360404Z","iopub.execute_input":"2025-06-30T00:55:08.361087Z","iopub.status.idle":"2025-06-30T00:55:09.430559Z","shell.execute_reply.started":"2025-06-30T00:55:08.361051Z","shell.execute_reply":"2025-06-30T00:55:09.42948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check input files","metadata":{}},{"cell_type":"code","source":"import os\n\n# Walk through all directories and files under the '/kaggle/input' folder\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    # Loop through each file in the current directory\n    for filename in filenames:\n        # Print the full path to the file by joining the directory and filename\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T00:55:09.431847Z","iopub.execute_input":"2025-06-30T00:55:09.432129Z","iopub.status.idle":"2025-06-30T00:55:09.501182Z","shell.execute_reply.started":"2025-06-30T00:55:09.432104Z","shell.execute_reply":"2025-06-30T00:55:09.500336Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile test.py\nimport pandas as pd\n\n# Load the original test CSV file from the Kaggle input directory\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# Add dummy label columns indicating model A is always the winner\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\n\n# Save the original test dataframe (with dummy labels) to a Parquet file\ndf.to_parquet(\"test.parquet\", index=False)\n\n# Swap responses A and B — simulating the reverse comparison\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\n\n# Save the swapped version to a separate Parquet file\ndf.to_parquet(\"test_swap.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2025-06-30T00:55:09.502439Z","iopub.execute_input":"2025-06-30T00:55:09.503046Z","iopub.status.idle":"2025-06-30T00:55:09.509243Z","shell.execute_reply.started":"2025-06-30T00:55:09.503013Z","shell.execute_reply":"2025-06-30T00:55:09.508477Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python test.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T00:55:09.510311Z","iopub.execute_input":"2025-06-30T00:55:09.510568Z","iopub.status.idle":"2025-06-30T00:55:11.246338Z","shell.execute_reply.started":"2025-06-30T00:55:09.510548Z","shell.execute_reply":"2025-06-30T00:55:11.245313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile gemma.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n# --- Configuration ---\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"  # Pretrained model checkpoint path\ncsv_path = \"test.parquet\"  # Path to input data in .parquet format\n\n# --- Load tokenizer and processor ---\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n# Processor to tokenize input samples (prompt-response pairs)\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=False,\n)\n\n# Load dataset and apply preprocessing\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\n\n# DataLoader with custom collator to batch examples based on total token count\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,  # Each \"batch\" is a list of micro-batches\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192,  # Maximum total tokens per batch\n        base_collator=VarlenCollator()\n    ),\n)\n\n# --- Define pipeline parallelism across 2 GPUs ---\n\n# Total number of transformer layers in the model\nnum_hidden_layers = 42\n\n# Assign embedding, final layers, and score head to GPU 0 and 1\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\n\n# Split model layers between two GPUs: first half on cuda:0, second half on cuda:1\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\n# Load model with weights onto corresponding devices and use float16\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# --- Prepare rotary embeddings ---\nconfig = model.config\ndim = config.head_dim  # dimension of attention heads\n# Compute inverse frequencies for RoPE (rotary positional encoding)\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")  # For first half of model\ninv_freq1 = inv_freq.to(\"cuda:1\")  # For second half\n\n# --- Inference loop using pipelined execution ---\nis_first = True\nhidden_states = None\nouts = []\n\n# Loop through all batches\nfor batch in tqdm(dataloader):\n    # Each batch is a list of micro-batches\n    for micro_batch in batch:\n        # Move input tokens to GPU 0\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n\n        # Prepare sequence-related information\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n\n        # If first iteration, run only part 1 and store state\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n            is_first = False\n            # Move intermediate outputs to GPU 1 for next step\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n\n        # Run part 2 for previous micro-batch and part 1 for current micro-batch\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            # Compute final logits for previous hidden states\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n\n            # Compute hidden states for the next micro-batch\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n\n            # Move new hidden state and seq_info to GPU 1\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())  # Store prediction logits on CPU\n\n# --- Final prediction for the last micro-batch ---\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n# Concatenate all logits and compute probabilities\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\n\n# Evaluate predictions with dataset's built-in method\nprint(dataset.evaluate(prob.numpy()))\n\n# Save prediction probabilities to file\nnp.save('prob_m0.npy', prob)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-30T00:55:11.249378Z","iopub.execute_input":"2025-06-30T00:55:11.249653Z","iopub.status.idle":"2025-06-30T00:55:11.257866Z","shell.execute_reply.started":"2025-06-30T00:55:11.249631Z","shell.execute_reply":"2025-06-30T00:55:11.25701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python gemma.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T00:55:11.258799Z","iopub.execute_input":"2025-06-30T00:55:11.259058Z","iopub.status.idle":"2025-06-30T00:59:10.959287Z","shell.execute_reply.started":"2025-06-30T00:55:11.259034Z","shell.execute_reply":"2025-06-30T00:59:10.958313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"%%writefile llama.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n# --- Configurations ---\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"  # Path to pretrained LLaMA checkpoint\ncsv_path = \"test_swap.parquet\"  # Dataset file (responses are swapped)\n\n# --- Tokenizer and Processor setup ---\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n# Suppress tokenizer length warning\ntokenizer.deprecation_warnings[\n    \"sequence-length-is-longer-than-the-specified-maximum\"\n] = True\n\n# Processor formats prompt-response input for the model\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=True,\n)\n\n# --- Load Dataset ---\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\n\n# --- DataLoader setup with dynamic token batching ---\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192,\n        base_collator=VarlenCollator()\n    ),\n)\n\n# --- Device mapping for pipelined parallelism ---\nnum_hidden_layers = 32  # LLaMA-3 has 32 transformer layers\n\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\n\n# First half of layers on GPU 0\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n\n# Second half of layers on GPU 1\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\n# Load the model with float16 precision using the device map\nmodel = LlamaForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# --- Prepare rotary position encodings ---\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\n\n# Compute inverse frequencies for rotary embeddings\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n# --- Pipelined Inference across 2 GPUs ---\nis_first = True  # Special case for first micro-batch\nhidden_states = None\nouts = []  # Store output logits\n\n# Loop through batches\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        # Move inputs to GPU 0\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n\n        # Construct sequence info for attention mask and positional encoding\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n\n        if is_first:\n            # First micro-batch: run only forward_part1\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n\n            # Move intermediate results to GPU 1 for next step\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n\n        # Pipelined inference:\n        # - Run part2 for previous micro-batch on GPU 1\n        # - Run part1 for current micro-batch on GPU 0\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            # Prepare next micro-batch state\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())  # Save logits to CPU memory\n\n# --- Process final micro-batch (no part1 needed) ---\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n# --- Evaluate and Save ---\npred = torch.cat(outs, dim=0)         # Concatenate logits\nprob = pred.softmax(-1)               # Convert logits to probabilities\nprint(dataset.evaluate(prob.numpy())) # Evaluate predictions\n\nnp.save('prob_m3.npy', prob)          # Save prediction scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T00:59:10.96068Z","iopub.execute_input":"2025-06-30T00:59:10.960961Z","iopub.status.idle":"2025-06-30T00:59:10.968846Z","shell.execute_reply.started":"2025-06-30T00:59:10.960936Z","shell.execute_reply":"2025-06-30T00:59:10.968057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python llama.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T00:59:10.969906Z","iopub.execute_input":"2025-06-30T00:59:10.970232Z","iopub.status.idle":"2025-06-30T01:02:42.144502Z","shell.execute_reply.started":"2025-06-30T00:59:10.970206Z","shell.execute_reply":"2025-06-30T01:02:42.143546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprob = np.load('prob_m3.npy')\n\nprint(prob[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:42.145868Z","iopub.execute_input":"2025-06-30T01:02:42.146156Z","iopub.status.idle":"2025-06-30T01:02:42.153353Z","shell.execute_reply.started":"2025-06-30T01:02:42.146131Z","shell.execute_reply":"2025-06-30T01:02:42.152409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: Faiss","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\nimport faiss\n\nmodel_load_path = '/kaggle/input/some-pack/sentence-transformer-model' \nsentence_model = SentenceTransformer(model_load_path)\n\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\nclass CustomDebertaModel(nn.Module):\n    def __init__(self, model_name, num_labels, feature_dim=2, dropout_rate=0.1):\n        super(CustomDebertaModel, self).__init__()\n        \n        # Initialize DeBERTa model\n        self.base_model = AutoModel.from_pretrained(model_name)\n        \n        # Feature tower for similarity features (a small MLP)\n        self.feature_fc = nn.Sequential(\n            nn.Linear(feature_dim, 128),                 # Map input similarity features to 128-dimensional space\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(128, self.base_model.config.hidden_size),  # Project to same size as text embeddings\n            nn.ReLU()\n        )\n        \n        # Attention mechanism to allow interaction between text and similarity embeddings\n        self.attention = nn.MultiheadAttention(\n            embed_dim=self.base_model.config.hidden_size,\n            num_heads=4,  # Number of attention heads\n            batch_first=True  # Enable batch-first input format\n        )\n        \n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        # Final classifier layer (MLP for classification)\n        self.classifier = nn.Sequential(\n            nn.Linear(self.base_model.config.hidden_size * 2, self.base_model.config.hidden_size),  # Combine text + attention features\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(self.base_model.config.hidden_size, num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask, similarity_features, labels=None):\n        # Text tower: extract [CLS] token embedding from DeBERTa\n        base_outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeddings = base_outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n\n        # Feature tower: process similarity features through MLP\n        similarity_embeds = self.feature_fc(similarity_features)  # [batch_size, hidden_size]\n\n        # Cross-modal interaction using attention mechanism\n        query = text_embeddings.unsqueeze(1)       # Shape: [batch_size, 1, hidden_size]\n        key_value = similarity_embeds.unsqueeze(1) # Shape: [batch_size, 1, hidden_size]\n        attention_output, _ = self.attention(query, key_value, key_value)  # [batch_size, 1, hidden_size]\n\n        # Concatenate text and attended similarity features\n        combined_features = torch.cat([text_embeddings, attention_output.squeeze(1)], dim=1)\n\n        # Apply dropout and classification head\n        logits = self.classifier(self.dropout(combined_features))\n\n        # Output dictionary with logits\n        outputs = {\"logits\": logits}\n        \n        # If labels are provided (e.g., during training), compute the cross-entropy loss\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            outputs[\"loss\"] = loss_fn(logits, labels)\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:42.154481Z","iopub.execute_input":"2025-06-30T01:02:42.154735Z","iopub.status.idle":"2025-06-30T01:02:50.519921Z","shell.execute_reply.started":"2025-06-30T01:02:42.154715Z","shell.execute_reply":"2025-06-30T01:02:50.519127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport faiss\n\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# Compute semantic similarity scores using FAISS\ndef compute_semantic_features_with_faiss(df):\n    # Extract prompts and responses as lists\n    prompts = df['prompt'].tolist()\n    responses_a = df['response_a'].tolist()\n    responses_b = df['response_b'].tolist()\n\n    # Generate sentence embeddings and normalize them (unit vectors)\n    prompt_embeddings = np.array(sentence_model.encode(prompts))\n    prompt_embeddings = prompt_embeddings / np.linalg.norm(prompt_embeddings, axis=1, keepdims=True)\n\n    response_a_embeddings = np.array(sentence_model.encode(responses_a))\n    response_a_embeddings = response_a_embeddings / np.linalg.norm(response_a_embeddings, axis=1, keepdims=True)\n\n    response_b_embeddings = np.array(sentence_model.encode(responses_b))\n    response_b_embeddings = response_b_embeddings / np.linalg.norm(response_b_embeddings, axis=1, keepdims=True)\n\n    # Determine the embedding dimension\n    dim = prompt_embeddings.shape[1]\n    \n    # Create a FAISS index using inner product (cosine similarity since vectors are normalized)\n    index_flat = faiss.IndexFlatIP(dim)\n\n    # Compute similarity between response A and prompt\n    index_flat.add(prompt_embeddings)  # Add prompt embeddings to the FAISS index\n    similarity_a = index_flat.search(response_a_embeddings, k=1)[0].squeeze()  # Get top-1 similarity score\n\n    # Reset and compute similarity for response B\n    index_flat.reset()\n    index_flat.add(prompt_embeddings)\n    similarity_b = index_flat.search(response_b_embeddings, k=1)[0].squeeze()\n\n    # Store similarity scores in the DataFrame\n    df['similarity_a'] = similarity_a\n    df['similarity_b'] = similarity_b\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:50.521047Z","iopub.execute_input":"2025-06-30T01:02:50.521292Z","iopub.status.idle":"2025-06-30T01:02:50.530945Z","shell.execute_reply.started":"2025-06-30T01:02:50.521272Z","shell.execute_reply":"2025-06-30T01:02:50.530228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = compute_semantic_features_with_faiss(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:50.53193Z","iopub.execute_input":"2025-06-30T01:02:50.532232Z","iopub.status.idle":"2025-06-30T01:02:51.37259Z","shell.execute_reply.started":"2025-06-30T01:02:50.532212Z","shell.execute_reply":"2025-06-30T01:02:51.371603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the trained custom PyTorch model\nmodel = torch.load(\"/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\")\n\n# Set device to GPU if available, otherwise fallback to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()  # Important: set to evaluation mode\n\nprint(\"Custom model loaded successfully!\")\n\n# Load the tokenizer used during training\ntokenizer_path = \"/kaggle/input/akemiiiiii/custom_model_dir\"\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n# Function to preprocess a single test sample\ndef preprocess_test_data(row):\n    # Construct the input string in the same format used for training\n    input_text = f\"Prompt: {row['prompt']} Response A: {row['response_a']} Response B: {row['response_b']}\"\n\n    # Tokenize the input using the same tokenizer settings as training\n    tokenized_inputs = tokenizer(\n        input_text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n        return_tensors=\"pt\"  # Return PyTorch tensors\n    )\n\n    # Add similarity scores as additional features\n    # They should be shaped as [1, feature_dim] to batch correctly\n    similarity = torch.tensor([[row[\"similarity_a\"], row[\"similarity_b\"]]], dtype=torch.float32)\n\n    tokenized_inputs[\"similarity_features\"] = similarity\n\n    return tokenized_inputs\n\n# Apply the preprocessing function to every row of the test dataset\nprocessed_test_data = [preprocess_test_data(row) for _, row in test_data.iterrows()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:51.373718Z","iopub.execute_input":"2025-06-30T01:02:51.374044Z","iopub.status.idle":"2025-06-30T01:02:55.009237Z","shell.execute_reply.started":"2025-06-30T01:02:51.374013Z","shell.execute_reply":"2025-06-30T01:02:55.008478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ntest_dataset = TestDataset(processed_test_data)\n\ndef collate_fn_test(batch):\n    # Concatenate input_ids and attention_mask along batch dimension\n    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)\n    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)\n    \n    # Stack similarity features (already shape [1, 2])\n    similarity_features = torch.cat([item[\"similarity_features\"] for item in batch], dim=0)  # shape [B, 2]\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"similarity_features\": similarity_features,\n    }\n\n# DataLoader\ntest_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn_test, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:55.010275Z","iopub.execute_input":"2025-06-30T01:02:55.010536Z","iopub.status.idle":"2025-06-30T01:02:55.017168Z","shell.execute_reply.started":"2025-06-30T01:02:55.010516Z","shell.execute_reply":"2025-06-30T01:02:55.016302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the model to evaluation mode (important to deactivate dropout, etc.)\nmodel.eval()\n\n# List to store prediction results\npredictions = []\n\n# Disable gradient calculation (saves memory and speeds up inference)\nwith torch.no_grad():\n    for batch in test_dataloader:\n        # Move batch data to the correct device (GPU or CPU)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        similarity_features = batch[\"similarity_features\"].to(device)\n\n        # Run forward pass (model inference)\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            similarity_features=similarity_features\n        )\n\n        # Extract raw logits from output\n        logits = outputs[\"logits\"]\n\n        # Convert logits to probabilities using softmax\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n\n        # Move to CPU and convert to NumPy for later use\n        predictions.append(probs.cpu().numpy())\n\n# Concatenate all batch results into a single NumPy array\npredictions = np.concatenate(predictions, axis=0)\n\n# Print the final predictions\nprint(predictions)\n\n# Save predictions to a .npy file for downstream analysis or evaluation\nnp.save('prob_faiss.npy', predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:55.018279Z","iopub.execute_input":"2025-06-30T01:02:55.018543Z","iopub.status.idle":"2025-06-30T01:02:55.349677Z","shell.execute_reply.started":"2025-06-30T01:02:55.018506Z","shell.execute_reply":"2025-06-30T01:02:55.348754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"test.parquet\")\n\n\nprob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\nprob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3 (swap response_a and response_b)\nprob_faiss = np.load(\"prob_faiss.npy\")  # faiss\n\n# Combine predictions with weights\n# Adjust weights as needed for optimal performance\npreds = np.average(\n    [\n        prob_m0,       # Gemma2 results\n        prob_m3,       # Llama3 results\n        prob_faiss     # faiss results\n    ],\n    axis=0,\n    weights=[0.7, 0.2, 0.1]  # Weights for each model\n)\n\n# Create submission DataFrame\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\n\n# Save to CSV\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T01:02:55.350928Z","iopub.execute_input":"2025-06-30T01:02:55.351618Z","iopub.status.idle":"2025-06-30T01:02:55.408374Z","shell.execute_reply.started":"2025-06-30T01:02:55.35158Z","shell.execute_reply":"2025-06-30T01:02:55.407471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reference\n- LMSYS - Chatbot Arena Human Preference Predictions 2nd place solution - https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685\n- Blue - https://www.kaggle.com/code/blue0924/finetuning-test2","metadata":{}}]}