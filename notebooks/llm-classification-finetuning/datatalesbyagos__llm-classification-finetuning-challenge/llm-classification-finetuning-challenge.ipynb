{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6064,"sourceType":"modelInstanceVersion","modelInstanceId":4685,"modelId":2820}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport scipy.stats as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\n\npath = '/kaggle/input/llm-classification-finetuning/'\ndf = pd.read_csv(path + 'train.csv')\n\nprint(\"Shape:\", df.shape)\ndisplay(df.head())\n\nprint(\"\\nLabel distribution (winner_model_a / b / tie):\")\ncounts = {\n    'model_a_win': df['winner_model_a'].sum(),\n    'model_b_win': df['winner_model_b'].sum(),\n    'tie': df['winner_tie'].sum()\n}\nprint(counts)\nplt.figure(figsize=(5,4))\nsns.barplot(x=list(counts.keys()), y=list(counts.values()), palette='mako')\nplt.title(\"Win/Tie Distribution\")\nplt.show()\n\nprint(\"\\nMost frequent models in model_a:\")\nprint(Counter(df['model_a']).most_common(5))\nprint(\"\\nMost frequent models in model_b:\")\nprint(Counter(df['model_b']).most_common(5))\n\nprint(\"\\nWordClouds for prompts and responses\")\ntext_prompt = \" \".join(df['prompt'].astype(str).tolist()[:5000])\ntext_resp_a = \" \".join(df['response_a'].astype(str).tolist()[:5000])\ntext_resp_b = \" \".join(df['response_b'].astype(str).tolist()[:5000])\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\nplt.imshow(WordCloud(width=600, height=400, background_color='white').generate(text_prompt))\nplt.axis('off'); plt.title('Prompt')\nplt.subplot(1,3,2)\nplt.imshow(WordCloud(width=600, height=400, background_color='white').generate(text_resp_a))\nplt.axis('off'); plt.title('Response A')\nplt.subplot(1,3,3)\nplt.imshow(WordCloud(width=600, height=400, background_color='white').generate(text_resp_b))\nplt.axis('off'); plt.title('Response B')\nplt.show()\n\nprint(\"\\nExample comparison samples:\")\nfor i in range(3):\n    row = df.sample(1, random_state=i).iloc[0]\n    print(f\"\\nPrompt:\\n{row['prompt'][:200]}...\")\n    print(f\"\\nResponse A:\\n{row['response_a'][:200]}...\")\n    print(f\"\\nResponse B:\\n{row['response_b'][:200]}...\")\n    print(f\"Winner: {'A' if row['winner_model_a']==1 else 'B' if row['winner_model_b']==1 else 'Tie'}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:43:13.227047Z","iopub.execute_input":"2025-10-14T10:43:13.227303Z","iopub.status.idle":"2025-10-14T10:43:36.870075Z","shell.execute_reply.started":"2025-10-14T10:43:13.22728Z","shell.execute_reply":"2025-10-14T10:43:36.868968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Does response length influence which model wins?","metadata":{}},{"cell_type":"code","source":"df['label'] = df['winner_model_a'] + 2 * df['winner_model_b'] + 3 * df['winner_tie']\ndf['len_a'] = df['response_a'].astype(str).apply(len)\ndf['len_b'] = df['response_b'].astype(str).apply(len)\ndf['len_diff'] = df['len_a'] - df['len_b']\n\nplt.figure(figsize=(8,5))\nsns.kdeplot(df[df['label']==1]['len_diff'], label='A win', fill=True)\nsns.kdeplot(df[df['label']==2]['len_diff'], label='B win', fill=True)\nsns.kdeplot(df[df['label']==3]['len_diff'], label='Tie', fill=True)\nplt.axvline(0, color='black', linestyle='--', linewidth=1)\nplt.xlim(-2000, 2000)\nplt.title(\"Length Difference (A - B) by Outcome\")\nplt.xlabel(\"len_diff (positive = A longer)\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(6,5))\nsns.boxplot(x='label', y='len_diff', data=df)\nplt.title(\"Length Difference Distribution by Label\")\nplt.xlabel(\"Label (1=A win, 2=B win, 3=Tie)\")\nplt.ylabel(\"len_diff\")\nplt.axhline(0, color='black', linestyle='--', linewidth=1)\nplt.show()\n\nprint(\"\\nAverage length difference by label:\")\nprint(df.groupby('label')['len_diff'].describe()[['mean','std','min','max']])\n\na_win = df[df['label']==1]['len_diff']\nb_win = df[df['label']==2]['len_diff']\n\nstat, p_value = stats.mannwhitneyu(a_win, b_win, alternative='two-sided')\nprint(\"\\nMann-Whitney U test (A win vs B win):\")\nprint(f\"Statistic = {stat:.2f}, p-value = {p_value:.4f}\")\n\nif p_value < 0.05:\n    print(\"Significant difference in length when A wins vs B wins.\")\nelse:\n    print(\"No significant difference in length when A wins vs B wins.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:43:36.871464Z","iopub.execute_input":"2025-10-14T10:43:36.871881Z","iopub.status.idle":"2025-10-14T10:43:37.9332Z","shell.execute_reply.started":"2025-10-14T10:43:36.871851Z","shell.execute_reply":"2025-10-14T10:43:37.931862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Is response length enough to predict the winner?","metadata":{}},{"cell_type":"code","source":"X = df[['len_a', 'len_b', 'len_diff']]\ny = df['label']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nbaseline = LogisticRegression(max_iter=1000)\nbaseline.fit(X_train, y_train)\ny_pred = baseline.predict(X_test)\n\nprint(\"Baseline: Logistic Regression on length-based features\")\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:43:37.935441Z","iopub.execute_input":"2025-10-14T10:43:37.935809Z","iopub.status.idle":"2025-10-14T10:43:38.193051Z","shell.execute_reply.started":"2025-10-14T10:43:37.935778Z","shell.execute_reply":"2025-10-14T10:43:38.192129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Does wording itself help predict which response wins?","metadata":{}},{"cell_type":"code","source":"df['combined'] = df['prompt'] + \" [SEP] \" + df['response_a'] + \" [VS] \" + df['response_b']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df['combined'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n)\n\ntext_baseline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n    ('clf', LogisticRegression(max_iter=1000))\n])\n\ntext_baseline.fit(X_train, y_train)\ny_pred = text_baseline.predict(X_test)\n\nprint(\"Baseline: TF-IDF + Logistic Regression\")\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:43:38.194113Z","iopub.execute_input":"2025-10-14T10:43:38.194808Z","iopub.status.idle":"2025-10-14T10:45:39.675603Z","shell.execute_reply.started":"2025-10-14T10:43:38.194779Z","shell.execute_reply":"2025-10-14T10:45:39.674258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The length-based model did a bit better, around 43% accuracy, mostly picking up that longer answers tend to win.\nThe TF-IDF one dropped to about 38%, so wording alone doesn’t seem to explain much — the real difference probably comes from how well each response is written overall.","metadata":{}},{"cell_type":"code","source":"df['combined'] = df['prompt'] + \" [SEP] \" + df['response_a'] + \" [VS] \" + df['response_b']\nX_train = df['combined']\ny_train = df['label']\n\ntext_baseline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n    ('clf', LogisticRegression(max_iter=1000))\n])\n\ntext_baseline.fit(X_train, y_train)\n\ntest_df = pd.read_csv(path + 'test.csv')\ntest_df['combined'] = test_df['prompt'].fillna('') + \" [SEP] \" + \\\n                      test_df['response_a'].fillna('') + \" [VS] \" + \\\n                      test_df['response_b'].fillna('')\n\nprobs_test = text_baseline.predict_proba(test_df['combined'])\n\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'winner_model_a': probs_test[:, 0],\n    'winner_model_b': probs_test[:, 1],\n    'winner_tie': probs_test[:, 2]\n})\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:45:39.676337Z","iopub.execute_input":"2025-10-14T10:45:39.676602Z","iopub.status.idle":"2025-10-14T10:47:59.787321Z","shell.execute_reply.started":"2025-10-14T10:45:39.676581Z","shell.execute_reply":"2025-10-14T10:47:59.786579Z"}},"outputs":[],"execution_count":null}]}