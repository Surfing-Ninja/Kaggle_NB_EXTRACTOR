{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Model Approach\n\nThis notebook trains a multinomial logistic regression classifier to predict which LLM response wins in a head-to-head comparison.  \n\n- **Feature Representation**: The raw conversation text is transformed into numerical features using a TF-IDF vectorizer.  \n- **Model**: A multinomial logistic regression model from scikit-learn (`LogisticRegression`) is used to model the probability that a given response wins.  \n- **Calibration**: To improve the quality of the probability estimates, the base classifier is wrapped in a `CalibratedClassifierCV` with isotonic regression.  \n- **Cross-Validation**: The training data is split into stratified folds. For each fold, the model is trained on the training portion and predictions are generated for the validation portion and the test set. Test predictions are averaged across folds.  \n- **Evaluation**: Out-of-fold predictions are used to compute the overall log-loss on the training data, which provides an unbiased estimate of performance.  \n\nThis approach provides a simple yet effective baseline for the competition and can be extended with more complex feature representations or models.","metadata":{}},{"cell_type":"code","source":"# ==============================================\n# LLM Classification Finetuning - TFIDF Approach\n# ==============================================\n# This script uses local TF-IDF + Logistic Regression pipeline.\n\n# ==========================\n# Library Imports\n# ==========================\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import log_loss\nfrom scipy.sparse import hstack\n\n# ==========================\n# Data Loading\n# ==========================\ntrain_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# Map one-hot targets to single label (0: model_a, 1: model_b, 2: tie)\ntarget = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values\nlabels = target.argmax(axis=1)\n\nprint(f\"Training samples: {len(train_df)}, Test samples: {len(test_df)}\")\n\n# ==========================\n# Text Vectorization\n# ==========================\n# Combine all responses into a single corpus for fitting the vectorizer\ncorpus = pd.concat([\n    train_df['response_a'].astype(str),\n    train_df['response_b'].astype(str),\n    test_df['response_a'].astype(str),\n    test_df['response_b'].astype(str)\n])\n\n# Initialize TF-IDF vectorizer (unigrams + bigrams)\nvectorizer = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1, 2),\n    stop_words='english'\n)\nvectorizer.fit(corpus)\n\n# Transform responses\ntrain_a_tfidf = vectorizer.transform(train_df['response_a'].astype(str))\ntrain_b_tfidf = vectorizer.transform(train_df['response_b'].astype(str))\ntest_a_tfidf = vectorizer.transform(test_df['response_a'].astype(str))\ntest_b_tfidf = vectorizer.transform(test_df['response_b'].astype(str))\n\n# ==========================\n# Feature Engineering\n# ==========================\n# Difference-based features between model responses\ntrain_diff = train_a_tfidf - train_b_tfidf\ntrain_abs_diff = abs(train_a_tfidf - train_b_tfidf)\ntest_diff = test_a_tfidf - test_b_tfidf\ntest_abs_diff = abs(test_a_tfidf - test_b_tfidf)\n\n# Combine features horizontally\nX_train = hstack([train_diff, train_abs_diff])\nX_test = hstack([test_diff, test_abs_diff])\n\n# ==========================\n# Model Training\n# ==========================\noof_preds = np.zeros((len(train_df), 3))\ntest_preds = np.zeros((len(test_df), 3))\n\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train, labels), 1):\n    print(f\"\\n===== Fold {fold} / {n_splits} =====\")\n    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n    y_tr, y_val = labels[train_idx], labels[val_idx]\n\n    # Multinomial logistic regression with isotonic calibration\n    base_clf = LogisticRegression(max_iter=1000, multi_class='multinomial', n_jobs=-1)\n    calibrated_clf = CalibratedClassifierCV(base_clf, method='isotonic', cv=3)\n    calibrated_clf.fit(X_tr, y_tr)\n\n    # Store out-of-fold and test predictions\n    oof_preds[val_idx] = calibrated_clf.predict_proba(X_val)\n    test_preds += calibrated_clf.predict_proba(X_test) / n_splits\n\n# ==========================\n# Evaluation\n# ==========================\noof_logloss = log_loss(labels, oof_preds)\nprint(f\"\\nOverall OOF log loss: {oof_logloss:.6f}\")\n\n# ==========================\n# Submission\n# ==========================\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'winner_model_a': test_preds[:, 0],\n    'winner_model_b': test_preds[:, 1],\n    'winner_tie':    test_preds[:, 2],\n})\n\nprint(\"\\nSubmission preview:\")\nprint(submission.head())\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\nSaved submission.csv successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:26:27.926638Z","iopub.execute_input":"2025-10-27T21:26:27.926954Z","iopub.status.idle":"2025-10-27T21:31:07.572141Z","shell.execute_reply.started":"2025-10-27T21:26:27.926927Z","shell.execute_reply":"2025-10-27T21:31:07.571095Z"}},"outputs":[],"execution_count":null}]}