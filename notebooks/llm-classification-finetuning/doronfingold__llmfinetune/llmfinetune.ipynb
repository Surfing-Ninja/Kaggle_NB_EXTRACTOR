{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6489118,"sourceType":"datasetVersion","datasetId":3749864},{"sourceId":22784,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":18893,"modelId":28847}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM Classification Finetuning\n## Overview\nLLM Classification Finetuning competition task involving response classification, where the goal is to predict which of two model responses (A or B) is preferred or if it's a tie based on given prompts. It begins by loading and preprocessing data from train.csv and test.csv, cleaning and structuring conversational formats, adding features like length, sentiment, and TF-IDF similarity, and balancing classes. The data is transformed into a pointwise format for binary classification and tokenized using a DeBERTa-v3-base model with custom tokens. A custom model integrates the transformer base with extra features for scoring, trained using a CustomTrainer subclass of Hugging Face's Trainer, incorporating overrides for loss computation, prediction steps, and data collation to handle device placement and extra inputs. Hyperparameters are optimized via Optuna, and two models are trained for ensemble predictions. Finally, inference on the test set generates calibrated probabilities, which are used to produce a submission.csv with soft probabilities for winner_model_a, winner_model_b, and winner_tie, ensuring numerical stability.","metadata":{"_uuid":"e0808598-de69-47bf-8f2e-430d5ca20360","_cell_guid":"d90ec985-c13c-4521-9d64-38440f51da83","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Libraries","metadata":{"_uuid":"78989efe-138d-459a-8477-78d983a3f54d","_cell_guid":"018503e5-88c5-421b-aaee-cd2508f22377","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import ast\nimport re\nimport unicodedata\nimport torch\nimport torch.nn as nn\nimport optuna\nimport os\nimport pickle\nimport numpy as np\nfrom collections import Counter\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datetime import datetime\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom copy import deepcopy\nfrom transformers.trainer import Trainer, TrainerCallback\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, TrainingArguments, EarlyStoppingCallback\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.amp import autocast\nfrom datasets import Dataset, Value, DatasetDict, load_from_disk\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression","metadata":{"_uuid":"36f878f7-1b38-449b-9ebd-a3a3b4d71bfa","_cell_guid":"2885686b-32eb-4cf6-bae1-71fe846a98ce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T16:21:12.979595Z","iopub.execute_input":"2025-07-21T16:21:12.9798Z","iopub.status.idle":"2025-07-21T16:21:43.880878Z","shell.execute_reply.started":"2025-07-21T16:21:12.979781Z","shell.execute_reply":"2025-07-21T16:21:43.879978Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configurations and Settings\n\nHere, we define global configurations, including environment settings, caching functions to speed up repeated runs, a logging utility to track progress, and a fixed random seed for reproducibility.\n","metadata":{"_uuid":"11a1f9b8-7274-4848-b040-5afbdf3fc067","_cell_guid":"1321ace7-02bf-4f5f-aadf-e79253f8f239","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nUSE_CACHE = True\nIGNORE_ALL_CACHE = False\nCACHE_DIR = './cache'\nos.makedirs(CACHE_DIR, exist_ok=True)\n\ndef load_cache(name):\n    if not USE_CACHE or IGNORE_ALL_CACHE:\n        return None\n    pkl_path = os.path.join(CACHE_DIR, name + '.pkl')\n    dir_path = os.path.join(CACHE_DIR, name)\n    if os.path.exists(pkl_path):\n        with open(pkl_path, 'rb') as f:\n            return pickle.load(f)\n    elif os.path.isdir(dir_path):\n        return load_from_disk(dir_path)\n    return None\n\ndef save_cache(obj, name):\n    if not USE_CACHE or IGNORE_ALL_CACHE:\n        return\n    if isinstance(obj, pd.DataFrame):\n        obj.to_pickle(os.path.join(CACHE_DIR, name + '.pkl'))\n    elif isinstance(obj, (Dataset, DatasetDict)):\n        obj.save_to_disk(os.path.join(CACHE_DIR, name))\n\ndef log(message):\n    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')} - {message}\")\n\nSEED = 379\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)","metadata":{"_uuid":"f60f727b-ad3d-46ea-ba37-53fcb421ec19","_cell_guid":"9f1e7ebf-24fd-4c13-9ee3-f643042b3597","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T16:21:43.884999Z","iopub.execute_input":"2025-07-21T16:21:43.885292Z","iopub.status.idle":"2025-07-21T16:21:43.89856Z","shell.execute_reply.started":"2025-07-21T16:21:43.88527Z","shell.execute_reply":"2025-07-21T16:21:43.897894Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load and Explore Data\n\nWe load the training and testing datasets from CSV files and performs an initial exploration to understand their structure, view sample rows, and check for missing values.","metadata":{"_uuid":"adf9765f-8186-4f98-ae3d-aebe03ff52d0","_cell_guid":"7c6794bf-4ba1-477c-86d5-cda2d2f54393","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def load_data():\n    train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n    test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n    return train_df, test_df\n\ntrain_df, test_df = load_data()\nlog(\"Data loaded\")\n\nprint(\"Train Data Info:\")\nprint(train_df.info())\nprint(\"\\nTest Data Info:\")\nprint(test_df.info())\nlog(\"Printed data info\")\n\nprint(\"\\nTrain Data Sample:\")\nprint(train_df.head())\nprint(\"\\nTest Data Sample:\")\nprint(test_df.head())\nlog(\"Printed data samples\")\n\nprint(\"\\nMissing Values in Train:\")\nprint(train_df.isnull().sum())\nprint(\"\\nMissing Values in Test:\")\nprint(test_df.isnull().sum())\nlog(\"Printed missing values\")\n\n# sample 10,000 observations \n# train_df = train_df.sample(10000, random_state=SEED).reset_index(drop=True)\n# log(\"Sampled smaller data for POC\")","metadata":{"_uuid":"cad59781-6b8e-48b8-9a62-9081339a4bd4","_cell_guid":"fdd483e1-35f0-411e-8145-cd007d9edd5c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T16:21:52.806295Z","iopub.execute_input":"2025-07-21T16:21:52.806599Z","iopub.status.idle":"2025-07-21T16:21:56.197191Z","shell.execute_reply.started":"2025-07-21T16:21:52.806574Z","shell.execute_reply":"2025-07-21T16:21:56.196512Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Features\n\nThis extensive cell handles all feature engineering. It starts by cleaning and structuring the raw conversational text. Then, it extracts several features: text length, sentiment scores, and TF-IDF similarity between responses. Finally, it transforms the data into a pointwise format (one row per response) for binary classification, creates a Hugging Face Dataset, and calculates class weights to handle data imbalance.","metadata":{"_uuid":"b41ee32f-cf89-408a-8584-ce8ff1b87636","_cell_guid":"52b67518-4cbb-4020-a8ad-c176014605f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def count_segments(text):\n    if pd.isnull(text):\n        return 0\n    return len(re.findall(r'\\[\"[^\"]*\"\\]', text)) or len(re.findall(r'\"[^\"]*\"', text))\n\nn_prompts = train_df['prompt'].apply(count_segments)\nn_responses_a = train_df['response_a'].apply(count_segments)\nn_responses_b = train_df['response_b'].apply(count_segments)\nlog(\"Computed segment counts\")\n\nprompt_counts = n_prompts.value_counts().sort_index().reset_index()\nprompt_counts.columns = ['num_prompts', 'num_observations']\nresponse_a_counts = n_responses_a.value_counts().sort_index().reset_index()\nresponse_a_counts.columns = ['num_responses_a', 'num_observations']\nresponse_b_counts = n_responses_b.value_counts().sort_index().reset_index()\nresponse_b_counts.columns = ['num_responses_b', 'num_observations']\nlog(\"Counted frequencies\")\n\nprompt_counts['percent'] = 100 * prompt_counts['num_observations'] / prompt_counts['num_observations'].sum()\nresponse_a_counts['percent'] = 100 * response_a_counts['num_observations'] / response_a_counts['num_observations'].sum()\nresponse_b_counts['percent'] = 100 * response_b_counts['num_observations'] / response_b_counts['num_observations'].sum()\nlog(\"Computed percentages\")\n\ndef clean_text(text):\n    text = str(text)\n    text = text.encode('utf-8', 'replace').decode('utf-8')\n    text = re.sub(r'[\\uD800-\\uDFFF]', '?', text)\n    text = unicodedata.normalize('NFKD', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    if not text:\n        return '[EMPTY]'\n    return text\n\ndef parse_list(x):\n    if isinstance(x, list):\n        return x\n    if pd.isnull(x) or x == '':\n        return []\n    if isinstance(x, str):\n        x = clean_text(x)\n        try:\n            parsed = ast.literal_eval(x)\n            return [clean_text(item) for item in parsed]\n        except (ValueError, SyntaxError):\n            return [x]\n    return [str(x)]\n\ndef structure_conversational_data(row):\n    prompts_raw = parse_list(row.get('prompt', ''))\n    responses_a_raw = parse_list(row.get('response_a', ''))\n    responses_b_raw = parse_list(row.get('response_b', ''))\n    conv_a = \"\"\n    conv_b = \"\"\n    for p, ra, rb in zip(prompts_raw, responses_a_raw, responses_b_raw):\n        cp = clean_text(p) if p else '[EMPTY]'\n        cra = clean_text(ra) if ra else '[EMPTY]'\n        crb = clean_text(rb) if rb else '[EMPTY]'\n        conv_a += \"[USER] \" + cp + \" [ASSISTANT] \" + cra + \" \"\n        conv_b += \"[USER] \" + cp + \" [ASSISTANT] \" + crb + \" \"\n    return clean_text(conv_a.strip()), clean_text(conv_b.strip())\n\ncached_train_structured = load_cache('train_df_structured_v4')\nif cached_train_structured is not None:\n    train_df = cached_train_structured\nelse:\n    train_df[[\"response_a\", \"response_b\"]] = train_df.apply(lambda row: pd.Series(structure_conversational_data(row)), axis=1)\n    save_cache(train_df, 'train_df_structured_v4')\nlog(\"Structured conversational data for train_df\")\n\ncached_test_structured = load_cache('test_df_structured_v4')\nif cached_test_structured is not None:\n    test_df = cached_test_structured\nelse:\n    test_df[[\"response_a\", \"response_b\"]] = test_df.apply(lambda row: pd.Series(structure_conversational_data(row)), axis=1)\n    save_cache(test_df, 'test_df_structured_v4')\nlog(\"Structured conversational data for test_df\")\n\npositive_words = {'good', 'great', 'excellent', 'wonderful', 'best', 'love', 'like', 'positive', 'happy', 'awesome', 'fantastic', 'amazing', 'super', 'nice', 'cool'}\nnegative_words = {'bad', 'poor', 'terrible', 'worst', 'hate', 'dislike', 'negative', 'sad', 'awful', 'horrible', 'boring', 'stupid', 'wrong', 'false', 'fail'}\n\ndef get_sentiment(text):\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    pos = sum(1 for w in words if w in positive_words)\n    neg = sum(1 for w in words if w in negative_words)\n    total = len(words) + 1e-5\n    return (pos - neg) / total\n\ndef get_length(text):\n    return len(re.findall(r'\\b\\w+\\b', text))\n\nvectorizer = TfidfVectorizer(max_features=5000)\nall_responses = pd.concat([train_df['response_a'], train_df['response_b'], test_df['response_a'], test_df['response_b']])\nvectorizer.fit(all_responses)\n\ndef get_tfidf_sim(a, b):\n    vec_a = vectorizer.transform([a])\n    vec_b = vectorizer.transform([b])\n    return cosine_similarity(vec_a, vec_b)[0][0]\n\ncached_train_features = load_cache('train_df_features_v4')\nif cached_train_features is not None:\n    train_df = cached_train_features\nelse:\n    train_df['length_a'] = train_df['response_a'].apply(get_length)\n    train_df['length_b'] = train_df['response_b'].apply(get_length)\n    train_df['sentiment_a'] = train_df['response_a'].apply(get_sentiment)\n    train_df['sentiment_b'] = train_df['response_b'].apply(get_sentiment)\n    train_df['tfidf_sim'] = train_df.apply(lambda row: get_tfidf_sim(row['response_a'], row['response_b']), axis=1)\n    save_cache(train_df, 'train_df_features_v4')\nlog(\"Added features to train_df\")\n\ncached_test_features = load_cache('test_df_features_v4')\nif cached_test_features is not None:\n    test_df = cached_test_features\nelse:\n    test_df['length_a'] = test_df['response_a'].apply(get_length)\n    test_df['length_b'] = test_df['response_b'].apply(get_length)\n    test_df['sentiment_a'] = test_df['response_a'].apply(get_sentiment)\n    test_df['sentiment_b'] = test_df['response_b'].apply(get_sentiment)\n    test_df['tfidf_sim'] = test_df.apply(lambda row: get_tfidf_sim(row['response_a'], row['response_b']), axis=1)\n    save_cache(test_df, 'test_df_features_v4')\nlog(\"Added features to test_df\")\n\n# cached_train_balanced = load_cache('train_df_balanced_v4')\n# if cached_train_balanced is not None:\n#     train_df = cached_train_balanced\n# else:\n#     if 'winner_tie' not in train_df.columns:\n#         train_df['winner_tie'] = ((train_df['winner_model_a'] == 0) & (train_df['winner_model_b'] == 0)).astype(int)\n#     class_counts = train_df.groupby(['winner_model_a', 'winner_model_b', 'winner_tie']).size()\n#     min_count = class_counts.min()\n#     balanced_df = pd.DataFrame()\n#     for label, group in train_df.groupby(['winner_model_a', 'winner_model_b', 'winner_tie']):\n#         balanced_df = pd.concat([balanced_df, group.sample(min_count, random_state=SEED, replace=False)])\n#     train_df = balanced_df.reset_index(drop=True)\n#     save_cache(train_df, 'train_df_balanced_v4')\n# log(\"Balanced samples\")\n# log(f\"Label distribution in train_df: {train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].sum().to_dict()}\")\n\n\ndef build_pointwise_df(df):\n    rows = []\n    for i, row in df.iterrows():\n        tfidf_sim = row['tfidf_sim']\n        if row['winner_model_a'] == 1 and row['winner_model_b'] == 0:\n            # Response A is winner\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_a'],\n                'labels': 1,\n                'length': row['length_a'],\n                'sentiment': row['sentiment_a'],\n                'tfidf_sim': tfidf_sim\n            })\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_b'],\n                'labels': 0,\n                'length': row['length_b'],\n                'sentiment': row['sentiment_b'],\n                'tfidf_sim': tfidf_sim\n            })\n        elif row['winner_model_b'] == 1 and row['winner_model_a'] == 0:\n            # Response B is winner\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_a'],\n                'labels': 0,\n                'length': row['length_a'],\n                'sentiment': row['sentiment_a'],\n                'tfidf_sim': tfidf_sim\n            })\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_b'],\n                'labels': 1,\n                'length': row['length_b'],\n                'sentiment': row['sentiment_b'],\n                'tfidf_sim': tfidf_sim\n            })\n        elif row['winner_tie'] == 1:\n            # Tie: both are winners!\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_a'],\n                'labels': 1,\n                'length': row['length_a'],\n                'sentiment': row['sentiment_a'],\n                'tfidf_sim': tfidf_sim\n            })\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_b'],\n                'labels': 1,\n                'length': row['length_b'],\n                'sentiment': row['sentiment_b'],\n                'tfidf_sim': tfidf_sim\n            })\n    return pd.DataFrame(rows)\n    \ncached_pointwise = load_cache('pointwise_df_v1')\nif cached_pointwise is not None:\n    pointwise_df = cached_pointwise\nelse:\n    pointwise_df = build_pointwise_df(train_df)\n    pointwise_df['response'] = pointwise_df['response'].apply(clean_text)\n    save_cache(pointwise_df, 'pointwise_df_v1')\nlog(\"Built pointwise training dataframe\")\n\npointwise_df['labels'] = pointwise_df['labels'].astype('float')\nlog(f\"pointwise_df columns: {pointwise_df.columns.tolist()}\")\n#log(f\"pointwise_df sample: {pointwise_df.head().to_dict()}\")\nlog(f\"Label distribution in pointwise_df: {pointwise_df['labels'].value_counts().to_dict()}\")\n\ncached_dataset = load_cache('pointwise_dataset_v1')\nif cached_dataset is not None:\n    dataset = cached_dataset\nelse:\n    dataset = Dataset.from_pandas(pointwise_df)\n    save_cache(dataset, 'pointwise_dataset_v1')\nlog(\"Prepared pointwise dataset\")\n\n\nneg_count = (pointwise_df['labels'] == 0).sum()\npos_count = (pointwise_df['labels'] == 1).sum()\npos_weight = torch.tensor([neg_count / pos_count], dtype=torch.float32)","metadata":{"_uuid":"e89a587d-d205-40b9-9e2c-5c960aab677b","_cell_guid":"9963d5f4-0c93-491d-a8ae-43015bc004e0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T16:22:00.875251Z","iopub.execute_input":"2025-07-21T16:22:00.875873Z","iopub.status.idle":"2025-07-21T16:22:20.73846Z","shell.execute_reply.started":"2025-07-21T16:22:00.875846Z","shell.execute_reply":"2025-07-21T16:22:20.737815Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Test Split\n\nThe processed dataset is split into training and validation sets to prepare for model training and evaluation.","metadata":{"_uuid":"1c229b4c-3b49-4bc5-a4e3-1d88b78303a0","_cell_guid":"d59a3aca-59ce-4cf2-8457-24f513ae5115","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"cached_split = load_cache('pointwise_split_v1')\nif cached_split is not None:\n    split = cached_split\nelse:\n    split = dataset.train_test_split(test_size=0.1, seed=SEED)\n    split = DatasetDict({'train': split['train'], 'validation': split['test']})  # Rename for clarity\n    split['train'] = split['train'].cast_column('labels', Value('float32'))\n    split['validation'] = split['validation'].cast_column('labels', Value('float32'))\n    save_cache(split, 'pointwise_split_v1')\nlog(\"Prepared pointwise dataset and split (validation from train data)\")\n\nlog(f\"Dataset columns: {dataset.column_names}\")\nlog(f\"Train split columns: {split['train'].column_names}\")\nlog(f\"Test split columns: {split['validation'].column_names}\")","metadata":{"_uuid":"3140e8f3-3d7a-4e0b-a9fa-60fa885d73bf","_cell_guid":"27038703-bcf4-4e5b-983c-61ad44df70e3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T16:22:20.739357Z","iopub.execute_input":"2025-07-21T16:22:20.739571Z","iopub.status.idle":"2025-07-21T16:22:21.273634Z","shell.execute_reply.started":"2025-07-21T16:22:20.739554Z","shell.execute_reply":"2025-07-21T16:22:21.273006Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenize\n\nThis cell sets up the tokenizer based on the DeBERTa-v3 model. It defines a preprocessing function to tokenize the conversational text, format it with special tokens ([USER], [ASSISTANT]), and prepares the inputs for the model.","metadata":{"_uuid":"cf90b072-a40f-41d0-9007-80a933868967","_cell_guid":"283f9c1f-566c-49f2-a6f6-bffae2b9aa43","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model_path = '/kaggle/input/detect-ai-text-deberta-v3-large/pytorch/large/1'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\ntokenizer.add_special_tokens({'additional_special_tokens': ['[USER]', '[ASSISTANT]']})\nlog(\"Loaded tokenizer\")\n\ndef preprocess_function(examples):\n    inputs = ['[USER] ' + clean_text(p) + ' [ASSISTANT] ' + clean_text(r)\n              for p, r in zip(examples['prompt'], examples['response'])]\n    tokenized = tokenizer(\n        inputs,\n        truncation=True,\n        padding=True,\n        max_length=512,\n        return_tensors='pt'\n    )\n    vocab_size = len(tokenizer)\n    for i, input_ids in enumerate(tokenized['input_ids']):\n        if any(idx >= vocab_size for idx in input_ids):\n            log(f\"Warning: Invalid input_ids detected in sample {i}: {input_ids.tolist()}\")\n            raise ValueError(f\"input_ids exceed vocab size {vocab_size} in sample {i}\")\n        non_pad_tokens = [idx for idx in input_ids if idx != tokenizer.pad_token_id]\n        if len(non_pad_tokens) <= 2:\n            log(f\"Warning: Near-empty input detected in sample {i}: {input_ids.tolist()}\")\n            raise ValueError(f\"Near-empty input in sample {i}\")\n    if 'labels' in examples:\n        labels = examples['labels']\n        if not all(l in [0, 1] for l in labels):\n            log(f\"Warning: Invalid labels detected: {labels}\")\n            raise ValueError(\"Labels must be 0 or 1 only in pointwise training\")\n    input_lengths = [len([idx for idx in input_ids if idx != tokenizer.pad_token_id]) for input_ids in tokenized['input_ids']]\n    # log(f\"Input lengths stats: min={min(input_lengths)}, max={max(input_lengths)}, mean={np.mean(input_lengths):.2f}\")\n    tokenized['length'] = examples['length']\n    tokenized['sentiment'] = examples['sentiment']\n    tokenized['tfidf_sim'] = examples['tfidf_sim']\n    return tokenized\n\ncached_tokenized = load_cache('pointwise_tokenized_v1')\nif cached_tokenized is not None:\n    tokenized = cached_tokenized\nelse:\n    tokenized = split.map(\n        preprocess_function,\n        batched=True,\n        batch_size=1000,\n        remove_columns=[col for col in dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels', 'length', 'sentiment', 'tfidf_sim']]\n    )\n    save_cache(tokenized, 'pointwise_tokenized_v1')\nlog(\"Tokenized pointwise data\")\nlog(f\"Tokenized train columns: {tokenized['train'].column_names}\")\nlog(f\"Tokenized test columns: {tokenized['validation'].column_names}\")\nlog(f\"Label distribution in tokenized train: {Counter(tokenized['train']['labels'])}\")\nlog(f\"Label distribution in tokenized test: {Counter(tokenized['validation']['labels'])}\")","metadata":{"_uuid":"bac7e8ee-05c5-4cfe-9a81-e35bdca1ad86","_cell_guid":"bff3776f-be9a-4bf4-bc13-23d01e7f0cc0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T16:22:24.684817Z","iopub.execute_input":"2025-07-21T16:22:24.685075Z","iopub.status.idle":"2025-07-21T16:22:25.380611Z","shell.execute_reply.started":"2025-07-21T16:22:24.685057Z","shell.execute_reply":"2025-07-21T16:22:25.379835Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Model\n\nThis cell defines the core components for model training. It includes a custom model class that combines the DeBERTa transformer with the engineered features, a custom trainer to handle the unique data structure, a data collator, and a function to compute evaluation metrics. It also sets up the hyperparameter optimization  and loads the final parameters for training.","metadata":{"_uuid":"37b1619a-acbf-4439-9016-4c4ef8e088ee","_cell_guid":"192efab7-949d-4ae3-971f-d08b7f7a1ce6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def stable_sigmoid(x):\n    x = np.asanyarray(x)\n    result = np.empty_like(x)\n    pos = x >= 0\n    result[pos] = 1 / (1 + np.exp(-x[pos]))\n    result[~pos] = np.exp(x[~pos]) / (1 + np.exp(x[~pos]))\n    return result\n\ndef compute_metrics(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    logits = np.array(logits)\n    if logits.ndim == 2 and logits.shape[1] == 1:\n        logits = logits[:, 0]\n    elif logits.ndim == 1:\n        logits = logits\n    else:\n        logits = logits.flatten()\n    labels = np.array(labels)\n    if labels.ndim > 1:\n        labels = labels.flatten()\n    min_len = min(len(labels), len(logits))\n    labels = labels[:min_len]\n    logits = logits[:min_len]\n    if len(labels) == 0:\n        return {'accuracy': 0.0, 'f1': 0.0}\n    logits = np.nan_to_num(logits, nan=0.0, posinf=709.0, neginf=-709.0)\n    logits = np.clip(logits, -20, 20)\n    probs = stable_sigmoid(logits)\n    probs = np.nan_to_num(probs, nan=0.5)\n    preds_bin = (probs > 0.5).astype(int)\n    return {\n        'accuracy': accuracy_score(labels, preds_bin),\n        'f1': f1_score(labels, preds_bin, zero_division=1)\n    }\n\nfrom transformers import DataCollatorWithPadding\nfrom torch import tensor as torch_tensor  # To avoid name conflict with built-in tensor\n\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"length\"] = torch_tensor([f[\"length\"] for f in features], dtype=torch.float32)\n        batch[\"sentiment\"] = torch_tensor([f[\"sentiment\"] for f in features], dtype=torch.float32)\n        batch[\"tfidf_sim\"] = torch_tensor([f[\"tfidf_sim\"] for f in features], dtype=torch.float32)\n        if \"labels\" in features[0]:\n            batch[\"labels\"] = torch_tensor([f[\"labels\"] for f in features], dtype=torch.float32)\n        return batch\n\n\nclass GradientLoggingCallback(TrainerCallback):\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 1000 == 0:\n            model = kwargs['model']\n            for name, param in model.named_parameters():\n                if \"classifier\" in name and param.grad is not None:\n                    print(f\"{name}: grad norm = {param.grad.norm().item()}\")\n\nclass CustomModel(nn.Module):\n    def __init__(self, model_path, tokenizer, pos_weight=None):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n        base = AutoModel.from_pretrained(model_path, local_files_only=True)\n        base.resize_token_embeddings(len(tokenizer))\n        self.base = base\n        self.extra_fc = nn.Linear(3, 32)\n        self.classifier = nn.Linear(self.config.hidden_size + 32, 1)\n        self.pos_weight = pos_weight\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                length=None, sentiment=None, tfidf_sim=None, labels=None, **kwargs):\n        \n        outputs = self.base(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            **kwargs\n        )\n        pooled = outputs.last_hidden_state[:, 0]\n\n        if length is None or sentiment is None or tfidf_sim is None:\n            raise ValueError(\"Extra features (length, sentiment, tfidf_sim) must not be None\")\n        extra_feats = torch.stack([\n            length.float(), sentiment.float(), tfidf_sim.float()\n        ], dim=1)\n\n        extra = self.extra_fc(extra_feats)\n        combined = torch.cat([pooled, extra], dim=1)\n        logits = self.classifier(combined)\n\n        # Ensure logits is [batch_size, 1]\n        if logits.dim() == 1:\n            logits = logits.unsqueeze(1)\n        elif logits.dim() == 2 and logits.shape[1] != 1:\n            logits = logits[:, :1]  # If multi-class by mistake, take first\n        elif logits.dim() == 0:\n            logits = logits.unsqueeze(0).unsqueeze(1)\n\n        loss = None\n        if labels is not None:\n            # Pass the weight to the loss function\n            loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n            target = labels.float().view(-1, 1) \n            loss = loss_fn(logits, target)\n        \n        return {\n            \"loss\": loss,\n            \"logits\": logits\n        }\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        self.base.gradient_checkpointing_enable(\n            gradient_checkpointing_kwargs=gradient_checkpointing_kwargs\n        )\n\n        \nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        inputs = self._prepare_inputs(inputs)  \n        input_ids = inputs.get(\"input_ids\")\n        attention_mask = inputs.get(\"attention_mask\")\n        token_type_ids = inputs.get(\"token_type_ids\")\n        length = inputs.get(\"length\")\n        sentiment = inputs.get(\"sentiment\")\n        tfidf_sim = inputs.get(\"tfidf_sim\")\n        labels = inputs.get(\"labels\")\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            length=length,\n            sentiment=sentiment,\n            tfidf_sim=tfidf_sim,\n            labels=labels\n        )\n        loss = outputs[\"loss\"]\n        if return_outputs:\n            return (loss, outputs)\n        else:\n            return loss\n\n    def prediction_step(self, model: nn.Module, inputs: dict, prediction_loss_only: bool, ignore_keys: list = None):\n        inputs = self._prepare_inputs(inputs)  \n        has_labels = \"labels\" in inputs\n        with torch.no_grad():\n            with self.compute_loss_context_manager():\n                outputs = model(**inputs) \n        loss = outputs[\"loss\"].mean().detach() if outputs[\"loss\"] is not None else None\n        logits = outputs[\"logits\"]\n        labels = inputs[\"labels\"] if has_labels else None\n        if prediction_loss_only:\n            return (loss, None, None)\n        return (loss, logits, labels)\n\n    def get_train_dataloader(self):\n        dataset = self.train_dataset\n        data_collator = self.data_collator\n        return DataLoader(\n            dataset,\n            batch_size=self.args.per_device_train_batch_size,\n            collate_fn=data_collator,\n            num_workers=4,\n            pin_memory=True\n        )\n\n    def get_eval_dataloader(self, eval_dataset=None):\n        dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n        data_collator = self.data_collator\n        return DataLoader(\n            dataset,\n            batch_size=self.args.per_device_eval_batch_size,\n            collate_fn=data_collator,\n            num_workers=4,\n            pin_memory=True\n        )\n\n    def __init__(self, *args, tokenizer=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.data_collator = CustomDataCollator(tokenizer=tokenizer)\n\ndef objective(trial):\n    torch.cuda.empty_cache()\n    import gc\n    gc.collect()\n    log(\"Cleared cache and ran GC in objective\")\n    learning_rate = trial.suggest_float('learning_rate', 5e-5, 5e-4, log=True)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2])\n    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.1)\n    model = CustomModel(model_path, tokenizer, pos_weight=pos_weight)\n    log(\"Loaded model in objective\")\n    training_args = TrainingArguments(\n            output_dir='./results',\n            learning_rate=best_params['learning_rate'],\n            per_device_train_batch_size=best_params['per_device_train_batch_size'],\n            gradient_accumulation_steps=best_params['gradient_accumulation_steps'],\n            weight_decay=best_params['weight_decay'],\n            num_train_epochs=3,\n            eval_strategy=\"epoch\",\n            logging_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"f1\",\n            report_to=\"none\",\n            fp16=True,\n            save_total_limit=1,\n            gradient_checkpointing=True, \n            max_grad_norm=1.0,\n            adam_epsilon=1e-6\n        )\n    log(\"Set training args in objective\")\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized['train'],\n        eval_dataset=tokenized['validation'],\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        callbacks=[GradientLoggingCallback(), EarlyStoppingCallback(early_stopping_patience=5)]\n    )\n    log(\"Initialized trainer in objective\")\n    with autocast('cuda'):\n        trainer.train()\n    log(\"Trained in objective\")\n    eval_result = trainer.evaluate()\n    log(f\"Evaluation result: {eval_result}\")\n    torch.cuda.empty_cache()\n    gc.collect()\n    return eval_result['eval_f1']\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=3)\n# log(\"Optimized hyperparameters\")\n# best_params = study.best_params\n# print(f\"Best hyperparameters: {best_params}\")\n\n# uncomment above to run hyperparameters tuning with optuna.\nbest_params = {\n    'learning_rate': 8.01675539032066e-05,\n    'per_device_train_batch_size': 8,\n    'gradient_accumulation_steps': 2,\n    'weight_decay': 0.04828094508322041\n}\nlog(f\"Using pre-defined best parameters: {best_params}\")\n\n\nmodel1 = CustomModel(model_path, tokenizer, pos_weight=pos_weight)\nlog(\"Loaded final model1\")","metadata":{"_uuid":"5dab8985-5ee6-4add-9bce-5ea81fdc17ce","_cell_guid":"e3f4922c-b310-4aa0-847b-60966e985dd8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T16:29:55.520442Z","iopub.execute_input":"2025-07-21T16:29:55.521051Z","iopub.status.idle":"2025-07-21T16:29:56.960606Z","shell.execute_reply.started":"2025-07-21T16:29:55.521021Z","shell.execute_reply":"2025-07-21T16:29:56.959842Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With all components defined, this cell configures the training arguments and initiates the training process for the first model. It also sets up a second model for ensembling and prepares the test dataset for inference by tokenizing it in the same pointwise manner.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    learning_rate=best_params['learning_rate'],\n    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n    gradient_accumulation_steps=best_params['gradient_accumulation_steps'],\n    weight_decay=best_params['weight_decay'],\n    num_train_epochs=3,\n    eval_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    report_to=\"none\",\n    fp16=True,\n    save_total_limit=1,\n    gradient_checkpointing=True, # Helps with memory\n    max_grad_norm=1.0,\n    adam_epsilon=1e-6\n)\n\nlog(\"Set final training args\")\n\ntrainer1 = CustomTrainer(\n    model=model1,\n    args=training_args,\n    train_dataset=tokenized['train'],\n    eval_dataset=tokenized['validation'],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    callbacks=[GradientLoggingCallback(), EarlyStoppingCallback(early_stopping_patience=5)]\n)\nlog(\"Initialized trainer1\")\nwith autocast('cuda'):\n    trainer1.train()\nlog(\"Trained model1\")\n\ntorch.manual_seed(SEED + 18)\ntorch.cuda.manual_seed(SEED + 18)\n\nmodel2 = CustomModel(model_path, tokenizer, pos_weight=pos_weight)\nlog(\"Loaded model2\")\n\ntrainer2 = CustomTrainer(\n    model=model2,\n    args=training_args,\n    train_dataset=tokenized['train'],\n    eval_dataset=tokenized['validation'],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    callbacks=[GradientLoggingCallback(), EarlyStoppingCallback(early_stopping_patience=5)]\n)\n# log(\"Initialized trainer2\")\n# with autocast('cuda'):\n#     trainer2.train()\n# log(\"Trained model2\")\n\ncached_test_dataset = load_cache('test_dataset_v4')\nif cached_test_dataset is not None:\n    test_dataset = cached_test_dataset\nelse:\n    test_dataset = Dataset.from_pandas(test_df)\n    save_cache(test_dataset, 'test_dataset_v4')\nlog(\"Created test dataset\")\n\ndef add_extra_feats_pointwise(examples, response_key):\n    if response_key == 'response_a':\n        length = examples['length_a']\n        sentiment = examples['sentiment_a']\n    else:\n        length = examples['length_b']\n        sentiment = examples['sentiment_b']\n    tfidf_sim = examples['tfidf_sim']\n    return {\n        'length': length,\n        'sentiment': sentiment,\n        'tfidf_sim': tfidf_sim\n    }\n\ndef tokenize_pointwise_batch(examples, response_key):\n    inputs = ['[USER] ' + clean_text(p) + ' [ASSISTANT] ' + clean_text(r)\n              for p, r in zip(examples['prompt'], examples[response_key])]\n    tokenized = tokenizer(inputs, truncation=True, padding=True, max_length=512)\n    return tokenized\n\ncached_tokenized_test_a = load_cache('test_tokenized_a_pointwise_v1')\nif cached_tokenized_test_a is not None:\n    tokenized_test_a = cached_tokenized_test_a\nelse:\n    test_with_feats = test_dataset.map(lambda examples: add_extra_feats_pointwise(examples, 'response_a'), batched=True)\n    tokenized_test_a = test_with_feats.map(lambda examples: tokenize_pointwise_batch(examples, 'response_a'), batched=True, batch_size=1000)\n    save_cache(tokenized_test_a, 'test_tokenized_a_pointwise_v1')\n\ncached_tokenized_test_b = load_cache('test_tokenized_b_pointwise_v1')\nif cached_tokenized_test_b is not None:\n    tokenized_test_b = cached_tokenized_test_b\nelse:\n    test_with_feats = test_dataset.map(lambda examples: add_extra_feats_pointwise(examples, 'response_b'), batched=True)\n    tokenized_test_b = test_with_feats.map(lambda examples: tokenize_pointwise_batch(examples, 'response_b'), batched=True, batch_size=1000)\n    save_cache(tokenized_test_b, 'test_tokenized_b_pointwise_v1')\nlog(\"Tokenized test data for pointwise\")","metadata":{"_uuid":"24aa8d20-1804-4791-b86d-6095f5094971","_cell_guid":"b79342ad-f314-4f1f-8c39-764e60dbb3e1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-21T16:30:05.667481Z","iopub.execute_input":"2025-07-21T16:30:05.668009Z","execution_failed":"2025-07-22T04:21:02.525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction\n\nIn this final step, the trained model makes predictions on the test set. It calculates scores for both response A and response B for each example. A custom probability logic is then applied to these scores to determine the final probabilities for 'winner_model_a', 'winner_model_b', and 'winner_tie', which are then saved to submission.csv.","metadata":{"_uuid":"e2aa0db0-af2f-4ec1-af4c-0ddcffed1cd3","_cell_guid":"db924e1f-eb87-408d-a0de-b60e74b1189a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"## Prediction\n\nimport numpy as np\nfrom torch.amp import autocast\n\ndef flatten_logits(logits):\n    if isinstance(logits, dict):\n        logits = logits.get(\"logits\")\n    if logits is None:\n        raise ValueError(\"Logits are None!\")\n    return np.array(logits).flatten().astype(np.float64)\n\ndef clip_scores(scores):\n    return np.clip(scores, -20, 20)\n    \n# i comment out stacking of the 2nd model as the script runs out of time..\nlog(\"Starting prediction...\")\nwith autocast(device_type='cuda'):\n    logits_a_model1 = clip_scores(flatten_logits(trainer1.predict(tokenized_test_a).predictions))\n    logits_b_model1 = clip_scores(flatten_logits(trainer1.predict(tokenized_test_b).predictions))\n    # logits_a_model2 = clip_scores(flatten_logits(trainer2.predict(tokenized_test_a).predictions))\n    # logits_b_model2 = clip_scores(flatten_logits(trainer2.predict(tokenized_test_b).predictions))\n\navg_logits_a = logits_a_model1 #(logits_a_model1 + logits_a_model2) / 2\navg_logits_b = logits_b_model1 #(logits_b_model1 + logits_b_model2) / 2\nlog(\"Averaged logits from both models.\")\n\n\nlogit_threshold = 0.5 \ntemperature = 0.2    \n\nwinner_a_prob, winner_b_prob, winner_tie_prob = [], [], []\n\n# Loop over the raw averaged logits\nfor logit_a, logit_b in zip(avg_logits_a, avg_logits_b):\n    logit_diff = logit_a - logit_b\n    \n    # 1. Calculate tie probability based on the raw logit difference. This is more robust.\n    p_tie = np.exp(-np.abs(logit_diff) / logit_threshold)\n    \n    # 2. For the non-tie part, use the logits directly in a softmax-style calculation.\n    exp_a = np.exp(logit_a / temperature)\n    exp_b = np.exp(logit_b / temperature)\n    \n    # Handle potential overflow with large logits\n    if np.isinf(exp_a) or np.isinf(exp_b):\n        p_a_no_tie = 1.0 if logit_a > logit_b else 0.0\n    else:\n        p_a_no_tie = exp_a / (exp_a + exp_b)\n        \n    p_b_no_tie = 1.0 - p_a_no_tie\n    \n    # 3. Distribute the remaining probability (1 - p_tie) between A and B\n    w_a = (1 - p_tie) * p_a_no_tie\n    w_b = (1 - p_tie) * p_b_no_tie\n    \n    winner_a_prob.append(w_a)\n    winner_b_prob.append(w_b)\n    winner_tie_prob.append(p_tie)\n\nlog(\"Computed final soft probabilities.\")\n\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'winner_model_a': winner_a_prob,\n    'winner_model_b': winner_b_prob,\n    'winner_tie': winner_tie_prob\n})\n\n# Final normalization to ensure rows sum perfectly to 1\nprob_sum = submission_df[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\nsubmission_df['winner_model_a'] /= prob_sum\nsubmission_df['winner_model_b'] /= prob_sum\nsubmission_df['winner_tie'] /= prob_sum\n\nsubmission_df.to_csv('submission.csv', index=False)\nlog(\"Created and saved submission.csv\")","metadata":{"_uuid":"3f6c826d-e036-4b06-a4b0-7863d0d13a31","_cell_guid":"26a17fc0-9937-4d86-8b23-ba2ef3d1e2ac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nI kept running out of time trying to submit it. This probably has to do with using DeBERTa-v3-large instead of base. I will revisit this once I have more GPU time. There is lots of room for improvment including more complex semtiment features. perhaps sarcasm, as well as classification of topic or type of request (techincal question, current events, school related and so on) \n\nI also would like to think that real life data might have more meta data such as date and time - the expectation from LLM response as been increased of the years there for berhaps a winning response from a few years back will no longer win.. location - could indicate different standards etc. ","metadata":{}}]}