{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":2233309,"sourceType":"datasetVersion","datasetId":1335671}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"markdown","source":"Anda mungkin menemui library yang tertulis 2 kali pada cell ini atau dibawahnya.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss, accuracy_score, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nimport json\nimport ast # For parsing string representations of lists\nimport gc\nfrom catboost import CatBoostClassifier\nimport re\nimport unicodedata\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LogisticRegression\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport warnings\nimport json\nimport re\nfrom scipy.sparse import hstack, csr_matrix\nwarnings.filterwarnings('ignore')\n\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:43:13.06137Z","iopub.execute_input":"2025-06-09T02:43:13.061937Z","iopub.status.idle":"2025-06-09T02:43:13.067503Z","shell.execute_reply.started":"2025-06-09T02:43:13.061915Z","shell.execute_reply":"2025-06-09T02:43:13.066725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Dataset and Review Distribution","metadata":{}},{"cell_type":"markdown","source":"Sebaiknya lakukan EDA secara manual karena kurang efektif jika ditampilkan dengan visualisasi untuk dataset ini.","metadata":{}},{"cell_type":"code","source":"print(\"Loading datasets...\")\ntrain_df = pd.read_csv('../input/llm-classification-finetuning/train.csv')\ntest_df = pd.read_csv('../input/llm-classification-finetuning/test.csv')\nsample_submission = pd.read_csv('../input/llm-classification-finetuning/sample_submission.csv')\n\nprint(\"Loading data...\")\nprint(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n\n# --- Configuration ---\nSEED = 42\nN_FOLDS = 5\nTARGET_COLS = ['winner_model_a', 'winner_model_b', 'winner_tie']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:43:13.068773Z","iopub.execute_input":"2025-06-09T02:43:13.069031Z","iopub.status.idle":"2025-06-09T02:43:14.716893Z","shell.execute_reply.started":"2025-06-09T02:43:13.069005Z","shell.execute_reply":"2025-06-09T02:43:14.71615Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Saat melakukan preprocessing mungkin anda menemui masalah dengan unicodenya jadi perlu fallback untuk kembali ke ASCII","metadata":{}},{"cell_type":"code","source":"# --- Enhanced Preprocessing ---\nprint(\"Starting preprocessing...\")\n\ndef clean_json_like_string(text):\n    \"\"\"\n    Cleans text that looks like a JSON array of strings.\n    Example: \"[\\\"hello\\\", \\\"world\\\"]\" -> \"hello [SEP] world\"\n    \"\"\"\n    if isinstance(text, str) and text.startswith('[') and text.endswith(']'):\n        try:\n            text_list = ast.literal_eval(text)\n            if isinstance(text_list, list):\n                # Use separator to maintain structure info\n                return \" [SEP] \".join(str(item) for item in text_list)\n        except (ValueError, SyntaxError):\n            return text\n    return text\n\ndef robust_text_cleaning(text):\n    \"\"\"\n    Robust text cleaning to handle Unicode issues and various text problems.\n    \"\"\"\n    if pd.isna(text) or text is None:\n        return \"\"\n    \n    text = str(text)\n    \n    # Remove or replace problematic Unicode characters\n    try:\n        # First, try to normalize Unicode\n        text = unicodedata.normalize('NFKD', text)\n        \n        # Remove surrogate characters and other problematic Unicode\n        text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n        \n        # Remove control characters except common whitespace\n        text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C' or char in '\\t\\n\\r ')\n        \n        # Fix common escape sequences\n        text = re.sub(r'\\\\n', '\\n', text)\n        text = re.sub(r'\\\\t', '\\t', text)\n        \n        # Replace multiple whitespace with single space\n        text = re.sub(r'\\s+', ' ', text)\n        \n        # Strip leading/trailing whitespace\n        text = text.strip()\n        \n        # Ensure we have a valid string\n        if not text:\n            return \"\"\n            \n        # Final encoding check\n        text.encode('utf-8')\n        \n    except (UnicodeError, UnicodeDecodeError, UnicodeEncodeError) as e:\n        print(f\"Unicode error encountered, applying fallback cleaning: {e}\")\n        # Fallback: keep only ASCII characters\n        text = ''.join(char for char in text if ord(char) < 128)\n        text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:43:14.717678Z","iopub.execute_input":"2025-06-09T02:43:14.717894Z","iopub.status.idle":"2025-06-09T02:43:14.725598Z","shell.execute_reply.started":"2025-06-09T02:43:14.717868Z","shell.execute_reply":"2025-06-09T02:43:14.725018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Feature engineering ini telah disesuaikan dengan fitur importance yang dijelaskan di akhir notebook.","metadata":{}},{"cell_type":"code","source":"# --- BERT Setup ---\nBERT_PATH = '../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased' \n# thanks mr.saurav\nprint(f\"Loading BERT model and tokenizer from local path: {BERT_PATH}...\")\ntry:\n    # Memuat tokenizer dari path lokal\n    tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n    # Memuat model dari path lokal\n    bert_model = BertModel.from_pretrained(BERT_PATH)\n    print(\"BERT model and tokenizer loaded successfully from local path.\")\nexcept Exception as e:\n    print(f\"Error loading BERT model from local path: {e}. BERT features will not be available.\")\n    tokenizer = None\n    bert_model = None\n\n# Pindahkan model ke GPU jika tersedia (jaga-jaga kalau habis komputasinya wkwk)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif bert_model:\n    bert_model.to(device)\n    bert_model.eval() # Set model ke mode evaluasi\nprint(f\"BERT model on: {device}\")\n\ndef get_bert_embeddings(texts, batch_size=16):\n    \"\"\"\n    Generates BERT embeddings for a list of texts.\n    Uses mean pooling of last hidden state.\n    \"\"\"\n    if not bert_model or not tokenizer:\n        print(\"BERT model not loaded, returning zero embeddings.\")\n        # Return zero embeddings of expected dimension (768 for bert-base-uncased)\n        return np.zeros((len(texts), 768))\n\n    all_embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"BERT Embedding\"):\n        batch_texts = texts[i:i+batch_size]\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n        with torch.no_grad():\n            outputs = bert_model(**inputs)\n        \n        # Mean pooling: take the average of the last hidden state, ignoring padding tokens\n        last_hidden_states = outputs.last_hidden_state\n        attention_mask = inputs['attention_mask']\n        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n        sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9) # Avoid division by zero\n        mean_embeddings = sum_embeddings / sum_mask\n        all_embeddings.append(mean_embeddings.cpu().numpy())\n        \n    return np.concatenate(all_embeddings, axis=0)\n    \ndef extract_bert_features(df):\n    \"\"\"Extracts BERT-based features\"\"\"\n    if not bert_model or not tokenizer:\n        print(\"BERT model not available. Skipping BERT feature extraction.\")\n        return pd.DataFrame()\n\n    print(\"  - Extracting BERT embeddings (this may take a while)...\")\n    features = pd.DataFrame()\n\n    # Get embeddings for prompt, response_a, response_b\n    print(\"    - Processing prompts...\")\n    prompt_embeddings = get_bert_embeddings(df['prompt'].tolist())\n    print(\"    - Processing response_a...\")\n    response_a_embeddings = get_bert_embeddings(df['response_a'].tolist())\n    print(\"    - Processing response_b...\")\n    response_b_embeddings = get_bert_embeddings(df['response_b'].tolist())\n\n    bert_embedding_dim = prompt_embeddings.shape[1]\n\n    # Features based on differences and products (often more useful for comparison)\n    diff_a_b = response_a_embeddings - response_b_embeddings\n    prod_a_b = response_a_embeddings * response_b_embeddings # Element-wise product\n\n    for i in range(bert_embedding_dim):\n        features[f'bert_diff_a_b_{i}'] = diff_a_b[:, i]\n        features[f'bert_prod_a_b_{i}'] = prod_a_b[:, i]\n\n    # Cosine similarities\n    def row_cosine_similarity(u, v):\n        # u, v are 2D arrays (N_samples, N_features)\n        # Calculate cosine similarity row-wise\n        numerator = np.sum(u * v, axis=1)\n        denominator = np.linalg.norm(u, axis=1) * np.linalg.norm(v, axis=1)\n        # Handle potential division by zero if a vector is all zeros\n        similarity = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n        return similarity\n\n    features['bert_sim_prompt_a'] = row_cosine_similarity(prompt_embeddings, response_a_embeddings)\n    features['bert_sim_prompt_b'] = row_cosine_similarity(prompt_embeddings, response_b_embeddings)\n    features['bert_sim_a_b'] = row_cosine_similarity(response_a_embeddings, response_b_embeddings)\n    \n    # Difference in similarities\n    features['bert_sim_prompt_diff_a_b'] = features['bert_sim_prompt_a'] - features['bert_sim_prompt_b']\n\n    print(f\"  - Created {features.shape[1]} BERT-based features\")\n    return features\n\n# --- Feature Engineering Functions ---\ndef extract_length_features(df):\n    \"\"\"Extract length-based features that are highly predictive for preference tasks\"\"\"\n    features = pd.DataFrame()\n    \n    # Basic lengths\n    features['prompt_length'] = df['prompt'].str.len()\n    features['response_a_length'] = df['response_a'].str.len()\n    features['response_b_length'] = df['response_b'].str.len()\n    \n    # Length ratios and differences (very important for preference)\n    features['len_ratio_a_b'] = features['response_a_length'] / (features['response_b_length'] + 1)\n    features['len_ratio_b_a'] = features['response_b_length'] / (features['response_a_length'] + 1)\n    features['len_diff_a_b'] = features['response_a_length'] - features['response_b_length']\n    features['len_diff_abs'] = np.abs(features['len_diff_a_b'])\n    \n    # Relative to prompt\n    features['response_a_to_prompt_ratio'] = features['response_a_length'] / (features['prompt_length'] + 1)\n    features['response_b_to_prompt_ratio'] = features['response_b_length'] / (features['prompt_length'] + 1)\n    \n    return features\n\ndef extract_structure_features(df):\n    \"\"\"Extract structural features that indicate response quality\"\"\"\n    features = pd.DataFrame()\n    \n    # Line and paragraph structure\n    features['response_a_lines'] = df['response_a'].str.count('\\n')\n    features['response_b_lines'] = df['response_b'].str.count('\\n')\n    features['response_a_paragraphs'] = df['response_a'].str.count('\\n\\n') + 1\n    features['response_b_paragraphs'] = df['response_b'].str.count('\\n\\n') + 1\n    \n    # Sentence structure\n    features['response_a_sentences'] = df['response_a'].str.count(r'[.!?]+')\n    features['response_b_sentences'] = df['response_b'].str.count(r'[.!?]+')\n    \n    # List structure (often indicates organized response)\n    features['response_a_lists'] = df['response_a'].str.count(r'^\\s*[-*•]\\s')\n    features['response_b_lists'] = df['response_b'].str.count(r'^\\s*[-*•]\\s')\n    features['response_a_numbered'] = df['response_a'].str.count(r'^\\s*\\d+\\.\\s')\n    features['response_b_numbered'] = df['response_b'].str.count(r'^\\s*\\d+\\.\\s')\n    \n    # Structure ratios\n    features['lines_ratio_a_b'] = (features['response_a_lines'] + 1) / (features['response_b_lines'] + 1)\n    features['paragraphs_ratio_a_b'] = features['response_a_paragraphs'] / (features['response_b_paragraphs'] + 1)\n    \n    return features\n\ndef extract_content_features(df):\n    \"\"\"Extract content-based features that indicate response quality\"\"\"\n    features = pd.DataFrame()\n    \n    # Question marks (engagement)\n    features['response_a_questions'] = df['response_a'].str.count(r'\\?')\n    features['response_b_questions'] = df['response_b'].str.count(r'\\?')\n    \n    # Numbers and specificity\n    features['response_a_numbers'] = df['response_a'].str.count(r'\\d+')\n    features['response_b_numbers'] = df['response_b'].str.count(r'\\d+')\n    \n    # URLs and references\n    features['response_a_urls'] = df['response_a'].str.count(r'http[s]?://')\n    features['response_b_urls'] = df['response_b'].str.count(r'http[s]?://')\n    \n    # Quotations (citations)\n    features['response_a_quotes'] = df['response_a'].str.count(r'\"[^\"]*\"')\n    features['response_b_quotes'] = df['response_b'].str.count(r'\"[^\"]*\"')\n    \n    # Capital letters (might indicate emphasis or poor formatting)\n    features['response_a_caps_ratio'] = df['response_a'].str.count(r'[A-Z]') / (df['response_a'].str.len() + 1)\n    features['response_b_caps_ratio'] = df['response_b'].str.count(r'[A-Z]') / (df['response_b'].str.len() + 1)\n    \n    # Exclamation marks (enthusiasm vs professionalism)\n    features['response_a_exclamations'] = df['response_a'].str.count(r'!')\n    features['response_b_exclamations'] = df['response_b'].str.count(r'!')\n    \n    return features\n\ndef extract_bias_features(df):\n    \"\"\"Extract features that help mitigate known biases\"\"\"\n    features = pd.DataFrame()\n    \n    # Position bias indicators\n    features['a_longer_than_b'] = (df['response_a'].str.len() > df['response_b'].str.len()).astype(int)\n    features['b_longer_than_a'] = (df['response_b'].str.len() > df['response_a'].str.len()).astype(int)\n    \n    # Verbosity bias\n    features['extreme_length_diff'] = (np.abs(df['response_a'].str.len() - df['response_b'].str.len()) > 1000).astype(int)\n    \n    # Confidence indicators\n    confidence_words = r'(?i)\\b(definitely|certainly|absolutely|clearly|obviously|undoubtedly)\\b'\n    features['response_a_confidence'] = df['response_a'].str.count(confidence_words)\n    features['response_b_confidence'] = df['response_b'].str.count(confidence_words)\n    \n    # Politeness indicators\n    polite_words = r'(?i)\\b(please|thank you|sorry|excuse me|would you|could you)\\b'\n    features['response_a_politeness'] = df['response_a'].str.count(polite_words)\n    features['response_b_politeness'] = df['response_b'].str.count(polite_words)\n    \n    return features\n\n# --- Semantic Features ---\ndef extract_semantic_features(df):\n    \"\"\"Extract advanced semantic features that indicate response quality and relevance\"\"\"\n    features = pd.DataFrame()\n    \n    print(\"  - Extracting code and technical content features...\")\n    # Code blocks and technical content\n    features['response_a_code_blocks'] = df['response_a'].str.count(r'```|`[^`\\n]+`')\n    features['response_b_code_blocks'] = df['response_b'].str.count(r'```|`[^`\\n]+`')\n    features['response_a_code_inline'] = df['response_a'].str.count(r'`[^`\\n]+`')\n    features['response_b_code_inline'] = df['response_b'].str.count(r'`[^`\\n]+`')\n    \n    print(\"  - Extracting mathematical expressions...\")\n    # Mathematical expressions and formulas\n    features['response_a_math'] = df['response_a'].str.count(r'\\$[^$\\n]+\\$|\\\\\\([^)]+\\\\\\)|\\\\begin\\{|\\\\end\\{')\n    features['response_b_math'] = df['response_b'].str.count(r'\\$[^$\\n]+\\$|\\\\\\([^)]+\\\\\\)|\\\\begin\\{|\\\\end\\{')\n    \n    print(\"  - Extracting step-by-step indicators...\")\n    # Step-by-step explanations\n    step_patterns = r'(?i)\\b(step \\d+|first[ly]*|second[ly]*|third[ly]*|fourth[ly]*|fifth[ly]*|finally|lastly|next|then)\\b'\n    features['response_a_steps'] = df['response_a'].str.count(step_patterns)\n    features['response_b_steps'] = df['response_b'].str.count(step_patterns)\n    \n    print(\"  - Extracting uncertainty and confidence markers...\")\n    # Uncertainty vs confidence indicators\n    uncertainty_words = r'(?i)\\b(maybe|perhaps|might|could be|uncertain|not sure|possibly|probably|likely)\\b'\n    features['response_a_uncertainty'] = df['response_a'].str.count(uncertainty_words)\n    features['response_b_uncertainty'] = df['response_b'].str.count(uncertainty_words)\n    \n    # Strong assertion words\n    assertion_words = r'(?i)\\b(always|never|must|will|cannot|impossible|guaranteed|proven|fact)\\b'\n    features['response_a_assertions'] = df['response_a'].str.count(assertion_words)\n    features['response_b_assertions'] = df['response_b'].str.count(assertion_words)\n    \n    print(\"  - Extracting educational and explanatory features...\")\n    # Educational/explanatory patterns\n    explanation_words = r'(?i)\\b(because|since|therefore|thus|hence|as a result|due to|explained|example|for instance)\\b'\n    features['response_a_explanations'] = df['response_a'].str.count(explanation_words)\n    features['response_b_explanations'] = df['response_b'].str.count(explanation_words)\n    \n    # Question answering patterns\n    answer_patterns = r'(?i)\\b(answer|solution|result|conclusion|summary|in summary|to summarize)\\b'\n    features['response_a_answers'] = df['response_a'].str.count(answer_patterns)\n    features['response_b_answers'] = df['response_b'].str.count(answer_patterns)\n    \n    print(\"  - Extracting formatting and presentation features...\")\n    # Formatting quality indicators\n    features['response_a_headers'] = df['response_a'].str.count(r'^#+\\s', flags=re.MULTILINE)\n    features['response_b_headers'] = df['response_b'].str.count(r'^#+\\s', flags=re.MULTILINE)\n    features['response_a_bold'] = df['response_a'].str.count(r'\\*\\*[^*]+\\*\\*|__[^_]+__')\n    features['response_b_bold'] = df['response_b'].str.count(r'\\*\\*[^*]+\\*\\*|__[^_]+__')\n    features['response_a_italic'] = df['response_a'].str.count(r'\\*[^*]+\\*|_[^_]+_')\n    features['response_b_italic'] = df['response_b'].str.count(r'\\*[^*]+\\*|_[^_]+_')\n    \n    print(\"  - Extracting conversation and interaction features...\")\n    # Conversational elements\n    greeting_words = r'(?i)\\b(hello|hi|hey|good morning|good afternoon|good evening)\\b'\n    features['response_a_greetings'] = df['response_a'].str.count(greeting_words)\n    features['response_b_greetings'] = df['response_b'].str.count(greeting_words)\n    \n    # Direct addressing\n    address_words = r'(?i)\\b(you|your|yourself)\\b'\n    features['response_a_direct_address'] = df['response_a'].str.count(address_words)\n    features['response_b_direct_address'] = df['response_b'].str.count(address_words)\n    \n    print(\"  - Extracting domain-specific features...\")\n    # Domain-specific indicators\n    technical_words = r'(?i)\\b(algorithm|function|method|process|system|data|analysis|implementation)\\b'\n    features['response_a_technical'] = df['response_a'].str.count(technical_words)\n    features['response_b_technical'] = df['response_b'].str.count(technical_words)\n    \n    # Creative/subjective words\n    creative_words = r'(?i)\\b(beautiful|amazing|wonderful|creative|artistic|inspiring|emotional)\\b'\n    features['response_a_creative'] = df['response_a'].str.count(creative_words)\n    features['response_b_creative'] = df['response_b'].str.count(creative_words)\n    \n    print(\"  - Computing semantic ratios and differences...\")\n    # Compute ratios and differences for key semantic features\n    semantic_keys = ['code_blocks', 'math', 'steps', 'explanations', 'technical']\n    for key in semantic_keys:\n        col_a = f'response_a_{key}'\n        col_b = f'response_b_{key}'\n        if col_a in features.columns and col_b in features.columns:\n            features[f'{key}_ratio_a_b'] = (features[col_a] + 1) / (features[col_b] + 1)\n            features[f'{key}_diff_a_b'] = features[col_a] - features[col_b]\n    \n    print(f\"  - Created {features.shape[1]} semantic features\")\n    return features\n\n# --- Word overlap and similarity features ---\ndef extract_similarity_features(df):\n    \"\"\"Extract features based on word overlap and text similarity\"\"\"\n    features = pd.DataFrame()\n    \n    print(\"  - Computing word overlap features...\")\n    \n    def word_overlap_ratio(text1, text2):\n        \"\"\"Compute word overlap ratio between two texts\"\"\"\n        if pd.isna(text1) or pd.isna(text2) or text1 == \"\" or text2 == \"\":\n            return 0.0\n        \n        words1 = set(str(text1).lower().split())\n        words2 = set(str(text2).lower().split())\n        \n        if len(words1) == 0 and len(words2) == 0:\n            return 1.0\n        if len(words1) == 0 or len(words2) == 0:\n            return 0.0\n            \n        overlap = len(words1.intersection(words2))\n        return overlap / len(words1.union(words2))\n    \n    # Word overlap between prompt and responses\n    features['prompt_response_a_overlap'] = [\n        word_overlap_ratio(p, a) for p, a in zip(df['prompt'], df['response_a'])\n    ]\n    features['prompt_response_b_overlap'] = [\n        word_overlap_ratio(p, b) for p, b in zip(df['prompt'], df['response_b'])\n    ]\n    \n    # Word overlap between responses\n    features['response_a_b_overlap'] = [\n        word_overlap_ratio(a, b) for a, b in zip(df['response_a'], df['response_b'])\n    ]\n    \n    # Unique word ratios\n    def unique_word_ratio(text1, text2):\n        \"\"\"Compute ratio of unique words in text1 vs text2\"\"\"\n        if pd.isna(text1) or pd.isna(text2) or text1 == \"\" or text2 == \"\":\n            return 0.5\n            \n        words1 = set(str(text1).lower().split())\n        words2 = set(str(text2).lower().split())\n        \n        if len(words1) == 0:\n            return 0.0\n        \n        unique_in_1 = len(words1 - words2)\n        return unique_in_1 / len(words1)\n    \n    features['response_a_unique_ratio'] = [\n        unique_word_ratio(a, b) for a, b in zip(df['response_a'], df['response_b'])\n    ]\n    features['response_b_unique_ratio'] = [\n        unique_word_ratio(b, a) for a, b in zip(df['response_a'], df['response_b'])\n    ]\n    \n    print(f\"  - Created {features.shape[1]} similarity features\")\n    return features\n \ndef extract_advanced_text_features(df):\n    \"\"\"\n    Extract advanced text features including TF-IDF, semantic similarity, and text quality metrics\n    \"\"\"\n    print(\"Extracting advanced text features...\")\n    features = pd.DataFrame()\n    \n    # === 1. TF-IDF Features ===\n    #print(\"  - Computing TF-IDF features...\")\n    \n    # Combine all text for TF-IDF\n    #all_texts = []\n    #for idx in range(len(df)):\n    #    prompt = str(df.iloc[idx]['prompt'])\n    #    resp_a = str(df.iloc[idx]['response_a'])\n     #   resp_b = str(df.iloc[idx]['response_b'])\n      #  combined = f\"{prompt} [SEP] {resp_a} [SEP] {resp_b}\"\n      #  all_texts.append(combined)\n    \n    # TF-IDF Vectorizer with optimized parameters\n    #tfidf = TfidfVectorizer(\n     #   max_features=3000,  # Reduced for memory efficiency\n     #   ngram_range=(1, 2),  # Unigrams and bigrams\n     #   stop_words='english',\n    #    min_df=2,  # Minimum document frequency\n     #   max_df=0.95,  # Maximum document frequency\n      #  lowercase=True,\n     #   strip_accents='unicode'\n   # )\n    \n   # try:\n   #     tfidf_matrix = tfidf.fit_transform(all_texts)\n        \n        # Use SVD for dimensionality reduction\n    #    svd = TruncatedSVD(n_components=50, random_state=42)\n    #    tfidf_reduced = svd.fit_transform(tfidf_matrix)\n        \n        # Add TF-IDF features\n    #    for i in range(tfidf_reduced.shape[1]):\n    #        features[f'tfidf_svd_{i}'] = tfidf_reduced[:, i]\n   #     \n    #    print(f\"    Created {tfidf_reduced.shape[1]} TF-IDF SVD features\")\n        \n    #except Exception as e:\n    #    print(f\"    TF-IDF extraction failed: {e}\")\n        # Add dummy features to maintain consistency\n  #      for i in range(50):\n    #        features[f'tfidf_svd_{i}'] = 0.0\n    \n    # === 2. Response-Specific TF-IDF Similarity ===\n   # print(\"  - Computing TF-IDF response similarities...\")\n    \n  #  try:\n        # Separate TF-IDF for responses only\n     #   response_texts = []\n    #    for idx in range(len(df)):\n      #      resp_a = str(df.iloc[idx]['response_a'])\n      #      resp_b = str(df.iloc[idx]['response_b'])\n      #      response_texts.extend([resp_a, resp_b])\n      #  \n      #  resp_tfidf = TfidfVectorizer(\n        #    max_features=1000,\n       #     ngram_range=(1, 2),\n       #     stop_words='english',\n       #     min_df=2\n      #  )\n        \n       # resp_tfidf_matrix = resp_tfidf.fit_transform(response_texts)\n       # \n        # Compute similarities\n      #  similarities = []\n      #  for i in range(0, len(response_texts), 2):\n      #      if i + 1 < len(response_texts):\n       #         sim = cosine_similarity(\n             #       resp_tfidf_matrix[i:i+1], \n         #           resp_tfidf_matrix[i+1:i+2]\n        #        )[0, 0]\n       #         similarities.append(sim)\n       #     else:\n       #         similarities.append(0.0)\n        \n     #   features['tfidf_response_similarity'] = similarities\n    #    \n        # Prompt-Response TF-IDF similarities\n    #    prompt_response_sims_a = []\n      #  prompt_response_sims_b = []\n        \n       # for idx in range(len(df)):\n       #     prompt = str(df.iloc[idx]['prompt'])\n       #     resp_a = str(df.iloc[idx]['response_a'])\n        #    resp_b = str(df.iloc[idx]['response_b'])\n            \n         #   try:\n        #        temp_texts = [prompt, resp_a, resp_b]\n          #      temp_tfidf = TfidfVectorizer(stop_words='english', min_df=1)\n           #     temp_matrix = temp_tfidf.fit_transform(temp_texts)\n                \n           #     sim_a = cosine_similarity(temp_matrix[0:1], temp_matrix[1:2])[0, 0]\n           #     sim_b = cosine_similarity(temp_matrix[0:1], temp_matrix[2:3])[0, 0]\n                \n          #      prompt_response_sims_a.append(sim_a)\n         #       prompt_response_sims_b.append(sim_b)\n                \n       #     except:\n          #      prompt_response_sims_a.append(0.0)\n          #      prompt_response_sims_b.append(0.0)\n        \n       # features['tfidf_prompt_response_a_sim'] = prompt_response_sims_a\n       # features['tfidf_prompt_response_b_sim'] = prompt_response_sims_b\n     #   features['tfidf_prompt_response_diff'] = np.array(prompt_response_sims_a) - np.array(prompt_response_sims_b)\n        \n     #   print(f\"    Created TF-IDF similarity features\")\n        \n#    except Exception as e:\n  #      print(f\"    TF-IDF similarity extraction failed: {e}\")\n #       features['tfidf_response_similarity'] = 0.0\n     #   features['tfidf_prompt_response_a_sim'] = 0.0\n   #     features['tfidf_prompt_response_b_sim'] = 0.0\n     #   features['tfidf_prompt_response_diff'] = 0.0\n    \n    # === 3. Advanced Text Quality Features ===\n    print(\"  - Computing text quality features...\")\n    \n    def compute_text_quality(text):\n        \"\"\"Compute various text quality metrics\"\"\"\n        if pd.isna(text) or text == \"\":\n            return {\n                'avg_word_length': 0,\n                'avg_sentence_length': 0,\n                'vocabulary_richness': 0,\n                'punctuation_ratio': 0,\n                'uppercase_ratio': 0\n            }\n        \n        text = str(text)\n        words = text.split()\n        sentences = re.split(r'[.!?]+', text)\n        sentences = [s.strip() for s in sentences if s.strip()]\n        \n        # Average word length\n        avg_word_len = np.mean([len(word) for word in words]) if words else 0\n        \n        # Average sentence length\n        avg_sent_len = np.mean([len(sent.split()) for sent in sentences]) if sentences else 0\n        \n        # Vocabulary richness (unique words / total words)\n        vocab_richness = len(set(words)) / len(words) if words else 0\n        \n        # Punctuation ratio\n        punct_count = len(re.findall(r'[.,;:!?]', text))\n        punct_ratio = punct_count / len(text) if text else 0\n        \n        # Uppercase ratio\n        upper_count = sum(1 for c in text if c.isupper())\n        upper_ratio = upper_count / len(text) if text else 0\n        \n        return {\n            'avg_word_length': avg_word_len,\n            'avg_sentence_length': avg_sent_len,\n            'vocabulary_richness': vocab_richness,\n            'punctuation_ratio': punct_ratio,\n            'uppercase_ratio': upper_ratio\n        }\n    \n    # Apply quality metrics\n    for col in ['response_a', 'response_b']:\n        quality_metrics = df[col].apply(compute_text_quality)\n        \n        for metric in ['avg_word_length', 'avg_sentence_length', 'vocabulary_richness', \n                      'punctuation_ratio', 'uppercase_ratio']:\n            features[f'{col}_{metric}'] = [m[metric] for m in quality_metrics]\n    \n    # Quality comparison features\n    features['vocab_richness_diff'] = features['response_a_vocabulary_richness'] - features['response_b_vocabulary_richness']\n    features['word_length_diff'] = features['response_a_avg_word_length'] - features['response_b_avg_word_length']\n    features['sentence_length_diff'] = features['response_a_avg_sentence_length'] - features['response_b_avg_sentence_length']\n    \n    # === 4. N-gram Analysis ===\n    print(\"  - Computing n-gram features...\")\n    \n    def extract_ngram_features(text, n=2):\n        \"\"\"Extract n-gram based features\"\"\"\n        if pd.isna(text) or text == \"\":\n            return 0, 0\n        \n        words = str(text).lower().split()\n        if len(words) < n:\n            return 0, 0\n        \n        ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n        ngram_counts = Counter(ngrams)\n        \n        # Most common n-gram frequency\n        max_freq = max(ngram_counts.values()) if ngram_counts else 0\n        # Unique n-grams ratio\n        unique_ratio = len(ngram_counts) / len(ngrams) if ngrams else 0\n        \n        return max_freq, unique_ratio\n    \n    # Bigram features\n    for col in ['response_a', 'response_b']:\n        bigram_data = df[col].apply(lambda x: extract_ngram_features(x, 2))\n        features[f'{col}_bigram_max_freq'] = [x[0] for x in bigram_data]\n        features[f'{col}_bigram_unique_ratio'] = [x[1] for x in bigram_data]\n    \n    # === 5. Readability Features ===\n    print(\"  - Computing readability features...\")\n    \n    def simple_readability_score(text):\n        \"\"\"Simple readability score based on sentence and word length\"\"\"\n        if pd.isna(text) or text == \"\":\n            return 0\n        \n        text = str(text)\n        sentences = len(re.findall(r'[.!?]+', text))\n        words = len(text.split())\n        \n        if sentences == 0 or words == 0:\n            return 0\n        \n        # Simple approximation of readability\n        avg_words_per_sentence = words / sentences\n        avg_chars_per_word = len(text.replace(' ', '')) / words\n        \n        # Lower score = more readable\n        readability = avg_words_per_sentence * 0.5 + avg_chars_per_word * 2\n        return readability\n    \n    features['response_a_readability'] = df['response_a'].apply(simple_readability_score)\n    features['response_b_readability'] = df['response_b'].apply(simple_readability_score)\n    features['readability_diff'] = features['response_a_readability'] - features['response_b_readability']\n    \n    # === 6. Semantic Coherence Features ===\n    print(\"  - Computing semantic coherence features...\")\n    \n    def compute_coherence(text):\n        \"\"\"Compute text coherence based on word repetition and structure\"\"\"\n        if pd.isna(text) or text == \"\":\n            return 0\n        \n        text = str(text).lower()\n        words = text.split()\n        \n        if len(words) < 2:\n            return 0\n        \n        # Word repetition score\n        word_counts = Counter(words)\n        repetition_score = sum(count for count in word_counts.values() if count > 1)\n        repetition_ratio = repetition_score / len(words)\n        \n        return repetition_ratio\n    \n    features['response_a_coherence'] = df['response_a'].apply(compute_coherence)\n    features['response_b_coherence'] = df['response_b'].apply(compute_coherence)\n    features['coherence_diff'] = features['response_a_coherence'] - features['response_b_coherence']\n    \n    print(f\"  - Created {features.shape[1]} advanced text features\")\n    return features\n\ndef extract_response_quality_features(df):\n    \"\"\"\n    Extract features that specifically measure response quality and appropriateness\n    \"\"\"\n    print(\"Extracting response quality features...\")\n    features = pd.DataFrame()\n    \n    # === Response Completeness Features ===\n    print(\"  - Computing response completeness...\")\n    \n    def is_complete_response(text):\n        \"\"\"Check if response seems complete\"\"\"\n        if pd.isna(text) or text == \"\":\n            return 0\n        \n        text = str(text)\n        \n        # Check for completion indicators\n        completion_indicators = [\n            r'(?i)\\b(in conclusion|to conclude|finally|in summary|overall)\\b',\n            r'(?i)\\b(hope this helps|let me know|feel free to ask)\\b',\n            r'[.!]$',  # Ends with proper punctuation\n        ]\n        \n        score = 0\n        for pattern in completion_indicators:\n            if re.search(pattern, text):\n                score += 1\n        \n        # Check if response seems cut off\n        cutoff_indicators = [\n            r'(?i)\\b(continued|more on this|as mentioned)\\s*$',\n            r'[,:]$',  # Ends with comma or colon\n        ]\n        \n        for pattern in cutoff_indicators:\n            if re.search(pattern, text):\n                score -= 1\n        \n        return max(0, score)\n    \n    features['response_a_completeness'] = df['response_a'].apply(is_complete_response)\n    features['response_b_completeness'] = df['response_b'].apply(is_complete_response)\n    features['completeness_diff'] = features['response_a_completeness'] - features['response_b_completeness']\n    \n    # === Response Helpfulness Indicators ===\n    print(\"  - Computing helpfulness indicators...\")\n    \n    helpful_patterns = [\n        r'(?i)\\b(example|for instance|such as|like this)\\b',\n        r'(?i)\\b(step|steps|first|second|next|then)\\b',\n        r'(?i)\\b(you can|you should|try|consider)\\b',\n        r'(?i)\\b(here|this|these|solution|answer)\\b'\n    ]\n    \n    for col in ['response_a', 'response_b']:\n        helpfulness_score = df[col].apply(\n            lambda x: sum(len(re.findall(pattern, str(x))) for pattern in helpful_patterns)\n        )\n        features[f'{col}_helpfulness'] = helpfulness_score\n    \n    features['helpfulness_diff'] = features['response_a_helpfulness'] - features['response_b_helpfulness']\n    \n    # === Response Specificity ===\n    print(\"  - Computing response specificity...\")\n    \n    def compute_specificity(text):\n        \"\"\"Compute how specific/detailed the response is\"\"\"\n        if pd.isna(text) or text == \"\":\n            return 0\n        \n        text = str(text)\n        \n        # Count specific indicators\n        numbers = len(re.findall(r'\\d+', text))\n        proper_nouns = len(re.findall(r'\\b[A-Z][a-z]+\\b', text))\n        technical_terms = len(re.findall(r'(?i)\\b(algorithm|method|process|system|function|parameter|variable)\\b', text))\n        citations = len(re.findall(r'(?i)\\b(according to|research shows|study|paper|source)\\b', text))\n        \n        specificity_score = numbers * 0.5 + proper_nouns * 0.3 + technical_terms * 0.8 + citations * 1.2\n        return specificity_score\n    \n    features['response_a_specificity'] = df['response_a'].apply(compute_specificity)\n    features['response_b_specificity'] = df['response_b'].apply(compute_specificity)\n    features['specificity_diff'] = features['response_a_specificity'] - features['response_b_specificity']\n    \n    print(f\"  - Created {features.shape[1]} response quality features\")\n    return features\n    \ndef preprocess_data(df, is_train=True):\n    df_copy = df.copy()\n    \n    # Clean text columns with robust cleaning\n    text_cols_to_clean = ['prompt', 'response_a', 'response_b']\n    for col in text_cols_to_clean:\n        if col in df_copy.columns:\n            print(f\"Cleaning column: {col}\")\n            # First apply JSON-like cleaning\n            df_copy[col] = df_copy[col].apply(clean_json_like_string)\n            # Then apply robust Unicode cleaning\n            df_copy[col] = df_copy[col].apply(robust_text_cleaning)\n            \n            # Additional validation\n            print(f\"  - Column {col}: {df_copy[col].isna().sum()} NaN values\")\n            print(f\"  - Column {col}: {(df_copy[col] == '').sum()} empty strings\")\n\n    if is_train:\n        # Create a single target column for stratification and CatBoost MultiClass\n        conditions = [\n            df_copy['winner_model_a'] == 1,\n            df_copy['winner_model_b'] == 1,\n            df_copy['winner_tie'] == 1\n        ]\n        choices = [0, 1, 2]\n        df_copy['target'] = np.select(conditions, choices, default=-1)\n        \n        # Verify target creation\n        print(\"Target distribution:\")\n        print(df_copy['target'].value_counts(normalize=True))\n        if (df_copy['target'] == -1).any():\n            print(\"Warning: Some rows could not be mapped to a target class!\")\n            print(f\"Number of unmapped rows: {(df_copy['target'] == -1).sum()}\")\n    \n    # --- Extract all engineered features (for both train and test) ---\n    print(\"Extracting engineered features...\")\n\n    length_features = extract_length_features(df_copy)\n    structure_features = extract_structure_features(df_copy)\n    content_features = extract_content_features(df_copy)\n    bias_features = extract_bias_features(df_copy)\n\n    # --- Extract semantic and similarity features ---\n    print(\"Extracting semantic features...\")\n    semantic_features = extract_semantic_features(df_copy)\n\n    print(\"Extracting similarity features...\")\n    similarity_features = extract_similarity_features(df_copy)\n\n    print(\"Extracting advanced text features...\")\n    advanced_text_features = extract_advanced_text_features(df_copy)\n\n    print(\"Extracting response quality features...\")\n    quality_features = extract_response_quality_features(df_copy)\n\n    # --- Extract BERT features ---\n    bert_derived_features = extract_bert_features(df_copy)\n    \n\n    # Combine all features\n    all_feature_dfs = [\n        length_features, \n        structure_features, \n        content_features, \n        bias_features,\n        semantic_features,\n        similarity_features,\n        advanced_text_features,\n        quality_features\n    ]\n    if not bert_derived_features.empty: # Only add if BERT features were generated\n        all_feature_dfs.append(bert_derived_features)\n\n    engineered_features = pd.concat(all_feature_dfs, axis=1)\n    \n    print(f\"Created {engineered_features.shape[1]} total engineered features\")\n    \n    df_copy = pd.concat([df_copy, engineered_features], axis=1)\n    \n    return df_copy\n    \ntrain_processed_df = preprocess_data(train_df, is_train=True)\ntest_processed_df = preprocess_data(test_df, is_train=False)\n\n# --- Updated Feature Selection ---\n# Combine text features with engineered features\ntext_features_cols = ['prompt', 'response_a', 'response_b']\n\n# Get engineered feature column names\nengineered_cols = [col for col in train_processed_df.columns \n                  if col not in ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', \n                               'winner_model_a', 'winner_model_b', 'winner_tie', 'target']]\n\nprint(f\"Text features: {len(text_features_cols)}\")\nprint(f\"Engineered features: {len(engineered_cols)}\")\nprint(f\"Total features for training: {len(text_features_cols) + len(engineered_cols)}\")\n\n# Prepare feature sets\nX_text = train_processed_df[text_features_cols]\nX_engineered = train_processed_df[engineered_cols]\ny = train_processed_df['target']\n\nX_test_text = test_processed_df[text_features_cols]\nX_test_engineered = test_processed_df[engineered_cols]\n\nprint(f\"X_text shape: {X_text.shape}\")\nprint(f\"X_engineered shape: {X_engineered.shape}\")\nprint(f\"y shape: {y.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:43:14.727154Z","iopub.execute_input":"2025-06-09T02:43:14.727584Z","iopub.status.idle":"2025-06-09T03:41:56.169455Z","shell.execute_reply.started":"2025-06-09T02:43:14.727565Z","shell.execute_reply":"2025-06-09T03:41:56.168682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Training Model Layer 1 (Stacking Catboost + XGB) ","metadata":{}},{"cell_type":"markdown","source":"Catboost dan XGB dipilih karena dalam performa model tunggal,kedua machine learning ini memiliki nilai paling baik diantara semua machine learning yang pernah saya coba pada kasus ini, selain itu catboost cocok untuk training data text classification.","metadata":{}},{"cell_type":"code","source":"# --- Lightweight Stacking Approach with CatBoost + XGBoost ---\nprint(\"\\n=== Starting Lightweight Stacking Training ===\")\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# Initialize storage for out-of-fold predictions and models\ncatboost_oof_preds = np.zeros((len(X_text), 3))\nxgb_oof_preds = np.zeros((len(X_text), 3))\ncatboost_test_preds = np.zeros((len(X_test_text), 3))\nxgb_test_preds = np.zeros((len(X_test_text), 3))\n\ncatboost_models = []\nxgb_models = []\nval_scores_catboost = []\nval_scores_xgb = []\n\n# Enhanced CatBoost Parameters \ncatboost_params = {\n    'iterations': 2200,  # Reduced for faster training in stacking\n    'learning_rate': 0.047,\n    'depth': 5,\n    'loss_function': 'MultiClass',\n    'eval_metric': 'MultiClass',\n    'random_seed': SEED,\n    'l2_leaf_reg': 6,\n    'verbose': 100,\n    'early_stopping_rounds': 200,\n    'border_count': 64,\n    'bootstrap_type': 'Bernoulli', \n    'subsample': 0.8,\n    'feature_border_type': 'GreedyLogSum',\n}\n\n# Lightweight XGBoost Parameters\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 3,\n    'eval_metric': 'mlogloss',\n    \n    # Tree Structure - Slightly deeper but still fast\n    'max_depth': 6,  # Increased from 5 to 6 for better complexity\n    'min_child_weight': 3,  # Added to prevent overfitting\n    'gamma': 0.15,  # Added min split loss for regularization\n    \n    # Learning Parameters - Balanced approach\n    'learning_rate': 0.085,  # Slightly reduced from 0.1 for better convergence\n    \n    # Sampling Parameters - More aggressive for diversity\n    'subsample': 0.85,  # Increased from 0.8\n    'colsample_bytree': 0.85,  # Increased from 0.8\n    'colsample_bylevel': 0.8,  # Added for additional feature sampling\n    'colsample_bynode': 0.8,  # Added for node-level feature sampling\n    \n    # Regularization - Fine-tuned\n    'reg_alpha': 0.6,  # Reduced L1 regularization\n    'reg_lambda': 2,  # Increased L2 regularization\n    \n    # Performance Parameters\n    'random_state': SEED,\n    'n_jobs': -1,\n    'verbosity': 1,\n    'tree_method': 'hist',  # Added for faster training\n    \n    # Advanced Parameters for Better Performance\n    'grow_policy': 'lossguide',  # More efficient tree growth\n    'max_leaves': 256,  # Limit leaves for speed while maintaining complexity\n    'scale_pos_weight': 1,  # Can be adjusted based on class imbalance\n}\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_text, y)):\n    print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n    \n    # Split data\n    X_train_text_fold = X_text.iloc[train_idx]\n    X_train_eng_fold = X_engineered.iloc[train_idx]\n    X_val_text_fold = X_text.iloc[val_idx]\n    X_val_eng_fold = X_engineered.iloc[val_idx]\n    y_train_fold = y.iloc[train_idx]\n    y_val_fold = y.iloc[val_idx]\n    \n    # Combine features\n    X_train_combined = pd.concat([X_train_text_fold, X_train_eng_fold], axis=1)\n    X_val_combined = pd.concat([X_val_text_fold, X_val_eng_fold], axis=1)\n    \n    # === TRAIN CATBOOST MODEL ===\n    print(f\"Training CatBoost for fold {fold+1}...\")\n    catboost_model = CatBoostClassifier(**catboost_params)\n    \n    try:\n        catboost_model.fit(X_train_combined, y_train_fold,\n                          eval_set=[(X_val_combined, y_val_fold)],\n                          text_features=text_features_cols,\n                          use_best_model=True)\n\n        # Get out-of-fold predictions\n        catboost_val_preds = catboost_model.predict_proba(X_val_combined)\n        catboost_oof_preds[val_idx] = catboost_val_preds\n        \n        # Evaluate CatBoost\n        catboost_fold_score = log_loss(y_val_fold, catboost_val_preds)\n        val_scores_catboost.append(catboost_fold_score)\n        catboost_models.append(catboost_model)\n        \n        print(f\"CatBoost Fold {fold+1} LogLoss: {catboost_fold_score:.4f}\")\n        \n    except Exception as e:\n        print(f\"Error in CatBoost fold {fold+1}: {e}\")\n        catboost_models.append(None)\n        val_scores_catboost.append(999)\n    \n    # === TRAIN LIGHTWEIGHT XGBOOST MODEL ===\n    print(f\"Training XGBoost for fold {fold+1}...\")\n    \n    # For XGBoost, we only use engineered features (no text features)\n    # This makes it much faster while still contributing to ensemble diversity\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=750,  # Limited iterations for speed\n        **xgb_params\n    )\n    \n    try:\n        xgb_model.fit(\n            X_train_eng_fold, y_train_fold,\n            eval_set=[(X_val_eng_fold, y_val_fold)],\n            early_stopping_rounds=100,\n            verbose=False\n        )\n        \n        # Get out-of-fold predictions\n        xgb_val_preds = xgb_model.predict_proba(X_val_eng_fold)\n        xgb_oof_preds[val_idx] = xgb_val_preds\n        \n        # Evaluate XGBoost\n        xgb_fold_score = log_loss(y_val_fold, xgb_val_preds)\n        val_scores_xgb.append(xgb_fold_score)\n        xgb_models.append(xgb_model)\n        \n        print(f\"XGBoost Fold {fold+1} LogLoss: {xgb_fold_score:.4f}\")\n        \n    except Exception as e:\n        print(f\"Error in XGBoost fold {fold+1}: {e}\")\n        xgb_models.append(None)\n        val_scores_xgb.append(999)\n    \n    # Clean up memory\n    del X_train_combined, X_val_combined\n    del X_train_text_fold, X_train_eng_fold, X_val_text_fold, X_val_eng_fold\n    del y_train_fold, y_val_fold\n    gc.collect()\n\n# === EVALUATE INDIVIDUAL MODELS ===\nprint(\"\\n=== Individual Model Performance ===\")\n\n# CatBoost performance\nif val_scores_catboost and min(val_scores_catboost) < 999:\n    catboost_mean_score = np.mean([s for s in val_scores_catboost if s < 999])\n    catboost_std_score = np.std([s for s in val_scores_catboost if s < 999])\n    catboost_oof_score = log_loss(y, catboost_oof_preds)\n    print(f\"CatBoost CV LogLoss: {catboost_mean_score:.4f} (+/- {catboost_std_score:.4f})\")\n    print(f\"CatBoost OOF LogLoss: {catboost_oof_score:.4f}\")\nelse:\n    print(\"CatBoost: No valid models trained\")\n    catboost_mean_score = 999\n\n# XGBoost performance\nif val_scores_xgb and min(val_scores_xgb) < 999:\n    xgb_mean_score = np.mean([s for s in val_scores_xgb if s < 999])\n    xgb_std_score = np.std([s for s in val_scores_xgb if s < 999])\n    xgb_oof_score = log_loss(y, xgb_oof_preds)\n    print(f\"XGBoost CV LogLoss: {xgb_mean_score:.4f} (+/- {xgb_std_score:.4f})\")\n    print(f\"XGBoost OOF LogLoss: {xgb_oof_score:.4f}\")\nelse:\n    print(\"XGBoost: No valid models trained\")\n    xgb_mean_score = 999","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:44:38.152277Z","iopub.execute_input":"2025-06-09T03:44:38.153193Z","iopub.status.idle":"2025-06-09T04:25:22.907858Z","shell.execute_reply.started":"2025-06-09T03:44:38.153149Z","shell.execute_reply":"2025-06-09T04:25:22.907089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Layer 2 (Meta Model LightGBM classifier)","metadata":{}},{"cell_type":"markdown","source":"Meskipun meta layer cepat dilatih dan rawan overfit, sebaiknya tetap gunakan machine dengan parameter sedang karena jika terlalu takut overfit justru malah terkena fallback.","metadata":{}},{"cell_type":"code","source":"# === TRAIN META MODEL (Layer 2) ===\nprint(\"\\n=== Training Meta Model (Stacking Layer 2) ===\")\n\nif catboost_mean_score < 999 and xgb_mean_score < 999:\n    print(\"Training LightGBM meta-learner...\")\n    \n    # Prepare stacking features (OOF predictions from base models)\n    stacking_features = np.column_stack([catboost_oof_preds, xgb_oof_preds])\n    print(f\"Stacking features shape: {stacking_features.shape}\")\n    \n    # Train meta model with LightGBM\n    meta_model = lgb.LGBMClassifier(\n        objective='multiclass',\n        num_class=3,\n        metric='multi_logloss',\n        boosting_type='gbdt',\n        num_leaves=31,\n        learning_rate=0.05,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        verbose=-1,\n        random_state=SEED,\n        n_estimators=250,\n        min_child_samples=20,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        early_stopping_rounds=70\n    )\n\n# Train with validation for early stopping\n    meta_train_x, meta_val_x, meta_train_y, meta_val_y = train_test_split(\n        stacking_features, y, test_size=0.2, random_state=SEED, stratify=y\n    )\n    \n    meta_model.fit(\n        meta_train_x, meta_train_y,\n        eval_set=[(meta_val_x, meta_val_y)],\n        callbacks=[lgb.early_stopping(70), lgb.log_evaluation(0)]\n    )\n    \n    # Get meta model predictions on OOF data\n    meta_oof_preds = meta_model.predict_proba(stacking_features)\n    meta_oof_score = log_loss(y, meta_oof_preds)\n    \n    print(f\"Meta Model OOF LogLoss: {meta_oof_score:.4f}\")\n    \n    # Compare with individual models\n    print(f\"Improvement over CatBoost: {catboost_oof_score - meta_oof_score:.4f}\")\n    print(f\"Improvement over XGBoost: {xgb_oof_score - meta_oof_score:.4f}\")\n    \n    # Use meta model predictions as final ensemble\n    ensemble_oof_preds = meta_oof_preds\n    ensemble_oof_score = meta_oof_score\n    \nelif catboost_mean_score < 999:\n    print(\"Using only CatBoost predictions (XGBoost failed)\")\n    ensemble_oof_preds = catboost_oof_preds\n    ensemble_oof_score = catboost_oof_score\n    meta_model = None\n    \nelif xgb_mean_score < 999:\n    print(\"Using only XGBoost predictions (CatBoost failed)\")\n    ensemble_oof_preds = xgb_oof_preds\n    ensemble_oof_score = xgb_oof_score\n    meta_model = None\n    \nelse:\n    print(\"ERROR: Both models failed to train properly!\")\n    raise Exception(\"No valid models for ensemble\")\n\nprint(f\"Best iteration: {meta_model.best_iteration_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:26:09.892211Z","iopub.execute_input":"2025-06-09T04:26:09.892806Z","iopub.status.idle":"2025-06-09T04:26:11.599016Z","shell.execute_reply.started":"2025-06-09T04:26:09.892784Z","shell.execute_reply":"2025-06-09T04:26:11.598286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Prediction","metadata":{}},{"cell_type":"markdown","source":"Fallback sangat penting sebagai antisipasi jika OOF underfit ","metadata":{}},{"cell_type":"code","source":"# === GENERATE TEST PREDICTIONS ===\nprint(\"\\n=== Generating Test Predictions ===\")\n\nX_test_combined = pd.concat([X_test_text, X_test_engineered], axis=1)\n\n# CatBoost test predictions\nif catboost_models and any(m is not None for m in catboost_models):\n    print(\"Generating CatBoost test predictions...\")\n    catboost_test_preds_list = []\n    \n    for i, model in enumerate(catboost_models):\n        if model is not None:\n            try:\n                fold_preds = model.predict_proba(X_test_combined)\n                catboost_test_preds_list.append(fold_preds)\n                print(f\"  CatBoost model {i+1}: Success\")\n            except Exception as e:\n                print(f\"  CatBoost model {i+1}: Error - {e}\")\n    \n    if catboost_test_preds_list:\n        catboost_test_preds = np.mean(catboost_test_preds_list, axis=0)\n        print(f\"  Averaged {len(catboost_test_preds_list)} CatBoost models\")\n    else:\n        catboost_test_preds = np.zeros((len(X_test_text), 3))\n        print(\"  No valid CatBoost predictions\")\n\n# XGBoost test predictions\nif xgb_models and any(m is not None for m in xgb_models):\n    print(\"Generating XGBoost test predictions...\")\n    xgb_test_preds_list = []\n    \n    for i, model in enumerate(xgb_models):\n        if model is not None:\n            try:\n                fold_preds = model.predict_proba(X_test_engineered)\n                xgb_test_preds_list.append(fold_preds)\n                print(f\"  XGBoost model {i+1}: Success\")\n            except Exception as e:\n                print(f\"  XGBoost model {i+1}: Error - {e}\")\n    \n    if xgb_test_preds_list:\n        xgb_test_preds = np.mean(xgb_test_preds_list, axis=0)\n        print(f\"  Averaged {len(xgb_test_preds_list)} XGBoost models\")\n    else:\n        xgb_test_preds = np.zeros((len(X_test_text), 3))\n        print(\"  No valid XGBoost predictions\")\n\n# Ensemble test predictions\n# Generate stacking test predictions\nif meta_model is not None:\n    print(\"Generating stacking test predictions...\")\n    \n    # Prepare stacking features for test set\n    test_stacking_features = np.column_stack([catboost_test_preds, xgb_test_preds])\n    print(f\"Test stacking features shape: {test_stacking_features.shape}\")\n    \n    # Get meta model predictions\n    ensemble_test_preds = meta_model.predict_proba(test_stacking_features)\n    print(\"Meta model test predictions generated successfully\")\n    \nelse:\n    # Fallback to simple ensemble if meta model couldn't be trained\n    if catboost_mean_score < 999 and xgb_mean_score < 999:\n        catboost_weight = 1 / catboost_mean_score\n        xgb_weight = 1 / xgb_mean_score\n        total_weight = catboost_weight + xgb_weight\n        \n        catboost_weight = catboost_weight / total_weight\n        xgb_weight = xgb_weight / total_weight\n        \n        ensemble_test_preds = catboost_weight * catboost_test_preds + xgb_weight * xgb_test_preds\n    elif catboost_mean_score < 999:\n        ensemble_test_preds = catboost_test_preds\n    else:\n        ensemble_test_preds = xgb_test_preds\n\nprint(f\"Final test predictions shape: {ensemble_test_preds.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:26:52.424023Z","iopub.execute_input":"2025-06-09T04:26:52.42434Z","iopub.status.idle":"2025-06-09T04:26:52.740744Z","shell.execute_reply.started":"2025-06-09T04:26:52.42432Z","shell.execute_reply":"2025-06-09T04:26:52.740137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Submission","metadata":{}},{"cell_type":"code","source":"# === PREPARE SUBMISSION ===\nprint(\"\\n=== Preparing Submission ===\")\n\n# Create submission dataframe\nsubmission_df = sample_submission.copy()\nsubmission_df['winner_model_a'] = ensemble_test_preds[:, 0]\nsubmission_df['winner_model_b'] = ensemble_test_preds[:, 1]\nsubmission_df['winner_tie'] = ensemble_test_preds[:, 2]\n\n# Verify predictions sum to 1\npred_sums = submission_df[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\nprint(f\"Prediction sums - Min: {pred_sums.min():.6f}, Max: {pred_sums.max():.6f}\")\nprint(f\"Predictions close to 1.0: {np.allclose(pred_sums, 1.0)}\")\n\n# Display submission statistics\nprint(f\"\\nSubmission statistics:\")\nprint(f\"winner_model_a: {submission_df['winner_model_a'].mean():.4f} ± {submission_df['winner_model_a'].std():.4f}\")\nprint(f\"winner_model_b: {submission_df['winner_model_b'].mean():.4f} ± {submission_df['winner_model_b'].std():.4f}\")\nprint(f\"winner_tie: {submission_df['winner_tie'].mean():.4f} ± {submission_df['winner_tie'].std():.4f}\")\n\n# Save submission\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"\\nSubmission saved as 'submission.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:26:58.179195Z","iopub.execute_input":"2025-06-09T04:26:58.179649Z","iopub.status.idle":"2025-06-09T04:26:58.194894Z","shell.execute_reply.started":"2025-06-09T04:26:58.179627Z","shell.execute_reply":"2025-06-09T04:26:58.194159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Importance for Further Improvement","metadata":{}},{"cell_type":"markdown","source":"Anda mungkin menemukan insight baru untuk feature engineering yang lebih baik disini.","metadata":{}},{"cell_type":"code","source":"# === FEATURE IMPORTANCE ANALYSIS ===\nprint(\"\\n=== Feature Importance Analysis ===\")\n\nif catboost_models and any(m is not None for m in catboost_models):\n    print(\"CatBoost Feature Importance (Top 20):\")\n    \n    # Get feature importance from first valid model\n    valid_catboost = next(m for m in catboost_models if m is not None)\n    feature_names = list(X_text.columns) + list(X_engineered.columns)\n    \n    if hasattr(valid_catboost, 'feature_importances_'):\n        importances = valid_catboost.feature_importances_\n        feature_importance_df = pd.DataFrame({\n            'feature': feature_names,\n            'importance': importances\n        }).sort_values('importance', ascending=False)\n        \n        print(feature_importance_df.head(20).to_string(index=False))\n        \n        # Save feature importance\n        feature_importance_df.to_csv('catboost_feature_importance.csv', index=False)\n        print(\"\\nCatBoost feature importance saved as 'catboost_feature_importance.csv'\")\n\nif xgb_models and any(m is not None for m in xgb_models):\n    print(\"\\nXGBoost Feature Importance (Top 20):\")\n    \n    # Get feature importance from first valid model\n    valid_xgb = next(m for m in xgb_models if m is not None)\n    \n    if hasattr(valid_xgb, 'feature_importances_'):\n        importances = valid_xgb.feature_importances_\n        feature_importance_df = pd.DataFrame({\n            'feature': list(X_engineered.columns),\n            'importance': importances\n        }).sort_values('importance', ascending=False)\n        \n        print(feature_importance_df.head(20).to_string(index=False))\n        \n        # Save feature importance\n        feature_importance_df.to_csv('xgb_feature_importance.csv', index=False)\n        print(\"\\nXGBoost feature importance saved as 'xgb_feature_importance.csv'\")\n\n# === META MODEL ANALYSIS ===\nprint(\"\\n=== Meta Model Analysis ===\")\n\nif meta_model is not None:\n    print(\"LightGBM Meta Model Feature Importance:\")\n    \n    # Get feature names for stacking features\n    stacking_feature_names = []\n    for i in range(3):  # 3 classes\n        stacking_feature_names.append(f'CatBoost_class_{i}')\n    for i in range(3):\n        stacking_feature_names.append(f'XGBoost_class_{i}')\n    \n    # Get feature importance\n    importances = meta_model.feature_importances_\n    meta_importance_df = pd.DataFrame({\n        'feature': stacking_feature_names,\n        'importance': importances\n    }).sort_values('importance', ascending=False)\n    \n    print(meta_importance_df.to_string(index=False))\n    \n    # Save meta model feature importance\n    meta_importance_df.to_csv('meta_model_feature_importance.csv', index=False)\n    print(\"\\nMeta model feature importance saved as 'meta_model_feature_importance.csv'\")\n    \n    print(f\"\\nMeta model training score: {meta_model.best_score_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:27:02.184924Z","iopub.execute_input":"2025-06-09T04:27:02.185648Z","iopub.status.idle":"2025-06-09T04:27:02.225081Z","shell.execute_reply.started":"2025-06-09T04:27:02.185624Z","shell.execute_reply":"2025-06-09T04:27:02.224532Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Summary and Interpretation","metadata":{}},{"cell_type":"markdown","source":"## Summary","metadata":{}},{"cell_type":"code","source":"# === FINAL SUMMARY ===\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL TRAINING SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Training samples: {len(train_processed_df)}\")\nprint(f\"Test samples: {len(test_processed_df)}\")\nprint(f\"Text features: {len(text_features_cols)}\")\nprint(f\"Engineered features: {len(engineered_cols)}\")\nprint(f\"Total features: {len(text_features_cols) + len(engineered_cols)}\")\nprint(f\"Cross-validation folds: {N_FOLDS}\")\nprint()\nprint(\"Model Performance:\")\nif catboost_mean_score < 999:\n    print(f\"  CatBoost CV LogLoss: {catboost_mean_score:.4f} ± {catboost_std_score:.4f}\")\nif xgb_mean_score < 999:\n    print(f\"  XGBoost CV LogLoss: {xgb_mean_score:.4f} ± {xgb_std_score:.4f}\")\nif meta_model is not None:\n    print(f\"  Stacking Model OOF LogLoss: {ensemble_oof_score:.4f}\")\n    print(f\"  Stacking Architecture: CatBoost + XGBoost → LightGBM\")\nelse:\n    print(f\"  Simple Ensemble OOF LogLoss: {ensemble_oof_score:.4f}\")\nprint()\nprint(\"Files generated:\")\nprint(\"  - submission.csv\")\nif catboost_models and any(m is not None for m in catboost_models):\n    print(\"  - catboost_feature_importance.csv\")\nif xgb_models and any(m is not None for m in xgb_models):\n    print(\"  - xgb_feature_importance.csv\")\nif meta_model is not None:\n    print(\"  - meta_model_feature_importance.csv\")\nprint()\nprint(\"Training completed successfully!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:27:07.533078Z","iopub.execute_input":"2025-06-09T04:27:07.53337Z","iopub.status.idle":"2025-06-09T04:27:07.540835Z","shell.execute_reply.started":"2025-06-09T04:27:07.533349Z","shell.execute_reply":"2025-06-09T04:27:07.540023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Interpretation","metadata":{}},{"cell_type":"code","source":"# === VISUALIZATION AND INTERPRETATION ===\nprint(\"\\n\" + \"=\"*60)\nprint(\"VISUALIZATION AND INTERPRETATION\")\nprint(\"=\"*60)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.calibration import calibration_curve, CalibrationDisplay\n\n# --- Helper function untuk plot feature importance ---\ndef plot_feature_importance(importance_df, title, top_n=20, filename='feature_importance.png'):\n    if importance_df is None or importance_df.empty:\n        print(f\"  Skipping '{title}' - importance data not available.\")\n        return\n    try:\n        plt.figure(figsize=(10, max(6, top_n * 0.3))) # Adjust height based on top_n\n        sns.barplot(x='importance', y='feature', data=importance_df.head(top_n), palette='viridis')\n        plt.title(f'{title} (Top {top_n})')\n        plt.tight_layout()\n        plt.savefig(filename)\n        plt.show()\n        print(f\"  Saved {filename}\")\n    except Exception as e:\n        print(f\"  Error plotting {title}: {e}\")\n\n# --- Feature Importance CatBoost ---\nprint(\"\\nPlotting CatBoost Feature Importance...\")\ntry:\n    cb_fi_df = pd.read_csv('catboost_feature_importance.csv')\n    plot_feature_importance(cb_fi_df, 'CatBoost Feature Importance', filename='catboost_importance_plot.png')\nexcept FileNotFoundError:\n    print(\"  catboost_feature_importance.csv not found. Skipping plot.\")\nexcept Exception as e:\n    print(f\"  Error loading/plotting CatBoost importance: {e}\")\n\n\n# --- Feature Importance XGBoost ---\nprint(\"\\nPlotting XGBoost Feature Importance...\")\ntry:\n    xgb_fi_df = pd.read_csv('xgb_feature_importance.csv')\n    plot_feature_importance(xgb_fi_df, 'XGBoost Feature Importance', filename='xgb_importance_plot.png')\nexcept FileNotFoundError:\n    print(\"  xgb_feature_importance.csv not found. Skipping plot.\")\nexcept Exception as e:\n    print(f\"  Error loading/plotting XGBoost importance: {e}\")\n\n\n# --- Meta-Model Feature Importance ---\nprint(\"\\nPlotting Meta-Model Feature Importance...\")\nif meta_model is not None:\n    try:\n        meta_fi_df = pd.read_csv('meta_model_feature_importance.csv')\n        plot_feature_importance(meta_fi_df, 'Meta-Model Feature Importance (Base Model Weights)',\n                                top_n=len(meta_fi_df), # Show all meta-features\n                                filename='meta_model_importance_plot.png')\n    except FileNotFoundError:\n        print(\"  meta_model_feature_importance.csv not found. Skipping plot.\")\n    except Exception as e:\n        print(f\"  Error loading/plotting Meta-Model importance: {e}\")\nelse:\n    print(\"  Meta-model not trained. Skipping meta-model importance plot.\")\n\n\n# --- Skor Validasi Silang (CV Scores) ---\nprint(\"\\nPlotting Cross-Validation Scores...\")\ntry:\n    cv_scores_data = []\n    model_names = []\n    if 'val_scores_catboost' in locals() and val_scores_catboost and min(val_scores_catboost) < 999:\n        cv_scores_data.extend(val_scores_catboost)\n        model_names.extend(['CatBoost'] * len(val_scores_catboost))\n    if 'val_scores_xgb' in locals() and val_scores_xgb and min(val_scores_xgb) < 999:\n        cv_scores_data.extend(val_scores_xgb)\n        model_names.extend(['XGBoost'] * len(val_scores_xgb))\n\n    if cv_scores_data:\n        cv_df = pd.DataFrame({'Model': model_names, 'LogLoss': cv_scores_data})\n        plt.figure(figsize=(8, 6))\n        sns.boxplot(x='Model', y='LogLoss', data=cv_df, palette='pastel')\n        plt.title('Cross-Validation LogLoss by Model')\n        plt.ylabel('LogLoss (Lower is Better)')\n        plt.tight_layout()\n        plt.savefig('cv_scores_boxplot.png')\n        plt.show()\n        print(\"  Saved cv_scores_boxplot.png\")\n    else:\n        print(\"  No valid CV scores to plot.\")\nexcept Exception as e:\n    print(f\"  Error plotting CV scores: {e}\")\n\n\n# --- Matriks Konfusi (Confusion Matrix) untuk OOF Ensemble ---\nprint(\"\\nPlotting OOF Ensemble Confusion Matrix...\")\nif 'ensemble_oof_preds' in locals() and 'y' in locals():\n    try:\n        y_pred_labels_oof = np.argmax(ensemble_oof_preds, axis=1)\n        cm = confusion_matrix(y, y_pred_labels_oof)\n        class_names = ['Model A Wins', 'Model B Wins', 'Tie'] # Sesuai dengan target encoding 0, 1, 2\n\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n        fig, ax = plt.subplots(figsize=(8, 7))\n        disp.plot(ax=ax, cmap='Blues', values_format='d')\n        plt.title(f'Ensemble OOF Predictions Confusion Matrix\\nOOF LogLoss: {ensemble_oof_score:.4f}')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.savefig('ensemble_oof_confusion_matrix.png')\n        plt.show()\n        print(\"  Saved ensemble_oof_confusion_matrix.png\")\n    except Exception as e:\n        print(f\"  Error plotting confusion matrix: {e}\")\nelse:\n    print(\"  Ensemble OOF predictions or true labels not available for confusion matrix.\")\n\n# --- Kurva Kalibrasi (Calibration Curve) untuk OOF Ensemble ---\nprint(\"\\nPlotting OOF Ensemble Calibration Curves...\")\nif 'ensemble_oof_preds' in locals() and 'y' in locals():\n    try:\n        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n        class_names = ['Model A Wins', 'Model B Wins', 'Tie']\n        colors = ['blue', 'red', 'green']\n\n        for i in range(ensemble_oof_preds.shape[1]):\n            prob_true, prob_pred = calibration_curve(y == i, ensemble_oof_preds[:, i], n_bins=10, strategy='uniform')\n            disp = CalibrationDisplay(prob_true, prob_pred, ensemble_oof_preds[:, i])\n            disp.plot(ax=ax, name=f'Class: {class_names[i]}', color=colors[i])\n\n        ax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n        ax.set_title(f'Ensemble OOF Calibration Curves\\nOOF LogLoss: {ensemble_oof_score:.4f}')\n        ax.set_xlabel(\"Mean Predicted Probability (Positive Class)\")\n        ax.set_ylabel(\"Fraction of Positives (Positive Class)\")\n        ax.legend(loc=\"lower right\")\n        plt.tight_layout()\n        plt.savefig('ensemble_oof_calibration_curves.png')\n        plt.show()\n        print(\"  Saved ensemble_oof_calibration_curves.png\")\n    except Exception as e:\n        print(f\"  Error plotting calibration curves: {e}\")\nelse:\n    print(\"  Ensemble OOF predictions or true labels not available for calibration curves.\")\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VISUALIZATION COMPLETED\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:32:58.314258Z","iopub.execute_input":"2025-06-09T04:32:58.314558Z","iopub.status.idle":"2025-06-09T04:33:00.495723Z","shell.execute_reply.started":"2025-06-09T04:32:58.314536Z","shell.execute_reply":"2025-06-09T04:33:00.494984Z"}},"outputs":[],"execution_count":null}]}