{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12869493,"sourceType":"datasetVersion","datasetId":8140740},{"sourceId":543581,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":418278,"modelId":435942}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %pip install evaluate\n# %pip download evaluate\n# %pip download evaluate -d wheels/\n# from evaluate import load\n# load('accuracy')\n# accuracy = load(\"/kaggle/input/import-evaluate/wheels/evaluate/metrics/accuracy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:13:14.731426Z","iopub.execute_input":"2025-08-26T04:13:14.731762Z","iopub.status.idle":"2025-08-26T04:13:14.735451Z","shell.execute_reply.started":"2025-08-26T04:13:14.731738Z","shell.execute_reply":"2025-08-26T04:13:14.734711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %pip install --no-index --find-links=/kaggle/input/import-evaluate/wheels evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T03:51:29.210445Z","iopub.execute_input":"2025-08-26T03:51:29.211172Z","iopub.status.idle":"2025-08-26T03:51:32.518927Z","shell.execute_reply.started":"2025-08-26T03:51:29.211139Z","shell.execute_reply":"2025-08-26T03:51:32.518065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score\n\n# y_true = [0, 1, 0, 1]\n# y_pred = [0, 0, 0, 1]\n\n# print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:13:48.37764Z","iopub.execute_input":"2025-08-26T04:13:48.377915Z","iopub.status.idle":"2025-08-26T04:13:48.38655Z","shell.execute_reply.started":"2025-08-26T04:13:48.377895Z","shell.execute_reply":"2025-08-26T04:13:48.38586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# from evaluate import load\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nimport torch\n\n# -----------------------\n# 1. Load Data\n# -----------------------\n# data_dir = Path(\"/kaggle/input/playground-series-s5e8\")\n# train_path = data_dir / \"train.csv\"\n# test_path = data_dir / \"test.csv\"\n# sample_path = data_dir / \"sample_submission.csv\"\nout_path = \"/kaggle/working/submission.csv\"\n\ntrain = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# train = train[:75]\n# test = train[:25]\n\n# Map to single column\ntrain[\"y\"] = train[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].idxmax(axis=1)\nprint(train[\"y\"])\n# Replace column names with 0/1/2\nmapping = {\"winner_model_a\": 0, \"winner_model_b\": 1, \"winner_tie\": 2}\ntrain[\"y\"] = train[\"y\"].map(mapping)\n\n# train = train.drop(columns)\nlabel_list = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\", \"model_a\", \"model_b\"]\nTARGET = \"y\"\nID_COL = \"id\"\n# LABEL_LIST = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\nFEATURES = [col for col in train.columns if col not in [ID_COL, TARGET]+label_list]\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\nprint(\"Features:\", FEATURES)\n\n# -----------------------\n# 2. Prepare Text Feature\n# -----------------------\n# Concatenate features into one text column (since HuggingFace works with text input)\ntrain[\"text\"] = train[FEATURES].astype(str).agg(\" \".join, axis=1)\ntest[\"text\"] = test[FEATURES].astype(str).agg(\" \".join, axis=1)\n\n# Train/Validation split\ntrain_df, valid_df = train_test_split(train, test_size=0.2, stratify=train[TARGET], random_state=42)\n\n# Convert to HuggingFace Dataset\ndataset = DatasetDict({\n    \"train\": Dataset.from_pandas(train_df[[\"text\", TARGET]]).rename_column(TARGET, \"labels\"),\n    \"validation\": Dataset.from_pandas(valid_df[[\"text\", TARGET]]).rename_column(TARGET, \"labels\"),\n    \"test\": Dataset.from_pandas(test[[\"text\", ID_COL]])\n})\n# print(dataset[\"train\"][0])\n# -----------------------\n# 3. Tokenizer\n# -----------------------\n# MODEL = \"distilbert-base-uncased\"\n# MODEL = \"huawei-noah/TinyBERT_General_4L_312D\"\n# MODEL = \"ginnigarg/binary-classification-kaggle-tiny-bert\"\nMODEL = \"/kaggle/input/tiny-bert/transformers/default/1/results/checkpoint-1080\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n\n# Rename target to \"labels\" for Trainer\n# Using map\n# train_ds = train_ds.map(lambda x: {\"text_length\": len(x[\"input_ids\"])})\n# dataset[\"train\"] = dataset[\"train\"].rename_column(TARGET, \"labels\")\n# dataset[\"validation\"] = dataset[\"validation\"].rename_column(TARGET, \"labels\")\n\n# Set format for PyTorch\ndataset['train'].set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], output_all_columns=False)\ndataset['validation'].set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], output_all_columns=False)\n\n\n# -----------------------\n# 4. Model\n# -----------------------\nnum_labels = len(train[TARGET].unique())  # should be 2 for binary classification\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)\n\nBATCH_SIZE = 64\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to GPU\nmodel.to(device)\nmodel.eval()\n\n# -----------------------\n# 5. Training Setup\n# -----------------------\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    logging_dir=\"./logs\",\n)\n\n# Metrics\n# accuracy = load(\"accuracy\")\n# roc_auc = load(\"roc_auc\")\n\n# accuracy = load(\"/kaggle/input/import-evaluate/wheels/evaluate/metrics/accuracy\")\n# roc_auc = load(\"/kaggle/input/import-evaluate/wheels/evaluate/metrics/roc_auc\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    # acc = accuracy.compute(predictions=preds, references=labels)\n    acc = accuracy_score(labels, preds)\n    # auc = roc_auc.compute(prediction_scores=logits[:,1], references=labels)\n    return {\"accuracy\": acc} # \"roc_auc\": auc[\"roc_auc\"]}\n\n# -----------------------\n# 6. Trainer\n# -----------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# -----------------------\n# 7. Train\n# -----------------------\ntrainer.train()\n\n# -----------------------\n# 8. Predictions on Test\n# -----------------------\n\ntest_texts = test[\"text\"].tolist()\nall_probs = []\n\nwith torch.no_grad():\n    for i in range(0, len(test_texts), BATCH_SIZE):\n        batch_texts = test_texts[i:i+BATCH_SIZE]\n        batch_encodings = tokenizer(\n            batch_texts,\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        )\n\n        # Move batch tensors to GPU\n        batch_encodings = {k: v.to(device) for k, v in batch_encodings.items()}\n\n        outputs = model(**batch_encodings)\n        # probs = torch.softmax(outputs.logits, dim=-1)[:, 1]\n        probs = torch.softmax(outputs.logits, dim=-1)\n        all_probs.extend(probs.cpu().numpy())  # move back to CPU for storage\n\n# Convert to numpy array\nprint(all_probs)\nprint(all_probs[0])\nall_probs = np.array(all_probs)\n        \n# -----------------------\n# 9. Save Submission\n# -----------------------\n# submission = pd.DataFrame({\n#     \"id\": test[ID_COL],\n#     \"y\": (all_probs > 0.5).astype(int)  # threshold at 0.5\n# })\n\nsubmission = pd.DataFrame(all_probs, columns=[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"])\nsubmission['id'] = test[ID_COL]\nsubmission.to_csv(out_path, index=False)\n\nprint(\"âœ… Saved submission.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:02:55.904878Z","iopub.execute_input":"2025-08-25T17:02:55.905228Z","execution_failed":"2025-08-25T17:03:55.157Z"}},"outputs":[],"execution_count":null}]}