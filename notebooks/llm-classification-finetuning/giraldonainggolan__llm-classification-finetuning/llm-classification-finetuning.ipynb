{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:08:03.870065Z","iopub.execute_input":"2024-11-29T05:08:03.870365Z","iopub.status.idle":"2024-11-29T05:08:04.311241Z","shell.execute_reply.started":"2024-11-29T05:08:03.870326Z","shell.execute_reply":"2024-11-29T05:08:04.309909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import library yang diperlukan\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T21:25:03.482576Z","iopub.execute_input":"2024-11-29T21:25:03.483035Z","iopub.status.idle":"2024-11-29T21:25:04.065937Z","shell.execute_reply.started":"2024-11-29T21:25:03.482998Z","shell.execute_reply":"2024-11-29T21:25:04.06461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Load data\n# Ganti path_file dengan file yang sesuai dari Kaggle\ntrain_data = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")  # Dataset latih\ntest_data = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")    # Dataset uji","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:08:05.2602Z","iopub.execute_input":"2024-11-29T05:08:05.260696Z","iopub.status.idle":"2024-11-29T05:08:07.49842Z","shell.execute_reply.started":"2024-11-29T05:08:05.260631Z","shell.execute_reply":"2024-11-29T05:08:07.496978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Eksplorasi data\nprint(train_data.head())\nprint(train_data.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:08:07.499815Z","iopub.execute_input":"2024-11-29T05:08:07.500282Z","iopub.status.idle":"2024-11-29T05:08:07.558788Z","shell.execute_reply.started":"2024-11-29T05:08:07.500232Z","shell.execute_reply":"2024-11-29T05:08:07.557569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import set_seed\nimport ctypes, gc\nimport torch\nimport random\nimport numpy as np\n\nlibc = ctypes.CDLL(\"libc.so.6\")\n# Seed the same seed to all \ndef seed_everything(seed=42):\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    set_seed(seed)\n    \ndef clear_memory():\n    libc.malloc_trim(0)\n    torch.cuda.empty_cache()\n    gc.collect()\n\nSEED = 42\nseed_everything(SEED)\n# Set the GPUs\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T21:20:17.347302Z","iopub.execute_input":"2024-11-29T21:20:17.347983Z","iopub.status.idle":"2024-11-29T21:20:17.358622Z","shell.execute_reply.started":"2024-11-29T21:20:17.347939Z","shell.execute_reply":"2024-11-29T21:20:17.357171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T21:21:26.560968Z","iopub.execute_input":"2024-11-29T21:21:26.561386Z","iopub.status.idle":"2024-11-29T21:21:27.617179Z","shell.execute_reply.started":"2024-11-29T21:21:26.561352Z","shell.execute_reply":"2024-11-29T21:21:27.615515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/test.csv')\nsample_submission = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\nprint('train data shape :', train.shape)\nprint('test data shape :', test.shape)\nprint('sample_submission data shape :', sample_submission.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T21:21:34.078535Z","iopub.execute_input":"2024-11-29T21:21:34.079457Z","iopub.status.idle":"2024-11-29T21:21:38.386449Z","shell.execute_reply.started":"2024-11-29T21:21:34.079407Z","shell.execute_reply":"2024-11-29T21:21:38.385342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom scipy.sparse import hstack\nfrom lightgbm import LGBMClassifier\nimport gc\n\ny = train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)\ny = y.map({\n    'winner_model_a': 0,\n    'winner_model_b': 1,\n    'winner_tie': 2\n})\n\ndf_fit = test if len(test) > 3 else train\n\ntfidf_prompt = TfidfVectorizer(\n    max_features=500,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\ncount_prompt = CountVectorizer(\n    max_features=500,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\n\ntfidf_response = TfidfVectorizer(\n    max_features=1000,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\ncount_response = CountVectorizer(\n    max_features=1000,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\n\ntfidf_prompt_char = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=500,\n    min_df=0.002\n)\ncount_prompt_char = CountVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=500,\n    min_df=0.002\n)\n\ntfidf_response_char = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=1000,\n    min_df=0.002\n)\ncount_response_char = CountVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=1000,\n    min_df=0.002\n)\n\ntfidf_prompt.fit(df_fit['prompt'])\ncount_prompt.fit(df_fit['prompt'])\n\ntfidf_response.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\ncount_response.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\n\ntfidf_prompt_char.fit(df_fit['prompt'])\ncount_prompt_char.fit(df_fit['prompt'])\n\ntfidf_response_char.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\ncount_response_char.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\n\ndef get_features(df):\n    X_prompt_tfidf = tfidf_prompt.transform(df['prompt'])\n    X_prompt_count = count_prompt.transform(df['prompt'])\n    \n    X_prompt_tfidf_char = tfidf_prompt_char.transform(df['prompt'])\n    X_prompt_count_char = count_prompt_char.transform(df['prompt'])\n    \n    X_prompt_combined = hstack([\n        X_prompt_tfidf, \n        X_prompt_count, \n        X_prompt_tfidf_char, \n        X_prompt_count_char\n    ])\n    \n    response_combined = pd.concat([df['response_a'], df['response_b']], axis=0)\n    X_response_tfidf = tfidf_response.transform(response_combined)\n    X_response_count = count_response.transform(response_combined)\n    \n    X_response_tfidf_char = tfidf_response_char.transform(response_combined)\n    X_response_count_char = count_response_char.transform(response_combined)\n    \n    n = len(df)\n    X_response_a_tfidf = X_response_tfidf[:n]\n    X_response_b_tfidf = X_response_tfidf[n:]\n    \n    X_response_a_count = X_response_count[:n]\n    X_response_b_count = X_response_count[n:]\n    \n    X_response_a_tfidf_char = X_response_tfidf_char[:n]\n    X_response_b_tfidf_char = X_response_tfidf_char[n:]\n    \n    X_response_a_count_char = X_response_count_char[:n]\n    X_response_b_count_char = X_response_count_char[n:]\n    \n    afeat = hstack([\n        X_response_a_tfidf, \n        X_response_a_count,\n        X_response_a_tfidf_char,\n        X_response_a_count_char\n    ])\n    bfeat = hstack([\n        X_response_b_tfidf, \n        X_response_b_count,\n        X_response_b_tfidf_char,\n        X_response_b_count_char\n    ])\n    \n    v = hstack([\n        afeat - bfeat, \n        abs(afeat - bfeat)\n    ])\n    \n    extras = []\n    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n    for e in EXTRAS:\n        for c in ['prompt', 'response_a', 'response_b']:\n            extras.append(df[c].str.count(e).values)\n    \n    extras.append(df['prompt'].str.len().values)\n    extras.append(df['prompt'].str.split().apply(lambda x: len(x)).values)\n    \n    extras = np.stack(extras, axis=1)\n    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n    \n    final_features = hstack([v, extras, X_prompt_combined])\n    return final_features.tocsr()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = get_features(train)\nX_test = get_features(test)\n\n\n# def feature_select_wrapper(X, y, cumulative_importance=0.95, n_splits=5, random_state=42):\n\n#     skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n#     fse = np.zeros(X.shape[1])\n\n#     for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n\n#         X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n#         y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n\n#         model = LGBMClassifier(\n#             objective='multiclass',\n#             num_class=3,\n#             learning_rate=0.05,\n#             n_estimators=1000,\n#             random_state=random_state,\n#             n_jobs=-1\n#         )\n\n#         model.fit(\n#             X_train_fold, y_train_fold,\n#             eval_set=[(X_val_fold, y_val_fold)],\n#             eval_metric='multi_logloss',\n#         )\n\n#         importances = model.feature_importances_\n\n#         fse += importances\n\n#     fse /= n_splits\n\n#     sorted_indices = np.argsort(fse)[::-1]\n#     sorted_importances = fse[sorted_indices]\n\n#     cumulative_importance_values = np.cumsum(sorted_importances) / np.sum(sorted_importances)\n\n\n#     cutoff_index = np.searchsorted(cumulative_importance_values, cumulative_importance) + 1\n#     feature_select_indices = sorted_indices[:cutoff_index].tolist()\n\n\n#     return feature_select_indices\n\n# selected_feature_indices = feature_select_wrapper(\n#     X=X_train,\n#     y=y,\n#     cumulative_importance=0.9,\n#     n_splits=5,\n#     random_state=42\n# )\n\n# X_train_selected = X_train[:, selected_feature_indices]\n# X_test_selected = X_test[:, selected_feature_indices]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cross_validate_and_save_models(X, y, n_splits=10, random_state=42, save_path='models_and_weights.joblib'):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    log_losses = []\n    weights = []\n    models = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n        print(f'Fold {fold}/{n_splits}')\n\n        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n\n        # model = XGBClassifier(\n        #     objective='multi:softprob',\n        #     eval_metric='mlogloss',\n        #     num_class=3,\n        #     random_state=random_state,\n        #     n_estimators=1000,\n        #     learning_rate=0.05\n        # )\n\n        # model.fit(\n        #     X_train_fold, y_train_fold,\n        #     eval_set=[(X_val_fold, y_val_fold)],\n        #     early_stopping_rounds=50,\n        #     verbose=100\n        # )\n        \n        model = LGBMClassifier(\n            objective='multiclass',\n            num_class=3,\n            learning_rate=0.05,\n            n_estimators=1000,\n            random_state=random_state,\n            n_jobs=-1\n        )\n\n        model.fit(\n            X_train_fold, y_train_fold,\n            eval_set=[(X_val_fold, y_val_fold)],\n            eval_metric='multi_logloss',\n        )\n        y_val_pred = model.predict_proba(X_val_fold)\n        loss = log_loss(y_val_fold, y_val_pred)\n        log_losses.append(loss)\n        print(f'Fold {fold} Log Loss: {loss}')\n\n        weight = 1.0 / loss\n        weights.append(weight)\n\n        models.append(model)\n\n    average_log_loss = np.mean(log_losses)\n    print(f'Average {n_splits}-Fold Log Loss: {average_log_loss}')\n\n    models_and_weights = {\n        'models': models, \n        'weights': weights \n    }\n\n    joblib.dump(models_and_weights, save_path)\n    print(f'All models and weights have been saved to {save_path}')\n\ncross_validate_and_save_models(\n    X=X_train,\n    y=y,\n    n_splits=10,\n    random_state=42,\n    save_path='models_and_weights.joblib'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_models_and_weights(save_path='models_and_weights.joblib'):\n    models_and_weights = joblib.load(save_path)\n    models = models_and_weights['models']\n    weights = models_and_weights['weights']\n    print(f'Loaded {len(models)} models with corresponding weights.')\n    return models, weights\n\ndef weighted_predict(models, weights, X_test):\n    test_preds = np.zeros((X_test.shape[0], 3))\n    for model, weight in zip(models, weights):\n        y_pred = model.predict_proba(X_test)\n        test_preds += y_pred * weight\n    test_preds_weighted = test_preds / np.sum(weights)\n    return test_preds_weighted\n\nmodels, weights = load_models_and_weights(save_path='models_and_weights.joblib')\n\ny_test_proba = weighted_predict(models, weights, X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': y_test_proba[:, 0],\n    'winner_model_b': y_test_proba[:, 1],\n    'winner_tie': y_test_proba[:, 2]\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}