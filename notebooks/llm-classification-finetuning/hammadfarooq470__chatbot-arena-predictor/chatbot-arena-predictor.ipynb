{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This is a template for a Kaggle Notebook for the LLM Classification Finetuning competition.\n# You will need to fill in the specifics related to your data and chosen model.\n\n# 1. Setting up the environment\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\n\n# Example imports for a basic text classification model.\n# You might need more advanced NLP libraries like transformers, torch, tensorflow, etc.\n# based on your chosen approach (e.g., fine-tuning a pre-trained LLM, using embeddings).\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries imported successfully.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:30:37.121755Z","iopub.execute_input":"2025-07-01T01:30:37.122091Z","iopub.status.idle":"2025-07-01T01:30:40.820545Z","shell.execute_reply.started":"2025-07-01T01:30:37.122052Z","shell.execute_reply":"2025-07-01T01:30:40.819421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Loading the data\n# Assuming the data files are in '../input/llm-classification-finetuning/'\n# You'll need to check the actual file names and paths once you join the competition.\n\ntry:\n    train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n    test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n    # You might also have a sample submission file to guide the format\n    sample_submission_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\n    print(\"Data loaded successfully.\")\n    print(f\"Train data shape: {train_df.shape}\")\n    print(f\"Test data shape: {test_df.shape}\")\n    print(f\"Sample Submission data shape: {sample_submission_df.shape}\")\n\nexcept FileNotFoundError:\n    print(\"Error: Data files not found. Please ensure the dataset is added to your Kaggle Notebook.\")\n    print(\"Go to 'Add Data' on the right sidebar of your notebook and search for 'llm-classification-finetuning'.\")\n    exit() # Exit if data isn't found to prevent further errors\n\n# Display the first few rows of the dataframes to understand their structure\nprint(\"\\nTrain DataFrame Head:\")\nprint(train_df.head())\nprint(\"\\nTest DataFrame Head:\")\nprint(test_df.head())\nprint(\"\\nSample Submission DataFrame Head:\")\nprint(sample_submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:30:40.82233Z","iopub.execute_input":"2025-07-01T01:30:40.822852Z","iopub.status.idle":"2025-07-01T01:30:45.086893Z","shell.execute_reply.started":"2025-07-01T01:30:40.822827Z","shell.execute_reply":"2025-07-01T01:30:45.085982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Exploratory Data Analysis (EDA) - Initial Checks\nprint(\"\\nTrain DataFrame Info:\")\ntrain_df.info()\nprint(\"\\nTest DataFrame Info:\")\ntest_df.info()\n\nprint(\"\\nMissing values in Train DataFrame:\")\nprint(train_df.isnull().sum())\nprint(\"\\nMissing values in Test DataFrame:\")\nprint(test_df.isnull().sum())\n\n# Check the distribution of the target variable (winner_model_a, winner_model_b, winner_tie)\n# The competition description implies these are one-hot encoded, so we'll sum them to find the true winner.\n# Assuming the 'winner' columns are directly provided in the train.csv as the target.\n# If the target is a single column indicating the winner, you'll need to adjust this.\n# Let's create a 'winner' column for easier analysis if it's not already a single categorical column.\n\n# This part is highly dependent on the exact structure of your target variable in train.csv\n# If the target is already a single column like 'winner' with values 'model_a', 'model_b', 'tie',\n# then skip the below reconstruction and directly use that column.\nif 'winner_model_a' in train_df.columns and 'winner_model_b' in train_df.columns and 'winner_tie' in train_df.columns:\n    def get_winner(row):\n        if row['winner_model_a'] == 1:\n            return 'model_a'\n        elif row['winner_model_b'] == 1:\n            return 'model_b'\n        elif row['winner_tie'] == 1:\n            return 'tie'\n        else:\n            return 'unknown' # Should not happen if data is clean\n    train_df['winner'] = train_df.apply(get_winner, axis=1)\n    print(\"\\nDistribution of 'winner' in Train DataFrame:\")\n    print(train_df['winner'].value_counts(normalize=True))\nelse:\n    print(\"\\nWarning: 'winner_model_a', 'winner_model_b', or 'winner_tie' columns not found as expected.\")\n    print(\"Please inspect your 'train.csv' to understand the target variable's structure.\")\n    # If your target is a single categorical column, e.g., 'target_winner', then use:\n    # print(train_df['target_winner'].value_counts(normalize=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:30:45.087603Z","iopub.execute_input":"2025-07-01T01:30:45.087811Z","iopub.status.idle":"2025-07-01T01:30:45.622612Z","shell.execute_reply.started":"2025-07-01T01:30:45.087793Z","shell.execute_reply":"2025-07-01T01:30:45.621539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Feature Engineering\n# This is a critical step and will heavily depend on the content of your 'prompt', 'response_a', and 'response_b' columns.\n# You'll likely need to extract meaningful features from the text.\n\ndef preprocess_text(text):\n    if pd.isna(text):\n        return \"\"\n    text = str(text).lower() # Convert to string and lowercase\n    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n    return text\n\nprint(\"\\nApplying text preprocessing...\")\n# Apply preprocessing to relevant text columns\n# Assuming 'prompt', 'response_a', 'response_b' are the text columns.\n# You might have other columns like 'conversation_id', 'turn', etc. which could also be useful.\ntrain_df['processed_prompt'] = train_df['prompt'].apply(preprocess_text)\ntrain_df['processed_response_a'] = train_df['response_a'].apply(preprocess_text)\ntrain_df['processed_response_b'] = train_df['response_b'].apply(preprocess_text)\n\ntest_df['processed_prompt'] = test_df['prompt'].apply(preprocess_text)\ntest_df['processed_response_a'] = test_df['response_a'].apply(preprocess_text)\ntest_df['processed_response_b'] = test_df['response_b'].apply(preprocess_text)\n\n# Example basic feature engineering: combining prompt and responses\n# You could create features like:\n# - Length of prompt, response_a, response_b\n# - Word count of prompt, response_a, response_b\n# - Readability scores (Flesch-Kincaid)\n# - Sentiment scores\n# - Difference in length/word count between response_a and response_b\n# - Incorporating LLM specific metrics if available (e.g., perplexity, toxicity scores from external models)\n\ntrain_df['combined_text_a'] = train_df['processed_prompt'] + \" \" + train_df['processed_response_a']\ntrain_df['combined_text_b'] = train_df['processed_prompt'] + \" \" + train_df['processed_response_b']\n\ntest_df['combined_text_a'] = test_df['processed_prompt'] + \" \" + test_df['processed_response_a']\ntest_df['combined_text_b'] = test_df['processed_prompt'] + \" \" + test_df['processed_response_b']\n\nprint(\"Text preprocessing and basic combination complete.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:30:45.623647Z","iopub.execute_input":"2025-07-01T01:30:45.623959Z","iopub.status.idle":"2025-07-01T01:31:02.306279Z","shell.execute_reply.started":"2025-07-01T01:30:45.623932Z","shell.execute_reply":"2025-07-01T01:31:02.305305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Model Training\n# For a Getting Started competition, a simple yet effective approach is often a good start.\n# Logistic Regression with TF-IDF features is a common baseline for text classification.\n# For more advanced solutions, consider:\n# - Embeddings (Word2Vec, GloVe, FastText)\n# - Pre-trained language models (BERT, RoBERTa, etc.) and fine-tuning them for classification\n# - Siamese networks or other architectures that compare two texts\n\n# Define features (X) and target (y)\n# We need to predict probabilities for winner_model_a, winner_model_b, winner_tie\n# This means we're essentially doing a multi-class classification problem.\n\n# For simplicity, let's start with a single model that predicts the 'winner' category.\n# Then, we'll convert the predictions to the required probability format.\nX_train_text_a = train_df['combined_text_a']\nX_train_text_b = train_df['combined_text_b']\ny_train = train_df['winner'] # This assumes 'winner' column was created correctly\n\nX_test_text_a = test_df['combined_text_a']\nX_test_text_b = test_df['combined_text_b']\n\n\n# Label encode the target variable\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train)\n\n# We need to build a model that can compare response A and response B.\n# One approach is to treat this as two separate binary classification problems (A vs not A, B vs not B)\n# or a multi-class classification.\n# Given the competition asks for probabilities for A, B, and tie, a multi-class approach seems direct.\n\n# Let's try a simple pipeline: TF-IDF + Logistic Regression\n# We'll create features that represent the \"difference\" or \"comparison\" between A and B.\n# This is a simplified approach; more robust models would likely involve twin networks or more complex feature engineering.\n\n# A common strategy is to create pairs of (prompt, response_A) and (prompt, response_B) and learn a preference.\n# Another is to feed both into a model and have it output preference.\n\n# Let's simplify and just use the combined text 'combined_text_a' and 'combined_text_b'\n# and try to build features from their difference or concatenation.\n# For a multi-class classification, we can concatenate features from both responses.\n\n# Create a 'comparison' feature or represent both responses in a way the model can compare.\n# For a simple TF-IDF + Logistic Regression, we can create separate TF-IDF features and concatenate them.\n# Or, build a model where the input is structured for comparison.\n\n# Let's try a simpler approach: build a classifier for each response's quality, then combine.\n# This is not ideal for 'preference' but a starting point.\n# A better approach for preference: input (prompt, response_A, response_B) -> output (A_win, B_win, tie)\n\n# For a direct multi-class classification:\n# We need a unified input feature set that represents the comparison.\n# Let's create a single feature set by concatenating `combined_text_a` and `combined_text_b` with a separator.\n\ntrain_df['comparison_text'] = train_df['processed_prompt'] + \" [SEP] \" + \\\n                              train_df['processed_response_a'] + \" [SEP] \" + \\\n                              train_df['processed_response_b']\n\ntest_df['comparison_text'] = test_df['processed_prompt'] + \" [SEP] \" + \\\n                             test_df['processed_response_a'] + \" [SEP] \" + \\\n                             test_df['processed_response_b']\n\n\n# Define the model pipeline\n# Use TfidfVectorizer to convert text into numerical features\n# Then, use LogisticRegression for multi-class classification\nmodel_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))), # Experiment with max_features, ngram_range\n    ('classifier', LogisticRegression(random_state=42, solver='liblinear', multi_class='auto')) # You can try 'saga', 'lbfgs'\n])\n\nprint(\"\\nTraining the model...\")\nmodel_pipeline.fit(train_df['comparison_text'], y_train_encoded)\nprint(\"Model training complete.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:31:02.307292Z","iopub.execute_input":"2025-07-01T01:31:02.307556Z","iopub.status.idle":"2025-07-01T01:32:59.892588Z","shell.execute_reply.started":"2025-07-01T01:31:02.307533Z","shell.execute_reply":"2025-07-01T01:32:59.891513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Prediction\nprint(\"\\nGenerating predictions on the test set...\")\n# Predict probabilities for each class (model_a, model_b, tie)\n# The order of classes will be determined by LabelEncoder's internal sorting.\n# You need to map these back to 'winner_model_a', 'winner_model_b', 'winner_tie'.\npredicted_probabilities = model_pipeline.predict_proba(test_df['comparison_text'])\n\n# Get the class names in the order they were encoded\nencoded_classes = le.classes_\nprint(f\"Encoded classes order: {encoded_classes}\")\n\n# Create a DataFrame for predictions\n# Initialize with zeros\nsubmission_preds = pd.DataFrame(0.0, index=test_df['id'], columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n\n# Map probabilities back to the correct columns\n# Ensure 'model_a', 'model_b', 'tie' are correctly mapped.\n# This is crucial for the submission file.\nfor i, class_name in enumerate(encoded_classes):\n    if class_name == 'model_a':\n        submission_preds['winner_model_a'] = predicted_probabilities[:, i]\n    elif class_name == 'model_b':\n        submission_preds['winner_model_b'] = predicted_probabilities[:, i]\n    elif class_name == 'tie':\n        submission_preds['winner_tie'] = predicted_probabilities[:, i]\n\nprint(\"Prediction generation complete.\")\nprint(submission_preds.head())\n\n# Post-processing: Ensure probabilities sum to 1 (they should from predict_proba)\n# and handle potential edge cases if needed (e.g., very small probabilities).\n# For LogLoss, very small probabilities (close to 0) can cause issues.\n# The competition description mentions \"eps=auto\", which handles this.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:32:59.893669Z","iopub.execute_input":"2025-07-01T01:32:59.893937Z","iopub.status.idle":"2025-07-01T01:32:59.911637Z","shell.execute_reply.started":"2025-07-01T01:32:59.893916Z","shell.execute_reply":"2025-07-01T01:32:59.910422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Submission File Creation\nsubmission_df = test_df[['id']].copy()\nsubmission_df = submission_df.merge(submission_preds, left_on='id', right_index=True)\n\n# Ensure the columns are in the correct order as per the competition\nsubmission_df = submission_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# Save the submission file\nsubmission_file_path = 'submission.csv'\nsubmission_df.to_csv(submission_file_path, index=False)\n\nprint(f\"\\nSubmission file created at: {submission_file_path}\")\nprint(\"Submission DataFrame Head:\")\nprint(submission_df.head())\n\nprint(\"\\nKaggle Notebook script complete. You can now 'Save Version' and 'Submit' your notebook.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:32:59.914626Z","iopub.execute_input":"2025-07-01T01:32:59.914908Z","iopub.status.idle":"2025-07-01T01:32:59.959387Z","shell.execute_reply.started":"2025-07-01T01:32:59.914887Z","shell.execute_reply":"2025-07-01T01:32:59.958081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Incorrect line causing the error:\n# submission_df.to_csv(C:\\Users\\Hammad Farooq\\OneDrive\\Documents, index=False)\n\n# Correct line for Kaggle:\nsubmission_file_path = 'submission.csv'\nsubmission_df.to_csv(submission_file_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:32:59.960881Z","iopub.execute_input":"2025-07-01T01:32:59.961366Z","iopub.status.idle":"2025-07-01T01:32:59.974954Z","shell.execute_reply.started":"2025-07-01T01:32:59.961317Z","shell.execute_reply":"2025-07-01T01:32:59.973553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}