{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T05:38:15.415399Z","iopub.execute_input":"2025-09-29T05:38:15.415603Z","iopub.status.idle":"2025-09-29T05:38:17.964863Z","shell.execute_reply.started":"2025-09-29T05:38:15.415584Z","shell.execute_reply":"2025-09-29T05:38:17.963913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kaggle Notebook: LLM Classification Finetuning — working baseline\n# File: llm_classification_finetuning_kaggle_notebook.py\n# Instructions: Put this file into a Kaggle Notebook (Python) and run all cells.\n# Make sure the competition data (train.csv, test.csv) is present in the working directory\n# (Kaggle \"Data\" panel will mount them to /kaggle/input/llm-classification-finetuning/ by default).\n\n# =========================\n# Cell 1 — Imports\n# =========================\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import log_loss\n\n# =========================\n# Cell 2 — Paths & helper\n# =========================\nINPUT_DIR = '/kaggle/input/llm-classification-finetuning'\nif not os.path.exists(INPUT_DIR):\n    # sometimes kaggle mounts under /kaggle/input/<competition-name>\n    INPUT_DIR = '/kaggle/input/llm-classification-finetuning'\n\nTRAIN_CSV = os.path.join(INPUT_DIR, 'train.csv')\nTEST_CSV = os.path.join(INPUT_DIR, 'test.csv')\nSAMPLE_SUB = os.path.join(INPUT_DIR, 'sample_submission.csv')\n\nprint('Looking for files in:', INPUT_DIR)\nprint('Exists train:', os.path.exists(TRAIN_CSV))\nprint('Exists test:', os.path.exists(TEST_CSV))\n\n# =========================\n# Cell 3 — Load data\n# =========================\ntrain = pd.read_csv(TRAIN_CSV)\ntest = pd.read_csv(TEST_CSV)\nprint('train shape', train.shape)\nprint('test shape', test.shape)\nprint('Train columns:', list(train.columns))\n\n# =========================\n# Label extraction\n# =========================\nif all(c in train.columns for c in ['winner_model_a', 'winner_model_b', 'winner_tie']):\n    # Convert one-hot to single label\n    y = np.zeros(len(train), dtype=int)\n    y[train['winner_model_b'] == 1] = 1\n    y[train['winner_tie'] == 1] = 2\n\nelif 'winner' in train.columns:\n    # Map string labels to integers\n    mapping = {'a': 0, 'b': 1, 'tie': 2}\n    y = train['winner'].map(mapping).values\n\nelse:\n    raise ValueError(\n        f\"Could not find winner columns. Got: {list(train.columns)}\"\n    )\n\n# =========================\n# Cell 4 — Simple feature engineering\n# =========================\ntrain['text_a'] = train['prompt'].fillna('') + ' ' + train['response_a'].fillna('')\ntrain['text_b'] = train['prompt'].fillna('') + ' ' + train['response_b'].fillna('')\n\ntest['text_a'] = test['prompt'].fillna('') + ' ' + test['response_a'].fillna('')\ntest['text_b'] = test['prompt'].fillna('') + ' ' + test['response_b'].fillna('')\n\n# Helper transformers to extract columns for sklearn pipeline\nget_a = FunctionTransformer(lambda df: df['text_a'].values, validate=False)\nget_b = FunctionTransformer(lambda df: df['text_b'].values, validate=False)\nget_prompt = FunctionTransformer(lambda df: df['prompt'].fillna('').values, validate=False)\n\n# =========================\n# Cell 5 — Vectorizers & pipeline\n# =========================\nMAX_FEATURES = 20000\nvec = TfidfVectorizer(min_df=3, max_features=MAX_FEATURES, ngram_range=(1,2), sublinear_tf=True)\n\nfrom scipy import sparse\n\n# Fit vectorizer on combined corpus\nall_text = pd.concat([train['text_a'], train['text_b'], train['prompt']]).astype(str)\nvec.fit(all_text)\n\n# Transform train\nXa = vec.transform(train['text_a'].astype(str))\nXb = vec.transform(train['text_b'].astype(str))\nXp = vec.transform(train['prompt'].astype(str))\nX_train = sparse.hstack([Xa - Xb, Xp], format='csr')\n\n# Transform test\nXa_test = vec.transform(test['text_a'].astype(str))\nXb_test = vec.transform(test['text_b'].astype(str))\nXp_test = vec.transform(test['prompt'].astype(str))\nX_test = sparse.hstack([Xa_test - Xb_test, Xp_test], format='csr')\n\nprint('X_train shape', X_train.shape)\nprint('X_test shape', X_test.shape)\n\n# =========================\n# Cell 6 — Train/validation split and model\n# =========================\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y, test_size=0.12, random_state=42, stratify=y)\n\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=400, C=1.0)\nmodel.fit(X_tr, y_tr)\n\nprobs_val = model.predict_proba(X_val)\nvl = log_loss(y_val, probs_val)\nprint(f'Validation log loss: {vl:.6f}')\n\n# =========================\n# Cell 7 — Train on full training data\n# =========================\nmodel_full = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=400, C=1.0)\nmodel_full.fit(X_train, y)\n\n# =========================\n# Cell 8 — Predict on test and create submission.csv\n# =========================\nprobs_test = model_full.predict_proba(X_test)\n\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': probs_test[:, 0],\n    'winner_model_b': probs_test[:, 1],\n    'winner_tie': probs_test[:, 2]\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint('Wrote submission.csv — shape:', submission.shape)\nprint('Row 0 sum:', submission[['winner_model_a','winner_model_b','winner_tie']].iloc[0].sum())\nprint(submission.head())\n\n# End of notebook\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T05:42:39.371236Z","iopub.execute_input":"2025-09-29T05:42:39.371654Z","iopub.status.idle":"2025-09-29T05:47:49.954783Z","shell.execute_reply.started":"2025-09-29T05:42:39.371624Z","shell.execute_reply":"2025-09-29T05:47:49.952725Z"}},"outputs":[],"execution_count":null}]}