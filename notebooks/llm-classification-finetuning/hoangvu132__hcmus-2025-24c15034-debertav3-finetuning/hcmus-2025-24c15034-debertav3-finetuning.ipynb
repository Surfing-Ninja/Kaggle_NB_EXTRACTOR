{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":813452,"sourceType":"datasetVersion","datasetId":427411},{"sourceId":2839010,"sourceType":"datasetVersion","datasetId":1737128},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":343362,"sourceType":"modelInstanceVersion","modelInstanceId":287121,"modelId":307935}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:19:00.232242Z","iopub.execute_input":"2025-05-29T08:19:00.233088Z","iopub.status.idle":"2025-05-29T08:19:00.313627Z","shell.execute_reply.started":"2025-05-29T08:19:00.23305Z","shell.execute_reply":"2025-05-29T08:19:00.313066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom transformers import get_linear_schedule_with_warmup\nfrom datasets import Dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:19:00.314839Z","iopub.execute_input":"2025-05-29T08:19:00.315085Z","iopub.status.idle":"2025-05-29T08:19:00.319367Z","shell.execute_reply.started":"2025-05-29T08:19:00.31507Z","shell.execute_reply":"2025-05-29T08:19:00.318628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ENABLE_SUBMISSION = True  # Enable submission generation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:19:00.319989Z","iopub.execute_input":"2025-05-29T08:19:00.320166Z","iopub.status.idle":"2025-05-29T08:19:00.33594Z","shell.execute_reply.started":"2025-05-29T08:19:00.320152Z","shell.execute_reply":"2025-05-29T08:19:00.335354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"# model_name = '/kaggle/input/deberta-v3-small/transformers/default/1' V3 # current best\n# model_name = \"/kaggle/input/mdeberta-v3-base/mdeberta-v3-base\" V4 # 2nd best\n# model_name = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall\" # 3rd \nmodel_name = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-large\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel_path = \"./results/full_training\"\n\nEPOCH = 15\nFRAC = 1.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:19:00.336518Z","iopub.execute_input":"2025-05-29T08:19:00.336754Z","iopub.status.idle":"2025-05-29T08:19:02.70962Z","shell.execute_reply.started":"2025-05-29T08:19:00.336731Z","shell.execute_reply":"2025-05-29T08:19:02.709081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load and sample dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ndf = df.sample(frac=FRAC, random_state=42).reset_index(drop=True)\n\ndef map_labels(row):\n    if row[\"winner_model_a\"] == 1:\n        return 0\n    elif row[\"winner_model_b\"] == 1:\n        return 1\n    else:\n        return 2\n\ndf[\"labels\"] = df.apply(map_labels, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:19:02.711367Z","iopub.execute_input":"2025-05-29T08:19:02.711578Z","iopub.status.idle":"2025-05-29T08:19:06.42729Z","shell.execute_reply.started":"2025-05-29T08:19:02.711563Z","shell.execute_reply":"2025-05-29T08:19:06.426715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_val = train_test_split(df, test_size=0.1, stratify=df[\"labels\"], random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:19:06.427905Z","iopub.execute_input":"2025-05-29T08:19:06.428083Z","iopub.status.idle":"2025-05-29T08:19:06.463747Z","shell.execute_reply.started":"2025-05-29T08:19:06.428069Z","shell.execute_reply":"2025-05-29T08:19:06.463186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"def tokenize_batch(batch):\n    text_a = [f\"Prompt: {p} Response: {r}\" for p, r in zip(batch[\"prompt\"], batch[\"response_a\"])]\n    text_b = [f\"Prompt: {p} Response: {r}\" for p, r in zip(batch[\"prompt\"], batch[\"response_b\"])]\n    return tokenizer(text_a, text_b, padding=\"max_length\", truncation=True, max_length=256)\n\ntrain_dataset = Dataset.from_pandas(df_train).map(tokenize_batch, batched=True, batch_size=500)\nval_dataset = Dataset.from_pandas(df_val).map(tokenize_batch, batched=True, batch_size=500)\n\ndef clean(ds):\n    return ds.remove_columns([col for col in ds.column_names if col not in [\"input_ids\", \"attention_mask\", \"labels\"]]).with_format(\"torch\")\n\ntrain_dataset = clean(train_dataset)\nval_dataset = clean(val_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T08:19:06.464448Z","iopub.execute_input":"2025-05-29T08:19:06.46469Z","execution_failed":"2025-05-29T08:19:07.104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"eval_f1\": f1}\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-29T08:19:07.104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-29T08:19:07.105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=model_path,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=EPOCH,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=32,\n    learning_rate=5e-6,\n    weight_decay=0.01,\n    save_total_limit=2,\n    logging_dir=f\"./logs/full_training\",\n    logging_steps=10,\n    fp16=True,\n    report_to=\"none\",\n    remove_unused_columns=False,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\",\n    greater_is_better=True,\n    label_smoothing_factor=0.1,\n    lr_scheduler_type=\"cosine\", \n    warmup_ratio=0.1,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\nmodel.config.classifier_dropout_prob = 0.5\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-29T08:19:07.105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(model_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-29T08:19:07.105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if ENABLE_SUBMISSION:\n    test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n    test_ids = test_df[\"id\"]\n    test_dataset = Dataset.from_pandas(test_df).map(tokenize_batch, batched=True)\n    test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if col not in [\"input_ids\", \"attention_mask\"]]).with_format(\"torch\")\n    test_logits = trainer.predict(test_dataset).predictions\n    test_probs = torch.softmax(torch.tensor(test_logits), dim=-1).cpu().numpy()\n    submission = pd.DataFrame({\n        \"id\": test_ids,\n        \"winner_model_a\": test_probs[:, 0].round(6),\n        \"winner_model_b\": test_probs[:, 1].round(6),\n        \"winner_tie\": test_probs[:, 2].round(6)\n    })\n    submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n    print(\"âœ… Submission file saved: submission.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-29T08:19:07.105Z"}},"outputs":[],"execution_count":null}]}