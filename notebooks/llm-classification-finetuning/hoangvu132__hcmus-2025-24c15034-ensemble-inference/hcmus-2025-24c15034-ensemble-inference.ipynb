{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":8926343,"sourceType":"datasetVersion","datasetId":5369301},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10150018,"sourceType":"datasetVersion","datasetId":6265978},{"sourceId":10214229,"sourceType":"datasetVersion","datasetId":6267026},{"sourceId":10236316,"sourceType":"datasetVersion","datasetId":6266300},{"sourceId":10302322,"sourceType":"datasetVersion","datasetId":6265823},{"sourceId":75103,"sourceType":"modelInstanceVersion","modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Note: Changes from original notebook: remove Sentence Transformer model, only use LLMs for final result\n","metadata":{}},{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-06-24T07:40:41.392012Z","iopub.execute_input":"2025-06-24T07:40:41.392664Z","iopub.status.idle":"2025-06-24T07:41:30.930964Z","shell.execute_reply.started":"2025-06-24T07:40:41.392619Z","shell.execute_reply":"2025-06-24T07:41:30.929777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-06-24T07:41:30.933399Z","iopub.execute_input":"2025-06-24T07:41:30.934309Z","iopub.status.idle":"2025-06-24T07:42:19.279309Z","shell.execute_reply.started":"2025-06-24T07:41:30.934268Z","shell.execute_reply":"2025-06-24T07:42:19.278315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"execution":{"iopub.status.busy":"2025-06-24T07:42:19.280494Z","iopub.execute_input":"2025-06-24T07:42:19.280734Z","iopub.status.idle":"2025-06-24T07:42:20.44901Z","shell.execute_reply.started":"2025-06-24T07:42:19.280711Z","shell.execute_reply":"2025-06-24T07:42:20.448045Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:42:20.451735Z","iopub.execute_input":"2025-06-24T07:42:20.452506Z","iopub.status.idle":"2025-06-24T07:42:21.089411Z","shell.execute_reply.started":"2025-06-24T07:42:20.452472Z","shell.execute_reply":"2025-06-24T07:42:21.088665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile prepare_test_file.py\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\ndf.to_parquet(\"test.parquet\", index=False)\n\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\ndf.to_parquet(\"test_swap.parquet\", index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-06-24T07:42:21.090642Z","iopub.execute_input":"2025-06-24T07:42:21.090888Z","iopub.status.idle":"2025-06-24T07:42:21.096601Z","shell.execute_reply.started":"2025-06-24T07:42:21.090868Z","shell.execute_reply":"2025-06-24T07:42:21.095822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python prepare_test_file.py","metadata":{"execution":{"iopub.status.busy":"2025-06-24T07:42:21.097542Z","iopub.execute_input":"2025-06-24T07:42:21.097769Z","iopub.status.idle":"2025-06-24T07:42:23.155009Z","shell.execute_reply.started":"2025-06-24T07:42:21.097751Z","shell.execute_reply":"2025-06-24T07:42:23.153991Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m0.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"\ncsv_path = \"test.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=False,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 42\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.head_dim\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m0.npy', prob)","metadata":{"execution":{"iopub.status.busy":"2025-06-24T07:42:23.156863Z","iopub.execute_input":"2025-06-24T07:42:23.15738Z","iopub.status.idle":"2025-06-24T07:42:23.165225Z","shell.execute_reply.started":"2025-06-24T07:42:23.157336Z","shell.execute_reply":"2025-06-24T07:42:23.164231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python predict_m0.py","metadata":{"execution":{"iopub.status.busy":"2025-06-24T07:42:23.166522Z","iopub.execute_input":"2025-06-24T07:42:23.167022Z","iopub.status.idle":"2025-06-24T07:48:13.867877Z","shell.execute_reply.started":"2025-06-24T07:42:23.167001Z","shell.execute_reply":"2025-06-24T07:48:13.867019Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m3.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\ncsv_path = \"test_swap.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\ntokenizer.deprecation_warnings[\n    \"sequence-length-is-longer-than-the-specified-maximum\"\n] = True\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=True,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 32\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = LlamaForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m3.npy', prob)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:13.869543Z","iopub.execute_input":"2025-06-24T07:48:13.869909Z","iopub.status.idle":"2025-06-24T07:48:13.877627Z","shell.execute_reply.started":"2025-06-24T07:48:13.869873Z","shell.execute_reply":"2025-06-24T07:48:13.876575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python predict_m3.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:13.879771Z","iopub.execute_input":"2025-06-24T07:48:13.880018Z","iopub.status.idle":"2025-06-24T07:53:06.060786Z","shell.execute_reply.started":"2025-06-24T07:48:13.879998Z","shell.execute_reply":"2025-06-24T07:53:06.059628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprob = np.load('prob_m3.npy')\n\nprint(prob[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:53:06.062206Z","iopub.execute_input":"2025-06-24T07:53:06.062493Z","iopub.status.idle":"2025-06-24T07:53:06.068559Z","shell.execute_reply.started":"2025-06-24T07:53:06.062453Z","shell.execute_reply":"2025-06-24T07:53:06.067781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\ndf = pd.read_parquet(\"test.parquet\")\n\n\nprob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\nprob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3 (swap response_a and response_b)\n\n# Combine predictions with weights\n# Adjust weights as needed for optimal performance\npreds = np.average(\n    [\n        prob_m0,       # Gemma2 results\n        prob_m3,       # Llama3 results\n    ],\n    axis=0,\n    weights=[0.57, 0.43]  # Weights for each model\n)\n\n# Create submission DataFrame\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\n\n# Save to CSV\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:53:06.069782Z","iopub.execute_input":"2025-06-24T07:53:06.070306Z","iopub.status.idle":"2025-06-24T07:53:06.964775Z","shell.execute_reply.started":"2025-06-24T07:53:06.070271Z","shell.execute_reply":"2025-06-24T07:53:06.963767Z"}},"outputs":[],"execution_count":null}]}