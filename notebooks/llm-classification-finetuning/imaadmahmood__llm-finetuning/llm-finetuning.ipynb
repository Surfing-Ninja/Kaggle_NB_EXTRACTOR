{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-29T07:31:49.14629Z","iopub.execute_input":"2025-07-29T07:31:49.146541Z","iopub.status.idle":"2025-07-29T07:31:51.588672Z","shell.execute_reply.started":"2025-07-29T07:31:49.146512Z","shell.execute_reply":"2025-07-29T07:31:51.587871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the datasets\ntrain_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# =============================\n# üîç Basic Preview of the Data\n# =============================\nprint(\"‚úÖ Preview of Train Data:\\n\", train_df.head())\nprint(\"\\n‚úÖ Preview of Test Data:\\n\", test_df.head())\n\n# =============================\n# üîç Null Value Check\n# =============================\nprint(\"\\nüßº Null Values in Train:\\n\", train_df.isnull().sum())\nprint(\"\\nüßº Null Values in Test:\\n\", test_df.isnull().sum())\n\n# =============================\n# üîç Column Names\n# =============================\nprint(\"\\nüßæ Columns in Train:\", train_df.columns.tolist())\nprint(\"\\nüßæ Columns in Test:\", test_df.columns.tolist())\n\n# =============================\n# üîç Shape of the Data\n# =============================\nprint(\"\\nüî¢ Shape of Train:\", train_df.shape)\nprint(\"üî¢ Shape of Test:\", test_df.shape)\n\n# =============================\n# üîç Data Types\n# =============================\nprint(\"\\nüìå Data Types in Train:\\n\", train_df.dtypes)\nprint(\"\\nüìå Data Types in Test:\\n\", test_df.dtypes)\n\n# =============================\n# üîç Statistical Summary (Numerical Columns Only)\n# =============================\nprint(\"\\nüìä Statistical Summary of Train:\\n\", train_df.describe())\nprint(\"\\nüìä Statistical Summary of Test:\\n\", test_df.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T07:43:25.699734Z","iopub.execute_input":"2025-07-29T07:43:25.700326Z","iopub.status.idle":"2025-07-29T07:43:27.420962Z","shell.execute_reply.started":"2025-07-29T07:43:25.7003Z","shell.execute_reply":"2025-07-29T07:43:27.420177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# LLM PREFERENCE PREDICTION - OPTIMIZED COMPETITION SOLUTION\n# ================================================================================\n# Install required libraries:\n# pip install pandas numpy matplotlib seaborn scikit-learn lightgbm xgboost catboost\n# ================================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# ================================================================================\n# ------------------ OPTIMIZED LLM PREFERENCE PREDICTOR CLASS --------------------\n# ================================================================================\n\nclass OptimizedLLMPredictor:\n    def __init__(self):\n        \"\"\"Initialize the predictor with optimized settings\"\"\"\n        self.label_encoders = {}\n        self.scaler = StandardScaler()\n        self.models = {}\n        self.feature_names = []\n        \n        print(\"üöÄ OptimizedLLMPredictor initialized!\")\n        print(\"=\" * 60)\n    \n    # ============================================================================\n    # ----------------------- FAST FEATURE ENGINEERING ---------------------------\n    # ============================================================================\n    \n    def extract_fast_features(self, df, is_train=True):\n        \"\"\"Extract optimized features with focus on speed and performance\"\"\"\n        \n        print(\"üîß Starting feature engineering...\")\n        features = df[['id']].copy()\n        \n        # ========================================================================\n        # ------------------- Core Text Statistics (Vectorized) ------------------\n        # ========================================================================\n        \n        for resp in ['response_a', 'response_b']:\n            text_col = df[resp].astype(str)\n            \n            # Basic length metrics\n            features[f'{resp}_len'] = text_col.str.len()\n            features[f'{resp}_words'] = text_col.str.split().str.len()\n            features[f'{resp}_sentences'] = text_col.str.count(r'[.!?]+')\n            \n            # Advanced metrics\n            features[f'{resp}_avg_word_len'] = features[f'{resp}_len'] / (features[f'{resp}_words'] + 1)\n            features[f'{resp}_punct_ratio'] = text_col.str.count(r'[^\\w\\s]') / (features[f'{resp}_len'] + 1)\n            features[f'{resp}_upper_ratio'] = text_col.str.count(r'[A-Z]') / (features[f'{resp}_len'] + 1)\n            \n            # Structure indicators\n            features[f'{resp}_newlines'] = text_col.str.count(r'\\n')\n            features[f'{resp}_code_blocks'] = text_col.str.count(r'```')\n            features[f'{resp}_bullets'] = text_col.str.count(r'^\\s*[-*‚Ä¢]\\s', flags=re.MULTILINE)\n            features[f'{resp}_numbers'] = text_col.str.count(r'^\\s*\\d+\\.\\s', flags=re.MULTILINE)\n            \n            # Quality indicators\n            features[f'{resp}_questions'] = text_col.str.count(r'\\?')\n            features[f'{resp}_exclamations'] = text_col.str.count(r'!')\n            \n        print(\"‚úÖ Basic text features extracted\")\n        \n        # =====================================================================\n        # ----------------------- Prompt Analysis -----------------------------\n        # =====================================================================\n        \n        prompt_text = df['prompt'].astype(str)\n        features['prompt_len'] = prompt_text.str.len()\n        features['prompt_words'] = prompt_text.str.split().str.len()\n        features['prompt_questions'] = prompt_text.str.count(r'\\?')\n        \n        print(\"‚úÖ Prompt features extracted\")\n        \n        # ====================================================================\n        # ----------- Comparative Features (Key for Performance) -------------\n        # ====================================================================\n        \n        # Length comparisons\n        features['len_ratio_a_b'] = features['response_a_len'] / (features['response_b_len'] + 1)\n        features['len_diff_a_b'] = features['response_a_len'] - features['response_b_len']\n        features['word_ratio_a_b'] = features['response_a_words'] / (features['response_b_words'] + 1)\n        features['word_diff_a_b'] = features['response_a_words'] - features['response_b_words']\n        \n        # Quality comparisons  \n        features['struct_diff_a_b'] = (features['response_a_bullets'] + features['response_a_numbers']) - \\\n                                      (features['response_b_bullets'] + features['response_b_numbers'])\n        \n        features['engagement_diff_a_b'] = (features['response_a_questions'] + features['response_a_exclamations']) - \\\n                                          (features['response_b_questions'] + features['response_b_exclamations'])\n        \n        print(\"‚úÖ Comparative features extracted\")\n        \n        # ===================================================================\n        # ----- Model Performance Features (Consistent for Train/Test) ------\n        # ===================================================================\n        \n        if is_train and 'model_a' in df.columns:\n            # Fast model encoding\n            for model_col in ['model_a', 'model_b']:\n                if model_col not in self.label_encoders:\n                    self.label_encoders[model_col] = LabelEncoder()\n                    features[f'{model_col}_id'] = self.label_encoders[model_col].fit_transform(df[model_col])\n                else:\n                    features[f'{model_col}_id'] = self.label_encoders[model_col].transform(df[model_col])\n            \n            # Quick model stats\n            model_wins = df.groupby('model_a')['winner_model_a'].mean()\n            model_wins_b = df.groupby('model_b')['winner_model_b'].mean()\n            \n            features['model_a_win_rate'] = df['model_a'].map(model_wins).fillna(0.33)\n            features['model_b_win_rate'] = df['model_b'].map(model_wins_b).fillna(0.33)\n            \n            # Store model stats for test prediction\n            self.model_a_stats = model_wins.to_dict()\n            self.model_b_stats = model_wins_b.to_dict()\n            \n            print(\"‚úÖ Model features extracted\")\n        \n        elif not is_train:\n            # For test data, create dummy model features to maintain consistency\n            features['model_a_id'] = 0  # Default encoding for unknown models\n            features['model_b_id'] = 0  # Default encoding for unknown models\n            features['model_a_win_rate'] = 0.33  # Default win rate\n            features['model_b_win_rate'] = 0.33  # Default win rate\n            \n            print(\"‚úÖ Dummy model features added for test consistency\")\n        \n        # ==============================================================\n        # ------------ Text Similarity (Fast Version) ------------------\n        # ==============================================================\n        \n        def fast_word_overlap(row):\n            words_a = set(str(row['response_a']).lower().split())\n            words_b = set(str(row['response_b']).lower().split())\n            if len(words_a) == 0 or len(words_b) == 0:\n                return 0\n            return len(words_a & words_b) / len(words_a | words_b)\n        \n        features['word_overlap'] = df.apply(fast_word_overlap, axis=1)\n        \n        print(\"‚úÖ Similarity features extracted\")\n        \n        # Fill missing values and return\n        features = features.fillna(0)\n        self.feature_names = [col for col in features.columns if col != 'id']\n        \n        print(f\"üî• Feature engineering completed! Total features: {len(self.feature_names)}\")\n        print(\"=\" * 60)\n        \n        return features\n    \n    # ============================================================================\n    # -------------------- OPTIMIZED MODEL TRAINING ------------------------------\n    # ============================================================================\n    \n    def train_optimized_ensemble(self, X, y):\n        \"\"\"Train optimized ensemble with focus on speed and performance\"\"\"\n        \n        print(\"ü§ñ Training optimized ensemble models...\")\n        print(\"=\" * 60)\n        \n        # Convert to multiclass format\n        y_multiclass = np.argmax(y.values, axis=1)\n        \n        # ========================================\n        # Model 1: LightGBM (Primary Model)\n        # ========================================\n        print(\"üöÄ Training LightGBM (Primary Model)...\")\n        self.models['lgb'] = lgb.LGBMClassifier(\n            n_estimators=800,\n            learning_rate=0.08,\n            max_depth=6,\n            num_leaves=31,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            random_state=42,\n            objective='multiclass',\n            num_class=3,\n            verbose=-1,\n            force_col_wise=True  # Optimization for speed\n        )\n        self.models['lgb'].fit(X, y_multiclass)\n        print(\"‚úÖ LightGBM training completed!\")\n        \n        # ========================================\n        # Model 2: XGBoost (Secondary Model)\n        # ========================================\n        print(\"üöÄ Training XGBoost...\")\n        self.models['xgb'] = xgb.XGBClassifier(\n            n_estimators=600,\n            learning_rate=0.08,\n            max_depth=6,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            random_state=42,\n            objective='multi:softprob',\n            eval_metric='mlogloss',\n            verbosity=0,\n            tree_method='hist'  # Faster training\n        )\n        self.models['xgb'].fit(X, y_multiclass)\n        print(\"‚úÖ XGBoost training completed!\")\n        \n        # ========================================\n        # Model 3: CatBoost (Robust Model)\n        # ========================================\n        print(\"üöÄ Training CatBoost...\")\n        self.models['catboost'] = CatBoostClassifier(\n            iterations=500,\n            learning_rate=0.1,\n            depth=6,\n            random_state=42,\n            verbose=False,\n            loss_function='MultiClass',\n            task_type='CPU'  # Explicit CPU usage\n        )\n        self.models['catboost'].fit(X, y_multiclass)\n        print(\"‚úÖ CatBoost training completed!\")\n        \n        # ========================================  \n        # Ensemble Weights (Optimized)\n        # ========================================\n        self.ensemble_weights = {\n            'lgb': 0.5,      # Primary model\n            'xgb': 0.35,     # Strong secondary\n            'catboost': 0.15 # Robustness\n        }\n        \n        print(\"üéØ Ensemble training completed!\")\n        print(\"=\" * 60)\n    \n    # ============================================================================\n    # --------------------- PREDICTION AND VALIDATION ----------------------------\n    # ============================================================================\n    \n    def predict_optimized(self, X):\n        \"\"\"Make optimized ensemble predictions with feature validation\"\"\"\n        \n        print(\"üîÆ Making ensemble predictions...\")\n        \n        # Validate feature consistency\n        if hasattr(self, 'feature_names'):\n            if X.shape[1] != len(self.feature_names):\n                print(f\"‚ö†Ô∏è Feature mismatch detected!\")\n                print(f\"Expected: {len(self.feature_names)} features\")\n                print(f\"Received: {X.shape[1]} features\")\n                \n                # Ensure X has the same columns as training\n                if isinstance(X, pd.DataFrame):\n                    missing_features = set(self.feature_names) - set(X.columns)\n                    extra_features = set(X.columns) - set(self.feature_names)\n                    \n                    if missing_features:\n                        print(f\"Adding missing features: {missing_features}\")\n                        for feature in missing_features:\n                            X[feature] = 0  # Default value for missing features\n                    \n                    if extra_features:\n                        print(f\"Removing extra features: {extra_features}\")\n                        X = X.drop(columns=list(extra_features))\n                    \n                    # Reorder columns to match training\n                    X = X[self.feature_names]\n                \n        print(f\"‚úÖ Feature validation completed. Shape: {X.shape}\")\n        \n        predictions = np.zeros((X.shape[0], 3))\n        \n        for name, model in self.models.items():\n            pred = model.predict_proba(X)\n            predictions += self.ensemble_weights[name] * pred\n            print(f\"‚úÖ {name} predictions added (weight: {self.ensemble_weights[name]})\")\n        \n        print(\"üéØ Ensemble predictions completed!\")\n        print(\"=\" * 60)\n        return predictions\n    \n    def quick_validation(self, X, y, n_splits=3):\n        \"\"\"Quick cross-validation for performance check\"\"\"\n        \n        print(\"üìä Running quick validation...\")\n        print(\"=\" * 60)\n        \n        y_multiclass = np.argmax(y.values, axis=1)\n        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        \n        # Only validate primary model for speed\n        model = self.models['lgb']\n        scores = []\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_multiclass)):\n            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_fold_train, y_fold_val = y_multiclass[train_idx], y_multiclass[val_idx]\n            \n            # Quick training\n            fold_model = lgb.LGBMClassifier(\n                n_estimators=200, learning_rate=0.1, max_depth=6, \n                random_state=42, verbose=-1, objective='multiclass', num_class=3\n            )\n            fold_model.fit(X_fold_train, y_fold_train)\n            \n            # Prediction and scoring\n            pred_proba = fold_model.predict_proba(X_fold_val)\n            y_val_onehot = np.eye(3)[y_fold_val]\n            score = log_loss(y_val_onehot, pred_proba)\n            scores.append(score)\n            \n            print(f\"üìà Fold {fold+1} Log Loss: {score:.4f}\")\n        \n        avg_score = np.mean(scores)\n        std_score = np.std(scores)\n        \n        print(f\"üéØ Average CV Score: {avg_score:.4f} (+/- {std_score:.4f})\")\n        print(\"=\" * 60)\n        \n        return avg_score\n\n# ================================================================================\n# ------------------------ MAIN EXECUTION PIPELINE -------------------------------\n# ================================================================================\n\ndef main():\n    \"\"\"Optimized main pipeline for maximum performance\"\"\"\n    \n    print(\"üèÅ STARTING LLM PREFERENCE PREDICTION PIPELINE\")\n    print(\"=\" * 80)\n    \n    # ============================================================================\n    # ------------------------------ DATA LOADING --------------------------------\n    # ============================================================================\n    \n    print(\"üìÇ Loading competition data...\")\n    train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n    test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n    sample_submission = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n    \n    print(f\"üìä Train shape: {train_df.shape}\")\n    print(f\"üìä Test shape: {test_df.shape}\")\n    print(\"=\" * 60)\n    \n    # ============================================================================\n    # --------------------- MODEL INITIALIZATION --------------------------------\n    # ============================================================================\n    \n    predictor = OptimizedLLMPredictor()\n    \n    # ============================================================================\n    # ------------------------- FEATURE ENGINEERING ------------------------------\n    # ============================================================================\n    \n    print(\"‚ö° Processing training features...\")\n    train_features = predictor.extract_fast_features(train_df, is_train=True)\n    X_train = train_features.drop(['id'], axis=1)\n    y_train = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].copy()\n    \n    # Store feature names for consistency\n    predictor.feature_names = list(X_train.columns)\n    \n    print(f\"üî¢ Final training shape: {X_train.shape}\")\n    print(f\"üìã Feature names stored: {len(predictor.feature_names)} features\")\n    print(\"=\" * 60)\n    \n    # ============================================================================\n    # -------------------------- MODEL TRAINING ----------------------------------\n    # ============================================================================\n    \n    predictor.train_optimized_ensemble(X_train, y_train)\n    \n    # ============================================================================\n    # ---------------------------- QUICK VALIDATION ------------------------------\n    # ============================================================================\n    \n    try:\n        cv_score = predictor.quick_validation(X_train, y_train)\n        performance_indicator = \"üî• EXCELLENT\" if cv_score < 1.05 else \"‚úÖ GOOD\" if cv_score < 1.10 else \"‚ö†Ô∏è NEEDS IMPROVEMENT\"\n        print(f\"üéØ Model Performance: {performance_indicator}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Validation skipped: {str(e)}\")\n    \n    print(\"=\" * 60)\n    \n    # ============================================================================\n    # ----------------------- TEST PREDICTIONS -----------------------------------\n    # ============================================================================\n    \n    print(\"üîÆ Processing test data...\")\n    test_features = predictor.extract_fast_features(test_df, is_train=False)\n    X_test = test_features.drop(['id'], axis=1)\n    \n    print(f\"üìä Test features shape: {X_test.shape}\")\n    print(f\"üìã Expected shape: ({X_test.shape[0]}, {len(predictor.feature_names)})\")\n    \n    # Make predictions with feature validation\n    predictions = predictor.predict_optimized(X_test)\n    \n    # ============================================================================\n    # ------------------------ SUBMISSION CREATION ------------------------------\n    # ============================================================================\n    \n    print(\"üìù Creating optimized submission...\")\n    \n    submission = pd.DataFrame({\n        'id': test_df['id'],\n        'winner_model_a': predictions[:, 0],\n        'winner_model_b': predictions[:, 1], \n        'winner_tie': predictions[:, 2]\n    })\n    \n    # Normalize probabilities to ensure they sum to 1\n    prob_sums = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n    submission[['winner_model_a', 'winner_model_b', 'winner_tie']] = \\\n        submission[['winner_model_a', 'winner_model_b', 'winner_tie']].div(prob_sums, axis=0)\n    \n    # Save submission\n    submission.to_csv('submission.csv', index=False)\n    \n    print(\"‚úÖ Submission saved as 'submission.csv'\")\n    print(\"=\" * 60)\n    \n    # ============================================================================\n    # --------------------- FINAL VALIDATION AND SUMMARY ------------------------\n    # ============================================================================\n    \n    print(\"üìã SUBMISSION SUMMARY\")\n    print(\"=\" * 60)\n    print(f\"üìä Shape: {submission.shape}\")\n    print(f\"üìã Columns: {list(submission.columns)}\")\n    \n    # Check probability distributions\n    prob_stats = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].describe()\n    print(\"\\nüìà Probability Distributions:\")\n    print(prob_stats.round(4))\n    \n    print(\"\\nüéØ Sample Predictions:\")\n    print(submission.head().round(4))\n    \n    # Final validation\n    prob_sums_check = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n    print(f\"\\n‚úÖ Probability sums (should be ~1.0): Min={prob_sums_check.min():.6f}, Max={prob_sums_check.max():.6f}\")\n    \n    print(\"=\" * 80)\n    print(\"üèÜ PIPELINE COMPLETED SUCCESSFULLY!\")\n    print(\"üöÄ Ready for submission to leaderboard!\")\n    print(\"=\" * 80)\n    \n    return predictor, submission\n\n# ================================================================================\n# ------------------------------- EXECUTION --------------------------------------\n# ================================================================================\n\nif __name__ == \"__main__\":\n    predictor, submission = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T07:35:41.585843Z","iopub.execute_input":"2025-07-29T07:35:41.586174Z","iopub.status.idle":"2025-07-29T07:36:46.42107Z","shell.execute_reply.started":"2025-07-29T07:35:41.586132Z","shell.execute_reply":"2025-07-29T07:36:46.420301Z"}},"outputs":[],"execution_count":null}]}