{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Introduction\n\nIn this competition, my task is to predict which response in a head-to-head chatbot battle will be preferred by users. I am provided with three CSV files:\n\n-   **train.csv**: Contains conversation data (including prompts and two responses) and one-hot encoded labels indicating which response won (or if it was a tie).\n-   **test.csv**: Contains similar conversation data (without labels) for which I need to predict the probabilities.\n-   **sample_submission.csv**: Provides the required submission format.\n\nMy approach is based on text analysis. First, I parse the JSON-formatted text stored in columns (such as `\"prompt\"`, `\"response_a\"`, and `\"response_b\"`) into human-readable text. Then, I create combined texts by concatenating the prompt with each response and represent them numerically using a TF‑IDF vectorizer. The key idea is to use the difference between the TF‑IDF vectors of the two combined texts as features, and then train a multinomial logistic regression model to predict one of three classes:\n\n-   **0**: Response A wins.\n-   **1**: Response B wins.\n-   **2**: Tie.\n\nIn the following sections, I explore the data, build and evaluate my model, visualize interesting relationships, and summarize my findings.\n\n----------\n\n## 1. Data Loading & Basic Exploration\n\nIn this section, I load the datasets, parse the JSON strings in the text fields, and display key information about the data.","metadata":{}},{"cell_type":"code","source":"#%% [code]\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Load the CSV files from the input folder\ntrain_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\n\n# Helper function to parse JSON strings (they appear as a one-item list)\ndef extract_text(json_str):\n    try:\n        parsed = json.loads(json_str)\n        if isinstance(parsed, list) and len(parsed) > 0:\n            return parsed[0]\n        else:\n            return parsed\n    except Exception:\n        return json_str\n\n# Create parsed text columns for prompt and both responses\nfor col in ['prompt', 'response_a', 'response_b']:\n    train_df[col + '_text'] = train_df[col].apply(extract_text)\n    test_df[col + '_text'] = test_df[col].apply(extract_text)\n\n# Function to display basic information about a DataFrame\ndef print_data_info(name, df):\n    print(f\"\\n--- {name} Data ---\")\n    print(\"Shape:\", df.shape)\n    print(\"Columns:\", df.columns.tolist())\n    print(\"\\nData Types:\")\n    print(df.dtypes)\n    print(\"\\nMissing Values:\")\n    print(df.isnull().sum())\n    print(\"\\nFirst 5 Rows:\")\n    print(df.head())\n\n# Display basic info about each dataset\nprint_data_info(\"Train\", train_df)\nprint_data_info(\"Test\", test_df)\nprint_data_info(\"Sample Submission\", submission_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:15:37.731543Z","iopub.execute_input":"2025-02-03T14:15:37.73213Z","iopub.status.idle":"2025-02-03T14:15:40.660613Z","shell.execute_reply.started":"2025-02-03T14:15:37.732092Z","shell.execute_reply":"2025-02-03T14:15:40.659599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n_Output summary (abridged):_\n\n-   **Train Data**: 57,477 rows and 12 columns.\n-   The parsed text columns (`prompt_text`, `response_a_text`, `response_b_text`) look good, with only a few missing values in the response texts.\n-   **Test Data** and **Submission Data** also show their respective shapes and types.\n\n----------\n\n## 2. Data Visualization\n\nTo understand my data better, I visualized several aspects including text length distributions, model frequency counts, and even relationships between text lengths. This helps me gain insights into the structure and quality of the data.\n\n### 2.1 Visualizing Text Lengths\n\nI calculate the lengths of the prompt and response texts and then display summary statistics and histograms.","metadata":{}},{"cell_type":"code","source":"#%% [code]\nimport matplotlib.pyplot as plt\n\n# Calculate text lengths for prompt, response_a, and response_b in training data\nfor col in ['prompt_text', 'response_a_text', 'response_b_text']:\n    train_df[col + '_length'] = train_df[col].apply(lambda x: len(x) if isinstance(x, str) else 0)\n    print(f\"\\nSummary for {col} lengths:\")\n    print(train_df[col + '_length'].describe())\n\n# Plot histograms for text lengths\nplt.figure(figsize=(15, 4))\nfor i, col in enumerate(['prompt_text_length', 'response_a_text_length', 'response_b_text_length']):\n    plt.subplot(1, 3, i+1)\n    train_df[col].hist(bins=50)\n    plt.title(col)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:16:07.981982Z","iopub.execute_input":"2025-02-03T14:16:07.982438Z","iopub.status.idle":"2025-02-03T14:16:09.356911Z","shell.execute_reply.started":"2025-02-03T14:16:07.982405Z","shell.execute_reply":"2025-02-03T14:16:09.355924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n_Additional insight:_ I also compute the correlation between the text lengths (prompt, response_a, response_b) to see if there are any interesting relationships.\n","metadata":{}},{"cell_type":"code","source":"#%% [code]\n# Compute correlation matrix for text length features\nlength_cols = ['prompt_text_length', 'response_a_text_length', 'response_b_text_length']\ncorr_matrix = train_df[length_cols].corr()\nprint(\"Correlation Matrix of Text Lengths:\")\nprint(corr_matrix)\n\nimport seaborn as sns\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\nplt.title(\"Heatmap of Text Length Correlations\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:16:33.921628Z","iopub.execute_input":"2025-02-03T14:16:33.921991Z","iopub.status.idle":"2025-02-03T14:16:34.193124Z","shell.execute_reply.started":"2025-02-03T14:16:33.92196Z","shell.execute_reply":"2025-02-03T14:16:34.191946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 2.2 Frequency of Models\n\nI check the frequency counts of different LLM models used in the data and visualize the top 10 for each role (model A and model B).","metadata":{}},{"cell_type":"code","source":"#%% [code]\nprint(\"\\nModel A Frequency:\")\nprint(train_df['model_a'].value_counts().head(10))\nprint(\"\\nModel B Frequency:\")\nprint(train_df['model_b'].value_counts().head(10))\n\n# Visualize these counts using bar charts\ntrain_df['model_a'].value_counts().head(10).plot(kind=\"bar\", title=\"Top 10 Models (A)\", figsize=(8,4))\nplt.ylabel(\"Count\")\nplt.show()\n\ntrain_df['model_b'].value_counts().head(10).plot(kind=\"bar\", title=\"Top 10 Models (B)\", figsize=(8,4))\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:16:53.210183Z","iopub.execute_input":"2025-02-03T14:16:53.210549Z","iopub.status.idle":"2025-02-03T14:16:53.698143Z","shell.execute_reply.started":"2025-02-03T14:16:53.210519Z","shell.execute_reply":"2025-02-03T14:16:53.696854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 2.3 Word Cloud for Prompts\n\nTo get a feel for common words in the prompt texts, I create a word cloud.","metadata":{}},{"cell_type":"code","source":"#%% [code]\nfrom wordcloud import WordCloud\n\n# Combine all prompt_texts into one string\nall_prompts = \" \".join(train_df[\"prompt_text\"].dropna().tolist())\n\nwordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(all_prompts)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"Word Cloud of Prompt Texts\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:17:09.124295Z","iopub.execute_input":"2025-02-03T14:17:09.124651Z","iopub.status.idle":"2025-02-03T14:17:20.447727Z","shell.execute_reply.started":"2025-02-03T14:17:09.12462Z","shell.execute_reply":"2025-02-03T14:17:20.44664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 3. Feature Engineering & Label Creation\n\nI prepare my features by combining the prompt with each response and then computing the difference between their TF‑IDF vectors. I also create a target label from the one‑hot winner columns.","metadata":{}},{"cell_type":"code","source":"#%% [code]\n# Combine prompt with each response to create text_a and text_b\nfor df in [train_df, test_df]:\n    df[\"text_a\"] = df[\"prompt_text\"].fillna(\"\") + \" \" + df[\"response_a_text\"].fillna(\"\")\n    df[\"text_b\"] = df[\"prompt_text\"].fillna(\"\") + \" \" + df[\"response_b_text\"].fillna(\"\")\n\n# Create a single target label:\n# Label 0 if winner_model_a==1, Label 1 if winner_model_b==1, Label 2 if winner_tie==1.\ndef get_label(row):\n    if row[\"winner_model_a\"] == 1:\n        return 0\n    elif row[\"winner_model_b\"] == 1:\n        return 1\n    elif row[\"winner_tie\"] == 1:\n        return 2\n    else:\n        return 2  # Fallback; ideally should not occur\n\ntrain_df[\"target\"] = train_df.apply(get_label, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:17:43.06985Z","iopub.execute_input":"2025-02-03T14:17:43.070422Z","iopub.status.idle":"2025-02-03T14:17:43.769556Z","shell.execute_reply.started":"2025-02-03T14:17:43.070379Z","shell.execute_reply":"2025-02-03T14:17:43.768461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 4. Building the Predictive Model\n\n### 4.1 Text Vectorization & Feature Construction\n\nI use a TF‑IDF vectorizer to convert the combined texts into numerical representations and then compute the difference between the two vectors as my feature vector.","metadata":{}},{"cell_type":"code","source":"#%% [code]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create a combined corpus from training and test texts\nall_text = pd.concat([train_df[\"text_a\"], train_df[\"text_b\"], test_df[\"text_a\"], test_df[\"text_b\"]])\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\nvectorizer.fit(all_text)\n\n# Transform texts into TF-IDF vectors\nX_a_train = vectorizer.transform(train_df[\"text_a\"])\nX_b_train = vectorizer.transform(train_df[\"text_b\"])\n# Compute the difference vector as features\nX_train = X_a_train - X_b_train\n\nX_a_test = vectorizer.transform(test_df[\"text_a\"])\nX_b_test = vectorizer.transform(test_df[\"text_b\"])\nX_test = X_a_test - X_b_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:18:00.707646Z","iopub.execute_input":"2025-02-03T14:18:00.707969Z","iopub.status.idle":"2025-02-03T14:20:04.095665Z","shell.execute_reply.started":"2025-02-03T14:18:00.707942Z","shell.execute_reply":"2025-02-03T14:20:04.09435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 4.2 Model Training & Evaluation\n\nI train a multinomial logistic regression model (with an increased maximum number of iterations to ensure convergence) and evaluate its performance using 5‑fold cross‑validation (log loss as the metric).","metadata":{}},{"cell_type":"code","source":"#%% [code]\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\nclf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=500, random_state=42)\nclf.fit(X_train, train_df[\"target\"])\n\n# Evaluate using 5-fold cross-validation (log loss)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(clf, X_train, train_df[\"target\"], cv=cv, scoring=\"neg_log_loss\")\nprint(\"5-Fold CV Log Loss: {:.4f}\".format(-cv_scores.mean()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:35:12.537383Z","iopub.execute_input":"2025-02-03T14:35:12.537755Z","iopub.status.idle":"2025-02-03T14:37:24.644943Z","shell.execute_reply.started":"2025-02-03T14:35:12.537727Z","shell.execute_reply":"2025-02-03T14:37:24.643879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n_My model achieved an average log loss of around 1.09._\n\n----------\n\n## 5. Prediction & Submission\n\nWith the trained model, I now predict the probabilities on the test set and create the submission file.","metadata":{}},{"cell_type":"code","source":"#%% [code]\n# Predict probabilities on the test set.\ntest_probs = clf.predict_proba(X_test)\n# The class order is: 0 (response A wins), 1 (response B wins), 2 (tie).\n\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"winner_model_a\": test_probs[:, 0],\n    \"winner_model_b\": test_probs[:, 1],\n    \"winner_tie\": test_probs[:, 2]\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file 'submission.csv' created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:37:46.471882Z","iopub.execute_input":"2025-02-03T14:37:46.472344Z","iopub.status.idle":"2025-02-03T14:37:46.482015Z","shell.execute_reply.started":"2025-02-03T14:37:46.472302Z","shell.execute_reply":"2025-02-03T14:37:46.481049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 6. Further Visualizations & Analysis\n\nTo further investigate the data and my model’s features, I explored additional visualizations.\n\n### 6.1 Distribution of Target Classes\n\nI visualize the distribution of the target labels to ensure the class balance is reasonable.","metadata":{}},{"cell_type":"code","source":"#%% [code]\nimport seaborn as sns\n\nsns.countplot(x=train_df[\"target\"])\nplt.title(\"Distribution of Target Classes\")\nplt.xlabel(\"Target (0: A wins, 1: B wins, 2: Tie)\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:38:04.46632Z","iopub.execute_input":"2025-02-03T14:38:04.466802Z","iopub.status.idle":"2025-02-03T14:38:04.647859Z","shell.execute_reply.started":"2025-02-03T14:38:04.466765Z","shell.execute_reply":"2025-02-03T14:38:04.646788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 6.2 TF‑IDF Feature Distribution\n\nFor an idea of the feature space, I plot a histogram of the first TF‑IDF feature differences.","metadata":{}},{"cell_type":"code","source":"#%% [code]\nfeature_diff = X_train[:, 0].toarray().flatten()\nplt.hist(feature_diff, bins=50)\nplt.title(\"Histogram of First TF-IDF Feature Difference\")\nplt.xlabel(\"Feature Value\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:38:20.610217Z","iopub.execute_input":"2025-02-03T14:38:20.610575Z","iopub.status.idle":"2025-02-03T14:38:20.898782Z","shell.execute_reply.started":"2025-02-03T14:38:20.610547Z","shell.execute_reply":"2025-02-03T14:38:20.897625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 6.3 Relationship Between Text Lengths and Outcomes\n\nI also examine whether differences in text lengths are related to the outcomes by plotting a scatter plot between prompt length and response length differences.","metadata":{}},{"cell_type":"code","source":"#%% [code]\n# Calculate difference between response_a_text_length and response_b_text_length\ntrain_df[\"response_length_diff\"] = train_df[\"response_a_text_length\"] - train_df[\"response_b_text_length\"]\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(x=train_df[\"prompt_text_length\"], y=train_df[\"response_length_diff\"], hue=train_df[\"target\"], palette=\"viridis\", alpha=0.5)\nplt.title(\"Prompt Text Length vs. Response Length Difference\")\nplt.xlabel(\"Prompt Text Length\")\nplt.ylabel(\"Response A Length - Response B Length\")\nplt.legend(title=\"Target\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:38:35.770192Z","iopub.execute_input":"2025-02-03T14:38:35.770658Z","iopub.status.idle":"2025-02-03T14:38:39.240276Z","shell.execute_reply.started":"2025-02-03T14:38:35.770618Z","shell.execute_reply":"2025-02-03T14:38:39.239162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 7. Conclusions\n\nIn this notebook, I:\n\n-   **Introduced** the problem of predicting user preferences in head-to-head chatbot responses.\n-   **Explored** the data by parsing JSON strings, examining basic statistics, and visualizing text lengths and model frequencies.\n-   **Engineered features** by combining prompts and responses and using the difference of their TF‑IDF representations.\n-   **Built a predictive model** (multinomial logistic regression) and evaluated it via 5‑fold cross‑validation (log loss ≈ 1.09).\n-   **Predicted outcomes** on the test set and generated a submission file.\n-   **Enhanced my analysis** with additional visualizations (word clouds, correlation heatmaps, scatter plots) that offer deeper insights into the data.","metadata":{}}]}