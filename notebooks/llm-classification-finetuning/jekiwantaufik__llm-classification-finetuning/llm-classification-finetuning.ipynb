{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":8926343,"sourceType":"datasetVersion","datasetId":5369301},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10150018,"sourceType":"datasetVersion","datasetId":6265978},{"sourceId":10214229,"sourceType":"datasetVersion","datasetId":6267026},{"sourceId":10236316,"sourceType":"datasetVersion","datasetId":6266300},{"sourceId":10302322,"sourceType":"datasetVersion","datasetId":6265823},{"sourceId":75103,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================================\n# 1️⃣ Install package & setup environment\n# ==========================================================\n!pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\n\n!cp -r /kaggle/input/lmsys-modules-0805 human_pref\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# ==========================================================\n# 2️⃣ prepare_test_file.py\n# ==========================================================\nwith open(\"prepare_test_file.py\", \"w\") as f:\n    f.write('''\\\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\ndf.to_parquet(\"test.parquet\", index=False)\n\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\ndf.to_parquet(\"test_swap.parquet\", index=False)\n''')\n\n!python prepare_test_file.py\n\n# ==========================================================\n# 3️⃣ predict_m0.py (Gemma2)\n# ==========================================================\nwith open(\"predict_m0.py\", \"w\") as f:\n    f.write('''\\\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"\ncsv_path = \"test.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nprocessor = ProcessorPAB(tokenizer=tokenizer, max_length=4096, support_system_role=False)\ndataset = LMSYSDataset(csv_file=csv_path, query=None, processor=processor, include_swap=False, is_parquet=True)\ndataloader = DataLoader(dataset, batch_size=80, num_workers=4, collate_fn=ShardedMaxTokensCollator(max_tokens=8192, base_collator=VarlenCollator()))\n\nnum_hidden_layers = 42\ndevice_map = {\"model.embed_tokens\": \"cuda:0\", \"model.norm\": \"cuda:1\", \"score\": \"cuda:1\"}\nfor i in range(num_hidden_layers // 2): device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers): device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = Gemma2ForSequenceClassification.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=device_map)\n\nconfig = model.config\ndim = config.head_dim\ninv_freq = 1.0 / (config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\ninv_freq0 = inv_freq.to(\"cuda:0\"); inv_freq1 = inv_freq.to(\"cuda:1\")\n\nis_first = True\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device([seq_info, prev_hidden_states], \"cuda:1\")\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            prev_seq_info, prev_hidden_states = to_device([seq_info, hidden_states], \"cuda:1\")\n            outs.append(logits.cpu())\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\nnp.save('prob_m0.npy', prob)\n''')\n\n!python predict_m0.py\n\n# ==========================================================\n# 4️⃣ predict_m3.py (Llama3)\n# ==========================================================\nwith open(\"predict_m3.py\", \"w\") as f:\n    f.write('''\\\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\ncsv_path = \"test_swap.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nprocessor = ProcessorPAB(tokenizer=tokenizer, max_length=4096, support_system_role=True)\ndataset = LMSYSDataset(csv_file=csv_path, query=None, processor=processor, include_swap=False, is_parquet=True)\ndataloader = DataLoader(dataset, batch_size=80, num_workers=4, collate_fn=ShardedMaxTokensCollator(max_tokens=8192, base_collator=VarlenCollator()))\n\nnum_hidden_layers = 32\ndevice_map = {\"model.embed_tokens\": \"cuda:0\", \"model.norm\": \"cuda:1\", \"score\": \"cuda:1\"}\nfor i in range(num_hidden_layers // 2): device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers): device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = LlamaForSequenceClassification.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=device_map)\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\ninv_freq = 1.0 / (config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\ninv_freq0 = inv_freq.to(\"cuda:0\"); inv_freq1 = inv_freq.to(\"cuda:1\")\n\nis_first = True\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device([seq_info, prev_hidden_states], \"cuda:1\")\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            prev_seq_info, prev_hidden_states = to_device([seq_info, hidden_states], \"cuda:1\")\n            outs.append(logits.cpu())\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\nnp.save('prob_m3.npy', prob)\n''')\n\n!python predict_m3.py\n\n# ==========================================================\n# 5️⃣ ensemble.py — final prediction\n# ==========================================================\nwith open(\"ensemble.py\", \"w\") as f:\n    f.write('''\\\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_parquet(\"test.parquet\")\nprob_m0 = np.load(\"prob_m0.npy\")\nprob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]\n\npreds = np.average([prob_m0, prob_m3], axis=0, weights=[0.55, 0.45])\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n''')\n\n!python ensemble.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T13:04:57.68008Z","iopub.execute_input":"2025-11-04T13:04:57.680447Z","iopub.status.idle":"2025-11-04T13:15:18.650436Z","shell.execute_reply.started":"2025-11-04T13:04:57.68042Z","shell.execute_reply":"2025-11-04T13:15:18.649422Z"}},"outputs":[],"execution_count":null}]}