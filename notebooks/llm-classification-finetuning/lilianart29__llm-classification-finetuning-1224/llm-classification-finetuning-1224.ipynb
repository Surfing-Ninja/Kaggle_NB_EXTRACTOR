{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":10039957,"sourceType":"datasetVersion","datasetId":6184619},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":181518,"sourceType":"modelInstanceVersion","modelInstanceId":154703,"modelId":177175},{"sourceId":187742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":160057,"modelId":182433},{"sourceId":207908,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":177248,"modelId":199554}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:13:21.47109Z","iopub.execute_input":"2024-12-23T15:13:21.471309Z","iopub.status.idle":"2024-12-23T15:13:32.393481Z","shell.execute_reply.started":"2024-12-23T15:13:21.471285Z","shell.execute_reply":"2024-12-23T15:13:32.392527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport time\n\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:14:21.360561Z","iopub.execute_input":"2024-12-23T15:14:21.361539Z","iopub.status.idle":"2024-12-23T15:14:30.024354Z","shell.execute_reply.started":"2024-12-23T15:14:21.361488Z","shell.execute_reply":"2024-12-23T15:14:30.02368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_LENGTH = 1024\nBATCH_SIZE = 2\nDEVICE = torch.device(\"cuda:0\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:14:56.475074Z","iopub.execute_input":"2024-12-23T15:14:56.47558Z","iopub.status.idle":"2024-12-23T15:14:56.480308Z","shell.execute_reply.started":"2024-12-23T15:14:56.475548Z","shell.execute_reply":"2024-12-23T15:14:56.479215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/c/llm-classification-finetuning/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/c/llm-classification-finetuning/sample_submission.csv')\n\n# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:15:04.021852Z","iopub.execute_input":"2024-12-23T15:15:04.022831Z","iopub.status.idle":"2024-12-23T15:15:04.049176Z","shell.execute_reply.started":"2024-12-23T15:15:04.022781Z","shell.execute_reply":"2024-12-23T15:15:04.048331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\nprint(test['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:15:07.151582Z","iopub.execute_input":"2024-12-23T15:15:07.152275Z","iopub.status.idle":"2024-12-23T15:15:07.159127Z","shell.execute_reply.started":"2024-12-23T15:15:07.152238Z","shell.execute_reply":"2024-12-23T15:15:07.158201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/llama3.2-3b-instruct-hf/transformers/default/1', local_files_only=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n#tokenizer.save_pretrained('tokenizer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:15:10.206583Z","iopub.execute_input":"2024-12-23T15:15:10.207358Z","iopub.status.idle":"2024-12-23T15:15:11.134024Z","shell.execute_reply.started":"2024-12-23T15:15:10.207308Z","shell.execute_reply":"2024-12-23T15:15:11.133088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ntokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\nATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n\n# Move tensors to CPU and convert them to lists\ninput_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\nattention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_cpu\ndata['ATTENTION_MASKS'] = attention_masks_cpu\ndata[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:15:13.592932Z","iopub.execute_input":"2024-12-23T15:15:13.593519Z","iopub.status.idle":"2024-12-23T15:15:13.88734Z","shell.execute_reply.started":"2024-12-23T15:15:13.593484Z","shell.execute_reply":"2024-12-23T15:15:13.886461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = LlamaForSequenceClassification.from_pretrained(\n    '/kaggle/input/llama3.2-3b-instruct-hf/transformers/default/1',\n    num_labels=3,\n    #torch_dtype=torch.float32\n    torch_dtype=torch.bfloat16,\n    device_map='cuda:0'\n)\nbase_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:16:50.789443Z","iopub.execute_input":"2024-12-23T15:16:50.789819Z","iopub.status.idle":"2024-12-23T15:17:41.715067Z","shell.execute_reply.started":"2024-12-23T15:16:50.789785Z","shell.execute_reply":"2024-12-23T15:17:41.714348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=4,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:17:53.795049Z","iopub.execute_input":"2024-12-23T15:17:53.795385Z","iopub.status.idle":"2024-12-23T15:17:53.799656Z","shell.execute_reply.started":"2024-12-23T15:17:53.795356Z","shell.execute_reply":"2024-12-23T15:17:53.798832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get peft\ndevice='cuda:0'\nmodel_0 = get_peft_model(base_model, peft_config).to(device) \n# Load weights\nmodel_0.load_state_dict(torch.load('/kaggle/input/llama_3b__v2/transformers/default/1/model_llama_3_cp_1_v1.pth'), strict=False)\nmodel_0.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:18:17.396493Z","iopub.execute_input":"2024-12-23T15:18:17.397178Z","iopub.status.idle":"2024-12-23T15:18:22.509926Z","shell.execute_reply.started":"2024-12-23T15:18:17.397143Z","shell.execute_reply":"2024-12-23T15:18:22.509035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_0.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:18:24.85145Z","iopub.execute_input":"2024-12-23T15:18:24.851882Z","iopub.status.idle":"2024-12-23T15:18:24.858682Z","shell.execute_reply.started":"2024-12-23T15:18:24.851846Z","shell.execute_reply":"2024-12-23T15:18:24.857902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:18:27.808878Z","iopub.execute_input":"2024-12-23T15:18:27.809218Z","iopub.status.idle":"2024-12-23T15:18:27.968558Z","shell.execute_reply.started":"2024-12-23T15:18:27.809188Z","shell.execute_reply":"2024-12-23T15:18:27.967659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(df, model, device, batch_size=BATCH_SIZE):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:18:30.888625Z","iopub.execute_input":"2024-12-23T15:18:30.889242Z","iopub.status.idle":"2024-12-23T15:18:30.896359Z","shell.execute_reply.started":"2024-12-23T15:18:30.889205Z","shell.execute_reply":"2024-12-23T15:18:30.895527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nst = time.time()\n\n# Run inference on the entire dataset with the single model\ndata = inference(data, model_0, device)\n\nprint(f\"Processing complete. Total time: {time.time() - st}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:18:43.484411Z","iopub.execute_input":"2024-12-23T15:18:43.484929Z","iopub.status.idle":"2024-12-23T15:18:46.0674Z","shell.execute_reply.started":"2024-12-23T15:18:43.484879Z","shell.execute_reply":"2024-12-23T15:18:46.066476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n\nsample_sub[TARGETS] = data[TARGETS]\ndisplay(sample_sub)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:18:49.140916Z","iopub.execute_input":"2024-12-23T15:18:49.141589Z","iopub.status.idle":"2024-12-23T15:18:49.156305Z","shell.execute_reply.started":"2024-12-23T15:18:49.141554Z","shell.execute_reply":"2024-12-23T15:18:49.155397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T15:18:57.484018Z","iopub.execute_input":"2024-12-23T15:18:57.484505Z","iopub.status.idle":"2024-12-23T15:18:57.490158Z","shell.execute_reply.started":"2024-12-23T15:18:57.484465Z","shell.execute_reply":"2024-12-23T15:18:57.489281Z"}},"outputs":[],"execution_count":null}]}