{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":33547,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":28079,"modelId":39106}],"dockerImageVersionId":30841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install --upgrade pip\n!pip install -qq peft==0.6.0\n!pip install -qq bitsandbytes==0.41.1\n!pip install -qq accelerate==0.24.1\n!pip install -qq transformers==4.43.1\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD\n!pip install numpy==1.23.5  # 或者选择 1.22.x 系列中的另一个版本\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:31:42.391637Z","iopub.execute_input":"2025-01-27T05:31:42.392027Z","iopub.status.idle":"2025-01-27T05:32:08.697234Z","shell.execute_reply.started":"2025-01-27T05:31:42.391999Z","shell.execute_reply":"2025-01-27T05:32:08.696174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:12.929093Z","iopub.execute_input":"2025-01-27T05:32:12.929463Z","iopub.status.idle":"2025-01-27T05:32:12.938495Z","shell.execute_reply.started":"2025-01-27T05:32:12.929434Z","shell.execute_reply":"2025-01-27T05:32:12.93755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 16\n    DROPOUT = 0.05 \n    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-hf/1'\n    SEED = 2024 \n    MAX_LENGTH = 1024 \n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5 \n    NUM_LABELS = 3 \n    LORA_RANK = 16\n    LORA_ALPHA = 32\n    LORA_MODULES = ['q_proj', 'v_proj']\n    \nDEVICE = xm.xla_device() # Initialize TPU Device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:15.546662Z","iopub.execute_input":"2025-01-27T05:32:15.547005Z","iopub.status.idle":"2025-01-27T05:32:15.552414Z","shell.execute_reply.started":"2025-01-27T05:32:15.546975Z","shell.execute_reply":"2025-01-27T05:32:15.551088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n    # Set seed for all TPU cores\n    xm.set_rng_state(seed, device=xm.xla_device())  \n\nset_seeds(seed=CFG.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:17.736157Z","iopub.execute_input":"2025-01-27T05:32:17.736492Z","iopub.status.idle":"2025-01-27T05:32:17.742487Z","shell.execute_reply.started":"2025-01-27T05:32:17.736467Z","shell.execute_reply":"2025-01-27T05:32:17.741431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:19.920121Z","iopub.execute_input":"2025-01-27T05:32:19.920471Z","iopub.status.idle":"2025-01-27T05:32:20.703518Z","shell.execute_reply.started":"2025-01-27T05:32:19.920431Z","shell.execute_reply":"2025-01-27T05:32:20.702494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_token_lengths(texts):\n    # tokenize and receive input_ids for reach text\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    # return length of inputs_ids for each text\n    return [len(t) for t in input_ids]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:24.994257Z","iopub.execute_input":"2025-01-27T05:32:24.994583Z","iopub.status.idle":"2025-01-27T05:32:24.998793Z","shell.execute_reply.started":"2025-01-27T05:32:24.994554Z","shell.execute_reply":"2025-01-27T05:32:24.997847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n# Drop 'Null' for training\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint('Total train samples: ', len(train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:27.31119Z","iopub.execute_input":"2025-01-27T05:32:27.311491Z","iopub.status.idle":"2025-01-27T05:32:29.744503Z","shell.execute_reply.started":"2025-01-27T05:32:27.311463Z","shell.execute_reply":"2025-01-27T05:32:29.743647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:29:44.120084Z","iopub.execute_input":"2025-01-27T05:29:44.12044Z","iopub.status.idle":"2025-01-27T05:29:44.140032Z","shell.execute_reply.started":"2025-01-27T05:29:44.12041Z","shell.execute_reply":"2025-01-27T05:29:44.138798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(train['text'][4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:33.323833Z","iopub.execute_input":"2025-01-27T05:32:33.3242Z","iopub.status.idle":"2025-01-27T05:32:33.613931Z","shell.execute_reply.started":"2025-01-27T05:32:33.32415Z","shell.execute_reply":"2025-01-27T05:32:33.61282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.loc[:, 'token_count'] = get_token_lengths(train['text'])\n\n# prepare label for model\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\n# Display data\ndisplay(train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:32:40.767121Z","iopub.execute_input":"2025-01-27T05:32:40.767525Z","iopub.status.idle":"2025-01-27T05:33:11.984678Z","shell.execute_reply.started":"2025-01-27T05:32:40.767493Z","shell.execute_reply":"2025-01-27T05:33:11.983569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:30:24.334271Z","iopub.execute_input":"2025-01-27T05:30:24.334682Z","iopub.status.idle":"2025-01-27T05:30:24.342682Z","shell.execute_reply.started":"2025-01-27T05:30:24.33465Z","shell.execute_reply":"2025-01-27T05:30:24.341457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(train['token_count'].describe().to_frame().astype(int))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:30:26.707491Z","iopub.execute_input":"2025-01-27T05:30:26.707866Z","iopub.status.idle":"2025-01-27T05:30:26.721595Z","shell.execute_reply.started":"2025-01-27T05:30:26.707836Z","shell.execute_reply":"2025-01-27T05:30:26.720104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.percentile(train['token_count'], 90)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:30:29.60908Z","iopub.execute_input":"2025-01-27T05:30:29.609473Z","iopub.status.idle":"2025-01-27T05:30:29.616146Z","shell.execute_reply.started":"2025-01-27T05:30:29.609444Z","shell.execute_reply":"2025-01-27T05:30:29.615066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokens = tokenizer(\n    train['text'].tolist(), \n    padding='max_length', \n    max_length=CFG.MAX_LENGTH, \n    truncation=True, \n    return_tensors='np')\n\n# Input IDs are the token IDs\nINPUT_IDS = tokens['input_ids']\n# Attention Masks to Ignore Padding Tokens\nATTENTION_MASKS = tokens['attention_mask']\n# Label of Texts\nLABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:33:19.331112Z","iopub.execute_input":"2025-01-27T05:33:19.331478Z","iopub.status.idle":"2025-01-27T05:33:55.488646Z","shell.execute_reply.started":"2025-01-27T05:33:19.331447Z","shell.execute_reply":"2025-01-27T05:33:55.48766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_dataset(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        # Shuffle Indices\n        np.random.shuffle(IDXS)\n        # Iterate Over All Indices Once\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output\n            \n            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            \n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:33:58.209913Z","iopub.execute_input":"2025-01-27T05:33:58.210276Z","iopub.status.idle":"2025-01-27T05:33:58.216703Z","shell.execute_reply.started":"2025-01-27T05:33:58.210245Z","shell.execute_reply":"2025-01-27T05:33:58.215733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = LlamaForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=CFG.NUM_LABELS,\n    torch_dtype=torch.bfloat16)\n\nbase_model.config.pretraining_tp = 1 \n\n# Assign Padding TOKEN\nbase_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:34:00.817364Z","iopub.execute_input":"2025-01-27T05:34:00.817678Z","iopub.status.idle":"2025-01-27T05:34:00.971465Z","shell.execute_reply.started":"2025-01-27T05:34:00.817645Z","shell.execute_reply":"2025-01-27T05:34:00.970241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices\n    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n    lora_dropout= CFG.DROPOUT, \n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:50:56.146849Z","iopub.execute_input":"2025-01-26T06:50:56.147228Z","iopub.status.idle":"2025-01-26T06:50:56.151534Z","shell.execute_reply.started":"2025-01-26T06:50:56.147201Z","shell.execute_reply":"2025-01-26T06:50:56.150592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_peft_model(base_model, lora_config)\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:51:02.849886Z","iopub.execute_input":"2025-01-26T06:51:02.850243Z","iopub.status.idle":"2025-01-26T06:51:02.939684Z","shell.execute_reply.started":"2025-01-26T06:51:02.850218Z","shell.execute_reply":"2025-01-26T06:51:02.938475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n# distribute model\npartition_module(model, mesh)\n\nprint(f'num_devices: {num_devices}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:51:48.806577Z","iopub.execute_input":"2025-01-26T06:51:48.806856Z","iopub.status.idle":"2025-01-26T06:51:48.98307Z","shell.execute_reply.started":"2025-01-26T06:51:48.80683Z","shell.execute_reply":"2025-01-26T06:51:48.98176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_LAYERS_ROWS = []\nTRAINABLE_PARAMS = []\nN_TRAINABLE_PARAMS = 0\n\nfor name, param in model.named_parameters():\n    # Layer Parameter Count\n    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n    # Only Trainable Layers\n    if param.requires_grad:\n        # Add Layer Information\n        MODEL_LAYERS_ROWS.append({\n            'param': n_parameters,\n            'name': name,\n            'dtype': param.data.dtype,\n        })\n        # Append Trainable Parameter\n        TRAINABLE_PARAMS.append({ 'params': param })\n        # Add Number Of Trainable Parameters\"\n        N_TRAINABLE_PARAMS += n_parameters\n        \ndisplay(pd.DataFrame(MODEL_LAYERS_ROWS))\n\nprint(f\"\"\"\n===============================\nN_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\nN_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n===============================\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:53:05.552579Z","iopub.execute_input":"2025-01-26T06:53:05.553003Z","iopub.status.idle":"2025-01-26T06:53:05.577817Z","shell.execute_reply.started":"2025-01-26T06:53:05.552973Z","shell.execute_reply":"2025-01-26T06:53:05.576853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_SAMPLES = len(train)\nSTEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n\n# Cosine Learning Rate With Warmup\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer=OPTIMIZER,\n    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)\n\nprint(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:55:44.985813Z","iopub.execute_input":"2025-01-26T06:55:44.986218Z","iopub.status.idle":"2025-01-26T06:55:45.000819Z","shell.execute_reply.started":"2025-01-26T06:55:44.986191Z","shell.execute_reply":"2025-01-26T06:55:44.999396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for state in OPTIMIZER.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n            state[k] = v.to(dtype=torch.float32)\ninput_ids, attention_mask, labels = next(TRAIN_DATASET)\n\nprint(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\nprint(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\nprint(f'labels shape: {labels.shape}, dtype: {labels.dtype}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:57:40.246051Z","iopub.execute_input":"2025-01-26T06:57:40.246435Z","iopub.status.idle":"2025-01-26T06:57:40.257896Z","shell.execute_reply.started":"2025-01-26T06:57:40.246408Z","shell.execute_reply":"2025-01-26T06:57:40.256665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Dummy Prediction\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    \nprint(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:57:43.254332Z","iopub.execute_input":"2025-01-26T06:57:43.254713Z","iopub.status.idle":"2025-01-26T06:58:08.881815Z","shell.execute_reply.started":"2025-01-26T06:57:43.254682Z","shell.execute_reply":"2025-01-26T06:58:08.880582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\n\n# Loss Function, Cross Entropy\nLOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)\nst = time()\nwarnings.filterwarnings(\"error\")\nMETRICS = {\n    'loss': [],\n    'accuracy': {'y_true': [], 'y_pred': [] }}\n\nfor epoch in tqdm(range(CFG.NUM_EPOCHS)):\n    ste = time()\n    for step in range(STEPS_PER_EPOCH):\n        # Zero Out Gradients\n        OPTIMIZER.zero_grad()\n        \n        # Get Batch\n        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n        \n        # Forward Pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n       \n        # Logits Float32\n        logits = outputs.logits.to(dtype=torch.float32)\n        \n        # Backward Pass\n        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n        loss.backward()\n        \n        # optimizer step\n        OPTIMIZER.step()\n        xm.mark_step()\n        \n        # Update Learning Rate Scheduler\n        lr_scheduler.step()\n        \n        # Update Metrics And Progress Bar\n        METRICS['loss'].append(float(loss))\n        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n        \n        if (step + 1) % 200 == 0:  \n            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n                                                               METRICS['accuracy']['y_pred']))\n            lr = OPTIMIZER.param_groups[0]['lr']\n            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')\n    \n    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n\n    # If stopped, and to continue training in future on tpu we save model and optimizer\n    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    \n    \n    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:58:25.996964Z","iopub.execute_input":"2025-01-26T06:58:25.997345Z","execution_failed":"2025-01-26T07:08:59.025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(METRICS['loss'])    \nplt.xlabel('Step per epoch')\nplt.ylabel('Loss')\nplt.title('Loss Plot step per epoch')    \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}