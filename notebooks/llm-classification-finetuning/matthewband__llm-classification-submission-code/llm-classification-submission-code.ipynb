{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":11221025,"sourceType":"datasetVersion","datasetId":7007680},{"sourceId":11286194,"sourceType":"datasetVersion","datasetId":7056617},{"sourceId":11286241,"sourceType":"datasetVersion","datasetId":7056651},{"sourceId":11311207,"sourceType":"datasetVersion","datasetId":7074481},{"sourceId":11311287,"sourceType":"datasetVersion","datasetId":7074538},{"sourceId":11311365,"sourceType":"datasetVersion","datasetId":7074602},{"sourceId":11311435,"sourceType":"datasetVersion","datasetId":7074661},{"sourceId":11311722,"sourceType":"datasetVersion","datasetId":7074900},{"sourceId":11311795,"sourceType":"datasetVersion","datasetId":7074953},{"sourceId":11315168,"sourceType":"datasetVersion","datasetId":7077579},{"sourceId":11315179,"sourceType":"datasetVersion","datasetId":7077589},{"sourceId":11382748,"sourceType":"datasetVersion","datasetId":7127332},{"sourceId":11382840,"sourceType":"datasetVersion","datasetId":7127399}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, math, numpy as np\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n# !pip install transformers datasets accelerate\n\n!pip install accelerate --no-index --find-links=file:///kaggle/input/packages-llm-classification/accelerate\n!pip install transformers --no-index --find-links=file:///kaggle/input/packages-llm-classification/transformers\n!pip install datasets --no-index --find-links=file:///kaggle/input/packages-llm-classification/datasets\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport random\nfrom transformers import AutoTokenizer\n### Now, we have the datasets done, onto the actual cool stuff\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:02:50.036725Z","iopub.execute_input":"2025-04-13T01:02:50.036976Z","iopub.status.idle":"2025-04-13T01:03:25.472444Z","shell.execute_reply.started":"2025-04-13T01:02:50.036956Z","shell.execute_reply":"2025-04-13T01:03:25.471673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# splits the x and y data\ndef x_y_split(input_df, train_test = \"test\"):\n    X = input_df[['prompt', 'response_a', 'response_b']]\n    if (train_test == \"train\"):\n        Y = input_df[['winner_model_a', 'winner_model_b', 'winner_tie']]\n        return (X,Y)\n    return X\n\nX_test = x_y_split(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:03:32.73431Z","iopub.execute_input":"2025-04-13T01:03:32.734619Z","iopub.status.idle":"2025-04-13T01:03:32.763076Z","shell.execute_reply.started":"2025-04-13T01:03:32.73459Z","shell.execute_reply":"2025-04-13T01:03:32.762269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Custom Dataset class\nclass TestDatasetSingleBertModel(Dataset):\n    def __init__(self, X, tokenizer, MAX_LENGTH=None, split_ratios=[1/3,1/3,1/3], LOGS = False):\n\n        if MAX_LENGTH is None:\n            MAX_LENGTH = tokenizer.model_max_length\n\n        self.tokenized_inputs, self.attention_masks = self.tokenize_and_truncate(X, MAX_LENGTH, split_ratios,LOGS)\n\n\n    def __len__(self):\n        return len(self.tokenized_inputs)\n\n    def __getitem__(self, idx):\n        # create the item as a dictionary for easier access\n        item = {\n            'input_ids': torch.tensor(self.tokenized_inputs[idx], dtype=torch.long),\n            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n        }\n        return item\n        \n    ### Custom function to tokenize and pad, ensuring \n    def tokenize_and_truncate(self, X, MAX_LENGTH, split_ratios, LOGS = False):\n        # Create an empty list to store tokenized inputs\n        tokenized_inputs = []\n        attention_masks = []  # List to store the attention masks\n        \n        # Iterate through the DataFrame rows\n        for index, row in X.iterrows():\n            # Tokenize each part separately with truncation and no padding\n            prompt_tokens = tokenizer(row['prompt'], add_special_tokens=False, truncation=True, max_length=MAX_LENGTH)['input_ids']\n            response_a_tokens = tokenizer(row['response_a'], add_special_tokens=False, truncation=True, max_length=MAX_LENGTH)['input_ids']\n            response_b_tokens = tokenizer(row['response_b'], add_special_tokens=False, truncation=True, max_length=MAX_LENGTH)['input_ids']\n    \n            # Reserve space for [CLS] and [SEP] tokens (3 tokens in total)\n            total_available = MAX_LENGTH - 3  # 1 [CLS] + 2 [SEP] tokens\n            \n            # Split available space proportionally between the 3 inputs\n            prompt_max = int(total_available * split_ratios[0])\n            response_a_max = int(total_available * split_ratios[1])\n            response_b_max = total_available - (prompt_max + response_a_max)\n    \n            if (LOGS == True):\n                if (response_b_max != split_ratios[2]):\n                    print(f\"Calculated response_b_max ratio {response_b_max/MAX_LENGTH} vs. split_ratios {split_ratios[2]}\")\n            \n            # Truncate inputs according to the available space\n            prompt_tokens = prompt_tokens[:prompt_max]\n            response_a_tokens = response_a_tokens[:response_a_max]\n            response_b_tokens = response_b_tokens[:response_b_max]\n    \n            # Concatenate the inputs\n            input_ids = prompt_tokens + response_a_tokens + response_b_tokens\n            \n            # Add special tokens [CLS] and [SEP] to the input_ids\n            input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)\n    \n            # Create the attention mask: 1 for real tokens, 0 for padding tokens\n            attention_mask = [1] * len(input_ids)  # Start by marking all tokens as 1\n            \n            # Pad the remaining sequence with 0s for the attention mask (if needed)\n            while len(input_ids) < MAX_LENGTH:\n                input_ids.append(tokenizer.pad_token_id)  # Pad token ID\n                attention_mask.append(0)  # Padding token gets 0 in the attention mask\n    \n            # Append to the lists\n            tokenized_inputs.append(input_ids)\n            attention_masks.append(attention_mask)\n        \n        return tokenized_inputs, attention_masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:03:34.555721Z","iopub.execute_input":"2025-04-13T01:03:34.556035Z","iopub.status.idle":"2025-04-13T01:03:34.565397Z","shell.execute_reply.started":"2025-04-13T01:03:34.556008Z","shell.execute_reply":"2025-04-13T01:03:34.564286Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport re\n### Custom Dataset class\nclass TestDatasetSingleBertModelV2(Dataset):\n    def __init__(self, X, tokenizer, MAX_LENGTH=None, split_ratios=[1/3,1/3,1/3], LOGS=False):\n\n        self.split_ratios = split_ratios\n        self.tokenizer = tokenizer\n\n        self.LOGS = LOGS\n\n        self.cls_token = [tokenizer.cls_token_id]  # [CLS]\n        self.sep_token = [tokenizer.sep_token_id]  # [SEP]\n        \n        if MAX_LENGTH is None:\n            self.MAX_LENGTH = tokenizer.model_max_length\n\n        self.X_processed = X.map(self.process)\n        \n        # self.tokenized_inputs, self.attention_masks = self.tokenize_and_truncate(X_processed, MAX_LENGTH, split_ratios, LOGS)\n\n    def process(self, text):\n\n        text = text.encode('utf-8').decode()\n        text = text.replace(\"\\\\n\", \" \")  # Replace newline artifacts\n        text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize extra spaces\n        text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)  # Remove markdown bold formatting (**text** → text)\n        text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # Remove stray brackets (if unintended)\n        \n        return text\n\n    def __len__(self):\n        return len(self.X_processed)\n\n    def __getitem__(self, idx):\n        row = self.X_processed.iloc[idx]\n\n        response_b_length = round(self.split_ratios[2]*self.MAX_LENGTH)\n        response_a_length = round(self.split_ratios[1]*self.MAX_LENGTH)\n\n        prompt_length = self.MAX_LENGTH - response_b_length - response_a_length \n\n        if self.LOGS:\n            print(f\"prompt_length: {prompt_length}, response_a_length: {response_a_length}, response_b_length: {response_b_length}\")\n\n        prompt_tokens = self.tokenizer(row['prompt'], add_special_tokens=False, truncation=True, max_length=prompt_length)['input_ids']\n        response_a_tokens = self.tokenizer(row['response_a'], add_special_tokens=False, truncation=True, max_length=response_a_length)['input_ids']\n        response_b_tokens = self.tokenizer(row['response_b'], add_special_tokens=False, truncation=True, max_length=response_b_length)['input_ids']\n\n        if len(prompt_tokens) + len(response_a_tokens) + len(response_b_tokens) > self.MAX_LENGTH-3: # [CLS] and [SEP] tokens,\n            prompt_tokens = prompt_tokens[:-1]\n            response_a_tokens = response_a_tokens[:-1]\n            response_b_tokens = response_b_tokens[:-1]\n\n        # print(len(prompt_tokens))\n        # print(len(response_a_tokens))\n        # print(len(response_b_tokens))\n     \n        total_tokens =  self.cls_token  + prompt_tokens\n        \n        if len(prompt_tokens) > 0:\n            total_tokens = total_tokens + self.sep_token + response_a_tokens\n        else:\n            total_tokens = total_tokens + response_a_tokens\n        \n        if len(response_a_tokens) > 0:\n            total_tokens = total_tokens + self.sep_token + response_b_tokens\n        else:\n            total_tokens = total_tokens + response_b_tokens\n\n        attention_mask = [1.0]*len(total_tokens) + [0.]*(self.MAX_LENGTH - len(total_tokens))\n        total_tokens = total_tokens + [0.]*(self.MAX_LENGTH - len(total_tokens))\n\n        # print(\"t_t: \", len(total_tokens))\n        \n        # print(\"a_m\", len(attention_mask))\n\n    \n        # create the item as a dictionary for easier access\n        item = {\n            'input_ids': torch.tensor(total_tokens, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n        }\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:03:35.439706Z","iopub.execute_input":"2025-04-13T01:03:35.440021Z","iopub.status.idle":"2025-04-13T01:03:35.451014Z","shell.execute_reply.started":"2025-04-13T01:03:35.439994Z","shell.execute_reply":"2025-04-13T01:03:35.44986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport re\n### Custom Dataset class\nclass TrainDatasetParallel(Dataset):\n    def __init__(self, X, tokenizer, MAX_LENGTH=None, split_ratios=[1/3,1/3], LOGS=False):\n        self.split_ratios = split_ratios\n\n        self.LOGS = LOGS\n        self.tokenizer = tokenizer\n\n        self.cls_token = [self.tokenizer.cls_token_id]  # [CLS]\n        self.sep_token = [self.tokenizer.sep_token_id]  # [SEP]\n        \n        if MAX_LENGTH is None:\n            self.MAX_LENGTH = tokenizer.model_max_length\n\n        self.X_processed = X.map(self.process)\n    \n\n    def process(self, text):\n\n        text = text.encode('utf-8').decode()\n        text = text.replace(\"\\\\n\", \" \")  # Replace newline artifacts\n        text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize extra spaces\n        text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)  # Remove markdown bold formatting (**text** → text)\n        text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # Remove stray brackets (if unintended)\n        \n        return text\n\n    def build_tokens(self,prompt_tokens, response_tokens):\n        \n        if len(prompt_tokens) + len(response_tokens) > self.MAX_LENGTH-2: # [CLS] and [SEP] tokens,\n            prompt_tokens = prompt_tokens[:-1]\n            response_tokens = response_tokens[:-1]\n\n        total_tokens =  self.cls_token + prompt_tokens\n        \n        if len(prompt_tokens) > 0:\n            total_tokens = total_tokens + self.sep_token + response_tokens\n        else:\n            total_tokens = total_tokens + response_tokens\n\n        attention_mask = [1.0]*len(total_tokens) + [0.]*(self.MAX_LENGTH - len(total_tokens))\n        total_tokens = total_tokens + [0.]*(self.MAX_LENGTH - len(total_tokens))\n\n        # print(len(total_tokens))\n        # print(len(attention_mask))\n\n        return total_tokens, attention_mask\n\n    def __len__(self):\n        return len(self.X_processed)\n\n    def __getitem__(self, idx):\n        row = self.X_processed.iloc[idx]\n\n        response_length = round(self.split_ratios[1]*self.MAX_LENGTH)\n        prompt_length = self.MAX_LENGTH - response_length\n\n        if self.LOGS:\n            print(f\"prompt_length: {prompt_length}, response_length: {response_length}\")\n\n        prompt_tokens = self.tokenizer(row['prompt'], add_special_tokens=False, truncation=True, max_length=prompt_length)['input_ids']\n        response_a_tokens = self.tokenizer(row['response_a'], add_special_tokens=False, truncation=True, max_length=response_length)['input_ids']\n        response_b_tokens = self.tokenizer(row['response_b'], add_special_tokens=False, truncation=True, max_length=response_length)['input_ids']\n\n        input_ids_a, attention_mask_a = self.build_tokens(prompt_tokens, response_a_tokens)\n        input_ids_b, attention_mask_b = self.build_tokens(prompt_tokens, response_b_tokens)\n    \n        # create the item as a dictionary for easier access\n        item = {\n            'input_ids_a': torch.tensor(input_ids_a, dtype=torch.long),\n            'attention_mask_a': torch.tensor(attention_mask_a, dtype=torch.long),\n            'input_ids_b': torch.tensor(input_ids_b, dtype=torch.long),\n            'attention_mask_b': torch.tensor(attention_mask_b, dtype=torch.long),\n        }\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:03:38.105238Z","iopub.execute_input":"2025-04-13T01:03:38.105525Z","iopub.status.idle":"2025-04-13T01:03:38.116647Z","shell.execute_reply.started":"2025-04-13T01:03:38.105504Z","shell.execute_reply":"2025-04-13T01:03:38.115658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel, PreTrainedModel, AutoConfig\nimport torch.nn as nn\n\nclass ParallelTransformer(PreTrainedModel):\n    def __init__(self, model_name=\"distilbert/distilbert-base-uncased\", num_labels=3):\n        config = AutoConfig.from_pretrained(model_name)\n        # print(config)\n        super().__init__(config)\n        \n        self.bert = AutoModel.from_pretrained(model_name)  # Load BERT without a head\n        self.dropout = nn.Dropout(0.1)\n        \n        hidden_size = config.hidden_size  # 768 for BERT-base\n        self.classifier = nn.Linear(hidden_size * 2, num_labels)  # Merge resp_a & resp_b\n        \n    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b, labels=None):\n        # Encode resp_a\n        outputs_a = self.bert(input_ids=input_ids_a, attention_mask=attention_mask_a)\n        pooled_a = outputs_a.last_hidden_state[:, 0, :]  # Extract [CLS] embedding\n        \n        # Encode resp_b\n        outputs_b = self.bert(input_ids=input_ids_b, attention_mask=attention_mask_b)\n        pooled_b = outputs_b.last_hidden_state[:, 0, :]  # Extract [CLS] embedding\n        \n        # Merge resp_a and resp_b (Concatenation)\n        merged = torch.cat([pooled_a, pooled_b], dim=1)\n        merged = self.dropout(merged)\n\n        # Classification layer\n        logits = self.classifier(merged)\n        \n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits, labels)\n            loss = loss.unsqueeze(0)\n\n        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}","metadata":{"execution":{"iopub.status.busy":"2025-04-13T01:03:39.915198Z","iopub.execute_input":"2025-04-13T01:03:39.915493Z","iopub.status.idle":"2025-04-13T01:03:39.922465Z","shell.execute_reply.started":"2025-04-13T01:03:39.915472Z","shell.execute_reply":"2025-04-13T01:03:39.921705Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FineTuningBertFromLLMFineTune(PreTrainedModel):\n    def __init__(self, model_path):\n        config = AutoConfig.from_pretrained(model_path)\n\n        super().__init__(config)\n        self.hidden_size = config.hidden_size\n        self.dropout = nn.Dropout(0.1)\n\n        # \"/pytorch_model.bin\"\n        self.bert = AutoModel.from_pretrained(model_path)  # Load BERT without a head\n        \n\n        self.classifier = nn.Linear(self.hidden_size, 3) \n        # self.custom_head = CustomClassificationHead(hidden_size, 3)\n        self.freeze__first_nlayers = 0\n      \n    def freeze_my_layers(self, freeze__first_nlayers):\n        self.freeze__first_nlayers = freeze__first_nlayers\n        if freeze__first_nlayers > 0:\n          print(f\"Freezing {freeze__first_nlayers} layers\")\n          for i in range(freeze__first_nlayers): # Freeze transformer layers\n              # for param in self.bert.transformer.layer[i].parameters():\n              #     param.requires_grad = False\n              for param in self.bert.encoder.layer[i].parameters():\n                  param.requires_grad = False\n\n    def as_dict(self):\n        return {\n            \"hidden_size\" : self.hidden_size,\n            \"parallel_model_settings\" : self.bert.config.to_dict(),\n            \"freeze__first_nlayers\" : self.freeze__first_nlayers}\n\n\n\n    def forward(self, input_ids, attention_mask, labels=None):\n\n        # Encode resp_a\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] embedding\n\n        pooled = self.dropout(pooled)\n        logits = self.classifier(pooled)\n\n        loss = None\n        if labels is not None:\n\n            if labels.dim() > 1:\n                labels = labels.argmax(dim=1)\n            if logits.shape[0] != labels.shape[0]:\n                raise ValueError(f\"Mismatch in logits ({logits.shape}) and labels ({labels.shape}) batch size\")\n\n\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits, labels)\n\n        # print(logits)\n\n\n        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:03:41.285089Z","iopub.execute_input":"2025-04-13T01:03:41.285415Z","iopub.status.idle":"2025-04-13T01:03:41.292922Z","shell.execute_reply.started":"2025-04-13T01:03:41.285392Z","shell.execute_reply":"2025-04-13T01:03:41.292045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nencoder_model_name = \"/kaggle/input/distilbert-parallel-llm-distil/saved_model/distilbert-parallel-llm-distil-frozen3-epoc2\" # \"distilbert/distilbert-base-uncased\" \"\"google-bert/bert-base-uncased\nmethod = \"parallel_bert\" # parallel_bert\n\nif method == \"singe_bert\":\n    model = AutoModelForSequenceClassification.from_pretrained(encoder_model_name, num_labels=3)\n    result = model.load_state_dict(torch.load(encoder_model_name + \"/pytorch_model.bin\")) # Crucial\n    tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n    test_dataset = TestDatasetSingleBertModelV2(X_test, tokenizer, split_ratios=[0.1, 0.45,0.45], LOGS=False)\n\nelif method == 'fine_tuned_single':\n    model = FineTuningBertFromLLMFineTune(encoder_model_name)\n    result = model.load_state_dict(torch.load(encoder_model_name + \"/pytorch_model.bin\"), strict=False) # Crucial\n    tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n    test_dataset = TestDatasetSingleBertModelV2(X_test, tokenizer, split_ratios=[0.1, 0.45,0.45], LOGS=False)\nelif method == \"parallel_bert\":\n\n    model = ParallelTransformer(encoder_model_name)\n    result = model.load_state_dict(torch.load(encoder_model_name + \"/pytorch_model.bin\"), strict=False) # Crucial\n\n    tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n    \n    test_dataset = TrainDatasetParallel(X_test, tokenizer, split_ratios=[0.1, 0.9], LOGS=False)\n\nprint(type(model))\nprint(result)\n# print(train_val_dataset[0].keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:23:31.414774Z","iopub.execute_input":"2025-04-13T01:23:31.415118Z","iopub.status.idle":"2025-04-13T01:23:34.996286Z","shell.execute_reply.started":"2025-04-13T01:23:31.415091Z","shell.execute_reply":"2025-04-13T01:23:34.995425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport torch.nn.functional as F\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_eval_batch_size=16,  # Important for inference speed\n    fp16=True,  # Mixed precision (faster if using an NVIDIA GPU)\n    report_to=\"none\",  # No logging to external services\n)\n\ntrainer = Trainer(\n    model=model,  # Your trained model\n    args=training_args\n)\n\npredictions = trainer.predict(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:23:40.770454Z","iopub.execute_input":"2025-04-13T01:23:40.770756Z","iopub.status.idle":"2025-04-13T01:23:41.015931Z","shell.execute_reply.started":"2025-04-13T01:23:40.770715Z","shell.execute_reply":"2025-04-13T01:23:41.015032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer.save_model(\"./kaggle/working/bertmodel\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:23:42.896393Z","iopub.execute_input":"2025-04-13T01:23:42.896698Z","iopub.status.idle":"2025-04-13T01:23:42.899977Z","shell.execute_reply.started":"2025-04-13T01:23:42.896671Z","shell.execute_reply":"2025-04-13T01:23:42.899224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilities = F.softmax(torch.tensor(predictions.predictions), dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:23:43.341604Z","iopub.execute_input":"2025-04-13T01:23:43.341867Z","iopub.status.idle":"2025-04-13T01:23:43.346099Z","shell.execute_reply.started":"2025-04-13T01:23:43.341848Z","shell.execute_reply":"2025-04-13T01:23:43.345235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\n\nsub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]] = probabilities\n    \nsub.to_csv(\"submission.csv\",index=False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:23:44.1096Z","iopub.execute_input":"2025-04-13T01:23:44.109853Z","iopub.status.idle":"2025-04-13T01:23:44.127222Z","shell.execute_reply.started":"2025-04-13T01:23:44.109833Z","shell.execute_reply":"2025-04-13T01:23:44.12652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}