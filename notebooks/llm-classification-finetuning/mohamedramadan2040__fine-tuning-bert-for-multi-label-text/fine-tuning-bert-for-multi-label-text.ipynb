{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-07-09T11:56:22.341535Z","execution_failed":"2025-07-09T12:01:59.385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\ntrain_df.head()\n","metadata":{"execution":{"execution_failed":"2025-07-09T12:01:59.386Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_winner(row):\n    if row['winner_model_a'] == 1:\n        return 'A'\n    elif row['winner_model_b'] == 1:\n        return 'B'\n    else:\n        return 'tie'\n\ntrain_df['label'] = train_df.apply(get_winner, axis=1)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-09T12:01:59.387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(data=train_df, x='label')\nplt.title(\"Distribution of Winner Labels (A vs B vs Tie)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-09T12:01:59.387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_winner(row):\n    if row['winner_model_a'] == 1:\n        return 0  # model_a\n    elif row['winner_model_b'] == 1:\n        return 1  # model_b\n    else:\n        return 2  # tie\n\ntrain_df['label'] = train_df.apply(get_winner, axis=1)\n\n\nX_train, X_val, y_train, y_val = train_test_split(train_df['prompt'], train_df['label'], test_size=0.1, random_state=42)\n\n# Tokenization\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n \n\n# Convert to Dataset format\nfrom datasets import Dataset\n\n# تحويل إلى تنسيق Dataset المطلوب\ntrain_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train, 'label': y_train}))\nval_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_val, 'label': y_val}))\n\n\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True)\nval_dataset = val_dataset.map(tokenize, batched=True)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-09T12:01:59.387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U transformers\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-09T12:01:59.387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n\nfrom transformers import TrainingArguments\n\n \n\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-09T12:01:59.387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import pandas as pd\n\n# Load test data from competition input\n#test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# Generate dummy predictions (always 'model_a')\n#preds = ['model_a'] * len(test_df)\n\n# Prepare submission file\n#submission = pd.DataFrame({\n#    'id': test_df['id'],\n#    'winner': preds\n#})\n\n# Save the submission file\n#submission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-09T12:01:59.387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Create dummy predictions or use your actual model outputs\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'winner': ['model_a'] * len(test_df)  # Replace with real predictions if available\n})\n\n# Save the submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"✅ submission.csv saved.\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-09T12:01:59.388Z"}},"outputs":[],"execution_count":null}]}