{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:24:59.968621Z","iopub.execute_input":"2025-04-29T13:24:59.969003Z","iopub.status.idle":"2025-04-29T13:24:59.975409Z","shell.execute_reply.started":"2025-04-29T13:24:59.968966Z","shell.execute_reply":"2025-04-29T13:24:59.974496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install necessary packages\n!pip install -q transformers datasets evaluate\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments\n)\nimport evaluate\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# Label encoding\nlabel_map = {'model_a': 0, 'model_b': 1, 'tie': 2}\ninverse_label_map = {v: k for k, v in label_map.items()}\ntrain_df['label'] = train_df['winner'].map(label_map)\n\n# Combine prompt and responses into a single text string\ntrain_df['text'] = train_df['prompt'] + \" [SEP] \" + train_df['response_a'] + \" [SEP] \" + train_df['response_b']\ntest_df['text'] = test_df['prompt'] + \" [SEP] \" + test_df['response_a'] + \" [SEP] \" + test_df['response_b']\n\n# Split train into train and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df['text'], train_df['label'], test_size=0.1, random_state=42\n)\n\n# Tokenizer\nmodel_checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True)\n\n# Dataset class\nclass LLMDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n# Datasets\ntrain_dataset = LLMDataset(train_encodings, list(train_labels))\nval_dataset = LLMDataset(val_encodings, list(val_labels))\ntest_dataset = LLMDataset(test_encodings)\n\n# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n\n# Define metric\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return accuracy.compute(predictions=preds, references=labels)\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\"\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train\ntrainer.train()\n\n# Evaluate\ntrainer.evaluate()\n\n# Predict\npreds_output = trainer.predict(test_dataset)\npreds = np.argmax(preds_output.predictions, axis=1)\n\n# Create submission\ntest_df['winner'] = [inverse_label_map[p] for p in preds]\nsubmission = test_df[['id', 'winner']]\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:30:49.335691Z","iopub.execute_input":"2025-04-29T13:30:49.336036Z","iopub.status.idle":"2025-04-29T13:34:10.808759Z","shell.execute_reply.started":"2025-04-29T13:30:49.336013Z","shell.execute_reply":"2025-04-29T13:34:10.807402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}