{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8478420,"sourceType":"datasetVersion","datasetId":5056610},{"sourceId":8531842,"sourceType":"datasetVersion","datasetId":5095677},{"sourceId":9887086,"sourceType":"datasetVersion","datasetId":6071731}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # Import necessary libraries\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from sklearn.metrics import classification_report\n# from catboost import CatBoostClassifier\n# import numpy as np\n# from scipy.sparse import hstack\n\n# # Load the datasets\n# train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n# test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# # Encode the target columns\n# train['target'] = np.select(\n#     [train['winner_model_a'] == 1, train['winner_model_b'] == 1, train['winner_tie'] == 1],\n#     [0, 1, 2],\n#     default=-1\n# )\n\n# # Remove unnecessary columns\n# train.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie', 'id'], inplace=True)\n\n# # Prepare features and target variable\n# X = train[['model_a','model_b','prompt', 'response_a', 'response_b']]\n# y = train['target']\n\n# # Combine the text columns into a single feature for vectorization\n# X_combined =X['model_a'] +X['model_b']+ X['prompt'] + \" \" + X['response_a'] + \" \" + X['response_b']\n\n# # Split the data into training and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.02, random_state=42, stratify=y)\n\n# # Count Vectorization without stop words\n# count_vectorizer = CountVectorizer(max_features=50000, stop_words=None)\n# X_train_counts = count_vectorizer.fit_transform(X_train)\n# X_val_counts = count_vectorizer.transform(X_val)\n\n# # TF-IDF Vectorization without stop words\n# tfidf_vectorizer = TfidfVectorizer(max_features=50000, stop_words=None)\n# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n# X_val_tfidf = tfidf_vectorizer.transform(X_val)\n\n# # Combine Count and TF-IDF features\n# X_train_combined = hstack([X_train_counts, X_train_tfidf])\n# X_val_combined = hstack([X_val_counts, X_val_tfidf])\n\n# # Initialize CatBoost Classifier with appropriate parameters\n# model = CatBoostClassifier(iterations=5000, depth=6, learning_rate=0.01, verbose=100, loss_function='MultiClass')\n\n# # Train the model\n# model.fit(X_train_combined ,  y_train)\n\n# # Validate the model\n# val_predictions = model.predict(X_val_combined)\n# print(\"Classification Report for CatBoost:\")\n# print(classification_report(y_val, val_predictions))\n\n# # Prepare the test data by combining the text columns\n# X_test_combined =test['model_a']+ test['model_b'] + test['prompt'] + \" \" + test['response_a'] + \" \" + test['response_b']\n\n# # Transform test data with both Count and TF-IDF vectorizers\n# X_test_counts = count_vectorizer.transform(X_test_combined)\n# X_test_tfidf = tfidf_vectorizer.transform(X_test_combined)\n\n# # Combine test features\n# X_test_combined = hstack([X_test_counts, X_test_tfidf])\n\n# # Predict probabilities for each class\n# test_probabilities = model.predict_proba(X_test_combined)\n\n# # Create submission DataFrame\n# submission = pd.DataFrame(test_probabilities, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n# submission['id'] = test['id']\n\n# # Reorder columns for submission\n# submission = submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# # Save submission to CSV\n# submission_file = '/kaggle/working/submission.csv'\n# submission.to_csv(submission_file, index=False)\n# print(f\"Submission saved to {submission_file}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:45:03.092563Z","iopub.execute_input":"2024-11-13T19:45:03.093605Z","iopub.status.idle":"2024-11-13T19:45:03.100718Z","shell.execute_reply.started":"2024-11-13T19:45:03.09356Z","shell.execute_reply":"2024-11-13T19:45:03.099398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Import necessary libraries\n# import pandas as pd\n# import numpy as np\n# import pickle\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import classification_report\n# from sklearn.ensemble import HistGradientBoostingClassifier\n# import os\n\n# # Define paths to embedding files\n# TRAIN_EMBEDDINGS_DIR = \"/kaggle/input/training-llm-prompt-classifciation-bert-embedding/\"\n# TEST_EMBEDDINGS_DIR = \"/kaggle/working/\"\n\n# # Paths for training embeddings\n# TRAIN_PROMPT_EMBEDDING_PATH = os.path.join(TRAIN_EMBEDDINGS_DIR, \"prompt_bert.pkl\")\n# TRAIN_RESPONSE_A_EMBEDDING_PATH = os.path.join(TRAIN_EMBEDDINGS_DIR, \"response_a_bert.pkl\")\n# TRAIN_RESPONSE_B_EMBEDDING_PATH = os.path.join(TRAIN_EMBEDDINGS_DIR, \"response_b_bert.pkl\")\n\n# # Paths for test embeddings\n# TEST_PROMPT_EMBEDDING_PATH = os.path.join(TEST_EMBEDDINGS_DIR, \"prompt_bert.pkl\")\n# TEST_RESPONSE_A_EMBEDDING_PATH = os.path.join(TEST_EMBEDDINGS_DIR, \"response_a_bert.pkl\")\n# TEST_RESPONSE_B_EMBEDDING_PATH = os.path.join(TEST_EMBEDDINGS_DIR, \"response_b_bert.pkl\")\n\n# # Load training data\n# train_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n# test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# # Encode the target columns\n# train_df['target'] = np.select(\n#     [\n#         train_df['winner_model_a'] == 1,\n#         train_df['winner_model_b'] == 1,\n#         train_df['winner_tie'] == 1\n#     ],\n#     [0, 1, 2],\n#     default=-1  # Assign -1 for any invalid cases\n# )\n\n# # Verify that there are no invalid targets\n# if (train_df['target'] == -1).any():\n#     raise ValueError(\"There are invalid target labels in the training data.\")\n\n# # Define target variable\n# y = train_df['target']\n\n# # Load BERT embeddings for training data\n# with open(TRAIN_PROMPT_EMBEDDING_PATH, 'rb') as f:\n#     prompt_embeddings_train = pickle.load(f)\n\n# with open(TRAIN_RESPONSE_A_EMBEDDING_PATH, 'rb') as f:\n#     response_a_embeddings_train = pickle.load(f)\n\n# with open(TRAIN_RESPONSE_B_EMBEDDING_PATH, 'rb') as f:\n#     response_b_embeddings_train = pickle.load(f)\n\n# # Verify that all embeddings have the same number of samples\n# num_train_samples = train_df.shape[0]\n# assert prompt_embeddings_train.shape[0] == num_train_samples, \"Mismatch in number of samples for prompt embeddings.\"\n# assert response_a_embeddings_train.shape[0] == num_train_samples, \"Mismatch in number of samples for response A embeddings.\"\n# assert response_b_embeddings_train.shape[0] == num_train_samples, \"Mismatch in number of samples for response B embeddings.\"\n\n# # Concatenate all embeddings to form the feature matrix\n# X_train_full = np.hstack([\n#     prompt_embeddings_train,\n#     response_a_embeddings_train,\n#     response_b_embeddings_train\n# ])\n\n# # Load BERT embeddings for test data\n# with open(TEST_PROMPT_EMBEDDING_PATH, 'rb') as f:\n#     prompt_embeddings_test = pickle.load(f)\n\n# with open(TEST_RESPONSE_A_EMBEDDING_PATH, 'rb') as f:\n#     response_a_embeddings_test = pickle.load(f)\n\n# with open(TEST_RESPONSE_B_EMBEDDING_PATH, 'rb') as f:\n#     response_b_embeddings_test = pickle.load(f)\n\n# # Verify that all test embeddings have the same number of samples\n# num_test_samples = test_df.shape[0]\n# assert prompt_embeddings_test.shape[0] == num_test_samples, \"Mismatch in number of samples for test prompt embeddings.\"\n# assert response_a_embeddings_test.shape[0] == num_test_samples, \"Mismatch in number of samples for test response A embeddings.\"\n# assert response_b_embeddings_test.shape[0] == num_test_samples, \"Mismatch in number of samples for test response B embeddings.\"\n\n# # Concatenate all test embeddings to form the test feature matrix\n# X_test = np.hstack([\n#     prompt_embeddings_test,\n#     response_a_embeddings_test,\n#     response_b_embeddings_test\n# ])\n\n# # Split the training data into training and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(\n#     X_train_full,\n#     y,\n#     test_size=0.02,\n#     random_state=42,\n#     stratify=y\n# )\n\n# # Initialize HistGradientBoostingClassifier\n# hgb_model = HistGradientBoostingClassifier(\n#     max_iter=100,\n#     max_depth=10,\n#     random_state=42,\n#     class_weight=\"balanced\"  # Adjust for potential class imbalance\n# )\n\n# # Train the HistGradientBoosting model\n# print(\"Training HistGradientBoosting model...\")\n# hgb_model.fit(X_train, y_train)\n# print(\"HistGradientBoosting training completed.\")\n\n# # Validate the HistGradientBoosting model\n# print(\"Evaluating HistGradientBoosting model on validation set...\")\n# hgb_val_predictions = hgb_model.predict(X_val)\n# print(\"Classification Report for HistGradientBoosting:\")\n# print(classification_report(y_val, hgb_val_predictions))\n\n# # Make predictions on the test set using HistGradientBoosting\n# print(\"Making predictions on test data with HistGradientBoosting...\")\n# hgb_test_predictions = hgb_model.predict_proba(X_test)\n\n# # Ensure that hgb_test_predictions has the correct number of classes\n# num_classes = len(hgb_model.classes_)\n# if hgb_test_predictions.shape[1] != num_classes:\n#     raise ValueError(f\"Expected {num_classes} classes, but got {hgb_test_predictions.shape[1]}.\")\n\n# # Create HistGradientBoosting submission DataFrame\n# hgb_submission = pd.DataFrame(hgb_test_predictions, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n# hgb_submission['id'] = test_df['id']\n\n# # Reorder columns for submission\n# hgb_submission = hgb_submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# # Save the HistGradientBoosting submission file\n# hgb_submission_file = '/kaggle/working/submission.csv'\n# hgb_submission.to_csv(hgb_submission_file, index=False)\n# print(f\"HistGradientBoosting submission saved to {hgb_submission_file}\")\n\n# # Optionally, display the first few rows of the HistGradientBoosting submission file\n# print(\"First few rows of the HistGradientBoosting submission file:\")\n# print(hgb_submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:45:03.135719Z","iopub.execute_input":"2024-11-13T19:45:03.136133Z","iopub.status.idle":"2024-11-13T19:45:03.145247Z","shell.execute_reply.started":"2024-11-13T19:45:03.136093Z","shell.execute_reply":"2024-11-13T19:45:03.144113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Import necessary libraries\n# import pandas as pd\n# import numpy as np\n# import pickle\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import classification_report\n# from sklearn.ensemble import GradientBoostingClassifier\n# import os\n\n# # Define paths to embedding files\n# TRAIN_EMBEDDINGS_DIR = \"/kaggle/input/training-llm-prompt-classifciation-bert-embedding/\"\n# TEST_EMBEDDINGS_DIR = \"/kaggle/working/\"\n\n# # Paths for training embeddings\n# TRAIN_PROMPT_EMBEDDING_PATH = os.path.join(TRAIN_EMBEDDINGS_DIR, \"prompt_bert.pkl\")\n# TRAIN_RESPONSE_A_EMBEDDING_PATH = os.path.join(TRAIN_EMBEDDINGS_DIR, \"response_a_bert.pkl\")\n# TRAIN_RESPONSE_B_EMBEDDING_PATH = os.path.join(TRAIN_EMBEDDINGS_DIR, \"response_b_bert.pkl\")\n\n# # Paths for test embeddings\n# TEST_PROMPT_EMBEDDING_PATH = os.path.join(TEST_EMBEDDINGS_DIR, \"prompt_bert.pkl\")\n# TEST_RESPONSE_A_EMBEDDING_PATH = os.path.join(TEST_EMBEDDINGS_DIR, \"response_a_bert.pkl\")\n# TEST_RESPONSE_B_EMBEDDING_PATH = os.path.join(TEST_EMBEDDINGS_DIR, \"response_b_bert.pkl\")\n\n# # Load training data\n# train_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n# test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# # Encode the target columns\n# train_df['target'] = np.select(\n#     [\n#         train_df['winner_model_a'] == 1,\n#         train_df['winner_model_b'] == 1,\n#         train_df['winner_tie'] == 1\n#     ],\n#     [0, 1, 2],\n#     default=-1  # Assign -1 for any invalid cases\n# )\n\n# # Verify that there are no invalid targets\n# if (train_df['target'] == -1).any():\n#     raise ValueError(\"There are invalid target labels in the training data.\")\n\n# # Define target variable\n# y = train_df['target']\n\n# # Load BERT embeddings for training data\n# with open(TRAIN_PROMPT_EMBEDDING_PATH, 'rb') as f:\n#     prompt_embeddings_train = pickle.load(f)\n\n# with open(TRAIN_RESPONSE_A_EMBEDDING_PATH, 'rb') as f:\n#     response_a_embeddings_train = pickle.load(f)\n\n# with open(TRAIN_RESPONSE_B_EMBEDDING_PATH, 'rb') as f:\n#     response_b_embeddings_train = pickle.load(f)\n\n# # Verify that all embeddings have the same number of samples\n# num_train_samples = train_df.shape[0]\n# assert prompt_embeddings_train.shape[0] == num_train_samples, \"Mismatch in number of samples for prompt embeddings.\"\n# assert response_a_embeddings_train.shape[0] == num_train_samples, \"Mismatch in number of samples for response A embeddings.\"\n# assert response_b_embeddings_train.shape[0] == num_train_samples, \"Mismatch in number of samples for response B embeddings.\"\n\n# # Concatenate all embeddings to form the feature matrix\n# X_train_full = np.hstack([\n#     prompt_embeddings_train,\n#     response_a_embeddings_train,\n#     response_b_embeddings_train\n# ])\n\n# # Load BERT embeddings for test data\n# with open(TEST_PROMPT_EMBEDDING_PATH, 'rb') as f:\n#     prompt_embeddings_test = pickle.load(f)\n\n# with open(TEST_RESPONSE_A_EMBEDDING_PATH, 'rb') as f:\n#     response_a_embeddings_test = pickle.load(f)\n\n# with open(TEST_RESPONSE_B_EMBEDDING_PATH, 'rb') as f:\n#     response_b_embeddings_test = pickle.load(f)\n\n# # Verify that all test embeddings have the same number of samples\n# num_test_samples = test_df.shape[0]\n# assert prompt_embeddings_test.shape[0] == num_test_samples, \"Mismatch in number of samples for test prompt embeddings.\"\n# assert response_a_embeddings_test.shape[0] == num_test_samples, \"Mismatch in number of samples for test response A embeddings.\"\n# assert response_b_embeddings_test.shape[0] == num_test_samples, \"Mismatch in number of samples for test response B embeddings.\"\n\n# # Concatenate all test embeddings to form the test feature matrix\n# X_test = np.hstack([\n#     prompt_embeddings_test,\n#     response_a_embeddings_test,\n#     response_b_embeddings_test\n# ])\n\n# # Split the training data into training and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(\n#     X_train_full,\n#     y,\n#     test_size=0.02,\n#     random_state=42,\n#     stratify=y\n# )\n\n# # Initialize GradientBoostingClassifier\n# gb_model = GradientBoostingClassifier(\n#     n_estimators=100,\n#     max_depth=10,\n#     random_state=42\n# )\n\n# # Train the GradientBoosting model\n# print(\"Training GradientBoosting model...\")\n# gb_model.fit(X_train, y_train)\n# print(\"GradientBoosting training completed.\")\n\n# # Validate the GradientBoosting model\n# print(\"Evaluating GradientBoosting model on validation set...\")\n# gb_val_predictions = gb_model.predict(X_val)\n# print(\"Classification Report for GradientBoosting:\")\n# print(classification_report(y_val, gb_val_predictions))\n\n# # Make predictions on the test set using GradientBoosting\n# print(\"Making predictions on test data with GradientBoosting...\")\n# gb_test_predictions = gb_model.predict_proba(X_test)\n\n# # Ensure that gb_test_predictions has the correct number of classes\n# num_classes = len(gb_model.classes_)\n# if gb_test_predictions.shape[1] != num_classes:\n#     raise ValueError(f\"Expected {num_classes} classes, but got {gb_test_predictions.shape[1]}.\")\n\n# # Create GradientBoosting submission DataFrame\n# gb_submission = pd.DataFrame(gb_test_predictions, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n# gb_submission['id'] = test_df['id']\n\n# # Reorder columns for submission\n# gb_submission = gb_submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# # Save the GradientBoosting submission file\n# gb_submission_file = '/kaggle/working/submission.csv'\n# gb_submission.to_csv(gb_submission_file, index=False)\n# print(f\"GradientBoosting submission saved to {gb_submission_file}\")\n\n# # Optionally, display the first few rows of the GradientBoosting submission file\n# print(\"First few rows of the GradientBoosting submission file:\")\n# print(gb_submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:45:03.30638Z","iopub.execute_input":"2024-11-13T19:45:03.306773Z","iopub.status.idle":"2024-11-13T19:45:03.315294Z","shell.execute_reply.started":"2024-11-13T19:45:03.306734Z","shell.execute_reply":"2024-11-13T19:45:03.314152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"===========Getting Embedding(Vectors Tensor) of ============\")\n\n# import pandas as pd\n# import numpy as np\n# import re\n# import nltk\n# import torch\n# import pickle\n# from collections import Counter\n# from nltk.corpus import stopwords\n# from transformers import BertTokenizer, BertModel, XLNetTokenizer, XLNetModel, GPT2Tokenizer, GPT2Model, T5Tokenizer, T5Model\n# import spacy\n\n# # Ensure NLTK's stopwords are downloaded\n# nltk.download('stopwords')\n\n# # Check if GPU is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # Load the data\n# train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv', nrows=5000)\n# test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv', nrows=5000)\n\n# # Load models and transfer them to GPU if available\n# print(\"Loading models...\")\n\n# # BERT\n# bert_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_tokenizer')\n# bert_model = BertModel.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_model').to(device)\n\n\n# # XLNet\n# xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"/kaggle/input/vector-model-embedder/models/xlnet_tokenizer\")\n# xlnet_model = XLNetModel.from_pretrained(\"/kaggle/input/vector-model-embedder/models/xlnet_model\").to(device)\n\n\n# # GPT-2\n# gpt2_tokenizer = GPT2Tokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/gpt2_tokenizer')\n# gpt2_model = GPT2Model.from_pretrained('/kaggle/input/vector-model-embedder/models/gpt2_model').to(device)\n\n# # T5\n# t5_tokenizer = T5Tokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/t5_tokenizer')\n# t5_model = T5Model.from_pretrained('/kaggle/input/vector-model-embedder/models/t5_model').to(device)\n\n\n# # SpaCy\n# nlp_local = spacy.load(\"/kaggle/input/vector-model-embedder/models/spacy_model\")\n\n\n# # Text preprocessing function\n# def preprocess_text(text):\n#     text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n#     words = text.lower().split()  # Lowercase and split the text into words\n#     stop_words = set(stopwords.words('english'))  # Set of stopwords\n#     words = [word for word in words if word not in stop_words]  # Remove stopwords\n#     unique_words = list(Counter(words).keys())  # Get unique words\n#     return ' '.join(unique_words)  # Join them back into a string\n\n# # Apply preprocessing to all columns that will be used for embedding extraction\n# train = train.applymap(preprocess_text)\n# test = test.applymap(preprocess_text)\n\n# # Function to extract embeddings from a model\n# def extract_embeddings(text, model, tokenizer, max_length=512):\n#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n#     embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mean pooling over token embeddings\n#     return embedding\n\n# # Function to extract embeddings using SpaCy\n# def text_to_vector(text):\n#     doc = nlp_local(text)\n#     return doc.vector\n\n# # Function to save embeddings to pickle files\n# def save_embeddings(data, filename):\n#     with open(filename, 'wb') as f:\n#         pickle.dump(data, f)\n#     print(f\"Saved embeddings to {filename}\")\n\n# # Function to generate and save embeddings for a list of columns\n# def generate_and_save_embeddings(df, column_names):\n#     for column_name in column_names:\n#         print(f\"Processing column: {column_name}\")\n#         column_data = df[column_name].dropna()  # Remove any NaN values if present\n        \n#         # BERT embeddings\n#         bert_embeddings = np.array([extract_embeddings(text, bert_model, bert_tokenizer) for text in column_data])\n#         save_embeddings(bert_embeddings, f'/kaggle/working/{column_name}_bert.pkl')\n\n#         # XLNet embeddings\n#         xlnet_embeddings = np.array([extract_embeddings(text, xlnet_model, xlnet_tokenizer) for text in column_data])\n#         save_embeddings(xlnet_embeddings, f'/kaggle/working/{column_name}_xlnet.pkl')\n\n#         # GPT-2 embeddings\n#         gpt2_embeddings = np.array([extract_embeddings(text, gpt2_model, gpt2_tokenizer) for text in column_data])\n#         save_embeddings(gpt2_embeddings, f'/kaggle/working/{column_name}_gpt2.pkl')\n\n#         # T5 embeddings\n#         t5_embeddings = np.array([extract_embeddings(text, t5_model, t5_tokenizer) for text in column_data])\n#         save_embeddings(t5_embeddings, f'/kaggle/working/{column_name}_t5.pkl')\n\n#         # SpaCy embeddings\n#         spacy_embeddings = np.array([text_to_vector(text) for text in column_data])\n#         save_embeddings(spacy_embeddings, f'/kaggle/working/{column_name}_spacy.pkl')\n\n# # List of columns for which embeddings need to be generated\n# columns_to_embed = ['prompt', 'response_a', 'response_b']  # Replace with actual column names\n\n# # Generate and save embeddings for the specified columns in both train and test data\n# generate_and_save_embeddings(train, columns_to_embed)\n# generate_and_save_embeddings(test, columns_to_embed)\n\n# print(\"All embeddings generated and saved successfully.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:45:03.34793Z","iopub.execute_input":"2024-11-13T19:45:03.348376Z","iopub.status.idle":"2024-11-13T19:45:03.356241Z","shell.execute_reply.started":"2024-11-13T19:45:03.348333Z","shell.execute_reply":"2024-11-13T19:45:03.355211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\"\"================== Loading Train Features and Testing Features =============\"\"\")\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport pickle\n\n\"\"\"=================== Getting embedding for test file ===============\"\"\"\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport torch\nimport pickle\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Ensure NLTK's stopwords are downloaded\nnltk.download('stopwords')\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n\n# Load BERT model and tokenizer, and transfer the model to GPU if available\nprint(\"Loading BERT model...\")\nbert_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_tokenizer')\nbert_model = BertModel.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_model').to(device)\n\n\n# Text preprocessing function\ndef preprocess_text(text):\n    if isinstance(text, str):  # Ensure text is a string before processing\n        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n        words = text.lower().split()  # Lowercase and split the text into words\n        stop_words = set(stopwords.words('english'))  # Set of stopwords\n        words = [word for word in words if word not in stop_words]  # Remove stopwords\n        unique_words = list(Counter(words).keys())  # Get unique words\n        return ' '.join(unique_words)  # Join them back into a string\n    return ''  # Return empty string if text is not a valid string\n\n\n# Apply preprocessing only to specific columns\ncolumns_to_preprocess = ['prompt', 'response_a', 'response_b']  # Replace with actual column names\nfor col in columns_to_preprocess:\n    train[col] = train[col].apply(preprocess_text)\n    # test[col] = test[col].apply(preprocess_text)\n\n\n# Function to extract embeddings from BERT\ndef extract_embeddings(text, model, tokenizer, max_length=512):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mean pooling over token embeddings\n    return embedding\n\n\n# Function to save embeddings to a pickle file\ndef save_embeddings(data, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(data, f)\n    print(f\"Saved embeddings to {filename}\")\n\n\n# Function to generate and save embeddings for each column\ndef generate_and_save_embeddings(df, column_name):\n    print(f\"Processing column: {column_name}\")\n    column_data = df[column_name].dropna()  # Remove any NaN values if present\n\n    # BERT embeddings\n    embeddings = np.array([extract_embeddings(text, bert_model, bert_tokenizer) for text in column_data])\n    save_embeddings(embeddings, f'/kaggle/working/{column_name}_bert.pkl')\n    print(f\"SAVED IS  {column_name}\")\n    return embeddings\n\n\n# Generate and save embeddings for each column\nprompt_embeddings = generate_and_save_embeddings(train, 'prompt')\nresponse_a_embeddings = generate_and_save_embeddings(train, 'response_a')\nresponse_b_embeddings = generate_and_save_embeddings(train, 'response_b')\n\n\n\n# Define function to calculate metrics\ndef calculate_metrics(prompt_emb, response_a_emb, response_b_emb):\n    metrics = {}\n\n    # 1. Cosine Similarity\n    metrics['cosine_similarity_a'] = cosine_similarity([prompt_emb], [response_a_emb])[0][0]\n    metrics['cosine_similarity_b'] = cosine_similarity([prompt_emb], [response_b_emb])[0][0]\n\n    # 2. Cosine Distance\n    metrics['cosine_distance_a'] = 1 - metrics['cosine_similarity_a']\n    metrics['cosine_distance_b'] = 1 - metrics['cosine_similarity_b']\n\n    # 3. Mean Euclidean Distance\n    metrics['euclidean_distance_a'] = euclidean_distances([prompt_emb], [response_a_emb])[0][0]\n    metrics['euclidean_distance_b'] = euclidean_distances([prompt_emb], [response_b_emb])[0][0]\n\n    # 4. Semantic Overlap (measured by cosine similarity)\n    metrics['semantic_overlap_a'] = metrics['cosine_similarity_a']\n    metrics['semantic_overlap_b'] = metrics['cosine_similarity_b']\n\n    # 5. Topic Coherence (same as semantic overlap for now)\n    metrics['topic_coherence_a'] = metrics['cosine_similarity_a']\n    metrics['topic_coherence_b'] = metrics['cosine_similarity_b']\n\n    # 6. Prompt Adherence\n    metrics['prompt_adherence_a'] = metrics['cosine_similarity_a']\n    metrics['prompt_adherence_b'] = metrics['cosine_similarity_b']\n\n    # 7. Contextual Consistency\n    metrics['contextual_consistency'] = cosine_similarity([response_a_emb], [response_b_emb])[0][0]\n\n    # 8. Diversity of Response\n    metrics['response_diversity'] = 1 - metrics['contextual_consistency']\n\n    # 9. Sentiment Similarity Score\n    metrics['sentiment_similarity_a'] = metrics['cosine_similarity_a']  # Proxy\n    metrics['sentiment_similarity_b'] = metrics['cosine_similarity_b']  # Proxy\n\n    # 10. Topic Coverage (same as semantic overlap for simplicity)\n    metrics['topic_coverage_a'] = metrics['cosine_similarity_a']\n    metrics['topic_coverage_b'] = metrics['cosine_similarity_b']\n\n\n    # 12. Depth of Response (Variance of embeddings as proxy)\n    metrics['depth_of_response_a'] = np.var(response_a_emb)\n    metrics['depth_of_response_b'] = np.var(response_b_emb)\n\n    # 13. Focus Score (Inverse of Euclidean Distance)\n    metrics['focus_score_a'] = 1 / (1 + metrics['euclidean_distance_a'])\n    metrics['focus_score_b'] = 1 / (1 + metrics['euclidean_distance_b'])\n\n    # 14. Embedding Vector Magnitude Ratio\n    metrics['embedding_magnitude_ratio_a'] = np.linalg.norm(response_a_emb) / np.linalg.norm(prompt_emb)\n    metrics['embedding_magnitude_ratio_b'] = np.linalg.norm(response_b_emb) / np.linalg.norm(prompt_emb)\n\n    # 15. Off-Topic Score\n    metrics['off_topic_a'] = metrics['cosine_distance_a']\n    metrics['off_topic_b'] = metrics['cosine_distance_b']\n\n\n    # 16. Euclidean Distance\n    metrics['euclidean_distance_a'] = euclidean_distances([prompt_emb], [response_a_emb])[0][0]\n    metrics['euclidean_distance_b'] = euclidean_distances([prompt_emb], [response_b_emb])[0][0]\n\n    # 17. Embedding Density\n    metrics['embedding_density_a'] = np.sum(np.square(response_a_emb))\n    metrics['embedding_density_b'] = np.sum(np.square(response_b_emb))\n\n    # 18. Internal Coherence\n    metrics['internal_coherence_a'] = np.mean(cosine_similarity([response_a_emb]))\n    metrics['internal_coherence_b'] = np.mean(cosine_similarity([response_b_emb]))\n\n    # 19. Response Concreteness\n    metrics['response_concreteness_a'] = np.std(response_a_emb)\n    metrics['response_concreteness_b'] = np.std(response_b_emb)\n\n    # 20. Redundancy Score\n    metrics['redundancy_score_a'] = 1 - np.mean(np.std(response_a_emb))\n    metrics['redundancy_score_b'] = 1 - np.mean(np.std(response_b_emb))\n\n    # 21. Semantic Entailment (similar to cosine similarity)\n    metrics['semantic_entailment_a'] = metrics['cosine_similarity_a']\n    metrics['semantic_entailment_b'] = metrics['cosine_similarity_b']\n\n    # 22. Prompt-Response Embedding Ratio\n    metrics['embedding_ratio_a'] = np.linalg.norm(response_a_emb) / np.linalg.norm(prompt_emb)\n    metrics['embedding_ratio_b'] = np.linalg.norm(response_b_emb) / np.linalg.norm(prompt_emb)\n\n    # 23. Informativeness Score\n    metrics['informativeness_a'] = np.var(response_a_emb)\n    metrics['informativeness_b'] = np.var(response_b_emb)\n\n    # 24. Topic Drift Score\n    metrics['topic_drift_a'] = np.std(response_a_emb - prompt_emb)\n    metrics['topic_drift_b'] = np.std(response_b_emb - prompt_emb)\n\n    # 25. Context Alignment\n    metrics['context_alignment'] = cosine_similarity([response_a_emb], [response_b_emb])[0][0]\n\n    #26. Disparity Score\n    metrics['disparity_score'] = euclidean_distances([response_a_emb], [response_b_emb])[0][0]\n\n\n    return metrics\n\n\ndef load_data(prompt_path,response_a_path , response_b_path):\n    # Load precomputed embeddings (assuming they've been generated and saved as instructed)\n    prompt_embeddings = pickle.load(open(prompt_path, 'rb'))\n    response_a_embeddings = pickle.load(open(response_a_path, 'rb'))\n    response_b_embeddings = pickle.load(open(response_b_path , 'rb'))\n\n    train_features_data = []\n    for i in range(len(prompt_embeddings)):\n        metrics = calculate_metrics(prompt_embeddings[i], response_a_embeddings[i], response_b_embeddings[i])\n        train_features_data.append(metrics)\n    train_features_data = pd.DataFrame(train_features_data)\n    return train_features_data\n\nprompt_path = '/kaggle/input/training-llm-prompt-classifciation-bert-embedding/prompt_bert.pkl'\nresponse_a_path = '/kaggle/input/training-llm-prompt-classifciation-bert-embedding/response_a_bert.pkl'\nresponse_b_path  =  '/kaggle/input/training-llm-prompt-classifciation-bert-embedding/response_b_bert.pkl'\ntrain_features_from_embedding = load_data(prompt_path,response_a_path , response_b_path)\n\nprompt_path = '/kaggle/working/prompt_bert.pkl'\nresponse_a_path ='/kaggle/working/response_a_bert.pkl'\nresponse_b_path = '/kaggle/working/response_b_bert.pkl'\ntest_features_from_embedding = load_data(prompt_path,response_a_path , response_b_path)\n\nprint('======================== TRAIN_FEATURES AND TEST_FEATURES LOADED ===============================')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:45:03.361316Z","iopub.execute_input":"2024-11-13T19:45:03.362007Z","iopub.status.idle":"2024-11-13T19:48:25.18479Z","shell.execute_reply.started":"2024-11-13T19:45:03.36196Z","shell.execute_reply":"2024-11-13T19:48:25.183298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('===================Training and submission ceation =======')\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport os\n\n# Load training data\ntrain_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# Encode the target columns\ntrain_df['target'] = np.select(\n    [\n        train_df['winner_model_a'] == 1,\n        train_df['winner_model_b'] == 1,\n        train_df['winner_tie'] == 1\n    ],\n    [0, 1, 2],\n    default=-1  # Assign -1 for any invalid cases\n)\n\n# Verify that there are no invalid targets\nif (train_df['target'] == -1).any():\n    raise ValueError(\"There are invalid target labels in the training data.\")\n\n# Define target variable\ny = train_df['target']\n\n\n# Split the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    train_features_from_embedding,\n    y,\n    test_size=0.02,\n    random_state=42,\n    stratify=y\n)\n\n\n# Initialize GradientBoostingClassifier\ngb_model = GradientBoostingClassifier(\n    n_estimators=100,\n    max_depth=10,\n    random_state=42\n)\n\n# Train the GradientBoosting model\nprint(\"Training GradientBoosting model...\")\ngb_model.fit(X_train, y_train)\nprint(\"GradientBoosting training completed.\")\n\n# Validate the GradientBoosting model\nprint(\"Evaluating GradientBoosting model on validation set...\")\ngb_val_predictions = gb_model.predict(X_val)\nprint(\"Classification Report for GradientBoosting:\")\nprint(classification_report(y_val, gb_val_predictions))\n\n# Make predictions on the test set using GradientBoosting\nprint(\"Making predictions on test data with GradientBoosting...\")\ngb_test_predictions = gb_model.predict_proba(test_features_from_embedding)\n\n# Ensure that gb_test_predictions has the correct number of classes\nnum_classes = len(gb_model.classes_)\nif gb_test_predictions.shape[1] != num_classes:\n    raise ValueError(f\"Expected {num_classes} classes, but got {gb_test_predictions.shape[1]}.\")\n\n# Create GradientBoosting submission DataFrame\ngb_submission = pd.DataFrame(gb_test_predictions, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\ngb_submission['id'] = test_df['id']\n\n# Reorder columns for submission\ngb_submission = gb_submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# Save the GradientBoosting submission file\ngb_submission_file = '/kaggle/working/submission.csv'\ngb_submission.to_csv(gb_submission_file, index=False)\nprint(f\"GradientBoosting submission saved to {gb_submission_file}\")\n\n# Optionally, display the first few rows of the GradientBoosting submission file\nprint(\"First few rows of the GradientBoosting submission file:\")\nprint(gb_submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:48:41.969555Z","iopub.execute_input":"2024-11-13T19:48:41.970023Z","iopub.status.idle":"2024-11-13T20:06:59.4922Z","shell.execute_reply.started":"2024-11-13T19:48:41.969984Z","shell.execute_reply":"2024-11-13T20:06:59.489578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import spacy\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.metrics.pairwise import cosine_similarity\n# from nltk.translate.bleu_score import sentence_bleu\n# from sklearn.metrics.pairwise import cosine_similarity\n# from nltk.translate.bleu_score import sentence_bleu\n# from collections import Counter\n\n\n# # Helper functions for different metrics\n# def jaccard_similarity(text1, text2):\n#     words1 = set(text1.split())\n#     words2 = set(text2.split())\n#     return len(words1 & words2) / len(words1 | words2)\n\n# def lexical_diversity(text):\n#     words = text.split()\n#     return len(set(words)) / len(words)\n\n# def topic_modeling_score(text1, text2):\n#     # This can be replaced with actual topic modeling logic (e.g., using LDA or BERT embeddings)\n#     return cosine_similarity(model.encode([text1]), model.encode([text2]))[0][0]\n\n\n# def lexical_variety(text):\n#     words = text.split()\n#     return len(set(words)) / len(words)\n\n\n# # Initialize Spacy NLP model\n# nlp = spacy.load('en_core_web_sm')\n\n\n# # Function to compute cosine similarity (Relevance)\n# def calculate_relevance(prompt, response):\n#     vectorizer = TfidfVectorizer(stop_words='english')\n#     tfidf_matrix = vectorizer.fit_transform([prompt, response])\n#     similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n#     return similarity[0][0] * 10  # Scale to 0-10 range\n\n# # Function to calculate coherence (using sentence length and structure)\n# def calculate_coherence(response):\n#     doc = nlp(response)\n#     sentence_lengths = [len(sent) for sent in doc.sents]\n#     avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n#     return min(10, avg_sentence_length / 10)\n\n\n\n# # Function to calculate creativity (based on vocabulary diversity)\n# def calculate_creativity(response):\n#     doc = nlp(response)\n#     unique_words = set([token.text.lower() for token in doc if token.is_alpha])\n#     return min(10, len(unique_words) / 5)\n\n# # Function to calculate engagement (based on the use of questions)\n# def calculate_engagement(response):\n#     doc = nlp(response)\n#     questions = [sent for sent in doc.sents if '?' in sent.text]\n#     return min(10, len(questions))\n\n# # Function to calculate conciseness (based on word count)\n# def calculate_conciseness(response):\n#     word_count = len(response.split())\n#     return max(0, 10 - word_count / 10)\n\n# # Function to calculate fluency and naturalness (based on average sentence length and grammar)\n# def calculate_fluency(response):\n#     doc = nlp(response)\n#     sentence_lengths = [len(sent) for sent in doc.sents]\n#     avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n#     return min(10, avg_sentence_length / 10)\n\n# # Function to calculate completeness (based on the presence of main points)\n# def calculate_completeness(prompt, response):\n#     # Check if the response covers key points mentioned in the prompt\n#     response_tokens = set(response.lower().split())\n#     prompt_tokens = set(prompt.lower().split())\n#     common_tokens = response_tokens.intersection(prompt_tokens)\n#     completeness_score = len(common_tokens) / len(prompt_tokens) * 10\n#     return min(10, completeness_score)\n\n# # BLEU Score function\n# def calculate_bleu(reference, response):\n#     reference = [reference.split()]\n#     response = response.split()\n#     return sentence_bleu(reference, response) * 10  # Scale to 0-10 range\n\n\n# # Levenshtein (Edit Distance) Score function\n# def calculate_levenshtein(reference, response):\n#     return (1 - Levenshtein.distance(reference, response) / max(len(reference), len(response))) * 10  # Scale to 0-10 range\n\n\n\n# def evaluate_response(prompt, response_1, response_2):\n#     # Calculate all metrics for Response 1\n#     metrics_response = {\n#         'response_a_Coherence': calculate_coherence(response_1),\n#         'response_a_Completeness': calculate_completeness(prompt, response_1),\n#         'response_a_Engagement': calculate_engagement(response_1),\n#         'response_a_Conciseness': calculate_conciseness(response_1),\n#         'response_a_Creativity': calculate_creativity(response_1),\n#         'response_a_Fluency and Naturalness': calculate_fluency(response_1),\n#         'response_a_BLEU': calculate_bleu(prompt, response_1),\n#         'response_a_jaccard_1' : jaccard_similarity(prompt, response_1),\n#         'response_a_length_1' : len(response_1.split()),\n#         'response_a_lexical_1' : lexical_diversity(response_1),\n#         'response_b_Coherence': calculate_coherence(response_2),\n#         'response_b_Completeness': calculate_completeness(prompt, response_2),\n#         'response_b_Engagement': calculate_engagement(response_2),\n#         'response_b_Conciseness': calculate_conciseness(response_2),\n#         'response_b_Creativity': calculate_creativity(response_2),\n#         'response_b_Fluency and Naturalness': calculate_fluency(response_2),\n#         'response_b_BLEU': calculate_bleu(prompt, response_2),\n#         'response_b_jaccard' : jaccard_similarity(prompt, response_2),\n#         'response_b_length' :len(response_2.split()),\n#         'response_b_lexical' : lexical_diversity(response_2),\n#     }\n    \n#     return metrics_response\n\n\n# def load_text_features(file_path):\n#     df = pd.read_csv(file_path, usecols=['prompt', 'response_a', 'response_b'])\n#     features_list = []\n#     for index, row in df.iterrows():\n#         prompt = row['prompt']\n#         response_a = row['response_a']\n#         response_b = row['response_b']\n        \n#         features_row_dict = evaluate_response(prompt, response_a, response_b)\n#         features_list.append(features_row_dict)\n    \n#     features_df = pd.DataFrame(features_list)\n#     return  features_df\n    \n\n# import pandas as pd\n# train_path = '/kaggle/input/llm-classification-finetuning/train.csv'\n# train_text_features = load_text_features(train_path)\n# test_path = '/kaggle/input/llm-classification-finetuning/test.csv'\n# test_text_features = load_text_features(test_path)\n# print(train_text_features.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:48:25.201601Z","iopub.status.idle":"2024-11-13T19:48:25.202107Z","shell.execute_reply.started":"2024-11-13T19:48:25.201834Z","shell.execute_reply":"2024-11-13T19:48:25.201872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}