{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Finetune LLMs to Predict Human Preference using Chatbot Arena conversations**\n- This notebook contain a solution for the LLM Classification Finetuning on Kaggle\n\n**Main objective:** Predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models(LLMs).\n\nThe LLM Classification Finetuning competition challenges participants to predict user preferences between responses generated by different large language models (LLMs). The dataset comprises conversations from the Chatbot Arena, where users compare responses from two anonymous LLMs and select their preferred answer. The objective is to develop a model that accurately forecasts which response a user would favor in these head-to-head comparisons.\n\n\n\n","metadata":{"id":"ETb0hG7QPQ6j"}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom scipy.sparse import hstack\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:12:49.864245Z","iopub.execute_input":"2025-01-10T12:12:49.864506Z","iopub.status.idle":"2025-01-10T12:12:51.383096Z","shell.execute_reply.started":"2025-01-10T12:12:49.864481Z","shell.execute_reply":"2025-01-10T12:12:51.381905Z"},"id":"ipnqa0rLPQ6n","outputId":"4a3cbbfd-afa5-4d97-df77-483d7cbc580c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **1.Import the data**","metadata":{"id":"0HQbzXyaPQ6n"}},{"cell_type":"code","source":"# Read the dataset\ntrain = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:12:55.462673Z","iopub.execute_input":"2025-01-10T12:12:55.463058Z","iopub.status.idle":"2025-01-10T12:12:59.396516Z","shell.execute_reply.started":"2025-01-10T12:12:55.463024Z","shell.execute_reply":"2025-01-10T12:12:59.395219Z"},"id":"L_dMYoDvPQ6o"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:03.067401Z","iopub.execute_input":"2025-01-10T12:13:03.06776Z","iopub.status.idle":"2025-01-10T12:13:03.089065Z","shell.execute_reply.started":"2025-01-10T12:13:03.067728Z","shell.execute_reply":"2025-01-10T12:13:03.087878Z"},"id":"FBufa_L2PQ6o","outputId":"c7aab1e7-70e0-4a5c-f557-a0ccce0bfba2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:05.370904Z","iopub.execute_input":"2025-01-10T12:13:05.37131Z","iopub.status.idle":"2025-01-10T12:13:05.385533Z","shell.execute_reply.started":"2025-01-10T12:13:05.371277Z","shell.execute_reply":"2025-01-10T12:13:05.384378Z"},"id":"TkmSy9-CPQ6o","outputId":"e0280188-5f8c-4c53-cf47-a1edc63a386e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:10.381326Z","iopub.execute_input":"2025-01-10T12:13:10.38168Z","iopub.status.idle":"2025-01-10T12:13:10.391896Z","shell.execute_reply.started":"2025-01-10T12:13:10.381651Z","shell.execute_reply":"2025-01-10T12:13:10.390583Z"},"id":"2ARKUVhAPQ6p","outputId":"819fa082-9024-4af6-fe14-9aba8200f6d1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:13.061563Z","iopub.execute_input":"2025-01-10T12:13:13.061942Z","iopub.status.idle":"2025-01-10T12:13:13.088061Z","shell.execute_reply.started":"2025-01-10T12:13:13.061914Z","shell.execute_reply":"2025-01-10T12:13:13.087037Z"},"id":"hvJiq6vFPQ6p","outputId":"8196ad5b-578b-4bdc-ce57-86021f96c38b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"train data shape: {train.shape}\")\nprint(f\"test data shape: {test.shape}\")\nprint(f\"sample_submission data shape: {submission.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:21.602696Z","iopub.execute_input":"2025-01-10T12:13:21.603045Z","iopub.status.idle":"2025-01-10T12:13:21.609533Z","shell.execute_reply.started":"2025-01-10T12:13:21.603017Z","shell.execute_reply":"2025-01-10T12:13:21.608214Z"},"id":"XI58FB3ZPQ6q","outputId":"c96adfa7-bf2f-4ef6-d81b-bd37647b224c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n**In the context of large language models (LLMs) and machine learning, a prompt refers to the input or instruction given to the model to generate a response or perform a task. It is the text or query that is provided to the model, which influences the generated output**\n\nThe prompt could be a question, statement, or instruction.\nThe response would be the model's output based on that prompt.\n","metadata":{"id":"okn0jk_IPQ6r"}},{"cell_type":"code","source":"print(test['prompt'][0])\nprint(test[\"response_a\"][0])\nprint(test[\"response_b\"][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:27.198905Z","iopub.execute_input":"2025-01-10T12:13:27.19933Z","iopub.status.idle":"2025-01-10T12:13:27.205571Z","shell.execute_reply.started":"2025-01-10T12:13:27.199287Z","shell.execute_reply":"2025-01-10T12:13:27.204348Z"},"id":"7WFGgkEgPQ6r","outputId":"8611b819-a3ef-466f-f6fe-08287c0d3f7b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Lets generate a word cloud : A word cloud is a visual representation (image) of word data. In other words, it is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it's mentioned within a given text and the more important it is**","metadata":{"id":"dqzmM0jVPQ6r"}},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\nwordcloud = WordCloud(width=800, height=800).generate(' '.join(test['prompt'].dropna()))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:30.356528Z","iopub.execute_input":"2025-01-10T12:13:30.356903Z","iopub.status.idle":"2025-01-10T12:13:31.506789Z","shell.execute_reply.started":"2025-01-10T12:13:30.356871Z","shell.execute_reply":"2025-01-10T12:13:31.505738Z"},"id":"0ifoeGXMPQ6r","outputId":"43f90be4-8b96-46ed-e91c-7ea703ee4be3"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2.Data Cleaning & Preparing\n\n\n**After laoding the data we can clean it , ts the process of identifying and correcting or removing errors, inconsistencies, and inaccuracies in a dataset to ensure its quality and reliability.**","metadata":{"id":"U6lbeaIMPQ6s"}},{"cell_type":"code","source":"#checking null or missing data\ntrain.isnull()\ntest.isnull()\nsubmission.isnull()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:36.231612Z","iopub.execute_input":"2025-01-10T12:13:36.232226Z","iopub.status.idle":"2025-01-10T12:13:36.271127Z","shell.execute_reply.started":"2025-01-10T12:13:36.232187Z","shell.execute_reply":"2025-01-10T12:13:36.269948Z"},"id":"rAc7oBDLPQ6s","outputId":"88333ffe-d9ec-4953-e039-053c727f8059"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Check for duplicate data\nprint(test.duplicated().sum())\nprint(train.duplicated().sum())\nprint(submission.duplicated().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:39.725756Z","iopub.execute_input":"2025-01-10T12:13:39.726238Z","iopub.status.idle":"2025-01-10T12:13:40.226668Z","shell.execute_reply.started":"2025-01-10T12:13:39.726201Z","shell.execute_reply":"2025-01-10T12:13:40.225486Z"},"id":"C8POKaEmPQ6s","outputId":"82dc7589-e4bd-4a4a-94ed-92ceb49c6f4d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for duplicate id's\ntotal_id = len(train[\"id\"])\ntotal_unique_id = len(train[\"id\"].unique())\n\nprint(\"Total number of 'id' duplicates:\")\nprint(total_id - total_unique_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:42.958315Z","iopub.execute_input":"2025-01-10T12:13:42.958719Z","iopub.status.idle":"2025-01-10T12:13:42.967766Z","shell.execute_reply.started":"2025-01-10T12:13:42.958685Z","shell.execute_reply":"2025-01-10T12:13:42.966635Z"},"id":"Kn6l2U5XPQ6s","outputId":"d42f9528-bea3-47a9-ba96-02f1e6d92a0c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Features and Labels","metadata":{"id":"9iU2iDHZPQ6t"}},{"cell_type":"markdown","source":"We need to processes the  dataset to determine the winner among three possible outcomes:\n- `winner_model_a`\n- `winner_model_b`\n-  `winner_tie`.\n  So we define a function, `which_winner`, to assign a numerical value to each outcome. If `winner_model_a` is 1, the function returns 0 to indicate Model A is the winner; if `winner_model_b` is 1, it returns 1 for Model B as the winner; and if `winner_tie` is 1, it returns 2 to indicate a tie.\n--> This function is applied row by row to the , and the results are stored in a new column named `winner`.\n--> Next, the `winner` column is converted into a string-based label, `winner_model`, where numerical values are mapped to meaningful descriptions: `0` is converted to \"model a,\" `1` to \"model b,\" and `2` to \"winner tie.\"\n  **This will help simplifying the representation of the target variable Y .**","metadata":{"id":"waIitiOVPQ6t"}},{"cell_type":"code","source":"#barchart - model winner: winner model a, winner model b or winner tie\ndef which_winner(value):\n    if  value[\"winner_model_a\"] == 1:\n         #winner model a\n         value[\"winner_model_b\"] = 0\n         value[\"winner_tie\"] = 0\n         return 0\n    elif value[\"winner_model_b\"] == 1:\n         #winner model b\n         return 1\n    elif value[\"winner_tie\"] == 1:\n         #winner tie\n         return 2\n    return None\n\ntrain[\"winner\"] = train.apply(which_winner, axis=1)\n\ntrain[\"winner_model\"] = train[\"winner\"].astype(str)\ntrain.loc[train[\"winner_model\"] == \"0\", \"winner_model\"] = \"model a\"\ntrain.loc[train[\"winner_model\"] == \"1\", \"winner_model\"] = \"model b\"\ntrain.loc[train[\"winner_model\"] == \"2\", \"winner_model\"] = \"winner tie\"\n\nresult_model_winner = train[\"winner_model\"].value_counts()\nprint(\"model winner:\", result_model_winner)\n\nprint(\"-----\")\nprint(\"Matplotlib barchart, model winner:\")\n\nbarWidth = 0.75\nplt.figure(figsize=(8, 7))\nplt.bar(result_model_winner.index, result_model_winner.values, barWidth, color='b')\nplt.ylabel('Counts', fontweight ='bold', fontsize = 15)\nplt.xlabel('Model winner', fontweight ='bold', fontsize = 15)\nplt.title('LLMs Value Counts - model winner', fontweight ='bold', fontsize = 15)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:46.685931Z","iopub.execute_input":"2025-01-10T12:13:46.686331Z","iopub.status.idle":"2025-01-10T12:13:47.931214Z","shell.execute_reply.started":"2025-01-10T12:13:46.686296Z","shell.execute_reply":"2025-01-10T12:13:47.930031Z"},"id":"-qiptBVkPQ6t","outputId":"7e637365-b7ef-46c3-81ef-1fa261d344c7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:54.066203Z","iopub.execute_input":"2025-01-10T12:13:54.066597Z","iopub.status.idle":"2025-01-10T12:13:54.08323Z","shell.execute_reply.started":"2025-01-10T12:13:54.066561Z","shell.execute_reply":"2025-01-10T12:13:54.082191Z"},"id":"3876vJBLPQ6t","outputId":"7142c323-c349-4dbc-8eab-fa70eefa67da"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#transform text data into numerical form\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features = 150) #without max_features it crashes due to memory limit\nvectorizer_prompt = vectorizer.fit_transform(train[\"prompt\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_response_a = vectorizer.fit_transform(train[\"response_a\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_response_b = vectorizer.fit_transform(train[\"response_b\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\n\n\n\ntemp_prompt = vectorizer.transform(train[\"prompt\"])\ntemp_response_a = vectorizer.fit_transform(train[\"response_a\"])\ntemp_response_b = vectorizer.fit_transform(train[\"response_b\"])\n\nprint(\"vectorizer representation for 'prompt':\\n\", temp_prompt.toarray())\nprint(\"vectorizer representation for 'response a':\\n\", temp_response_a.toarray())\nprint(\"vectorizer representation for 'response b':\\n\", temp_response_b.toarray())\n\nprint(\"Number of elements for the vectorizer representation for 'prompt':\\n\", temp_prompt.shape)\nprint(\"Number of elements for the vectorizer representation for 'response a':\\n\", temp_response_a.shape)\nprint(\"Number of elements for the vectorizer representation for 'response b':\\n\", temp_response_b.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:13:58.234856Z","iopub.execute_input":"2025-01-10T12:13:58.235252Z","iopub.status.idle":"2025-01-10T12:14:46.903195Z","shell.execute_reply.started":"2025-01-10T12:13:58.235218Z","shell.execute_reply":"2025-01-10T12:14:46.902062Z"},"id":"GYW444gtPQ6t","outputId":"185e6ccf-5632-4a44-e651-c30fe35424b9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Using a simple feature and label selection\nimport numpy as np\n\n# the prediction target\ntrain_y = train[\"winner\"].values\n#choosing \"features\"\ntrain_X = np.concatenate((temp_prompt.toarray(), temp_response_a.toarray(), temp_response_b.toarray()), axis=1)\n\nprint(\"Selecting The Prediction Target and Choosing Features Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:15:22.596819Z","iopub.execute_input":"2025-01-10T12:15:22.597262Z","iopub.status.idle":"2025-01-10T12:15:23.062096Z","shell.execute_reply.started":"2025-01-10T12:15:22.597224Z","shell.execute_reply":"2025-01-10T12:15:23.060893Z"},"id":"TOgWAhSvPQ6t","outputId":"2680e98e-9f88-41af-de2b-5cdf3c5652cd"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Model Selection and evaluatingÂ¶\n","metadata":{"id":"Z92oNah0PQ6t"}},{"cell_type":"markdown","source":"**Logistic Regression**\nLogistic Regression predicts the probability for each target class as requested .","metadata":{"id":"AQnrVAT8PQ6u"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom datetime import datetime\n\n#start time to calculate the execution time\nstart_time = datetime.now()\n\nprint(\"Use logistic regression\")\n#Apply the Logistic Regression\nmodel = LogisticRegression(max_iter=500, multi_class='multinomial', solver='saga')\nmodel.fit(train_X, train_y)\n#end time\nend_time = datetime.now()\n#calculate the execution time\nexecution_time = (end_time - start_time).total_seconds()\nprint(f\"The execution time is : {execution_time} secondes\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:15:27.928872Z","iopub.execute_input":"2025-01-10T12:15:27.929644Z","iopub.status.idle":"2025-01-10T12:15:45.815324Z","shell.execute_reply.started":"2025-01-10T12:15:27.929587Z","shell.execute_reply":"2025-01-10T12:15:45.814205Z"},"id":"XLfwJMKfPQ6u","outputId":"c7953246-7612-4299-9742-f2bc3dc21434"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Trying to evaluate the logistic regression model with basic Evaluation Metrics :\n- Accuracy\n- confusion matrix\n- Clqssificqtion Report\n","metadata":{"id":"QvBXPLk7PQ6u"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\n# Split into validation and training data\ntrain_X_train, train_X_val, train_y_train, train_y_val = train_test_split(\n    train_X, train_y, test_size=0.2, random_state=42\n)\n\n# Record start time to calculate the execution time\nstart = datetime.now()\n\n# Make predictions on the validation set\nvalue_y_predict = model.predict(train_X_val)\nprint('Model predicted values:', value_y_predict)\nprint('True values:', train_y_val)\n\n# Predicted probabilities\nvalue_y_probabilities = model.predict_proba(train_X_val)\nprint('Model prediction probabilities (class-wise):\\n', value_y_probabilities)\n\n# Model accuracy\nscore = model.score(train_X_val, train_y_val)\nprint('The Model Accuracy Score:', score)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(train_y_val, value_y_predict)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# Classification report\nreport = classification_report(train_y_val, value_y_predict)  # Arguments fixed\nprint(\"Classification Report:\\n\", report)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:17:10.926771Z","iopub.execute_input":"2025-01-10T12:17:10.927192Z","iopub.status.idle":"2025-01-10T12:17:11.121413Z","shell.execute_reply.started":"2025-01-10T12:17:10.927155Z","shell.execute_reply":"2025-01-10T12:17:11.120165Z"},"id":"iQA_BknmPQ6u","outputId":"5af0f9b6-7fd2-457c-f3c9-b3242436f63d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\nmodel_log_loss = log_loss(train_y_val, value_y_probabilities)\n\nprint('Model Log loss:', model_log_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:17:17.509753Z","iopub.execute_input":"2025-01-10T12:17:17.510114Z","iopub.status.idle":"2025-01-10T12:17:17.522577Z","shell.execute_reply.started":"2025-01-10T12:17:17.510066Z","shell.execute_reply":"2025-01-10T12:17:17.521498Z"},"id":"S8xEgjRCPQ6u","outputId":"6724ff2a-d3d8-4031-a0f2-1144135f5e94"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features = 150) #without max_features it crashes due to memory limit\nvectorizer_prompt = vectorizer.fit_transform(test[\"prompt\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_test_response_a = vectorizer.fit_transform(test[\"response_a\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\nvectorizer_test_response_b = vectorizer.fit_transform(test[\"response_b\"])\nprint(vectorizer.idf_)\nprint(vectorizer.get_feature_names_out())\n\n\n\ntemp_test_prompt = vectorizer.transform(test[\"prompt\"])\ntemp_test_response_a = vectorizer.fit_transform(test[\"response_a\"])\ntemp_test_response_b = vectorizer.fit_transform(test[\"response_b\"])\n\nprint(\"vectorizer representation for 'prompt':\\n\", temp_test_prompt.toarray())\nprint(\"vectorizer representation for 'response a':\\n\", temp_test_response_a.toarray())\nprint(\"vectorizer representation for 'response b':\\n\", temp_test_response_b.toarray())\n\nprint(\"Number of elements for the vectorizer representation for 'prompt':\\n\", temp_test_prompt.shape)\nprint(\"Number of elements for the vectorizer representation for 'response a':\\n\", temp_test_response_a.shape)\nprint(\"Number of elements for the vectorizer representation for 'response b':\\n\", temp_test_response_b.shape)\ntest_X = np.concatenate((temp_test_prompt.toarray(), temp_test_response_a.toarray(), temp_test_response_b.toarray()), axis=1)\nvalue_test_y_probabilities = model.predict_proba(test_X)\nprint('Model winner prediction, probability', value_test_y_probabilities) #winner model a | winner\noutput = pd.DataFrame({'id': test.id,\n                        'winner_model_a': value_test_y_probabilities[:, 0],\n                        'winner_model_b': value_test_y_probabilities[:, 1],\n                        'winner_tie': value_test_y_probabilities[:, 2]})\noutput.to_csv('submission.csv', index=False)","metadata":{"id":"OClGtmNfyt60","outputId":"ac8ae7c2-20aa-4b13-eb77-8ab8d4933a05","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T12:17:19.935896Z","iopub.execute_input":"2025-01-10T12:17:19.936325Z","iopub.status.idle":"2025-01-10T12:17:20.002569Z","shell.execute_reply.started":"2025-01-10T12:17:19.936287Z","shell.execute_reply":"2025-01-10T12:17:20.001233Z"}},"outputs":[],"execution_count":null}]}