{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":8926343,"sourceType":"datasetVersion","datasetId":5369301},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10150018,"sourceType":"datasetVersion","datasetId":6265978},{"sourceId":10214229,"sourceType":"datasetVersion","datasetId":6267026},{"sourceId":10236316,"sourceType":"datasetVersion","datasetId":6266300},{"sourceId":10302322,"sourceType":"datasetVersion","datasetId":6265823},{"sourceId":12179221,"sourceType":"datasetVersion","datasetId":7670586},{"sourceId":75103,"sourceType":"modelInstanceVersion","modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-06-18T06:26:53.486737Z","iopub.execute_input":"2025-06-18T06:26:53.487079Z","iopub.status.idle":"2025-06-18T06:27:41.066263Z","shell.execute_reply.started":"2025-06-18T06:26:53.48703Z","shell.execute_reply":"2025-06-18T06:27:41.065234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-06-18T06:27:41.068723Z","iopub.execute_input":"2025-06-18T06:27:41.069016Z","iopub.status.idle":"2025-06-18T06:28:31.098247Z","shell.execute_reply.started":"2025-06-18T06:27:41.068987Z","shell.execute_reply":"2025-06-18T06:28:31.09712Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install transformers peft accelerate bitsandbytes \\-U --no-index --find-links /kaggle/input/lmsys-wheel-files\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:28:31.099589Z","iopub.execute_input":"2025-06-18T06:28:31.099861Z","iopub.status.idle":"2025-06-18T06:28:31.104269Z","shell.execute_reply.started":"2025-06-18T06:28:31.099834Z","shell.execute_reply":"2025-06-18T06:28:31.103319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"execution":{"iopub.status.busy":"2025-06-18T06:28:31.105362Z","iopub.execute_input":"2025-06-18T06:28:31.105693Z","iopub.status.idle":"2025-06-18T06:28:32.289083Z","shell.execute_reply.started":"2025-06-18T06:28:31.105661Z","shell.execute_reply":"2025-06-18T06:28:32.288079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile prepare_test_file.py\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\ndf.to_parquet(\"test.parquet\", index=False)\n\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\ndf.to_parquet(\"test_swap.parquet\", index=False)\n\n###/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\n'''\nimport pandas as pd\n\n# 讀取主要的訓練資料集和 reward 分數資訊\nfull_data = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")  # 包含 id, prompt, response_a, response_b 等欄位\nrewards_data = pd.read_csv(\"/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\")  # 包含 id, fold\n\n# 合併 reward 分數到主要資料集中\nmerged_data = full_data.merge(rewards_data, on='id', how='inner')\nmerged_data.rename(columns={'fold': 'reward'}, inplace=True)  # 將 'fold' 欄位重命名為 'reward'\n\n# 檢查合併後的資料\nprint(merged_data.head())\n\n# 儲存合併後的資料\nmerged_data.to_parquet(\"train_with_rewards.parquet\", index=False)\n\n'''\n\n\n'''\n# %%writefile prepare_test_file.py\nimport pandas as pd\n\n# 讀取原始測試資料和 reward 資料\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\nfolds_df = pd.read_csv(\"/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\")\n\n# 合併 reward 資料到測試資料中\nmerged_df = test_df.merge(folds_df, on=\"id\", how=\"inner\")\n\n# 假設 fold 欄位即為 reward 分數，設置 winner labels\nmerged_df[\"winner_model_a\"] = (merged_df[\"fold\"] == 0).astype(int)\nmerged_df[\"winner_model_b\"] = (merged_df[\"fold\"] == 1).astype(int)\nmerged_df[\"winner_tie\"] = (merged_df[\"fold\"] == 2).astype(int)\n\n# 保存資料\nmerged_df.to_parquet(\"test.parquet\", index=False)\n\n# 創建 response_a 和 response_b 調換版本的資料集\nmerged_df[\"response_a\"], merged_df[\"response_b\"] = merged_df[\"response_b\"], merged_df[\"response_a\"]\nmerged_df.to_parquet(\"test_swap.parquet\", index=False)\n'''\n","metadata":{"execution":{"iopub.status.busy":"2025-06-18T06:28:32.290555Z","iopub.execute_input":"2025-06-18T06:28:32.290836Z","iopub.status.idle":"2025-06-18T06:28:32.298462Z","shell.execute_reply.started":"2025-06-18T06:28:32.290803Z","shell.execute_reply":"2025-06-18T06:28:32.297601Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python prepare_test_file.py","metadata":{"execution":{"iopub.status.busy":"2025-06-18T06:28:32.299496Z","iopub.execute_input":"2025-06-18T06:28:32.299778Z","iopub.status.idle":"2025-06-18T06:28:34.146625Z","shell.execute_reply.started":"2025-06-18T06:28:32.299759Z","shell.execute_reply":"2025-06-18T06:28:34.145377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m0.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"\ncsv_path = \"test.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=False,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 42\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.head_dim\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m0.npy', prob)","metadata":{"execution":{"iopub.status.busy":"2025-06-18T06:28:34.149984Z","iopub.execute_input":"2025-06-18T06:28:34.15027Z","iopub.status.idle":"2025-06-18T06:28:34.157661Z","shell.execute_reply.started":"2025-06-18T06:28:34.150244Z","shell.execute_reply":"2025-06-18T06:28:34.156706Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python predict_m0.py","metadata":{"execution":{"iopub.status.busy":"2025-06-18T06:28:34.158636Z","iopub.execute_input":"2025-06-18T06:28:34.158841Z","iopub.status.idle":"2025-06-18T06:32:54.377021Z","shell.execute_reply.started":"2025-06-18T06:28:34.158824Z","shell.execute_reply":"2025-06-18T06:32:54.375978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"# %%writefile predict_m3.py\n# import torch\n# import numpy as np\n# from torch.utils.data import DataLoader\n# from tqdm import tqdm\n# from transformers import AutoTokenizer\n\n# from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n# from human_pref.models.modeling_llama import LlamaForSequenceClassification\n# from human_pref.data.processors import ProcessorPAB\n# from human_pref.data.dataset import LMSYSDataset\n# from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n# from human_pref.utils import to_device\n\n\n# model_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\n# csv_path = \"test.parquet\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n# tokenizer.deprecation_warnings[\n#     \"sequence-length-is-longer-than-the-specified-maximum\"\n# ] = True\n# processor = ProcessorPAB(\n#     tokenizer=tokenizer,\n#     max_length=4096,\n#     support_system_role=True,\n# )\n# dataset = LMSYSDataset(\n#     csv_file=csv_path,\n#     query=None,\n#     processor=processor,\n#     include_swap=False,\n#     is_parquet=True,\n# )\n# dataloader = DataLoader(\n#     dataset,\n#     batch_size=80,\n#     num_workers=4,\n#     collate_fn=ShardedMaxTokensCollator(\n#         max_tokens=8192, base_collator=VarlenCollator()\n#     ),\n# )\n\n# # model for pipelined inference\n# num_hidden_layers = 32\n# device_map = {\n#     \"model.embed_tokens\": \"cuda:0\",\n#     \"model.norm\": \"cuda:1\",\n#     \"score\": \"cuda:1\",\n# }\n# for i in range(num_hidden_layers // 2):\n#     device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n# for i in range(num_hidden_layers // 2, num_hidden_layers):\n#     device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\n# model = LlamaForSequenceClassification.from_pretrained(\n#     model_name_or_path,\n#     torch_dtype=torch.float16,\n#     device_map=device_map,\n# )\n\n# # inv_freq clones for each device\n# config = model.config\n# dim = config.hidden_size // config.num_attention_heads\n# inv_freq = 1.0 / (\n#     config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n# )\n# inv_freq0 = inv_freq.to(\"cuda:0\")\n# inv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# # for name, p in model.named_parameters():\n# #     print(name, p.device)\n# # for name, b in model.model.named_buffers():\n# #     print(name, b.device)\n\n# # pipeline parallelism with two GPUs\n# is_first = True\n# hidden_states = None\n# outs = []\n# for batch in tqdm(dataloader):\n#     for micro_batch in batch:\n#         input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n#         seq_info = dict(\n#             cu_seqlens=micro_batch[\"cu_seqlens\"],\n#             position_ids=micro_batch[\"position_ids\"],\n#             max_seq_len=micro_batch[\"max_seq_len\"],\n#             attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n#         )\n#         seq_info = to_device(seq_info, \"cuda:0\")\n#         if is_first:\n#             with torch.no_grad(), torch.cuda.amp.autocast():\n#                 prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n#             is_first = False\n#             prev_seq_info, prev_hidden_states = to_device(\n#                 [seq_info, prev_hidden_states], \"cuda:1\"\n#             )\n#             continue\n#         with torch.no_grad(), torch.cuda.amp.autocast():\n#             logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n#             hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n#             prev_seq_info, prev_hidden_states = to_device(\n#                 [seq_info, hidden_states], \"cuda:1\"\n#             )\n#             outs.append(logits.cpu())\n\n# # last micro-batch\n# with torch.no_grad(), torch.cuda.amp.autocast():\n#     logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n#     outs.append(logits.cpu())\n\n\n# pred = torch.cat(outs, dim=0)\n# prob = pred.softmax(-1)\n# print(dataset.evaluate(prob.numpy()))\n\n# np.save('prob_m3.npy', prob)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.378819Z","iopub.execute_input":"2025-06-18T06:32:54.379508Z","iopub.status.idle":"2025-06-18T06:32:54.386686Z","shell.execute_reply.started":"2025-06-18T06:32:54.379464Z","shell.execute_reply":"2025-06-18T06:32:54.385843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python predict_m3.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.387708Z","iopub.execute_input":"2025-06-18T06:32:54.387949Z","iopub.status.idle":"2025-06-18T06:32:54.405028Z","shell.execute_reply.started":"2025-06-18T06:32:54.387916Z","shell.execute_reply":"2025-06-18T06:32:54.404111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n%%time\n\ntokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\ntokenizer.add_eos_token = True\ntokenizer.padding_side = \"right\"\n\ndata = pd.DataFrame()\ndata[\"id\"] = test[\"id\"]\ndata[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\ndata[\"length\"] = data[\"input_ids\"].apply(len)\n\naug_data = pd.DataFrame()\naug_data[\"id\"] = test[\"id\"]\n# swap response_a & response_b\naug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\naug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.406154Z","iopub.execute_input":"2025-06-18T06:32:54.406421Z","iopub.status.idle":"2025-06-18T06:32:54.420573Z","shell.execute_reply.started":"2025-06-18T06:32:54.4064Z","shell.execute_reply":"2025-06-18T06:32:54.419738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n#%%writefile predict_lora.py\n\n\nimport time\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nfrom transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\nfrom peft import PeftModel\n\nassert torch.cuda.device_count() == 2\n@dataclass\nclass Config:\n    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n    max_length = 2048\n    batch_size = 4\n    device = torch.device(\"cuda\")    \n    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n\ncfg = Config()\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\ndef process_text(text: str) -> str:\n    return \" \".join(eval(text, {\"null\": \"\"}))\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process_text)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process_text)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n\ndisplay(test.head(5))\ndef tokenize(\n    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n):\n    prompt = [\"<prompt>: \" + p for p in prompt]\n    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n    if spread_max_length:\n        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        attention_mask = [[1]* len(i) for i in input_ids]\n    else:\n        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n        input_ids = tokenized.input_ids\n        attention_mask = tokenized.attention_mask\n    return input_ids, attention_mask\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.421638Z","iopub.execute_input":"2025-06-18T06:32:54.421859Z","iopub.status.idle":"2025-06-18T06:32:54.439326Z","shell.execute_reply.started":"2025-06-18T06:32:54.421841Z","shell.execute_reply":"2025-06-18T06:32:54.438622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Load base model on GPU 0\ndevice_0 = torch.device('cuda:0')\nmodel_0 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_0,\n    use_cache=False,\n)\n\n# Load base model on GPU 1\ndevice_1 = torch.device('cuda:1')\nmodel_1 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_1,\n    use_cache=False,\n)\n\nmodel_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\nmodel_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.440271Z","iopub.execute_input":"2025-06-18T06:32:54.440856Z","iopub.status.idle":"2025-06-18T06:32:54.456356Z","shell.execute_reply.started":"2025-06-18T06:32:54.440808Z","shell.execute_reply":"2025-06-18T06:32:54.455577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n#%%writefile predict_qlora.py\n@torch.no_grad()\n@torch.cuda.amp.autocast()\ndef inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n    a_win, b_win, tie = [], [], []\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        tmp = df.iloc[start_idx:end_idx]\n        input_ids = tmp[\"input_ids\"].to_list()\n        attention_mask = tmp[\"attention_mask\"].to_list()\n        inputs = pad_without_fast_tokenizer_warning(\n            tokenizer,\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n            padding=\"longest\",\n            pad_to_multiple_of=None,\n            return_tensors=\"pt\",\n        )\n        outputs = model(**inputs.to(device))\n        proba = outputs.logits.softmax(-1).cpu()\n        \n        a_win.extend(proba[:, 0].tolist())\n        b_win.extend(proba[:, 1].tolist())\n        tie.extend(proba[:, 2].tolist())\n    \n    df[\"winner_model_a\"] = a_win\n    df[\"winner_model_b\"] = b_win\n    df[\"winner_tie\"] = tie\n    \n    return df\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.457344Z","iopub.execute_input":"2025-06-18T06:32:54.457657Z","iopub.status.idle":"2025-06-18T06:32:54.467873Z","shell.execute_reply.started":"2025-06-18T06:32:54.45762Z","shell.execute_reply":"2025-06-18T06:32:54.467095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!python predict_qlora.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.468874Z","iopub.execute_input":"2025-06-18T06:32:54.469068Z","iopub.status.idle":"2025-06-18T06:32:54.482633Z","shell.execute_reply.started":"2025-06-18T06:32:54.469052Z","shell.execute_reply":"2025-06-18T06:32:54.481785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nst = time.time()\n\n# sort by input length to fully leverage dynaminc padding\ndata = data.sort_values(\"length\", ascending=False)\n# the total #tokens in sub_1 and sub_2 should be more or less the same\nsub_1 = data.iloc[0::2].copy()\nsub_2 = data.iloc[1::2].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\nresult_df = pd.concat(list(results), axis=0)\ndisplay(result_df)\n\nproba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n\nprint(f\"elapsed time: {time.time() - st}\")\n\nst = time.time()\n\nif cfg.tta:\n    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n    sub_1 = data.iloc[0::2].copy()\n    sub_2 = data.iloc[1::2].copy()\n\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\n    tta_result_df = pd.concat(list(results), axis=0)\n    # recall TTA's order is flipped\n    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n    # average original result and TTA result.\n    proba = (proba + tta_proba) / 2\n\nprint(f\"elapsed time: {time.time() - st}\")\n\nresult_df.loc[:, \"winner_model_a\"] = proba[:, 0]\nresult_df.loc[:, \"winner_model_b\"] = proba[:, 1]\nresult_df.loc[:, \"winner_tie\"] = proba[:, 2]\nsubmission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\nsubmission_df.to_csv('submission.csv', index=False)\ndisplay(submission_df)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.483557Z","iopub.execute_input":"2025-06-18T06:32:54.483889Z","iopub.status.idle":"2025-06-18T06:32:54.49556Z","shell.execute_reply.started":"2025-06-18T06:32:54.483822Z","shell.execute_reply":"2025-06-18T06:32:54.494869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nsubmission_df = pd.DataFrame(submission_df)\n\nprob_qlora = submission_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].to_numpy()\n\nprint(prob_qlora)\nnp.save('prob_qlora.npy', proba)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.49643Z","iopub.execute_input":"2025-06-18T06:32:54.496704Z","iopub.status.idle":"2025-06-18T06:32:54.508649Z","shell.execute_reply.started":"2025-06-18T06:32:54.496685Z","shell.execute_reply":"2025-06-18T06:32:54.507901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!python predict_lorasciia.py\n#display(result_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.509668Z","iopub.execute_input":"2025-06-18T06:32:54.509902Z","iopub.status.idle":"2025-06-18T06:32:54.518222Z","shell.execute_reply.started":"2025-06-18T06:32:54.509884Z","shell.execute_reply":"2025-06-18T06:32:54.517638Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sentence Transformer","metadata":{}},{"cell_type":"code","source":"# !pip install /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install --no-index --find-links=/kaggle/input/some-pack/sentence_transformers_packages sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.519097Z","iopub.execute_input":"2025-06-18T06:32:54.519336Z","iopub.status.idle":"2025-06-18T06:32:54.529041Z","shell.execute_reply.started":"2025-06-18T06:32:54.519317Z","shell.execute_reply":"2025-06-18T06:32:54.528383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sentence_transformers import SentenceTransformer\n# import numpy as np\n# import pandas as pd\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import DataLoader\n# from transformers import AutoModel\n# from transformers import AutoTokenizer\n# from transformers import AutoTokenizer\n# from sklearn.preprocessing import MinMaxScaler\n# import faiss\n# model_load_path = '/kaggle/input/some-pack/sentence-transformer-model' \n# sentence_model = SentenceTransformer(model_load_path)\n# test_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n# class CustomDebertaModel(nn.Module):\n#     def __init__(self, model_name, num_labels, feature_dim=2, dropout_rate=0.1):\n#         super(CustomDebertaModel, self).__init__()\n#         # 初始化DeBERTa模型\n#         self.base_model = AutoModel.from_pretrained(model_name)\n#         # 相似度特徵塔 (MLP)\n#         self.feature_fc = nn.Sequential(\n#             nn.Linear(feature_dim, 128),  # 映射到128維\n#             nn.ReLU(),\n#             nn.Dropout(p=dropout_rate),\n#             nn.Linear(128, self.base_model.config.hidden_size),  # 映射到文本嵌入維度\n#             nn.ReLU()\n#         )\n#         # 注意力機制\n#         self.attention = nn.MultiheadAttention(\n#             embed_dim=self.base_model.config.hidden_size,\n#             num_heads=4,  # 設定注意力頭的數量\n#             batch_first=True\n#         )\n#         # Dropout 層\n#         self.dropout = nn.Dropout(p=dropout_rate)\n\n#         # 最終分類層\n#         self.classifier = nn.Sequential(\n#             nn.Linear(self.base_model.config.hidden_size * 2, self.base_model.config.hidden_size),\n#             nn.ReLU(),\n#             nn.Dropout(p=dropout_rate),\n#             nn.Linear(self.base_model.config.hidden_size, num_labels)\n#         )\n\n#     def forward(self, input_ids, attention_mask, similarity_features, labels=None):\n#         # 文本塔：提取文本嵌入\n#         base_outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n#         text_embeddings = base_outputs.last_hidden_state[:, 0, :]  # 提取 [CLS] token 嵌入\n\n#         # 特徵塔：處理相似度特徵\n#         similarity_embeds = self.feature_fc(similarity_features)\n\n#         # 使用注意力機制進行交互\n#         query = text_embeddings.unsqueeze(1)  # [batch_size, 1, hidden_size]\n#         key_value = similarity_embeds.unsqueeze(1)  # [batch_size, 1, hidden_size]\n#         attention_output, _ = self.attention(query, key_value, key_value)\n\n#         # 拼接文本嵌入與注意力輸出\n#         combined_features = torch.cat([text_embeddings, attention_output.squeeze(1)], dim=1)\n\n#         # Dropout 和分類\n#         logits = self.classifier(self.dropout(combined_features))\n\n#         # 輸出結果\n#         outputs = {\"logits\": logits}\n#         if labels is not None:\n#             # 如果有標籤，計算損失\n#             loss_fn = nn.CrossEntropyLoss()\n#             outputs[\"loss\"] = loss_fn(logits, labels)\n#         return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.530324Z","iopub.execute_input":"2025-06-18T06:32:54.530596Z","iopub.status.idle":"2025-06-18T06:32:54.543768Z","shell.execute_reply.started":"2025-06-18T06:32:54.530577Z","shell.execute_reply":"2025-06-18T06:32:54.542941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.preprocessing import MinMaxScaler\n# import faiss\n# test_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n# # 使用 FAISS 計算語義相似性分數\n# def compute_semantic_features_with_faiss(df):\n#     prompts = df['prompt'].tolist()\n#     responses_a = df['response_a'].tolist()\n#     responses_b = df['response_b'].tolist()\n\n#     # 提取嵌入向量並正規化\n#     prompt_embeddings = np.array(sentence_model.encode(prompts))\n#     prompt_embeddings = prompt_embeddings / np.linalg.norm(prompt_embeddings, axis=1, keepdims=True)\n\n#     response_a_embeddings = np.array(sentence_model.encode(responses_a))\n#     response_a_embeddings = response_a_embeddings / np.linalg.norm(response_a_embeddings, axis=1, keepdims=True)\n\n#     response_b_embeddings = np.array(sentence_model.encode(responses_b))\n#     response_b_embeddings = response_b_embeddings / np.linalg.norm(response_b_embeddings, axis=1, keepdims=True)\n\n#     dim = prompt_embeddings.shape[1]\n#     index_flat = faiss.IndexFlatIP(dim)  # 使用內積計算相似度\n\n#     # 計算相似度\n#     index_flat.add(prompt_embeddings)  # 添加 prompt 嵌入向量到索引\n#     similarity_a = index_flat.search(response_a_embeddings, k=1)[0].squeeze()\n\n#     index_flat.reset()\n#     index_flat.add(prompt_embeddings)\n#     similarity_b = index_flat.search(response_b_embeddings, k=1)[0].squeeze()\n\n#     df['similarity_a'] = similarity_a\n#     df['similarity_b'] = similarity_b\n\n#     return df\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.544792Z","iopub.execute_input":"2025-06-18T06:32:54.545029Z","iopub.status.idle":"2025-06-18T06:32:54.556715Z","shell.execute_reply.started":"2025-06-18T06:32:54.545011Z","shell.execute_reply":"2025-06-18T06:32:54.555928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_data = compute_semantic_features_with_faiss(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.560765Z","iopub.execute_input":"2025-06-18T06:32:54.561258Z","iopub.status.idle":"2025-06-18T06:32:54.570211Z","shell.execute_reply.started":"2025-06-18T06:32:54.561222Z","shell.execute_reply":"2025-06-18T06:32:54.569562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = torch.load(\"/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\")\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n# print(\"Custom model loaded successfully!\")\n\n\n# tokenizer_path = \"/kaggle/input/akemiiiiii/custom_model_dir\" \n# tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n# def preprocess_test_data(row):\n#     input_text = f\"Prompt: {row['prompt']} Response A: {row['response_a']} Response B: {row['response_b']}\"\n#     tokenized_inputs = tokenizer(\n#         input_text,\n#         truncation=True,\n#         padding=\"max_length\",\n#         max_length=512,\n#         return_tensors=\"pt\",\n#     )\n#     tokenized_inputs[\"similarity_a\"] = torch.tensor([row[\"similarity_a\"]], dtype=torch.float32)\n#     tokenized_inputs[\"similarity_b\"] = torch.tensor([row[\"similarity_b\"]], dtype=torch.float32)\n#     return tokenized_inputs\n\n# processed_test_data = [preprocess_test_data(row) for _, row in test_data.iterrows()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.571379Z","iopub.execute_input":"2025-06-18T06:32:54.572132Z","iopub.status.idle":"2025-06-18T06:32:54.582336Z","shell.execute_reply.started":"2025-06-18T06:32:54.5721Z","shell.execute_reply":"2025-06-18T06:32:54.581499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 創建自定義測試數據集\n# class TestDataset(torch.utils.data.Dataset):\n#     def __init__(self, data):\n#         self.data = data\n\n#     def __len__(self):\n#         return len(self.data)\n\n#     def __getitem__(self, idx):\n#         return self.data[idx]\n\n# # 初始化數據集\n# test_dataset = TestDataset(processed_test_data)\n\n# # 自定義 DataLoader 的 collate_fn 函數\n# def collate_fn_test(batch):\n#     input_ids = torch.cat([item[\"input_ids\"] for item in batch])\n#     attention_mask = torch.cat([item[\"attention_mask\"] for item in batch])\n#     similarity_features = torch.cat(\n#         [torch.cat([item[\"similarity_a\"], item[\"similarity_b\"]], dim=0).unsqueeze(0) for item in batch]\n#     )\n#     return {\n#         \"input_ids\": input_ids,\n#         \"attention_mask\": attention_mask,\n#         \"similarity_features\": similarity_features,\n#     }\n\n# # 創建 DataLoader\n# test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn_test, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.583532Z","iopub.execute_input":"2025-06-18T06:32:54.583809Z","iopub.status.idle":"2025-06-18T06:32:54.596837Z","shell.execute_reply.started":"2025-06-18T06:32:54.583775Z","shell.execute_reply":"2025-06-18T06:32:54.595812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 模型推理\n# model.eval()\n# predictions = []\n\n# with torch.no_grad():\n#     for batch in test_dataloader:\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         similarity_features = batch[\"similarity_features\"].to(device)\n\n#         # 推理\n#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, similarity_features=similarity_features)\n#         logits = outputs[\"logits\"]\n#         probs = torch.nn.functional.softmax(logits, dim=-1)\n#         predictions.append(probs.cpu().numpy())\n\n# # 合併所有預測結果\n# predictions = np.concatenate(predictions, axis=0)\n\n# # 檢視預測結果\n# print(predictions)\n\n# np.save('prob_faiss.npy', predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.59815Z","iopub.execute_input":"2025-06-18T06:32:54.598483Z","iopub.status.idle":"2025-06-18T06:32:54.609566Z","shell.execute_reply.started":"2025-06-18T06:32:54.598429Z","shell.execute_reply":"2025-06-18T06:32:54.608825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"#%%writefile make_submission.py\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_parquet(\"test.parquet\")\n\n\nprob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\n# prob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3 (swap response_a and response_b)\n# prob_faiss = np.load(\"prob_faiss.npy\")  # faiss\n\n# Combine predictions with weights\n# Adjust weights as needed for optimal performance\npreds = np.average(\n    [\n        prob_m0,       # Gemma2 results\n        # prob_m3,       # Llama3 results\n        # prob_faiss     # faiss results\n    ],\n    axis=0,\n    weights=[1.0]\n    # weights=[0.65, 0.35]\n    # weights=[0.5, 0.3, 0.2]\n    # \n    # weights=[0.6, 0.4]\n    # weights=[0.45, 0.275, 0.275]  # Weights for each model    \n    #weights=[0.5, 0.25, 0.25]  # Weights for each model\n)\n\n# Create submission DataFrame\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\n\n# Save to CSV\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:54.610818Z","iopub.execute_input":"2025-06-18T06:32:54.611696Z","iopub.status.idle":"2025-06-18T06:32:55.037143Z","shell.execute_reply.started":"2025-06-18T06:32:54.611673Z","shell.execute_reply":"2025-06-18T06:32:55.03647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!python make_submission.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:32:55.038359Z","iopub.execute_input":"2025-06-18T06:32:55.038662Z","iopub.status.idle":"2025-06-18T06:32:55.042333Z","shell.execute_reply.started":"2025-06-18T06:32:55.038639Z","shell.execute_reply":"2025-06-18T06:32:55.041503Z"}},"outputs":[],"execution_count":null}]}