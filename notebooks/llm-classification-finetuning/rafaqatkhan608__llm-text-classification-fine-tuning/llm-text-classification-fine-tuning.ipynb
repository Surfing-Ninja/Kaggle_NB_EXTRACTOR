{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":480870,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":385672,"modelId":404870}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\nimport shutil\nimport polars as pl\nfrom datasets import Dataset\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch  # base\nimport torch.nn.functional as F\nimport json\nfrom transformers.trainer import Trainer\nfrom transformers.training_args import TrainingArguments\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n)\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom datetime import datetime\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:31.49625Z","iopub.execute_input":"2025-07-21T05:31:31.496519Z","iopub.status.idle":"2025-07-21T05:31:35.063401Z","shell.execute_reply.started":"2025-07-21T05:31:31.496498Z","shell.execute_reply":"2025-07-21T05:31:35.062641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting Cuda","metadata":{}},{"cell_type":"code","source":"if torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Using MPS (Apple Silicon GPU)\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA (NVIDIA GPU)\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\nprint(f\"Device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:38.608854Z","iopub.execute_input":"2025-07-21T05:31:38.610096Z","iopub.status.idle":"2025-07-21T05:31:38.61588Z","shell.execute_reply.started":"2025-07-21T05:31:38.610064Z","shell.execute_reply":"2025-07-21T05:31:38.615171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ¤– ChatBot Arena Dataset Overview\n\nThis dataset comes from the **ChatBot Arena**, where judges evaluate responses from two competing large language models (LLMs) given the same prompt.\n\n## ðŸ“Š Dataset Statistics\n- **ðŸ“¦ Train Size:** 55,000 interactions\n- **ðŸ§ª Test Size:** ~25,000 interactions  \n- **ðŸ“ Files Included:** `train.csv`, `test.csv`\n\n## ðŸ“‘ `train.csv` Columns\n\n- `id`: Unique row identifier\n- `model_a` / `model_b`: Model names (only in train)\n- `prompt`: The input prompt shown to both models\n- `response_a` / `response_b`: Respective model responses\n- `winner_model_a` / `winner_model_b` / `winner_tie`: One-hot encoded ground truth indicating which model won or if it was a tie\n\n## ðŸ“‘ `test.csv` Columns\n\n- `id`: Unique row identifier\n- `prompt`: The shared prompt\n- `response_a` / `response_b`: Responses from the two anonymous models\n\n## ðŸ§  Objective\n\nThe objective is to train a model to predict the more preferred response (*model_a*, *model_b*, or *tie*) based on the prompt and both responses.","metadata":{}},{"cell_type":"code","source":"# Assuming the data files are in '../input/llm-classification-finetuning/'\n# You'll need to check the actual file names and paths once you join the competition.\n\ntry:\n    train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n    test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n    # You might also have a sample submission file to guide the format\n    sample_submission_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\n    print(\"Data loaded successfully.\")\n    print(f\"Train data shape: {train_df.shape}\")\n    print(f\"Test data shape: {test_df.shape}\")\n    print(f\"Sample Submission data shape: {sample_submission_df.shape}\")\n\nexcept FileNotFoundError:\n    print(\"Error: Data files not found. Please ensure the dataset is added to your Kaggle Notebook.\")\n    print(\"Go to 'Add Data' on the right sidebar of your notebook and search for 'llm-classification-finetuning'.\")\n    exit() # Exit if data isn't found to prevent further errors\n\n# Display the first few rows of the dataframes to understand their structure\nprint(\"\\nTrain DataFrame Head:\")\nprint(train_df.head())\nprint(\"\\nTest DataFrame Head:\")\nprint(test_df.head())\nprint(\"\\nSample Submission DataFrame Head:\")\nprint(sample_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:39.573545Z","iopub.execute_input":"2025-07-21T05:31:39.573862Z","iopub.status.idle":"2025-07-21T05:31:43.129345Z","shell.execute_reply.started":"2025-07-21T05:31:39.573835Z","shell.execute_reply":"2025-07-21T05:31:43.128409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"print(\"\\n=== MISSING VALUES ===\")\nprint(train_df.isnull().sum())\n\nprint(\"\\n=== UNIQUE VALUES IN KEY COLUMNS ===\")\nprint(f\"Unique model_a: {train_df['model_a'].nunique()}\")\nprint(f\"Unique model_b: {train_df['model_b'].nunique()}\")\nprint(f\"Sample models: {train_df['model_a'].unique()[:5]}\")\n\nprint(\"\\n=== TARGET DISTRIBUTION ===\")\nprint(f\"winner_model_a = 1: {train_df['winner_model_a'].sum()}\")\nprint(f\"winner_model_b = 1: {train_df['winner_model_b'].sum()}\")\nprint(f\"winner_tie = 1: {train_df['winner_tie'].sum()}\")\n\nprint(\"\\n=== DATA QUALITY CHECKS ===\")\nprint(f\"Total rows: {len(train_df)}\")\nprint(f\"Rows where exactly one winner is 1:\")\nvalid_rows = ((train_df['winner_model_a'] + train_df['winner_model_b'] + train_df['winner_tie']) == 1).sum()\nprint(f\"Valid rows: {valid_rows}\")\nprint(f\"Invalid rows: {len(train_df) - valid_rows}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:43.130613Z","iopub.execute_input":"2025-07-21T05:31:43.130937Z","iopub.status.idle":"2025-07-21T05:31:43.184085Z","shell.execute_reply.started":"2025-07-21T05:31:43.130909Z","shell.execute_reply":"2025-07-21T05:31:43.183204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== CONVERTING STRING LISTS TO ACTUAL LISTS ===\")\n# Check if prompt/responses are string representations of lists\nprint(\"Sample prompt type:\", type(train_df['prompt'].iloc[0]))\nprint(\"Sample prompt value:\", train_df['prompt'].iloc[0])\n\ndef safe_eval(x):\n    \"\"\"Convert string representation of list to actual list\"\"\"\n    if isinstance(x, str):\n        try:\n            # Try to evaluate as Python literal (safe)\n            result = ast.literal_eval(x)\n            return result if isinstance(result, list) else [result]\n        except:\n            # If it fails, treat as regular string\n            return [x]\n    return x\n\n# Apply to prompt and response columns\ntrain_df['prompt_processed'] = train_df['prompt'].apply(safe_eval)\ntrain_df['response_a_processed'] = train_df['response_a'].apply(safe_eval)\ntrain_df['response_b_processed'] = train_df['response_b'].apply(safe_eval)\n\nprint(\"\\n=== CHECKING PROCESSED DATA ===\")\nprint(\"Sample processed prompt:\", train_df['prompt_processed'].iloc[0])\nprint(\"Type:\", type(train_df['prompt_processed'].iloc[0]))\n\n# Check if we have multiple prompts/responses per row\nprint(\"\\n=== CHECKING FOR MULTIPLE PROMPTS/RESPONSES ===\")\nprompt_lengths = train_df['prompt_processed'].apply(len)\nresponse_a_lengths = train_df['response_a_processed'].apply(len)\nresponse_b_lengths = train_df['response_b_processed'].apply(len)\n\nprint(f\"Prompt lengths: min={prompt_lengths.min()}, max={prompt_lengths.max()}, mean={prompt_lengths.mean():.2f}\")\nprint(f\"Response A lengths: min={response_a_lengths.min()}, max={response_a_lengths.max()}, mean={response_a_lengths.mean():.2f}\")\nprint(f\"Response B lengths: min={response_b_lengths.min()}, max={response_b_lengths.max()}, mean={response_b_lengths.mean():.2f}\")\n\n# Extract first prompt/response from each list\ndef extract_first(x):\n    \"\"\"Extract first element if it's a list, otherwise return as string\"\"\"\n    if isinstance(x, list):\n        return x[0] if len(x) > 0 else \"\"\n    return str(x)\n\ntrain_df['prompt_text'] = train_df['prompt_processed'].apply(extract_first)\ntrain_df['response_a_text'] = train_df['response_a_processed'].apply(extract_first)\ntrain_df['response_b_text'] = train_df['response_b_processed'].apply(extract_first)\n\nprint(\"\\n=== FINAL TEXT DATA ===\")\nprint(f\"Sample prompt text: {train_df['prompt_text'].iloc[0][:100]}...\")\nprint(f\"Sample response A text: {train_df['response_a_text'].iloc[0][:100]}...\")\nprint(f\"Sample response B text: {train_df['response_b_text'].iloc[0][:100]}...\")\n\n# Check text lengths\nprint(f\"\\nPrompt text lengths: mean={train_df['prompt_text'].str.len().mean():.0f}, max={train_df['prompt_text'].str.len().max()}\")\nprint(f\"Response A text lengths: mean={train_df['response_a_text'].str.len().mean():.0f}, max={train_df['response_a_text'].str.len().max()}\")\nprint(f\"Response B text lengths: mean={train_df['response_b_text'].str.len().mean():.0f}, max={train_df['response_b_text'].str.len().max()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:43.184889Z","iopub.execute_input":"2025-07-21T05:31:43.185175Z","iopub.status.idle":"2025-07-21T05:31:44.140684Z","shell.execute_reply.started":"2025-07-21T05:31:43.18515Z","shell.execute_reply":"2025-07-21T05:31:44.139918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CREATING TARGET LABELS","metadata":{}},{"cell_type":"code","source":"print(\"=== CREATING TARGET LABELS ===\")\n\n# Check current winner columns\nprint(\"Current winner columns:\")\nprint(f\"winner_model_a: {train_df['winner_model_a'].value_counts()}\")\nprint(f\"winner_model_b: {train_df['winner_model_b'].value_counts()}\")\nprint(f\"winner_tie: {train_df['winner_tie'].value_counts()}\")\n\n# Check if each row has exactly one winner\nprint(\"\\n=== CHECKING DATA VALIDITY ===\")\nwinner_sum = train_df['winner_model_a'] + train_df['winner_model_b'] + train_df['winner_tie']\nprint(f\"Rows with exactly one winner: {(winner_sum == 1).sum()}\")\nprint(f\"Rows with no winner: {(winner_sum == 0).sum()}\")\nprint(f\"Rows with multiple winners: {(winner_sum > 1).sum()}\")\n\n# Create single target label\ndef get_winner_label(row):\n    \"\"\"Convert winner columns to single label\"\"\"\n    if row['winner_model_a'] == 1:\n        return 0  # Model A wins\n    elif row['winner_model_b'] == 1:\n        return 1  # Model B wins\n    elif row['winner_tie'] == 1:\n        return 2  # Tie\n    else:\n        return -1  # Invalid/missing label\n\ntrain_df['label'] = train_df.apply(get_winner_label, axis=1)\n\nprint(\"\\n=== LABEL DISTRIBUTION ===\")\nprint(f\"Label 0 (Model A wins): {(train_df['label'] == 0).sum()}\")\nprint(f\"Label 1 (Model B wins): {(train_df['label'] == 1).sum()}\")\nprint(f\"Label 2 (Tie): {(train_df['label'] == 2).sum()}\")\nprint(f\"Label -1 (Invalid): {(train_df['label'] == -1).sum()}\")\n\n# Remove invalid labels\nprint(f\"\\nRows before cleaning: {len(train_df)}\")\ntrain_df_clean = train_df[train_df['label'] != -1].copy()\nprint(f\"Rows after cleaning: {len(train_df_clean)}\")\n\n# Final label distribution\nlabels = train_df_clean['label'].values\nprint(f\"\\n=== FINAL LABEL DISTRIBUTION ===\")\nprint(f\"Model A wins: {np.sum(labels == 0)} ({np.mean(labels == 0)*100:.1f}%)\")\nprint(f\"Model B wins: {np.sum(labels == 1)} ({np.mean(labels == 1)*100:.1f}%)\")\nprint(f\"Tie: {np.sum(labels == 2)} ({np.mean(labels == 2)*100:.1f}%)\")\n\n# Show some examples\nprint(f\"\\n=== SAMPLE LABELED DATA ===\")\nfor i in range(3):\n    row = train_df_clean.iloc[i]\n    label_text = ['Model A wins', 'Model B wins', 'Tie'][row['label']]\n    print(f\"\\nExample {i+1}:\")\n    print(f\"Prompt: {row['prompt_text'][:80]}...\")\n    print(f\"Response A: {row['response_a_text'][:80]}...\")\n    print(f\"Response B: {row['response_b_text'][:80]}...\")\n    print(f\"Label: {row['label']} ({label_text})\")\n\n# Update train_df to clean version\ntrain_df = train_df_clean\nprint(f\"\\nDataset ready for training with {len(train_df)} samples!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:44.142392Z","iopub.execute_input":"2025-07-21T05:31:44.142644Z","iopub.status.idle":"2025-07-21T05:31:44.586387Z","shell.execute_reply.started":"2025-07-21T05:31:44.142626Z","shell.execute_reply":"2025-07-21T05:31:44.585515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CREATING TRAINING FEATURES","metadata":{}},{"cell_type":"code","source":"# Step 4: Create Training Features\n\nprint(\"=== CREATING TRAINING FEATURES ===\")\n\n# Method 1: Simple concatenation for transformer models\ndef create_model_input(row):\n    \"\"\"Create input text for transformer models\"\"\"\n    return f\"Prompt: {row['prompt_text']} Response A: {row['response_a_text']} Response B: {row['response_b_text']}\"\n\ntrain_df['model_input'] = train_df.apply(create_model_input, axis=1)\n\nprint(\"=== FEATURE STATISTICS ===\")\ninput_lengths = train_df['model_input'].str.len()\nprint(f\"Input text lengths:\")\nprint(f\"  Mean: {input_lengths.mean():.0f} characters\")\nprint(f\"  Min: {input_lengths.min()} characters\")\nprint(f\"  Max: {input_lengths.max()} characters\")\nprint(f\"  Median: {input_lengths.median():.0f} characters\")\n\n# Check for very long inputs (might need truncation)\nlong_inputs = (input_lengths > 4000).sum()\nprint(f\"  Inputs > 4000 chars: {long_inputs} ({long_inputs/len(train_df)*100:.1f}%)\")\n\nprint(\"\\n=== SAMPLE TRAINING FEATURES ===\")\nfor i in range(3):\n    row = train_df.iloc[i]\n    label_text = ['Model A wins', 'Model B wins', 'Tie'][row['label']]\n    print(f\"\\nExample {i+1}:\")\n    print(f\"Input: {row['model_input'][:200]}...\")\n    print(f\"Label: {row['label']} ({label_text})\")\n\n# Create X (features) and y (labels) arrays\nX = train_df['model_input'].values\ny = train_df['label'].values\n\nprint(f\"\\n=== FINAL TRAINING DATA ===\")\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"X sample: {X[0][:100]}...\")\nprint(f\"y sample: {y[:10]}\")\n\n# Check for any missing values\nprint(f\"\\nMissing values in X: {sum(1 for x in X if pd.isna(x) or x == '')}\")\nprint(f\"Missing values in y: {sum(1 for label in y if pd.isna(label))}\")\n\nprint(\"\\n=== READY FOR TRAIN/VALIDATION SPLIT! ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:44.587241Z","iopub.execute_input":"2025-07-21T05:31:44.58756Z","iopub.status.idle":"2025-07-21T05:31:45.303864Z","shell.execute_reply.started":"2025-07-21T05:31:44.587533Z","shell.execute_reply":"2025-07-21T05:31:45.30307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SPLITTING DATA","metadata":{}},{"cell_type":"code","source":"print(\"=== SPLITTING DATA ===\")\nprint(f\"Total samples: {len(X)}\")\nprint(f\"Label distribution before split:\")\nprint(f\"  Model A wins: {np.sum(y == 0)} ({np.mean(y == 0)*100:.1f}%)\")\nprint(f\"  Model B wins: {np.sum(y == 1)} ({np.mean(y == 1)*100:.1f}%)\")\nprint(f\"  Tie: {np.sum(y == 2)} ({np.mean(y == 2)*100:.1f}%)\")\n\n# Split data with stratification to maintain label distribution\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, \n    test_size=0.2,           # 20% for validation\n    stratify=y,              # Keep same label distribution in both sets\n    random_state=42          # For reproducibility\n)\n\nprint(f\"\\n=== SPLIT RESULTS ===\")\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Validation samples: {len(X_val)}\")\n\nprint(f\"\\nTraining set label distribution:\")\nprint(f\"  Model A wins: {np.sum(y_train == 0)} ({np.mean(y_train == 0)*100:.1f}%)\")\nprint(f\"  Model B wins: {np.sum(y_train == 1)} ({np.mean(y_train == 1)*100:.1f}%)\")\nprint(f\"  Tie: {np.sum(y_train == 2)} ({np.mean(y_train == 2)*100:.1f}%)\")\n\nprint(f\"\\nValidation set label distribution:\")\nprint(f\"  Model A wins: {np.sum(y_val == 0)} ({np.mean(y_val == 0)*100:.1f}%)\")\nprint(f\"  Model B wins: {np.sum(y_val == 1)} ({np.mean(y_val == 1)*100:.1f}%)\")\nprint(f\"  Tie: {np.sum(y_val == 2)} ({np.mean(y_val == 2)*100:.1f}%)\")\n\nprint(f\"\\n=== DATA READY FOR MODEL TRAINING! ===\")\nprint(\"Variables created:\")\nprint(\"  X_train: Training features\")\nprint(\"  y_train: Training labels\")\nprint(\"  X_val: Validation features\")\nprint(\"  y_val: Validation labels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:48.609353Z","iopub.execute_input":"2025-07-21T05:31:48.609635Z","iopub.status.idle":"2025-07-21T05:31:48.6621Z","shell.execute_reply.started":"2025-07-21T05:31:48.609615Z","shell.execute_reply":"2025-07-21T05:31:48.661268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using TF-IDF + Logistic Regression","metadata":{}},{"cell_type":"code","source":"# print(\"=== TRAINING BASELINE MODEL ===\")\n# print(\"Using TF-IDF + Logistic Regression\")\n\n# # Step 1: Convert text to numerical features using TF-IDF\n# print(\"\\n1. Converting text to TF-IDF features...\")\n# vectorizer = TfidfVectorizer(\n#     max_features=10000,      # Use top 10k words\n#     stop_words='english',    # Remove common words\n#     ngram_range=(1, 2),      # Use 1-word and 2-word phrases\n#     max_df=0.95,             # Ignore words in >95% of docs\n#     min_df=2                 # Ignore words in <2 docs\n# )\n\n# # Fit on training data and transform both sets\n# X_train_tfidf = vectorizer.fit_transform(X_train)\n# X_val_tfidf = vectorizer.transform(X_val)\n\n# print(f\"TF-IDF feature shape: {X_train_tfidf.shape}\")\n\n# # Step 2: Train Logistic Regression\n# print(\"\\n2. Training Logistic Regression...\")\n# model = LogisticRegression(\n#     max_iter=1000,\n#     random_state=42,\n#     class_weight='balanced'  # Handle class imbalance\n# )\n\n# model.fit(X_train_tfidf, y_train)\n# print(\"Model trained!\")\n\n# # Step 3: Make predictions\n# print(\"\\n3. Making predictions...\")\n# y_train_pred = model.predict(X_train_tfidf)\n# y_val_pred = model.predict(X_val_tfidf)\n\n# # Step 4: Evaluate performance\n# print(\"\\n=== MODEL PERFORMANCE ===\")\n# train_accuracy = accuracy_score(y_train, y_train_pred)\n# val_accuracy = accuracy_score(y_val, y_val_pred)\n\n# print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n# print(f\"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n\n# # Check if overfitting\n# if train_accuracy - val_accuracy > 0.05:\n#     print(\"âš ï¸  Possible overfitting detected!\")\n# else:\n#     print(\"âœ… Good generalization!\")\n\n# # Detailed performance report\n# print(f\"\\n=== DETAILED VALIDATION RESULTS ===\")\n# print(classification_report(y_val, y_val_pred, \n#                           target_names=['Model A wins', 'Model B wins', 'Tie']))\n\n# # Confusion matrix\n# print(f\"\\n=== CONFUSION MATRIX ===\")\n# cm = confusion_matrix(y_val, y_val_pred)\n# print(\"Rows: True labels, Columns: Predicted labels\")\n# print(\"        A_wins  B_wins  Tie\")\n# for i, row_label in enumerate(['A_wins', 'B_wins', 'Tie']):\n#     print(f\"{row_label:>7} {cm[i]}\")\n\n# # Show some prediction examples\n# print(f\"\\n=== PREDICTION EXAMPLES ===\")\n# for i in range(5):\n#     true_label = ['Model A wins', 'Model B wins', 'Tie'][y_val[i]]\n#     pred_label = ['Model A wins', 'Model B wins', 'Tie'][y_val_pred[i]]\n#     correct = \"âœ…\" if y_val[i] == y_val_pred[i] else \"âŒ\"\n    \n#     print(f\"\\nExample {i+1}: {correct}\")\n#     print(f\"Input: {X_val[i][:100]}...\")\n#     print(f\"True: {true_label}\")\n#     print(f\"Predicted: {pred_label}\")\n\n# print(f\"\\n=== BASELINE RESULTS SUMMARY ===\")\n# print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n# print(\"This is your baseline to beat with better models!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:31:50.235246Z","iopub.execute_input":"2025-07-21T05:31:50.235553Z","iopub.status.idle":"2025-07-21T05:31:50.240733Z","shell.execute_reply.started":"2025-07-21T05:31:50.235534Z","shell.execute_reply":"2025-07-21T05:31:50.239809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Accuracy is very less on regression cause for this type of dataset we need to capture the sematic/contextual meaning of data for that we need to use transformers model like BERT","metadata":{}},{"cell_type":"markdown","source":"## Preparing Dataset for Finetuning","metadata":{}},{"cell_type":"code","source":"model_path = \"/kaggle/input/bert/transformers/default/1/bert-base-uncased-local\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:37:26.122369Z","iopub.execute_input":"2025-07-21T05:37:26.123022Z","iopub.status.idle":"2025-07-21T05:37:26.276044Z","shell.execute_reply.started":"2025-07-21T05:37:26.122988Z","shell.execute_reply":"2025-07-21T05:37:26.275248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode labels\nlabel_names = ['Model A wins', 'Model B wins', 'Tie']\nlabel_encoder = LabelEncoder()\ny_train_enc = label_encoder.fit_transform(y_train)\ny_val_enc = label_encoder.transform(y_val)\n\n# Tokenize data\ndef tokenize_function(example):\n    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n\ndef clean_text(text):\n    return text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n    \nX_train_cleaned = [clean_text(t) for t in X_train]\nX_val_cleaned = [clean_text(t) for t in X_val]\n\n\n# Wrap into HuggingFace Dataset format\ntrain_dataset = Dataset.from_dict({'text': X_train_cleaned, 'label': y_train_enc})\nval_dataset = Dataset.from_dict({'text': X_val_cleaned, 'label': y_val_enc})\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:32:00.425774Z","iopub.execute_input":"2025-07-21T05:32:00.426098Z","iopub.status.idle":"2025-07-21T05:33:12.591857Z","shell.execute_reply.started":"2025-07-21T05:32:00.426076Z","shell.execute_reply":"2025-07-21T05:33:12.591089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine Tuning Using Bert","metadata":{}},{"cell_type":"code","source":"# âœ… Paths & Configs\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nMODEL_DIR = \"/kaggle/working/models\"\ntimestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\nrun_name = f\"deberta-v3-large-{timestamp}\"\n\n# âœ… Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=f\"{MODEL_DIR}/results\",\n    run_name=run_name,\n    num_train_epochs=4,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_steps=10000,\n    logging_first_step=True,\n    eval_strategy=\"no\",\n    save_strategy=\"no\",\n    fp16=True,\n    dataloader_num_workers=0,\n    seed=42,\n    report_to=[]\n)\n\n# âœ… Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=lambda p: {\n        'accuracy': (p.predictions.argmax(axis=-1) == p.label_ids).mean()}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:50:30.045372Z","iopub.execute_input":"2025-07-21T05:50:30.04596Z","iopub.status.idle":"2025-07-21T05:50:30.086554Z","shell.execute_reply.started":"2025-07-21T05:50:30.045934Z","shell.execute_reply":"2025-07-21T05:50:30.085849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:37:34.441913Z","iopub.execute_input":"2025-07-21T05:37:34.442757Z","iopub.status.idle":"2025-07-21T05:47:03.17105Z","shell.execute_reply.started":"2025-07-21T05:37:34.442723Z","shell.execute_reply":"2025-07-21T05:47:03.170417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_DIR = \"./models\"\nfinal_model_path = f\"{MODEL_DIR}/final\"\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:07.380752Z","iopub.execute_input":"2025-07-21T05:49:07.381464Z","iopub.status.idle":"2025-07-21T05:49:08.353977Z","shell.execute_reply.started":"2025-07-21T05:49:07.381442Z","shell.execute_reply":"2025-07-21T05:49:08.353337Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# results = trainer.evaluate()\n# print(results)\n\n# # Classification Repor\n# preds = trainer.predict(val_dataset)\n# y_pred = np.argmax(preds.predictions, axis=1)\n\n# print(classification_report(y_val_enc, y_pred, target_names=label_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T10:32:13.410345Z","iopub.execute_input":"2025-07-18T10:32:13.411004Z","iopub.status.idle":"2025-07-18T10:33:35.934938Z","shell.execute_reply.started":"2025-07-18T10:32:13.410974Z","shell.execute_reply":"2025-07-18T10:33:35.934445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def load_model(model_path):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    return model, tokenizer\n\n\nmodel, tokenizer = load_model(final_model_path)\nmodel.to(device)\n\ntext = \"This is a test sentence\"\ninputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\noutputs = model(**inputs.to(device))\npredictions = outputs.logits\npredictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:12.130925Z","iopub.execute_input":"2025-07-21T05:49:12.131212Z","iopub.status.idle":"2025-07-21T05:49:12.581308Z","shell.execute_reply.started":"2025-07-21T05:49:12.131192Z","shell.execute_reply":"2025-07-21T05:49:12.580631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = F.softmax(predictions, dim=-1)\npreds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:14.944923Z","iopub.execute_input":"2025-07-21T05:49:14.945427Z","iopub.status.idle":"2025-07-21T05:49:14.95194Z","shell.execute_reply.started":"2025-07-21T05:49:14.945403Z","shell.execute_reply":"2025-07-21T05:49:14.951173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_bert_fmt(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"text\"] = df[\"prompt\"] + \" [SEP] \" + df[\"response_a\"] + \" [SEP] \" + df[\"response_b\"]\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:17.190625Z","iopub.execute_input":"2025-07-21T05:49:17.191176Z","iopub.status.idle":"2025-07-21T05:49:17.195018Z","shell.execute_reply.started":"2025-07-21T05:49:17.191151Z","shell.execute_reply":"2025-07-21T05:49:17.194195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = apply_bert_fmt(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:18.983322Z","iopub.execute_input":"2025-07-21T05:49:18.984087Z","iopub.status.idle":"2025-07-21T05:49:18.988765Z","shell.execute_reply.started":"2025-07-21T05:49:18.984052Z","shell.execute_reply":"2025-07-21T05:49:18.988059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Tokenize texts\nencodings = tokenizer(\n    df_test[\"text\"].tolist(),\n    truncation=True,\n    padding=True,\n    return_tensors=\"pt\"\n)\n\n# Step 2: Create a custom Dataset\nclass CustomTextDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\ntest_dataset = CustomTextDataset(encodings)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# Step 3: Run inference\nall_probabilities = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = {k: v.to(model.device) for k, v in batch.items()}\n        outputs = model(**inputs)\n        probabilities = F.softmax(outputs.logits, dim=-1)\n        all_probabilities.extend(probabilities.cpu().numpy())\n\nfinal_probs = np.vstack(all_probabilities)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:21.951334Z","iopub.execute_input":"2025-07-21T05:49:21.952046Z","iopub.status.idle":"2025-07-21T05:49:22.033568Z","shell.execute_reply.started":"2025-07-21T05:49:21.952019Z","shell.execute_reply":"2025-07-21T05:49:22.032785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:25.114847Z","iopub.execute_input":"2025-07-21T05:49:25.115503Z","iopub.status.idle":"2025-07-21T05:49:25.120545Z","shell.execute_reply.started":"2025-07-21T05:49:25.115481Z","shell.execute_reply":"2025-07-21T05:49:25.119717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"submission_df = df_test\nsubmission_df[\"winner_model_a\"] = final_probs[:, 0]\nsubmission_df[\"winner_model_b\"] = final_probs[:, 1]\nsubmission_df[\"winner_tie\"] = final_probs[:, 2]\n\nsubmission_df = submission_df[[\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]\nsubmission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:28.205951Z","iopub.execute_input":"2025-07-21T05:49:28.20662Z","iopub.status.idle":"2025-07-21T05:49:28.228427Z","shell.execute_reply.started":"2025-07-21T05:49:28.206599Z","shell.execute_reply":"2025-07-21T05:49:28.22773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T05:49:32.166466Z","iopub.execute_input":"2025-07-21T05:49:32.167024Z","iopub.status.idle":"2025-07-21T05:49:32.179537Z","shell.execute_reply.started":"2025-07-21T05:49:32.167Z","shell.execute_reply":"2025-07-21T05:49:32.178919Z"}},"outputs":[],"execution_count":null}]}