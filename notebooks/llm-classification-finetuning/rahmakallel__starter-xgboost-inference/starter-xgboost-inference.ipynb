{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":188673364,"sourceType":"kernelVersion"},{"sourceId":203706354,"sourceType":"kernelVersion"},{"sourceId":85509,"sourceType":"modelInstanceVersion","modelInstanceId":71846,"modelId":96809}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import essential libraries \nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nimport xgboost as xgb\nfrom torch.utils.data import DataLoader\n\n# Define batch size for data loading\nbatch_size = 32\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:03:36.003677Z","iopub.execute_input":"2024-10-27T14:03:36.004481Z","iopub.status.idle":"2024-10-27T14:03:43.70227Z","shell.execute_reply.started":"2024-10-27T14:03:36.004407Z","shell.execute_reply":"2024-10-27T14:03:43.70094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{}},{"cell_type":"code","source":"# Custom dataset class for LLM classification data\nclass LmsysDataset:\n    def __init__(self, data, target=None, tokenizer=None):\n        self.data = data\n        self.target = target\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        def process_text(text):\n            return ' '.join([s.strip('\"') for s in text.strip('[]').split('\",\"')])\n        \n        prompt = self.data.iloc[idx][\"prompt\"]\n        response_a = self.data.iloc[idx][\"response_a\"]\n        response_b = self.data.iloc[idx][\"response_b\"]\n        \n        if self.target is not None:\n            y = torch.tensor([self.target.iloc[idx][\"winner_model_a\"], \n                              self.target.iloc[idx][\"winner_model_b\"], \n                              self.target.iloc[idx][\"winner_tie\"]])\n        else:\n            y = torch.tensor([0, 0, 0])\n\n        # Combine prompt and responses for input text\n        text = \"prompt: \" + process_text(prompt) + \" model_a: \" + process_text(response_a) + \" model_b: \" + process_text(response_b)\n\n        if self.tokenizer is not None:\n            # Tokenize and encode the combined text\n            encoding = self.tokenizer.encode_plus(text, truncation=True, padding='max_length', max_length=2048, return_tensors=\"pt\")\n            \n            input_ids = encoding['input_ids'].squeeze(0)\n            attention_mask = encoding['attention_mask'].squeeze(0)\n\n            return input_ids, attention_mask, y\n        else:\n            return text, y\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:03:43.704544Z","iopub.execute_input":"2024-10-27T14:03:43.705055Z","iopub.status.idle":"2024-10-27T14:03:43.719679Z","shell.execute_reply.started":"2024-10-27T14:03:43.705023Z","shell.execute_reply":"2024-10-27T14:03:43.718004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding generation","metadata":{}},{"cell_type":"code","source":"# Load test data and select relevant columns\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndata = df[[\"prompt\", \"response_a\", \"response_b\"]]\n\n# Initialize tokenizer and model from pre-trained files\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/lmsys-gte/gte_tokenizer\")\nmodel = AutoModel.from_pretrained(\"/kaggle/input/base_custom_gte/transformers/default/1\", trust_remote_code=True)\n\n# Create dataset instance with tokenized data\ndataset = LmsysDataset(data, tokenizer=tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:03:43.726266Z","iopub.execute_input":"2024-10-27T14:03:43.72688Z","iopub.status.idle":"2024-10-27T14:03:48.494331Z","shell.execute_reply.started":"2024-10-27T14:03:43.726846Z","shell.execute_reply":"2024-10-27T14:03:48.493024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for mean pooling to average token embeddings based on attention mask\ndef mean_pooling(last_hidden_state, attention_mask):\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n    \n# Function to process each batch and extract embeddings\ndef process_batch(model, batch, device):\n    input_ids, attention_mask, labels = batch\n    input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    embeddings = mean_pooling(outputs.last_hidden_state, attention_mask)\n    labels = labels.numpy()\n\n    return embeddings, labels\n","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-10-27T14:03:48.496043Z","iopub.execute_input":"2024-10-27T14:03:48.496664Z","iopub.status.idle":"2024-10-27T14:03:48.506741Z","shell.execute_reply.started":"2024-10-27T14:03:48.496628Z","shell.execute_reply":"2024-10-27T14:03:48.505201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set device to GPU if available, otherwise CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Initialize dataloader for batching\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\ndata_list = []\n\n# Process each batch, collect embeddings and labels\nfor batch in dataloader:\n    embeddings, labels = process_batch(model, batch, device)\n    for emb, label in zip(embeddings, labels):\n        data_list.append({\n            \"emb\": emb.tolist(),\n            \"label\": label.tolist()\n        })\n\n# Convert embeddings and labels to a DataFrame\nemb_df = pd.DataFrame(data_list, columns=[\"emb\", \"label\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:03:48.508511Z","iopub.execute_input":"2024-10-27T14:03:48.508945Z","iopub.status.idle":"2024-10-27T14:04:16.233451Z","shell.execute_reply.started":"2024-10-27T14:03:48.5089Z","shell.execute_reply":"2024-10-27T14:04:16.232366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# Load pretrained XGBoost classifier model\nxgb_model = xgb.XGBClassifier()\nxgb_model.load_model('/kaggle/input/lmsys-xgb/xgb_model.json')\n\n# Prepare data and make predictions\nX = np.vstack(emb_df['emb'].values)\ny_pred = xgb_model.predict_proba(X)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:04:16.235065Z","iopub.execute_input":"2024-10-27T14:04:16.236031Z","iopub.status.idle":"2024-10-27T14:04:16.304339Z","shell.execute_reply.started":"2024-10-27T14:04:16.235986Z","shell.execute_reply":"2024-10-27T14:04:16.303274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission DataFrame with prediction probabilities for each outcome\nsubmission = pd.DataFrame({\n    'id': df['id'],\n    'winner_model_a': y_pred[:, 0],\n    'winner_model_b': y_pred[:, 1],\n    'winner_tie': y_pred[:, 2]\n})\n\n# Save submission file\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:04:16.305997Z","iopub.execute_input":"2024-10-27T14:04:16.30667Z","iopub.status.idle":"2024-10-27T14:04:16.321224Z","shell.execute_reply.started":"2024-10-27T14:04:16.306627Z","shell.execute_reply":"2024-10-27T14:04:16.320007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:04:16.32286Z","iopub.execute_input":"2024-10-27T14:04:16.323257Z","iopub.status.idle":"2024-10-27T14:04:16.342997Z","shell.execute_reply.started":"2024-10-27T14:04:16.323226Z","shell.execute_reply":"2024-10-27T14:04:16.341779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimentation and Optimization\n\nFeel free to experiment with the following ideas to enhance your model's performance:\n\n- **Different Pretrained Models**: Try using various pretrained models from the Hugging Face model hub to see which one works best for your dataset.\n\n- **Grid Search for XGBoost**: Implement a grid search to optimize the hyperparameters of the XGBoost classifier. Consider parameters like `max_depth`, `learning_rate`, `n_estimators`, and `subsample`.\n\n- **Feature Engineering**: Explore different ways to preprocess and engineer features from the text data. Experiment with techniques like adding more context, using different tokenization strategies, or leveraging embeddings differently.\n\n- **Model Ensembling**: Combine predictions from multiple models (e.g., stacking or voting) to potentially improve accuracy and robustness.\n\n- **Cross-Validation**: Use cross-validation techniques to better understand model performance and avoid overfitting.\n\n- **Hyperparameter Tuning for Other Models**: Besides XGBoost, consider tuning hyperparameters for any other models you incorporate.\n\n- **Error Analysis**: Perform an analysis of the predictions to identify common misclassifications and refine your approach accordingly.\n\nExplore these ideas to see how they impact your results!\n","metadata":{}}]}