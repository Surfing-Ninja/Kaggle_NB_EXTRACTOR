{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":4137906,"sourceType":"datasetVersion","datasetId":2444257},{"sourceId":11381550,"sourceType":"datasetVersion","datasetId":7126480}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install sentence-transformers\n# !pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport threading\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Check available GPUs\nnum_gpus = torch.cuda.device_count()\nprint(f\"Available GPUs: {num_gpus}\")\nif num_gpus < 2:\n    raise RuntimeError(\"This script requires at least 2 GPUs\")\n\ndef get_semantic_overlap(df):\n    \n    # Create models for each GPU\n    model_path = \"/kaggle/input/sentencetransformersallmpnetbasev2/all-mpnet-base-v2\"\n    prompt_model = SentenceTransformer(model_path)\n    prompt_model = prompt_model.to('cuda:0')  # First GPU\n    \n    response_a_model = SentenceTransformer(model_path)\n    response_a_model = response_a_model.to('cuda:1')  # Second GPU\n    \n    # Will use either GPU 0 or 1 based on which finishes first\n    response_b_model = None  # Will initialize this later\n    \n    # Variables to store results\n    prompt_embeddings = None\n    response_a_embeddings = None\n    response_b_embeddings = None\n    \n    # Lock for thread synchronization\n    lock = threading.Lock()\n    prompt_done = False\n    response_a_done = False\n    gpu_assigned = False  # New flag to track if GPU has been assigned for response_b\n    \n    # Define functions for parallel processing\n    def generate_prompt_embeddings():\n        nonlocal prompt_embeddings, prompt_done, gpu_assigned\n        print(\"Generating prompt embeddings on GPU 0...\")\n        prompt_embeddings = prompt_model.encode(df['prompt'].tolist(), batch_size=128, show_progress_bar=True)\n        with lock:\n            prompt_done = True\n            # If GPU hasn't been assigned yet, assign GPU 0 for response_b\n            if not gpu_assigned:\n                gpu_assigned = True\n                gpu_to_use.append(0)\n        print(\"Prompt embeddings completed on GPU 0\")\n    \n    def generate_response_a_embeddings():\n        nonlocal response_a_embeddings, response_a_done, gpu_assigned\n        print(\"Generating response A embeddings on GPU 1...\")\n        response_a_embeddings = response_a_model.encode(df['response_a'].tolist(), batch_size=64, show_progress_bar=True)\n        with lock:\n            response_a_done = True\n            # If GPU hasn't been assigned yet, assign GPU 1 for response_b\n            if not gpu_assigned:\n                gpu_assigned = True\n                gpu_to_use.append(1)\n        print(\"Response A embeddings completed on GPU 1\")\n    \n    # Create a shared variable to communicate which GPU to use\n    gpu_to_use = []\n    \n    def generate_response_b_embeddings():\n        nonlocal response_b_embeddings, response_b_model\n        # Wait until a GPU is assigned\n        while True:\n            with lock:\n                if gpu_to_use:  # If a GPU has been assigned\n                    break\n            # Small sleep to prevent CPU hogging\n            import time\n            time.sleep(0.1)\n        \n        chosen_gpu = gpu_to_use[0]  # Get the assigned GPU\n        print(f\"Generating response B embeddings on GPU {chosen_gpu}...\")\n        response_b_model = SentenceTransformer(model_path)\n        response_b_model = response_b_model.to(f'cuda:{chosen_gpu}')\n        response_b_embeddings = response_b_model.encode(df['response_b'].tolist(), batch_size=64, show_progress_bar=True)\n        print(f\"Response B embeddings completed on GPU {chosen_gpu}\")\n    \n    # Start threads for parallel processing\n    thread_prompt = threading.Thread(target=generate_prompt_embeddings)\n    thread_response_a = threading.Thread(target=generate_response_a_embeddings)\n    thread_response_b = threading.Thread(target=generate_response_b_embeddings)\n    \n    thread_prompt.start()\n    thread_response_a.start()\n    thread_response_b.start()\n    \n    # Wait for all threads to complete\n    thread_prompt.join()\n    thread_response_a.join()\n    thread_response_b.join()\n    \n    # Calculate cosine similarities\n    print(\"Calculating similarities...\")\n    df['a_semantic_overlap'] = np.array([\n        cosine_similarity([p], [r])[0][0] \n        for p, r in zip(prompt_embeddings, response_a_embeddings)\n    ])\n    df['b_semantic_overlap'] = np.array([\n        cosine_similarity([p], [r])[0][0] \n        for p, r in zip(prompt_embeddings, response_b_embeddings)\n    ])\n    \n    # Save the results\n    # df.to_csv('/kaggle/working/df_with_semantic_overlap.csv', index=False)\n    print(\"All done!\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:12:38.657043Z","iopub.execute_input":"2025-04-12T20:12:38.657453Z","iopub.status.idle":"2025-04-12T20:12:38.673956Z","shell.execute_reply.started":"2025-04-12T20:12:38.657415Z","shell.execute_reply":"2025-04-12T20:12:38.672828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your data\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:12:38.675562Z","iopub.execute_input":"2025-04-12T20:12:38.675906Z","iopub.status.idle":"2025-04-12T20:12:40.667255Z","shell.execute_reply.started":"2025-04-12T20:12:38.67588Z","shell.execute_reply":"2025-04-12T20:12:40.666286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ntest_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:12:40.669714Z","iopub.execute_input":"2025-04-12T20:12:40.669956Z","iopub.status.idle":"2025-04-12T20:12:40.682897Z","shell.execute_reply.started":"2025-04-12T20:12:40.669936Z","shell.execute_reply":"2025-04-12T20:12:40.681662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_semantic_overlap = get_semantic_overlap(df)\n# \ndf_test_with_semantic_overlap = get_semantic_overlap(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:12:40.685516Z","iopub.execute_input":"2025-04-12T20:12:40.685774Z","iopub.status.idle":"2025-04-12T20:43:53.953244Z","shell.execute_reply.started":"2025-04-12T20:12:40.685751Z","shell.execute_reply":"2025-04-12T20:43:53.952428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report\nimport xgboost as xgb\nimport re\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:43:53.954001Z","iopub.execute_input":"2025-04-12T20:43:53.954254Z","iopub.status.idle":"2025-04-12T20:43:53.95919Z","shell.execute_reply.started":"2025-04-12T20:43:53.954227Z","shell.execute_reply":"2025-04-12T20:43:53.958249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download necessary NLTK resources\n# nltk.download('vader_lexicon')\nnltk_data_dir = \"/kaggle/input/vader-lexicon/vader_lexicon.txt\"\nnltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')\nsentiment_dir = os.path.join(nltk_data_dir, 'sentiment')\nos.makedirs(sentiment_dir, exist_ok=True)\n\n# 3. Copy the lexicon file from the input directory to NLTK's expected location\nvader_source = \"/kaggle/input/vaderlexicon/vader_lexicon.txt\"\nvader_destination = os.path.join(sentiment_dir, \"vader_lexicon.txt\")\n\n# Only copy if the file doesn't already exist in the destination\nif not os.path.exists(vader_destination):\n    shutil.copy(vader_source, vader_destination)\n    print(f\"Copied VADER lexicon from {vader_source} to {vader_destination}\")\nelse:\n    print(f\"VADER lexicon already exists at {vader_destination}\")\n\n# 4. Tell NLTK to look for data in your custom directory\nnltk.data.path.append(nltk_data_dir)\n\n# Feature Engineering Function\n\n# Initialize sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Create a new dataframe for features\nfeatures_df = pd.DataFrame(index=df.index)\n\nfeatures_df_test = pd.DataFrame(index=test_df.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:43:53.9602Z","iopub.execute_input":"2025-04-12T20:43:53.960528Z","iopub.status.idle":"2025-04-12T20:43:53.996467Z","shell.execute_reply.started":"2025-04-12T20:43:53.960494Z","shell.execute_reply":"2025-04-12T20:43:53.995628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:43:53.997298Z","iopub.execute_input":"2025-04-12T20:43:53.997576Z","iopub.status.idle":"2025-04-12T20:43:54.015015Z","shell.execute_reply.started":"2025-04-12T20:43:53.997553Z","shell.execute_reply":"2025-04-12T20:43:54.014149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# features_df[\"a_semantic_overlap\"] = df[\"a_semantic_overlap\"]\n# features_df[\"b_semantic_overlap\"] = df[\"b_semantic_overlap\"]\n\n# 1. Model type features\n# Encode model names\n\ndef add_features(features_df, df):\n    \n    # 2. Text-based features for responses\n    \n    # Response length features\n    features_df['resp_a_length'] = df['response_a'].apply(len)\n    features_df['resp_b_length'] = df['response_b'].apply(len)\n    features_df['resp_length_diff'] = features_df['resp_a_length'] - features_df['resp_b_length']\n    features_df['resp_length_ratio'] = features_df['resp_a_length'] / features_df['resp_b_length']\n    \n    # Sentiment features\n    features_df['resp_a_sentiment'] = df['response_a'].apply(lambda x: sia.polarity_scores(x)['compound'])\n    features_df['resp_b_sentiment'] = df['response_b'].apply(lambda x: sia.polarity_scores(x)['compound'])\n    features_df['sentiment_diff'] = features_df['resp_a_sentiment'] - features_df['resp_b_sentiment']\n    \n    # 3. Response complexity features\n    \n    # Average word length\n    features_df['resp_a_avg_word_len'] = df['response_a'].apply(\n        lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0\n    )\n    features_df['resp_b_avg_word_len'] = df['response_b'].apply(\n        lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0\n    )\n    \n    # Lexical diversity (unique words ratio)\n    features_df['resp_a_lexical_div'] = df['response_a'].apply(\n        lambda x: len(set(x.lower().split())) / len(x.split()) if len(x.split()) > 0 else 0\n    )\n    features_df['resp_b_lexical_div'] = df['response_b'].apply(\n        lambda x: len(set(x.lower().split())) / len(x.split()) if len(x.split()) > 0 else 0\n    )\n    \n    # 4. Semantic similarity between prompt and responses\n    \n    # For a real implementation, you would use embeddings or cosine similarity\n    # Here, I'll use a simple approach: percentage of prompt words in response\n    def word_overlap_ratio(prompt, response):\n        prompt_words = set(prompt.lower().split())\n        response_words = set(response.lower().split())\n        if len(prompt_words) == 0:\n            return 0\n        return len(prompt_words.intersection(response_words)) / len(prompt_words)\n    \n    features_df['prompt_resp_a_overlap'] = df.apply(\n        lambda row: word_overlap_ratio(row['prompt'], row['response_a']), axis=1\n    )\n    features_df['prompt_resp_b_overlap'] = df.apply(\n        lambda row: word_overlap_ratio(row['prompt'], row['response_b']), axis=1\n    )\n    \n    # 5. Response similarity\n    features_df['resp_similarity'] = df.apply(\n        lambda row: word_overlap_ratio(row['response_a'], row['response_b']), axis=1\n    )\n    \n    # 6. First-person language features (\"I\", \"me\", \"my\")\n    features_df['resp_a_first_person'] = df['response_a'].apply(\n        lambda x: len(re.findall(r'\\b(I|me|my|myself)\\b', x, re.IGNORECASE))\n    )\n    features_df['resp_b_first_person'] = df['response_b'].apply(\n        lambda x: len(re.findall(r'\\b(I|me|my|myself)\\b', x, re.IGNORECASE))\n    )\n    \n    # # 7. Use of technical terms or jargon (simplified approach)\n    # technical_terms = ['function', 'algorithm', 'data', 'model', 'system', 'process', 'code', 'api']\n    # features_df['resp_a_technical'] = df['response_a'].apply(\n    #     lambda x: sum(1 for term in technical_terms if term.lower() in x.lower().split())\n    # )\n    # features_df['resp_b_technical'] = df['response_b'].apply(\n    #     lambda x: sum(1 for term in technical_terms if term.lower() in x.lower().split())\n    # )\n    \n    # 8. Question marks and direct engagement\n    # features_df['resp_a_questions'] = df['response_a'].apply(lambda x: x.count('?'))\n    # features_df['resp_b_questions'] = df['response_b'].apply(lambda x: x.count('?'))\n    return features_df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:43:54.017305Z","iopub.execute_input":"2025-04-12T20:43:54.017533Z","iopub.status.idle":"2025-04-12T20:43:54.042052Z","shell.execute_reply.started":"2025-04-12T20:43:54.017505Z","shell.execute_reply":"2025-04-12T20:43:54.041371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df = add_features(features_df, df)\nfeatures_df['a_semantic_overlap'] = df_with_semantic_overlap['a_semantic_overlap']\nfeatures_df['b_semantic_overlap'] = df_with_semantic_overlap['b_semantic_overlap']\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:43:54.04357Z","iopub.execute_input":"2025-04-12T20:43:54.043916Z","iopub.status.idle":"2025-04-12T20:48:04.000042Z","shell.execute_reply.started":"2025-04-12T20:43:54.04389Z","shell.execute_reply":"2025-04-12T20:48:03.999286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:04.000883Z","iopub.execute_input":"2025-04-12T20:48:04.00112Z","iopub.status.idle":"2025-04-12T20:48:04.019633Z","shell.execute_reply.started":"2025-04-12T20:48:04.001098Z","shell.execute_reply":"2025-04-12T20:48:04.018827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_features_df = add_features(features_df_test, test_df)\ntest_features_df['a_semantic_overlap'] =  df_test_with_semantic_overlap['a_semantic_overlap']\ntest_features_df['b_semantic_overlap'] =  df_test_with_semantic_overlap['b_semantic_overlap']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:04.020502Z","iopub.execute_input":"2025-04-12T20:48:04.02075Z","iopub.status.idle":"2025-04-12T20:48:04.057065Z","shell.execute_reply.started":"2025-04-12T20:48:04.020727Z","shell.execute_reply":"2025-04-12T20:48:04.055976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Generate features\nfeatures = features_df\n\n# Prepare target variable (1 if model_a wins, 0 if model_b wins, 2 if tie)\ny = df['winner_model_a'] * 1 + df['winner_model_b'] * 0 + df['winner_tie'] * 2\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42)\n\n# Set up XGBoost with log loss optimization\nxgb_model = xgb.XGBClassifier(\n    objective='multi:softprob',  # This optimizes for log loss\n    num_class=3,\n    learning_rate=0.05,  # Smaller learning rate for better generalization\n    max_depth=4,\n    n_estimators=200,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.01,\n    reg_lambda=1,\n    random_state=42\n)\n\n# Train the model\nxgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    eval_metric='mlogloss',  # Log loss evaluation metric\n    early_stopping_rounds=20,\n    verbose=True\n)\n\n# Make predictions on the test set\ny_pred = xgb_model.predict(X_test)\ny_pred_proba = xgb_model.predict_proba(X_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:04.058153Z","iopub.execute_input":"2025-04-12T20:48:04.058529Z","iopub.status.idle":"2025-04-12T20:48:08.182879Z","shell.execute_reply.started":"2025-04-12T20:48:04.058493Z","shell.execute_reply":"2025-04-12T20:48:08.181423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_proba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:08.183808Z","iopub.execute_input":"2025-04-12T20:48:08.184106Z","iopub.status.idle":"2025-04-12T20:48:08.191026Z","shell.execute_reply.started":"2025-04-12T20:48:08.184081Z","shell.execute_reply":"2025-04-12T20:48:08.190093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate log loss (this is the competition metric)\nfrom sklearn.metrics import log_loss\ntest_log_loss = log_loss(y_test, y_pred_proba)\nprint(f\"\\nTest Log Loss: {test_log_loss:.4f} (Lower is better)\")\n\n# Calculate accuracy and print classification report\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Cross-validation with log loss scoring\nfrom sklearn.model_selection import cross_val_score\ncv_log_loss = cross_val_score(xgb_model, features, y, cv=5, scoring='neg_log_loss')\nprint(\"\\nCross-validation evaluation:\")\nprint(f\"CV Log Loss scores: {-cv_log_loss}\")  # Negate to get the actual log loss\nprint(f\"Mean CV Log Loss: {-cv_log_loss.mean():.4f} (Lower is better)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:08.191786Z","iopub.execute_input":"2025-04-12T20:48:08.192013Z","iopub.status.idle":"2025-04-12T20:48:19.850625Z","shell.execute_reply.started":"2025-04-12T20:48:08.191992Z","shell.execute_reply":"2025-04-12T20:48:19.849662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Get feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': features.columns,\n    'Importance': xgb_model.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"Top 10 most important features:\")\nprint(feature_importance.head(10))\n\n# This is how you would use the model to make predictions on new data\ndef predict_preferred_model(new_data):\n    new_features = engineer_features(new_data)\n    predictions = xgb_model.predict(new_features)\n    probabilities = xgb_model.predict_proba(new_features)\n    \n    # Map predictions to model preference\n    prediction_map = {\n        0: \"User prefers model_a\",\n        1: \"User prefers model_b\", \n        2: \"User has no preference (tie)\"\n    }\n    \n    results = []\n    for i, pred in enumerate(predictions):\n        results.append({\n            'Prediction': prediction_map[pred],\n            'Probability': probabilities[i][pred],\n            'model_a': new_data.iloc[i]['model_a'],\n            'model_b': new_data.iloc[i]['model_b']\n        })\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:19.851565Z","iopub.execute_input":"2025-04-12T20:48:19.85192Z","iopub.status.idle":"2025-04-12T20:48:19.869153Z","shell.execute_reply.started":"2025-04-12T20:48:19.851882Z","shell.execute_reply":"2025-04-12T20:48:19.868136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = test_features_df\ntest_ids = X_test.index\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:19.870275Z","iopub.execute_input":"2025-04-12T20:48:19.870653Z","iopub.status.idle":"2025-04-12T20:48:19.878804Z","shell.execute_reply.started":"2025-04-12T20:48:19.870614Z","shell.execute_reply":"2025-04-12T20:48:19.877787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to prepare final submission\ndef prepare_final_submission(model, test_data, test_ids):\n    \"\"\"\n    Prepare the final submission file according to competition format\n    \"\"\"\n    # Get probability predictions\n    probas = model.predict_proba(test_data)\n    \n    # Create submission DataFrame\n    submission = pd.DataFrame({\n        'id': test_ids,\n        'winner_model_a': probas[:, 1],  # Class 1 (model_a wins)\n        'winner_model_b': probas[:, 0],  # Class 0 (model_b wins)\n        'winner_tie': probas[:, 2]       # Class 2 (tie)\n    })\n    \n    # Ensure probabilities sum to 1\n    submission_probs = submission[['winner_model_a', 'winner_model_b', 'winner_tie']]\n    row_sums = submission_probs.sum(axis=1)\n    submission[['winner_model_a', 'winner_model_b', 'winner_tie']] = submission_probs.div(row_sums, axis=0)\n    \n    # Round all probability columns to 2 decimal places\n    submission[['winner_model_a', 'winner_model_b', 'winner_tie']] = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].round(2)\n    \n    # Re-normalize after rounding to ensure they sum to 1 again\n    submission_probs = submission[['winner_model_a', 'winner_model_b', 'winner_tie']]\n    row_sums = submission_probs.sum(axis=1)\n    submission[['winner_model_a', 'winner_model_b', 'winner_tie']] = submission_probs.div(row_sums, axis=0)\n    \n    return submission\n\n# Example of submission preparation (with mock test data)\nfinal_submission = prepare_final_submission(xgb_model, X_test, test_ids)\nfinal_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:19.879626Z","iopub.execute_input":"2025-04-12T20:48:19.87988Z","iopub.status.idle":"2025-04-12T20:48:19.89769Z","shell.execute_reply.started":"2025-04-12T20:48:19.879857Z","shell.execute_reply":"2025-04-12T20:48:19.896694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}