{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":104623,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30734,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libs ","metadata":{}},{"cell_type":"code","source":"# Install libs\n!pip install -qq sentencepiece\n!pip install -qq datasets\n!pip install -qq peft==0.9.0\n!pip install -qq bitsandbytes==0.41.1\n!pip install -qq accelerate==0.27.2\n!pip install -qq transformers==4.42.3\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:45:36.830613Z","iopub.execute_input":"2025-05-13T15:45:36.830839Z","iopub.status.idle":"2025-05-13T15:47:55.797514Z","shell.execute_reply.started":"2025-05-13T15:45:36.830815Z","shell.execute_reply":"2025-05-13T15:47:55.796508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\nfrom torch.cuda.amp import autocast\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:47:55.799578Z","iopub.execute_input":"2025-05-13T15:47:55.799854Z","iopub.status.idle":"2025-05-13T15:48:05.609634Z","shell.execute_reply.started":"2025-05-13T15:47:55.799826Z","shell.execute_reply":"2025-05-13T15:48:05.608931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\nfrom dataclasses import dataclass\nfrom datasets import Dataset\nfrom transformers import (\n    BitsAndBytesConfig,\n    Gemma2ForSequenceClassification,\n    GemmaTokenizerFast,\n    Gemma2Config,\n    PreTrainedTokenizerBase, \n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom sklearn.metrics import log_loss, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:05.610507Z","iopub.execute_input":"2025-05-13T15:48:05.610906Z","iopub.status.idle":"2025-05-13T15:48:07.916937Z","shell.execute_reply.started":"2025-05-13T15:48:05.610879Z","shell.execute_reply":"2025-05-13T15:48:07.916218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass Config:\n    output_dir: str = \"output\"\n    checkpoint: str = '/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2'  # gemma-2-9b-instruct\n    max_length: int = 1024\n    n_splits: int = 5\n    fold_idx: int = 0\n    optim_type: str = \"adamw_torch_xla\"\n    per_device_train_batch_size: int = 8\n    gradient_accumulation_steps: int = 1\n    per_device_eval_batch_size: int = 8\n    n_epochs: int = 1\n    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n    lr: float = 5e-5\n    warmup_steps: int = 128\n    lora_r: int = 4\n    lora_alpha: float = lora_r * 2\n    lora_dropout: float = 0.05\n    lora_bias: str = \"none\"\n    \nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:07.917905Z","iopub.execute_input":"2025-05-13T15:48:07.918304Z","iopub.status.idle":"2025-05-13T15:48:07.924431Z","shell.execute_reply.started":"2025-05-13T15:48:07.918278Z","shell.execute_reply":"2025-05-13T15:48:07.923734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"output\",\n    overwrite_output_dir=True,\n    report_to=\"none\",\n    num_train_epochs=config.n_epochs,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"steps\",\n    save_steps=200,\n    optim=config.optim_type,\n    fp16=False,\n    learning_rate=config.lr,\n    warmup_steps=config.warmup_steps,\n    # Add argument to use TPU with XLA device\n    tpu_num_cores=8,  # Specify the number of TPU cores\n    no_cuda=False  # Enable CUDA for TPU\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:07.925387Z","iopub.execute_input":"2025-05-13T15:48:07.925647Z","iopub.status.idle":"2025-05-13T15:48:12.797009Z","shell.execute_reply.started":"2025-05-13T15:48:07.925622Z","shell.execute_reply":"2025-05-13T15:48:12.79606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    # only target self-attention\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n    lora_dropout=config.lora_dropout,\n    bias=config.lora_bias,\n    task_type=TaskType.SEQ_CLS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:12.798021Z","iopub.execute_input":"2025-05-13T15:48:12.798381Z","iopub.status.idle":"2025-05-13T15:48:12.81222Z","shell.execute_reply.started":"2025-05-13T15:48:12.798353Z","shell.execute_reply":"2025-05-13T15:48:12.811509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:12.814316Z","iopub.execute_input":"2025-05-13T15:48:12.814676Z","iopub.status.idle":"2025-05-13T15:48:13.993885Z","shell.execute_reply.started":"2025-05-13T15:48:12.814649Z","shell.execute_reply":"2025-05-13T15:48:13.992984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model without quantization\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    config.checkpoint,\n    num_labels=3,\n    torch_dtype=torch.bfloat16,  \n    device_map=\"auto\",  \n)\n\nmodel.config.use_cache = False\nmodel = get_peft_model(model, lora_config)\n\n# Move the model to TPU\ndevice = xm.xla_device()\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:13.994885Z","iopub.execute_input":"2025-05-13T15:48:13.995169Z","iopub.status.idle":"2025-05-13T15:48:26.946374Z","shell.execute_reply.started":"2025-05-13T15:48:13.995142Z","shell.execute_reply":"2025-05-13T15:48:26.945465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:26.947368Z","iopub.execute_input":"2025-05-13T15:48:26.947852Z","iopub.status.idle":"2025-05-13T15:48:26.95392Z","shell.execute_reply.started":"2025-05-13T15:48:26.947824Z","shell.execute_reply":"2025-05-13T15:48:26.953186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = Dataset.from_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\nds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:26.95483Z","iopub.execute_input":"2025-05-13T15:48:26.955065Z","iopub.status.idle":"2025-05-13T15:48:31.36738Z","shell.execute_reply.started":"2025-05-13T15:48:26.955041Z","shell.execute_reply":"2025-05-13T15:48:31.366407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List, Dict\nfrom transformers import PreTrainedTokenizerBase\n\nclass CustomTokenizer:\n    def __init__(\n        self, \n        tokenizer: PreTrainedTokenizerBase, \n        max_length: int\n    ) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __call__(self, batch: dict) -> dict:\n        # Add <prompt>, <response_a>, <response_b> and process text properly\n        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n        \n        # Combine the prompt, response_a, and response_b into a list of text\n        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        \n        # Tokenize the combined texts\n        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n        \n        # Generate labels based on the winner model data (ensuring the right check)\n        labels = []\n        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n            if a_win:  # If model A wins\n                label = 0\n            elif b_win:  # If model B wins\n                label = 1\n            else:  # In case of a tie\n                label = 2\n            labels.append(label)\n        \n        # Return tokenized data with the generated labels\n        return {**tokenized, \"labels\": labels}\n        \n    @staticmethod\n    def process_text(text: str) -> str:\n        # Safely process the text, replacing 'null' with an empty string\n        return text.replace(\"null\", \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:31.368407Z","iopub.execute_input":"2025-05-13T15:48:31.368646Z","iopub.status.idle":"2025-05-13T15:48:31.376533Z","shell.execute_reply.started":"2025-05-13T15:48:31.368621Z","shell.execute_reply":"2025-05-13T15:48:31.375745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encode = CustomTokenizer(tokenizer, max_length=config.max_length)\nds = ds.map(encode, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:31.377416Z","iopub.execute_input":"2025-05-13T15:48:31.377666Z","iopub.status.idle":"2025-05-13T15:48:31.918579Z","shell.execute_reply.started":"2025-05-13T15:48:31.377641Z","shell.execute_reply":"2025-05-13T15:48:31.917769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_preds: EvalPrediction) -> dict:\n    preds = eval_preds.predictions\n    labels = eval_preds.label_ids\n    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n    loss = log_loss(y_true=labels, y_pred=probs)\n    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n    return {\"acc\": acc, \"log_loss\": loss}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:31.919474Z","iopub.execute_input":"2025-05-13T15:48:31.919726Z","iopub.status.idle":"2025-05-13T15:48:31.924042Z","shell.execute_reply.started":"2025-05-13T15:48:31.919702Z","shell.execute_reply":"2025-05-13T15:48:31.923412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folds = [\n    (\n        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n    ) \n    for fold_idx in range(config.n_splits)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:31.924846Z","iopub.execute_input":"2025-05-13T15:48:31.925078Z","iopub.status.idle":"2025-05-13T15:48:31.944408Z","shell.execute_reply.started":"2025-05-13T15:48:31.925055Z","shell.execute_reply":"2025-05-13T15:48:31.943788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_idx, eval_idx = folds[config.fold_idx]\n\ntrainer = Trainer(\n    args=training_args, \n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=ds.select(train_idx),\n    eval_dataset=ds.select(eval_idx),\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n)\n\n# Start training with AMP\ndef training_step(model, inputs):\n    # Unpack inputs from the dataloader\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n    labels = inputs[\"labels\"]\n    \n    # Autocast for mixed precision forward pass\n    with autocast(xm.xla_device()):  # Cast to bfloat16 for operations\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = model.compute_loss(outputs, labels)  # Assuming compute_loss method exists\n\n    # Backward pass and optimization\n    loss.backward()\n    xm.optimizer_step(optimizer)\n\n    return loss\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:48:31.945271Z","iopub.execute_input":"2025-05-13T15:48:31.945512Z","iopub.status.idle":"2025-05-13T15:50:46.69793Z","shell.execute_reply.started":"2025-05-13T15:48:31.945489Z","shell.execute_reply":"2025-05-13T15:50:46.696794Z"}},"outputs":[],"execution_count":null}]}