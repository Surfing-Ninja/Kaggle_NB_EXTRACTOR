{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30734,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libs ","metadata":{}},{"cell_type":"code","source":"# Install libs\n!pip install -qq peft==0.9.0\n!pip install -qq bitsandbytes==0.41.1\n!pip install -qq accelerate==0.24.1\n!pip install -qq transformers==4.35.0\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\nimport psutil\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_memory_usage(stage=\"\"):\n    device_type_str = str(xm.xla_device()) \n    \n    if \"TPU\" in device_type_str.upper(): \n        process = psutil.Process(os.getpid())\n        mem_info = process.memory_info()\n        rss_mb = mem_info.rss / (1024 * 1024) \n        vms_mb = mem_info.vms / (1024 * 1024) \n        print(f\"Memory Usage ({stage}) on XLA Device (Process): RSS={rss_mb:.2f} MB, VMS={vms_mb:.2f} MB\")\n        return rss_mb, vms_mb \n    elif torch.cuda.is_available(): \n        allocated_mb = torch.cuda.memory_allocated() / (1024 * 1024)\n        reserved_mb = torch.cuda.memory_reserved() / (1024 * 1024)\n        print(f\"GPU Memory Usage ({stage}): Allocated={allocated_mb:.2f} MB, Reserved={reserved_mb:.2f} MB\")\n        return allocated_mb, reserved_mb\n    else:\n        process = psutil.Process(os.getpid())\n        mem_info = process.memory_info()\n        rss_mb = mem_info.rss / (1024 * 1024)\n        vms_mb = mem_info.vms / (1024 * 1024)\n        print(f\"System Memory Usage ({stage}) (Non-GPU/TPU): RSS={rss_mb:.2f} MB, VMS={vms_mb:.2f} MB\")\n        return rss_mb, vms_mb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_memory_usage(\"Initial - Before everything\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 8\n    DROPOUT = 0.05 \n    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    SEED = 2024 \n    MAX_LENGTH = 1024 \n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5 \n    NUM_LABELS = 3 \n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']\n    \nDEVICE = xm.xla_device()\nget_memory_usage(\"After device initialization\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n    xm.set_rng_state(seed, device=xm.xla_device())  \n\nset_seeds(seed=CFG.SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer\ntokenizer.save_pretrained('tokenizer')\nget_memory_usage(\"After tokenizer loaded\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Utility function giving token length\ndef get_token_lengths(texts):\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    return [len(t) for t in input_ids]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare train\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint('Total train samples: ', len(train))\nget_memory_usage(\"After train data loaded and processed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(train['text'][4])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train with only take 50% train dataset\ntrain = train[:int(len(train) * 0.5)]\n\ntrain.loc[:, 'token_count'] = get_token_lengths(train['text'])\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\ndisplay(train.head())\nget_memory_usage(\"After text column and labels created\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# token Count\ndisplay(train['token_count'].describe().to_frame().astype(int))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.percentile(train['token_count'], 90)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"# Tokenize Data\ntokens = tokenizer(\n    train['text'].tolist(), \n    padding='max_length', \n    max_length=CFG.MAX_LENGTH, \n    truncation=True, \n    return_tensors='np')\n\nINPUT_IDS = tokens['input_ids']\nATTENTION_MASKS = tokens['attention_mask']\nLABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')\nget_memory_usage(\"After tokenization\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_dataset(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        # Shuffle Indices\n        np.random.shuffle(IDXS)\n        # Iterate Over All Indices Once\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)\n            \n            # Shard Over TPU Nodes\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            \n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"get_memory_usage(\"Before loading base model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model for classification with 3 target label\nbase_model = LlamaForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=CFG.NUM_LABELS,\n    torch_dtype=torch.bfloat16)\n\nbase_model.config.pretraining_tp = 1 \n\n# Assign Padding TOKEN\nbase_model.config.pad_token_id = tokenizer.pad_token_id\nget_memory_usage(\"After loading base model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Low-Rank Adaptation [LORA]","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.LORA_RANK,\n    lora_alpha = CFG.LORA_ALPHA,\n    lora_dropout= CFG.DROPOUT, \n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=CFG.LORA_MODULES )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create LoRa Model\nmodel = get_peft_model(base_model, lora_config)\n\n# Trainable Parameters\nmodel.print_trainable_parameters()\nget_memory_usage(\"After creating PEFT model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of TPU Nodes\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n\n# distribute model\npartition_module(model, mesh)\n\nprint(f'num_devices: {num_devices}')\nget_memory_usage(\"After model partitioning/to_device\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_LAYERS_ROWS = []\nTRAINABLE_PARAMS = []\nN_TRAINABLE_PARAMS = 0\n\nfor name, param in model.named_parameters():\n    # Layer Parameter Count\n    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n    if param.requires_grad:\n        # Add Layer Information\n        MODEL_LAYERS_ROWS.append({\n            'param': n_parameters,\n            'name': name,\n            'dtype': param.data.dtype,\n        })\n        # Append Trainable Parameter\n        TRAINABLE_PARAMS.append({ 'params': param })\n        # Add Number Of Trainable Parameters\"\n        N_TRAINABLE_PARAMS += n_parameters\n        \ndisplay(pd.DataFrame(MODEL_LAYERS_ROWS))\n\nprint(f\"\"\"\n===============================\nN_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\nN_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n===============================\n\"\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# LR & Optimizer\nN_SAMPLES = len(train)\nSTEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n\n# Cosine Learning Rate With Warmup\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer=OPTIMIZER,\n    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)\n\nprint(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the data type for the optimizer's state (e.g., momentum buffers)\nfor state in OPTIMIZER.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n            state[v] = v.to(dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids, attention_mask, labels = next(TRAIN_DATASET)\n\nprint(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\nprint(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\nprint(f'labels shape: {labels.shape}, dtype: {labels.dtype}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Dummy Prediction\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    \nprint(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')\nget_memory_usage(\"After dummy prediction\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Put Model In Train Mode\nmodel.train()\n\n# Loss Function, Cross Entropy\nLOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"st = time()\nwarnings.filterwarnings(\"error\")\nMETRICS = {\n    'loss': [],\n    'accuracy': {'y_true': [], 'y_pred': [] }}\n\nfor epoch in tqdm(range(CFG.NUM_EPOCHS)):\n    ste = time()\n    for step in range(STEPS_PER_EPOCH):\n        # Zero Out Gradients\n        OPTIMIZER.zero_grad()\n        \n        # Get Batch\n        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n        \n        # Forward Pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n       \n        # Logits Float32\n        logits = outputs.logits.to(dtype=torch.float32)\n        \n        # Backward Pass\n        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n        loss.backward()\n        \n        # optimizer step\n        OPTIMIZER.step()\n        xm.mark_step()\n        \n        # Update Learning Rate Scheduler\n        lr_scheduler.step()\n        \n        # Update Metrics And Progress Bar\n        METRICS['loss'].append(float(loss))\n        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n        \n        if (step + 1) % 200 == 0:  \n            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n                                                               METRICS['accuracy']['y_pred']))\n            lr = OPTIMIZER.param_groups[0]['lr']\n            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')\n    \n    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n\n    # If stopped, and to continue training in future on tpu we save model and optimizer\n    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    \n    \n    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')\n    get_memory_usage(f\"End of Epoch {epoch+1}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(METRICS['loss'])    \nplt.xlabel('Step per epoch')\nplt.ylabel('Loss')\nplt.title('Loss Plot step per epoch')    \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Model\n","metadata":{}},{"cell_type":"code","source":"get_memory_usage(\"Before final model save\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')\nget_memory_usage(\"After final model save\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}