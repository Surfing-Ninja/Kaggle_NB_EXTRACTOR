{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8449074,"sourceType":"datasetVersion","datasetId":5034873},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":8926343,"sourceType":"datasetVersion","datasetId":5369301},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":239643310,"sourceType":"kernelVersion"},{"sourceId":238955884,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106},{"sourceId":104623,"sourceType":"modelInstanceVersion","modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":148.347272,"end_time":"2024-07-10T01:15:35.655682","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-10T01:13:07.30841","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0f59addf0d2f40309e025976c382cad8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_268e3946106b4e41849bf11c5a375dac","placeholder":"​","style":"IPY_MODEL_5bb130c471af4927a6644f932ae47523","value":" 2/2 [00:03&lt;00:00,  1.48s/it]"}},"19ef2d43bafa44a8b20dc5230aea5ae4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ca042ffa14e4dbebdc66435f7b1f07f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ef2d43bafa44a8b20dc5230aea5ae4","placeholder":"​","style":"IPY_MODEL_d8a7714cd80d479e859b6ae31ebce7e5","value":"Loading checkpoint shards: 100%"}},"1d03719518b8423099b8b68a92e449d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64ef70cfa9c04764868fa52963323322","placeholder":"​","style":"IPY_MODEL_5e81b324ca1b46a2a96d02bb2acadc0a","value":"Loading checkpoint shards: 100%"}},"1d89705c74d34016bbc1e0601ead825c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d409334237014614bf9ae742597d98ea","placeholder":"​","style":"IPY_MODEL_875758123e4f41f0b5c3fa0cd4fb47c6","value":" 2/2 [01:18&lt;00:00, 34.68s/it]"}},"1ef6d64d40d8461d9e6adddd513089b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"268e3946106b4e41849bf11c5a375dac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"324c2396f44f45c89d9ec264007ef9ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5bb130c471af4927a6644f932ae47523":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d243712a1a545e99fa858b0cf19831d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e81b324ca1b46a2a96d02bb2acadc0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64ef70cfa9c04764868fa52963323322":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ae4b02a13f4570ad729c437ebd28ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ca042ffa14e4dbebdc66435f7b1f07f","IPY_MODEL_81b4a9a7cde64b17856b89dbd238c0ef","IPY_MODEL_0f59addf0d2f40309e025976c382cad8"],"layout":"IPY_MODEL_9422a87b93ab4473913e601da3a18689"}},"81b4a9a7cde64b17856b89dbd238c0ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ef6d64d40d8461d9e6adddd513089b8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_324c2396f44f45c89d9ec264007ef9ff","value":2}},"85d217d6b45847869cea506db59e8b42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d243712a1a545e99fa858b0cf19831d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96385fd98f304649ab5c4ae81333fb63","value":2}},"875758123e4f41f0b5c3fa0cd4fb47c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9422a87b93ab4473913e601da3a18689":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96385fd98f304649ab5c4ae81333fb63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d409334237014614bf9ae742597d98ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d576a283e6424206ab4c25d809241c21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a7714cd80d479e859b6ae31ebce7e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d98918fae8174629b4819a1114f21202":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d03719518b8423099b8b68a92e449d7","IPY_MODEL_85d217d6b45847869cea506db59e8b42","IPY_MODEL_1d89705c74d34016bbc1e0601ead825c"],"layout":"IPY_MODEL_d576a283e6424206ab4c25d809241c21"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers peft accelerate bitsandbytes \\\n    -U --no-index --find-links /kaggle/input/lmsys-wheel-files","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"papermill":{"duration":31.479497,"end_time":"2024-07-10T01:13:41.690971","exception":false,"start_time":"2024-07-10T01:13:10.211474","status":"completed"},"scrolled":true,"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:55:29.449032Z","iopub.execute_input":"2025-05-16T01:55:29.449259Z","iopub.status.idle":"2025-05-16T01:55:53.381459Z","shell.execute_reply.started":"2025-05-16T01:55:29.449232Z","shell.execute_reply":"2025-05-16T01:55:53.380627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:55:53.383787Z","iopub.execute_input":"2025-05-16T01:55:53.384058Z","iopub.status.idle":"2025-05-16T01:56:28.252524Z","shell.execute_reply.started":"2025-05-16T01:55:53.384034Z","shell.execute_reply":"2025-05-16T01:56:28.251356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport time\n\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig, Gemma2ForSequenceClassification, GemmaTokenizerFast\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:28.25394Z","iopub.execute_input":"2025-05-16T01:56:28.254205Z","iopub.status.idle":"2025-05-16T01:56:44.731821Z","shell.execute_reply.started":"2025-05-16T01:56:28.254182Z","shell.execute_reply":"2025-05-16T01:56:44.731101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"assert torch.cuda.device_count() == 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:44.732909Z","iopub.execute_input":"2025-05-16T01:56:44.733387Z","iopub.status.idle":"2025-05-16T01:56:44.764863Z","shell.execute_reply.started":"2025-05-16T01:56:44.733362Z","shell.execute_reply":"2025-05-16T01:56:44.764235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass Config:\n    llama_dir = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    weight_llama = '/kaggle/input/1-1-training-llama-lora-4-5/llama_3_finetuned_model.pth' \n    gemma_dir = '/kaggle/input/m/google/gemma-2/transformers/gemma-2-2b-it/2'\n    weight_gemma = '/kaggle/input/2-1-training-gemma-lora-4-5/output/checkpoint-20'\n    max_length = 1024\n    batch_size = 8\n    device = torch.device(\"cuda\")    \n    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n\ncfg = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:44.765814Z","iopub.execute_input":"2025-05-16T01:56:44.76612Z","iopub.status.idle":"2025-05-16T01:56:44.77143Z","shell.execute_reply.started":"2025-05-16T01:56:44.766092Z","shell.execute_reply":"2025-05-16T01:56:44.770462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:44.772374Z","iopub.execute_input":"2025-05-16T01:56:44.77257Z","iopub.status.idle":"2025-05-16T01:56:44.793716Z","shell.execute_reply.started":"2025-05-16T01:56:44.772552Z","shell.execute_reply":"2025-05-16T01:56:44.793107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:44.796889Z","iopub.execute_input":"2025-05-16T01:56:44.797098Z","iopub.status.idle":"2025-05-16T01:56:44.82262Z","shell.execute_reply.started":"2025-05-16T01:56:44.797082Z","shell.execute_reply":"2025-05-16T01:56:44.821845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\nprint(test['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:44.823784Z","iopub.execute_input":"2025-05-16T01:56:44.824297Z","iopub.status.idle":"2025-05-16T01:56:44.830625Z","shell.execute_reply.started":"2025-05-16T01:56:44.82427Z","shell.execute_reply":"2025-05-16T01:56:44.829627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n\ntokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=cfg.max_length, truncation=True, return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids'].to(cfg.device, dtype=torch.int32)\nATTENTION_MASKS = tokens['attention_mask'].to(cfg.device, dtype=torch.int32)\n\n# Move tensors to CPU and convert them to lists\ninput_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\nattention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_cpu\ndata['ATTENTION_MASKS'] = attention_masks_cpu\ndata[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:44.831744Z","iopub.execute_input":"2025-05-16T01:56:44.831969Z","iopub.status.idle":"2025-05-16T01:56:45.494956Z","shell.execute_reply.started":"2025-05-16T01:56:44.831951Z","shell.execute_reply":"2025-05-16T01:56:45.494139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BitsAndBytes configuration\nbnb_config =  BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16,\n    bnb_8bit_use_double_quant=False)\n\n# Load base model on GPU 0\ndevice0 = torch.device('cuda:0')\n\nbase_model_0 = LlamaForSequenceClassification.from_pretrained(\n    cfg.llama_dir,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:0')\nbase_model_0.config.pad_token_id = tokenizer.pad_token_id\n\n# Load base model on GPU 1\ndevice1 = torch.device('cuda:1')\nbase_model_1 = LlamaForSequenceClassification.from_pretrained(\n    cfg.llama_dir,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:1')\nbase_model_1.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:56:45.496191Z","iopub.execute_input":"2025-05-16T01:56:45.496819Z","iopub.status.idle":"2025-05-16T01:58:49.589591Z","shell.execute_reply.started":"2025-05-16T01:56:45.496788Z","shell.execute_reply":"2025-05-16T01:58:49.588962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:49.59075Z","iopub.execute_input":"2025-05-16T01:58:49.591079Z","iopub.status.idle":"2025-05-16T01:58:49.59572Z","shell.execute_reply.started":"2025-05-16T01:58:49.591051Z","shell.execute_reply":"2025-05-16T01:58:49.594866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get peft\nmodel_0 = get_peft_model(base_model_0, peft_config).to(device0) \n# Load weights\nmodel_0.load_state_dict(torch.load(cfg.weight_llama), strict=False)\nmodel_0.eval()\n\nmodel_1 = get_peft_model(base_model_1, peft_config).to(device1)\nmodel_1.load_state_dict(torch.load(cfg.weight_llama), strict=False)\nmodel_1.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:49.596604Z","iopub.execute_input":"2025-05-16T01:58:49.596859Z","iopub.status.idle":"2025-05-16T01:58:49.900033Z","shell.execute_reply.started":"2025-05-16T01:58:49.596841Z","shell.execute_reply":"2025-05-16T01:58:49.899146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trainable Parameters\nmodel_0.print_trainable_parameters(), model_1.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:49.900993Z","iopub.execute_input":"2025-05-16T01:58:49.901231Z","iopub.status.idle":"2025-05-16T01:58:49.911982Z","shell.execute_reply.started":"2025-05-16T01:58:49.901211Z","shell.execute_reply":"2025-05-16T01:58:49.911106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:49.91284Z","iopub.execute_input":"2025-05-16T01:58:49.913063Z","iopub.status.idle":"2025-05-16T01:58:50.173251Z","shell.execute_reply.started":"2025-05-16T01:58:49.913046Z","shell.execute_reply":"2025-05-16T01:58:50.172284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(df, model, device, batch_size=cfg.batch_size):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(cfg.device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(cfg.device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:50.17443Z","iopub.execute_input":"2025-05-16T01:58:50.175036Z","iopub.status.idle":"2025-05-16T01:58:50.183173Z","shell.execute_reply.started":"2025-05-16T01:58:50.175005Z","shell.execute_reply":"2025-05-16T01:58:50.182355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"st = time.time()\n\nN_SAMPLES = len(data)\n\n# Split the data into two subsets\nhalf = round(N_SAMPLES / 2)\nsub1 = data.iloc[0:half].copy()\nsub2 = data.iloc[half:N_SAMPLES].copy()\n\n# Function to run inference in a thread\ndef run_inference(df, model, device, results, index):\n    results[index] = inference(df, model, cfg.device)\n\n# Dictionary to store results from threads\nresults = {}\n\n# start threads\nt0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\nt1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n\nt0.start()\nt1.start()\n\n# Wait for all threads to finish\nt0.join()\nt1.join()\n\n# Combine results back into the original DataFrame\ndata = pd.concat([results[0], results[1]], axis=0)\n\nprint(f\"Processing complete. Total time: {time.time() - st}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:50.184128Z","iopub.execute_input":"2025-05-16T01:58:50.18434Z","iopub.status.idle":"2025-05-16T01:58:53.062077Z","shell.execute_reply.started":"2025-05-16T01:58:50.184323Z","shell.execute_reply":"2025-05-16T01:58:53.06109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n\nsample_sub[TARGETS] = data[TARGETS]\ndisplay(sample_sub)\nllama_preds = data[TARGETS].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:53.063087Z","iopub.execute_input":"2025-05-16T01:58:53.063356Z","iopub.status.idle":"2025-05-16T01:58:53.077876Z","shell.execute_reply.started":"2025-05-16T01:58:53.063334Z","shell.execute_reply":"2025-05-16T01:58:53.077003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **GEMMA**","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:53.078934Z","iopub.execute_input":"2025-05-16T01:58:53.079163Z","iopub.status.idle":"2025-05-16T01:58:53.088366Z","shell.execute_reply.started":"2025-05-16T01:58:53.079145Z","shell.execute_reply":"2025-05-16T01:58:53.087276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_text(text: str) -> str:\n    return \" \".join(eval(text, {\"null\": \"\"}))\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process_text)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process_text)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n\ndisplay(test.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:53.089651Z","iopub.execute_input":"2025-05-16T01:58:53.089972Z","iopub.status.idle":"2025-05-16T01:58:53.101688Z","shell.execute_reply.started":"2025-05-16T01:58:53.089945Z","shell.execute_reply":"2025-05-16T01:58:53.100824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize(\n    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n):\n    prompt = [\"<prompt>: \" + p for p in prompt]\n    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n    if spread_max_length:\n        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        attention_mask = [[1]* len(i) for i in input_ids]\n    else:\n        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n        input_ids = tokenized.input_ids\n        attention_mask = tokenized.attention_mask\n    return input_ids, attention_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:53.102957Z","iopub.execute_input":"2025-05-16T01:58:53.103213Z","iopub.status.idle":"2025-05-16T01:58:53.110546Z","shell.execute_reply.started":"2025-05-16T01:58:53.103193Z","shell.execute_reply":"2025-05-16T01:58:53.10967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ntokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\ntokenizer.add_eos_token = True\ntokenizer.padding_side = \"right\"\n\ndata = pd.DataFrame()\ndata[\"id\"] = test[\"id\"]\ndata[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\ndata[\"length\"] = data[\"input_ids\"].apply(len)\n\naug_data = pd.DataFrame()\naug_data[\"id\"] = test[\"id\"]\n# swap response_a & response_b\naug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\naug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:53.111643Z","iopub.execute_input":"2025-05-16T01:58:53.111917Z","iopub.status.idle":"2025-05-16T01:58:54.026644Z","shell.execute_reply.started":"2025-05-16T01:58:53.111899Z","shell.execute_reply":"2025-05-16T01:58:54.025741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(data[\"input_ids\"][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:54.031383Z","iopub.execute_input":"2025-05-16T01:58:54.031883Z","iopub.status.idle":"2025-05-16T01:58:54.036547Z","shell.execute_reply.started":"2025-05-16T01:58:54.031859Z","shell.execute_reply":"2025-05-16T01:58:54.035676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(aug_data[\"input_ids\"][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:54.037673Z","iopub.execute_input":"2025-05-16T01:58:54.038024Z","iopub.status.idle":"2025-05-16T01:58:54.04741Z","shell.execute_reply.started":"2025-05-16T01:58:54.037985Z","shell.execute_reply":"2025-05-16T01:58:54.046661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:54.048296Z","iopub.execute_input":"2025-05-16T01:58:54.048537Z","iopub.status.idle":"2025-05-16T01:58:54.056152Z","shell.execute_reply.started":"2025-05-16T01:58:54.048517Z","shell.execute_reply":"2025-05-16T01:58:54.055327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load base model on GPU 0\ndevice_0 = torch.device('cuda:0')\nmodel_0 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_0,\n    num_labels=3,\n    use_cache=False,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n)\n\n# Load base model on GPU 1\ndevice_1 = torch.device('cuda:1')\nmodel_1 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_1,\n    num_labels=3,\n    use_cache=False,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T01:58:54.057011Z","iopub.execute_input":"2025-05-16T01:58:54.05725Z","iopub.status.idle":"2025-05-16T01:59:41.599179Z","shell.execute_reply.started":"2025-05-16T01:58:54.057232Z","shell.execute_reply":"2025-05-16T01:59:41.598336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_0 = PeftModel.from_pretrained(model_0, cfg.weight_gemma, strict=False)\nmodel_1 = PeftModel.from_pretrained(model_1, cfg.weight_gemma, strict=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:09:06.578601Z","iopub.execute_input":"2025-05-16T02:09:06.579014Z","iopub.status.idle":"2025-05-16T02:09:06.773867Z","shell.execute_reply.started":"2025-05-16T02:09:06.578984Z","shell.execute_reply":"2025-05-16T02:09:06.772907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\n@torch.cuda.amp.autocast()\ndef inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n    a_win, b_win, tie = [], [], []\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        tmp = df.iloc[start_idx:end_idx]\n        input_ids = tmp[\"input_ids\"].to_list()\n        attention_mask = tmp[\"attention_mask\"].to_list()\n        inputs = pad_without_fast_tokenizer_warning(\n            tokenizer,\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n            padding=\"longest\",\n            pad_to_multiple_of=None,\n            return_tensors=\"pt\",\n        )\n        outputs = model(**inputs.to(device))\n        proba = outputs.logits.softmax(-1).cpu()\n        \n        a_win.extend(proba[:, 0].tolist())\n        b_win.extend(proba[:, 1].tolist())\n        tie.extend(proba[:, 2].tolist())\n    \n    df[\"winner_model_a\"] = a_win\n    df[\"winner_model_b\"] = b_win\n    df[\"winner_tie\"] = tie\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:09:11.589492Z","iopub.execute_input":"2025-05-16T02:09:11.590164Z","iopub.status.idle":"2025-05-16T02:09:11.597177Z","shell.execute_reply.started":"2025-05-16T02:09:11.590138Z","shell.execute_reply":"2025-05-16T02:09:11.596131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"st = time.time()\n\n# sort by input length to fully leverage dynaminc padding\ndata = data.sort_values(\"length\", ascending=False)\n# the total #tokens in sub_1 and sub_2 should be more or less the same\nsub_1 = data.iloc[0::2].copy()\nsub_2 = data.iloc[1::2].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\nresult_df = pd.concat(list(results), axis=0)\nproba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n\nprint(f\"elapsed time: {time.time() - st}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:09:16.784162Z","iopub.execute_input":"2025-05-16T02:09:16.784861Z","iopub.status.idle":"2025-05-16T02:09:17.521576Z","shell.execute_reply.started":"2025-05-16T02:09:16.784832Z","shell.execute_reply":"2025-05-16T02:09:17.520698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"st = time.time()\n\nif cfg.tta:\n    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n    sub_1 = data.iloc[0::2].copy()\n    sub_2 = data.iloc[1::2].copy()\n\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\n    tta_result_df = pd.concat(list(results), axis=0)\n    # recall TTA's order is flipped\n    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n    # average original result and TTA result.\n    proba = (proba + tta_proba) / 2\n\nprint(f\"elapsed time: {time.time() - st}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:09:20.037792Z","iopub.execute_input":"2025-05-16T02:09:20.038126Z","iopub.status.idle":"2025-05-16T02:09:20.044856Z","shell.execute_reply.started":"2025-05-16T02:09:20.038103Z","shell.execute_reply":"2025-05-16T02:09:20.043826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\nresult_df.loc[:, \"winner_model_b\"] = proba[:, 1]\nresult_df.loc[:, \"winner_tie\"] = proba[:, 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:09:22.742981Z","iopub.execute_input":"2025-05-16T02:09:22.743988Z","iopub.status.idle":"2025-05-16T02:09:22.749535Z","shell.execute_reply.started":"2025-05-16T02:09:22.743946Z","shell.execute_reply":"2025-05-16T02:09:22.748665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:09:24.965716Z","iopub.execute_input":"2025-05-16T02:09:24.966065Z","iopub.status.idle":"2025-05-16T02:09:24.982885Z","shell.execute_reply.started":"2025-05-16T02:09:24.966041Z","shell.execute_reply":"2025-05-16T02:09:24.981888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llama_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:11:44.609894Z","iopub.execute_input":"2025-05-16T02:11:44.610258Z","iopub.status.idle":"2025-05-16T02:11:44.61647Z","shell.execute_reply.started":"2025-05-16T02:11:44.610232Z","shell.execute_reply":"2025-05-16T02:11:44.615546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TAG = 'llm-classification-finetuning'\n\nimport os\nRUNPOD = os.path.exists('/workspace/')\nKAGGLE = not RUNPOD\nif KAGGLE: print('kaggle')\nDATA = '/data/' if RUNPOD else 'data/' \\\n        if not os.path.exists('/kaggle/') \\\n            else '/kaggle/input/{}/'.format(TAG)\ndf = test\ntrain = pd.read_csv(open(DATA + 'train.csv', 'r'))\nout = pd.DataFrame(llama_preds, \n                index = df.id, \n                    columns = train.columns[-3:])\ndisplay(out.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:17:12.868174Z","iopub.execute_input":"2025-05-16T02:17:12.86851Z","iopub.status.idle":"2025-05-16T02:17:16.018367Z","shell.execute_reply.started":"2025-05-16T02:17:12.868486Z","shell.execute_reply":"2025-05-16T02:17:16.016596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df = pd.merge(out, result_df, on='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:17:22.692077Z","iopub.execute_input":"2025-05-16T02:17:22.69281Z","iopub.status.idle":"2025-05-16T02:17:22.710543Z","shell.execute_reply.started":"2025-05-16T02:17:22.692783Z","shell.execute_reply":"2025-05-16T02:17:22.709598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:17:25.933516Z","iopub.execute_input":"2025-05-16T02:17:25.93423Z","iopub.status.idle":"2025-05-16T02:17:25.949552Z","shell.execute_reply.started":"2025-05-16T02:17:25.934201Z","shell.execute_reply":"2025-05-16T02:17:25.948522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df['winner_model_a']= (0.7*merged_df['winner_model_a_y']) + (0.3*merged_df['winner_model_a_x'])\nmerged_df['winner_model_b']= (0.7*merged_df['winner_model_b_y']) + (0.3*merged_df['winner_model_b_x'])\nmerged_df['winner_tie']= (0.7*merged_df['winner_tie_y']) + (0.3*merged_df['winner_tie_x'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:17:29.679339Z","iopub.execute_input":"2025-05-16T02:17:29.680157Z","iopub.status.idle":"2025-05-16T02:17:29.68716Z","shell.execute_reply.started":"2025-05-16T02:17:29.680127Z","shell.execute_reply":"2025-05-16T02:17:29.686169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = merged_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\nsubmission_df.to_csv('submission.csv', index=False)\ndisplay(submission_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T02:17:33.174515Z","iopub.execute_input":"2025-05-16T02:17:33.175371Z","iopub.status.idle":"2025-05-16T02:17:33.189162Z","shell.execute_reply.started":"2025-05-16T02:17:33.175343Z","shell.execute_reply":"2025-05-16T02:17:33.188458Z"}},"outputs":[],"execution_count":null}]}