{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":129073,"sourceType":"modelInstanceVersion","modelInstanceId":108753,"modelId":133071}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import the package","metadata":{}},{"cell_type":"code","source":"# Code Block 1: Environment Setup and Data Loading\n\nimport warnings\nwarnings.filterwarnings('ignore', message='.*overflowing tokens.*')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# (Optional) Set random seeds for reproducibility\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# Check and set device (using GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Define the local model path for the pretrained BERT model\nmodel_path = '/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased'\n\n# Load the BERT tokenizer from the local path\ntokenizer = BertTokenizer.from_pretrained(model_path)\n\n# Load training data\ntrain_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\nprint(\"Training data shape:\", train_data.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:23:10.557153Z","iopub.execute_input":"2025-02-15T09:23:10.557481Z","iopub.status.idle":"2025-02-15T09:23:31.985922Z","shell.execute_reply.started":"2025-02-15T09:23:10.557454Z","shell.execute_reply":"2025-02-15T09:23:31.985118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code Block 2: Preprocessing Training Data (Batch Tokenization)\n\ndef preprocess_dataset(df, tokenizer, max_length=512):\n    # Convert the responses to lists\n    responses_a = df['response_a'].tolist()\n    responses_b = df['response_b'].tolist()\n    \n    # Tokenize all responses in batch\n    encoding_a = tokenizer(\n        responses_a,\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    encoding_b = tokenizer(\n        responses_b,\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    return {\n        'input_ids_a': encoding_a['input_ids'],\n        'attention_mask_a': encoding_a['attention_mask'],\n        'input_ids_b': encoding_b['input_ids'],\n        'attention_mask_b': encoding_b['attention_mask']\n    }\n\nprint(\"Preprocessing training data...\")\npreprocessed_train = preprocess_dataset(train_data, tokenizer, max_length=512)\nprint(\"Tokenized training data shapes:\",\n      preprocessed_train['input_ids_a'].shape,\n      preprocessed_train['input_ids_b'].shape)\n\n# Process label data: assume the label columns are 'winner_model_a', 'winner_model_b', 'winner_tie'\nlabels = torch.tensor(train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:23:31.986924Z","iopub.execute_input":"2025-02-15T09:23:31.987198Z","iopub.status.idle":"2025-02-15T09:34:05.327254Z","shell.execute_reply.started":"2025-02-15T09:23:31.987166Z","shell.execute_reply":"2025-02-15T09:34:05.326472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code Block 3: Define Dataset and DataLoader\n\nclass ChatbotDataset(Dataset):\n    def __init__(self, preprocessed_data, labels):\n        self.input_ids_a = preprocessed_data['input_ids_a']\n        self.attention_mask_a = preprocessed_data['attention_mask_a']\n        self.input_ids_b = preprocessed_data['input_ids_b']\n        self.attention_mask_b = preprocessed_data['attention_mask_b']\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids_a': self.input_ids_a[idx],\n            'attention_mask_a': self.attention_mask_a[idx],\n            'input_ids_b': self.input_ids_b[idx],\n            'attention_mask_b': self.attention_mask_b[idx],\n            'label': self.labels[idx]\n        }\n\n# Create the training dataset and DataLoader\ntrain_dataset = ChatbotDataset(preprocessed_train, labels)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:34:05.328771Z","iopub.execute_input":"2025-02-15T09:34:05.329021Z","iopub.status.idle":"2025-02-15T09:34:05.336458Z","shell.execute_reply.started":"2025-02-15T09:34:05.329002Z","shell.execute_reply":"2025-02-15T09:34:05.335598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code Block 4: Define the Multi-Task BERT Model\n\nclass MultiTaskBERTModel(nn.Module):\n    def __init__(self, model_path, num_labels=3):\n        super(MultiTaskBERTModel, self).__init__()\n        self.bert = BertForSequenceClassification.from_pretrained(\n            model_path, \n            num_labels=num_labels, \n            ignore_mismatched_sizes=True  # In case of slight differences in model architecture\n        )\n    \n    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n        # Compute logits for response_a and response_b separately\n        output_a = self.bert(input_ids=input_ids_a, attention_mask=attention_mask_a)\n        output_b = self.bert(input_ids=input_ids_b, attention_mask=attention_mask_b)\n        return output_a.logits, output_b.logits\n\n# Initialize the model and move it to the specified device\nmodel = MultiTaskBERTModel(model_path, num_labels=3)\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:34:05.337602Z","iopub.execute_input":"2025-02-15T09:34:05.337965Z","iopub.status.idle":"2025-02-15T09:34:11.354011Z","shell.execute_reply.started":"2025-02-15T09:34:05.337944Z","shell.execute_reply":"2025-02-15T09:34:11.353248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code Block 5: Training Setup and Loop\n\n# Define optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = StepLR(optimizer, step_size=2, gamma=0.1)\nloss_fn = nn.CrossEntropyLoss()\n\n# Set up mixed precision training\nscaler = torch.cuda.amp.GradScaler()\n\nepochs = 3\nprint(\"Start training...\")\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0.0\n    \n    for batch in train_loader:\n        optimizer.zero_grad()\n        \n        # Move batch data to GPU\n        input_ids_a = batch['input_ids_a'].to(device)\n        attention_mask_a = batch['attention_mask_a'].to(device)\n        input_ids_b = batch['input_ids_b'].to(device)\n        attention_mask_b = batch['attention_mask_b'].to(device)\n        labels_batch = batch['label'].to(device)\n        \n        # Forward pass with mixed precision\n        with torch.cuda.amp.autocast():\n            logits_a, logits_b = model(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n            # Compute individual losses for each task:\n            loss_a = loss_fn(logits_a, labels_batch[:, 0])   # For winner_model_a\n            loss_b = loss_fn(logits_b, labels_batch[:, 1])   # For winner_model_b\n            # For tie prediction, use the average of logits_a and logits_b\n            loss_tie = loss_fn((logits_a + logits_b) / 2, labels_batch[:, 2])\n            loss = loss_a + loss_b + loss_tie\n        \n        # Backward pass and optimization\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n    \n    scheduler.step()\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:34:11.355084Z","iopub.execute_input":"2025-02-15T09:34:11.355415Z","execution_failed":"2025-02-15T10:14:04.334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code Block 6: Inference and Submission File Generation\n\n# Load test data (ensure test.csv contains 'id', 'response_a', 'response_b')\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nprint(\"Test data shape:\", test_data.shape)\n\ndef preprocess_test_dataset(df, tokenizer, max_length=512):\n    responses_a = df['response_a'].tolist()\n    responses_b = df['response_b'].tolist()\n    \n    encoding_a = tokenizer(\n        responses_a,\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    encoding_b = tokenizer(\n        responses_b,\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    return {\n        'input_ids_a': encoding_a['input_ids'],\n        'attention_mask_a': encoding_a['attention_mask'],\n        'input_ids_b': encoding_b['input_ids'],\n        'attention_mask_b': encoding_b['attention_mask']\n    }\n\nprint(\"Preprocessing test data...\")\npreprocessed_test = preprocess_test_dataset(test_data, tokenizer, max_length=512)\n\n# Define test Dataset and DataLoader\nclass ChatbotTestDataset(Dataset):\n    def __init__(self, preprocessed_data):\n        self.input_ids_a = preprocessed_data['input_ids_a']\n        self.attention_mask_a = preprocessed_data['attention_mask_a']\n        self.input_ids_b = preprocessed_data['input_ids_b']\n        self.attention_mask_b = preprocessed_data['attention_mask_b']\n    \n    def __len__(self):\n        return self.input_ids_a.size(0)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids_a': self.input_ids_a[idx],\n            'attention_mask_a': self.attention_mask_a[idx],\n            'input_ids_b': self.input_ids_b[idx],\n            'attention_mask_b': self.attention_mask_b[idx]\n        }\n\ntest_dataset = ChatbotTestDataset(preprocessed_test)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n# Perform inference and generate predictions\nmodel.eval()  # Set model to evaluation mode\n\npredictions_a = []\npredictions_b = []\npredictions_tie = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        # Move data to GPU\n        input_ids_a = batch['input_ids_a'].to(device)\n        attention_mask_a = batch['attention_mask_a'].to(device)\n        input_ids_b = batch['input_ids_b'].to(device)\n        attention_mask_b = batch['attention_mask_b'].to(device)\n        \n        logits_a, logits_b = model(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n        \n        # Compute softmax probabilities\n        probs_a = F.softmax(logits_a, dim=-1)  # [batch, 3]\n        probs_b = F.softmax(logits_b, dim=-1)\n        probs_tie = F.softmax((logits_a + logits_b) / 2, dim=-1)\n        \n        # Extract probability for the target class (adjust index if necessary)\n        predictions_a.append(probs_a[:, 1].detach().cpu())\n        predictions_b.append(probs_b[:, 1].detach().cpu())\n        predictions_tie.append(probs_tie[:, 1].detach().cpu())\n\n# Concatenate predictions from all batches\npredictions_a = torch.cat(predictions_a)\npredictions_b = torch.cat(predictions_b)\npredictions_tie = torch.cat(predictions_tie)\n\n# Construct the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test_data['id'],\n    'winner_model_a': predictions_a.numpy(),\n    'winner_model_b': predictions_b.numpy(),\n    'winner_tie': predictions_tie.numpy()\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as submission.csv\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-15T10:14:04.334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}