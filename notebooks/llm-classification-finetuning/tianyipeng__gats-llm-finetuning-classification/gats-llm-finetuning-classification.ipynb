{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":129073,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":108753,"modelId":133071}],"dockerImageVersionId":30887,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸŽ¯ LLM Classification Finetuning with PyTorch Lightning\n\nThis notebook implements a comprehensive solution for the LLM Classification Finetuning competition using PyTorch Lightning and BERT. The approach involves:\n\n1. **Data Processing**: Loading and preprocessing the training/test data\n2. **Model Architecture**: Using BERT with a custom classification head\n3. **Training**: Fine-tuning with PyTorch Lightning\n4. **Inference**: Generating predictions for the test set\n\n## ðŸ“Š Problem Overview\n\nWe need to classify which response (Model A, Model B, or Tie) is better for given prompts. This is a 3-class classification problem where we predict probabilities for:\n- `winner_model_a`: Probability that Model A's response is better\n- `winner_model_b`: Probability that Model B's response is better  \n- `winner_tie`: Probability that both responses are equally good\n","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“¦ Import Libraries and Setup\n\nFirst, let's import all necessary libraries and set up our environment for reproducible results.\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', message='.*overflowing tokens.*')\n\nimport os\nimport random\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nprint(\"ðŸ“š Libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"PyTorch Lightning version: {pl.__version__}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:09:49.990115Z","iopub.execute_input":"2025-09-06T01:09:49.990403Z","iopub.status.idle":"2025-09-06T01:10:10.816327Z","shell.execute_reply.started":"2025-09-06T01:09:49.990384Z","shell.execute_reply":"2025-09-06T01:10:10.815484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## âš™ï¸ Configuration and Setup\n\nDefine all hyperparameters and configuration settings. We're using Kaggle's input directory structure and downloading BERT directly from HuggingFace.\n","metadata":{}},{"cell_type":"code","source":"# Configuration\nclass Config:\n    # Paths - Kaggle format\n    BASE_DIR = Path.cwd()\n    DATA_PATH = BASE_DIR / '..' / 'input' / 'llm-classification-finetuning'\n    \n    # Model - Download directly from HuggingFace\n    MODEL_NAME = BASE_DIR / '..' / 'input/bert-base-uncased/pytorch/default/1/bert-base-uncased'\n    \n    # Training parameters\n    BATCH_SIZE = 16\n    MAX_LENGTH = 512\n    LEARNING_RATE = 5e-5\n    MAX_EPOCHS = 2\n    NUM_WORKERS = 4\n    HIDDEN_DIM = 256\n    NUM_CLASSES = 3\n    DROPOUT_RATE = 0.1\n    \n    # Other settings\n    SEED = 42\n    TEST_SIZE = 0.2\n\ndef set_seed(seed=42):\n    \"\"\"Set random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n# Initialize configuration and set seed\nconfig = Config()\nset_seed(config.SEED)\n\nprint(f\"ðŸ”§ Configuration initialized\")\nprint(f\"ðŸ“ Data path: {config.DATA_PATH}\")\nprint(f\"ðŸ¤– Model: {config.MODEL_NAME}\")\nprint(f\"ðŸŽ² Random seed: {config.SEED}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:35.528533Z","iopub.execute_input":"2025-09-06T01:23:35.528827Z","iopub.status.idle":"2025-09-06T01:23:35.53732Z","shell.execute_reply.started":"2025-09-06T01:23:35.528805Z","shell.execute_reply":"2025-09-06T01:23:35.53659Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“Š Data Loading and Exploration\n\nLoad the training data and explore its structure to understand what we're working with.\n","metadata":{}},{"cell_type":"code","source":"def load_data(config):\n    \"\"\"Load and split the training data\"\"\"\n    print(\"ðŸ“Š Loading training data...\")\n    \n    # Load training data\n    train_file_path = config.DATA_PATH / 'train.csv'\n    if not train_file_path.exists():\n        raise FileNotFoundError(f\"Training data not found at {train_file_path}\")\n    \n    train_data = pd.read_csv(train_file_path)\n    print(f\"Training data shape: {train_data.shape}\")\n    print(f\"Training data columns: {train_data.columns.tolist()}\")\n    \n    # Split the data into train and validation sets\n    train_data_split, validation_data = train_test_split(\n        train_data, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n    )\n    \n    print(f\"\\nðŸ“ˆ Data split completed:\")\n    print(f\"Training samples: {len(train_data_split)}\")\n    print(f\"Validation samples: {len(validation_data)}\")\n    \n    # Verify label distribution\n    print(\"\\nðŸ·ï¸ Training set label distribution:\")\n    print(f\"Model A wins: {train_data_split['winner_model_a'].sum()}\")\n    print(f\"Model B wins: {train_data_split['winner_model_b'].sum()}\")\n    print(f\"Ties: {train_data_split['winner_tie'].sum()}\")\n    \n    print(\"\\nðŸ·ï¸ Validation set label distribution:\")\n    print(f\"Model A wins: {validation_data['winner_model_a'].sum()}\")\n    print(f\"Model B wins: {validation_data['winner_model_b'].sum()}\")\n    print(f\"Ties: {validation_data['winner_tie'].sum()}\")\n    \n    return train_data_split, validation_data\n\n# Load and explore the data\ntrain_df, val_df = load_data(config)\n\n# Display sample data\nprint(\"\\nðŸ“‹ Sample training data:\")\ntrain_df.head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:38.855813Z","iopub.execute_input":"2025-09-06T01:23:38.856159Z","iopub.status.idle":"2025-09-06T01:23:40.892148Z","shell.execute_reply.started":"2025-09-06T01:23:38.856131Z","shell.execute_reply":"2025-09-06T01:23:40.891255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ”¤ Tokenizer Setup\n\nInitialize the BERT tokenizer that will be used to convert text into tokens that the model can understand.\n","metadata":{}},{"cell_type":"code","source":"# Initialize tokenizer - Download directly from HuggingFace\nprint(f\"ðŸ”¤ Loading tokenizer: {config.MODEL_NAME}\")\ntokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n\nprint(f\"âœ… Tokenizer loaded successfully!\")\nprint(f\"Vocabulary size: {len(tokenizer)}\")\nprint(f\"Max length: {config.MAX_LENGTH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:40.893506Z","iopub.execute_input":"2025-09-06T01:23:40.893831Z","iopub.status.idle":"2025-09-06T01:23:40.945075Z","shell.execute_reply.started":"2025-09-06T01:23:40.893799Z","shell.execute_reply":"2025-09-06T01:23:40.944344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“š Dataset Classes\n\nDefine custom dataset classes for training and testing data. These classes handle:\n- Text preprocessing and formatting\n- Label conversion (from one-hot to single class)\n- Data structure for efficient batch processing\n","metadata":{}},{"cell_type":"code","source":"class LLMClassificationDataset(Dataset):\n    \"\"\"Dataset for LLM Classification with on-the-fly tokenization\"\"\"\n    \n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Prepare the three text inputs\n        prompt = row['prompt']\n        model_a_response = row['response_a']\n        model_b_response = row['response_b']\n        \n        # Convert labels to single class (0: model_a wins, 1: model_b wins, 2: tie)\n        if row['winner_model_a'] == 1:\n            label = 0\n        elif row['winner_model_b'] == 1:\n            label = 1\n        else:  # winner_tie == 1\n            label = 2\n            \n        return {\n            'prompt': prompt,\n            'model_a_response': model_a_response,\n            'model_b_response': model_b_response,\n            'label': label\n        }\n\nclass TestDataset(Dataset):\n    \"\"\"Test dataset for inference\"\"\"\n    \n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Prepare the three text inputs\n        prompt = row['prompt']\n        model_a_response = row['response_a']\n        model_b_response = row['response_b']\n            \n        return {\n            'prompt': prompt,\n            'model_a_response': model_a_response,\n            'model_b_response': model_b_response\n        }\n\nprint(\"ðŸ“š Dataset classes defined successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:41.167007Z","iopub.execute_input":"2025-09-06T01:23:41.167299Z","iopub.status.idle":"2025-09-06T01:23:41.174997Z","shell.execute_reply.started":"2025-09-06T01:23:41.167277Z","shell.execute_reply":"2025-09-06T01:23:41.174269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ”„ Data Collation Functions\n\nDefine collate functions that handle batch tokenization. These functions:\n- Tokenize text inputs in batches for efficiency\n- Handle padding and truncation\n- Prepare tensors for model input\n","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch, tokenizer, max_length=512):\n    \"\"\"Custom collate function for batch tokenization\"\"\"\n    prompts = [item['prompt'] for item in batch]\n    model_a_responses = [item['model_a_response'] for item in batch]\n    model_b_responses = [item['model_b_response'] for item in batch]\n    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n    \n    # Tokenize each input type\n    prompt_encoding = tokenizer(\n        prompts, padding=True, truncation=True, \n        max_length=max_length, return_tensors='pt'\n    )\n    \n    model_a_encoding = tokenizer(\n        model_a_responses, padding=True, truncation=True,\n        max_length=max_length, return_tensors='pt'\n    )\n    \n    model_b_encoding = tokenizer(\n        model_b_responses, padding=True, truncation=True,\n        max_length=max_length, return_tensors='pt'\n    )\n    \n    return {\n        'prompt_input_ids': prompt_encoding['input_ids'],\n        'prompt_attention_mask': prompt_encoding['attention_mask'],\n        'model_a_input_ids': model_a_encoding['input_ids'],\n        'model_a_attention_mask': model_a_encoding['attention_mask'],\n        'model_b_input_ids': model_b_encoding['input_ids'],\n        'model_b_attention_mask': model_b_encoding['attention_mask'],\n        'labels': labels\n    }\n\ndef test_collate_fn(batch, tokenizer, max_length=512):\n    \"\"\"Custom collate function for test data\"\"\"\n    prompts = [item['prompt'] for item in batch]\n    model_a_responses = [item['model_a_response'] for item in batch]\n    model_b_responses = [item['model_b_response'] for item in batch]\n    \n    # Tokenize each input type\n    prompt_encoding = tokenizer(\n        prompts, padding=True, truncation=True, \n        max_length=max_length, return_tensors='pt'\n    )\n    \n    model_a_encoding = tokenizer(\n        model_a_responses, padding=True, truncation=True,\n        max_length=max_length, return_tensors='pt'\n    )\n    \n    model_b_encoding = tokenizer(\n        model_b_responses, padding=True, truncation=True,\n        max_length=max_length, return_tensors='pt'\n    )\n    \n    return {\n        'prompt_input_ids': prompt_encoding['input_ids'],\n        'prompt_attention_mask': prompt_encoding['attention_mask'],\n        'model_a_input_ids': model_a_encoding['input_ids'],\n        'model_a_attention_mask': model_a_encoding['attention_mask'],\n        'model_b_input_ids': model_b_encoding['input_ids'],\n        'model_b_attention_mask': model_b_encoding['attention_mask']\n    }\n\nprint(\"ðŸ”„ Collate functions defined successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:41.479132Z","iopub.execute_input":"2025-09-06T01:23:41.479379Z","iopub.status.idle":"2025-09-06T01:23:41.487356Z","shell.execute_reply.started":"2025-09-06T01:23:41.479357Z","shell.execute_reply":"2025-09-06T01:23:41.486601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ§  Model Architecture\n\nDefine the main model architecture using PyTorch Lightning. The model:\n- Uses pre-trained BERT as the backbone\n- Freezes BERT parameters\n- Processes three text inputs: prompt, model_a_response, model_b_response\n- Concatenates BERT embeddings and passes through a classification head\n- Outputs probabilities for 3 classes\n","metadata":{}},{"cell_type":"code","source":"class LLMClassificationModel(pl.LightningModule):\n    \"\"\"PyTorch Lightning model for LLM Classification\"\"\"\n    \n    def __init__(self, model_name='bert-base-uncased', learning_rate=5e-5, \n                 hidden_dim=256, num_classes=3, dropout_rate=0.1):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Load BERT model - Download directly from HuggingFace\n        print(f\"ðŸ¤– Loading BERT model: {model_name}\")\n        self.bert = BertModel.from_pretrained(model_name)\n        \n        # Freeze most BERT parameters\n        for param in self.bert.parameters():\n            param.requires_grad = False\n            \n        # Classification head\n        bert_dim = self.bert.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Linear(bert_dim * 3, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_dim // 2, num_classes)\n        )\n        \n        # Loss function\n        self.loss_fn = nn.CrossEntropyLoss()\n        \n    def forward(self, prompt_input_ids, prompt_attention_mask,\n                model_a_input_ids, model_a_attention_mask,\n                model_b_input_ids, model_b_attention_mask):\n        \n        # Get embeddings for each input\n        prompt_outputs = self.bert(input_ids=prompt_input_ids, attention_mask=prompt_attention_mask)\n        prompt_embedding = prompt_outputs.last_hidden_state[:, 0, :]  # CLS token\n        \n        model_a_outputs = self.bert(input_ids=model_a_input_ids, attention_mask=model_a_attention_mask)\n        model_a_embedding = model_a_outputs.last_hidden_state[:, 0, :]  # CLS token\n        \n        model_b_outputs = self.bert(input_ids=model_b_input_ids, attention_mask=model_b_attention_mask)\n        model_b_embedding = model_b_outputs.last_hidden_state[:, 0, :]  # CLS token\n        \n        # Concatenate embeddings\n        concatenated = torch.cat([prompt_embedding, model_a_embedding, model_b_embedding], dim=1)\n        \n        # Classification\n        logits = self.classifier(concatenated)\n        return logits\n    \n    def training_step(self, batch, batch_idx):\n        logits = self.forward(\n            batch['prompt_input_ids'], batch['prompt_attention_mask'],\n            batch['model_a_input_ids'], batch['model_a_attention_mask'],\n            batch['model_b_input_ids'], batch['model_b_attention_mask']\n        )\n        \n        loss = self.loss_fn(logits, batch['labels'])\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == batch['labels']).float().mean()\n        \n        # Logging\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        logits = self.forward(\n            batch['prompt_input_ids'], batch['prompt_attention_mask'],\n            batch['model_a_input_ids'], batch['model_a_attention_mask'],\n            batch['model_b_input_ids'], batch['model_b_attention_mask']\n        )\n        \n        loss = self.loss_fn(logits, batch['labels'])\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == batch['labels']).float().mean()\n        \n        # Logging\n        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n        \n        return {'val_loss': loss, 'preds': preds, 'labels': batch['labels']}\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n        return [optimizer], [scheduler]\n\nprint(\"ðŸ§  Model architecture defined successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:48.623858Z","iopub.execute_input":"2025-09-06T01:23:48.6242Z","iopub.status.idle":"2025-09-06T01:23:48.636434Z","shell.execute_reply.started":"2025-09-06T01:23:48.624169Z","shell.execute_reply":"2025-09-06T01:23:48.635514Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“Š Data Module\n\nPyTorch Lightning DataModule that handles:\n- Dataset initialization\n- DataLoader creation with proper batch processing\n- Train/validation data splitting\n","metadata":{}},{"cell_type":"code","source":"class LLMDataModule(pl.LightningDataModule):\n    \"\"\"PyTorch Lightning DataModule for LLM Classification\"\"\"\n    \n    def __init__(self, train_df, val_df, tokenizer, batch_size=16, max_length=512, num_workers=4):\n        super().__init__()\n        self.train_df = train_df\n        self.val_df = val_df\n        self.tokenizer = tokenizer\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.num_workers = num_workers\n        \n    def setup(self, stage=None):\n        if stage == 'fit' or stage is None:\n            self.train_dataset = LLMClassificationDataset(self.train_df, self.tokenizer, self.max_length)\n            self.val_dataset = LLMClassificationDataset(self.val_df, self.tokenizer, self.max_length)\n    \n    def _collate_fn(self, batch):\n        \"\"\"Wrapper for collate_fn that can be pickled\"\"\"\n        return collate_fn(batch, self.tokenizer, self.max_length)\n    \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=4,  \n            collate_fn=self._collate_fn,\n            pin_memory=False\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=4,  \n            collate_fn=self._collate_fn,\n            pin_memory=False\n        )\n\nprint(\"ðŸ“Š Data module defined successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:54.662592Z","iopub.execute_input":"2025-09-06T01:23:54.662913Z","iopub.status.idle":"2025-09-06T01:23:54.669992Z","shell.execute_reply.started":"2025-09-06T01:23:54.662886Z","shell.execute_reply":"2025-09-06T01:23:54.669015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸš€ Model Training\n\nNow let's train our model using PyTorch Lightning. The training includes:\n- Model checkpointing (saves best models)\n- Early stopping (prevents overfitting)\n- Mixed precision training (for efficiency)\n- Validation once per epoch (as requested)\n","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:23:56.987662Z","iopub.execute_input":"2025-09-06T01:23:56.987951Z","iopub.status.idle":"2025-09-06T01:23:56.998375Z","shell.execute_reply.started":"2025-09-06T01:23:56.987927Z","shell.execute_reply":"2025-09-06T01:23:56.997594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize data module\ndata_module = LLMDataModule(\n    train_df=train_df,\n    val_df=val_df,\n    tokenizer=tokenizer,\n    batch_size=config.BATCH_SIZE,\n    max_length=config.MAX_LENGTH,\n    num_workers=config.NUM_WORKERS\n)\n\n# Initialize model\nmodel = LLMClassificationModel(\n    model_name=config.MODEL_NAME,\n    learning_rate=config.LEARNING_RATE,\n    hidden_dim=config.HIDDEN_DIM,\n    num_classes=config.NUM_CLASSES,\n    dropout_rate=config.DROPOUT_RATE\n)\n\n# Set up callbacks\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc',\n    dirpath='./checkpoints',\n    filename='llm-classification-{epoch:02d}-{val_acc:.2f}',\n    save_top_k=3,\n    mode='max'\n)\n\n# Set up logger\nlogger = TensorBoardLogger('tb_logs', name='llm_classification')\n\n# Initialize trainer with mixed precision - VALIDATION ONCE PER EPOCH\ntrainer = pl.Trainer(\n    max_epochs=config.MAX_EPOCHS,\n    precision='16-mixed',  # Enable mixed precision training\n    accelerator='auto',    # Automatically detect GPU/CPU\n    devices='auto',        # Use all available devices\n    callbacks=[checkpoint_callback],\n    logger=logger,\n    log_every_n_steps=50,\n    val_check_interval=1.0,  # Validate once per epoch\n    gradient_clip_val=1.0,   # Gradient clipping\n    accumulate_grad_batches=1,\n    deterministic=True\n)\n\nprint(\"ðŸš€ Starting training with PyTorch Lightning...\")\nprint(f\"Using device: {trainer.strategy.root_device}\")\nprint(f\"Mixed precision: {'Enabled' if trainer.precision == '16-mixed' else 'Disabled'}\")\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"Validation frequency: Once per epoch\")\n\n# Train the model\ntrainer.fit(model, data_module)\n\nprint(\"âœ… Training completed!\")\nprint(f\"Best model saved at: {checkpoint_callback.best_model_path}\")\nprint(f\"Best validation accuracy: {checkpoint_callback.best_model_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T01:24:11.23645Z","iopub.execute_input":"2025-09-06T01:24:11.236739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ” Test Data Loading\n\nLoad the test data for inference and prepare it for prediction.\n","metadata":{}},{"cell_type":"code","source":"def load_test_data(config):\n    \"\"\"Load test data for inference\"\"\"\n    test_file_path = config.DATA_PATH / 'test.csv'\n    if not test_file_path.exists():\n        raise FileNotFoundError(f\"Test data not found at {test_file_path}\")\n    \n    test_data = pd.read_csv(test_file_path)\n    print(f\"Test data shape: {test_data.shape}\")\n    print(f\"Test data columns: {test_data.columns.tolist()}\")\n    \n    return test_data\n\n# Load test data\nprint(\"ðŸ” Loading test data...\")\ntest_data = load_test_data(config)\n\nprint(\"\\nðŸ“‹ Sample test data:\")\ntest_data.head(2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T02:41:30.880536Z","iopub.execute_input":"2025-09-06T02:41:30.880946Z","iopub.status.idle":"2025-09-06T02:41:30.897184Z","shell.execute_reply.started":"2025-09-06T02:41:30.880909Z","shell.execute_reply":"2025-09-06T02:41:30.896255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸŽ¯ Inference and Submission\n\nGenerate predictions on the test set using our trained model and create the submission file. The output will be saved as `submission.csv` with probabilities for each class.\n","metadata":{}},{"cell_type":"code","source":"def run_inference(test_data, checkpoint_path, tokenizer, config):\n    \"\"\"Run inference on test data\"\"\"\n    print(\"ðŸ” Starting inference...\")\n    \n    # Load the best model checkpoint\n    best_model = LLMClassificationModel.load_from_checkpoint(checkpoint_path)\n    best_model.eval()\n    best_model.freeze()\n    \n    # Create test dataset and dataloader\n    test_dataset = TestDataset(test_data, tokenizer, max_length=config.MAX_LENGTH)\n    \n    def _test_collate_fn(batch):\n        \"\"\"Wrapper for test_collate_fn that can be pickled\"\"\"\n        return test_collate_fn(batch, tokenizer, max_length=config.MAX_LENGTH)\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=32,\n        shuffle=False,\n        num_workers=4, \n        collate_fn=_test_collate_fn,\n        pin_memory=False\n    )\n    \n    # Perform inference\n    print(\"ðŸ” Running inference on test data...\")\n    all_predictions = []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(test_loader):\n            # Move batch to device\n            batch = {k: v.to(best_model.device) for k, v in batch.items()}\n            \n            # Get logits\n            logits = best_model(\n                batch['prompt_input_ids'], batch['prompt_attention_mask'],\n                batch['model_a_input_ids'], batch['model_a_attention_mask'],\n                batch['model_b_input_ids'], batch['model_b_attention_mask']\n            )\n            \n            # Apply softmax to get probabilities\n            probabilities = F.softmax(logits, dim=-1)\n            all_predictions.append(probabilities.cpu())\n            \n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Processed {batch_idx + 1}/{len(test_loader)} batches\")\n    \n    # Concatenate all predictions\n    all_predictions = torch.cat(all_predictions, dim=0)\n    \n    # Extract probabilities for each class\n    winner_model_a_probs = all_predictions[:, 0].numpy()  # Class 0: model_a wins\n    winner_model_b_probs = all_predictions[:, 1].numpy()  # Class 1: model_b wins\n    winner_tie_probs = all_predictions[:, 2].numpy()      # Class 2: tie\n    \n    # Create submission dataframe\n    submission = pd.DataFrame({\n        'id': test_data['id'],\n        'winner_model_a': winner_model_a_probs,\n        'winner_model_b': winner_model_b_probs,\n        'winner_tie': winner_tie_probs\n    })\n    \n    # Verify probabilities sum to 1\n    prob_sums = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n    print(f\"Probability sums - Min: {prob_sums.min():.4f}, Max: {prob_sums.max():.4f}, Mean: {prob_sums.mean():.4f}\")\n    \n    print(\"ðŸ“Š Submission preview:\")\n    print(submission.head())\n    \n    # Save submission as requested filename\n    submission.to_csv('submission.csv', index=False)\n    print(\"âœ… Submission file saved as 'submission.csv'\")\n    \n    return submission\n\n# Run inference and create submission\nsubmission = run_inference(test_data, checkpoint_callback.best_model_path, tokenizer, config)\n\nprint(\"\\nðŸŽ‰ Pipeline completed successfully!\")\nprint(f\"ðŸ“ Best model: {checkpoint_callback.best_model_path}\")\nprint(f\"ðŸ“„ Submission: submission.csv\")\nprint(f\"ðŸŽ¯ Final validation accuracy: {checkpoint_callback.best_model_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T02:43:45.800884Z","iopub.execute_input":"2025-09-06T02:43:45.801204Z","iopub.status.idle":"2025-09-06T02:43:47.371893Z","shell.execute_reply.started":"2025-09-06T02:43:45.801178Z","shell.execute_reply":"2025-09-06T02:43:47.37097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“ˆ Results Summary\n\nLet's review our final results and submission statistics.\n","metadata":{}},{"cell_type":"code","source":"# Display submission statistics\nprint(\"ðŸ“ˆ Final Submission Statistics:\")\nprint(f\"Total test samples: {len(submission)}\")\nprint(\"\\nðŸ“Š Probability distributions:\")\nprint(f\"Model A wins - Mean: {submission['winner_model_a'].mean():.4f}, Std: {submission['winner_model_a'].std():.4f}\")\nprint(f\"Model B wins - Mean: {submission['winner_model_b'].mean():.4f}, Std: {submission['winner_model_b'].std():.4f}\")\nprint(f\"Ties - Mean: {submission['winner_tie'].mean():.4f}, Std: {submission['winner_tie'].std():.4f}\")\n\n# Check for any potential issues\nprint(\"\\nðŸ” Data validation:\")\nprint(f\"Any NaN values: {submission.isnull().any().any()}\")\nprint(f\"All probabilities >= 0: {(submission[['winner_model_a', 'winner_model_b', 'winner_tie']] >= 0).all().all()}\")\nprint(f\"All probabilities <= 1: {(submission[['winner_model_a', 'winner_model_b', 'winner_tie']] <= 1).all().all()}\")\n\nprint(\"\\nâœ… Submission file 'submission.csv' is ready for upload!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T02:43:53.86259Z","iopub.execute_input":"2025-09-06T02:43:53.862977Z","iopub.status.idle":"2025-09-06T02:43:53.874802Z","shell.execute_reply.started":"2025-09-06T02:43:53.862945Z","shell.execute_reply":"2025-09-06T02:43:53.873932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}