{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T20:36:06.87897Z","iopub.execute_input":"2024-10-31T20:36:06.879431Z","iopub.status.idle":"2024-10-31T20:36:08.099483Z","shell.execute_reply.started":"2024-10-31T20:36:06.879378Z","shell.execute_reply":"2024-10-31T20:36:08.098063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:08.102009Z","iopub.execute_input":"2024-10-31T20:36:08.103008Z","iopub.status.idle":"2024-10-31T20:36:10.559099Z","shell.execute_reply.started":"2024-10-31T20:36:08.102922Z","shell.execute_reply":"2024-10-31T20:36:10.557846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/test.csv')\nsample_submission = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\nprint('train data shape :', train.shape)\nprint('test data shape :', test.shape)\nprint('sample_submission data shape :', sample_submission.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:10.56115Z","iopub.execute_input":"2024-10-31T20:36:10.561726Z","iopub.status.idle":"2024-10-31T20:36:14.509151Z","shell.execute_reply.started":"2024-10-31T20:36:10.56168Z","shell.execute_reply":"2024-10-31T20:36:14.507663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:14.512536Z","iopub.execute_input":"2024-10-31T20:36:14.513759Z","iopub.status.idle":"2024-10-31T20:36:14.544882Z","shell.execute_reply.started":"2024-10-31T20:36:14.513691Z","shell.execute_reply":"2024-10-31T20:36:14.543447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get value counts in descending order\nvalue_counts = train['model_a'].value_counts(ascending=False)\n\n# Plotting\nplt.figure(figsize=(15, 6))\nsns.barplot(x=value_counts.index, y=value_counts.values)\nplt.xlabel('Category')\nplt.ylabel('Counts')\nplt.xticks(rotation=90)\nplt.title('Value Counts in Descending Order')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:14.550135Z","iopub.execute_input":"2024-10-31T20:36:14.551055Z","iopub.status.idle":"2024-10-31T20:36:15.785135Z","shell.execute_reply.started":"2024-10-31T20:36:14.550991Z","shell.execute_reply":"2024-10-31T20:36:15.783567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['model_b'].value_counts())\nplt.figure(figsize=(15, 6))\nsns.countplot(x='model_b', data=train)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:15.786584Z","iopub.execute_input":"2024-10-31T20:36:15.787036Z","iopub.status.idle":"2024-10-31T20:36:16.892537Z","shell.execute_reply.started":"2024-10-31T20:36:15.786989Z","shell.execute_reply":"2024-10-31T20:36:16.891309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop_duplicates()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:16.894017Z","iopub.execute_input":"2024-10-31T20:36:16.894423Z","iopub.status.idle":"2024-10-31T20:36:17.407899Z","shell.execute_reply.started":"2024-10-31T20:36:16.894384Z","shell.execute_reply":"2024-10-31T20:36:17.406717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:17.409376Z","iopub.execute_input":"2024-10-31T20:36:17.409772Z","iopub.status.idle":"2024-10-31T20:36:17.464589Z","shell.execute_reply.started":"2024-10-31T20:36:17.409731Z","shell.execute_reply":"2024-10-31T20:36:17.463145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the data:\ntrain.drop('id', axis=1).hist(figsize=(10,5),color = 'skyblue', edgecolor='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:17.466371Z","iopub.execute_input":"2024-10-31T20:36:17.466929Z","iopub.status.idle":"2024-10-31T20:36:18.327711Z","shell.execute_reply.started":"2024-10-31T20:36:17.46685Z","shell.execute_reply":"2024-10-31T20:36:18.326356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:18.33287Z","iopub.execute_input":"2024-10-31T20:36:18.333331Z","iopub.status.idle":"2024-10-31T20:36:18.347291Z","shell.execute_reply.started":"2024-10-31T20:36:18.333288Z","shell.execute_reply":"2024-10-31T20:36:18.345969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:18.349087Z","iopub.execute_input":"2024-10-31T20:36:18.349499Z","iopub.status.idle":"2024-10-31T20:36:18.366937Z","shell.execute_reply.started":"2024-10-31T20:36:18.349449Z","shell.execute_reply":"2024-10-31T20:36:18.365671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the data:\nsample_submission.drop('id', axis=1).hist(figsize=(10,5),color = 'skyblue', edgecolor='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:18.368766Z","iopub.execute_input":"2024-10-31T20:36:18.369319Z","iopub.status.idle":"2024-10-31T20:36:19.11868Z","shell.execute_reply.started":"2024-10-31T20:36:18.369261Z","shell.execute_reply":"2024-10-31T20:36:19.117315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack  # to concatenate sparse matrices\n\n# Initialize TF-IDF Vectorizers for each text column\ntfidf_prompt = TfidfVectorizer(max_features=500)   # Adjust max_features as needed\ntfidf_response_a = TfidfVectorizer(max_features=500)\ntfidf_response_b = TfidfVectorizer(max_features=500)\n\n# Fit and transform each text column\nX_prompt_tfidf = tfidf_prompt.fit_transform(train['prompt'])\nX_response_a_tfidf = tfidf_response_a.fit_transform(train['response_a'])\nX_response_b_tfidf = tfidf_response_b.fit_transform(train['response_b'])\n\n# Combine TF-IDF matrices with other features\nX_tfidf = hstack([X_prompt_tfidf, X_response_a_tfidf, X_response_b_tfidf])\n\nX = X_tfidf ","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:19.120332Z","iopub.execute_input":"2024-10-31T20:36:19.120813Z","iopub.status.idle":"2024-10-31T20:36:56.182431Z","shell.execute_reply.started":"2024-10-31T20:36:19.120771Z","shell.execute_reply":"2024-10-31T20:36:56.18111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and transform each text column\ntest_prompt_tfidf = tfidf_prompt.transform(test['prompt'])\ntest_response_a_tfidf = tfidf_response_a.transform(test['response_a'])\ntest_response_b_tfidf = tfidf_response_b.transform(test['response_b'])\n\n# Combine TF-IDF matrices with other features\ntest_tfidf = hstack([test_prompt_tfidf, test_response_a_tfidf, test_response_b_tfidf])\n\ntest = test_tfidf","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:56.18408Z","iopub.execute_input":"2024-10-31T20:36:56.184475Z","iopub.status.idle":"2024-10-31T20:36:56.197994Z","shell.execute_reply.started":"2024-10-31T20:36:56.184434Z","shell.execute_reply":"2024-10-31T20:36:56.196752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = X_tfidf\ny = train[['winner_model_a','winner_model_b','winner_tie']]\ntest = test_tfidf","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:56.199431Z","iopub.execute_input":"2024-10-31T20:36:56.199826Z","iopub.status.idle":"2024-10-31T20:36:56.214084Z","shell.execute_reply.started":"2024-10-31T20:36:56.199779Z","shell.execute_reply":"2024-10-31T20:36:56.212675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'n_estimators': 229, 'max_depth': 9, 'learning_rate': 0.028184526290102357, 'subsample': 0.6607687169383815, 'colsample_bytree': 0.642663510005148}","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:56.215474Z","iopub.execute_input":"2024-10-31T20:36:56.21586Z","iopub.status.idle":"2024-10-31T20:36:56.230846Z","shell.execute_reply.started":"2024-10-31T20:36:56.215821Z","shell.execute_reply":"2024-10-31T20:36:56.229761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the MultiOutputClassifier with XGBClassifier\nmulti_target_model = MultiOutputClassifier(XGBClassifier(**params))\n\n# Define K-Fold cross-validation\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Arrays to store predictions and log loss for each fold\nfold_probs = []\nfold_log_losses = []\npreds = []\n\n# Perform K-Fold cross-validation\nfor fold, (train_index, val_index) in enumerate(kf.split(X)):\n    # Split the data into train and validation sets for this fold\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n    # Fit the model on the training data of this fold\n    multi_target_model.fit(X_train, y_train)\n\n    # Get probability predictions on the validation set\n    y_val_proba = [estimator.predict_proba(X_val)[:, 1] for estimator in multi_target_model.estimators_]\n    y_val_proba = np.column_stack(y_val_proba)  # Stack to (n_samples, n_targets)\n    pred = [estimator.predict_proba(test)[:, 1] for estimator in multi_target_model.estimators_]\n    preds.append(pred)\n\n    # Store predictions\n    fold_probs.append(y_val_proba)\n\n    # Calculate log loss for each target and store the results\n    log_losses = [log_loss(y_val.iloc[:, i], y_val_proba[:, i]) for i in range(y_val.shape[1])]\n    mean_log_loss = np.mean(log_losses)\n    fold_log_losses.append(mean_log_loss)\n    \n    print(f\"Fold {fold + 1} Mean Log Loss: {mean_log_loss}\")\n\n# Calculate the average log loss across all folds\navg_log_loss = np.mean(fold_log_losses)\nprint(f\"\\nAverage Log Loss across all folds: {avg_log_loss}\")\n\n# Optional: Convert fold probabilities into a DataFrame\nall_probs = np.vstack(fold_probs)  # Stack probabilities from all folds if needed\nprobs_df = pd.DataFrame(all_probs, columns=[f\"{col}_proba\" for col in y.columns])\nprint(\"\\nProbability predictions for each target:\\n\", probs_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:36:56.232254Z","iopub.execute_input":"2024-10-31T20:36:56.232673Z","iopub.status.idle":"2024-10-31T21:14:26.809531Z","shell.execute_reply.started":"2024-10-31T20:36:56.232619Z","shell.execute_reply":"2024-10-31T21:14:26.808216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'winner_model_a': np.mean(preds[0],axis=0),\n    'winner_model_b': np.mean(preds[1],axis=0),\n    'winner_tie': np.mean(preds[2],axis=0)\n})","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:14:26.811308Z","iopub.execute_input":"2024-10-31T21:14:26.811808Z","iopub.status.idle":"2024-10-31T21:14:26.8219Z","shell.execute_reply.started":"2024-10-31T21:14:26.811744Z","shell.execute_reply":"2024-10-31T21:14:26.820672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': sample_submission.id, 'winner_model_a' : np.mean(preds[0],axis=0), 'winner_model_b' : np.mean(preds[1],axis=0), 'winner_tie' : np.mean(preds[2],axis=0) })\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:14:26.823488Z","iopub.execute_input":"2024-10-31T21:14:26.823917Z","iopub.status.idle":"2024-10-31T21:14:26.856107Z","shell.execute_reply.started":"2024-10-31T21:14:26.823853Z","shell.execute_reply":"2024-10-31T21:14:26.854541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from sklearn.multioutput import MultiOutputClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize MultiOutputClassifier with XGBClassifier\nmulti_target_model = MultiOutputClassifier(XGBClassifier(**params))\n\n# Train the model\nmulti_target_model.fit(X_train, y_train)\n\n# Get probability predictions\ny_proba = [estimator.predict_proba(X_test)[:, 1] for estimator in multi_target_model.estimators_]\n\n# Display probabilities for each target\nfor i, col in enumerate(y.columns):\n    print(f\"Probability predictions for {col}:\\n {y_proba[i]}\")\n    \n#pred = multi_target_model.predict_proba(test)\npred = [estimator.predict_proba(test)[:, 1] for estimator in multi_target_model.estimators_]\npred","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:14:26.857385Z","iopub.execute_input":"2024-10-31T21:14:26.857794Z","iopub.status.idle":"2024-10-31T21:21:50.648633Z","shell.execute_reply.started":"2024-10-31T21:14:26.857753Z","shell.execute_reply":"2024-10-31T21:21:50.6477Z"}}},{"cell_type":"markdown","source":"submission = pd.DataFrame({\n    'winner_model_a': pred[0].flatten(),\n    'winner_model_b': pred[1].flatten(),\n    'winner_tie': pred[2].flatten()\n})\n\nsubmission = pd.DataFrame({'id': sample_submission.id, 'winner_model_a' : pred[0].flatten(), 'winner_model_b' : pred[1].flatten(), 'winner_tie' : pred[2].flatten() })\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:21:50.650381Z","iopub.execute_input":"2024-10-31T21:21:50.65113Z","iopub.status.idle":"2024-10-31T21:21:50.658135Z","shell.execute_reply.started":"2024-10-31T21:21:50.651083Z","shell.execute_reply":"2024-10-31T21:21:50.656577Z"}}},{"cell_type":"markdown","source":"# Separate each target column individually\ny_model_a = train['winner_model_a']\ny_model_b = train['winner_model_b']\ny_tie = train['winner_tie']\n\n# Split into training and testing sets for each target\nX_train, X_test, y_a_train, y_a_test = train_test_split(X, y_model_a, test_size=0.2, random_state=42)\n_, _, y_b_train, y_b_test = train_test_split(X, y_model_b, test_size=0.2, random_state=42)\n_, _, y_tie_train, y_tie_test = train_test_split(X, y_tie, test_size=0.2, random_state=42)\n\n\n# Initialize classifiers\nmodel_a = xgb.XGBClassifier(**params)\nmodel_b = xgb.XGBClassifier(**params)\nmodel_tie = xgb.XGBClassifier(**params)\n\n# Train each model on the same X but different targets\nmodel_a.fit(X, y_model_a)\nmodel_b.fit(X, y_model_b)\nmodel_tie.fit(X, y_tie)\n\n# Predict on the test set for each model\ny_a_pred = model_a.predict(X_test)\ny_b_pred = model_b.predict(X_test)\ny_tie_pred = model_tie.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n# Evaluate accuracy\naccuracy_a = accuracy_score(y_a_test, y_a_pred)\naccuracy_b = accuracy_score(y_b_test, y_b_pred)\naccuracy_tie = accuracy_score(y_tie_test, y_tie_pred)\n\nlog_loss_a = log_loss(y_a_test, y_a_pred)\nlog_loss_b = log_loss(y_b_test, y_b_pred)\nlog_loss_tie = log_loss(y_tie_test, y_tie_pred)\n\nprint(f\"Accuracy for winner_model_a: {accuracy_a}\")\nprint(f\"Accuracy for winner_model_b: {accuracy_b}\")\nprint(f\"Accuracy for winner_tie: {accuracy_tie}\")\n\nprint()\n\nprint(f\"log_loss for winner_model_a: {log_loss_a}\")\nprint(f\"log_loss for winner_model_b: {log_loss_b}\")\nprint(f\"log_loss for winner_tie: {log_loss_tie}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:21:50.682067Z","iopub.execute_input":"2024-10-31T21:21:50.68304Z","iopub.status.idle":"2024-10-31T21:30:03.739179Z","shell.execute_reply.started":"2024-10-31T21:21:50.682983Z","shell.execute_reply":"2024-10-31T21:30:03.737802Z"}}}]}