{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10584661,"sourceType":"datasetVersion","datasetId":6550343},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":222983840,"sourceType":"kernelVersion"},{"sourceId":223198626,"sourceType":"kernelVersion"},{"sourceId":225183641,"sourceType":"kernelVersion"},{"sourceId":257229,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":219877,"modelId":241636},{"sourceId":262659,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":224617,"modelId":246363},{"sourceId":263359,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":224617,"modelId":246363},{"sourceId":272415,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":224617,"modelId":246363},{"sourceId":281850,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":224617,"modelId":246363}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llama3-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llama3-pip/\n!pip install -q -U tokenizers --no-index --find-links ../input/llama3-pip/\n!pip install -q -U accelerate --no-index --find-links ../input/llama3-pip/\n!pip install -q -U peft --no-index --find-links ../input/llama3-pip/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport accelerate\nimport time\nimport emoji\nimport os\nimport torch.nn as nn\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers import AutoTokenizer, LlamaModel,LlamaPreTrainedModel, LlamaForSequenceClassification,AutoConfig, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\n#放弃向huggingface访问\n# os.environ['TRANSFORMERS_OFFLINE']=\"1\"\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nprint(torch.device(\"cuda\"))\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n\nprint(\"pip all imported\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = '/kaggle/input/llama3-2-1b-model/model/'\nWEIGHTS_PATH = '/kaggle/input/llama-3-finetuned-model/transformers/default/1/llama_3_finetuned_model.pth'\nMAX_LENGTH = 2048\nbatch_size= 3\nDEVICE = torch.device(\"cpu\")    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Data ","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\n\n# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    text = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(text)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\n\n# train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n# train.loc[:, 'prompt'] = train['prompt'].apply(process)\n# train.loc[:, 'response_a'] = train['response_a'].apply(process)\n# train.loc[:, 'response_b'] = train['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(6))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n#train\n#train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(test['text'][0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"%%time\n\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/llama-fine-tune/tokenizer',local_files_only=True)\n#train\n# train_tokens = tokenizer(train['text'].tolist(), padding='max_length',\n#                    max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n\n# train_INPUT_IDS = train_tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n# train_ATTENTION_MASKS = train_tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n\n#test\ntest_tokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\ntest_INPUT_IDS = test_tokens['input_ids'].to(DEVICE, dtype=torch.int32)\ntest_ATTENTION_MASKS = test_tokens['attention_mask'].to(DEVICE, dtype=torch.int32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids_test = [tensor.tolist() for tensor in test_INPUT_IDS]\nattention_masks_test = [tensor.tolist() for tensor in test_ATTENTION_MASKS]\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_test\ndata['ATTENTION_MASKS'] = attention_masks_test\ndata[:2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#train_datasets\n\nfrom torch.utils.data import Dataset, DataLoader,random_split\nclass LlamaDataset(Dataset):\n    def __init__(self, input_ids, attention_mask, labels):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": torch.tensor(self.labels[0][idx] ,dtype=torch.long)\n        }\n\n# train_lables = [train['winner_model_a'].values.astype(int)*0 + train['winner_model_b'].values.astype(int)*1 + train['winner_tie'].values.astype(int)*2]\n# dataset = LlamaDataset(train_INPUT_IDS, train_ATTENTION_MASKS, train_lables)\n\n# l = len(dataset)\n# train_size = int(0.9 * l)  # 用于训练\n# eval_size = l - train_size  # 剩余用于测试\n\n# print(len(dataset) == train_size+eval_size)\n# train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n# print(eval_dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load model \nWe load 1 model on each gpu.  ","metadata":{}},{"cell_type":"code","source":"from peft import IA3Config, TaskType, get_peft_model,PeftModel\nbnb_config =  BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\n\n#inference use for eval \npeft_config = LoraConfig(\n    # r=4,\n    # lora_alpha=8,\n    # lora_dropout=0.05,\n    # bias='none',\n    inference_mode=True,  #注意不要打开eval模式\n    task_type=TaskType.SEQ_CLS,\n    # target_modules=['q_proj', 'v_proj'],  # 修改这里\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments , Trainer\n\n#trainner_set\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working\",\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=4,\n    num_train_epochs=5,\n    learning_rate=2e-5,              # PEFT参数学习率\n    weight_decay=0.01,    #L2 正则化系数。\n    warmup_ratio=0.03,\n    fp16=False,                     \n    bf16=torch.cuda.is_bf16_supported(), \n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_total_limit=2,  #旧的 checkpoint 会被删除，以节省存储空间\n    report_to=\"tensorboard\",\n    dataloader_pin_memory=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass LlamaForClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        # 动态分类头维度\n        self.classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size // 2),\n            nn.GELU(),\n            nn.Linear(config.hidden_size // 2, 3)\n        )\n        self.post_init()\n\n    def forward(self, input_ids, attention_mask, labels=None,inputs_embeds=None, **kwargs):\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n             inputs_embeds=inputs_embeds,\n            **kwargs  # Pass any other keyword arguments\n        )\n        hidden_states = outputs.last_hidden_state\n        logits = self.classifier(hidden_states[:, -1, :])\n\n        #无需显式处理损失计算的细节\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        \n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions\n        )\n        \ndevice = torch.device(\"cuda\")\n\n#train from zero use model_name\n#continue train or eval use checkpoints\n            \ncheckpoints = \"/kaggle/input/1epoch/transformers/default/2/checkpoint-1500\"\nconfig = AutoConfig.from_pretrained(MODEL_NAME, num_labels=3)\nmodel =  LlamaForClassification.from_pretrained(\n    MODEL_NAME,\n    config=config,\n    device_map=\"auto\",\n)\n\nmodel = PeftModel.from_pretrained(model, checkpoints)\n#从头trainer需要重新设置peft的操作\n#model = get_peft_model(model, peft_config)\nmodel.eval()\nprint(\"model load success\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we have sucessfully loaded one model on each GPU!","metadata":{}},{"cell_type":"markdown","source":"# model weight","metadata":{}},{"cell_type":"code","source":"#load weight\n\n#model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\nmodel.print_trainable_parameters()\n\n#trainner start\n\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset,\n   \n# )\n# trainer.train(resume_from_checkpoint = \"/kaggle/input/check-points/transformers/default/1/checkpoint-1000\")\n# trainer.save_model(\"/kaggle/working/llama3-model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\n","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(df, model, device, batch_size=batch_size):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Function to run inference in a thread\ndef run_inference(df, model, device, results, index):\n    results[index] = inference(df, model, device)\n\n# Dictionary to store results from threads\nresults = {}\n\nprint(\"ready for start\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"st = time.time()\n# start threads\nrun_inference(data, model, device, results, 0)\nprint(f\"model eval spend 推理总花费时常: {time.time() - st}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\nsample_sub[TARGETS] = data[TARGETS]\n\n\n#llama3  swap data 0,1\n# sample_sub.iloc[:,[1, 2]] = sample_sub.iloc[:,[2, 1]].values\n\ndisplay(sample_sub)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_sub.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"answer get\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :) ","metadata":{}}]}