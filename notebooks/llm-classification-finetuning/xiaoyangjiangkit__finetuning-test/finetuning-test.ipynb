{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":8926343,"sourceType":"datasetVersion","datasetId":5369301},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10150018,"sourceType":"datasetVersion","datasetId":6265978},{"sourceId":10214229,"sourceType":"datasetVersion","datasetId":6267026},{"sourceId":10236316,"sourceType":"datasetVersion","datasetId":6266300},{"sourceId":10302322,"sourceType":"datasetVersion","datasetId":6265823},{"sourceId":75103,"sourceType":"modelInstanceVersion","modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-02-20T11:25:59.457478Z","iopub.execute_input":"2025-02-20T11:25:59.458575Z","iopub.status.idle":"2025-02-20T11:26:05.104108Z","shell.execute_reply.started":"2025-02-20T11:25:59.458537Z","shell.execute_reply":"2025-02-20T11:26:05.102894Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2025-02-20T11:26:05.10648Z","iopub.execute_input":"2025-02-20T11:26:05.10735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install transformers peft accelerate bitsandbytes \\-U --no-index --find-links /kaggle/input/lmsys-wheel-files\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile prepare_test_file.py\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\ndf.to_parquet(\"test.parquet\", index=False)\n\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\ndf.to_parquet(\"test_swap.parquet\", index=False)\n\n###/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\n'''\nimport pandas as pd\n\n# 讀取主要的訓練資料集和 reward 分數資訊\nfull_data = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")  # 包含 id, prompt, response_a, response_b 等欄位\nrewards_data = pd.read_csv(\"/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\")  # 包含 id, fold\n\n# 合併 reward 分數到主要資料集中\nmerged_data = full_data.merge(rewards_data, on='id', how='inner')\nmerged_data.rename(columns={'fold': 'reward'}, inplace=True)  # 將 'fold' 欄位重命名為 'reward'\n\n# 檢查合併後的資料\nprint(merged_data.head())\n\n# 儲存合併後的資料\nmerged_data.to_parquet(\"train_with_rewards.parquet\", index=False)\n\n'''\n\n\n'''\n# %%writefile prepare_test_file.py\nimport pandas as pd\n\n# 讀取原始測試資料和 reward 資料\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\nfolds_df = pd.read_csv(\"/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\")\n\n# 合併 reward 資料到測試資料中\nmerged_df = test_df.merge(folds_df, on=\"id\", how=\"inner\")\n\n# 假設 fold 欄位即為 reward 分數，設置 winner labels\nmerged_df[\"winner_model_a\"] = (merged_df[\"fold\"] == 0).astype(int)\nmerged_df[\"winner_model_b\"] = (merged_df[\"fold\"] == 1).astype(int)\nmerged_df[\"winner_tie\"] = (merged_df[\"fold\"] == 2).astype(int)\n\n# 保存資料\nmerged_df.to_parquet(\"test.parquet\", index=False)\n\n# 創建 response_a 和 response_b 調換版本的資料集\nmerged_df[\"response_a\"], merged_df[\"response_b\"] = merged_df[\"response_b\"], merged_df[\"response_a\"]\nmerged_df.to_parquet(\"test_swap.parquet\", index=False)\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python prepare_test_file.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m0.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"\ncsv_path = \"test.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=False,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 42\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.head_dim\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m0.npy', prob)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python predict_m0.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m3.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\ncsv_path = \"test_swap.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\ntokenizer.deprecation_warnings[\n    \"sequence-length-is-longer-than-the-specified-maximum\"\n] = True\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=True,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 32\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = LlamaForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m3.npy', prob)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python predict_m3.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprob = np.load('prob_m3.npy')\n\nprint(prob[:5])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n%%time\n\ntokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\ntokenizer.add_eos_token = True\ntokenizer.padding_side = \"right\"\n\ndata = pd.DataFrame()\ndata[\"id\"] = test[\"id\"]\ndata[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\ndata[\"length\"] = data[\"input_ids\"].apply(len)\n\naug_data = pd.DataFrame()\naug_data[\"id\"] = test[\"id\"]\n# swap response_a & response_b\naug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\naug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n#%%writefile predict_lora.py\n\n\nimport time\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nfrom transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\nfrom peft import PeftModel\n\nassert torch.cuda.device_count() == 2\n@dataclass\nclass Config:\n    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n    max_length = 2048\n    batch_size = 4\n    device = torch.device(\"cuda\")    \n    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n\ncfg = Config()\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\ndef process_text(text: str) -> str:\n    return \" \".join(eval(text, {\"null\": \"\"}))\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process_text)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process_text)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n\ndisplay(test.head(5))\ndef tokenize(\n    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n):\n    prompt = [\"<prompt>: \" + p for p in prompt]\n    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n    if spread_max_length:\n        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        attention_mask = [[1]* len(i) for i in input_ids]\n    else:\n        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n        input_ids = tokenized.input_ids\n        attention_mask = tokenized.attention_mask\n    return input_ids, attention_mask\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Load base model on GPU 0\ndevice_0 = torch.device('cuda:0')\nmodel_0 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_0,\n    use_cache=False,\n)\n\n# Load base model on GPU 1\ndevice_1 = torch.device('cuda:1')\nmodel_1 = Gemma2ForSequenceClassification.from_pretrained(\n    cfg.gemma_dir,\n    device_map=device_1,\n    use_cache=False,\n)\n\nmodel_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\nmodel_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n#%%writefile predict_qlora.py\n@torch.no_grad()\n@torch.cuda.amp.autocast()\ndef inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n    a_win, b_win, tie = [], [], []\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        tmp = df.iloc[start_idx:end_idx]\n        input_ids = tmp[\"input_ids\"].to_list()\n        attention_mask = tmp[\"attention_mask\"].to_list()\n        inputs = pad_without_fast_tokenizer_warning(\n            tokenizer,\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n            padding=\"longest\",\n            pad_to_multiple_of=None,\n            return_tensors=\"pt\",\n        )\n        outputs = model(**inputs.to(device))\n        proba = outputs.logits.softmax(-1).cpu()\n        \n        a_win.extend(proba[:, 0].tolist())\n        b_win.extend(proba[:, 1].tolist())\n        tie.extend(proba[:, 2].tolist())\n    \n    df[\"winner_model_a\"] = a_win\n    df[\"winner_model_b\"] = b_win\n    df[\"winner_tie\"] = tie\n    \n    return df\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!python predict_qlora.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nst = time.time()\n\n# sort by input length to fully leverage dynaminc padding\ndata = data.sort_values(\"length\", ascending=False)\n# the total #tokens in sub_1 and sub_2 should be more or less the same\nsub_1 = data.iloc[0::2].copy()\nsub_2 = data.iloc[1::2].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\nresult_df = pd.concat(list(results), axis=0)\ndisplay(result_df)\n\nproba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n\nprint(f\"elapsed time: {time.time() - st}\")\n\nst = time.time()\n\nif cfg.tta:\n    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n    sub_1 = data.iloc[0::2].copy()\n    sub_2 = data.iloc[1::2].copy()\n\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\n    tta_result_df = pd.concat(list(results), axis=0)\n    # recall TTA's order is flipped\n    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n    # average original result and TTA result.\n    proba = (proba + tta_proba) / 2\n\nprint(f\"elapsed time: {time.time() - st}\")\n\nresult_df.loc[:, \"winner_model_a\"] = proba[:, 0]\nresult_df.loc[:, \"winner_model_b\"] = proba[:, 1]\nresult_df.loc[:, \"winner_tie\"] = proba[:, 2]\nsubmission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\nsubmission_df.to_csv('submission.csv', index=False)\ndisplay(submission_df)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nsubmission_df = pd.DataFrame(submission_df)\n\nprob_qlora = submission_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].to_numpy()\n\nprint(prob_qlora)\nnp.save('prob_qlora.npy', proba)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!python predict_lorasciia.py\n#display(result_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sentence Transformer","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install --no-index --find-links=/kaggle/input/some-pack/sentence_transformers_packages sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\nimport faiss\nmodel_load_path = '/kaggle/input/some-pack/sentence-transformer-model' \nsentence_model = SentenceTransformer(model_load_path)\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nclass CustomDebertaModel(nn.Module):\n    def __init__(self, model_name, num_labels, feature_dim=2, dropout_rate=0.1):\n        super(CustomDebertaModel, self).__init__()\n        # 初始化DeBERTa模型\n        self.base_model = AutoModel.from_pretrained(model_name)\n        # 相似度特徵塔 (MLP)\n        self.feature_fc = nn.Sequential(\n            nn.Linear(feature_dim, 128),  # 映射到128維\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(128, self.base_model.config.hidden_size),  # 映射到文本嵌入維度\n            nn.ReLU()\n        )\n        # 注意力機制\n        self.attention = nn.MultiheadAttention(\n            embed_dim=self.base_model.config.hidden_size,\n            num_heads=4,  # 設定注意力頭的數量\n            batch_first=True\n        )\n        # Dropout 層\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        # 最終分類層\n        self.classifier = nn.Sequential(\n            nn.Linear(self.base_model.config.hidden_size * 2, self.base_model.config.hidden_size),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(self.base_model.config.hidden_size, num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask, similarity_features, labels=None):\n        # 文本塔：提取文本嵌入\n        base_outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeddings = base_outputs.last_hidden_state[:, 0, :]  # 提取 [CLS] token 嵌入\n\n        # 特徵塔：處理相似度特徵\n        similarity_embeds = self.feature_fc(similarity_features)\n\n        # 使用注意力機制進行交互\n        query = text_embeddings.unsqueeze(1)  # [batch_size, 1, hidden_size]\n        key_value = similarity_embeds.unsqueeze(1)  # [batch_size, 1, hidden_size]\n        attention_output, _ = self.attention(query, key_value, key_value)\n\n        # 拼接文本嵌入與注意力輸出\n        combined_features = torch.cat([text_embeddings, attention_output.squeeze(1)], dim=1)\n\n        # Dropout 和分類\n        logits = self.classifier(self.dropout(combined_features))\n\n        # 輸出結果\n        outputs = {\"logits\": logits}\n        if labels is not None:\n            # 如果有標籤，計算損失\n            loss_fn = nn.CrossEntropyLoss()\n            outputs[\"loss\"] = loss_fn(logits, labels)\n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport faiss\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n# 使用 FAISS 計算語義相似性分數\ndef compute_semantic_features_with_faiss(df):\n    prompts = df['prompt'].tolist()\n    responses_a = df['response_a'].tolist()\n    responses_b = df['response_b'].tolist()\n\n    # 提取嵌入向量並正規化\n    prompt_embeddings = np.array(sentence_model.encode(prompts))\n    prompt_embeddings = prompt_embeddings / np.linalg.norm(prompt_embeddings, axis=1, keepdims=True)\n\n    response_a_embeddings = np.array(sentence_model.encode(responses_a))\n    response_a_embeddings = response_a_embeddings / np.linalg.norm(response_a_embeddings, axis=1, keepdims=True)\n\n    response_b_embeddings = np.array(sentence_model.encode(responses_b))\n    response_b_embeddings = response_b_embeddings / np.linalg.norm(response_b_embeddings, axis=1, keepdims=True)\n\n    dim = prompt_embeddings.shape[1]\n    index_flat = faiss.IndexFlatIP(dim)  # 使用內積計算相似度\n\n    # 計算相似度\n    index_flat.add(prompt_embeddings)  # 添加 prompt 嵌入向量到索引\n    similarity_a = index_flat.search(response_a_embeddings, k=1)[0].squeeze()\n\n    index_flat.reset()\n    index_flat.add(prompt_embeddings)\n    similarity_b = index_flat.search(response_b_embeddings, k=1)[0].squeeze()\n\n    df['similarity_a'] = similarity_a\n    df['similarity_b'] = similarity_b\n\n    return df\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = compute_semantic_features_with_faiss(test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = torch.load(\"/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(\"Custom model loaded successfully!\")\n\n\ntokenizer_path = \"/kaggle/input/akemiiiiii/custom_model_dir\" \ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\ndef preprocess_test_data(row):\n    input_text = f\"Prompt: {row['prompt']} Response A: {row['response_a']} Response B: {row['response_b']}\"\n    tokenized_inputs = tokenizer(\n        input_text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n        return_tensors=\"pt\",\n    )\n    tokenized_inputs[\"similarity_a\"] = torch.tensor([row[\"similarity_a\"]], dtype=torch.float32)\n    tokenized_inputs[\"similarity_b\"] = torch.tensor([row[\"similarity_b\"]], dtype=torch.float32)\n    return tokenized_inputs\n\nprocessed_test_data = [preprocess_test_data(row) for _, row in test_data.iterrows()]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 創建自定義測試數據集\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# 初始化數據集\ntest_dataset = TestDataset(processed_test_data)\n\n# 自定義 DataLoader 的 collate_fn 函數\ndef collate_fn_test(batch):\n    input_ids = torch.cat([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch])\n    similarity_features = torch.cat(\n        [torch.cat([item[\"similarity_a\"], item[\"similarity_b\"]], dim=0).unsqueeze(0) for item in batch]\n    )\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"similarity_features\": similarity_features,\n    }\n\n# 創建 DataLoader\ntest_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn_test, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 模型推理\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        similarity_features = batch[\"similarity_features\"].to(device)\n\n        # 推理\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, similarity_features=similarity_features)\n        logits = outputs[\"logits\"]\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        predictions.append(probs.cpu().numpy())\n\n# 合併所有預測結果\npredictions = np.concatenate(predictions, axis=0)\n\n# 檢視預測結果\nprint(predictions)\n\nnp.save('prob_faiss.npy', predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"'''\n%%writefile make_submission.py\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_parquet(\"test.parquet\")\npreds = np.average(\n    [\n        np.load(\"prob_m0.npy\"),\n        np.load(\"prob_m3.npy\")[:, [1, 0, 2]],\n    ],\n    axis=0,\n    weights=[2, 1],\n)\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##Model embedding\n\n#%%writefile make_submission.py\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_parquet(\"test.parquet\")\n\n\nprob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\nprob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3 (swap response_a and response_b)\nprob_faiss = np.load(\"prob_faiss.npy\")  # faiss\n\n# Combine predictions with weights\n# Adjust weights as needed for optimal performance\n\nfrom scipy.optimize import minimize\n\n\n\n\npreds = np.average(\n    [\n        prob_m0,       # Gemma2 results\n        prob_m3,       # Llama3 results\n        prob_faiss     # faiss results\n    ],\n    axis=0,\n    weights=[0.5, 0.3, 0.2]  # Weights for each model\n)\n\n\n\n\n\n# Create submission DataFrame\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\n\n\n\n# Save to CSV\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!python make_submission.py","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}