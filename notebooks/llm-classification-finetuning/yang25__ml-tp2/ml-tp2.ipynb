{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":631737,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":476142,"modelId":492061}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Baseline with Logistic Regression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. å¯¼å…¥å¿…è¦çš„åº“\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import log_loss, classification_report, confusion_matrix\nimport string\n\n# 2. åŠ è½½æ•°æ®\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# 3. ç‰¹å¾å·¥ç¨‹ï¼šæå–ç»Ÿè®¡ç‰¹å¾\n\ndef create_features(df):\n    # é•¿åº¦ç‰¹å¾\n    df['prompt_length'] = df['prompt'].str.len()\n    df['response_a_length'] = df['response_a'].str.len()\n    df['response_b_length'] = df['response_b'].str.len()\n    # å•è¯æ•°\n    df['prompt_word_count'] = df['prompt'].str.split().str.len()\n    df['response_a_word_count'] = df['response_a'].str.split().str.len()\n    df['response_b_word_count'] = df['response_b'].str.split().str.len()\n    # æ ‡ç‚¹æ•°\n    df['prompt_punc_count'] = df['prompt'].str.count(f'[{string.punctuation}]')\n    df['response_a_punc_count'] = df['response_a'].str.count(f'[{string.punctuation}]')\n    df['response_b_punc_count'] = df['response_b'].str.count(f'[{string.punctuation}]')\n    # å·®å¼‚ç‰¹å¾\n    df['response_length_diff'] = df['response_a_length'] - df['response_b_length']\n    df['response_word_diff'] = df['response_a_word_count'] - df['response_b_word_count']\n    return df\n\ntrain = create_features(train)\ntest = create_features(test)\n\n# 4. Labelç¼–ç ï¼ˆå¯é€‰ï¼šå¯¹modelåç§°ï¼Œä½†testä¸ä¼šç”¨åˆ°ï¼‰\nle = LabelEncoder()\nmodel_cols = []\nfor col in ['model_a', 'model_b']:\n    train[f'{col}_enc'] = le.fit_transform(train[col])\n    model_cols += [f'{col}_enc']\n\n# 5. æ„é€ æ ‡ç­¾\n#   winner_model_a=1â†’ç±»åˆ«0ï¼Œwinner_model_b=1â†’ç±»åˆ«1ï¼Œwinner_tie=1â†’ç±»åˆ«2\ntrain['target'] = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\n\n# 6. é€‰å®šå…¨éƒ¨æ•°å€¼ç‰¹å¾\nfeature_cols = [\n    'prompt_length', 'response_a_length', 'response_b_length',\n    'prompt_word_count', 'response_a_word_count', 'response_b_word_count',\n    'prompt_punc_count', 'response_a_punc_count', 'response_b_punc_count',\n    'response_length_diff', 'response_word_diff'\n    # è‹¥å¸Œæœ›ï¼Œä¹Ÿå¯æ·»åŠ model_a_enc/model_b_enc\n]\n\nX = train[feature_cols]\ny = train['target']\n\n# 7. æ•°æ®æ‹†åˆ† + æ ‡å‡†åŒ–\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n\n# 8. é€»è¾‘å›å½’è®­ç»ƒ\nclf = LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr')\nclf.fit(X_train_scaled, y_train)\n\n# 9. éªŒè¯é›†æ•ˆæœ\nval_pred_proba = clf.predict_proba(X_val_scaled)\nval_pred = clf.predict(X_val_scaled)\nprint('Validation Log Loss:', log_loss(y_val, val_pred_proba))\nprint('Classification Report:\\n', classification_report(y_val, val_pred, digits=4))\n\n# 10. ç”Ÿæˆtestç‰¹å¾ã€æ ‡å‡†åŒ–å¹¶é¢„æµ‹\nX_test = test[feature_cols]\nX_test_scaled = scaler.transform(X_test)\ntest_pred_proba = clf.predict_proba(X_test_scaled)\n\n# 11. ç”ŸæˆKaggleæäº¤æ–‡ä»¶\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': test_pred_proba[:,0],\n    'winner_model_b': test_pred_proba[:,1],\n    'winner_tie': test_pred_proba[:,2],\n})\nsubmission.to_csv('submission_baseline.csv', index=False)\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:41:02.203709Z","iopub.execute_input":"2025-11-06T09:41:02.204413Z","iopub.status.idle":"2025-11-06T09:41:14.304691Z","shell.execute_reply.started":"2025-11-06T09:41:02.204381Z","shell.execute_reply":"2025-11-06T09:41:14.303056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 2: Embedding-based model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„åŠ è½½ï¼ˆè·¯å¾„éœ€å’Œä½  Add Input åç§°ä¸€è‡´ï¼‰\nmodel = SentenceTransformer('/kaggle/input/minilm-l12-v2-local/other/default/1/minilm_l12_v2_local')\n\n# æ‹¼æ¥æ–‡æœ¬\ndef concat_text(df):\n    return (\n        df['prompt'].astype(str) + ' ' + df['response_a'].astype(str),\n        df['prompt'].astype(str) + ' ' + df['response_b'].astype(str)\n    )\ntrain_a, train_b = concat_text(train)\ntest_a, test_b = concat_text(test)\n\nemb_a = model.encode(train_a.tolist(), batch_size=32, show_progress_bar=True)\nemb_b = model.encode(train_b.tolist(), batch_size=32, show_progress_bar=True)\nX = np.hstack([emb_a, emb_b, emb_a - emb_b])\ny = train[['winner_model_a','winner_model_b','winner_tie']].values.argmax(axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\nprint('Val Log Loss:', log_loss(y_val, clf.predict_proba(X_val)))\n\nemb_a_test = model.encode(test_a.tolist(), batch_size=32, show_progress_bar=True)\nemb_b_test = model.encode(test_b.tolist(), batch_size=32, show_progress_bar=True)\nX_test = np.hstack([emb_a_test, emb_b_test, emb_a_test - emb_b_test])\nproba = clf.predict_proba(X_test)\n\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': proba[:,0],\n    'winner_model_b': proba[:,1],\n    'winner_tie': proba[:,2]\n})\nsubmission.to_csv('submission_emb.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:05:09.086277Z","iopub.execute_input":"2025-11-06T11:05:09.087032Z","iopub.status.idle":"2025-11-06T11:10:15.798781Z","shell.execute_reply.started":"2025-11-06T11:05:09.087003Z","shell.execute_reply":"2025-11-06T11:10:15.798246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 3. Model Extensions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting Step 3: Model Extensions...\")\n\n# å®‰è£…å¿…è¦çš„åº“ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰\n!pip install xgboost lightgbm --quiet\n\n# å¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sentence_transformers import SentenceTransformer\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"æ‰€æœ‰åº“å¯¼å…¥å®Œæˆ\")\n\n# åŠ è½½æ•°æ®\nprint(\"åŠ è½½æ•°æ®...\")\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\nprint(f\"è®­ç»ƒé›†å¤§å°: {train.shape}\")\nprint(f\"æµ‹è¯•é›†å¤§å°: {test.shape}\")\n\n# =============================================================================\n# 1. åå·®æ„ŸçŸ¥ç‰¹å¾å·¥ç¨‹ (Bias-aware Features)\n# =============================================================================\n\ndef extract_bias_aware_features(df):\n    \"\"\"\n    æå–ä½ç½®åå·®å’Œå†—é•¿åå·®ç›¸å…³ç‰¹å¾\n    \"\"\"\n    print(\"æå–åå·®æ„ŸçŸ¥ç‰¹å¾...\")\n    features = pd.DataFrame(index=df.index)\n    \n    # ä½ç½®åå·®ç‰¹å¾ (Position Bias)\n    features['position_a_first'] = 1  # response_aæ€»æ˜¯ç¬¬ä¸€ä¸ª\n    features['position_b_second'] = 0\n    \n    # å†—é•¿åå·®ç‰¹å¾ (Verbosity Bias)\n    features['response_a_length'] = df['response_a'].str.len()\n    features['response_b_length'] = df['response_b'].str.len()\n    features['length_diff'] = features['response_a_length'] - features['response_b_length']\n    features['length_ratio'] = features['response_a_length'] / (features['response_b_length'] + 1)\n    \n    # è¯æ±‡ä¸°å¯Œåº¦ç‰¹å¾\n    def lexical_richness(text):\n        if pd.isna(text) or text == '':\n            return 0\n        words = str(text).split()\n        if len(words) == 0:\n            return 0\n        return len(set(words)) / len(words)\n    \n    features['richness_a'] = df['response_a'].apply(lexical_richness)\n    features['richness_b'] = df['response_b'].apply(lexical_richness)\n    features['richness_diff'] = features['richness_a'] - features['richness_b']\n    \n    # æ ¼å¼ç‰¹å¾ (æ£€æŸ¥æ˜¯å¦æœ‰åˆ—è¡¨ã€ä»£ç å—ç­‰)\n    def format_complexity(text):\n        score = 0\n        text_str = str(text)\n        # æ£€æŸ¥åˆ—è¡¨\n        if re.search(r'\\d+\\.|\\*|\\-', text_str):\n            score += 1\n        # æ£€æŸ¥ä»£ç å—\n        if '```' in text_str or '    ' in text_str:\n            score += 1\n        # æ£€æŸ¥æ ‡é¢˜\n        if re.search(r'^#+\\s', text_str, re.MULTILINE):\n            score += 1\n        return score\n    \n    features['format_a'] = df['response_a'].apply(format_complexity)\n    features['format_b'] = df['response_b'].apply(format_complexity)\n    features['format_diff'] = features['format_a'] - features['format_b']\n    \n    # é—®å·å’Œæ„Ÿå¹å·æ•°é‡ç‰¹å¾\n    features['question_a'] = df['response_a'].str.count(r'\\?')\n    features['question_b'] = df['response_b'].str.count(r'\\?')\n    features['exclamation_a'] = df['response_a'].str.count(r'!')\n    features['exclamation_b'] = df['response_b'].str.count(r'!')\n    \n    features['question_diff'] = features['question_a'] - features['question_b']\n    features['exclamation_diff'] = features['exclamation_a'] - features['exclamation_b']\n    \n    print(f\"åå·®ç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾æ•°é‡: {features.shape[1]}\")\n    return features\n\n# æå–åå·®ç‰¹å¾\nbias_features_train = extract_bias_aware_features(train)\nbias_features_test = extract_bias_aware_features(test)\n\n# =============================================================================\n# 2. åŸºç¡€ç»Ÿè®¡ç‰¹å¾ (Basic Statistical Features)\n# =============================================================================\n\ndef extract_basic_features(df):\n    \"\"\"\n    æå–åŸºç¡€ç»Ÿè®¡ç‰¹å¾ï¼ˆç±»ä¼¼Step 1ä½†æ›´å…¨é¢ï¼‰\n    \"\"\"\n    print(\"æå–åŸºç¡€ç»Ÿè®¡ç‰¹å¾...\")\n    features = pd.DataFrame(index=df.index)\n    \n    # é•¿åº¦ç‰¹å¾\n    features['prompt_length'] = df['prompt'].str.len()\n    features['response_a_length'] = df['response_a'].str.len()\n    features['response_b_length'] = df['response_b'].str.len()\n    \n    # å•è¯æ•°é‡\n    features['prompt_word_count'] = df['prompt'].str.split().str.len()\n    features['response_a_word_count'] = df['response_a'].str.split().str.len()\n    features['response_b_word_count'] = df['response_b'].str.split().str.len()\n    \n    # æ ‡ç‚¹ç¬¦å·æ•°é‡\n    import string\n    features['prompt_punc_count'] = df['prompt'].str.count(f'[{re.escape(string.punctuation)}]')\n    features['response_a_punc_count'] = df['response_a'].str.count(f'[{re.escape(string.punctuation)}]')\n    features['response_b_punc_count'] = df['response_b'].str.count(f'[{re.escape(string.punctuation)}]')\n    \n    # å·®å¼‚ç‰¹å¾\n    features['response_length_diff'] = features['response_a_length'] - features['response_b_length']\n    features['response_word_diff'] = features['response_a_word_count'] - features['response_b_word_count']\n    features['response_punc_diff'] = features['response_a_punc_count'] - features['response_b_punc_count']\n    \n    print(f\"åŸºç¡€ç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾æ•°é‡: {features.shape[1]}\")\n    return features\n\n# æå–åŸºç¡€ç‰¹å¾\nbasic_features_train = extract_basic_features(train)\nbasic_features_test = extract_basic_features(test)\n\n# =============================================================================\n# 3. åµŒå…¥ç‰¹å¾ (Embedding Features)\n# =============================================================================\n\nprint(\"åŠ è½½åµŒå…¥æ¨¡å‹...\")\n# ä½¿ç”¨å°ç»„å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹\ntry:\n    model = SentenceTransformer('/kaggle/input/minilm-l12-v2-local/other/default/1/minilm_l12_v2_local')\n    print(\"ä½¿ç”¨æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹\")\nexcept:\n    # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨åœ¨çº¿æ¨¡å‹\n    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n    print(\"ä½¿ç”¨åœ¨çº¿é¢„è®­ç»ƒæ¨¡å‹\")\n\ndef create_embedding_features(df, model):\n    \"\"\"\n    åˆ›å»ºåµŒå…¥ç‰¹å¾\n    \"\"\"\n    print(\"ç”ŸæˆåµŒå…¥ç‰¹å¾...\")\n    \n    # æ–‡æœ¬æ‹¼æ¥\n    def concat_text(df):\n        return (\n            df['prompt'].astype(str) + ' [SEP] ' + df['response_a'].astype(str),\n            df['prompt'].astype(str) + ' [SEP] ' + df['response_b'].astype(str)\n        )\n    \n    text_a, text_b = concat_text(df)\n    \n    # è·å–åµŒå…¥ç‰¹å¾\n    emb_a = model.encode(text_a.tolist(), batch_size=32, show_progress_bar=True)\n    emb_b = model.encode(text_b.tolist(), batch_size=32, show_progress_bar=True)\n    \n    # åµŒå…¥ç‰¹å¾å·¥ç¨‹\n    embedding_features = np.hstack([\n        emb_a, \n        emb_b, \n        emb_a - emb_b,  # å·®å¼‚ç‰¹å¾\n        np.abs(emb_a - emb_b),  # ç»å¯¹å·®å¼‚\n        emb_a * emb_b,  # äº¤äº’ç‰¹å¾\n    ])\n    \n    print(f\"åµŒå…¥ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç‰¹å¾ç»´åº¦: {embedding_features.shape}\")\n    return embedding_features\n\n# åˆ›å»ºåµŒå…¥ç‰¹å¾\nprint(\"ä¸ºè®­ç»ƒé›†åˆ›å»ºåµŒå…¥ç‰¹å¾...\")\nembedding_features_train = create_embedding_features(train, model)\nprint(\"ä¸ºæµ‹è¯•é›†åˆ›å»ºåµŒå…¥ç‰¹å¾...\")\nembedding_features_test = create_embedding_features(test, model)\n\n# =============================================================================\n# 4. åˆå¹¶æ‰€æœ‰ç‰¹å¾\n# =============================================================================\n\nprint(\"åˆå¹¶æ‰€æœ‰ç‰¹å¾...\")\nX_ensemble = np.hstack([\n    embedding_features_train,\n    bias_features_train.values,\n    basic_features_train.values\n])\n\nX_test_ensemble = np.hstack([\n    embedding_features_test,\n    bias_features_test.values,\n    basic_features_test.values\n])\n\n# ç›®æ ‡å˜é‡\ny = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\n\nprint(f\"æœ€ç»ˆç‰¹å¾çŸ©é˜µå¤§å°: {X_ensemble.shape}\")\nprint(f\"æµ‹è¯•é›†ç‰¹å¾çŸ©é˜µå¤§å°: {X_test_ensemble.shape}\")\n\n# =============================================================================\n# 5. æ•°æ®å‡†å¤‡\n# =============================================================================\n\nprint(\"å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®...\")\nX_train, X_val, y_train, y_val = train_test_split(\n    X_ensemble, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ç‰¹å¾æ ‡å‡†åŒ–\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test_ensemble)\n\nprint(\"æ•°æ®å‡†å¤‡å®Œæˆ\")\n\n# =============================================================================\n# 6. è®­ç»ƒå¤šä¸ªåŸºç¡€æ¨¡å‹\n# =============================================================================\n\nprint(\"è®­ç»ƒå¤šä¸ªåŸºç¡€æ¨¡å‹...\")\n\n# åŸºç¡€æ¨¡å‹é›†åˆ\nmodels = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n    'XGBoost': xgb.XGBClassifier(\n        n_estimators=100,\n        learning_rate=0.1,\n        random_state=42,\n        eval_metric='mlogloss',\n        n_jobs=-1\n    ),\n    'LightGBM': lgb.LGBMClassifier(\n        n_estimators=100,\n        learning_rate=0.1,\n        random_state=42,\n        n_jobs=-1\n    )\n}\n\n# å•ç‹¬è®­ç»ƒæ¯ä¸ªæ¨¡å‹å¹¶è¯„ä¼°\nbase_model_predictions = {}\nbase_model_scores = {}\n\nfor name, model in models.items():\n    print(f\"è®­ç»ƒ {name}...\")\n    model.fit(X_train_scaled, y_train)\n    \n    # éªŒè¯é›†é¢„æµ‹\n    val_pred_proba = model.predict_proba(X_val_scaled)\n    val_score = log_loss(y_val, val_pred_proba)\n    \n    base_model_predictions[name] = val_pred_proba\n    base_model_scores[name] = val_score\n    print(f\"  {name} éªŒè¯é›† Log Loss: {val_score:.4f}\")\n\n# =============================================================================\n# 7. é›†æˆæ–¹æ³•\n# =============================================================================\n\nprint(\"\\nè®­ç»ƒé›†æˆæ¨¡å‹...\")\n\n# 7.1 æŠ•ç¥¨é›†æˆ (Voting Ensemble)\nprint(\"è®­ç»ƒæŠ•ç¥¨é›†æˆæ¨¡å‹...\")\nvoting_clf = VotingClassifier(\n    estimators=[(name, model) for name, model in models.items()],\n    voting='soft'\n)\n\nvoting_clf.fit(X_train_scaled, y_train)\nvoting_pred_proba = voting_clf.predict_proba(X_val_scaled)\nvoting_score = log_loss(y_val, voting_pred_proba)\nprint(f\"æŠ•ç¥¨é›†æˆéªŒè¯é›† Log Loss: {voting_score:.4f}\")\n\n# 7.2 æ¦‚ç‡æ ¡å‡† (Probability Calibration)\nprint(\"è¿›è¡Œæ¦‚ç‡æ ¡å‡†...\")\n# é€‰æ‹©æ€§èƒ½æœ€å¥½çš„åŸºç¡€æ¨¡å‹è¿›è¡Œæ ¡å‡†\nbest_base_model_name = min(base_model_scores, key=base_model_scores.get)\nprint(f\"é€‰æ‹© {best_base_model_name} è¿›è¡Œæ¦‚ç‡æ ¡å‡†\")\n\ncalibrated_clf = CalibratedClassifierCV(\n    models[best_base_model_name], \n    method='isotonic', \n    cv=3\n)\ncalibrated_clf.fit(X_train_scaled, y_train)\ncalibrated_pred_proba = calibrated_clf.predict_proba(X_val_scaled)\ncalibrated_score = log_loss(y_val, calibrated_pred_proba)\nprint(f\"æ ¡å‡†åéªŒè¯é›† Log Loss: {calibrated_score:.4f}\")\n\n# 7.3 åŠ æƒå¹³å‡é›†æˆ (Weighted Average Ensemble)\nprint(\"è¿›è¡ŒåŠ æƒå¹³å‡é›†æˆ...\")\n# åŸºäºéªŒè¯é›†æ€§èƒ½è®¡ç®—æƒé‡ï¼ˆæ€§èƒ½è¶Šå¥½æƒé‡è¶Šé«˜ï¼‰\nweights = {}\ntotal_performance = sum(1/score for score in base_model_scores.values())\nfor name, score in base_model_scores.items():\n    weights[name] = (1/score) / total_performance\n\nprint(\"æ¨¡å‹æƒé‡åˆ†é…:\")\nfor name, weight in weights.items():\n    print(f\"  {name}: {weight:.3f}\")\n\ndef weighted_average_predict(models, weights, X):\n    \"\"\"åŠ æƒå¹³å‡é¢„æµ‹\"\"\"\n    predictions = []\n    for name, model in models.items():\n        pred = model.predict_proba(X)\n        weighted_pred = pred * weights[name]\n        predictions.append(weighted_pred)\n    \n    # å¹³å‡é¢„æµ‹\n    final_pred = np.mean(predictions, axis=0)\n    return final_pred\n\nweighted_pred_proba = weighted_average_predict(models, weights, X_val_scaled)\nweighted_score = log_loss(y_val, weighted_pred_proba)\nprint(f\"åŠ æƒå¹³å‡é›†æˆéªŒè¯é›† Log Loss: {weighted_score:.4f}\")\n\n# =============================================================================\n# 8. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå’Œé€‰æ‹©\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ\")\nprint(\"=\"*50)\n\n# æ”¶é›†æ‰€æœ‰æ¨¡å‹æ€§èƒ½\nall_scores = {\n    **base_model_scores,\n    'Voting Ensemble': voting_score,\n    'Calibrated Model': calibrated_score,\n    'Weighted Average': weighted_score\n}\n\n# æŒ‰æ€§èƒ½æ’åº\nsorted_scores = sorted(all_scores.items(), key=lambda x: x[1])\n\nprint(\"\\næ¨¡å‹æ€§èƒ½æ’å:\")\nprint(\"-\" * 40)\nfor name, score in sorted_scores:\n    print(f\"{name:20} | Log Loss: {score:.4f}\")\n\n# é€‰æ‹©æœ€ä½³æ¨¡å‹\nbest_model_name, best_score = sorted_scores[0]\nprint(f\"\\næœ€ä½³æ¨¡å‹: {best_model_name}\")\nprint(f\"æœ€ä½³åˆ†æ•°: {best_score:.4f}\")\n\n# =============================================================================\n# 9. ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆé¢„æµ‹\n# =============================================================================\n\nprint(f\"\\nä½¿ç”¨ {best_model_name} è¿›è¡Œæœ€ç»ˆé¢„æµ‹...\")\n\nif best_model_name == 'Voting Ensemble':\n    final_model = voting_clf\n    # åœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒ\n    X_all_scaled = scaler.fit_transform(X_ensemble)\n    final_model.fit(X_all_scaled, y)\n    final_predictions = final_model.predict_proba(X_test_scaled)\n    \nelif best_model_name == 'Calibrated Model':\n    final_model = calibrated_clf\n    # æ³¨æ„ï¼šCalibratedClassifierCVå·²ç»ä½¿ç”¨äº†äº¤å‰éªŒè¯ï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒ\n    final_predictions = final_model.predict_proba(X_test_scaled)\n    \nelif best_model_name == 'Weighted Average':\n    # å¯¹äºåŠ æƒå¹³å‡ï¼Œåœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹\n    print(\"åœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ç”¨äºåŠ æƒå¹³å‡...\")\n    for name, model in models.items():\n        X_all_scaled = scaler.fit_transform(X_ensemble)\n        model.fit(X_all_scaled, y)\n    final_predictions = weighted_average_predict(models, weights, X_test_scaled)\n    \nelse:\n    # å•ä¸ªåŸºç¡€æ¨¡å‹\n    final_model = models[best_model_name]\n    X_all_scaled = scaler.fit_transform(X_ensemble)\n    final_model.fit(X_all_scaled, y)\n    final_predictions = final_model.predict_proba(X_test_scaled)\n\n# =============================================================================\n# 10. ç”Ÿæˆæäº¤æ–‡ä»¶\n# =============================================================================\n\nprint(\"ç”Ÿæˆæäº¤æ–‡ä»¶...\")\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': final_predictions[:, 0],\n    'winner_model_b': final_predictions[:, 1],\n    'winner_tie': final_predictions[:, 2],\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"é«˜çº§é›†æˆæ¨¡å‹æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: submission_advanced_ensemble.csv\")\n\nprint(\"\\næäº¤æ–‡ä»¶é¢„è§ˆ:\")\nprint(submission.head())\n\n# =============================================================================\n# 11. è¯¯å·®åˆ†æ (ä¿®å¤åçš„ç‰ˆæœ¬)\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"è¯¯å·®åˆ†æ\")\nprint(\"=\"*50)\n\n# ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„é¢„æµ‹è¿›è¡Œåˆ†æ\nif best_model_name == 'Weighted Average':\n    val_predictions_final = weighted_average_predict(models, weights, X_val_scaled)\nelse:\n    val_predictions_final = final_model.predict_proba(X_val_scaled)\n\nval_pred_labels = np.argmax(val_predictions_final, axis=1)\n\n# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„log loss (ä¿®å¤ï¼šæ·»åŠ labelså‚æ•°)\nclass_names = ['model_a', 'model_b', 'tie']\nprint(\"\\nå„ç±»åˆ«Log Lossåˆ†æ:\")\nfor class_idx in range(3):\n    class_mask = (y_val == class_idx)\n    if np.sum(class_mask) > 0:\n        # ä¿®å¤ï¼šæ·»åŠ labelså‚æ•°ï¼Œæ˜ç¡®æŒ‡å®šæ‰€æœ‰å¯èƒ½çš„ç±»åˆ«\n        class_loss = log_loss(y_val[class_mask], val_predictions_final[class_mask], labels=[0, 1, 2])\n        print(f\"  ç±»åˆ« {class_names[class_idx]}: {class_loss:.4f}\")\n    else:\n        print(f\"  ç±»åˆ« {class_names[class_idx]}: æ— æ ·æœ¬\")\n\n# æ··æ·†çŸ©é˜µ\ncm = confusion_matrix(y_val, val_pred_labels)\nprint(f\"\\næ··æ·†çŸ©é˜µ:\")\nprint(cm)\n\n# å‡†ç¡®ç‡\naccuracy = np.mean(y_val == val_pred_labels)\nprint(f\"\\néªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}\")\n\n# =============================================================================\n# 12. ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Step 3 å®Œæˆæ€»ç»“\")\nprint(\"=\"*50)\n\nprint(\"Step 3 é«˜çº§é›†æˆæ¨¡å‹å®Œæˆ!\")\nprint(f\"æœ€ä½³æ¨¡å‹: {best_model_name}\")\nprint(f\"éªŒè¯é›†Log Loss: {best_score:.4f}\")\nprint(f\"éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}\")\nprint(\"æäº¤æ–‡ä»¶: submission_advanced_ensemble.csv\")\n\nprint(\"\\nè¯·å°†æ­¤æ–‡ä»¶æäº¤åˆ°KaggleæŸ¥çœ‹æœ€ç»ˆåˆ†æ•°!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T14:33:15.474879Z","iopub.execute_input":"2025-11-06T14:33:15.476105Z","iopub.status.idle":"2025-11-06T14:57:50.422862Z","shell.execute_reply.started":"2025-11-06T14:33:15.476077Z","shell.execute_reply":"2025-11-06T14:57:50.422232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #Part 4: Error Analysis","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import log_loss, confusion_matrix, classification_report, accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\" * 70)\nprint(\"ERROR ANALYSIS - PART 4\")\nprint(\"Based on Baseline Model and Embedding Model Actual Results\")\nprint(\"=\" * 70)\n\n# Set style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Use your actual results\nbaseline_results = {\n    'log_loss': 1.069314343567316,\n    'accuracy': 0.4358,\n    'precision': [0.4299, 0.4427, 0.4333],\n    'recall': [0.6272, 0.5838, 0.0557],\n    'f1_score': [0.5101, 0.5036, 0.0988],\n    'support': [4013, 3931, 3552]\n}\n\nfinal_model_results = {\n    'log_loss': 1.0210,                # <-- æ¥è‡ªæ‚¨çš„ best_score\n    'accuracy': 0.4856,                # <-- æ¥è‡ªæ‚¨çš„ accuracy\n    'precision': [0.495, 0.498, 0.452],# <-- æˆ‘ä»¬åˆšåˆšè®¡ç®—å‡ºçš„\n    'recall': [0.544, 0.546, 0.353],   # <-- æˆ‘ä»¬åˆšåˆšè®¡ç®—å‡ºçš„ (Tieå¬å›ç‡æå‡è‡³ 35.3%!)\n    'f1_score': [0.518, 0.521, 0.396], # <-- æˆ‘ä»¬åˆšåˆšè®¡ç®—å‡ºçš„\n    'support': [4013, 3931, 3552]      # éªŒè¯é›†æ ·æœ¬åˆ†å¸ƒï¼Œä¿æŒä¸å˜\n}\n\nembedding_results = final_model_results \n\n# Main analysis function\ndef perform_comprehensive_analysis():\n    \"\"\"Comprehensive analysis based on actual results\"\"\"\n    \n    print(\"\\n COMPREHENSIVE MODEL PERFORMANCE ANALYSIS\")\n    print(\"=\" * 50)\n    \n    # 1. Basic comparison\n    print(f\"\\n1. Basic Metrics Comparison:\")\n    print(f\"   Baseline Model Log Loss: {baseline_results['log_loss']:.6f}\")\n    print(f\"   Embedding Model Log Loss: {embedding_results['log_loss']:.6f}\")\n    improvement = baseline_results['log_loss'] - embedding_results['log_loss']\n    print(f\"   Improvement: {improvement:.6f}\")\n    print(f\"   Relative Improvement: {improvement / baseline_results['log_loss'] * 100:.2f}%\")\n    \n    # 2. Key problem identification\n    print(f\"\\n2. Key Problem Identification:\")\n    \n    # Serious issue with Tie class\n    tie_recall = baseline_results['recall'][2]\n    if tie_recall < 0.1:\n        print(f\"    CRITICAL ISSUE: Extremely low recall for Tie class ({tie_recall:.3f})\")\n        print(f\"       - Model almost cannot identify tie situations\")\n        print(f\"       - Tie samples are mostly misclassified as A Wins or B Wins\")\n    \n    # Limited improvement from embedding model\n    if abs(improvement) < 0.01:\n        print(f\"     Limited improvement from embedding model\")\n        print(f\"       - Traditional feature engineering remains effective\")\n        print(f\"       - MiniLM embeddings may not fully capture preference judgment information\")\n    \n    # 3. Detailed class-wise analysis\n    print(f\"\\n3. Class-wise Performance Analysis:\")\n    class_names = ['A Wins', 'B Wins', 'Tie']\n    \n    for i, name in enumerate(class_names):\n        precision = baseline_results['precision'][i]\n        recall = baseline_results['recall'][i]\n        f1 = baseline_results['f1_score'][i]\n        support = baseline_results['support'][i]\n        \n        status = \"âœ… Good\" if recall > 0.5 else \"âš ï¸ Needs Improvement\" if recall > 0.3 else \"âŒ Critical Issue\"\n        print(f\"   {name:8s}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, Support={support} {status}\")\n\ndef create_detailed_visualizations():\n    \"\"\"Create detailed error analysis visualizations\"\"\"\n    \n    print(f\"\\nğŸ“ˆ Generating Visualization Analysis Charts...\")\n    \n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle('Error Analysis - Baseline Model Detailed Diagnosis', fontsize=16, fontweight='bold')\n    \n    # 1. Model performance comparison\n    models = ['Baseline', 'Embedding']\n    losses = [baseline_results['log_loss'], embedding_results['log_loss']]\n    \n    bars = axes[0, 0].bar(models, losses, color=['skyblue', 'lightcoral'], alpha=0.8, edgecolor='black')\n    axes[0, 0].set_ylabel('Log Loss')\n    axes[0, 0].set_title('Model Performance Comparison (Log Loss)')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Add value labels\n    for bar, loss in zip(bars, losses):\n        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n                       f'{loss:.4f}', ha='center', va='bottom', fontweight='bold')\n    \n    # 2. Class-wise performance radar chart\n    class_names = ['A Wins', 'B Wins', 'Tie']\n    \n    # Prepare data\n    precision_data = baseline_results['precision']\n    recall_data = baseline_results['recall']\n    f1_data = baseline_results['f1_score']\n    \n    x = np.arange(len(class_names))\n    width = 0.25\n    \n    axes[0, 1].bar(x - width, precision_data, width, label='Precision', alpha=0.8)\n    axes[0, 1].bar(x, recall_data, width, label='Recall', alpha=0.8)\n    axes[0, 1].bar(x + width, f1_data, width, label='F1-Score', alpha=0.8)\n    \n    axes[0, 1].set_xlabel('Class')\n    axes[0, 1].set_ylabel('Score')\n    axes[0, 1].set_title('Detailed Class-wise Performance Comparison')\n    axes[0, 1].set_xticks(x)\n    axes[0, 1].set_xticklabels(class_names)\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    axes[0, 1].set_ylim(0, 1)\n    \n    # 3. Recall problem focus analysis\n    recall_data = baseline_results['recall']\n    colors = ['green' if x > 0.5 else 'orange' if x > 0.1 else 'red' for x in recall_data]\n    \n    bars = axes[1, 0].bar(class_names, recall_data, color=colors, alpha=0.8, edgecolor='black')\n    axes[1, 0].set_ylabel('Recall')\n    axes[1, 0].set_title('Class-wise Recall Analysis - Key Problem Identification')\n    axes[1, 0].set_ylim(0, 1)\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Add value labels and problem markers\n    for bar, recall in zip(bars, recall_data):\n        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n                       f'{recall:.3f}', ha='center', va='bottom', fontweight='bold')\n        if recall < 0.1:\n            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, \n                           'CRITICAL ISSUE!\\nRecall<10%', ha='center', va='center', \n                           fontweight='bold', color='white', fontsize=10)\n    \n    # 4. Sample distribution vs performance relationship\n    supports = baseline_results['support']\n    recalls = baseline_results['recall']\n    \n    scatter = axes[1, 1].scatter(supports, recalls, s=100, c=recalls, cmap='RdYlGn', alpha=0.7)\n    axes[1, 1].set_xlabel('Sample Count (Support)')\n    axes[1, 1].set_ylabel('Recall')\n    axes[1, 1].set_title('Sample Distribution vs Recall Relationship')\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    # Add class labels\n    for i, (support, recall, name) in enumerate(zip(supports, recalls, class_names)):\n        axes[1, 1].annotate(name, (support, recall), xytext=(5, 5), \n                           textcoords='offset points', fontweight='bold')\n    \n    plt.colorbar(scatter, ax=axes[1, 1], label='Recall')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef generate_confusion_analysis():\n    \"\"\"Confusion matrix analysis based on results\"\"\"\n    \n    print(f\"\\n Confusion Matrix Analysis\")\n    print(\"=\" * 50)\n    \n    # Infer confusion matrix based on recall and support numbers\n    total_samples = sum(baseline_results['support'])\n    recalls = baseline_results['recall']\n    supports = baseline_results['support']\n    \n    print(f\"Total validation samples: {total_samples}\")\n    print(f\"Class distribution:\")\n    \n    for i, (name, support, recall) in enumerate(zip(['A Wins', 'B Wins', 'Tie'], supports, recalls)):\n        percentage = support / total_samples * 100\n        correctly_classified = int(support * recall)\n        misclassified = support - correctly_classified\n        print(f\"  {name}: {support} samples ({percentage:.1f}%), Correct: {correctly_classified}, Misclassified: {misclassified}\")\n    \n    print(f\"\\nInferred confusion patterns:\")\n    print(f\"  - Tie samples mainly misclassified as: A Wins and B Wins\")\n    print(f\"  - Some mutual misclassification between A Wins and B Wins\")\n    print(f\"  - Model has almost zero ability to identify ties\")\n\ndef performance_summary_table():\n    \"\"\"Generate performance summary table\"\"\"\n    \n    print(f\"\\n Performance Summary Table\")\n    print(\"=\" * 70)\n    print(f\"{'Metric':<20} {'Baseline':<12} {'Embedding':<12} {'Status':<15}\")\n    print(\"-\" * 70)\n    \n    metrics = [\n        (\"Log Loss\", \n         f\"{baseline_results['log_loss']:.4f}\", \n         f\"{embedding_results['log_loss']:.4f}\", \n         \"Slight Improvement\" if embedding_results['log_loss'] < baseline_results['log_loss'] else \"No Improvement\"),\n        \n        (\"Accuracy\", \n         f\"{baseline_results['accuracy']:.4f}\", \n         \"N/A\", \n         \"Needs Improvement\"),\n        \n        (\"Precision (A)\", \n         f\"{baseline_results['precision'][0]:.4f}\", \n         \"N/A\", \n         \"Average\"),\n        \n        (\"Recall (A)\", \n         f\"{baseline_results['recall'][0]:.4f}\", \n         \"N/A\", \n         \"Good\"),\n        \n        (\"F1 (A)\", \n         f\"{baseline_results['f1_score'][0]:.4f}\", \n         \"N/A\", \n         \"Average\"),\n        \n        (\"Precision (B)\", \n         f\"{baseline_results['precision'][1]:.4f}\", \n         \"N/A\", \n         \"Average\"),\n        \n        (\"Recall (B)\", \n         f\"{baseline_results['recall'][1]:.4f}\", \n         \"N/A\", \n         \"Good\"),\n        \n        (\"F1 (B)\", \n         f\"{baseline_results['f1_score'][1]:.4f}\", \n         \"N/A\", \n         \"Average\"),\n        \n        (\"Precision (Tie)\", \n         f\"{baseline_results['precision'][2]:.4f}\", \n         \"N/A\", \n         \"Average\"),\n        \n        (\"Recall (Tie)\", \n         f\"{baseline_results['recall'][2]:.4f}\", \n         \"N/A\", \n         \"Critical Issue\"),\n        \n        (\"F1 (Tie)\", \n         f\"{baseline_results['f1_score'][2]:.4f}\", \n         \"N/A\", \n         \"Critical Issue\"),\n    ]\n    \n    for metric, baseline, embedding, status in metrics:\n        print(f\"{metric:<20} {baseline:<12} {embedding:<12} {status:<15}\")\n\ndef generate_insights_and_recommendations():\n    \"\"\"Generate deep insights and improvement recommendations\"\"\"\n    \n    print(f\"\\n Deep Insights and Improvement Recommendations\")\n    print(\"=\" * 60)\n    \n    # Key insights\n    tie_recall = baseline_results['recall'][2]\n    improvement = baseline_results['log_loss'] - embedding_results['log_loss']\n    \n    print(f\"\\n Key Insights:\")\n    print(f\"  1. Tie identification is the biggest bottleneck (Recall: {tie_recall:.1%})\")\n    print(f\"  2. Embedding model shows limited improvement over baseline (Log Loss improvement: {improvement:.4f})\")\n    print(f\"  3. A Wins and B Wins predictions are relatively balanced but need improvement\")\n    print(f\"  4. Overall accuracy is low ({baseline_results['accuracy']:.1%}), indicating challenging task\")\n    \n    print(f\"\\n Specific Improvement Recommendations:\")\n    \n    print(f\"\\n   For Tie Identification Problem:\")\n    print(f\"    â€¢ Design specialized tie detection features:\")\n    print(f\"      - Response semantic similarity calculation\")\n    print(f\"      - Response length difference thresholds\")\n    print(f\"      - Keyword matching degree comparison\")\n    print(f\"    â€¢ Use oversampling techniques to increase tie sample impact\")\n    print(f\"    â€¢ Adjust class weights to increase loss weight for tie class\")\n    print(f\"    â€¢ Try binary classification: first determine if tie, then determine A/B win\")\n    \n    print(f\"\\n   For Embedding Model Improvement:\")\n    print(f\"    â€¢ Try different pre-trained models:\")\n    print(f\"      - sentence-transformers/all-mpnet-base-v2\")\n    print(f\"      - intfloat/e5-base-v2\")\n    print(f\"      - BAAI/bge-base-en\")\n    print(f\"    â€¢ Add richer interaction features:\")\n    print(f\"      - Cosine similarity: prompt vs response_a, prompt vs response_b\")\n    print(f\"      - Similarity difference between responses\")\n    print(f\"      - Embedding vector dot product, Euclidean distance\")\n    print(f\"    â€¢ Use more complex classifiers: MLP, XGBoost, LightGBM\")\n    \n    print(f\"\\n   Overall Architecture Optimization:\")\n    print(f\"    â€¢ Feature combination: traditional features + embedding features\")\n    print(f\"    â€¢ Model ensemble:\")\n    print(f\"      - Voting ensemble: baseline model + embedding model\")\n    print(f\"      - Stacking ensemble: use multiple base model predictions as meta-features\")\n    print(f\"    â€¢ Probability calibration:\")\n    print(f\"      - Temperature Scaling\")\n    print(f\"      - Platt Scaling\")\n    print(f\"    â€¢ Domain-specific features:\")\n    print(f\"      - Response relevance scoring\")\n    print(f\"      - Factual accuracy assessment\")\n    print(f\"      - Language fluency analysis\")\n    \n    print(f\"\\n   Expected Improvement Targets:\")\n    print(f\"    â€¢ Tie recall: {tie_recall:.1%} â†’ > 30%\")\n    print(f\"    â€¢ Overall Log Loss: {baseline_results['log_loss']:.3f} â†’ < 1.0\")\n    print(f\"    â€¢ Accuracy: {baseline_results['accuracy']:.1%} â†’ > 50%\")\n    print(f\"    â€¢ Kaggle ranking: significant improvement\")\n\ndef calculate_expected_improvements():\n    \"\"\"Calculate expected improvement effects\"\"\"\n    \n    print(f\"\\n Expected Improvement Effect Analysis\")\n    print(\"=\" * 50)\n    \n    current_tie_recall = baseline_results['recall'][2]\n    current_accuracy = baseline_results['accuracy']\n    current_loss = baseline_results['log_loss']\n    \n    print(f\"Current Status:\")\n    print(f\"  â€¢ Tie recall: {current_tie_recall:.1%}\")\n    print(f\"  â€¢ Overall accuracy: {current_accuracy:.1%}\")\n    print(f\"  â€¢ Log Loss: {current_loss:.3f}\")\n    \n    print(f\"\\nImprovement Scenario Analysis:\")\n    \n    # Scenario 1: Tie recall improved to 30%\n    if current_tie_recall < 0.3:\n        expected_improvement = (0.3 - current_tie_recall) * baseline_results['support'][2] / sum(baseline_results['support'])\n        new_accuracy = current_accuracy + expected_improvement\n        print(f\"  Scenario 1 - Tie recall improved to 30%:\")\n        print(f\"    â€¢ Expected accuracy improvement: {expected_improvement:.3f}\")\n        print(f\"    â€¢ New accuracy: {new_accuracy:.3f} ({new_accuracy-current_accuracy:+.3f})\")\n    \n    # Scenario 2: Log Loss reduced to 1.0\n    if current_loss > 1.0:\n        loss_improvement = current_loss - 1.0\n        print(f\"  Scenario 2 - Log Loss reduced to 1.0:\")\n        print(f\"    â€¢ Required improvement: {loss_improvement:.3f}\")\n        print(f\"    â€¢ Relative improvement: {loss_improvement/current_loss*100:.1f}%\")\n\n# Main execution function\ndef main():\n    \"\"\"Main execution function\"\"\"\n    \n    print(\"Starting Part 4: Error Analysis Based on Actual Results\")\n    \n    # 1. Comprehensive analysis\n    perform_comprehensive_analysis()\n    \n    # 2. Visualization analysis\n    create_detailed_visualizations()\n    \n    # 3. Confusion matrix analysis\n    generate_confusion_analysis()\n    \n    # 4. Performance summary table\n    performance_summary_table()\n    \n    # 5. Deep insights and recommendations\n    generate_insights_and_recommendations()\n    \n    # 6. Expected improvement analysis\n    calculate_expected_improvements()\n    \n    print(f\"\\n\" + \"=\"*70)\n    print(\" Part 4: Error Analysis Completed!\")\n    print(\"=\"*70)\n    print(\"\\nMain Conclusions:\")\n    print(\"  â€¢ Tie identification is the biggest bottleneck (recall only 5.6%)\")\n    print(\"  â€¢ Embedding model shows limited improvement over baseline\")\n    print(\"  â€¢ Need specialized optimization for tie class\")\n    print(\"  â€¢ Recommend trying richer feature engineering and model architectures\")\n\n# Execute main function\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:14:24.518106Z","iopub.execute_input":"2025-11-07T09:14:24.518505Z","iopub.status.idle":"2025-11-07T09:14:25.433201Z","shell.execute_reply.started":"2025-11-07T09:14:24.51848Z","shell.execute_reply":"2025-11-07T09:14:25.432461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Final Model (TF-IDF + Embedding + Calibration + Ensemble)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, string, numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\nTRAIN_PATH = '/kaggle/input/llm-classification-finetuning/train.csv'\nTEST_PATH  = '/kaggle/input/llm-classification-finetuning/test.csv'\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\n\ny = train[['winner_model_a','winner_model_b','winner_tie']].values.argmax(axis=1)\n\ntrain_idx, val_idx = train_test_split(\n    np.arange(len(train)),\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\ny_train, y_val = y[train_idx], y[val_idx]\n\ndef create_features(df):\n    out = pd.DataFrame(index=df.index)\n    out['prompt_length']      = df['prompt'].astype(str).str.len()\n    out['response_a_length']  = df['response_a'].astype(str).str.len()\n    out['response_b_length']  = df['response_b'].astype(str).str.len()\n\n    out['prompt_word_count']     = df['prompt'].astype(str).str.split().str.len()\n    out['response_a_word_count'] = df['response_a'].astype(str).str.split().str.len()\n    out['response_b_word_count'] = df['response_b'].astype(str).str.split().str.len()\n\n    out['prompt_punc_count']     = df['prompt'].astype(str).str.count(f'[{string.punctuation}]')\n    out['response_a_punc_count'] = df['response_a'].astype(str).str.count(f'[{string.punctuation}]')\n    out['response_b_punc_count'] = df['response_b'].astype(str).str.count(f'[{string.punctuation}]')\n\n    out['response_length_diff'] = out['response_a_length'] - out['response_b_length']\n    out['response_word_diff']   = out['response_a_word_count'] - out['response_b_word_count']\n    return out\n\nX_all_base  = create_features(train)\nX_test_base = create_features(test)\n\nXtr_base = X_all_base.iloc[train_idx]\nXva_base = X_all_base.iloc[val_idx]\n\nscaler = StandardScaler()\nXtrb = scaler.fit_transform(Xtr_base)\nXvab = scaler.transform(Xva_base)\nXteb  = scaler.transform(X_test_base)\n\nclf_base = LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr')\nclf_base.fit(Xtrb, y_train)\n\nproba_val_base = clf_base.predict_proba(Xvab)\nproba_test_base = clf_base.predict_proba(Xteb)\nll_base = log_loss(y_val, proba_val_base)\nprint(f\"[Baseline] Val logloss: {ll_base:.6f}\")\n\nuse_embedding = True\nproba_val_emb = None\nproba_test_emb = None\n\nif use_embedding:\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        def concat_text(df):\n            # ç®€å•æ‹¼æ¥ prompt + response_a / response_b\n            a = (df['prompt'].astype(str) + ' ' + df['response_a'].astype(str)).tolist()\n            b = (df['prompt'].astype(str) + ' ' + df['response_b'].astype(str)).tolist()\n            return a, b\n\n        # CORRECT: model_path and st_model are now INSIDE the try block\n        model_path = '/kaggle/input/minilm-l12-v2-local/other/default/1/minilm_l12_v2_local'\n        st_model = SentenceTransformer(model_path)\n\n        a_all, b_all = concat_text(train)\n        a_test, b_test = concat_text(test)\n\n        a_tr = [a_all[i] for i in train_idx]\n        b_tr = [b_all[i] for i in train_idx]\n        a_va = [a_all[i] for i in val_idx]\n        b_va = [b_all[i] for i in val_idx]\n\n        emb_a_tr = st_model.encode(a_tr, batch_size=32, show_progress_bar=True)\n        emb_b_tr = st_model.encode(b_tr, batch_size=32, show_progress_bar=True)\n        Xtr_emb  = np.hstack([emb_a_tr, emb_b_tr, emb_a_tr - emb_b_tr])\n\n        emb_a_va = st_model.encode(a_va, batch_size=32, show_progress_bar=True)\n        emb_b_va = st_model.encode(b_va, batch_size=32, show_progress_bar=True)\n        Xva_emb  = np.hstack([emb_a_va, emb_b_va, emb_a_va - emb_b_va])\n\n        clf_emb = LogisticRegression(max_iter=200, random_state=42)\n        clf_emb.fit(Xtr_emb, y_train)\n        proba_val_emb = clf_emb.predict_proba(Xva_emb)\n        ll_emb = log_loss(y_val, proba_val_emb)\n        print(f\"[Embedding] Val logloss: {ll_emb:.6f}\")\n\n        # test\n        emb_a_te = st_model.encode(a_test, batch_size=32, show_progress_bar=True)\n        emb_b_te = st_model.encode(b_test, batch_size=32, show_progress_bar=True)\n        Xte_emb  = np.hstack([emb_a_te, emb_b_te, emb_a_te - emb_b_te])\n        proba_test_emb = clf_emb.predict_proba(Xte_emb)\n\n    # CORRECT: The 'except' now correctly aligns with the 'try'\n    except Exception as e:\n        print(\"[Embedding] åŠ è½½/æ¨ç†å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨ Baselineã€‚é”™è¯¯ï¼š\", repr(e))\n        use_embedding = False\n\nif use_embedding and (proba_val_emb is not None):\n    best_alpha, best_ll = 0.5, 1e9\n    for alpha in np.linspace(0, 1, 21):  # 0.00, 0.05, ..., 1.00\n        blend_val = alpha*proba_val_base + (1-alpha)*proba_val_emb\n        ll = log_loss(y_val, blend_val)\n        if ll < best_ll:\n            best_ll = ll\n            best_alpha = alpha\n    print(f\"[Ensemble] Best alpha={best_alpha:.2f}, Val logloss={best_ll:.6f}\")\n\n    proba_test_final = best_alpha*proba_test_base + (1-best_alpha)*proba_test_emb\nelse:\n    print(\"[Ensemble] ä»…ä½¿ç”¨ Baseline ç»“æœã€‚\")\n    proba_test_final = proba_test_base\n\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': proba_test_final[:, 0],\n    'winner_model_b': proba_test_final[:, 1],\n    'winner_tie':     proba_test_final[:, 2],\n})\nsubmission.to_csv('submission_f.csv', index=False)\nprint(\"Saved -> submission_f.csv\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:23:09.877859Z","iopub.execute_input":"2025-11-07T09:23:09.878402Z","iopub.status.idle":"2025-11-07T09:28:25.654813Z","shell.execute_reply.started":"2025-11-07T09:23:09.878374Z","shell.execute_reply":"2025-11-07T09:28:25.654256Z"}},"outputs":[],"execution_count":null}]}