{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":129073,"sourceType":"modelInstanceVersion","modelInstanceId":108753,"modelId":133071}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#############################################################\n# Code Block 1: Environment Setup and Data Loading\n#############################################################\n\nimport warnings\nwarnings.filterwarnings('ignore', message='.*overflowing tokens.*')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Set random seed for reproducibility\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Load the pre-trained BERT tokenizer\nmodel_path = \"/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_path)\n\n# Load training and test data\ntrain_data = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\nprint(\"Training data shape:\", train_data.shape)\nprint(\"Test data shape:\", test_data.shape)\n\n#############################################################\n# Code Block 2: Data Preprocessing\n#############################################################\n\ndef tokenize_texts(df, tokenizer, max_length=512):\n    \"\"\"\n    Tokenizes the input text and converts them into numerical format\n    \"\"\"\n    encoding_a = tokenizer(\n        df[\"response_a\"].tolist(),\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n    encoding_b = tokenizer(\n        df[\"response_b\"].tolist(),\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n    return {\n        \"input_ids_a\": encoding_a[\"input_ids\"],\n        \"attention_mask_a\": encoding_a[\"attention_mask\"],\n        \"input_ids_b\": encoding_b[\"input_ids\"],\n        \"attention_mask_b\": encoding_b[\"attention_mask\"]\n    }\n\nprint(\"Preprocessing training data...\")\npreprocessed_train = tokenize_texts(train_data, tokenizer)\n\n# Extract labels\nlabels = torch.tensor(train_data[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values, dtype=torch.float32)\n\n#############################################################\n# Code Block 3: Define Dataset and DataLoader\n#############################################################\n\nclass ChatbotDataset(Dataset):\n    \"\"\"\n    Custom PyTorch dataset for chatbot classification task\n    \"\"\"\n    def __init__(self, preprocessed_data, labels=None):\n        self.input_ids_a = preprocessed_data[\"input_ids_a\"]\n        self.attention_mask_a = preprocessed_data[\"attention_mask_a\"]\n        self.input_ids_b = preprocessed_data[\"input_ids_b\"]\n        self.attention_mask_b = preprocessed_data[\"attention_mask_b\"]\n        self.labels = labels if labels is not None else torch.zeros(len(self.input_ids_a), 3)  # Avoid NoneType error\n\n    def __len__(self):\n        return len(self.input_ids_a)\n\n    def __getitem__(self, idx):\n        batch = {\n            \"input_ids_a\": self.input_ids_a[idx],\n            \"attention_mask_a\": self.attention_mask_a[idx],\n            \"input_ids_b\": self.input_ids_b[idx],\n            \"attention_mask_b\": self.attention_mask_b[idx],\n        }\n        if self.labels is not None:\n            batch[\"label\"] = self.labels[idx]\n        return batch\n\ntrain_dataset = ChatbotDataset(preprocessed_train, labels)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n\n#############################################################\n# Code Block 4: Define the Multi-Task BERT Model\n#############################################################\n\nclass MultiTaskBERTModel(nn.Module):\n    \"\"\"\n    Multi-task classification model using BERT as a backbone\n    \"\"\"\n    def __init__(self, model_path, num_labels=3):\n        super(MultiTaskBERTModel, self).__init__()\n        self.bert = BertForSequenceClassification.from_pretrained(\n            model_path, \n            num_labels=num_labels, \n            ignore_mismatched_sizes=True\n        )\n        self.bert.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)  # Reinitialize classifier layer\n        nn.init.xavier_uniform_(self.bert.classifier.weight)  # Reinitialize weights properly\n        nn.init.zeros_(self.bert.classifier.bias)\n    \n    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n        output_a = self.bert(input_ids=input_ids_a, attention_mask=attention_mask_a).logits\n        output_b = self.bert(input_ids=input_ids_b, attention_mask=attention_mask_b).logits\n        return output_a, output_b\n\nmodel = MultiTaskBERTModel(model_path, num_labels=3)\nmodel.to(device)\n\n#############################################################\n# Code Block 5: Generate Submission File\n#############################################################\n\n# Model predictions\nprint(\"Generating predictions...\")\ntest_preprocessed = tokenize_texts(test_data, tokenizer)\ntest_dataset = ChatbotDataset(test_preprocessed, labels=None)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\nlogits_a, logits_b = [], []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items() if \"input_ids\" in k or \"attention_mask\" in k}\n        out_a, out_b = model(**batch)\n        logits_a.append(out_a.cpu())\n        logits_b.append(out_b.cpu())\n\nlogits_a = torch.cat(logits_a, dim=0)\nlogits_b = torch.cat(logits_b, dim=0)\nlogits_tie = torch.zeros_like(logits_a[:, 0])  # Ensure 1D array for proper submission formatting\n\n# Normalize scores\nlogits = torch.stack([logits_a[:, 0], logits_b[:, 0], logits_tie], dim=1)\nprobs = F.softmax(logits, dim=1).numpy()\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    \"id\": test_data[\"id\"].values,\n    \"winner_model_a\": probs[:, 0],\n    \"winner_model_b\": probs[:, 1],\n    \"winner_tie\": probs[:, 2]\n})\n\n# Save the submission file\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file generated: submission.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:07:26.939251Z","iopub.execute_input":"2025-03-09T13:07:26.939756Z","iopub.status.idle":"2025-03-09T13:20:28.125035Z","shell.execute_reply.started":"2025-03-09T13:07:26.939718Z","shell.execute_reply":"2025-03-09T13:20:28.123847Z"}},"outputs":[],"execution_count":null}]}