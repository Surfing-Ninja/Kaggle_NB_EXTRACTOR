{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":86518,"databundleVersionId":9809560},{"sourceType":"datasetVersion","sourceId":10066622,"datasetId":6204120,"databundleVersionId":10342673},{"sourceType":"modelInstanceVersion","sourceId":188799,"databundleVersionId":10386360,"modelInstanceId":160388},{"sourceType":"modelInstanceVersion","sourceId":188130,"databundleVersionId":10379056,"modelInstanceId":160388}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inspecting Files","metadata":{}},{"cell_type":"code","source":"import os\nprint(\"Available files:\", os.listdir(\"/kaggle/input/llm-classification-finetuning\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:32:53.753686Z","iopub.execute_input":"2024-12-05T07:32:53.754064Z","iopub.status.idle":"2024-12-05T07:32:53.758993Z","shell.execute_reply.started":"2024-12-05T07:32:53.754034Z","shell.execute_reply":"2024-12-05T07:32:53.758224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install","metadata":{"id":"0mcfzHuPh5pK"}},{"cell_type":"code","source":"import pkg_resources\npackages = ['transformers', 'torch', 'tqdm', 'scikit-learn']\nfor package in packages:\n    try:\n        version = pkg_resources.get_distribution(package).version\n        print(f\"{package}: {version}\")\n    except:\n        print(f\"{package} not installed\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:32:53.760944Z","iopub.execute_input":"2024-12-05T07:32:53.761869Z","iopub.status.idle":"2024-12-05T07:32:53.982719Z","shell.execute_reply.started":"2024-12-05T07:32:53.761826Z","shell.execute_reply":"2024-12-05T07:32:53.9818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install transformers\n\n# !pip install torch\n\n# !pip install tqdm\n\n# !pip install scikit-learn","metadata":{"id":"ZvYJxgwmh7Ec","outputId":"91aff71d-2fb4-4ccb-c0ed-32b751386784","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:32:53.984115Z","iopub.execute_input":"2024-12-05T07:32:53.984615Z","iopub.status.idle":"2024-12-05T07:32:53.988251Z","shell.execute_reply.started":"2024-12-05T07:32:53.984508Z","shell.execute_reply":"2024-12-05T07:32:53.987435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Device: {torch.cuda.get_device_name()}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:32:53.989148Z","iopub.execute_input":"2024-12-05T07:32:53.989374Z","iopub.status.idle":"2024-12-05T07:32:59.588743Z","shell.execute_reply.started":"2024-12-05T07:32:53.989351Z","shell.execute_reply":"2024-12-05T07:32:59.587905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nimport numpy as np\n\nfrom transformers import AutoTokenizer, AutoModel\n\nimport torch\n\nfrom torch import nn\n\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\n\nfrom sklearn.metrics import log_loss\n\n\n# Enable memory efficient settings\n\ntorch.backends.cudnn.benchmark = False\n\ntorch.backends.cuda.matmul.allow_tf32 = False\n\ntorch.backends.cudnn.deterministic = True\n\n\n\n# Verify CUDA is still available after imports\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"id":"eUHjPUBjh8KP","outputId":"9b0a4573-8a74-484d-fdf3-8581db7ac3d0","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:32:59.591765Z","iopub.execute_input":"2024-12-05T07:32:59.592358Z","iopub.status.idle":"2024-12-05T07:33:06.043731Z","shell.execute_reply.started":"2024-12-05T07:32:59.592324Z","shell.execute_reply":"2024-12-05T07:33:06.042734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Environment Check","metadata":{"id":"_Ys_4KKwiEFe"}},{"cell_type":"code","source":"# Check GPU availability and memory\n\n!nvidia-smi\n\n\nimport torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nprint(f\"Current device: {torch.cuda.current_device()}\")\n\nprint(f\"Device name: {torch.cuda.get_device_name()}\")\n\n\n# Clear any existing cache\n\ntorch.cuda.empty_cache()","metadata":{"id":"2pDOxvyWiEkY","outputId":"c2c8f0ac-fc20-44e1-d813-80490ec1f58a","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:33:06.04499Z","iopub.execute_input":"2024-12-05T07:33:06.045868Z","iopub.status.idle":"2024-12-05T07:33:07.189475Z","shell.execute_reply.started":"2024-12-05T07:33:06.045823Z","shell.execute_reply":"2024-12-05T07:33:07.188373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import and Environment Check","metadata":{"id":"KPFh9qCd7L2T"}},{"cell_type":"code","source":"# Step 1: Imports\n\nimport os\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom transformers import AutoTokenizer, AutoModel\n\nimport torch\n\nfrom torch import nn\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\n\nimport torch.nn.functional as F\n\nfrom sklearn.metrics import log_loss\n\n\n\n# Clear GPU memory\n\ntorch.cuda.empty_cache()","metadata":{"id":"a3iYxUUX7KyC","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:33:07.191049Z","iopub.execute_input":"2024-12-05T07:33:07.191369Z","iopub.status.idle":"2024-12-05T07:33:07.217758Z","shell.execute_reply.started":"2024-12-05T07:33:07.191337Z","shell.execute_reply":"2024-12-05T07:33:07.217162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Data Loading\n\ndata_path1 = '/kaggle/input/llm-classification-finetuning'\n\nfile_name1 = 'train.csv'\n\nfile_name2 = 'test.csv'\n\nfile_path1 = os.path.join(data_path1, file_name1)\n\nfile_path2 = os.path.join(data_path1, file_name2)\n\n\n\ndf_train = pd.read_csv(file_path1)\n\ndf_test = pd.read_csv(file_path2)\n\n# df_train\n# df_test","metadata":{"id":"d0Gt1_wU7bjI","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:07.218592Z","iopub.execute_input":"2024-12-05T07:33:07.21881Z","iopub.status.idle":"2024-12-05T07:33:10.823511Z","shell.execute_reply.started":"2024-12-05T07:33:07.218788Z","shell.execute_reply":"2024-12-05T07:33:10.822455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Updating packages fix","metadata":{}},{"cell_type":"code","source":"# !pip install --upgrade transformers\n# !pip install --upgrade ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:10.824732Z","iopub.execute_input":"2024-12-05T07:33:10.824991Z","iopub.status.idle":"2024-12-05T07:33:10.828673Z","shell.execute_reply.started":"2024-12-05T07:33:10.824966Z","shell.execute_reply":"2024-12-05T07:33:10.827728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip uninstall -y transformers tokenizers\n# !pip install transformers>=4.33.1\n# import transformers\n# print(f\"Transformers version: {transformers.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:10.829748Z","iopub.execute_input":"2024-12-05T07:33:10.830061Z","iopub.status.idle":"2024-12-05T07:33:10.840369Z","shell.execute_reply.started":"2024-12-05T07:33:10.830026Z","shell.execute_reply":"2024-12-05T07:33:10.839584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In a notebook with internet access\n# save_path = '/kaggle/working/model_files'\n# os.makedirs(save_path, exist_ok=True)\n\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n# model = AutoModel.from_pretrained(\"microsoft/deberta-v3-small\")\n\n# tokenizer.save_pretrained(f'{save_path}/deberta-tokenizer')\n# model.save_pretrained(f'{save_path}/deberta-model')\n\n# Create dataset version in Kaggle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:10.84139Z","iopub.execute_input":"2024-12-05T07:33:10.841655Z","iopub.status.idle":"2024-12-05T07:33:10.852115Z","shell.execute_reply.started":"2024-12-05T07:33:10.841631Z","shell.execute_reply":"2024-12-05T07:33:10.851309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# download cache\n# Add this in your development environment before going offline\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n# model = AutoModel.from_pretrained(\"microsoft/deberta-v3-small\")\n# # Save them locally\n# tokenizer.save_pretrained(\"./deberta-tokenizer\")\n# model.save_pretrained(\"./deberta-model\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:33:10.853067Z","iopub.execute_input":"2024-12-05T07:33:10.853332Z","iopub.status.idle":"2024-12-05T07:33:10.863171Z","shell.execute_reply.started":"2024-12-05T07:33:10.853306Z","shell.execute_reply":"2024-12-05T07:33:10.862213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# print(f\"Does directory exist? {os.path.exists('/kaggle/working/model_files')}\")\n# print(\"\\nDirectory contents:\")\n# !ls -la /kaggle/working/model_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:10.864295Z","iopub.execute_input":"2024-12-05T07:33:10.864601Z","iopub.status.idle":"2024-12-05T07:33:10.874583Z","shell.execute_reply.started":"2024-12-05T07:33:10.864572Z","shell.execute_reply":"2024-12-05T07:33:10.873674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save_path = '/kaggle/working/model_files'\n# print(f\"1. Creating directory at {save_path}\")\n# os.makedirs(save_path, exist_ok=True)\n\n# print(\"\\n2. Downloading tokenizer...\")\n# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n# print(\"Tokenizer downloaded successfully\")\n\n# print(\"\\n3. Downloading model...\")\n# model = AutoModel.from_pretrained(\"microsoft/deberta-v3-small\")\n# print(\"Model downloaded successfully\")\n\n# print(\"\\n4. Saving tokenizer...\")\n# tokenizer.save_pretrained(f'{save_path}/deberta-tokenizer')\n# print(\"Tokenizer saved\")\n\n# print(\"\\n5. Saving model...\")\n# model.save_pretrained(f'{save_path}/deberta-model')\n# print(\"Model saved\")\n\n# print(\"\\n6. Final directory structure:\")\n# !ls -R /kaggle/working/model_files","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:33:10.877576Z","iopub.execute_input":"2024-12-05T07:33:10.878084Z","iopub.status.idle":"2024-12-05T07:33:10.885921Z","shell.execute_reply.started":"2024-12-05T07:33:10.878045Z","shell.execute_reply":"2024-12-05T07:33:10.885317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check file sizes\n# !du -h /kaggle/working/model_files/*\n\n# # Check contents of a config file\n# !cat /kaggle/working/model_files/deberta-model/config.json","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:33:10.88678Z","iopub.execute_input":"2024-12-05T07:33:10.887001Z","iopub.status.idle":"2024-12-05T07:33:10.902018Z","shell.execute_reply.started":"2024-12-05T07:33:10.886978Z","shell.execute_reply":"2024-12-05T07:33:10.901368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModel\n\n# # Try loading from saved files\n# test_tokenizer = AutoTokenizer.from_pretrained('/kaggle/working/model_files/deberta-tokenizer', local_files_only=True)\n# test_model = AutoModel.from_pretrained('/kaggle/working/model_files/deberta-model', local_files_only=True)\n\n# print(\"Successfully loaded saved files!\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-05T07:33:10.902835Z","iopub.execute_input":"2024-12-05T07:33:10.903056Z","iopub.status.idle":"2024-12-05T07:33:10.912416Z","shell.execute_reply.started":"2024-12-05T07:33:10.903031Z","shell.execute_reply":"2024-12-05T07:33:10.911795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataPreprocessor","metadata":{"id":"OyzcNHV7jh1D"}},{"cell_type":"code","source":"# Step 3: Data Processor\nimport kagglehub\nfrom transformers import DebertaV2Tokenizer, DebertaV2Model, DebertaV2Config\n\nclass DataProcessor:\n   def __init__(self, max_length=128):\n        # Get model path from kagglehub\n       model_path = kagglehub.model_download(\"ziyonaressker/deberta-v3-small/transformers/v1\")\n        \n       self.tokenizer = DebertaV2Tokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n       self.max_length = max_length\n\n\n\n   def clean_text(self, text):\n\n       return text.strip('[]\"').replace('\\\\n', ' ').replace('\\\\', '')\n\n\n\n   def prepare_features(self, prompts, responses_a, responses_b):\n\n       prompts = [self.clean_text(str(p)) for p in prompts]\n\n       responses_a = [self.clean_text(str(r)) for r in responses_a]\n\n       responses_b = [self.clean_text(str(r)) for r in responses_b]\n\n\n\n       prompt_encodings = self.tokenizer(prompts, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n\n       resp_a_encodings = self.tokenizer(responses_a, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n\n       resp_b_encodings = self.tokenizer(responses_b, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n\n\n\n       return {\n\n           'prompt_ids': prompt_encodings['input_ids'],\n\n           'prompt_mask': prompt_encodings['attention_mask'],\n\n           'resp_a_ids': resp_a_encodings['input_ids'],\n\n           'resp_a_mask': resp_a_encodings['attention_mask'],\n\n           'resp_b_ids': resp_b_encodings['input_ids'],\n\n           'resp_b_mask': resp_b_encodings['attention_mask']\n\n       }","metadata":{"id":"rXl3pdWJ7_C6","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:10.913283Z","iopub.execute_input":"2024-12-05T07:33:10.913514Z","iopub.status.idle":"2024-12-05T07:33:12.288955Z","shell.execute_reply.started":"2024-12-05T07:33:10.913486Z","shell.execute_reply":"2024-12-05T07:33:12.288214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{"id":"RA45iZl6kH3g"}},{"cell_type":"code","source":"# Step 4: Model Architecture\n\nclass PreferenceClassifier(nn.Module):\n   def __init__(self):\n       super().__init__()\n       # Get model path from kagglehub\n       model_path = kagglehub.model_download(\"ziyonaressker/deberta-v3-small/transformers/v1\")\n       \n       # Load config and model\n       config = DebertaV2Config.from_pretrained(model_path)\n       self.encoder = DebertaV2Model.from_pretrained(\n            model_path,\n            config=config,\n            local_files_only=True\n        )\n\n       hidden_size = self.encoder.config.hidden_size\n\n\n       self.classifier = nn.Sequential(\n\n           nn.Linear(hidden_size * 3, hidden_size),\n\n           nn.LayerNorm(hidden_size),\n\n           nn.Dropout(0.1),\n\n           nn.ReLU(),\n\n           nn.Linear(hidden_size, 3)\n\n       )\n\n\n\n   def forward(self, prompt_ids, prompt_mask, resp_a_ids, resp_a_mask, resp_b_ids, resp_b_mask):\n\n       prompt_enc = self.encoder(prompt_ids, attention_mask=prompt_mask).last_hidden_state[:, 0, :]\n\n       resp_a_enc = self.encoder(resp_a_ids, attention_mask=resp_a_mask).last_hidden_state[:, 0, :]\n\n       resp_b_enc = self.encoder(resp_b_ids, attention_mask=resp_b_mask).last_hidden_state[:, 0, :]\n\n\n\n       combined = torch.cat([prompt_enc, resp_a_enc, resp_b_enc], dim=1)\n\n       return self.classifier(combined)","metadata":{"id":"IS1qL2m-8Cuw","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:12.289929Z","iopub.execute_input":"2024-12-05T07:33:12.290322Z","iopub.status.idle":"2024-12-05T07:33:12.297456Z","shell.execute_reply.started":"2024-12-05T07:33:12.290295Z","shell.execute_reply":"2024-12-05T07:33:12.29664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and DataLoader classes","metadata":{"id":"IB4AWsbakQQ5"}},{"cell_type":"code","source":"# Step 5: Dataset Class\n\nclass PreferenceDataset(torch.utils.data.Dataset):\n\n   def __init__(self, features, labels=None):\n\n       self.features = features\n\n       self.labels = labels\n\n\n\n   def __len__(self):\n\n       return len(self.features['prompt_ids'])\n\n\n\n   def __getitem__(self, idx):\n\n       item = {\n\n           'prompt_ids': self.features['prompt_ids'][idx],\n\n           'prompt_mask': self.features['prompt_mask'][idx],\n\n           'resp_a_ids': self.features['resp_a_ids'][idx],\n\n           'resp_a_mask': self.features['resp_a_mask'][idx],\n\n           'resp_b_ids': self.features['resp_b_ids'][idx],\n\n           'resp_b_mask': self.features['resp_b_mask'][idx]\n\n       }\n\n       if self.labels is not None:\n\n           item['labels'] = self.labels[idx]\n\n       return item","metadata":{"id":"K1LRyhPh9Lg2","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:12.29862Z","iopub.execute_input":"2024-12-05T07:33:12.298999Z","iopub.status.idle":"2024-12-05T07:33:12.315294Z","shell.execute_reply.started":"2024-12-05T07:33:12.298962Z","shell.execute_reply":"2024-12-05T07:33:12.31468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Function and Evaluation","metadata":{"id":"xHzwXHUrkpNB"}},{"cell_type":"code","source":"# Step 6: Training Functions\n\ndef create_data_loader(df, processor, batch_size=16, is_training=True):\n\n   chunk_size = 1000\n\n   all_features = []\n\n   all_labels = []\n\n\n\n   for i in range(0, len(df), chunk_size):\n\n       chunk = df.iloc[i:i+chunk_size]\n\n       features = processor.prepare_features(\n\n           chunk['prompt'].tolist(),\n\n           chunk['response_a'].tolist(),\n\n           chunk['response_b'].tolist()\n\n       )\n\n       all_features.append(features)\n\n\n\n       if is_training:\n\n           labels = torch.tensor(\n\n               chunk[['winner_model_a', 'winner_model_b', 'winner_tie']].values,\n\n               dtype=torch.float32\n\n           )\n\n           all_labels.append(labels)\n\n\n\n   combined_features = {k: torch.cat([f[k] for f in all_features]) for k in all_features[0].keys()}\n\n   dataset = PreferenceDataset(\n\n       combined_features,\n\n       torch.cat(all_labels) if is_training else None\n\n   )\n\n\n\n   return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=is_training, num_workers=0)\n\n\n\ndef compute_log_loss(y_true, y_pred):\n\n   y_true_indices = np.argmax(y_true, axis=1)\n\n   return log_loss(y_true_indices, y_pred, labels=[0, 1, 2])\n\n\n\ndef train_epoch(model, train_loader, optimizer, criterion, device, scaler):\n\n   model.train()\n\n   total_loss = 0\n\n   all_preds = []\n\n   all_labels = []\n\n\n\n   for batch in tqdm(train_loader):\n\n       batch = {k: v.to(device) for k, v in batch.items()}\n\n       labels = batch.pop('labels')\n\n       labels = torch.argmax(labels, dim=1)  # Convert one-hot to indices\n\n\n\n       optimizer.zero_grad()\n\n\n\n       with torch.amp.autocast(device_type='cuda'):\n\n           outputs = model(**batch)\n\n           loss = criterion(outputs, labels)\n\n           probs = F.softmax(outputs, dim=1)\n\n\n\n       scaler.scale(loss).backward()\n\n       torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n       scaler.step(optimizer)\n\n       scaler.update()\n\n\n\n       total_loss += loss.item()\n\n       all_preds.extend(probs.detach().cpu().numpy())\n\n       all_labels.extend(F.one_hot(labels, num_classes=3).cpu().numpy())\n\n\n\n       del outputs, loss, probs\n\n       torch.cuda.empty_cache()\n\n\n\n   return total_loss / len(train_loader), np.array(all_preds), np.array(all_labels)\n\n\n\ndef evaluate(model, dataloader, device):\n\n   model.eval()\n\n   all_preds = []\n\n   all_labels = []\n\n\n\n   with torch.no_grad():\n\n       for batch in dataloader:\n\n           batch = {k: v.to(device) for k, v in batch.items()}\n\n           labels = batch.pop('labels')\n\n           labels = torch.argmax(labels, dim=1)\n\n\n\n           with torch.amp.autocast(device_type='cuda'):\n\n               outputs = model(**batch)\n\n               probs = F.softmax(outputs, dim=1)\n\n\n\n           all_preds.extend(probs.cpu().numpy())\n\n           all_labels.extend(F.one_hot(labels, num_classes=3).cpu().numpy())\n\n\n\n   return compute_log_loss(np.array(all_labels), np.array(all_preds))\n\n\n\ndef train_model(model, train_loader, valid_loader, device, num_epochs=3):\n\n   optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n   criterion = nn.CrossEntropyLoss()\n\n   scaler = torch.amp.GradScaler()\n\n   best_logloss = float('inf')\n\n   checkpoint_path = 'model_checkpoint.pt'\n\n\n\n   for epoch in range(num_epochs):\n\n       train_loss, train_preds, train_labels = train_epoch(\n\n           model, train_loader, optimizer, criterion, device, scaler\n\n       )\n\n\n\n       train_logloss = compute_log_loss(train_labels, train_preds)\n\n       val_logloss = evaluate(model, valid_loader, device)\n\n\n\n       print(f\"Epoch {epoch+1}:\")\n\n       print(f\"Train Loss={train_loss:.4f}, Train LogLoss={train_logloss:.4f}\")\n\n       print(f\"Val LogLoss={val_logloss:.4f}\")\n\n\n\n       if val_logloss < best_logloss:\n\n           best_logloss = val_logloss\n\n           torch.save(model.state_dict(), 'best_model.pt')\n\n           print(\"Best model saved!\")\n\n\n\n       if (epoch + 1) % 1 == 0:\n\n           torch.save({\n\n               'epoch': epoch,\n\n               'model_state_dict': model.state_dict(),\n\n               'optimizer_state_dict': optimizer.state_dict(),\n\n               'best_logloss': best_logloss,\n\n           }, checkpoint_path)\n\n\n\n# def generate_submission(model, test_loader, device):\n\n#    model.eval()\n\n#    predictions = []\n\n\n\n#    with torch.no_grad():\n\n#        for batch in tqdm(test_loader):\n\n#            batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n\n#            with torch.amp.autocast(device_type='cuda'):\n\n#                outputs = model(**batch)\n\n#                probs = F.softmax(outputs, dim=1)\n\n#            predictions.extend(probs.cpu().numpy())\n\n\n\n   # predictions = np.array(predictions)\n\n   # submission = pd.DataFrame({\n\n   #     'id': df_test['id'],\n\n   #     'winner_model_a': predictions[:, 0],\n\n   #     'winner_model_b': predictions[:, 1],\n\n   #     'winner_tie': predictions[:, 2]\n\n   # })\n\n\n\n   # return submission\n\ndef generate_submission(model, test_loader, device):\n   model.eval()\n   predictions = []\n\n   with torch.no_grad():\n       for batch in tqdm(test_loader):\n           batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n           with torch.amp.autocast(device_type='cuda'):\n               outputs = model(**batch)\n               probs = F.softmax(outputs, dim=1)\n           predictions.extend(probs.cpu().numpy())\n\n   predictions = np.array(predictions)\n    \n    # Create submission exactly like the successful version\n   sub_df = df_test[[\"id\"]].copy()\n   class_names = ['winner_model_a', 'winner_model_b', 'winner_tie']\n   sub_df[class_names] = predictions\n\n    # Save with default pandas precision \n   sub_df.to_csv('submission.csv', index=False)\n    \n    # Verify format\n   print(\"\\nSubmission Preview:\")\n   print(sub_df.head())\n   print(\"\\nColumn dtypes:\", sub_df.dtypes)\n   print(\"\\nFile size:\", os.path.getsize('submission.csv'), \"bytes\")\n    \n   return sub_df\n","metadata":{"id":"_Mq1YGOu9vwr","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:12.316581Z","iopub.execute_input":"2024-12-05T07:33:12.316836Z","iopub.status.idle":"2024-12-05T07:33:12.342389Z","shell.execute_reply.started":"2024-12-05T07:33:12.316811Z","shell.execute_reply":"2024-12-05T07:33:12.34149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Training and Prediction Loop","metadata":{"id":"UphVkv9Dkt33"}},{"cell_type":"code","source":"# Step 7: Main Execution\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprocessor = DataProcessor()\n\n\n\ntrain_data, valid_data = train_test_split(df_train, test_size=0.1, random_state=42)\n\ntrain_loader = create_data_loader(train_data, processor, batch_size=16)\n\nvalid_loader = create_data_loader(valid_data, processor, batch_size=16)\n\n\n\nmodel = PreferenceClassifier()\n\nmodel = model.to(device)\n\n\n\ntrain_model(model, train_loader, valid_loader, device)\n\n\n\ntest_loader = create_data_loader(df_test, processor, batch_size=16, is_training=False)\n\ntry:\n\n   model.load_state_dict(torch.load('best_model.pt'))\n\nexcept:\n\n   print(\"Error loading model. Continuing with current weights.\")\n\n\n\nsubmission = generate_submission(model, test_loader, device)\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"id":"47_UzuqqqjkD","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:33:12.343414Z","iopub.execute_input":"2024-12-05T07:33:12.343763Z","iopub.status.idle":"2024-12-05T08:39:43.20706Z","shell.execute_reply.started":"2024-12-05T07:33:12.343733Z","shell.execute_reply":"2024-12-05T08:39:43.206187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save a Model","metadata":{}},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModel\n# import os\n\n# # Create directory in Kaggle working directory\n# LOCAL_MODEL_DIR = '/kaggle/working/deberta-v3-small'\n# os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n\n# # Download model and tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n# model = AutoModel.from_pretrained(\"microsoft/deberta-v3-small\")\n\n# # Save locally\n# tokenizer.save_pretrained(LOCAL_MODEL_DIR)\n# model.save_pretrained(LOCAL_MODEL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:39:43.208359Z","iopub.execute_input":"2024-12-05T08:39:43.208748Z","iopub.status.idle":"2024-12-05T08:39:43.213107Z","shell.execute_reply.started":"2024-12-05T08:39:43.208709Z","shell.execute_reply":"2024-12-05T08:39:43.212203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Install kagglehub if not already installed\n# !pip install --upgrade kagglehub==0.3.4\n\n# import kagglehub\n# kagglehub.login()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:39:43.214304Z","iopub.execute_input":"2024-12-05T08:39:43.215024Z","iopub.status.idle":"2024-12-05T08:39:45.498982Z","shell.execute_reply.started":"2024-12-05T08:39:43.214976Z","shell.execute_reply":"2024-12-05T08:39:45.498011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Set up directories and model info\n# LOCAL_MODEL_DIR = '/kaggle/working/deberta-v3-small'\n# MODEL_SLUG = 'deberta-v3-small'\n# VARIATION_SLUG = 'v1'  # Using v1 to indicate first version\n\n# # Create directory and download model if not already done\n# os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n# model = AutoModel.from_pretrained(\"microsoft/deberta-v3-small\")\n\n# # Save locally\n# tokenizer.save_pretrained(LOCAL_MODEL_DIR)\n# model.save_pretrained(LOCAL_MODEL_DIR)\n\n# # Upload model with version info\n# kagglehub.model_upload(\n#     handle=f\"ziyonaressker/{MODEL_SLUG}/transformers/{VARIATION_SLUG}\",\n#     local_model_dir=LOCAL_MODEL_DIR,\n#     version_notes='DeBERTa-v3-small model v1 for LLM Classification task - 2024-12-04'\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:39:45.500108Z","iopub.execute_input":"2024-12-05T08:39:45.50037Z","iopub.status.idle":"2024-12-05T08:39:46.863393Z","shell.execute_reply.started":"2024-12-05T08:39:45.500345Z","shell.execute_reply":"2024-12-05T08:39:46.862444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #verification\n# # Test downloading the model\n# test_path = kagglehub.model_download(f\"ziyonaressker/{MODEL_SLUG}/transformers/{VARIATION_SLUG}\")\n# print(f\"Model downloaded to: {test_path}\")\n\n# # Verify files exist\n# print(\"\\nFiles in downloaded directory:\")\n# print(os.listdir(test_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:39:46.865218Z","iopub.execute_input":"2024-12-05T08:39:46.865732Z","iopub.status.idle":"2024-12-05T08:39:48.73715Z","shell.execute_reply.started":"2024-12-05T08:39:46.865681Z","shell.execute_reply":"2024-12-05T08:39:48.735891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}