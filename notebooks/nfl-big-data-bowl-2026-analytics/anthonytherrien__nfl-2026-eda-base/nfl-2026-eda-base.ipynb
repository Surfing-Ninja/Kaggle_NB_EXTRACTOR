{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114250,"databundleVersionId":13838823,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End-to-End ML Workflow — Multi-File Ingestion\n\n**Date:** 2025-09-28\n\nThis notebook implements a complete ML pipeline with **points** and **sub-points**, and it automatically ingests all matching files in the provided folder:\n- Pattern for features: `input_2023_*.csv`\n- Pattern for targets: `output_2023_*.csv`\n\nIt will also include `supplementary_data.csv` if present.\n","metadata":{}},{"cell_type":"code","source":"# Import core libraries\nimport os\nimport glob\nimport math\nimport warnings\nimport typing as t\n\n# Import data stack\nimport numpy as np\nimport pandas as pd\n\n# Import plotting\nimport matplotlib.pyplot as plt\n\n# Import modeling tools\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n    mean_absolute_error, mean_squared_error, r2_score, classification_report,\n    confusion_matrix, precision_recall_curve, roc_curve\n)\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\n\n# Global display options\npd.set_option('display.max_columns', 120)\npd.set_option('display.width', 140)\nwarnings.filterwarnings('ignore')\n\n# Paths and patterns\nBASE_DIR = '/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/train'\nINPUT_GLOB = os.path.join(BASE_DIR, 'input_2023_*.csv')\nOUTPUT_GLOB = os.path.join(BASE_DIR, 'output_2023_*.csv')\nSUPPLEMENT_PATH = '/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/supplementary_data.csv'\n\n# User-editable configuration\nTARGET_COLUMN = None\nID_COLUMNS = None\nJOIN_KEYS = None\nTASK_TYPE = None\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\nCV_FOLDS = 5\nMAX_EDA_PLOTS = 15\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1) Problem Definition — Goal and Success Criteria\n\n- **Goal**\n  - Identify the variable to predict and whether the task is classification or regression.\n  - Describe how the prediction will be used by stakeholders.\n- **Success Criteria**\n  - Select primary metric: ROC AUC / F1 (classification) or MAE / RMSE / R² (regression).\n  - Select secondary metrics: PR AUC, calibration error, or inference latency.\n  - Set target thresholds representing success.\n- **Assumptions and Constraints**\n  - Data coverage and freshness across weekly files.\n  - Joining logic between feature and target files.\n  - Fairness, privacy, and operational constraints.\n","metadata":{}},{"cell_type":"code","source":"# Define a function to load many CSVs by glob\ndef load_many_csvs(pattern):\n    # Collect paths\n    paths = sorted(glob.glob(pattern))\n    # Return None if no matches\n    if len(paths) == 0:\n        return None, []\n    # Read all dataframes\n    frames = []\n    # Iterate paths\n    for p in paths:\n        # Read csv\n        df = pd.read_csv(p, low_memory=False)\n        # Keep source file name\n        df['__source_file'] = os.path.basename(p)\n        # Append frame\n        frames.append(df)\n    # Concatenate frames\n    combined = pd.concat(frames, ignore_index=True, sort=False)\n    # Return combined and list of paths\n    return combined, paths\n\n# Define a function to detect target column and task type\ndef detect_target_and_task(input_df, output_df):\n    # Initialize results\n    target_col = None\n    task_type = None\n    \n    # Choose candidates\n    candidates = ['target', 'label', 'y', 'outcome']\n    \n    # Search output first\n    if output_df is not None:\n        # Intersect with columns\n        for c in candidates:\n            if c in output_df.columns:\n                target_col = c\n                break\n        # Fallback to last column\n        if target_col is None and len(output_df.columns) > 0:\n            target_col = output_df.columns[-1]\n    \n    # If still None, search input\n    if target_col is None and input_df is not None:\n        for c in candidates:\n            if c in input_df.columns:\n                target_col = c\n                break\n    \n    # Determine task type\n    if target_col is not None:\n        # Choose source df\n        src = output_df if output_df is not None and target_col in output_df.columns else input_df\n        # Compute unique count\n        nunique = src[target_col].nunique(dropna=False)\n        # Infer type\n        if nunique <= 20 and not np.issubdtype(src[target_col].dtype, np.floating):\n            task_type = 'classification'\n        else:\n            task_type = 'regression'\n    \n    # Return results\n    return target_col, task_type\n\n# Define a function to propose join keys by shared columns\ndef propose_join_keys(df_left, df_right):\n    # Return None if unavailable\n    if df_left is None or df_right is None:\n        return None\n    # Compute shared columns\n    shared = [c for c in df_left.columns if c in df_right.columns]\n    # Exclude non-keys\n    exclude = {'target', 'label', 'y', 'outcome', '__source_file'}\n    # Filter\n    shared = [c for c in shared if c not in exclude]\n    # Return top candidates\n    if len(shared) == 0:\n        return None\n    if len(shared) == 1:\n        return [shared[0]]\n    return shared[:2]\n\n# Define a main loader\ndef load_all():\n    # Load inputs\n    df_in, in_paths = load_many_csvs(INPUT_GLOB)\n    # Load outputs\n    df_out, out_paths = load_many_csvs(OUTPUT_GLOB)\n    # Load supplement\n    df_sup = pd.read_csv(SUPPLEMENT_PATH, low_memory=False) if os.path.exists(SUPPLEMENT_PATH) else None\n    \n    # Detect target and task\n    detected_target, detected_task = detect_target_and_task(df_in, df_out)\n    \n    # Propose join keys\n    proposed_keys = propose_join_keys(df_in, df_out)\n    \n    # Apply user overrides\n    target_col = TARGET_COLUMN if TARGET_COLUMN else detected_target\n    task_type = TASK_TYPE if TASK_TYPE else detected_task\n    join_keys = JOIN_KEYS if JOIN_KEYS else proposed_keys\n    \n    # Print summary\n    print('Input files:', len(in_paths))\n    print('Output files:', len(out_paths))\n    print('Input shape:', None if df_in is None else df_in.shape)\n    print('Output shape:', None if df_out is None else df_out.shape)\n    print('Supplementary shape:', None if df_sup is None else df_sup.shape)\n    print('Detected target:', target_col)\n    print('Detected task type:', task_type)\n    print('Proposed join keys:', join_keys)\n    \n    # Return artifacts\n    return df_in, df_out, df_sup, target_col, task_type, join_keys\n\n# Execute loader\ndf_input, df_output, df_supp, target_col, task_type, join_keys = load_all()\n\n# Show head\nif df_input is not None:\n    display(df_input.head())\nif df_output is not None:\n    display(df_output.head())\nif df_supp is not None:\n    display(df_supp.head())\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2) Exploratory Data Analysis (EDA) — Distributions, Correlations, Feature Relationships\n\n- **2.1 Structure and Missingness**\n  - Shapes, dtypes, and missing values by column\n  - Duplicate detection and candidate IDs\n- **2.2 Distributions**\n  - Numeric histograms for top features\n  - Bar plots for low-cardinality categoricals\n- **2.3 Correlations and Relationships**\n  - Numeric correlation matrix\n  - Pairwise checks on sampled columns\n- **2.4 Target Analysis**\n  - Class balance or target distribution\n  - Quick baseline estimates\n","metadata":{}},{"cell_type":"code","source":"# Define function for basic summaries\ndef basic_eda(df, name):\n    # Print structure\n    print(f'[{name}] shape:', df.shape)\n    print(f'[{name}] dtypes:')\n    print(df.dtypes.head(40))\n    # Missingness\n    miss = df.isna().sum().sort_values(ascending=False)\n    print(f'[{name}] missing values (top 25):')\n    print(miss.head(25))\n\n# Define function to plot numeric histograms\ndef plot_numeric_histograms(df, limit=MAX_EDA_PLOTS):\n    # Select numeric columns\n    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    # Truncate to limit\n    num_cols = num_cols[:limit]\n    # Iterate columns\n    for c in num_cols:\n        # New figure\n        plt.figure()\n        # Histogram\n        df[c].hist(bins=30)\n        # Title\n        plt.title(f'Distribution: {c}')\n        # Show\n        plt.show()\n\n# Define function to plot correlation heatmap\ndef plot_correlation(df, limit=25):\n    # Select numeric subset\n    cols = df.select_dtypes(include=[np.number]).columns.tolist()[:limit]\n    # Return if not enough columns\n    if len(cols) < 2:\n        return\n    # Compute correlation\n    corr = df[cols].corr()\n    # New figure\n    plt.figure()\n    # Show matrix\n    plt.imshow(corr.values, aspect='auto')\n    # Title\n    plt.title('Correlation matrix (numeric subset)')\n    # Ticks\n    plt.xticks(range(len(cols)), cols, rotation=90)\n    plt.yticks(range(len(cols)), cols)\n    # Color bar\n    plt.colorbar()\n    # Show\n    plt.show()\n\n# Run EDA on input\nif df_input is not None:\n    # Summaries\n    basic_eda(df_input, 'input')\n    # Histograms\n    plot_numeric_histograms(df_input)\n    # Correlation\n    plot_correlation(df_input)\n\n# Target analysis\nif target_col is not None:\n    # Choose source\n    src = df_output if df_output is not None and target_col in df_output.columns else df_input\n    # Classification\n    if task_type == 'classification':\n        # Distribution\n        print('Target class distribution:')\n        print(src[target_col].value_counts(dropna=False))\n    # Regression\n    else:\n        # Summary\n        print('Target distribution summary:')\n        print(src[target_col].describe())\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3) Data Preprocessing — Categorical Encoding and Preparation\n\n- **3.1 Join and Alignment**\n  - Merge features and targets on detected or user-defined join keys\n  - Integrate supplementary data if available\n- **3.2 Train/Test Split**\n  - Stratify for classification\n  - Guard against leakage through group or time splits if required\n- **3.3 Transformations**\n  - Impute missing values\n  - One-hot encode categoricals\n  - Scale numeric features\n","metadata":{}},{"cell_type":"code","source":"# Define a function to assemble the dataset\ndef assemble_dataset(df_in, df_out, df_sup, target_col, join_keys):\n    # Initialize df\n    df = df_in.copy() if df_in is not None else None\n    # Merge output\n    if df is not None and df_out is not None and join_keys is not None:\n        df = df.merge(df_out, on=join_keys, how='inner')\n    # Merge supplement\n    if df is not None and df_sup is not None and join_keys is not None:\n        shared = [c for c in join_keys if c in df_sup.columns]\n        if len(shared) > 0:\n            df = df.merge(df_sup, on=shared, how='left')\n    # Drop obvious non-features\n    drop_cols = ['__source_file']\n    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n    # Return assembled\n    return df\n\n# Define a function to split and build preprocessing pipeline\ndef build_preprocessor(df, target_col):\n    # Identify feature columns\n    features = [c for c in df.columns if c != target_col]\n    # Split by dtype\n    num_cols = [c for c in features if np.issubdtype(df[c].dtype, np.number)]\n    cat_cols = [c for c in features if c not in num_cols]\n    # Numeric pipeline\n    num_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler(with_mean=False))\n    ])\n    # Categorical pipeline\n    cat_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))\n    ])\n    # Column transformer\n    preproc = ColumnTransformer(transformers=[\n        ('num', num_pipeline, num_cols),\n        ('cat', cat_pipeline, cat_cols)\n    ])\n    # Return objects\n    return preproc, features, num_cols, cat_cols\n\n# Assemble dataset\ndataset = None\nif df_input is not None and target_col is not None:\n    dataset = assemble_dataset(df_input, df_output, df_supp, target_col, join_keys)\n\n# Train test split\nX_train, X_test, y_train, y_test = None, None, None, None\nif dataset is not None:\n    y = dataset[target_col]\n    X = dataset.drop(columns=[target_col])\n    if task_type == 'classification':\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n        )\n    else:\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n        )\n    print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n\n# Build preprocessor\npreproc = None\nfeature_cols = None\nnum_cols = None\ncat_cols = None\nif dataset is not None:\n    preproc, feature_cols, num_cols, cat_cols = build_preprocessor(dataset, target_col)\n    print('Numeric features:', len(num_cols), 'Categorical features:', len(cat_cols))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4) Modeling and Evaluation — Compare Models on Test Data\n\n- **4.1 Baselines**\n  - Dummy model to establish a floor\n- **4.2 Stronger Models**\n  - Linear/Logistic models\n  - Random Forest models\n- **4.3 Cross-Validation**\n  - Stratified K-Fold for classification or K-Fold for regression\n- **4.4 Test Evaluation**\n  - Compute primary and secondary metrics\n  - Plot ROC/PR curves for classification\n","metadata":{}},{"cell_type":"code","source":"# Define helpers to build models\ndef build_models(task_type):\n    # Classification models\n    if task_type == 'classification':\n        return {\n            'Dummy': DummyClassifier(strategy='most_frequent'),\n            'LogisticRegression': LogisticRegression(max_iter=200),\n            'RandomForest': RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)\n        }\n    # Regression models\n    else:\n        return {\n            'Dummy': DummyRegressor(strategy='mean'),\n            'LinearRegression': LinearRegression(),\n            'RandomForest': RandomForestRegressor(n_estimators=300, random_state=RANDOM_STATE)\n        }\n\n# Define evaluation for classification\ndef eval_classification(y_true, y_proba, y_pred):\n    # Compute metrics\n    auc = roc_auc_score(y_true, y_proba[:, 1]) if y_proba.shape[1] > 1 else np.nan\n    ap = average_precision_score(y_true, y_proba[:, 1]) if y_proba.shape[1] > 1 else np.nan\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    acc = accuracy_score(y_true, y_pred)\n    # Print report\n    print('Accuracy:', round(acc, 4))\n    print('F1 (weighted):', round(f1, 4))\n    print('ROC AUC:', round(auc, 4))\n    print('PR AUC:', round(ap, 4))\n    print('Classification Report:')\n    print(classification_report(y_true, y_pred))\n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Plot confusion matrix\n    plt.figure()\n    plt.imshow(cm, aspect='equal')\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    # ROC curve\n    if y_proba.shape[1] > 1:\n        fpr, tpr, _ = roc_curve(y_true, y_proba[:, 1])\n        plt.figure()\n        plt.plot(fpr, tpr)\n        plt.title('ROC Curve')\n        plt.xlabel('FPR')\n        plt.ylabel('TPR')\n        plt.show()\n        # PR Curve\n        precision, recall, _ = precision_recall_curve(y_true, y_proba[:, 1])\n        plt.figure()\n        plt.plot(recall, precision)\n        plt.title('Precision-Recall Curve')\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.show()\n\n# Define evaluation for regression\ndef eval_regression(y_true, y_pred):\n    # Compute metrics\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    r2 = r2_score(y_true, y_pred)\n    # Print metrics\n    print('MAE:', round(mae, 4))\n    print('RMSE:', round(rmse, 4))\n    print('R²:', round(r2, 4))\n\n# Train and evaluate models\ntrained = {}\nif dataset is not None:\n    # Build candidate models\n    models = build_models(task_type)\n    # Iterate over models\n    for name, base_model in models.items():\n        # Create pipeline\n        pipe = Pipeline(steps=[('preproc', preproc), ('model', base_model)])\n        # Fit\n        pipe.fit(X_train, y_train)\n        # Predict\n        if task_type == 'classification':\n            y_pred = pipe.predict(X_test)\n            # Predict probabilities with fallback\n            if hasattr(pipe.named_steps['model'], 'predict_proba'):\n                y_proba = pipe.predict_proba(X_test)\n            else:\n                proba = pipe.decision_function(X_test)\n                proba = (proba - proba.min()) / (proba.max() - proba.min() + 1e-9)\n                y_proba = np.vstack([1 - proba, proba]).T\n            # Print header\n            print(f'=== {name} ===')\n            # Evaluate\n            eval_classification(y_test, y_proba, y_pred)\n        else:\n            y_pred = pipe.predict(X_test)\n            print(f'=== {name} ===')\n            eval_regression(y_test, y_pred)\n        # Save\n        trained[name] = pipe\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5) Error Analysis and Interpretation — Residuals, Subgroups, Feature Importance\n\n- **5.1 Residuals and Hard Cases**\n  - Inspect largest errors or misclassifications\n- **5.2 Subgroup Performance**\n  - Evaluate by key categorical segments\n- **5.3 Feature Importance**\n  - Report impurity-based importances from random forests\n","metadata":{}},{"cell_type":"code","source":"# Define a function to get top errors or misclassifications\ndef top_errors(pipe, X_test, y_test, k=25):\n    # Classification\n    if task_type == 'classification':\n        # Predict probabilities\n        if hasattr(pipe.named_steps['model'], 'predict_proba'):\n            proba = pipe.predict_proba(X_test)\n            conf = np.max(proba, axis=1)\n            pred = np.argmax(proba, axis=1)\n        else:\n            decision = pipe.decision_function(X_test)\n            decision = (decision - decision.min()) / (decision.max() - decision.min() + 1e-9)\n            conf = np.maximum(decision, 1 - decision)\n            pred = (decision > 0.5).astype(int)\n        # Convert y_test\n        y_true = y_test.values\n        # Compute correctness\n        correct = (pred == y_true)\n        # Compute confidence errors\n        order = np.argsort(conf[~correct]) if (~correct).sum() > 0 else np.array([], dtype=int)\n        # Return indices\n        return np.where(~correct)[0][order][:k]\n    # Regression\n    else:\n        # Predict values\n        pred = pipe.predict(X_test)\n        # Compute absolute error\n        err = np.abs(pred - y_test.values)\n        # Sort errors\n        order = np.argsort(-err)\n        # Return top-k\n        return order[:k]\n\n# Define a function to subgroup performance by top categorical columns\ndef subgroup_eval(pipe, X, y, cat_cols, max_groups=3):\n    # Select up to max_groups categorical columns\n    cats = cat_cols[:max_groups]\n    # Evaluate for each categorical column\n    for c in cats:\n        # Skip if column not present\n        if c not in X.columns:\n            continue\n        # Value counts\n        vc = X[c].astype(str).value_counts().head(5).index.tolist()\n        # Iterate groups\n        for v in vc:\n            # Build mask\n            mask = X[c].astype(str) == v\n            # Skip tiny groups\n            if mask.sum() < 20:\n                continue\n            # Predict subset\n            y_pred = pipe.predict(X[mask])\n            # Classification metrics\n            if task_type == 'classification':\n                acc = accuracy_score(y[mask], y_pred)\n                f1 = f1_score(y[mask], y_pred, average='weighted')\n                print(f'[Subgroup] {c}={v}  n={mask.sum()}  acc={acc:.3f}  f1={f1:.3f}')\n            # Regression metrics\n            else:\n                mae = mean_absolute_error(y[mask], y_pred)\n                rmse = mean_squared_error(y[mask], y_pred, squared=False)\n                print(f'[Subgroup] {c}={v}  n={mask.sum()}  MAE={mae:.3f}  RMSE={rmse:.3f}')\n\n# Run error analysis on the RandomForest model if trained\nif 'RandomForest' in globals().get('trained', {}):\n    # Select model\n    rf_pipe = trained['RandomForest']\n    # Compute top errors\n    idx = top_errors(rf_pipe, X_test, y_test, k=25)\n    # Print indices\n    print('Top error indices:', idx)\n    # Subgroup evaluation\n    subgroup_eval(rf_pipe, X_test, y_test, cat_cols)\n\n# Feature importance via RF if available\nif 'RandomForest' in globals().get('trained', {}):\n    # Extract model\n    rf = trained['RandomForest'].named_steps['model']\n    # Check attribute\n    if hasattr(rf, 'feature_importances_'):\n        # Fit preprocessor on full training set\n        trained['RandomForest'].named_steps['preproc'].fit(X_train, y_train)\n        # Get transformed feature names\n        num_features = [f'num__{c}' for c in num_cols]\n        cat_features = []\n        if len(cat_cols) > 0:\n            ohe = trained['RandomForest'].named_steps['preproc'].named_transformers_['cat'].named_steps['onehot']\n            cat_features = [f'cat__{i}' for i in range(len(ohe.get_feature_names_out()))]\n        # Plot importance\n        importances = rf.feature_importances_\n        order = np.argsort(-importances)[:20]\n        plt.figure()\n        plt.bar(range(len(order)), importances[order])\n        plt.title('RandomForest Feature Importances (Top 20)')\n        plt.xlabel('Feature index')\n        plt.ylabel('Importance')\n        plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6) Conclusion and Next Steps — Summary and Improvements\n\n- **6.1 Summary of Findings**\n  - Recap model that performed best and key metrics\n- **6.2 Data and Features**\n  - Note data quality issues and promising features\n- **6.3 Next Steps**\n  - Hyperparameter tuning, temporal validation, model calibration\n  - Robust leakage checks and domain-specific features\n","metadata":{}}]}