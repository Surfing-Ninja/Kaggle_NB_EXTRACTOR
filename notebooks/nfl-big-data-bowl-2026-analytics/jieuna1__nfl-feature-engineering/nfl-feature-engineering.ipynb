{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114250,"databundleVersionId":13838823,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch\nfrom joblib import Parallel, delayed\nimport warnings\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OneHotEncoder\nwarnings.filterwarnings('ignore')\n\n\n### Configuration class\nclass Config:\n    ROOT_DIR = Path('/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final')\n    \n    SEED = 42\n    K_FOLD = 5\n    N_EPOCHS = 150\n    PATIENCE = 15\n    LR = 1e-3 #Â If training from scratch that should be maybe higher\n    BATCH_SIZE = 128\n    \n    Y_MIN, Y_MAX = 0.0, 53.3\n    X_MIN, X_MAX = 0.0, 120.0\n    \n    N_JOBS = 8  \n    IS_NOTEBOOK = True\n    \n    #Attention Model Config\n    pass\n    \n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def set_seed(self):\n        torch.manual_seed(self.SEED)\n        torch.cuda.manual_seed_all(self.SEED)\n        random.seed(self.SEED)\n        np.random.seed(self.SEED)\n        os.environ['PYTHONHASHSEED'] = str(self.SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\n### Load Data function\ndef load_single_week_data(TRAIN_ROOT_DIR, week):\n    \n    try:\n        input_path = f\"input_2023_w{week:02d}.csv\"\n        output_path = f\"output_2023_w{week:02d}.csv\"\n        \n        if not os.path.exists(os.path.join(TRAIN_ROOT_DIR, input_path)):\n            raise FileNotFoundError(f\"The file {input_path} does not exist in {TRAIN_ROOT_DIR}.\")\n        if not os.path.exists(os.path.join(TRAIN_ROOT_DIR, output_path)):\n            raise FileNotFoundError(f\"The file {output_path} does not exist in {TRAIN_ROOT_DIR}.\")\n        \n        input_df, output_df = pd.read_csv(os.path.join(TRAIN_ROOT_DIR, input_path)), pd.read_csv(os.path.join(TRAIN_ROOT_DIR, output_path))\n        \n        return input_df, output_df\n        \n    except Exception as e:\n        print(f\"Error loading data for week {week:02d}: {e}\")\n        return None, None\n\ndef load_data(ROOT_DIR, N_JOBS):\n    if not os.path.exists(ROOT_DIR):\n        raise FileNotFoundError(f\"The DIR {ROOT_DIR} does not exist.\")\n\n    train_root_dir = os.path.join(ROOT_DIR, 'train')\n    \n    if not os.path.exists(train_root_dir):\n        raise FileNotFoundError(f\"The DIR {train_root_dir} does not exist.\")\n    \n    weeks = list(range(1, 19))\n    results = Parallel(n_jobs=N_JOBS, backend='threading')(delayed(load_single_week_data)(train_root_dir, week) for week in tqdm(weeks, desc=\"Loading data\"))\n    \n    input_dfs, output_dfs = zip(*results)\n    input_df = pd.concat([df for df in input_dfs if df is not None], ignore_index=True)\n    output_df = pd.concat([df for df in output_dfs if df is not None], ignore_index=True)\n    \n    print(\"Data loading complete.\")\n    print(f\"Input data shape: {input_df.shape}\")\n    print(f\"Output data shape: {output_df.shape}\")\n    \n    print(\"Columns in input data:\\n\", input_df.columns.tolist())\n    print(\"-\"*50)\n    print(\"Columns in output data:\\n\", output_df.columns.tolist())\n    \n    return input_df, output_df\n\n\n# Plot functions\ndef plot_distribution(data, column, title, xlabel, ylabel, notebook=False):\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data[column], bins=30, kde=True)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    if notebook:\n        plt.show()\n    plt.savefig(f\"{column}_distribution.png\")\n\n\n### Future Engineering functions\ndef merge_output_and_input(input_df, output_df):\n    merged_df = (\n        input_df.merge(\n                output_df,\n                on=[\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"],\n                how=\"inner\",\n                suffixes=(\"_input\", \"_target\")\n                )\n            )\n    \n    print(f\"Merged data shape: {merged_df.shape}\")\n    print(\"Columns in merged data:\\n\", merged_df.columns.tolist())\n\n    return merged_df\n\n\ndef add_additional_features(df, is_notebook=False):\n    df = df.copy()\n\n    height_ft = df[\"player_height\"].str.split(\"-\", expand=True)[0].astype(float)\n    height_in = df[\"player_height\"].str.split(\"-\", expand=True)[1].astype(float)\n    df[\"height_m\"] = (height_ft * 12 + height_in) * 0.0254\n    df[\"weight_kg\"] = df[\"player_weight\"].astype(float) * 0.453592\n\n    # Calculate BMI\n    df[\"BMI\"] = df[\"weight_kg\"] / (df[\"height_m\"] ** 2)\n    plot_distribution(df, \"BMI\", \"BMI Distribution\", \"BMI\", \"Frequency\", is_notebook)\n\n    # Calculate age\n    birth_year = pd.to_datetime(df[\"player_birth_date\"]).dt.year\n    df[\"Age\"] = 2023 - birth_year\n    plot_distribution(df, \"Age\", \"Age Distribution\", \"Age\", \"Frequency\", is_notebook)\n    \n    \n    #One hot encode player side, role and position\n    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n    side_and_role_position_encoded = ohe.fit_transform(df[[\"player_side\", \"player_role\", \"player_position\"]])\n    side_role_position_cols = ohe.get_feature_names_out([\"player_side\", \"player_role\", \"player_position\"])\n    df_side_role_position = pd.DataFrame(side_and_role_position_encoded, columns=side_role_position_cols, index=df.index)\n    \n    df = pd.concat([df, df_side_role_position], axis=1)\n    \n    #Degree to radian conversions and trigonometric features\n    df[\"dir_rad\"] = np.deg2rad(df[\"dir\"])\n    df[\"o_rad\"]   = np.deg2rad(df[\"o\"])\n    \n    df[\"dir_sin\"] = np.sin(df[\"dir_rad\"])\n    df[\"dir_cos\"] = np.cos(df[\"dir_rad\"])\n    df[\"o_sin\"]   = np.sin(df[\"o_rad\"])\n    df[\"o_cos\"]   = np.cos(df[\"o_rad\"])\n    \n    #Components of velocity and acceleration\n    df[\"vx\"] = df[\"s\"] * df[\"dir_cos\"]\n    df[\"vy\"] = df[\"s\"] * df[\"dir_sin\"]\n    df[\"ax\"] = df[\"a\"] * df[\"dir_cos\"]\n    df[\"ay\"] = df[\"a\"] * df[\"dir_sin\"]\n    \n    #Angle difference between orientation and direction\n    df[\"angle_diff\"] = np.abs(df[\"o\"] - df[\"dir\"])\n    df[\"angle_diff\"] = np.where(df[\"angle_diff\"] > 180, 360 - df[\"angle_diff\"], df[\"angle_diff\"])\n    #Angle difference in radians\n    df[\"angle_diff_rad\"] = np.deg2rad(df[\"angle_diff\"])\n    plot_distribution(df, \"angle_diff\", \"Angle Difference Distribution\", \"Angle Difference (degrees)\", \"Frequency\", is_notebook)\n    \n    #Distance to ball landing spot\n    df[\"dist_to_ball\"] = np.sqrt((df[\"x_input\"] - df[\"ball_land_x\"])**2 + (df[\"y_input\"] - df[\"ball_land_y\"])**2)\n    plot_distribution(df, \"dist_to_ball\", \"Distance to Ball Landing Spot Distribution\", \"Distance to Ball Landing Spot\", \"Frequency\", is_notebook)\n    \n    #Play direction encoding\n    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n    play_direction_encoded = encoder.fit_transform(df[[\"play_direction\"]])\n    play_direction_cols = encoder.get_feature_names_out([\"play_direction\"])\n    df_play_direction = pd.DataFrame(play_direction_encoded, columns=play_direction_cols, index=df.index)\n    \n    df = pd.concat([df, df_play_direction], axis=1)\n    \n    \n    #Drop unused columns\n    df.drop(columns=[\"player_side\", \"player_birth_date\", \"player_height\", \"player_weight\"], inplace=True)\n    \n    return df\n\ndef aggregate_features(df, groupby_cols, agg_cols):\n    agg_df = df.groupby(groupby_cols)[agg_cols].agg(['mean', 'std', 'min', 'max'])\n    \n    agg_df.columns = ['_'.join(col).strip('_') for col in agg_df.columns.values]\n    \n    agg_df = agg_df.reset_index()\n    \n    print(f\"Aggregated data shape: {agg_df.shape}\")\n    return agg_df\n    \nif __name__ == \"__main__\":\n    config = Config()\n    config.set_seed()\n    \n    print(f\"Using device: {config.DEVICE}\")\n    print(f\"Root directory: {config.ROOT_DIR}\")\n    print(f\"Training for {config.N_EPOCHS} epochs with batch size {config.BATCH_SIZE}\")\n    print(f\"Learning rate: {config.LR}\")\n    print(f\"K-Fold Cross Validation with K={config.K_FOLD}\")\n    print(f\"Early stopping patience: {config.PATIENCE} epochs\")\n    print(f\"Y range: [{config.Y_MIN}, {config.Y_MAX}]\")\n    print(f\"X range: [{config.X_MIN}, {config.X_MAX}]\")\n    print(\"Configuration setup complete.\")\n    \n    # Load data\n    input_df, output_df = load_data(config.ROOT_DIR, config.N_JOBS)\n    \n    merged_df = merge_output_and_input(input_df, output_df)\n    \n    feature_engineered_df = add_additional_features(merged_df, config.IS_NOTEBOOK)\n    \n    print(\"Feature engineering complete.\")\n    print(f\"Final data shape: {feature_engineered_df.shape}\")\n    print(\"Columns in final data:\\n\", feature_engineered_df.columns.tolist())\n    print(feature_engineered_df.head())\n    \n    groupby_cols = ['nfl_id', 'player_name', 'player_position', 'player_role', 'game_id']\n    agg_cols = ['s', 'a', 'vx', 'vy', 'ax', 'ay', 'dist_to_ball']\n    \n    aggregated_df = aggregate_features(feature_engineered_df, groupby_cols=groupby_cols, agg_cols=agg_cols)\n    \n    merged_df = pd.merge(feature_engineered_df, aggregated_df, on=groupby_cols, how='left', suffixes=('', '_agg'))\n    final_df = merged_df.drop(columns=agg_cols)\n    print(f\"Final data shape after aggregation: {final_df.shape}\")\n    print('Columns in final data after aggregation:\\n', final_df.columns.tolist())\n    \n    print(\"Aggregation complete. Feature engineering pipeline finished.\")\n    print(final_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-21T15:25:11.595362Z","iopub.execute_input":"2025-10-21T15:25:11.595742Z","iopub.status.idle":"2025-10-21T15:25:30.041853Z","shell.execute_reply.started":"2025-10-21T15:25:11.595719Z","shell.execute_reply":"2025-10-21T15:25:30.041064Z"}},"outputs":[],"execution_count":null}]}