{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114250,"databundleVersionId":13838823,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Updated metrics function with Route Mirroring + grouping block ===\nimport numpy as np\nimport pandas as pd\n\ndef calculate_play_metrics_with_route_mirroring(play_df):\n    \"\"\"\n    Returns a pandas.Series with:\n      - primary_defender_id\n      - receiver_id\n      - coverage_tightness (avg separation)\n      - ball_hawk_score (avg closing speed toward ball landing point)\n      - route_mirroring (avg frame-level mirror score combining dir alignment and speed ratio)\n    Works with estimated vx/vy (prefers vx/vy columns), otherwise computes simple diffs.\n    \"\"\"\n    keys = ['primary_defender_id','receiver_id','coverage_tightness','ball_hawk_score','route_mirroring']\n    res = pd.Series({k: np.nan for k in keys})\n    try:\n        pdf = play_df.copy()\n        # basic guards\n        if 'player_role' not in pdf.columns or {'x','y','frame_id'}.difference(pdf.columns):\n            return res\n\n        # normalize roles\n        pdf['role_lower'] = pdf['player_role'].astype(str).str.lower()\n        receiver_mask = pdf['role_lower'].str.contains('target|receiver', na=False)\n        defender_mask = pdf['role_lower'].str.contains('defens|coverage|defend|db', na=False)\n        if not receiver_mask.any() or not defender_mask.any():\n            return res\n\n        receiver = pdf[receiver_mask].copy()\n        defenders = pdf[defender_mask].copy()\n\n        # determine start frame\n        start_frame = int(pdf['frame_id'].min())\n\n        rec_start = receiver[receiver['frame_id'] == start_frame]\n        def_start = defenders[defenders['frame_id'] == start_frame]\n        if rec_start.empty:\n            rec_start = receiver.groupby('nfl_id', dropna=True).first().reset_index()\n        if def_start.empty:\n            def_start = defenders.groupby('nfl_id', dropna=True).first().reset_index()\n        if rec_start.empty or def_start.empty:\n            return res\n\n        # ball landing coords\n        if 'ball_land_x' not in pdf.columns or pdf['ball_land_x'].dropna().empty:\n            return res\n        ball_land_x = float(pdf['ball_land_x'].dropna().iloc[0])\n        ball_land_y = float(pdf['ball_land_y'].dropna().iloc[0])\n\n        # receiver basic\n        rec_row = rec_start.iloc[0]\n        rec_x, rec_y = rec_row.get('x', np.nan), rec_row.get('y', np.nan)\n        res['receiver_id'] = rec_row.get('nfl_id', np.nan)\n\n        # primary defender selection: closest at start frame\n        def_start = def_start.copy()\n        def_start['dist_to_rec'] = np.sqrt((def_start['x'] - rec_x)**2 + (def_start['y'] - rec_y)**2)\n        if def_start['dist_to_rec'].isnull().all():\n            return res\n        primary_idx = def_start['dist_to_rec'].idxmin()\n        primary_defender_id = def_start.loc[primary_idx, 'nfl_id']\n        res['primary_defender_id'] = primary_defender_id\n\n        # Build receiver & defender paths; prefer existing vx/vy if present\n        recv_cols = ['frame_id','x','y']\n        if 'vx' in pdf.columns and 'vy' in pdf.columns:\n            recv_cols += ['vx','vy']\n        receiver_path = receiver[recv_cols].rename(columns={'x':'rec_x','y':'rec_y','vx':'rec_vx','vy':'rec_vy'}).copy()\n\n        def_cols = ['frame_id','x','y']\n        if 'vx' in pdf.columns and 'vy' in pdf.columns:\n            def_cols += ['vx','vy']\n        defender_path = defenders[def_cols].rename(columns={'x':'def_x','y':'def_y','vx':'def_vx','vy':'def_vy'}).copy()\n        defender_path = defender_path[defender_path['nfl_id'] == primary_defender_id] if 'nfl_id' in defender_path.columns else defender_path\n        # note: if 'nfl_id' column was kept we already filtered; otherwise we filter above using defenders selection\n        if 'nfl_id' not in defender_path.columns and primary_defender_id is not None:\n            # defender_path may already be only primary defender due to earlier filtering; if not, filter on id in play_df\n            defender_path = defenders[defenders['nfl_id'] == primary_defender_id][def_cols].rename(columns={'x':'def_x','y':'def_y','vx':'def_vx','vy':'def_vy'}).copy()\n\n        if receiver_path.empty or defender_path.empty:\n            return res\n\n        merged = pd.merge(receiver_path, defender_path, on='frame_id', how='inner', suffixes=('_r','_d')).sort_values('frame_id').reset_index(drop=True)\n        if merged.empty:\n            return res\n\n        # ---- Coverage Tightness ----\n        merged['separation'] = np.sqrt((merged['rec_x'] - merged['def_x'])**2 + (merged['rec_y'] - merged['def_y'])**2)\n        res['coverage_tightness'] = float(merged['separation'].mean())\n\n        # ---- Ball Hawk Score ----\n        merged['ball_vec_x'] = ball_land_x - merged['def_x']\n        merged['ball_vec_y'] = ball_land_y - merged['def_y']\n        norm_ball = np.sqrt(merged['ball_vec_x']**2 + merged['ball_vec_y']**2).replace(0,1.0)\n        merged['ball_dir_x'] = merged['ball_vec_x'] / norm_ball\n        merged['ball_dir_y'] = merged['ball_vec_y'] / norm_ball\n\n        # defender velocity: prefer def_vx/def_vy (already in merged if vx present), otherwise try s/dir fallback (rare)\n        if {'def_vx','def_vy'}.issubset(merged.columns):\n            closing_speed = merged['def_vx'] * merged['ball_dir_x'] + merged['def_vy'] * merged['ball_dir_y']\n            res['ball_hawk_score'] = float(closing_speed.mean())\n        elif {'s','dir'}.issubset(merged.columns):\n            math_dir = np.deg2rad(90.0 - merged['dir'].astype(float))\n            def_vx = merged['s'].astype(float) * np.cos(math_dir)\n            def_vy = merged['s'].astype(float) * np.sin(math_dir)\n            closing_speed = def_vx * merged['ball_dir_x'] + def_vy * merged['ball_dir_y']\n            res['ball_hawk_score'] = float(closing_speed.mean())\n        else:\n            res['ball_hawk_score'] = np.nan\n\n        # ---- Route Mirroring ----\n        # Get receiver velocities: prefer rec_vx/rec_vy, otherwise compute from rec_x diffs across merged frames\n        if {'rec_vx','rec_vy'}.issubset(merged.columns):\n            merged['r_vx'] = merged['rec_vx'].astype(float)\n            merged['r_vy'] = merged['rec_vy'].astype(float)\n        else:\n            # compute per-frame diff (units per frame). Use forward/backward diff to estimate velocity centered on frame.\n            merged['r_vx'] = merged['rec_x'].diff().fillna(0)\n            merged['r_vy'] = merged['rec_y'].diff().fillna(0)\n\n        # Defender velocities: prefer def_vx/def_vy, otherwise estimate from def_x diffs\n        if {'def_vx','def_vy'}.issubset(merged.columns):\n            merged['d_vx'] = merged['def_vx'].astype(float)\n            merged['d_vy'] = merged['def_vy'].astype(float)\n        else:\n            merged['d_vx'] = merged['def_x'].diff().fillna(0)\n            merged['d_vy'] = merged['def_y'].diff().fillna(0)\n\n        # speeds\n        merged['r_speed'] = np.sqrt(merged['r_vx']**2 + merged['r_vy']**2)\n        merged['d_speed'] = np.sqrt(merged['d_vx']**2 + merged['d_vy']**2)\n\n        # avoid divide by zero: where speed==0 set direction to NaN (these frames are not informative)\n        r_nonzero = merged['r_speed'] > 1e-6\n        d_nonzero = merged['d_speed'] > 1e-6\n        valid_both = r_nonzero & d_nonzero\n\n        if valid_both.any():\n            # normalized directions\n            merged.loc[valid_both, 'r_dir_x'] = merged.loc[valid_both, 'r_vx'] / merged.loc[valid_both, 'r_speed']\n            merged.loc[valid_both, 'r_dir_y'] = merged.loc[valid_both, 'r_vy'] / merged.loc[valid_both, 'r_speed']\n            merged.loc[valid_both, 'd_dir_x'] = merged.loc[valid_both, 'd_vx'] / merged.loc[valid_both, 'd_speed']\n            merged.loc[valid_both, 'd_dir_y'] = merged.loc[valid_both, 'd_vy'] / merged.loc[valid_both, 'd_speed']\n\n            # directional alignment (cosine similarity)\n            merged.loc[valid_both, 'dir_score'] = (\n                merged.loc[valid_both, 'r_dir_x'] * merged.loc[valid_both, 'd_dir_x'] +\n                merged.loc[valid_both, 'r_dir_y'] * merged.loc[valid_both, 'd_dir_y']\n            ).clip(-1.0, 1.0)\n\n            # speed ratio (0..1)\n            merged.loc[valid_both, 'speed_ratio'] = (\n                np.minimum(merged.loc[valid_both, 'r_speed'], merged.loc[valid_both, 'd_speed']) /\n                np.maximum(merged.loc[valid_both, 'r_speed'], merged.loc[valid_both, 'd_speed'])\n            )\n\n            # frame-level mirror score: product of direction alignment and speed ratio\n            merged.loc[valid_both, 'mirror_frame_score'] = merged.loc[valid_both, 'dir_score'] * merged.loc[valid_both, 'speed_ratio']\n\n            # aggregate to play-level: mean of frame scores\n            route_mirroring = float(merged.loc[valid_both, 'mirror_frame_score'].mean())\n            res['route_mirroring'] = route_mirroring\n        else:\n            res['route_mirroring'] = np.nan\n\n        return res\n\n    except Exception as e:\n        # in case of any unexpected error return NaNs (optionally log e)\n        # print(\"Exception in route mirroring calc:\", e)\n        return pd.Series({k: np.nan for k in keys})\n\n\n# ---- Group by plays and compute metrics (use include_groups=False to silence deprecation warning) ----\nif 'df' not in globals():\n    raise RuntimeError(\"Dataframe 'df' not found. Please ensure your merged/enriched tracking DataFrame is stored in variable `df`.\")\n\nif {'game_id','play_id'}.issubset(df.columns):\n    play_metrics = df.groupby(['game_id','play_id'], group_keys=False, sort=False).apply(\n        lambda g: calculate_play_metrics_with_route_mirroring(g),\n        include_groups=False\n    ).reset_index()\n\n    # tidy types for convenience\n    if 'primary_defender_id' in play_metrics.columns:\n        play_metrics['primary_defender_id'] = pd.to_numeric(play_metrics['primary_defender_id'], errors='coerce').astype('Int64')\n    if 'receiver_id' in play_metrics.columns:\n        play_metrics['receiver_id'] = pd.to_numeric(play_metrics['receiver_id'], errors='coerce').astype('Int64')\n\n    # save results\n    play_metrics.to_csv('play_metrics_with_route_mirroring.csv', index=False)\n    print(\"Saved play_metrics_with_route_mirroring.csv — rows:\", len(play_metrics))\n\n    # short summary\n    total_plays = df[['game_id','play_id']].drop_duplicates().shape[0]\n    print(f\"Total plays found: {total_plays}\")\n    print(\"Plays with coverage_tightness computed:\", play_metrics['coverage_tightness'].notna().sum())\n    print(\"Plays with ball_hawk_score computed:\", play_metrics['ball_hawk_score'].notna().sum())\n    print(\"Plays with route_mirroring computed:\", play_metrics['route_mirroring'].notna().sum())\n\n    # show first 10 rows\n    display(play_metrics.head(10))\nelse:\n    raise RuntimeError(\"Dataframe must contain 'game_id' and 'play_id' columns for grouping.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T18:47:04.878336Z","iopub.execute_input":"2025-09-30T18:47:04.878717Z","iopub.status.idle":"2025-09-30T18:47:25.118306Z","shell.execute_reply.started":"2025-09-30T18:47:04.87869Z","shell.execute_reply":"2025-09-30T18:47:25.117276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# load results & pick a sample play\npm = pd.read_csv('play_metrics.csv')\nsample_game, sample_play = pm.loc[0, ['game_id','play_id']].astype(int).tolist()\n\n# filter frames for that play\nsample = df[(df['game_id']==sample_game) & (df['play_id']==sample_play)]\n\nrid = int(pm.loc[0,'receiver_id'])\npdid = int(pm.loc[0,'primary_defender_id'])\n\nrecv = sample[sample['nfl_id']==rid][['frame_id','x','y']].rename(columns={'x':'rec_x','y':'rec_y'})\ndefn = sample[sample['nfl_id']==pdid][['frame_id','x','y','vx','vy']].rename(columns={'x':'def_x','y':'def_y'})\n\nmerged = pd.merge(recv, defn, on='frame_id', how='inner').sort_values('frame_id').reset_index(drop=True)\nif merged.empty:\n    print(\"No overlapping frames for this sample play.\")\nelse:\n    # separation\n    merged['separation'] = np.sqrt((merged['rec_x']-merged['def_x'])**2 + (merged['rec_y']-merged['def_y'])**2)\n    # closing speed toward ball_land (if ball_land in sample)\n    if 'ball_land_x' in sample.columns and not sample['ball_land_x'].dropna().empty:\n        bx = float(sample['ball_land_x'].dropna().iloc[0])\n        by = float(sample['ball_land_y'].dropna().iloc[0])\n        merged['ball_vec_x'] = bx - merged['def_x']\n        merged['ball_vec_y'] = by - merged['def_y']\n        norm = np.sqrt(merged['ball_vec_x']**2 + merged['ball_vec_y']**2).replace(0,1)\n        merged['ball_dir_x'] = merged['ball_vec_x']/norm\n        merged['ball_dir_y'] = merged['ball_vec_y']/norm\n        # use per-frame vx/vy; multiply by frame_rate if you want per-second\n        merged['closing_speed'] = merged['vx'] * merged['ball_dir_x'] + merged['vy'] * merged['ball_dir_y']\n    else:\n        merged['closing_speed'] = np.nan\n\n    # plot\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,2,1)\n    plt.plot(merged['frame_id'], merged['separation'], marker='o')\n    plt.title('Separation over frames')\n    plt.xlabel('frame_id')\n    plt.ylabel('separation (field units)')\n\n    plt.subplot(1,2,2)\n    plt.plot(merged['frame_id'], merged['closing_speed'], marker='o')\n    plt.title('Closing speed (toward ball)')\n    plt.xlabel('frame_id')\n    plt.ylabel('speed (units/frame)')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T18:38:09.566649Z","iopub.execute_input":"2025-09-30T18:38:09.566982Z","iopub.status.idle":"2025-09-30T18:38:10.055807Z","shell.execute_reply.started":"2025-09-30T18:38:09.566959Z","shell.execute_reply":"2025-09-30T18:38:10.054783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ---------------- Load play-level metrics ----------------\ntry:\n    pm = pd.read_csv('play_metrics_with_route_mirroring.csv')\n    print(\"Loaded play_metrics_with_route_mirroring.csv\")\nexcept FileNotFoundError:\n    try:\n        pm  # use in-memory variable if it exists\n        print(\"Using in-memory play_metrics variable\")\n    except NameError:\n        raise RuntimeError(\"play_metrics_with_route_mirroring.csv not found and no in-memory 'pm' variable. Run metrics step first.\")\n\n# ensure we have the dataframe variable in pm\nif 'pm' not in globals():\n    pm = pd.read_csv('play_metrics_with_route_mirroring.csv')\n\n# Clean types\npm['primary_defender_id'] = pd.to_numeric(pm['primary_defender_id'], errors='coerce').astype('Int64')\npm['receiver_id'] = pd.to_numeric(pm['receiver_id'], errors='coerce').astype('Int64')\n\n# Drop rows where primary_defender_id is missing\npm = pm.dropna(subset=['primary_defender_id']).copy()\n\n# ---------------- Aggregate per defender ----------------\nagg = pm.groupby('primary_defender_id').agg(\n    plays_count = ('coverage_tightness', 'count'),\n    avg_coverage = ('coverage_tightness', 'mean'),\n    median_coverage = ('coverage_tightness', 'median'),\n    std_coverage = ('coverage_tightness', 'std'),\n    avg_ballhawk = ('ball_hawk_score', 'mean'),\n    median_ballhawk = ('ball_hawk_score', 'median'),\n    std_ballhawk = ('ball_hawk_score', 'std'),\n    avg_mirroring = ('route_mirroring', 'mean'),\n    median_mirroring = ('route_mirroring', 'median'),\n    std_mirroring = ('route_mirroring', 'std'),\n).reset_index()\n\n# ---------------- Filter by minimum plays ----------------\nmin_plays_threshold = 5   # tune this as needed\nagg = agg[agg['plays_count'] >= min_plays_threshold].copy()\nprint(f\"Defenders with >= {min_plays_threshold} plays: {len(agg)}\")\n\n# ---------------- Normalization (min-max) ----------------\ndef minmax_scale(series):\n    if series.isnull().all():\n        return series\n    mn, mx = series.min(), series.max()\n    if mn == mx:\n        return pd.Series(0.5, index=series.index)\n    return (series - mn) / (mx - mn)\n\n# coverage: lower is better → invert after scaling\nagg['coverage_norm'] = minmax_scale(agg['avg_coverage'])\nagg['coverage_inv'] = 1.0 - agg['coverage_norm']\n\n# ballhawk: higher is better\nagg['ballhawk_norm'] = minmax_scale(agg['avg_ballhawk'])\n\n# mirroring: higher is better\nagg['mirror_norm'] = minmax_scale(agg['avg_mirroring'])\n\n# ---------------- Combined score ----------------\n# Tune these weights to emphasize different skills\nweight_coverage = 0.45\nweight_ballhawk = 0.25\nweight_mirroring = 0.30\n\n# make sure weights sum to 1 (normalize if not)\nw_sum = weight_coverage + weight_ballhawk + weight_mirroring\nweight_coverage /= w_sum\nweight_ballhawk /= w_sum\nweight_mirroring /= w_sum\n\nagg['combined_score'] = (\n    agg['coverage_inv'] * weight_coverage +\n    agg['ballhawk_norm'] * weight_ballhawk +\n    agg['mirror_norm'] * weight_mirroring\n)\n\n# ---------------- Ranking and Output ----------------\ntop_n = 30\ntop_by_coverage = agg.sort_values('avg_coverage').head(top_n)\ntop_by_ballhawk = agg.sort_values('avg_ballhawk', ascending=False).head(top_n)\ntop_by_mirroring = agg.sort_values('avg_mirroring', ascending=False).head(top_n)\ntop_by_combined = agg.sort_values('combined_score', ascending=False).head(top_n)\n\nprint(f\"\\nDefenders with >= {min_plays_threshold} plays: {len(agg)}\")\n\nprint(\"\\n=== Top by avg coverage (lower better) ===\")\ndisplay(top_by_coverage[['primary_defender_id','plays_count','avg_coverage']].head(10))\n\nprint(\"\\n=== Top by avg ball_hawk_score (higher better) ===\")\ndisplay(top_by_ballhawk[['primary_defender_id','plays_count','avg_ballhawk']].head(10))\n\nprint(\"\\n=== Top by avg route_mirroring (higher better) ===\")\ndisplay(top_by_mirroring[['primary_defender_id','plays_count','avg_mirroring']].head(10))\n\nprint(f\"\\n=== Top by combined score \"\n      f\"(weights: coverage {weight_coverage:.2f}, ballhawk {weight_ballhawk:.2f}, mirroring {weight_mirroring:.2f}) ===\")\ndisplay(top_by_combined[['primary_defender_id','plays_count','combined_score',\n                         'avg_coverage','avg_ballhawk','avg_mirroring']].head(10))\n\n# Save leaderboard\nagg_sorted = agg.sort_values('combined_score', ascending=False).reset_index(drop=True)\nagg_sorted.to_csv('defender_leaderboard_3metrics.csv', index=False)\nprint(\"\\n✅ Saved defender_leaderboard_3metrics.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T18:56:21.783804Z","iopub.execute_input":"2025-09-30T18:56:21.784119Z","iopub.status.idle":"2025-09-30T18:56:21.873166Z","shell.execute_reply.started":"2025-09-30T18:56:21.7841Z","shell.execute_reply":"2025-09-30T18:56:21.872182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ---------- Load metrics ----------\npm = pd.read_csv(\"play_metrics_with_route_mirroring.csv\")\n\n# Clean types\npm['primary_defender_id'] = pd.to_numeric(pm['primary_defender_id'], errors='coerce').astype('Int64')\npm = pm.dropna(subset=['primary_defender_id'])\n\n# ---------- Aggregate per defender ----------\nagg = pm.groupby('primary_defender_id').agg(\n    plays_count=('coverage_tightness','count'),\n    avg_coverage=('coverage_tightness','mean'),\n    avg_ballhawk=('ball_hawk_score','mean'),\n    avg_mirroring=('route_mirroring','mean')\n).reset_index()\n\n# ---------- Filter defenders ----------\nmin_plays_threshold = 5\nagg = agg[agg['plays_count'] >= min_plays_threshold].copy()\n\n# ---------- Normalize metrics ----------\ndef minmax(series):\n    mn, mx = series.min(), series.max()\n    if mn == mx: return pd.Series(0.5, index=series.index)\n    return (series - mn) / (mx - mn)\n\nagg['coverage_norm'] = minmax(agg['avg_coverage'])\nagg['coverage_inv'] = 1 - agg['coverage_norm']\nagg['ballhawk_norm'] = minmax(agg['avg_ballhawk'])\nagg['mirror_norm']   = minmax(agg['avg_mirroring'])\n\n# ---------- Overall Defender Score ----------\nw_cov, w_bh, w_mir = 0.40, 0.25, 0.35\nagg['ODS'] = (agg['coverage_inv']*w_cov +\n              agg['ballhawk_norm']*w_bh +\n              agg['mirror_norm']*w_mir)\n\n# ---------- Rankings ----------\nagg_sorted = agg.sort_values('ODS', ascending=False).reset_index(drop=True)\nagg_sorted.to_csv(\"defender_overall_score.csv\", index=False)\n\nprint(f\"✅ Saved defender_overall_score.csv — {len(agg_sorted)} defenders ranked\")\nprint(\"\\n=== Top 10 Overall Defenders ===\")\ndisplay(agg_sorted[['primary_defender_id','plays_count','ODS',\n                    'avg_coverage','avg_ballhawk','avg_mirroring']].head(10))\n\n# ---------- Visualizations ----------\ntop_n = 15\ntop_overall = agg_sorted.head(top_n)\n\nplt.figure(figsize=(10,6))\nplt.bar(top_overall['primary_defender_id'].astype(str), top_overall['ODS'])\nplt.xticks(rotation=90)\nplt.title(f\"Top {top_n} Defenders by Overall Defender Score\")\nplt.ylabel(\"Overall Defender Score (ODS)\")\nplt.tight_layout()\nplt.show()\n\n# Individual metric comparison\nfig, axes = plt.subplots(1,3, figsize=(15,5), sharey=True)\naxes[0].bar(top_overall['primary_defender_id'].astype(str), top_overall['coverage_inv'])\naxes[0].set_title(\"Coverage (inverted, higher=better)\")\naxes[1].bar(top_overall['primary_defender_id'].astype(str), top_overall['ballhawk_norm'])\naxes[1].set_title(\"Ball Hawk (normalized)\")\naxes[2].bar(top_overall['primary_defender_id'].astype(str), top_overall['mirror_norm'])\naxes[2].set_title(\"Route Mirroring (normalized)\")\nfor ax in axes: ax.tick_params(axis='x', rotation=90)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:04:01.101619Z","iopub.execute_input":"2025-09-30T19:04:01.102534Z","iopub.status.idle":"2025-09-30T19:04:02.000067Z","shell.execute_reply.started":"2025-09-30T19:04:01.102451Z","shell.execute_reply":"2025-09-30T19:04:01.998438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROSTER MERGE: try players.csv first, otherwise build roster from input_df\nimport os\nimport pandas as pd\n\n# 1) Adjust this path to the top-level folder of your dataset if different.\nkaggle_input_root = '/kaggle/input'\n\n# Helper: try to find likely roster files under /kaggle/input\npossible_rosters = []\nfor root, dirs, files in os.walk(kaggle_input_root):\n    for f in files:\n        name = f.lower()\n        if 'player' in name and name.endswith('.csv'):\n            possible_rosters.append(os.path.join(root, f))\n\nprint(\"Found candidate roster files (first 10):\", possible_rosters[:10])\n\n# Try to load players.csv if present\nroster = None\nif possible_rosters:\n    # prefer a file called players.csv if it exists\n    players_paths = [p for p in possible_rosters if os.path.basename(p).lower() == 'players.csv']\n    candidate = players_paths[0] if players_paths else possible_rosters[0]\n    try:\n        roster = pd.read_csv(candidate, low_memory=False)\n        print(\"Loaded roster from:\", candidate)\n    except Exception as e:\n        print(\"Could not read candidate roster:\", candidate, \" — error:\", e)\n        roster = None\n\n# If roster still None, try to build from input_df (you mentioned you have input_df loaded earlier)\nif roster is None:\n    try:\n        # input_df should already exist in your notebook from earlier steps\n        input_df  # just referencing to trigger NameError if not present\n        print(\"Building roster from input_df (using nfl_id, player_name, player_position if present).\")\n        cols = []\n        for c in ['nfl_id','player_name','player_position','position','display_name']:\n            if c in input_df.columns:\n                cols.append(c)\n        # Prefer column names mapping: nfl_id, display_name, position\n        if 'nfl_id' not in input_df.columns:\n            raise KeyError(\"input_df does not contain 'nfl_id' — cannot build roster automatically.\")\n        # find name column\n        name_col = None\n        for nc in ['display_name','player_name','name']:\n            if nc in input_df.columns:\n                name_col = nc\n                break\n        pos_col = None\n        for pc in ['player_position','position','pos']:\n            if pc in input_df.columns:\n                pos_col = pc\n                break\n\n        roster = input_df[['nfl_id'] + ([name_col] if name_col else []) + ([pos_col] if pos_col else [])].drop_duplicates().rename(\n            columns={name_col: 'display_name', pos_col: 'position'} if name_col or pos_col else {}\n        )\n        print(\"Built roster from input_df; sample rows:\")\n        display(roster.head(10))\n    except NameError:\n        roster = None\n        print(\"input_df not found in memory — cannot build roster. If you have a roster CSV, upload it to the Kaggle input or run the earlier load steps to create input_df.\")\n\nif roster is None:\n    raise RuntimeError(\"Roster not found and could not be built. Place a roster CSV in the dataset or ensure input_df is loaded.\")\n\n# Normalize roster columns for merging\n# Expect roster has 'nfl_id', 'display_name', 'position' (if not present, create placeholders)\nif 'nfl_id' not in roster.columns:\n    raise RuntimeError(\"Roster doesn't contain nfl_id column; cannot proceed.\")\nfor col in ['display_name','position']:\n    if col not in roster.columns:\n        roster[col] = None\n\n# Make sure nfl_id numeric\nroster['nfl_id'] = pd.to_numeric(roster['nfl_id'], errors='coerce').astype('Int64')\n\n# --- Now create agg if not created yet (aggregate play_metrics into defender-level agg) ---\n# If you already have `agg` or `agg_named` from earlier leaderboard code, this will reuse it.\ntry:\n    agg  # if exists, keep it\nexcept NameError:\n    # build agg from play_metrics_with_route_mirroring.csv\n    pm_path = 'play_metrics_with_route_mirroring.csv'\n    if not os.path.exists(pm_path):\n        raise RuntimeError(f\"{pm_path} not found. Run the metric computation step first.\")\n    pm = pd.read_csv(pm_path)\n    pm['primary_defender_id'] = pd.to_numeric(pm['primary_defender_id'], errors='coerce').astype('Int64')\n    pm = pm.dropna(subset=['primary_defender_id'])\n    agg = pm.groupby('primary_defender_id').agg(\n        plays_count=('coverage_tightness','count'),\n        avg_coverage=('coverage_tightness','mean'),\n        avg_ballhawk=('ball_hawk_score','mean'),\n        avg_mirroring=('route_mirroring','mean')\n    ).reset_index()\n    # filter (same threshold you used before)\n    min_plays_threshold = 5\n    agg = agg[agg['plays_count'] >= min_plays_threshold].copy()\n\n# Normalize & compute ODS (same as earlier)\ndef minmax(series):\n    mn, mx = series.min(), series.max()\n    if mn == mx: return pd.Series(0.5, index=series.index)\n    return (series - mn) / (mx - mn)\n\nagg['coverage_norm'] = minmax(agg['avg_coverage'])\nagg['coverage_inv'] = 1 - agg['coverage_norm']\nagg['ballhawk_norm'] = minmax(agg['avg_ballhawk'])\nagg['mirror_norm'] = minmax(agg['avg_mirroring'])\n\nw_cov, w_bh, w_mir = 0.40, 0.25, 0.35\nagg['ODS'] = (agg['coverage_inv']*w_cov +\n              agg['ballhawk_norm']*w_bh +\n              agg['mirror_norm']*w_mir)\n\n# Merge roster to build agg_named\nagg_named = agg.merge(roster[['nfl_id','display_name','position']], left_on='primary_defender_id', right_on='nfl_id', how='left')\nagg_named = agg_named.drop(columns=['nfl_id'])\nagg_named['display_name'] = agg_named['display_name'].fillna(agg_named['primary_defender_id'].astype(str))\nagg_named['position'] = agg_named['position'].fillna('UNK')\n\n# Save and show\nagg_named = agg_named.sort_values('ODS', ascending=False).reset_index(drop=True)\nagg_named.to_csv('defender_leaderboard_named.csv', index=False)\nprint(\"Created defender_leaderboard_named.csv — sample:\")\ndisplay(agg_named.head(15))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:14:47.332111Z","iopub.execute_input":"2025-09-30T19:14:47.332502Z","iopub.status.idle":"2025-09-30T19:14:47.475695Z","shell.execute_reply.started":"2025-09-30T19:14:47.332476Z","shell.execute_reply":"2025-09-30T19:14:47.474657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# --- Load play-level metrics ---\npm = pd.read_csv(\"play_metrics_with_route_mirroring.csv\")\n\n# Clean defender IDs\npm['primary_defender_id'] = pd.to_numeric(pm['primary_defender_id'], errors='coerce').astype('Int64')\npm = pm.dropna(subset=['primary_defender_id'])\n\n# --- Aggregate defender averages ---\nagg = pm.groupby('primary_defender_id').agg(\n    plays_count=('coverage_tightness','count'),\n    avg_coverage=('coverage_tightness','mean'),\n    avg_ballhawk=('ball_hawk_score','mean'),\n    avg_mirroring=('route_mirroring','mean')\n).reset_index()\n\n# --- Filter defenders with minimum plays ---\nmin_plays_threshold = 5\nagg = agg[agg['plays_count'] >= min_plays_threshold].copy()\n\n# --- Normalize metrics (all scaled to 0–1, higher=better) ---\ndef minmax(series):\n    mn, mx = series.min(), series.max()\n    if mn == mx: return pd.Series(0.5, index=series.index)\n    return (series - mn) / (mx - mn)\n\nagg['coverage_norm'] = minmax(agg['avg_coverage'])\nagg['coverage_inv'] = 1 - agg['coverage_norm']   # invert coverage (lower is better)\nagg['ballhawk_norm'] = minmax(agg['avg_ballhawk'])\nagg['mirror_norm']   = minmax(agg['avg_mirroring'])\n\n# --- Overall Defender Score (ODS) ---\nw_cov, w_bh, w_mir = 0.40, 0.25, 0.35\nagg['ODS'] = (agg['coverage_inv']*w_cov +\n              agg['ballhawk_norm']*w_bh +\n              agg['mirror_norm']*w_mir)\n\n# --- Merge roster for names/positions ---\ntry:\n    roster = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2026-analytics/players.csv\")\n    agg_named = agg.merge(roster[['nfl_id','display_name','position']],\n                          left_on='primary_defender_id', right_on='nfl_id', how='left')\n    agg_named = agg_named.drop(columns=['nfl_id'])\nexcept FileNotFoundError:\n    print(\"⚠️ Roster file not found, continuing without names\")\n    agg_named = agg.copy()\n    agg_named['display_name'] = agg_named['primary_defender_id']\n    agg_named['position'] = \"UNK\"\n\n# --- Save leaderboard ---\nagg_named = agg_named.sort_values('ODS', ascending=False).reset_index(drop=True)\nagg_named.to_csv(\"defender_leaderboard_named.csv\", index=False)\n\nprint(f\"✅ Created defender_leaderboard_named.csv with {len(agg_named)} defenders\")\ndisplay(agg_named.head(15))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:15:06.985036Z","iopub.execute_input":"2025-09-30T19:15:06.987247Z","iopub.status.idle":"2025-09-30T19:15:07.061122Z","shell.execute_reply.started":"2025-09-30T19:15:06.987193Z","shell.execute_reply":"2025-09-30T19:15:07.05873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef plot_radar(player_row, title=None, metrics=None, labels=None):\n    \"\"\"\n    Robust radar (spider) chart function.\n    - player_row: a Series-like row containing metric values.\n    - metrics: list of column names to read from player_row (defaults to\n               ['coverage_inv','ballhawk_norm','mirror_norm','ODS']).\n    - labels: display labels for the metrics (defaults accordingly).\n    \"\"\"\n    if metrics is None:\n        metrics = ['coverage_inv','ballhawk_norm','mirror_norm','ODS']\n    if labels is None:\n        labels = ['Coverage','Ball Hawk','Mirroring','Overall']\n    assert len(metrics) == len(labels), \"metrics and labels must match length\"\n\n    # Extract values; replace NaN with 0.0 (or 0.5 if you prefer neutral)\n    values = []\n    for m in metrics:\n        v = player_row.get(m, np.nan)\n        if pd.isna(v):\n            v = 0.0\n        values.append(float(v))\n\n    # Ensure values are in [0,1] (they should be if normalized earlier)\n    values = [max(0.0, min(1.0, v)) for v in values]\n\n    # number of variables\n    n = len(labels)\n\n    # compute angles for each axis (n angles)\n    angles = np.linspace(0, 2 * np.pi, n, endpoint=False).tolist()\n\n    # close the loop by appending first value & angle\n    values += values[:1]\n    angles += angles[:1]\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(5,5), subplot_kw=dict(polar=True))\n    ax.plot(angles, values, marker='o', linewidth=2)\n    ax.fill(angles, values, alpha=0.25)\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels(labels)\n    ax.set_ylim(0, 1)\n    if title is not None:\n        ax.set_title(title, fontsize=12, fontweight='bold')\n    plt.show()\n\n\n# Example: draw radar charts for top 5 defenders in agg_named\n# Make sure agg_named exists and contains the normalized columns used below.\nrequired_cols = ['coverage_inv','ballhawk_norm','mirror_norm','ODS','display_name','position']\nmissing = [c for c in required_cols if c not in agg_named.columns]\nif missing:\n    raise RuntimeError(f\"agg_named is missing required columns: {missing}. Run the leaderboard creation block first.\")\n\n# Plot top N\ntop_n = 5\nfor i in range(min(top_n, len(agg_named))):\n    row = agg_named.iloc[i]\n    title = f\"{row['display_name']} ({row.get('position','UNK')}) — ODS {row['ODS']:.2f}\"\n    plot_radar(row, title=title)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:17:05.465624Z","iopub.execute_input":"2025-09-30T19:17:05.466562Z","iopub.status.idle":"2025-09-30T19:17:07.590939Z","shell.execute_reply.started":"2025-09-30T19:17:05.466523Z","shell.execute_reply":"2025-09-30T19:17:07.589475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Use normalized metrics only\nX = agg_named[['coverage_inv','ballhawk_norm','mirror_norm']].fillna(0)\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nagg_named['cluster'] = kmeans.fit_predict(X)\n\nprint(\"Cluster counts:\")\nprint(agg_named['cluster'].value_counts())\n\n# Inspect cluster centroids\ncentroids = pd.DataFrame(kmeans.cluster_centers_, columns=['coverage','ballhawk','mirroring'])\nprint(\"\\nCluster archetypes (centroids):\")\ndisplay(centroids)\n\n# Visualize clusters (2D PCA for easy plotting)\nfrom sklearn.decomposition import PCA\nX_pca = PCA(n_components=2).fit_transform(X)\nagg_named['pca1'], agg_named['pca2'] = X_pca[:,0], X_pca[:,1]\n\nplt.figure(figsize=(8,6))\nfor c in agg_named['cluster'].unique():\n    subset = agg_named[agg_named['cluster']==c]\n    plt.scatter(subset['pca1'], subset['pca2'], label=f\"Cluster {c}\", alpha=0.7)\nplt.legend()\nplt.title(\"Defender Archetypes (PCA view)\")\nplt.xlabel(\"PCA 1\")\nplt.ylabel(\"PCA 2\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:12:02.231686Z","iopub.execute_input":"2025-09-30T19:12:02.23205Z","iopub.status.idle":"2025-09-30T19:12:02.623356Z","shell.execute_reply.started":"2025-09-30T19:12:02.232025Z","shell.execute_reply":"2025-09-30T19:12:02.622032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Clean, single-cell final report =====\n# Paste & run this in your Kaggle notebook (assumes previous files exist)\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom IPython.display import display, Markdown, HTML\n\n# --- Settings (tweak if needed) ---\nPLAY_METRICS_CSV = \"play_metrics_with_route_mirroring.csv\"\nLEADERBOARD_CSV = \"defender_leaderboard_named.csv\"\nMIN_PLAYS = 5\nTOP_N = 10   # how many to show in leaderboard and bar chart\nRADAR_N = 3  # top K to show radar charts\nW_COV, W_BH, W_MIR = 0.40, 0.25, 0.35  # ODS weights\n\n# --- Helper functions ---\ndef minmax(s):\n    mn, mx = s.min(), s.max()\n    if pd.isna(mn) or pd.isna(mx) or mn == mx:\n        return pd.Series(0.5, index=s.index)\n    return (s - mn) / (mx - mn)\n\ndef find_roster():\n    root = \"/kaggle/input\"\n    for r, dirs, files in os.walk(root):\n        for f in files:\n            if f.lower() == \"players.csv\" or (\"player\" in f.lower() and f.lower().endswith(\".csv\")):\n                return os.path.join(r, f)\n    return None\n\ndef plot_radar_axes(ax, vals, labels, title):\n    n = len(vals)\n    angles = np.linspace(0, 2*np.pi, n, endpoint=False).tolist()\n    vals = vals + vals[:1]\n    angles = angles + angles[:1]\n    ax.plot(angles, vals, marker='o', linewidth=1.8)\n    ax.fill(angles, vals, alpha=0.25)\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels(labels)\n    ax.set_ylim(0,1)\n    ax.set_title(title, fontsize=10, weight='bold')\n\n# --- 1) Load play-level metrics and show small sample (clean) ---\nif not os.path.exists(PLAY_METRICS_CSV):\n    raise RuntimeError(f\"{PLAY_METRICS_CSV} not found — run metrics generation first.\")\n\nplays = pd.read_csv(PLAY_METRICS_CSV)\ndisplay(Markdown(\"## 1) Sample play-level metrics (first 10 rows)\"))\ncols_sample = ['game_id','play_id','primary_defender_id','receiver_id','coverage_tightness','ball_hawk_score','route_mirroring']\ncols_sample = [c for c in cols_sample if c in plays.columns]\ndisplay(plays[cols_sample].head(10).style.set_table_attributes(\"style='display:inline'\").set_caption(\"Sample plays\"))\n\n# --- 2) Build defender leaderboard with ODS and merge roster if possible ---\nplays['primary_defender_id'] = pd.to_numeric(plays['primary_defender_id'], errors='coerce').astype('Int64')\nplays = plays.dropna(subset=['primary_defender_id'])\n\nagg = plays.groupby('primary_defender_id').agg(\n    plays_count=('coverage_tightness','count'),\n    avg_coverage=('coverage_tightness','mean'),\n    avg_ballhawk=('ball_hawk_score','mean'),\n    avg_mirroring=('route_mirroring','mean')\n).reset_index()\n\nagg = agg[agg['plays_count'] >= MIN_PLAYS].copy()\nagg['coverage_norm'] = minmax(agg['avg_coverage'])\nagg['coverage_inv'] = 1.0 - agg['coverage_norm']\nagg['ballhawk_norm'] = minmax(agg['avg_ballhawk'])\nagg['mirror_norm'] = minmax(agg['avg_mirroring'])\nagg['ODS'] = agg['coverage_inv']*W_COV + agg['ballhawk_norm']*W_BH + agg['mirror_norm']*W_MIR\n\n# attempt to merge roster for nicer labels\nroster_path = find_roster()\nif roster_path:\n    roster = pd.read_csv(roster_path, low_memory=False)\n    # find reasonable column names\n    id_col = next((c for c in ['nfl_id','nflId','player_id','id'] if c in roster.columns), None)\n    name_col = next((c for c in ['display_name','full_name','player_name','name'] if c in roster.columns), None)\n    pos_col = next((c for c in ['position','player_position','pos'] if c in roster.columns), None)\n    roster['nfl_id'] = pd.to_numeric(roster[id_col], errors='coerce').astype('Int64') if id_col else None\n    roster['display_name'] = roster[name_col] if name_col in roster.columns else roster.get('display_name', None)\n    roster['position'] = roster[pos_col] if pos_col in roster.columns else roster.get('position', None)\n    roster = roster[['nfl_id','display_name','position']].drop_duplicates(subset=['nfl_id'])\n    agg_named = agg.merge(roster, left_on='primary_defender_id', right_on='nfl_id', how='left').drop(columns=['nfl_id'])\nelse:\n    # fallback: show ID as name\n    agg_named = agg.copy()\n    agg_named['display_name'] = agg_named['primary_defender_id'].astype(str)\n    agg_named['position'] = 'UNK'\n\n# reorder columns for display\ndisplay_cols = ['primary_defender_id','display_name','position','plays_count','ODS','avg_coverage','avg_ballhawk','avg_mirroring']\ndisplay_cols = [c for c in display_cols if c in agg_named.columns]\n\n# Save leaderboard\nagg_named = agg_named.sort_values('ODS', ascending=False).reset_index(drop=True)\nagg_named.to_csv(LEADERBOARD_CSV, index=False)\n\ndisplay(Markdown(\"## 2) Top defenders (leaderboard)\"))\ndisplay(agg_named[display_cols].head(TOP_N).style.set_table_attributes(\"style='display:inline'\").set_caption(\"Top defenders by ODS\"))\n\n# --- 3) Clean bar chart for Top N by ODS ---\ndisplay(Markdown(\"## 3) Top defenders by Overall Defender Score (ODS)\"))\ntop = agg_named.head(TOP_N).copy()\nplt.figure(figsize=(10,4))\nplt.bar(top['display_name'].astype(str), top['ODS'])\nplt.xticks(rotation=45, ha='right')\nplt.ylabel(\"ODS (0-1)\")\nplt.title(f\"Top {TOP_N} Defenders by ODS\")\nplt.tight_layout()\nplt.show()\n\n# --- 4) Compact radar charts for top RADAR_N defenders (single row) ---\ndisplay(Markdown(f\"## 4) Skill profiles (top {RADAR_N} defenders)\"))\nlabels = ['Coverage','Ball Hawk','Mirroring','Overall']\nmetrics = ['coverage_inv','ballhawk_norm','mirror_norm','ODS']\n\nfig, axes = plt.subplots(1, min(RADAR_N, len(agg_named)), figsize=(4*min(RADAR_N, len(agg_named)),4), subplot_kw=dict(polar=True))\nif min(RADAR_N, len(agg_named)) == 1:\n    axes = [axes]\nfor i, ax in enumerate(axes):\n    row = agg_named.iloc[i]\n    vals = [float(row.get(m,0.0) if not pd.isna(row.get(m)) else 0.0) for m in metrics]\n    vals = [max(0.0, min(1.0, v)) for v in vals]\n    plot_radar_axes(ax, vals, labels, f\"{row.get('display_name','ID:'+str(int(row['primary_defender_id'])))}\\n{row.get('position','')}\\nODS {row['ODS']:.2f}\")\nplt.tight_layout()\nplt.show()\n\n# --- 5) Clustering & PCA scatter (compact) ---\ndisplay(Markdown(\"## 5) Defender archetypes (clustering & PCA)\"))\nX = agg_named[['coverage_inv','ballhawk_norm','mirror_norm']].fillna(0)\nif len(X) >= 3:\n    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X)\n    agg_named['cluster'] = kmeans.labels_\n    centroids = pd.DataFrame(kmeans.cluster_centers_, columns=['coverage_inv','ballhawk_norm','mirror_norm'])\n    display(Markdown(\"**Cluster centroids (archetypes)**\"))\n    display(centroids.style.set_table_attributes(\"style='display:inline'\"))\n    # PCA for 2D scatter\n    X_pca = PCA(n_components=2).fit_transform(X)\n    agg_named['pca1'], agg_named['pca2'] = X_pca[:,0], X_pca[:,1]\n    plt.figure(figsize=(8,5))\n    for c in sorted(agg_named['cluster'].unique()):\n        s = agg_named[agg_named['cluster']==c]\n        plt.scatter(s['pca1'], s['pca2'], label=f\"Cluster {c}\", alpha=0.8)\n    plt.legend(); plt.title(\"Defender archetypes (PCA projection)\"); plt.xlabel(\"PCA1\"); plt.ylabel(\"PCA2\")\n    plt.tight_layout(); plt.show()\nelse:\n    display(Markdown(\"_Not enough defenders to perform clustering (need >=3)_\"))\n\n# --- 6) Auto-generate an executive summary file (optional) ---\nsummary_md = f\"\"\"\n# Executive Summary — Defender Analytics\n\n**What**: Three defender metrics were created from tracking: \n- **Coverage Tightness** (lower is better),\n- **Ball Hawk Score** (higher is better),\n- **Route Mirroring** (higher is better).\n\nThese were combined into an **Overall Defender Score (ODS)** with weights:\n- Coverage {W_COV}, Ball Hawk {W_BH}, Mirroring {W_MIR}.\n\n**Top results**: saved to `{LEADERBOARD_CSV}` (top {TOP_N} shown above).  \n**Clustering**: simple K-Means found archetypes — centroids printed above.\n\n_Notes_: metrics are normalized (min-max). Coverage was inverted so higher=better for ODS.\n\"\"\"\nwith open(\"EXECUTIVE_SUMMARY.md\",\"w\") as f:\n    f.write(summary_md)\n\ndisplay(Markdown(\"✅ **Report generated.** Files saved:\"))\ndisplay(HTML(f\"<ul><li><b>{PLAY_METRICS_CSV}</b></li><li><b>{LEADERBOARD_CSV}</b></li><li><b>EXECUTIVE_SUMMARY.md</b></li></ul>\"))\n\n# ===== done =====\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:22:49.747671Z","iopub.execute_input":"2025-09-30T19:22:49.74806Z","iopub.status.idle":"2025-09-30T19:22:51.365437Z","shell.execute_reply.started":"2025-09-30T19:22:49.748037Z","shell.execute_reply":"2025-09-30T19:22:51.36378Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Executive Summary — Defender Analytics\n\n## What we built\n\nWe engineered **three advanced defender metrics** from player tracking and combined them into a single, defensible score:\n\n* **Coverage Tightness** — average spatial separation between a defender and the targeted receiver during the route. (Lower = better.)\n* **Ball Hawk Score** — average velocity component of the defender **toward the ball’s landing point** (positive = closing toward the ball).\n* **Route Mirroring** — how well the defender mirrors the receiver’s *direction* and *speed* frame-by-frame (cosine alignment × speed-ratio; higher = better).\n\nThese were normalized and combined into an **Overall Defender Score (ODS)** to rank defenders.\n\n---\n\n## How ODS is computed\n\n1. Per-defender aggregates: `avg_coverage`, `avg_ballhawk`, `avg_mirroring`.\n2. Min–max normalize each metric to `[0,1]`.\n3. Invert coverage so higher = better: `coverage_inv = 1 - coverage_norm`.\n4. Weighted sum (default weights used in this notebook):\n\n```\nODS = 0.40 * coverage_inv  +  0.25 * ballhawk_norm  +  0.35 * mirror_norm\n```\n\n(Weights can be tuned; coverage prioritized for discipline, mirroring emphasizes agility, ball-hawk measures opportunism.)\n\n---\n\n## Key results (what to look at)\n\n* `play_metrics_with_route_mirroring.csv` — per-play metrics (coverage_tightness, ball_hawk_score, route_mirroring).\n* `defender_leaderboard_named.csv` — per-defender aggregates + ODS (includes names/positions if roster available).\n* Top defenders are shown in the leaderboard table and ranked by ODS.\n\n**Interpreting leaderboard columns:**\n\n* `primary_defender_id`: defender’s nfl_id (or map to `display_name` if roster merged).\n* `plays_count`: number of plays used to compute averages (we filtered low-sample defenders).\n* `avg_coverage`: average separation (yards or field units).\n* `avg_ballhawk`: average closing-speed component (units/frame).\n* `avg_mirroring`: average mirror score (−1..1, realistically 0..1 after filtering).\n* `ODS`: combined 0–1 overall score (higher = better).\n\n---\n\n## Methods (short)\n\n* Data: tracking + supplementary/input files from the dataset.\n* Velocities: estimated from per-player `x,y,frame_id` diffs (units per frame). If you know frame rate (e.g., 10 fps), multiply velocities by that to convert to units/second.\n* Primary defender: chosen as the defender closest to the targeted receiver at route start.\n* Route Mirroring: for frames where both players move, compute normalized direction vectors and take `dir_score = dot(rec_dir, def_dir)`, `speed_ratio = min(def_speed, rec_speed)/max(def_speed, rec_speed)`, then `mirror_frame_score = dir_score * speed_ratio`. Play metric = mean of frame scores.\n\n---\n\n## Immediate insights & next steps\n\n* **Top-performers**: defenders with high ODS combine tight coverage, strong mirroring, and solid ball-hawk behavior — these are likely high-value coverage players.\n* **Archetypes** (from K-Means): defenders tend to cluster into styles such as:\n\n  * Sticky coverage specialists (tight separation, high coverage_inv),\n  * Ball-hawks (high ballhawk_norm, sometimes larger avg_coverage),\n  * Agile mirrors (high mirror_norm).\n\n**Suggested next analyses**\n\n* Merge play context (down, distance, passer type) to control for play difficulty.\n* Convert velocities to `yards/sec` using frame rate for domain-friendly interpretation.\n* Produce play-level visualizations for top defenders (animated traces + metric timelines) to validate edge cases.\n* Increase `min_plays` threshold for more robust defender rankings or apply bootstrapped confidence intervals.\n\n---\n\n## Files produced\n\n* `play_metrics_with_route_mirroring.csv` — per-play metrics\n* `defender_leaderboard_named.csv` — per-defender aggregates + ODS (merged with roster when available)\n* `defender_overall_score.csv` — alternative name for ODS output (if present)\n* `EXECUTIVE_SUMMARY.md` — this summary (also saved to disk)\n\n---\n\n## Short note on reproducibility\n\nRun the notebook top-to-bottom. Key blocks to run (in order):\n\n1. Load tracking + input + supplementary CSVs.\n2. Merge player role & ball landing data.\n3. Estimate velocities (vx, vy) from `x,y,frame_id`.\n4. Compute per-play metrics (coverage, ball-hawk, mirroring).\n5. Aggregate to defender-level, normalize, compute ODS, and produce visualizations.","metadata":{}},{"cell_type":"code","source":"# show top-level sizes under /kaggle/input and /kaggle/working\n!du -sh /kaggle/input/* | sort -h\n!du -sh /kaggle/working/* | sort -h\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:35:48.19573Z","iopub.execute_input":"2025-09-30T19:35:48.196212Z","iopub.status.idle":"2025-09-30T19:35:48.576948Z","shell.execute_reply.started":"2025-09-30T19:35:48.196182Z","shell.execute_reply":"2025-09-30T19:35:48.575446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nroot = Path('/kaggle/input')\nif root.exists():\n    csvs = list(root.rglob('*.csv'))\n    if not csvs:\n        print(\"No CSVs found under /kaggle/input\")\n    else:\n        sizes = []\n        for p in csvs:\n            sizes.append((p, p.stat().st_size))\n        sizes.sort(key=lambda x: x[1], reverse=True)\n        for p, s in sizes[:50]:   # show top 50 largest csvs\n            print(f\"{p} \\t {s/1024/1024:.2f} MB\")\nelse:\n    print(\"/kaggle/input not found\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:36:56.274178Z","iopub.execute_input":"2025-09-30T19:36:56.27455Z","iopub.status.idle":"2025-09-30T19:36:56.316611Z","shell.execute_reply.started":"2025-09-30T19:36:56.274519Z","shell.execute_reply":"2025-09-30T19:36:56.315517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\nmem = psutil.virtual_memory()\nprint(f\"Total RAM: {mem.total/1024**3:.1f} GB, Available: {mem.available/1024**3:.1f} GB\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:37:26.849862Z","iopub.execute_input":"2025-09-30T19:37:26.850205Z","iopub.status.idle":"2025-09-30T19:37:26.857081Z","shell.execute_reply.started":"2025-09-30T19:37:26.85018Z","shell.execute_reply":"2025-09-30T19:37:26.855805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Full-dataset run (pandas, memory-optimized, parallel per-play) =====\n# Paste into one Kaggle cell and run. Adjust small variables below if desired.\n\nimport os, glob, gc, math\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\nimport multiprocessing\nfrom tqdm.auto import tqdm\n\n# --------- Settings (tweak if needed) ----------\nINPUT_ROOT = \"/kaggle/input\"\nOUT_PLAY_METRICS = \"play_metrics_full.csv\"\nOUT_LEADERBOARD = \"defender_leaderboard_full.csv\"\nMIN_PLAYS_FOR_LEADERBOARD = 5\nN_JOBS = max(1, multiprocessing.cpu_count() - 1)  # parallel workers for play processing\n\n# --------- Helper: classify CSV files by sample columns ----------\ndef classify_csv(path, nrows=200):\n    try:\n        sample = pd.read_csv(path, nrows=nrows, low_memory=False)\n    except Exception:\n        return \"unknown\"\n    cols = set(sample.columns.str.lower())\n    # tracking heuristics\n    if {'frame_id','x','y'}.issubset(cols) or {'x','y','frame_id'} & cols:\n        return \"tracking\"\n    # input file heuristics: player_role, ball_land_x, nfl_id hints\n    if {'player_role','ball_land_x'}.intersection(cols) or 'player_name' in cols:\n        return \"input\"\n    # supplementary heuristics\n    if 'pass_result' in cols or 'play_description' in cols:\n        return \"supplementary\"\n    # fallback: unknown\n    return \"unknown\"\n\n# --------- find all csv files under /kaggle/input (common Kaggle dataset layout) ----------\nall_csvs = [str(p) for p in Path(INPUT_ROOT).rglob(\"*.csv\")]\nprint(\"Found CSV files:\", len(all_csvs))\n\ntracking_files, input_files, supp_files, other_files = [], [], [], []\nfor f in all_csvs:\n    kind = classify_csv(f, nrows=150)\n    if kind == \"tracking\":\n        tracking_files.append(f)\n    elif kind == \"input\":\n        input_files.append(f)\n    elif kind == \"supplementary\":\n        supp_files.append(f)\n    else:\n        other_files.append(f)\n\nprint(f\"tracking: {len(tracking_files)}, input: {len(input_files)}, supplementary: {len(supp_files)}, other: {len(other_files)}\")\n\nif len(tracking_files) == 0:\n    raise RuntimeError(\"No tracking CSVs found under /kaggle/input. Check dataset organization.\")\n\n# --------- read and concat input and supplementary (small) ----------\n# load ALL input files (player roles, ball landing coords)\nusecols_input_guess = ['game_id','play_id','nfl_id','player_role','ball_land_x','ball_land_y','player_name','player_position']\ninput_dfs = []\nfor p in input_files:\n    try:\n        df = pd.read_csv(p, usecols=[c for c in usecols_input_guess if c in pd.read_csv(p, nrows=0).columns], low_memory=False)\n        input_dfs.append(df)\n    except Exception:\n        # fallback: read whole file and select columns present\n        df = pd.read_csv(p, low_memory=False)\n        cols = [c for c in usecols_input_guess if c in df.columns]\n        input_dfs.append(df[cols])\nif input_dfs:\n    input_df = pd.concat(input_dfs, ignore_index=True).drop_duplicates()\nelse:\n    input_df = pd.DataFrame(columns=['game_id','play_id','nfl_id','player_role','ball_land_x','ball_land_y'])\nprint(\"Input rows:\", len(input_df))\n\n# try to load a single supplementary file (if present)\nsupplementary_df = pd.DataFrame()\nif supp_files:\n    # pick first supplementary file\n    try:\n        supplementary_df = pd.read_csv(supp_files[0], low_memory=False)\n        print(\"Loaded supplementary:\", supp_files[0], \"rows:\", len(supplementary_df))\n    except Exception:\n        supplementary_df = pd.DataFrame()\n\n# --------- read and concat tracking files in memory-stepwise (only needed cols) ----------\n# tracking columns we'd ideally like:\ntracking_needed = ['game_id','play_id','nfl_id','frame_id','x','y','s','dir','o','a']\n# We'll detect columns for each file and read only present ones\ntracking_parts = []\nfor p in tqdm(tracking_files, desc=\"Reading tracking files\"):\n    try:\n        cols = pd.read_csv(p, nrows=0, low_memory=False).columns.tolist()\n    except Exception:\n        cols = None\n    if cols:\n        usecols = [c for c in tracking_needed if c in cols]\n        # always include game_id/play_id/nfl_id/frame_id/x/y (if available)\n        fallback_cols = ['game_id','play_id','nfl_id','frame_id','x','y']\n        for c in fallback_cols:\n            if c in (pd.read_csv(p, nrows=0, low_memory=False).columns.tolist()) and c not in usecols:\n                usecols.append(c)\n        try:\n            df = pd.read_csv(p, usecols=usecols, low_memory=False)\n            tracking_parts.append(df)\n        except Exception:\n            # fallback: read entire file (rare)\n            df = pd.read_csv(p, low_memory=False)\n            tracking_parts.append(df[[c for c in df.columns if c in tracking_needed or c in ['game_id','play_id','nfl_id','frame_id','x','y']]])\n    else:\n        print(\"Could not detect columns for\", p)\n\n# concat all tracking parts\ntracking = pd.concat(tracking_parts, ignore_index=True)\nprint(\"Total tracking rows:\", len(tracking))\ndel tracking_parts\ngc.collect()\n\n# --------- Merge input_df info (player_role, ball_land_x/y) into tracking by (game_id,play_id,nfl_id) ----------\nif not input_df.empty:\n    # make sure keys have matching dtype\n    for c in ['game_id','play_id','nfl_id']:\n        if c in input_df.columns and c in tracking.columns:\n            try:\n                input_df[c] = pd.to_numeric(input_df[c], errors='coerce').astype('Int64')\n                tracking[c] = pd.to_numeric(tracking[c], errors='coerce').astype('Int64')\n            except Exception:\n                pass\n    player_info = input_df[['game_id','play_id','nfl_id','player_role','ball_land_x','ball_land_y']].drop_duplicates()\n    tracking = tracking.merge(player_info, on=['game_id','play_id','nfl_id'], how='left')\n\n# --------- Convert dtypes to save memory where possible ----------\nfor c in ['game_id','play_id','nfl_id','frame_id']:\n    if c in tracking.columns:\n        tracking[c] = pd.to_numeric(tracking[c], errors='coerce').astype('Int64')\nfor c in ['x','y','s','dir','a','o']:\n    if c in tracking.columns:\n        tracking[c] = pd.to_numeric(tracking[c], errors='coerce').astype('float32')\n\n# --------- Estimate velocities (vx, vy, s_est) per player over frames (vectorized) ----------\ntracking = tracking.sort_values(['game_id','play_id','nfl_id','frame_id']).reset_index(drop=True)\ngrp = tracking.groupby(['game_id','play_id','nfl_id'], sort=False)\n# compute framewise diffs\ntracking[['dx','dy','dframe']] = grp[['x','y','frame_id']].diff().fillna(0)\n# guard dframe==0\ntracking['dframe'] = tracking['dframe'].replace(0, 1.0)\ntracking['vx'] = tracking['dx'] / tracking['dframe']\ntracking['vy'] = tracking['dy'] / tracking['dframe']\ntracking['s_est'] = np.sqrt(tracking['vx'].astype(float)**2 + tracking['vy'].astype(float)**2)\n# cleanup temporary cols\ntracking.drop(columns=[c for c in ['dx','dy','dframe'] if c in tracking.columns], inplace=True)\ngc.collect()\n\n# --------- Define metric function (same as earlier; compact) ----------\ndef calculate_play_metrics_with_route_mirroring(play_df):\n    # Keep compact and robust — return a dict for this play\n    keys = ['game_id','play_id','primary_defender_id','receiver_id','coverage_tightness','ball_hawk_score','route_mirroring']\n    out = {k: np.nan for k in keys}\n    try:\n        pdf = play_df.copy()\n        # basic guards\n        if 'player_role' not in pdf.columns or {'x','y','frame_id'}.difference(pdf.columns):\n            return out\n        pdf['role_lower'] = pdf['player_role'].astype(str).str.lower()\n        rec_mask = pdf['role_lower'].str.contains('target|receiver', na=False)\n        def_mask = pdf['role_lower'].str.contains('defens|coverage|defend|db', na=False)\n        if not rec_mask.any() or not def_mask.any():\n            return out\n        receiver = pdf[rec_mask].copy()\n        defenders = pdf[def_mask].copy()\n        start_frame = int(pdf['frame_id'].min())\n        rec_start = receiver[receiver['frame_id']==start_frame]\n        def_start = defenders[defenders['frame_id']==start_frame]\n        if rec_start.empty:\n            rec_start = receiver.groupby('nfl_id', dropna=True).first().reset_index()\n        if def_start.empty:\n            def_start = defenders.groupby('nfl_id', dropna=True).first().reset_index()\n        if rec_start.empty or def_start.empty:\n            return out\n        # ball landing\n        if 'ball_land_x' not in pdf.columns or pdf['ball_land_x'].dropna().empty:\n            return out\n        ball_x = float(pdf['ball_land_x'].dropna().iloc[0])\n        ball_y = float(pdf['ball_land_y'].dropna().iloc[0])\n        rec_row = rec_start.iloc[0]\n        rec_x = rec_row.get('x', np.nan); rec_y = rec_row.get('y', np.nan)\n        out['receiver_id'] = rec_row.get('nfl_id', np.nan)\n        # primary defender at start\n        def_start = def_start.copy()\n        def_start['dist_to_rec'] = np.sqrt((def_start.get('x',0)-rec_x)**2 + (def_start.get('y',0)-rec_y)**2)\n        if def_start['dist_to_rec'].isnull().all():\n            return out\n        primary_idx = def_start['dist_to_rec'].idxmin()\n        primary_defender_id = def_start.loc[primary_idx, 'nfl_id']\n        out['primary_defender_id'] = primary_defender_id\n        # build paths\n        recv_cols = ['frame_id','x','y','vx','vy'] if {'vx','vy'}.issubset(receiver.columns) else ['frame_id','x','y']\n        defender_cols = ['frame_id','x','y','vx','vy'] if {'vx','vy'}.issubset(defenders.columns) else ['frame_id','x','y']\n        recv_path = receiver[[c for c in recv_cols if c in receiver.columns]].rename(columns={'x':'rec_x','y':'rec_y','vx':'rec_vx','vy':'rec_vy'})\n        defender_path = defenders[defenders['nfl_id']==primary_defender_id][[c for c in defender_cols if c in defenders.columns]].rename(columns={'x':'def_x','y':'def_y','vx':'def_vx','vy':'def_vy'})\n        if recv_path.empty or defender_path.empty:\n            return out\n        merged = pd.merge(recv_path, defender_path, on='frame_id', how='inner').sort_values('frame_id')\n        if merged.empty:\n            return out\n        # coverage\n        merged['separation'] = np.sqrt((merged['rec_x']-merged['def_x'])**2 + (merged['rec_y']-merged['def_y'])**2)\n        out['coverage_tightness'] = float(merged['separation'].mean())\n        # ball hawk\n        merged['ball_vec_x'] = ball_x - merged['def_x']; merged['ball_vec_y'] = ball_y - merged['def_y']\n        norm_ball = np.sqrt(merged['ball_vec_x']**2 + merged['ball_vec_y']**2).replace(0,1.0)\n        merged['ball_dir_x'] = merged['ball_vec_x'] / norm_ball; merged['ball_dir_y'] = merged['ball_vec_y'] / norm_ball\n        if {'def_vx','def_vy'}.issubset(merged.columns):\n            closing_speed = merged['def_vx'] * merged['ball_dir_x'] + merged['def_vy'] * merged['ball_dir_y']\n            out['ball_hawk_score'] = float(closing_speed.mean())\n        else:\n            out['ball_hawk_score'] = np.nan\n        # route mirroring: get rec_vx/rec_vy\n        if 'rec_vx' in merged.columns and 'rec_vy' in merged.columns:\n            merged['r_vx'] = merged['rec_vx']; merged['r_vy'] = merged['rec_vy']\n        else:\n            merged['r_vx'] = merged['rec_x'].diff().fillna(0); merged['r_vy'] = merged['rec_y'].diff().fillna(0)\n        if 'def_vx' not in merged.columns or 'def_vy' not in merged.columns:\n            merged['d_vx'] = merged['def_x'].diff().fillna(0); merged['d_vy'] = merged['def_y'].diff().fillna(0)\n        else:\n            merged['d_vx'] = merged['def_vx']; merged['d_vy'] = merged['def_vy']\n        merged['r_speed'] = np.sqrt(merged['r_vx']**2 + merged['r_vy']**2)\n        merged['d_speed'] = np.sqrt(merged['d_vx']**2 + merged['d_vy']**2)\n        valid = (merged['r_speed'] > 1e-6) & (merged['d_speed'] > 1e-6)\n        if valid.any():\n            merged.loc[valid, 'r_dir_x'] = merged.loc[valid,'r_vx']/merged.loc[valid,'r_speed']\n            merged.loc[valid, 'r_dir_y'] = merged.loc[valid,'r_vy']/merged.loc[valid,'r_speed']\n            merged.loc[valid, 'd_dir_x'] = merged.loc[valid,'d_vx']/merged.loc[valid,'d_speed']\n            merged.loc[valid, 'd_dir_y'] = merged.loc[valid,'d_vy']/merged.loc[valid,'d_speed']\n            merged.loc[valid, 'dir_score'] = (merged.loc[valid,'r_dir_x']*merged.loc[valid,'d_dir_x'] + merged.loc[valid,'r_dir_y']*merged.loc[valid,'d_dir_y']).clip(-1.0,1.0)\n            merged.loc[valid, 'speed_ratio'] = np.minimum(merged.loc[valid,'r_speed'], merged.loc[valid,'d_speed']) / np.maximum(merged.loc[valid,'r_speed'], merged.loc[valid,'d_speed'])\n            merged.loc[valid, 'mirror_frame'] = merged.loc[valid,'dir_score'] * merged.loc[valid,'speed_ratio']\n            out['route_mirroring'] = float(merged.loc[valid,'mirror_frame'].mean())\n        else:\n            out['route_mirroring'] = np.nan\n        out['game_id'] = int(pdf['game_id'].iloc[0]) if 'game_id' in pdf.columns else np.nan\n        out['play_id'] = int(pdf['play_id'].iloc[0]) if 'play_id' in pdf.columns else np.nan\n        return out\n    except Exception as e:\n        # optional: print(\"error\", e)\n        return out\n\n# --------- Prepare per-play keys to process in parallel ----------\nplays_keys = tracking[['game_id','play_id']].drop_duplicates().dropna().astype(int)\nplay_list = list(plays_keys.itertuples(index=False, name=None))\nprint(\"Total plays to process:\", len(play_list))\n\n# helper to extract play df and compute metrics\ndef process_play_pair(gid_pid):\n    gid, pid = gid_pid\n    play_df = tracking[(tracking['game_id']==gid) & (tracking['play_id']==pid)]\n    if play_df.empty:\n        return None\n    return calculate_play_metrics_with_route_mirroring(play_df)\n\n# run in parallel (joblib)\nresults = Parallel(n_jobs=N_JOBS, backend=\"loky\")(delayed(process_play_pair)(kp) for kp in tqdm(play_list, desc=\"Processing plays\"))\n\n# collect results\nmetrics = [r for r in results if r is not None]\nmetrics_df = pd.DataFrame(metrics)\nmetrics_df.to_csv(OUT_PLAY_METRICS, index=False)\nprint(\"Saved per-play metrics to:\", OUT_PLAY_METRICS, \"rows:\", len(metrics_df))\n\n# --------- Defender-level leaderboard (aggregate) ----------\nmetrics_df['primary_defender_id'] = pd.to_numeric(metrics_df['primary_defender_id'], errors='coerce').astype('Int64')\nmetrics_df = metrics_df.dropna(subset=['primary_defender_id'])\nagg = metrics_df.groupby('primary_defender_id').agg(\n    plays_count=('coverage_tightness','count'),\n    avg_coverage=('coverage_tightness','mean'),\n    avg_ballhawk=('ball_hawk_score','mean'),\n    avg_mirroring=('route_mirroring','mean')\n).reset_index()\nagg = agg[agg['plays_count'] >= MIN_PLAYS_FOR_LEADERBOARD].copy()\n\n# normalize metrics, compute ODS\ndef minmax(series):\n    mn, mx = series.min(), series.max()\n    if mn == mx or pd.isna(mn) or pd.isna(mx):\n        return pd.Series(0.5, index=series.index)\n    return (series - mn) / (mx - mn)\nagg['coverage_norm'] = minmax(agg['avg_coverage'])\nagg['coverage_inv'] = 1.0 - agg['coverage_norm']\nagg['ballhawk_norm'] = minmax(agg['avg_ballhawk'])\nagg['mirror_norm'] = minmax(agg['avg_mirroring'])\nagg['ODS'] = agg['coverage_inv']*W_COV + agg['ballhawk_norm']*W_BH + agg['mirror_norm']*W_MIR\n\n# try merge roster if a players.csv exists\nroster_path = None\nfor p in all_csvs:\n    if os.path.basename(p).lower() == 'players.csv':\n        roster_path = p; break\nif roster_path:\n    roster = pd.read_csv(roster_path, low_memory=False)\n    id_col = next((c for c in ['nfl_id','nflId','player_id','id'] if c in roster.columns), None)\n    name_col = next((c for c in ['display_name','full_name','player_name','name'] if c in roster.columns), None)\n    pos_col = next((c for c in ['position','player_position','pos'] if c in roster.columns), None)\n    roster['nfl_id'] = pd.to_numeric(roster[id_col], errors='coerce').astype('Int64') if id_col else None\n    roster['display_name'] = roster[name_col] if name_col in roster.columns else roster.get('display_name', None)\n    roster['position'] = roster[pos_col] if pos_col in roster.columns else roster.get('position', None)\n    roster = roster[['nfl_id','display_name','position']].drop_duplicates(subset=['nfl_id'])\n    agg_named = agg.merge(roster, left_on='primary_defender_id', right_on='nfl_id', how='left').drop(columns=['nfl_id'])\nelse:\n    agg_named = agg.copy()\n    agg_named['display_name'] = agg_named['primary_defender_id'].astype(str)\n    agg_named['position'] = 'UNK'\n\nagg_named = agg_named.sort_values('ODS', ascending=False).reset_index(drop=True)\nagg_named.to_csv(OUT_LEADERBOARD, index=False)\nprint(\"Saved defender leaderboard to:\", OUT_LEADERBOARD, \"rows:\", len(agg_named))\n\n# -- Done --\nprint(\"Full run complete. Play-metrics rows:\", len(metrics_df), \"Defender rows:\", len(agg_named))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:38:56.021713Z","iopub.execute_input":"2025-09-30T19:38:56.02211Z","iopub.status.idle":"2025-09-30T19:46:10.093842Z","shell.execute_reply.started":"2025-09-30T19:38:56.022085Z","shell.execute_reply":"2025-09-30T19:46:10.092695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Diagnostic step (run this cell)\nimport os, pandas as pd\nfrom pathlib import Path\n\nprint(\"== Diagnostic check: play_metrics_full.csv and input files ==\")\npm_path = \"play_metrics_full.csv\"\nif os.path.exists(pm_path):\n    pm = pd.read_csv(pm_path)\n    print(f\"\\nplay_metrics file found: {pm_path}  rows = {len(pm)}\")\n    print(\"Columns:\", pm.columns.tolist())\n    if 'primary_defender_id' in pm.columns:\n        nonnull = pm['primary_defender_id'].notna().sum()\n        pct = 100.0 * nonnull / len(pm) if len(pm)>0 else 0.0\n        print(f\"primary_defender_id non-null: {nonnull} / {len(pm)}  ({pct:.1f}%)\")\n    else:\n        print(\"primary_defender_id column: NOT PRESENT\")\n    print(\"\\nFirst 8 rows of play_metrics (showing key cols):\")\n    cols_to_show = [c for c in ['game_id','play_id','primary_defender_id','receiver_id','coverage_tightness','ball_hawk_score','route_mirroring'] if c in pm.columns]\n    display(pm[cols_to_show].head(8))\nelse:\n    print(f\"\\nplay_metrics file NOT found at: {pm_path}\")\n    pm = None\n\n# Search for input-like files under /kaggle/input\nprint(\"\\nSearching /kaggle/input for candidate input files (player_role / ball_land columns)...\")\ncandidates = []\nfor p in Path('/kaggle/input').rglob('*.csv'):\n    name = p.name.lower()\n    if 'input' in name or 'player' in name or 'train' in name:\n        candidates.append(str(p))\n# Deduplicate and print a few\ncandidates = list(dict.fromkeys(candidates))\nprint(\"Candidate CSVs found:\", len(candidates))\nfor c in candidates[:10]:\n    print(\" -\", c)\n\n# Show sample columns for promising candidates (first one or two)\nchecked = 0\nfor c in candidates[:6]:\n    try:\n        sample = pd.read_csv(c, nrows=3)\n    except Exception as e:\n        print(\"  cannot read\", c, \":\", e)\n        continue\n    cols = sample.columns.tolist()\n    # check for required columns\n    has_role = any(x.lower()=='player_role' or 'role' in x.lower() for x in cols)\n    has_ball = any('ball_land' in x.lower() for x in cols)\n    has_nfl = any(x.lower()=='nfl_id' or 'nfl' in x.lower() for x in cols)\n    print(f\"\\nSample columns for {c}:\")\n    print(cols)\n    print(\"Contains player_role?\", has_role, \"| contains ball_land_x/ball_land_y?\", has_ball, \"| contains nfl_id?\", has_nfl)\n    checked += 1\n    if checked>=3:\n        break\n\n# Final guidance message\nprint(\"\\n=== Guidance ===\")\nif pm is None:\n    print(\"1) play_metrics_full.csv missing. Re-run the per-play metric pipeline (full-run cell).\")\nelse:\n    if 'primary_defender_id' not in pm.columns or pm['primary_defender_id'].notna().sum() == 0:\n        print(\"2) primary_defender_id is missing or all-null in play_metrics_full.csv.\")\n        print(\"   Likely cause: the pipeline did not merge the `input` files containing player_role/ball_land_x before computing metrics.\")\n        print(\"   Next action: locate the correct input CSV (one of the candidate files above should contain 'player_role' and 'ball_land_x'), then re-run the full metrics cell so each play is computed with player_role & ball landing info.\")\n    else:\n        print(\"3) primary_defender_id is present. Good — you can regenerate the defender leaderboard from play_metrics_full.csv now.\")\n        print(\"   If you want, run: \")\n        print(\"      # rebuild leaderboard from saved play metrics\")\n        print(\"      import pandas as pd\")\n        print(\"      pm = pd.read_csv('play_metrics_full.csv')\")\n        print(\"      pm['primary_defender_id']=pd.to_numeric(pm['primary_defender_id'],errors='coerce').astype('Int64')\")\n        print(\"      pm = pm.dropna(subset=['primary_defender_id'])\")\n        print(\"      # then aggregate & compute ODS as earlier\")\n        \nprint(\"\\nAfter running this cell, paste the printed output here and I'll give the exact one-line or one-cell fix to run next.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:54:05.925281Z","iopub.execute_input":"2025-09-30T19:54:05.925684Z","iopub.status.idle":"2025-09-30T19:54:06.031571Z","shell.execute_reply.started":"2025-09-30T19:54:05.925658Z","shell.execute_reply":"2025-09-30T19:54:06.030534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Robust locator + test-merger for tracking/input files\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport re\n\nROOT = Path(\"/kaggle/input\")\nprint(\"Searching under:\", ROOT)\n\n# 1) list top CSV files to inspect names (first 60)\nall_csvs = list(ROOT.rglob(\"*.csv\"))\nprint(\"Total CSVs found:\", len(all_csvs))\nprint(\"\\nExample CSVs (first 60):\")\nfor p in all_csvs[:60]:\n    print(\" \", p)\n\n# 2) Heuristic functions to detect tracking vs input by peeking columns\ndef is_tracking(path):\n    try:\n        cols = pd.read_csv(path, nrows=2).columns.str.lower().tolist()\n    except Exception:\n        return False\n    # tracking usually has frame_id, x, y and nfl_id\n    return ('frame_id' in cols and 'x' in cols and 'y' in cols)\n\ndef is_input(path):\n    try:\n        cols = pd.read_csv(path, nrows=2).columns.str.lower().tolist()\n    except Exception:\n        return False\n    # input files often have player_role and ball_land_x\n    return ('player_role' in cols or 'ball_land_x' in cols or 'player_name' in cols)\n\n# 3) find candidates\ntracking_candidates = []\ninput_candidates = []\nfor p in all_csvs:\n    pstr = str(p)\n    if is_tracking(pstr):\n        tracking_candidates.append(pstr)\n    if is_input(pstr):\n        input_candidates.append(pstr)\n\nprint(f\"\\nHeuristic results -> tracking candidates: {len(tracking_candidates)}, input candidates: {len(input_candidates)}\")\n\n# show examples\nprint(\"\\nFirst 12 tracking candidates:\")\nfor t in tracking_candidates[:12]:\n    print(\" \", t)\nprint(\"\\nFirst 12 input candidates:\")\nfor t in input_candidates[:12]:\n    print(\" \", t)\n\n# 4) try pairing by week/order using filename patterns\ndef extract_week_identifier(path_str):\n    # typical filenames: input_2023_w01.csv or output_2023_w01.csv etc.\n    m = re.search(r'(?:w|week|_w)(\\d{1,2})', path_str, flags=re.IGNORECASE)\n    if m:\n        return int(m.group(1))\n    # try year-week pattern like 20230907 or _w01 alternative patterns\n    m2 = re.search(r'2023[_-]?w?(\\d{1,2})', path_str, flags=re.IGNORECASE)\n    if m2:\n        return int(m2.group(1))\n    return None\n\n# build dicts\ntrack_by_week = {}\nfor p in tracking_candidates:\n    w = extract_week_identifier(p)\n    track_by_week.setdefault(w, []).append(p)\n\ninput_by_week = {}\nfor p in input_candidates:\n    w = extract_week_identifier(p)\n    input_by_week.setdefault(w, []).append(p)\n\nprint(\"\\nMatched weeks (example keys):\")\nprint(\" tracking weeks:\", sorted(k for k in track_by_week.keys() if k is not None)[:10])\nprint(\" input weeks:   \", sorted(k for k in input_by_week.keys() if k is not None)[:10])\n\n# 5) create list of matched pairs (week-based first), then fallback to best-effort by reading columns\npairs = []\n# match by week id when possible\nfor w in sorted(set(list(track_by_week.keys()) + list(input_by_week.keys()))):\n    tlist = track_by_week.get(w, [])\n    ilist = input_by_week.get(w, [])\n    if tlist and ilist:\n        # pair the first track file with first input file for this week\n        pairs.append((tlist[0], ilist[0]))\n# fallback: if no week-based pairs, try to pair first available tracking with first input\nif not pairs and tracking_candidates and input_candidates:\n    pairs.append((tracking_candidates[0], input_candidates[0]))\n\nprint(f\"\\nNumber of initial pairs found: {len(pairs)}\")\nif len(pairs) > 0:\n    print(\"\\nFirst pair:\")\n    print(\" TRACKING:\", pairs[0][0])\n    print(\" INPUT:   \", pairs[0][1])\nelse:\n    print(\"No pairs found by week; trying column-based best-effort pairing...\")\n\n# 6) If no pairs yet, attempt best-effort by checking which tracking file's game_id/play_id match input file game_id/play_id sample\nif not pairs:\n    for t in tracking_candidates[:10]:\n        for i in input_candidates[:10]:\n            try:\n                tdf = pd.read_csv(t, nrows=20)\n                idf = pd.read_csv(i, nrows=20)\n                # if share many game_id/play_id pairs in sample, treat as match\n                common = set(tdf.columns).intersection(set(idf.columns))\n                if {'game_id','play_id','nfl_id'}.issubset({c.lower() for c in tdf.columns}) and {'game_id','play_id'}.issubset({c.lower() for c in idf.columns}):\n                    pairs.append((t,i)); break\n            except Exception:\n                continue\n        if pairs: break\n    if pairs:\n        print(\"Found a pair via column-based heuristic:\")\n        print(\" TRACKING:\", pairs[0][0])\n        print(\" INPUT:   \", pairs[0][1])\n\n# 7) Try a test merge of first matched pair to confirm necessary columns exist\nif pairs:\n    tpath, ipath = pairs[0]\n    print(\"\\nAttempting a test merge of the first pair (reading small number of rows)...\")\n    try:\n        tdf = pd.read_csv(tpath, nrows=200)\n        idf = pd.read_csv(ipath, nrows=200)\n        # lower-case columns map\n        print(\"Tracking columns:\", tdf.columns.tolist())\n        print(\"Input columns:   \", idf.columns.tolist())\n        # try merge\n        if 'game_id' in tdf.columns and 'play_id' in tdf.columns:\n            merged = tdf.merge(idf, on=['game_id','play_id'], how='left', suffixes=('_trk','_in'))\n            print(\"\\nMerged columns sample:\", merged.columns.tolist()[:40])\n            # show presence of key columns\n            key_present = { 'player_role': 'player_role' in merged.columns or 'player_role' in [c.lower() for c in merged.columns],\n                           'ball_land_x': any('ball_land_x'==c.lower() for c in merged.columns) or any('ball_land' in c.lower() for c in merged.columns)}\n            print(\"Key columns present after merge:\", key_present)\n            display(merged.head(6))\n        else:\n            print(\"TRACKING file lacks 'game_id'/'play_id' columns in sample; cannot merge reliably.\")\n    except Exception as e:\n        print(\"Test merge failed:\", e)\nelse:\n    print(\"No candidate pair found. Please paste a few CSV names from /kaggle/input that look like tracking files (or attach a screenshot).\")\n\n# Final instructions printed:\nprint(\"\\n--- NEXT STEP ---\")\nif pairs:\n    print(\"If the test merge above included player_role and ball_land columns, we can proceed to re-run the full pipeline pairing each matching tracking file with its input file.\")\n    print(\"Say 'yes' and I will produce the exact full-run code that loops through all pairs, merges them and recomputes metrics.\")\nelse:\n    print(\"No suitable tracking/input pairs found automatically. If you see tracking files in the 'Example CSVs' list above, tell me one full path for a tracking file and one full path for its corresponding input file and I'll write the merge + metric rerun cell for you.\")\n\n\n# ====== Full re-run over all matched tracking+input files ======\nimport os, re, gc\nfrom pathlib import Path\nimport pandas as pd, numpy as np\nfrom joblib import Parallel, delayed\nimport multiprocessing\nfrom tqdm.auto import tqdm\n\nROOT = Path(\"/kaggle/input\")\nOUT_PLAY = \"play_metrics_full.csv\"\nOUT_LEADER = \"defender_leaderboard_full.csv\"\nMIN_PLAYS = 5\nN_JOBS = max(1, multiprocessing.cpu_count() - 1)\n\n# ------- helper: find CSVs and classify -------\nall_csvs = list(ROOT.rglob(\"*.csv\"))\ndef peek_cols(p):\n    try:\n        return pd.read_csv(p, nrows=2).columns.tolist()\n    except Exception:\n        return []\n\ndef is_tracking(p):\n    cols = [c.lower() for c in peek_cols(p)]\n    return ('frame_id' in cols and 'x' in cols and 'y' in cols)\n\ndef is_input(p):\n    cols = [c.lower() for c in peek_cols(p)]\n    return ('player_role' in cols or 'ball_land_x' in cols or 'player_to_predict' in cols or 'player_name' in cols)\n\n# build lists\ntracking_files = [str(p) for p in all_csvs if is_tracking(p)]\ninput_files    = [str(p) for p in all_csvs if is_input(p)]\n\nprint(\"Found tracking files:\", len(tracking_files), \"input files:\", len(input_files))\nif len(tracking_files)==0 or len(input_files)==0:\n    raise RuntimeError(\"Could not locate tracking or input files automatically. Inspect /kaggle/input and rerun.\")\n\n# ------- pair by week identifier when possible -------\ndef extract_week(path):\n    s = str(path)\n    m = re.search(r'(?:_w|_week|w)(\\d{1,2})', s, flags=re.IGNORECASE)\n    if m:\n        return int(m.group(1))\n    m2 = re.search(r'2023(\\d{2})', s)   # sometimes date-like chunk\n    if m2:\n        # fallback: return two-digit chunk\n        try:\n            return int(m2.group(1))\n        except:\n            return None\n    return None\n\ntrack_by_week = {}\nfor p in tracking_files:\n    w = extract_week(p)\n    track_by_week.setdefault(w, []).append(p)\ninput_by_week = {}\nfor p in input_files:\n    w = extract_week(p)\n    input_by_week.setdefault(w, []).append(p)\n\npairs = []\n# pair same-week first\nfor w in sorted(set(list(track_by_week.keys()) + list(input_by_week.keys()))):\n    if w in track_by_week and w in input_by_week:\n        pairs.append((track_by_week[w][0], input_by_week[w][0]))\n# fallback: zip-by-order if not enough pairs\nif not pairs:\n    m = min(len(tracking_files), len(input_files))\n    pairs = list(zip(tracking_files[:m], input_files[:m]))\n\nprint(\"Prepared\", len(pairs), \"tracking+input pairs to process (sample):\")\nfor a,b in pairs[:6]:\n    print(\"  \", Path(a).name, \"<-->\", Path(b).name)\n\n# ------- robust metric function (same logic, returns dict) -------\ndef calculate_play_metrics_with_route_mirroring(play_df):\n    out = {\n        'game_id': np.nan, 'play_id': np.nan,\n        'primary_defender_id': np.nan, 'receiver_id': np.nan,\n        'coverage_tightness': np.nan, 'ball_hawk_score': np.nan, 'route_mirroring': np.nan\n    }\n    try:\n        pdf = play_df.copy()\n        # ensure necessary columns\n        if 'player_role' not in pdf.columns or not {'x','y','frame_id'}.issubset(set(pdf.columns)):\n            return out\n        pdf['role_lower'] = pdf['player_role'].astype(str).str.lower()\n        recv = pdf[pdf['role_lower'].str.contains('target|targeted|receiver', na=False)]\n        defs = pdf[pdf['role_lower'].str.contains('defens|coverage|defend|db|cb|safety|lb|olb|mlb', na=False)]\n        if recv.empty or defs.empty:\n            return out\n        start_frame = int(pdf['frame_id'].min())\n        rec_start = recv[recv['frame_id']==start_frame] if not recv[recv['frame_id']==start_frame].empty else recv.groupby('nfl_id', dropna=True).first().reset_index()\n        def_start = defs[defs['frame_id']==start_frame] if not defs[defs['frame_id']==start_frame].empty else defs.groupby('nfl_id', dropna=True).first().reset_index()\n        if rec_start.empty or def_start.empty:\n            return out\n        # ball landing coords\n        if 'ball_land_x' not in pdf.columns or pdf['ball_land_x'].dropna().empty:\n            return out\n        ball_x = float(pdf['ball_land_x'].dropna().iloc[0])\n        ball_y = float(pdf['ball_land_y'].dropna().iloc[0])\n        rec_row = rec_start.iloc[0]\n        rec_x, rec_y = rec_row.get('x',np.nan), rec_row.get('y',np.nan)\n        out['receiver_id'] = rec_row.get('nfl_id', np.nan)\n        # choose primary defender closest at start\n        def_start = def_start.copy()\n        def_start['dist_to_rec'] = np.sqrt((def_start.get('x',0)-rec_x)**2 + (def_start.get('y',0)-rec_y)**2)\n        if def_start['dist_to_rec'].isnull().all():\n            return out\n        primary_idx = def_start['dist_to_rec'].idxmin()\n        primary_defender_id = def_start.loc[primary_idx,'nfl_id']\n        out['primary_defender_id'] = primary_defender_id\n        # build time series\n        recv_path = recv[['frame_id','x','y']].rename(columns={'x':'rec_x','y':'rec_y'})\n        defender_path = defs[defs['nfl_id']==primary_defender_id][['frame_id','x','y']].rename(columns={'x':'def_x','y':'def_y'})\n        merged = pd.merge(recv_path, defender_path, on='frame_id', how='inner').sort_values('frame_id')\n        if merged.empty:\n            return out\n        merged['separation'] = np.sqrt((merged['rec_x']-merged['def_x'])**2 + (merged['rec_y']-merged['def_y'])**2)\n        out['coverage_tightness'] = float(merged['separation'].mean())\n        # ball-hawk (need def velocity); estimate defender velocity if not provided\n        if {'def_vx','def_vy'}.issubset(merged.columns):\n            vx = merged['def_vx']; vy = merged['def_vy']\n        else:\n            # approximate via diff of def_x/def_y\n            merged['def_vx'] = merged['def_x'].diff().fillna(0)\n            merged['def_vy'] = merged['def_y'].diff().fillna(0)\n            vx = merged['def_vx']; vy = merged['def_vy']\n        merged['ball_vec_x'] = ball_x - merged['def_x']; merged['ball_vec_y'] = ball_y - merged['def_y']\n        normb = np.sqrt(merged['ball_vec_x']**2 + merged['ball_vec_y']**2).replace(0,1.0)\n        merged['ball_dir_x'] = merged['ball_vec_x'] / normb; merged['ball_dir_y'] = merged['ball_vec_y'] / normb\n        closing = vx * merged['ball_dir_x'] + vy * merged['ball_dir_y']\n        out['ball_hawk_score'] = float(closing.mean())\n        # route mirroring: use receiver diff for direction & defender diff\n        merged['r_vx'] = merged['rec_x'].diff().fillna(0); merged['r_vy'] = merged['rec_y'].diff().fillna(0)\n        merged['d_vx'] = vx; merged['d_vy'] = vy\n        merged['r_speed'] = np.sqrt(merged['r_vx']**2 + merged['r_vy']**2)\n        merged['d_speed'] = np.sqrt(merged['d_vx']**2 + merged['d_vy']**2)\n        valid = (merged['r_speed'] > 1e-6) & (merged['d_speed'] > 1e-6)\n        if valid.any():\n            merged.loc[valid, 'r_dir_x'] = merged.loc[valid,'r_vx']/merged.loc[valid,'r_speed']\n            merged.loc[valid, 'r_dir_y'] = merged.loc[valid,'r_vy']/merged.loc[valid,'r_speed']\n            merged.loc[valid, 'd_dir_x'] = merged.loc[valid,'d_vx']/merged.loc[valid,'d_speed']\n            merged.loc[valid, 'd_dir_y'] = merged.loc[valid,'d_vy']/merged.loc[valid,'d_speed']\n            merged.loc[valid, 'dir_score'] = (merged.loc[valid,'r_dir_x']*merged.loc[valid,'d_dir_x'] + merged.loc[valid,'r_dir_y']*merged.loc[valid,'d_dir_y']).clip(-1,1)\n            merged.loc[valid, 'speed_ratio'] = np.minimum(merged.loc[valid,'r_speed'], merged.loc[valid,'d_speed']) / np.maximum(merged.loc[valid,'r_speed'], merged.loc[valid,'d_speed'])\n            merged.loc[valid, 'mirror_frame'] = merged.loc[valid,'dir_score'] * merged.loc[valid,'speed_ratio']\n            out['route_mirroring'] = float(merged.loc[valid,'mirror_frame'].mean())\n        else:\n            out['route_mirroring'] = np.nan\n        out['game_id'] = int(pdf['game_id'].iloc[0]) if 'game_id' in pdf.columns else np.nan\n        out['play_id'] = int(pdf['play_id'].iloc[0]) if 'play_id' in pdf.columns else np.nan\n        return out\n    except Exception:\n        return out\n\n# ------- core loop: for each matched pair, merge & process plays -------\nall_play_results = []\nfor tpath, ipath in tqdm(pairs, desc=\"Pairs\"):\n    # load sensible columns to reduce memory load\n    tcols = peek_cols(tpath)\n    icol = peek_cols(ipath)\n    use_tcols = [c for c in ['game_id','play_id','nfl_id','frame_id','x','y','s','dir'] if c in tcols]\n    use_icols = [c for c in ['game_id','play_id','nfl_id','player_role','ball_land_x','ball_land_y','player_to_predict','player_position','player_name'] if c in icol]\n    try:\n        trk = pd.read_csv(tpath, usecols=use_tcols, low_memory=False)\n        inp = pd.read_csv(ipath, usecols=use_icols, low_memory=False)\n    except Exception:\n        trk = pd.read_csv(tpath, low_memory=False)\n        inp = pd.read_csv(ipath, low_memory=False)\n    # ensure key dtypes\n    for c in ['game_id','play_id','nfl_id','frame_id']:\n        if c in trk.columns:\n            trk[c] = pd.to_numeric(trk[c], errors='coerce')\n        if c in inp.columns:\n            inp[c] = pd.to_numeric(inp[c], errors='coerce')\n    # merge on game/play/nfl_id\n    merged = trk.merge(inp, on=['game_id','play_id','nfl_id'], how='left', suffixes=('_trk','_in'))\n    # Standardize role: create player_role if missing\n    if 'player_role' not in merged.columns:\n        if 'player_to_predict' in merged.columns:\n            # mark targeted receiver where True\n            merged['player_role'] = merged['player_to_predict'].astype(bool).map({True:'Targeted Receiver', False:'Other Route Runner'})\n        else:\n            # use position heuristics: defensive positions -> Defensive Coverage\n            def is_def_pos(p):\n                if pd.isna(p): return False\n                s = str(p).upper()\n                for tok in ['CB','DB','S','FS','SS','LB','MLB','OLB','SAFETY']:\n                    if tok in s: return True\n                return False\n            merged['player_role'] = merged.get('player_position', '').apply(lambda v: 'Defensive Coverage' if is_def_pos(v) else 'Other Route Runner')\n    # ensure ball_land_x/y exist (if input uses different names, try variants)\n    if 'ball_land_x' not in merged.columns and 'ball_land_x_in' in merged.columns:\n        merged['ball_land_x'] = merged['ball_land_x_in']\n    if 'ball_land_y' not in merged.columns and 'ball_land_y_in' in merged.columns:\n        merged['ball_land_y'] = merged['ball_land_y_in']\n    # compute simple velocities if not present\n    if not {'vx','vy'}.issubset(set(merged.columns)):\n        merged = merged.sort_values(['game_id','play_id','nfl_id','frame_id']).reset_index(drop=True)\n        grp = merged.groupby(['game_id','play_id','nfl_id'], sort=False)\n        merged[['dx','dy','dframe']] = grp[['x','y','frame_id']].diff().fillna(0)\n        merged['dframe'] = merged['dframe'].replace(0,1.0)\n        merged['vx'] = merged['dx'] / merged['dframe']\n        merged['vy'] = merged['dy'] / merged['dframe']\n        # drop temp\n        merged.drop(columns=[c for c in ['dx','dy','dframe'] if c in merged.columns], inplace=True)\n    # now iterate plays in this merged chunk\n    play_keys = merged[['game_id','play_id']].drop_duplicates().dropna()\n    play_list = list(play_keys.itertuples(index=False, name=None))\n    # process each play (serially or in small parallel batch to keep memory stable)\n    # here use local parallelization per pair for speed\n    def process_one_play(k):\n        gid,pid = k\n        pdf = merged[(merged['game_id']==gid) & (merged['play_id']==pid)]\n        return calculate_play_metrics_with_route_mirroring(pdf)\n    results = Parallel(n_jobs=max(1, min(N_JOBS, 4)), backend=\"loky\")(delayed(process_one_play)(k) for k in play_list)\n    all_play_results.extend(results)\n    # memory cleanup\n    del merged, trk, inp, play_keys, play_list, results\n    gc.collect()\n\n# ------- collect results and save -------\nplay_df = pd.DataFrame(all_play_results)\nplay_df.to_csv(OUT_PLAY, index=False)\nprint(\"Saved per-play metrics:\", OUT_PLAY, \"rows:\", len(play_df))\n\n# ------- defender-level aggregation -------\nif 'primary_defender_id' in play_df.columns:\n    play_df['primary_defender_id'] = pd.to_numeric(play_df['primary_defender_id'], errors='coerce').astype('Int64')\n    valid = play_df.dropna(subset=['primary_defender_id']).copy()\n    print(\"Valid plays with defender id:\", len(valid))\n    agg = valid.groupby('primary_defender_id').agg(\n        plays_count=('coverage_tightness','count'),\n        avg_coverage=('coverage_tightness','mean'),\n        avg_ballhawk=('ball_hawk_score','mean'),\n        avg_mirroring=('route_mirroring','mean')\n    ).reset_index()\n    agg = agg[agg['plays_count'] >= MIN_PLAYS].copy()\n    # normalize + ODS\n    def minmax(s):\n        if s.isna().all(): return s\n        mn,mx = s.min(), s.max()\n        if mn==mx: return pd.Series(0.5,index=s.index)\n        return (s-mn)/(mx-mn)\n    agg['coverage_inv'] = 1.0 - minmax(agg['avg_coverage'])\n    agg['ballhawk_norm'] = minmax(agg['avg_ballhawk'])\n    agg['mirror_norm'] = minmax(agg['avg_mirroring'])\n    agg['ODS'] = agg['coverage_inv']*0.40 + agg['ballhawk_norm']*0.25 + agg['mirror_norm']*0.35\n    agg = agg.sort_values('ODS', ascending=False).reset_index(drop=True)\n    agg.to_csv(OUT_LEADER, index=False)\n    print(\"Saved defender leaderboard:\", OUT_LEADER, \"rows:\", len(agg))\nelse:\n    print(\"primary_defender_id not in play_df columns; leaderboard cannot be built.\")\n\n# final summary\nprint(\"Done. play rows:\", len(play_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T01:01:02.283088Z","iopub.execute_input":"2025-10-01T01:01:02.283446Z","iopub.status.idle":"2025-10-01T01:31:23.362141Z","shell.execute_reply.started":"2025-10-01T01:01:02.28342Z","shell.execute_reply":"2025-10-01T01:31:23.360416Z"}},"outputs":[],"execution_count":null}]}