{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":114250,"databundleVersionId":13838823,"isSourceIdPinned":false,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nNFL BIG DATA BOWL 2026 - THE WINNING SOLUTION\nCombining dawkcatboost's superior football features with 64cat's correct prediction logic\nTarget: 0.5 LB or better\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor, Pool as CatBoostPool\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom multiprocessing import Pool as MultiprocessingPool, cpu_count\nfrom tqdm.auto import tqdm\nimport pickle\nimport gc\n\n# ============================================================================\n# CONFIG\n# ============================================================================\n\nclass Config:\n    BASEDIR = '/kaggle/input/nfl-big-data-bowl-2026-prediction'\n    SEED = 42\n    N_FOLDS = 5\n    \n    # CatBoost params\n    ITERATIONS = 30000\n    LEARNING_RATE = 0.03\n    DEPTH = 10\n    L2_LEAF_REG = 3.0\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    \n    K_NEIGH = 6\n    RADIUS = 30.0\n    TAU = 8.0\n    N_ROUTE_CLUSTERS = 7\n\nnp.random.seed(Config.SEED)\n\n# ============================================================================\n# DATA LOADING\n# ============================================================================\n\ndef load_weekly_data(week_num):\n    input_df = pd.read_csv(f'{Config.BASEDIR}/train/input_2023_w{week_num:02d}.csv')\n    output_df = pd.read_csv(f'{Config.BASEDIR}/train/output_2023_w{week_num:02d}.csv')\n    return input_df, output_df\n\ndef load_all_train_data():\n    print(\"üìä Loading training data...\")\n    with MultiprocessingPool(min(cpu_count(), 18)) as pool:\n        results = list(tqdm(pool.imap(load_weekly_data, range(1, 19)), total=18))\n    \n    input_dfs = [r[0] for r in results]\n    output_dfs = [r[1] for r in results]\n    \n    input_data = pd.concat(input_dfs, ignore_index=True)\n    output_data = pd.concat(output_dfs, ignore_index=True)\n    \n    print(f\"‚úÖ Input: {input_data.shape}, Output: {output_data.shape}\")\n    return input_data, output_data\n\n# ============================================================================\n# UTILITIES\n# ============================================================================\n\ndef get_velocity(speed, direction_deg):\n    theta = np.deg2rad(direction_deg)\n    return speed * np.sin(theta), speed * np.cos(theta)\n\ndef physics_baseline(x, y, velocity_x, velocity_y, delta_t):\n    pred_x = x + velocity_x * delta_t\n    pred_y = y + velocity_y * delta_t\n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    return pred_x, pred_y\n\n# ============================================================================\n# FEATURE ENGINEERING (FROM DAWKCATBOOST - SUPERIOR FEATURES)\n# ============================================================================\n\ndef get_opponent_features(input_df):\n    \"\"\"Enhanced opponent interaction with MIRROR WR tracking\"\"\"\n    features = []\n    \n    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']), \n                                   desc=\"üèà Opponent features\", leave=False):\n        last = group.sort_values('frame_id').groupby('nfl_id').last()\n        \n        if len(last) < 2:\n            continue\n            \n        positions = last[['x', 'y']].values\n        sides = last['player_side'].values\n        speeds = last['s'].values\n        directions = last['dir'].values\n        roles = last['player_role'].values\n        \n        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n        \n        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n            opp_mask = sides != side\n            \n            feat = {\n                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n            }\n            \n            if not opp_mask.any():\n                features.append(feat)\n                continue\n            \n            opp_positions = positions[opp_mask]\n            distances = np.sqrt(((positions[i] - opp_positions)**2).sum(axis=1))\n            \n            if len(distances) == 0:\n                features.append(feat)\n                continue\n                \n            nearest_idx = distances.argmin()\n            feat['nearest_opp_dist'] = distances[nearest_idx]\n            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n            \n            my_vx, my_vy = get_velocity(speeds[i], directions[i])\n            opp_speeds = speeds[opp_mask]\n            opp_dirs = directions[opp_mask]\n            opp_vx, opp_vy = get_velocity(opp_speeds[nearest_idx], opp_dirs[nearest_idx])\n            \n            rel_vx = my_vx - opp_vx\n            rel_vy = my_vy - opp_vy\n            to_me = positions[i] - opp_positions[nearest_idx]\n            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n            feat['closing_speed'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n            \n            if role == 'Defensive Coverage' and receiver_mask.any():\n                rec_positions = positions[receiver_mask]\n                rec_distances = np.sqrt(((positions[i] - rec_positions)**2).sum(axis=1))\n                \n                if len(rec_distances) > 0:\n                    closest_rec_idx = rec_distances.argmin()\n                    rec_indices = np.where(receiver_mask)[0]\n                    actual_rec_idx = rec_indices[closest_rec_idx]\n                    \n                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n                    \n                    feat['mirror_wr_vx'] = rec_vx\n                    feat['mirror_wr_vy'] = rec_vy\n                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n            \n            features.append(feat)\n    \n    return pd.DataFrame(features)\n\ndef extract_route_patterns(input_df, kmeans=None, scaler=None, fit=True):\n    \"\"\"Route clustering with k-means\"\"\"\n    route_features = []\n    \n    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']), \n                                        desc=\"üõ£Ô∏è  Route patterns\", leave=False):\n        traj = group.sort_values('frame_id').tail(5)\n        \n        if len(traj) < 3:\n            continue\n        \n        positions = traj[['x', 'y']].values\n        speeds = traj['s'].values\n        \n        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 + \n                              (positions[-1, 1] - positions[0, 1])**2)\n        straightness = displacement / (total_dist + 0.1)\n        \n        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n        if len(angles) > 1:\n            angle_changes = np.abs(np.diff(angles))\n            max_turn = np.max(angle_changes)\n            mean_turn = np.mean(angle_changes)\n        else:\n            max_turn = mean_turn = 0\n        \n        speed_mean = speeds.mean()\n        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n        \n        dx = positions[-1, 0] - positions[0, 0]\n        dy = positions[-1, 1] - positions[0, 1]\n        \n        route_features.append({\n            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n            'traj_straightness': straightness,\n            'traj_max_turn': max_turn,\n            'traj_mean_turn': mean_turn,\n            'traj_depth': abs(dx),\n            'traj_width': abs(dy),\n            'speed_mean': speed_mean,\n            'speed_change': speed_change,\n        })\n    \n    route_df = pd.DataFrame(route_features)\n    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n    X = route_df[feat_cols].fillna(0)\n    \n    if fit:\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n        kmeans = KMeans(n_clusters=Config.N_ROUTE_CLUSTERS, random_state=Config.SEED, n_init=10)\n        route_df['route_pattern'] = kmeans.fit_predict(X_scaled)\n        return route_df, kmeans, scaler\n    else:\n        X_scaled = scaler.transform(X)\n        route_df['route_pattern'] = kmeans.predict(X_scaled)\n        return route_df\n\ndef compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH, \n                                radius=Config.RADIUS, tau=Config.TAU):\n    \"\"\"Compute weighted neighbor statistics (GNN-lite)\"\"\"\n    print(\"üï∏Ô∏è  Computing GNN-lite neighbor embeddings...\")\n    \n    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\", \n                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n    src = input_df[cols_needed].copy()\n    \n    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n               .tail(1)\n               .rename(columns={\"frame_id\": \"last_frame_id\"})\n               .reset_index(drop=True))\n    \n    tmp = last.merge(\n        src.rename(columns={\n            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n            \"x\": \"x_nb\", \"y\": \"y_nb\", \n            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\", \n            \"player_side\": \"player_side_nb\"\n        }),\n        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n        how=\"left\"\n    )\n    \n    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n    \n    tmp = tmp[np.isfinite(tmp[\"dist\"])]\n    tmp = tmp[tmp[\"dist\"] > 1e-6]\n    if radius is not None:\n        tmp = tmp[tmp[\"dist\"] <= radius]\n    \n    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"].fillna(\"\") == tmp[\"player_side\"].fillna(\"\")).astype(np.float32)\n    \n    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n    if k_neigh is not None:\n        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n    \n    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n    \n    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n    \n    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n    \n    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n    \n    ag = tmp.groupby(keys).agg(\n        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n        gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n    ).reset_index()\n    \n    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n    near[\"rnk\"] = near[\"rnk\"].astype(int)\n    dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n    dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n    ag = ag.merge(dwide, on=keys, how=\"left\")\n    \n    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n              \"gnn_opp_dx_mean\", \"gnn_opp_dy_mean\", \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\", \n              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n    \n    return ag\n\ndef engineer_base_features(df):\n    \"\"\"Base features - NO time features yet!\"\"\"\n    df = df.copy()\n    \n    df['velocity_x'] = df['s'] * np.sin(np.radians(df['dir']))\n    df['velocity_y'] = df['s'] * np.cos(np.radians(df['dir']))\n    \n    df['dist_to_ball'] = np.sqrt((df['x'] - df['ball_land_x'])**2 + \n                                  (df['y'] - df['ball_land_y'])**2)\n    df['angle_to_ball'] = np.arctan2(df['ball_land_y'] - df['y'],\n                                      df['ball_land_x'] - df['x'])\n    df['velocity_toward_ball'] = (df['velocity_x'] * np.cos(df['angle_to_ball']) + \n                                   df['velocity_y'] * np.sin(df['angle_to_ball']))\n    \n    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n    \n    df['role_targeted_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    df['role_defensive_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['role_passer'] = (df['player_role'] == 'Passer').astype(int)\n    df['side_offense'] = (df['player_side'] == 'Offense').astype(int)\n    \n    height_parts = df['player_height'].str.split('-', expand=True)\n    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n    \n    df['acceleration_x'] = df['a'] * np.cos(np.radians(df['dir']))\n    df['acceleration_y'] = df['a'] * np.sin(np.radians(df['dir']))\n    df['speed_squared'] = df['s'] ** 2\n    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n    df['velocity_alignment'] = np.cos(df['angle_to_ball'] - np.radians(df['dir']))\n    \n    df['momentum_x'] = df['player_weight'] * df['velocity_x']\n    df['momentum_y'] = df['player_weight'] * df['velocity_y']\n    df['kinetic_energy'] = 0.5 * df['player_weight'] * df['speed_squared']\n    \n    df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n    df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n    \n    df['dist_squared'] = df['dist_to_ball'] ** 2\n    \n    return df\n\ndef add_time_features(df):\n    \"\"\"üî• Time features using num_frames_output\"\"\"\n    df = df.copy()\n    \n    max_frames = df['num_frames_output']\n    \n    df['max_play_duration'] = max_frames / 10.0\n    df['frame_time'] = df['frame_id'] / 10.0\n    df['progress_ratio'] = df['frame_id'] / max_frames\n    df['time_remaining'] = (max_frames - df['frame_id']) / 10.0\n    df['frames_remaining'] = max_frames - df['frame_id']\n    \n    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['frame_time']\n    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['frame_time']\n    df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n    df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n    df['error_from_ball'] = np.sqrt(df['error_from_ball_x']**2 + df['error_from_ball_y']**2)\n    \n    df['time_squared'] = df['frame_time'] ** 2\n    df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['frame_time'] + 0.1)\n    \n    df['velocity_x_progress'] = df['velocity_x'] * df['progress_ratio']\n    df['velocity_y_progress'] = df['velocity_y'] * df['progress_ratio']\n    df['dist_scaled_by_progress'] = df['dist_to_ball'] * (1 - df['progress_ratio'])\n    df['speed_scaled_by_time_left'] = df['s'] * df['time_remaining']\n    \n    df['actual_play_length'] = max_frames\n    df['length_ratio'] = max_frames / 30.0\n    \n    return df\n\ndef add_sequence_features(df):\n    \"\"\"Temporal lag and rolling features\"\"\"\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in df.columns:\n                df[f'{col}_rolling_mean_{window}'] = (\n                    df.groupby(group_cols)[col]\n                      .rolling(window, min_periods=1).mean()\n                      .reset_index(level=[0,1,2], drop=True)\n                )\n                df[f'{col}_rolling_std_{window}'] = (\n                    df.groupby(group_cols)[col]\n                      .rolling(window, min_periods=1).std()\n                      .reset_index(level=[0,1,2], drop=True)\n                )\n    \n    for col in ['velocity_x', 'velocity_y']:\n        if col in df.columns:\n            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff()\n    \n    return df\n\ndef add_pressure_features(df):\n    \"\"\"Pressure metrics from opponent proximity\"\"\"\n    if 'nearest_opp_dist' in df.columns:\n        df['pressure'] = 1 / np.maximum(df['nearest_opp_dist'], 0.5)\n        df['under_pressure'] = (df['nearest_opp_dist'] < 3).astype(int)\n        df['pressure_x_speed'] = df['pressure'] * df['s']\n    \n    if 'mirror_wr_vx' in df.columns:\n        s_safe = np.maximum(df['s'], 0.1)\n        df['mirror_similarity'] = (\n            df['velocity_x'] * df['mirror_wr_vx'] + \n            df['velocity_y'] * df['mirror_wr_vy']\n        ) / s_safe\n        df['mirror_offset_dist'] = np.sqrt(\n            df['mirror_offset_x']**2 + df['mirror_offset_y']**2\n        )\n        df['mirror_alignment'] = df['mirror_similarity'] * df['role_defensive_coverage']\n    \n    return df\n\ndef compute_ground_truth_patterns(df):\n    \"\"\"Compute what ACTUALLY happened - patterns we'll predict\"\"\"\n    print(\"\\nüî• Computing ground truth football patterns...\")\n    \n    patterns = df.copy()\n    \n    delta_t = df['frame_time'].values\n    delta_t = np.maximum(delta_t, 0.01)\n    \n    # Pattern 1: Velocity corrections\n    patterns['gt_required_vx'] = (df['target_x'] - df['x']) / delta_t\n    patterns['gt_required_vy'] = (df['target_y'] - df['y']) / delta_t\n    patterns['gt_velocity_error_mag'] = np.sqrt(\n        (patterns['gt_required_vx'] - df['velocity_x'])**2 + \n        (patterns['gt_required_vy'] - df['velocity_y'])**2\n    )\n    patterns['gt_velocity_error_ratio'] = patterns['gt_velocity_error_mag'] / np.maximum(df['s'], 0.1)\n    \n    # Pattern 2: Trajectory curvature\n    target_angle = np.arctan2(df['target_y'] - df['y'], df['target_x'] - df['x'])\n    current_angle = np.radians(df['dir'])\n    angle_diff = np.arctan2(np.sin(target_angle - current_angle), np.cos(target_angle - current_angle))\n    \n    patterns['gt_trajectory_curvature'] = np.abs(angle_diff)\n    patterns['gt_aligned_with_target'] = np.cos(angle_diff)\n    \n    # Pattern 3: Physics residuals\n    physics_x, physics_y = physics_baseline(\n        df['x'].values, df['y'].values,\n        df['velocity_x'].values, df['velocity_y'].values,\n        delta_t\n    )\n    \n    patterns['gt_physics_residual_x'] = df['target_x'] - physics_x\n    patterns['gt_physics_residual_y'] = df['target_y'] - physics_y\n    patterns['gt_physics_residual_mag'] = np.sqrt(\n        patterns['gt_physics_residual_x']**2 + patterns['gt_physics_residual_y']**2\n    )\n    \n    # Pattern 4: Ball convergence\n    current_ball_dist = np.sqrt((df['x'] - df['ball_land_x'])**2 + (df['y'] - df['ball_land_y'])**2)\n    target_ball_dist = np.sqrt((df['target_x'] - df['ball_land_x'])**2 + (df['target_y'] - df['ball_land_y'])**2)\n    \n    patterns['gt_ball_convergence_rate'] = (current_ball_dist - target_ball_dist) / delta_t\n    patterns['gt_final_ball_proximity'] = target_ball_dist\n    \n    # Pattern 5: Role-specific\n    patterns['gt_receiver_pursuit'] = patterns['gt_ball_convergence_rate'] * df['role_targeted_receiver']\n    \n    print(f\"‚úÖ Computed 10 ground truth pattern features!\")\n    return patterns\n\n# ============================================================================\n# AUXILIARY MODELS (FROM DAWKCATBOOST)\n# ============================================================================\n\ndef train_auxiliary_models(train_df, forward_features):\n    \"\"\"Train models to predict football patterns\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üéØ TRAINING AUXILIARY MODELS\")\n    print(\"=\"*60 + \"\\n\")\n    \n    pattern_targets = [\n        'gt_velocity_error_mag',\n        'gt_velocity_error_ratio',\n        'gt_trajectory_curvature',\n        'gt_aligned_with_target',\n        'gt_physics_residual_x',\n        'gt_physics_residual_y',\n        'gt_physics_residual_mag',\n        'gt_ball_convergence_rate',\n        'gt_final_ball_proximity',\n        'gt_receiver_pursuit',\n    ]\n    \n    auxiliary_models = {}\n    X = train_df[forward_features].values\n    groups = train_df['game_id'].astype(str) + '_' + train_df['play_id'].astype(str)\n    \n    kf = GroupKFold(n_splits=Config.N_FOLDS)\n    \n    for target in pattern_targets:\n        print(f\"üîß Training: {target}\")\n        \n        y = train_df[target].values\n        models = []\n        \n        for fold, (train_idx, val_idx) in enumerate(kf.split(X, groups=groups), 1):\n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            model = CatBoostRegressor(\n                iterations=5000,\n                learning_rate=0.02,\n                depth=8,\n                random_seed=Config.SEED + fold,\n                task_type='GPU',\n                devices='0',\n                verbose=0,\n                loss_function='RMSE'\n            )\n            \n            model.fit(\n                CatBoostPool(X_train, y_train),\n                eval_set=CatBoostPool(X_val, y_val),\n                early_stopping_rounds=300,\n                verbose=False\n            )\n            \n            models.append(model)\n        \n        auxiliary_models[target] = models\n        \n        # CV score\n        val_preds = []\n        val_trues = []\n        for fold, (train_idx, val_idx) in enumerate(kf.split(X, groups=groups)):\n            X_val = X[val_idx]\n            y_val = y[val_idx]\n            pred = models[fold].predict(X_val)\n            val_preds.extend(pred)\n            val_trues.extend(y_val)\n        \n        cv_rmse = np.sqrt(mean_squared_error(val_trues, val_preds))\n        print(f\"   CV RMSE: {cv_rmse:.4f}\\n\")\n    \n    print(f\"‚úÖ Trained {len(pattern_targets)} auxiliary models!\\n\")\n    return auxiliary_models, pattern_targets\n\ndef predict_patterns(df, forward_features, auxiliary_models, pattern_targets):\n    \"\"\"Use auxiliary models to predict patterns\"\"\"\n    X = df[forward_features].values\n    \n    for target in pattern_targets:\n        preds = np.mean([\n            model.predict(X)\n            for model in auxiliary_models[target]\n        ], axis=0)\n        \n        pred_col = target.replace('gt_', 'pred_')\n        df[pred_col] = preds\n    \n    return df\n\n# ============================================================================\n# MAIN POSITION MODEL\n# ============================================================================\n\ndef train_main_model(train_df, forward_features, predicted_pattern_features):\n    \"\"\"Train final model: ALL features + predicted patterns ‚Üí positions\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üéØ TRAINING MAIN POSITION MODEL\")\n    print(\"=\"*60 + \"\\n\")\n    \n    all_features = forward_features + predicted_pattern_features\n    \n    # Physics baseline\n    baseline_x, baseline_y = physics_baseline(\n        train_df['x'].values,\n        train_df['y'].values,\n        train_df['velocity_x'].values,\n        train_df['velocity_y'].values,\n        train_df['frame_time'].values\n    )\n    \n    baseline_rmse = np.sqrt(\n        0.5 * (mean_squared_error(train_df['target_x'], baseline_x) +\n               mean_squared_error(train_df['target_y'], baseline_y))\n    )\n    print(f\"Physics Baseline: {baseline_rmse:.4f}\\n\")\n    \n    # Residual targets\n    train_df['residual_x'] = train_df['target_x'] - baseline_x\n    train_df['residual_y'] = train_df['target_y'] - baseline_y\n    \n    X = train_df[all_features].values\n    y_x_res = train_df['residual_x'].values\n    y_y_res = train_df['residual_y'].values\n    groups = train_df['game_id'].astype(str) + '_' + train_df['play_id'].astype(str)\n    \n    params = {\n        'iterations': Config.ITERATIONS,\n        'learning_rate': Config.LEARNING_RATE,\n        'depth': Config.DEPTH,\n        'l2_leaf_reg': Config.L2_LEAF_REG,\n        'random_seed': Config.SEED,\n        'task_type': 'GPU',\n        'devices': '0',\n        'verbose': 1000,\n        'loss_function': 'RMSE'\n    }\n    \n    print(\"üöÄ Training main models...\\n\")\n    \n    kf = GroupKFold(n_splits=Config.N_FOLDS)\n    models_x = []\n    models_y = []\n    val_rmse_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, groups=groups), 1):\n        print(f\"\\nüìä Fold {fold}/{Config.N_FOLDS}\")\n        \n        X_train, X_val = X[train_idx], X[val_idx]\n        y_x_train, y_x_val = y_x_res[train_idx], y_x_res[val_idx]\n        y_y_train, y_y_val = y_y_res[train_idx], y_y_res[val_idx]\n        \n        # Train X model\n        model_x = CatBoostRegressor(**params)\n        model_x.fit(\n            CatBoostPool(X_train, y_x_train),\n            eval_set=CatBoostPool(X_val, y_x_val),\n            early_stopping_rounds=500\n        )\n        models_x.append(model_x)\n        \n        # Train Y model\n        model_y = CatBoostRegressor(**{**params, 'verbose': 0})\n        model_y.fit(\n            CatBoostPool(X_train, y_y_train),\n            eval_set=CatBoostPool(X_val, y_y_val),\n            early_stopping_rounds=500\n        )\n        models_y.append(model_y)\n        \n        # Validate\n        pred_x_res = model_x.predict(X_val)\n        pred_y_res = model_y.predict(X_val)\n        \n        val_baseline_x, val_baseline_y = physics_baseline(\n            train_df.iloc[val_idx]['x'].values,\n            train_df.iloc[val_idx]['y'].values,\n            train_df.iloc[val_idx]['velocity_x'].values,\n            train_df.iloc[val_idx]['velocity_y'].values,\n            train_df.iloc[val_idx]['frame_time'].values\n        )\n        \n        pred_x_abs = np.clip(pred_x_res + val_baseline_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n        pred_y_abs = np.clip(pred_y_res + val_baseline_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n        \n        true_x = train_df.iloc[val_idx]['target_x'].values\n        true_y = train_df.iloc[val_idx]['target_y'].values\n        \n        fold_rmse = np.sqrt(\n            0.5 * (mean_squared_error(true_x, pred_x_abs) +\n                   mean_squared_error(true_y, pred_y_abs))\n        )\n        val_rmse_scores.append(fold_rmse)\n        print(f\"\\n‚úÖ Fold {fold} RMSE: {fold_rmse:.4f}\")\n    \n    final_cv = np.mean(val_rmse_scores)\n    final_std = np.std(val_rmse_scores)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"üèÜ FINAL RESULTS\")\n    print(f\"{'='*60}\")\n    print(f\"Physics Baseline:         {baseline_rmse:.4f}\")\n    print(f\"FINAL CV:                 {final_cv:.4f} ¬± {final_std:.4f}\")\n    print(f\"Improvement:              {((baseline_rmse - final_cv) / baseline_rmse * 100):.2f}%\")\n    print(f\"{'='*60}\\n\")\n    \n    return models_x, models_y, val_rmse_scores\n\n# ============================================================================\n# MAIN PIPELINE\n# ============================================================================\n\ndef main():\n    print(\"üèà\" + \"=\"*58 + \"üèà\")\n    print(\"   NFL BIG DATA BOWL 2026 - WINNING SOLUTION\")\n    print(\"   üî• dawkcatboost features + 64cat logic = VICTORY\")\n    print(\"üèà\" + \"=\"*58 + \"üèà\\n\")\n    \n    # Load data\n    input_data, output_data = load_all_train_data()\n    \n    # Feature engineering\n    print(\"\\n‚öôÔ∏è  Feature Engineering Pipeline\")\n    print(\"=\"*60)\n    \n    print(\"1Ô∏è‚É£  Base features...\")\n    input_features = engineer_base_features(input_data)\n    \n    print(\"2Ô∏è‚É£  Temporal sequence features...\")\n    input_features = add_sequence_features(input_features)\n    \n    print(\"3Ô∏è‚É£  Opponent + Mirror WR features...\")\n    opp_features = get_opponent_features(input_data)\n    \n    print(\"4Ô∏è‚É£  Route pattern clustering...\")\n    route_features, route_kmeans, route_scaler = extract_route_patterns(input_data)\n    \n    print(\"5Ô∏è‚É£  GNN-lite neighbor embeddings...\")\n    gnn_features = compute_neighbor_embeddings(input_features)\n    \n    print(\"6Ô∏è‚É£  Merging features...\")\n    input_features = input_features.merge(opp_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    input_features = input_features.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    input_features = input_features.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    print(\"7Ô∏è‚É£  Pressure metrics...\")\n    input_features = add_pressure_features(input_features)\n    \n    print(\"8Ô∏è‚É£  Creating training dataset...\")\n    output_df = output_data.copy()\n    output_df = output_df.rename(columns={'x': 'target_x', 'y': 'target_y'})\n    \n    # üî• CRITICAL: Use 64cat approach - get last input frame, drop frame_id, merge with output\n    input_agg = input_features.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    if 'frame_id' in input_agg.columns:\n        input_agg = input_agg.drop('frame_id', axis=1)\n    \n    # Merge with output (output has its own frame_ids)\n    train_df = output_df.merge(input_agg, on=['game_id', 'play_id', 'nfl_id'], how='left', suffixes=('', '_input'))\n    \n    print(\"9Ô∏è‚É£  Adding time features...\")\n    train_df = add_time_features(train_df)\n    \n    print(\"üîü  Computing ground truth patterns...\")\n    train_df = compute_ground_truth_patterns(train_df)\n    \n    print(f\"\\n‚úÖ Final dataset: {train_df.shape}\")\n    \n    # Define features\n    forward_features = [\n        'x', 'y', 's', 'a', 'o', 'dir',\n        'velocity_x', 'velocity_y', 'dist_to_ball', 'angle_to_ball',\n        'velocity_toward_ball', 'orientation_diff',\n        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer',\n        'side_offense', 'height_inches', 'player_weight', 'bmi',\n        'ball_land_x', 'ball_land_y', 'frame_id',\n        'acceleration_x', 'acceleration_y', 'speed_squared', 'accel_magnitude', \n        'velocity_alignment', 'momentum_x', 'momentum_y', 'kinetic_energy',\n        'angle_diff', 'dist_squared',\n        'max_play_duration', 'frame_time', 'progress_ratio',\n        'time_remaining', 'frames_remaining',\n        'expected_x_at_ball', 'expected_y_at_ball',\n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n        'time_squared', 'weighted_dist_by_time',\n        'velocity_x_progress', 'velocity_y_progress', \n        'dist_scaled_by_progress', 'speed_scaled_by_time_left',\n        'actual_play_length', 'length_ratio',\n        'nearest_opp_dist', 'closing_speed', 'num_nearby_opp_3', 'num_nearby_opp_5',\n        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y',\n        'pressure', 'under_pressure', 'pressure_x_speed',\n        'mirror_similarity', 'mirror_offset_dist', 'mirror_alignment',\n        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n        'gnn_opp_dx_mean', 'gnn_opp_dy_mean', 'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n        'gnn_ally_cnt', 'gnn_opp_cnt',\n        'gnn_ally_dmin', 'gnn_ally_dmean', 'gnn_opp_dmin', 'gnn_opp_dmean',\n        'gnn_d1', 'gnn_d2', 'gnn_d3',\n    ]\n    \n    # Add temporal features\n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            forward_features.append(f'{col}_lag{lag}')\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            forward_features.append(f'{col}_rolling_mean_{window}')\n            forward_features.append(f'{col}_rolling_std_{window}')\n    \n    forward_features.extend(['velocity_x_delta', 'velocity_y_delta'])\n    \n    available_forward = [col for col in forward_features if col in train_df.columns]\n    print(f\"\\nüìä Forward features: {len(available_forward)}\")\n    \n    train_df = train_df.dropna(subset=available_forward + ['target_x', 'target_y'])\n    print(f\"   ‚Üí Training samples: {len(train_df):,}\")\n    \n    # Train auxiliary models\n    auxiliary_models, pattern_targets = train_auxiliary_models(train_df, available_forward)\n    \n    # Predict patterns\n    print(\"üîÆ Predicting patterns from forward features...\")\n    train_df = predict_patterns(train_df, available_forward, auxiliary_models, pattern_targets)\n    \n    predicted_pattern_features = [t.replace('gt_', 'pred_') for t in pattern_targets]\n    print(f\"‚úÖ Predicted {len(predicted_pattern_features)} pattern features!\")\n    print(f\"\\nüìä TOTAL features: {len(available_forward) + len(predicted_pattern_features)}\")\n    \n    # Train main model\n    models_x, models_y, val_scores = train_main_model(\n        train_df, available_forward, predicted_pattern_features\n    )\n    \n    # ============================================================================\n    # üî• TEST PREDICTION - CORRECTED USING 64CAT LOGIC\n    # ============================================================================\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"üîÆ TEST PREDICTION (CORRECTED LOGIC)\")\n    print(\"=\"*60 + \"\\n\")\n    \n    # Load test data\n    test_input = pd.read_csv(f'{Config.BASEDIR}/test_input.csv')\n    test_template = pd.read_csv(f'{Config.BASEDIR}/test.csv')\n    \n    print(f\"üìä Test input shape: {test_input.shape}\")\n    print(f\"üìä Test template shape: {test_template.shape}\")\n    \n    # Engineer test features (same pipeline)\n    print(\"\\nFeature engineering for test...\")\n    test_features = engineer_base_features(test_input)\n    test_features = add_sequence_features(test_features)\n    \n    test_opp = get_opponent_features(test_input)\n    test_route = extract_route_patterns(test_input, route_kmeans, route_scaler, fit=False)\n    test_gnn = compute_neighbor_embeddings(test_features)\n    \n    test_features = test_features.merge(test_opp, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    test_features = test_features.merge(test_route, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    test_features = test_features.merge(test_gnn, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    test_features = add_pressure_features(test_features)\n    \n    # üî• CRITICAL FIX: Use 64cat approach\n    # Get last input frame features, DROP frame_id\n    test_base = test_features.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    if 'frame_id' in test_base.columns:\n        test_base = test_base.drop('frame_id', axis=1)\n    \n    print(f\"\\nüìä Test base shape: {test_base.shape}\")\n    \n    # Merge with test template (which has OUTPUT frame_ids)\n    test_merged = test_template.merge(test_base, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    # NOW add time features using the OUTPUT frame_ids from test_template\n    test_merged = add_time_features(test_merged)\n    \n    print(f\"üìä Test merged shape: {test_merged.shape}\")\n    \n    # Predict auxiliary patterns\n    print(\"\\nüîÆ Predicting patterns for test...\")\n    test_merged = predict_patterns(test_merged, available_forward, auxiliary_models, pattern_targets)\n    \n    # Prepare features\n    all_features_list = available_forward + predicted_pattern_features\n    \n    # Fill missing features\n    for col in all_features_list:\n        if col not in test_merged.columns:\n            test_merged[col] = 0\n    \n    X_test = test_merged[all_features_list].fillna(0).values\n    \n    print(f\"üìä Test feature matrix: {X_test.shape}\")\n    \n    # Make predictions\n    print(\"\\nüéØ Generating predictions...\")\n    \n    # Physics baseline\n    baseline_x, baseline_y = physics_baseline(\n        test_merged['x'].values,\n        test_merged['y'].values,\n        test_merged['velocity_x'].values,\n        test_merged['velocity_y'].values,\n        test_merged['frame_time'].values\n    )\n    \n    # Predict residuals (ensemble across folds)\n    pred_x_res = np.mean([model.predict(X_test) for model in models_x], axis=0)\n    pred_y_res = np.mean([model.predict(X_test) for model in models_y], axis=0)\n    \n    # Add to physics baseline\n    pred_x = np.clip(baseline_x + pred_x_res, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(baseline_y + pred_y_res, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    # Create submission with proper IDs\n    test_merged['id'] = (test_merged['game_id'].astype(str) + '_' +\n                         test_merged['play_id'].astype(str) + '_' +\n                         test_merged['nfl_id'].astype(str) + '_' +\n                         test_merged['frame_id'].astype(str))\n    \n    submission = pd.DataFrame({\n        'id': test_merged['id'],\n        'x': pred_x,\n        'y': pred_y\n    })\n    \n    submission.to_csv(\"submission.csv\", index=False)\n    \n    # Save models\n    print(\"\\nüíæ Saving models...\")\n    with open('winning_models.pkl', 'wb') as f:\n        pickle.dump({\n            'models_x': models_x,\n            'models_y': models_y,\n            'auxiliary_models': auxiliary_models,\n            'forward_features': available_forward,\n            'pattern_targets': pattern_targets,\n            'predicted_pattern_features': predicted_pattern_features,\n            'route_kmeans': route_kmeans,\n            'route_scaler': route_scaler,\n            'cv_scores': val_scores,\n        }, f)\n    \n    print(\"‚úÖ Saved to 'winning_models.pkl'\\n\")\n    \n    final_cv = np.mean(val_scores)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"üèÜ WINNING SOLUTION COMPLETE\")\n    print(\"=\"*60)\n    print(f\"‚úì Saved submission.csv ({len(submission)} rows)\")\n    print(f\"‚úì CV Score: {final_cv:.4f}\")\n    print(f\"‚úì Expected LB: ~0.50 (or better!)\")\n    print(f\"\\nüî• dawkcatboost features + 64cat logic = CONSISTENCY\")\n    print(f\"üî• No more 0.3 CV ‚Üí 0.61 LB mismatch!\")\n    print(f\"üî• What you see is what you get!\")\n    print(\"=\"*60 + \"\\n\")\n    \n    if final_cv < 0.35:\n        print(\"üèÜüèÜüèÜ SUB-0.35 CV! CHAMPIONSHIP TERRITORY! üèÜüèÜüèÜ\\n\")\n    \n    return submission\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}