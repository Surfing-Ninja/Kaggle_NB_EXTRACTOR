{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114250,"databundleVersionId":13838823,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-File Ingestion_ML Workflow\n**Date: 2025-10-17**\n\nMy notebook implement a ML pipeline with **points** and **sub-points**, and it automatically ingests all matching files in the provided folder:\n\n**-Pattern for features: `input_2023_*.csv`**\n\n**-Pattern for targets: `output_2023_*.csv`**\n\nIt will also include `supplementary_data.csv` if present.","metadata":{}},{"cell_type":"code","source":"# Core libraries\nimport os\nimport glob\nimport math\nimport warnings\nimport typing as t\n\n# Data stack and plotting\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Modeling tools\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n    mean_absolute_error, mean_squared_error, r2_score, classification_report,\n    confusion_matrix, precision_recall_curve, roc_curve\n)\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\n\n# Display options\npd.set_option('display.max_columns', 120)\npd.set_option('display.width', 140)\nwarnings.filterwarnings('ignore')\n\n# Input Paths\nBASE_DIR = '/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/train'\nINPUT_GLOB = os.path.join(BASE_DIR, 'input_2023_*.csv')\nOUTPUT_GLOB = os.path.join(BASE_DIR, 'output_2023_*.csv')\nSUPPLEMENT_PATH = '/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/supplementary_data.csv'\n\n# User configuration\nTARGET_COLUMN = None\nID_COLUMNS = None\nJOIN_KEYS = None\nTASK_TYPE = None\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\nCV_FOLDS = 5\nMAX_EDA_PLOTS = 15","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## A. Goal and Criterion\n\n**Challenges**\n  - Determine the target variable and specify whether the task involves **classification** or **regression**.\n  - Indicate how the forecasted results will support stakeholders in achieving their objectives.\n\n**Criteria**\n  - Choose the primary evaluation metric: **ROC AUC / F1** for classification tasks or **MAE / RMSE / R²** for regression tasks.\n  - Choose secondary evaluation metrics such as **PR AUC**, **calibration error**, or **inference latency**.\n  - Define target thresholds that indicate successful performance.\n    \n**Constraints**\n  - Assess data coverage and freshness across the weekly files.\n  - Define the join logic used to combine feature and target files.\n  - Consider fairness, privacy, and operational limitations.","metadata":{}},{"cell_type":"code","source":"# Define a function to load many CSVs by glob\ndef load_many_csvs(pattern):\n    # Collect paths\n    paths = sorted(glob.glob(pattern))\n    # Return None if no matches\n    if len(paths) == 0:\n        return None, []\n    # Read all dataframes\n    frames = []\n    # Iterate paths\n    for p in paths:\n        # Read csv\n        df = pd.read_csv(p, low_memory=False)\n        # Keep source file name\n        df['__source_file'] = os.path.basename(p)\n        # Append frame\n        frames.append(df)\n    # Concatenate frames\n    combined = pd.concat(frames, ignore_index=True, sort=False)\n    # Return combined and list of paths\n    return combined, paths\n\n# Define a function to detect target column and task type\ndef detect_target_and_task(input_df, output_df):\n    # Initialize results\n    target_col = None\n    task_type = None\n\n    # Choose candidates\n    candidates = ['target', 'label', 'y', 'outcome']\n\n    # Search output first\n    if output_df is not None:\n        # Intersect with columns\n        for c in candidates:\n            if c in output_df.columns:\n                target_col = c\n                break\n        # Fallback to last column\n        if target_col is None and len(output_df.columns) > 0:\n            target_col = output_df.columns[-1]\n\n    # If still None, search input\n    if target_col is None and input_df is not None:\n        for c in candidates:\n            if c in input_df.columns:\n                target_col = c\n                break\n\n    # Determine task type\n    if target_col is not None:\n        # Choose source df\n        src = output_df if output_df is not None and target_col in output_df.columns else input_df\n        # Compute unique count\n        nunique = src[target_col].nunique(dropna=False)\n        # Infer type\n        if nunique <= 20 and not np.issubdtype(src[target_col].dtype, np.floating):\n            task_type = 'classification'\n        else:\n            task_type = 'regression'\n\n    # Return results\n    return target_col, task_type\n\n# Define a function to propose join keys by shared columns\ndef propose_join_keys(df_left, df_right):\n    # Return None if unavailable\n    if df_left is None or df_right is None:\n        return None\n    # Compute shared columns\n    shared = [c for c in df_left.columns if c in df_right.columns]\n    # Exclude non-keys\n    exclude = {'target', 'label', 'y', 'outcome', '__source_file'}\n    # Filter\n    shared = [c for c in shared if c not in exclude]\n    # Return top candidates\n    if len(shared) == 0:\n        return None\n    if len(shared) == 1:\n        return [shared[0]]\n    return shared[:2]\n\n# Define a main loader\ndef load_all():\n    # Load inputs\n    df_in, in_paths = load_many_csvs(INPUT_GLOB)\n    # Load outputs\n    df_out, out_paths = load_many_csvs(OUTPUT_GLOB)\n    # Load supplement\n    df_sup = pd.read_csv(SUPPLEMENT_PATH, low_memory=False) if os.path.exists(SUPPLEMENT_PATH) else None\n\n    # Detect target and task\n    detected_target, detected_task = detect_target_and_task(df_in, df_out)\n\n    # Propose join keys\n    proposed_keys = propose_join_keys(df_in, df_out)\n\n    # Apply user overrides\n    target_col = TARGET_COLUMN if TARGET_COLUMN else detected_target\n    task_type = TASK_TYPE if TASK_TYPE else detected_task\n    join_keys = JOIN_KEYS if JOIN_KEYS else proposed_keys\n\n    # Print summary\n    print('Input files:', len(in_paths))\n    print('Output files:', len(out_paths))\n    print('Input shape:', None if df_in is None else df_in.shape)\n    print('Output shape:', None if df_out is None else df_out.shape)\n    print('Supplementary shape:', None if df_sup is None else df_sup.shape)\n    print('Detected target:', target_col)\n    print('Detected task type:', task_type)\n    print('Proposed join keys:', join_keys)\n\n    # Return artifacts\n    return df_in, df_out, df_sup, target_col, task_type, join_keys\n\n# Execute loader\ndf_input, df_output, df_supp, target_col, task_type, join_keys = load_all()\n\n# Show head\nif df_input is not None:\n    display(df_input.head())\nif df_output is not None:\n    display(df_output.head())\nif df_supp is not None:\n    display(df_supp.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## B. Exploratory Data Analysis (EDA) — Structure, Distributions, Correlations, Analysis\n\n**1. Structure**\n  - Inspect column-wise shapes, data types, and missing values.\n  - Identify duplicate records and associated candidate IDs.\n\n**2. Distributions**\n  - Generate numeric histograms for the most important features.\n  - Create bar charts for categorical features with low cardinality.\n    \n**3. Correlations**\n  - Compute the correlation matrix for numerical features.\n  - Perform pairwise comparisons on a sample of columns.\n    \n**4. Analysis**\n  - Analyze class balance or the distribution of the target variable.\n  - Generate quick baseline performance estimates.\n","metadata":{}},{"cell_type":"code","source":"# Define function for basic summaries\ndef basic_eda(df, name):\n    # Print structure\n    print(f'[{name}] shape:', df.shape)\n    print(f'[{name}] dtypes:')\n    print(df.dtypes.head(40))\n    # Missingness\n    miss = df.isna().sum().sort_values(ascending=False)\n    print(f'[{name}] missing values (top 25):')\n    print(miss.head(25))\n\n# Define function to plot numeric histograms\ndef plot_numeric_histograms(df, limit=MAX_EDA_PLOTS):\n    # Select numeric columns\n    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    # Truncate to limit\n    num_cols = num_cols[:limit]\n    # Iterate columns\n    for c in num_cols:\n        # New figure\n        plt.figure()\n        # Histogram\n        df[c].hist(bins=30)\n        # Title\n        plt.title(f'Distribution: {c}')\n        # Show\n        plt.show()\n\n# Define function to plot correlation heatmap\ndef plot_correlation(df, limit=25):\n    # Select numeric subset\n    cols = df.select_dtypes(include=[np.number]).columns.tolist()[:limit]\n    # Return if not enough columns\n    if len(cols) < 2:\n        return\n    # Compute correlation\n    corr = df[cols].corr()\n    # New figure\n    plt.figure()\n    # Show matrix\n    plt.imshow(corr.values, aspect='auto')\n    # Title\n    plt.title('Correlation matrix (numeric subset)')\n    # Ticks\n    plt.xticks(range(len(cols)), cols, rotation=90)\n    plt.yticks(range(len(cols)), cols)\n    # Color bar\n    plt.colorbar()\n    # Show\n    plt.show()\n\n# Run EDA on input\nif df_input is not None:\n    # Summaries\n    basic_eda(df_input, 'input')\n    # Histograms\n    plot_numeric_histograms(df_input)\n    # Correlation\n    plot_correlation(df_input)\n\n# Target analysis\nif target_col is not None:\n    # Choose source\n    src = df_output if df_output is not None and target_col in df_output.columns else df_input\n    # Classification\n    if task_type == 'classification':\n        # Distribution\n        print('Target class distribution:')\n        print(src[target_col].value_counts(dropna=False))\n    # Regression\n    else:\n        # Summary\n        print('Target distribution summary:')\n        print(src[target_col].describe())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### C. Data Preprocessing\n\n**1. Alignment**\n  - Combine features and targets using detected or user-specified join keys.\n  - Incorporate supplementary data when available.\n\n**2. Train/Test**\n  - Apply stratification for classification tasks.\n  - Prevent data leakage using group-based or time-based splits when necessary.\n\n**3. Categorical Developments**\n  - Fill in missing values.\n  - Apply one-hot encoding to categorical variables.\n  - Normalize or scale numerical features.","metadata":{}}]}