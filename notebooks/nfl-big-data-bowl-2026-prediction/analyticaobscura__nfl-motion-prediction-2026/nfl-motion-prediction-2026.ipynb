{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":13417.407454,"end_time":"2025-10-06T17:00:11.429727","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-06T13:16:34.022273","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"16f69dd0ed0e4b4f873e406a05be5b9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ce2dece73544a2d9dc0c1f7f73491dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3eba886843684afa962dfa2960ec8679":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_6ccb82d03c354b43864be083f8d8268a","max":18,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16f69dd0ed0e4b4f873e406a05be5b9b","tabbable":null,"tooltip":null,"value":18}},"6c8cd34ee17f4f018a17157be5d23051":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ccb82d03c354b43864be083f8d8268a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86de40c16c0b4be2a654e2ab5b13a57c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8a0b4f3dd53244cbb291110b03fb6b21","placeholder":"​","style":"IPY_MODEL_fcc1b6f03afe41d8963fe7612e85e955","tabbable":null,"tooltip":null,"value":"100%"}},"8a0b4f3dd53244cbb291110b03fb6b21":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99dceefbbf21446093934c00963d91dd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0417de93a4e4ee88cedaff0ddd57ece":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_6c8cd34ee17f4f018a17157be5d23051","placeholder":"​","style":"IPY_MODEL_1ce2dece73544a2d9dc0c1f7f73491dd","tabbable":null,"tooltip":null,"value":" 18/18 [00:10&lt;00:00,  2.12it/s]"}},"f5f0088e34de4ad28a9af76ee199ad2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86de40c16c0b4be2a654e2ab5b13a57c","IPY_MODEL_3eba886843684afa962dfa2960ec8679","IPY_MODEL_f0417de93a4e4ee88cedaff0ddd57ece"],"layout":"IPY_MODEL_99dceefbbf21446093934c00963d91dd","tabbable":null,"tooltip":null}},"fcc1b6f03afe41d8963fe7612e85e955":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"b31ad57f-e9bc-48bc-a189-0aa703031d37","cell_type":"markdown","source":"<center>\n  <div style=\"background-color:#334155; color:#e2e8f0; padding:2rem; border-radius:1rem; text-align:center; font-family:sans-serif; margin-bottom:2rem;\">\n    <h1 style=\"color:#58a6ff; margin-bottom:0.5rem;\">Ozan MÖHÜRCÜ</h1>\n    <h2 style=\"color:#cbd5e1; font-weight:400; font-size:1.25rem; margin-top:0; margin-bottom:1.5rem;\">Data Analyst | Data Scientist</h2>\n    <a href=\"https://www.linkedin.com/in/ozanmhrc/\" target=\"_blank\" rel=\"noopener noreferrer\">\n      <img src=\"https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn Profile\">\n    </a>\n    <a href=\"https://github.com/Ozan-Mohurcu\" target=\"_blank\" rel=\"noopener noreferrer\">\n      <img src=\"https://img.shields.io/badge/GitHub-171515?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub Profile\">\n    </a>\n    <a href=\"https://ozan-mohurcu.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">\n      <img src=\"https://img.shields.io/badge/Portfolio-6A1B9A?style=for-the-badge&logo=google-chrome&logoColor=white\" alt=\"Portfolio Website\">\n    </a>\n  </div>\n</center>","metadata":{}},{"id":"d749319a-e7e1-4350-896b-7845f13e56d0","cell_type":"markdown","source":"<div style=\"background-color: #111827; color: #F3F4F6; border-left: 10px solid #3B82F6; padding: 25px; margin: 20px 0px; border-radius: 8px; box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2); text-align: justify;\">\n  <h1 style=\"color: #60A5FA; text-align: center; margin-top: -10px; margin-bottom: 25px;\">NFL Big Data Bowl 2026: An End-to-End Play Prediction Strategy</h1>\n  \n  <h2 style=\"color: #60A5FA; border-bottom: 2px solid #3B82F6; padding-bottom: 10px;\">Project Overview & Objectives</h2>\n  <p>Welcome to this comprehensive walkthrough for the NFL Big Data Bowl 2026! The core challenge of this competition is to predict the future locations (x, y coordinates) of players on the field based on a sequence of tracking data. This is a fascinating problem that blends time-series analysis, physics-based modeling, and understanding complex player interactions.</p>\n  <p><b>Our strategic approach in this notebook will be:</b></p>\n  <ul>\n    <li><b>Foundation in Physics:</b> We'll start by building a simple, yet powerful, constant velocity baseline model. This gives us a solid benchmark and serves as the foundation for our advanced model.</li>\n    <li><b>Advanced Feature Engineering:</b> We will extract a rich set of features, capturing player kinematics, geometry relative to the ball, and historical movement patterns (lags, rolling stats).</li>\n    <li><b>Modeling Player Interactions (GNN-lite):</b> A player's movement is heavily influenced by those around them. We'll implement a lightweight Graph Neural Network (GNN) concept to create \"neighbor embeddings,\" summarizing the state of nearby allies and opponents.</li>\n    <li><b>Residual Prediction with CatBoost:</b> Instead of predicting the absolute final coordinates, we will train our model to predict the <i>error (or residual)</i> of our physics baseline. This is a powerful technique that helps the model focus on learning the complex, non-linear dynamics that the simple model misses.</li>\n    <li><b>Robust Validation:</b> We will use GroupKFold cross-validation to ensure our model generalizes well and that we don't have data leakage between training and validation sets.</li>\n  </ul>\n</div>","metadata":{}},{"id":"b7b15960-0ef8-4e38-a276-42d2b6f83d90","cell_type":"markdown","source":"1. Setup and Configuration\n<div style=\"background-color: #f0f8ff; color: #333; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0px; border-radius: 5px;\">\n<p>First, let's set up our environment. We'll import the necessary libraries and define our global configuration parameters. This includes settings for our model, feature engineering, and cross-validation strategy. Encapsulating these in a single location makes the notebook cleaner and easier to modify.</p>\n</div>","metadata":{}},{"id":"147e0f92-8594-42b8-9bb7-e6a96ea0f3a9","cell_type":"code","source":"import os\nimport gc\nimport math\nimport pickle\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom multiprocessing import Pool as MP, cpu_count\n\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor, Pool as CatPool\nfrom tqdm.auto import tqdm\n\n# Ignore harmless warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --- Configuration Block ---\n\nclass CFG:\n    # Paths\n    BASE_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    SAVE_DIR = Path(\"/kaggle/working\")\n    \n    # Data Specs\n    N_WEEKS = 18\n    \n    # Model Parameters (CatBoost)\n    # Using a high number of iterations with early stopping is a robust approach.\n    ITERATIONS = 20000\n    LEARNING_RATE = 0.08\n    DEPTH = 8\n    L2_REG = 3.0\n    EARLY_STOPPING_ROUNDS = 500\n    \n    # CV Strategy\n    N_FOLDS = 5\n    USE_GROUP_KFOLD = True  # Highly recommended to prevent data leakage\n    SEED = 42\n    \n    # GNN-lite (Neighbor Embedding) Parameters\n    K_NEIGHBORS = 6       # How many nearest neighbors to consider for each player\n    RADIUS_LIMIT = 30.0   # Max distance (yards) to look for neighbors\n    TAU = 8.0             # Temperature parameter for softmax weighting (controls influence falloff)\n    \n    # Environment\n    # Automatically detect if a GPU is available for CatBoost\n    USE_GPU = bool(os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\"))\n\n# Create save directory if it doesn't exist\nCFG.SAVE_DIR.mkdir(exist_ok=True)\n\nprint(f\"Running with GPU support: {CFG.USE_GPU}\")\nprint(f\"CPU cores available: {cpu_count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T12:56:17.733391Z","iopub.execute_input":"2025-10-08T12:56:17.733645Z","iopub.status.idle":"2025-10-08T12:56:20.949269Z","shell.execute_reply.started":"2025-10-08T12:56:17.73362Z","shell.execute_reply":"2025-10-08T12:56:20.948499Z"}},"outputs":[],"execution_count":null},{"id":"ebee7999-e26d-442f-85ba-4dafcf717782","cell_type":"markdown","source":"2. Data Loading & Initial Exploration\n<div style=\"background-color: #f0f8ff; color: #333; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0px; border-radius: 5px;\">\n<p>The training data is split into 18 weekly files. We'll write a utility function to load all of them in parallel, which significantly speeds up the process. After loading, we'll perform a quick sanity check to understand the structure and volume of our dataset.</p>\n<h4>Understanding the Coordinate System</h4>\n<p>It's critical to correctly interpret the geometry of the data. In the NFL's system:</p>\n<ul>\n<li>The field is 120 yards long (including end zones) and 53.3 yards wide.</li>\n<li><code>(0, 0)</code> is the corner of the home team's end zone.</li>\n<li><code>x</code> represents the yard line, from 0 to 120.</li>\n<li><code>y</code> represents the distance from the sideline, from 0 to 53.3.</li>\n<li>Direction (<code>dir</code>) and Orientation (<code>o</code>) are in degrees, where <b>0 degrees points straight down the field towards the opponent's end zone (+Y direction in physics calculations, but this needs careful conversion)</b>. This is a common source of bugs! We will standardize this.</li>\n</ul>\n</div>","metadata":{}},{"id":"e560210e-00ad-4352-8226-48cd942d653b","cell_type":"code","source":"def load_week_data(week_num: int):\n    \"\"\"Loads the input and output data for a single week.\"\"\"\n    input_path = CFG.BASE_DIR / f\"train/input_2023_w{week_num:02d}.csv\"\n    output_path = CFG.BASE_DIR / f\"train/output_2023_w{week_num:02d}.csv\"\n    return pd.read_csv(input_path), pd.read_csv(output_path)\n\ndef load_all_training_data():\n    \"\"\"Loads all 18 weeks of training data in parallel.\"\"\"\n    print(\"Loading all training data using parallel processing...\")\n    # Use a multiprocessing pool to load files concurrently\n    with MP(min(cpu_count(), CFG.N_WEEKS)) as pool:\n        results = list(tqdm(pool.imap(load_week_data, range(1, CFG.N_WEEKS + 1)), total=CFG.N_WEEKS))\n    \n    # Concatenate the results into two large DataFrames\n    train_input_df = pd.concat([res[0] for res in results], ignore_index=True)\n    train_output_df = pd.concat([res[1] for res in results], ignore_index=True)\n    \n    print(f\"Total train input rows:  {len(train_input_df):,}\")\n    print(f\"Total train output rows: {len(train_output_df):,}\")\n    \n    # Free up memory\n    del results\n    gc.collect()\n    \n    return train_input_df, train_output_df\n\n# Execute the loading process\ntrain_input_df, train_output_df = load_all_training_data()\n\nprint(\"\\nSample of Input Data:\")\ndisplay(train_input_df.head())\nprint(\"\\nSample of Output Data:\")\ndisplay(train_output_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T12:56:20.950229Z","iopub.execute_input":"2025-10-08T12:56:20.950606Z","iopub.status.idle":"2025-10-08T12:56:31.02681Z","shell.execute_reply.started":"2025-10-08T12:56:20.95058Z","shell.execute_reply":"2025-10-08T12:56:31.026147Z"}},"outputs":[],"execution_count":null},{"id":"d8919f79-ad13-48bb-8880-ffdd26ecc7c3","cell_type":"markdown","source":"3. Feature Engineering Pipeline\n<div style=\"background-color: #1F2937; color: #D1D5DB; border-left: 10px solid #F59E0B; padding: 25px; margin: 20px 0px; border-radius: 8px;\">\n<h3 style=\"color: #FBBF24; border-bottom: 2px solid #F59E0B; padding-bottom: 10px;\">The Art and Science of Feature Creation</h3>\n<p>This is where we add the most value. Raw tracking data is just a starting point. We need to create features that explicitly provide the model with information about a player's physical state, their intent, their movement history, and their relationship with other players on the field. We'll build this in a modular pipeline.</p>\n</div>\n\n3.1. Core Physics and Geometry Features\n<div style=\"background-color: #f0f8ff; color: #333; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0px; border-radius: 5px;\">\n<p>Here, we convert raw stats into a more meaningful physical representation. A key step is standardizing the angle convention. The NFL's dir is unconventional for physics. We will convert it to a standard mathematical angle where 0 degrees is the +X axis, which allows us to use sin and cos correctly to decompose vectors.</p>\n<ul>\n<li><b>Player Attributes:</b> Convert height to inches and calculate Body Mass Index (BMI).</li>\n<li><b>Standardized Kinematics:</b> Decompose speed (<code>s</code>) and acceleration (<code>a</code>) into their X and Y components (<code>velocity_x</code>, <code>velocity_y</code>, etc.) using the corrected heading.</li>\n<li><b>Target Geometry:</b> Calculate the player's distance and angle to the predicted ball landing spot. This is a powerful indicator of a player's objective.</li>\n<li><b>Advanced Physics:</b> Features like momentum and kinetic energy can help the model understand the \"cost\" of changing direction.</li>\n</ul>\n</div>","metadata":{}},{"id":"7289a239-4abf-4c24-a1e0-823a8980be5d","cell_type":"code","source":"def convert_height_to_inches(h_str):\n    \"\"\"Converts height string 'feet-inches' to total inches.\"\"\"\n    try:\n        feet, inches = map(int, str(h_str).split('-'))\n        return float(feet) * 12.0 + float(inches)\n    except:\n        return np.nan\n\ndef add_physics_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Engineers physics-based and geometric features.\"\"\"\n    df = df.copy()\n    \n    # --- Player Attributes ---\n    df['height_inches'] = df['player_height'].apply(convert_height_to_inches)\n    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703.0\n\n    # --- Standardize Angles and Decompose Vectors ---\n    # The 'dir' column is clockwise from the +y axis. Let's convert to standard\n    # counter-clockwise from +x axis for easier trigonometry.\n    # Standard Angle (rad) = pi/2 - dir(rad)\n    dir_rad = np.radians(df['dir'].fillna(0.0))\n    std_angle_rad = np.pi/2 - dir_rad\n    \n    df['heading_x'] = np.cos(std_angle_rad)\n    df['heading_y'] = np.sin(std_angle_rad)\n\n    s = df['s'].fillna(0.0)\n    a = df['a'].fillna(0.0)\n    df['velocity_x'] = s * df['heading_x']\n    df['velocity_y'] = s * df['heading_y']\n    df['acceleration_x'] = a * df['heading_x']\n    df['acceleration_y'] = a * df['heading_y']\n\n    # --- Geometry relative to ball landing spot ---\n    dx_ball = df['ball_land_x'] - df['x']\n    dy_ball = df['ball_land_y'] - df['y']\n    dist_ball = np.sqrt(dx_ball**2 + dy_ball**2)\n    \n    df['dist_to_ball'] = dist_ball\n    # Angle to ball in radians, using standard atan2\n    df['angle_to_ball_rad'] = np.arctan2(dy_ball, dx_ball)\n    \n    # Unit vector towards the ball\n    ball_unit_x = dx_ball / (dist_ball + 1e-6)\n    ball_unit_y = dy_ball / (dist_ball + 1e-6)\n    \n    # Velocity component towards the ball (dot product of velocity and ball unit vector)\n    df['velocity_towards_ball'] = df['velocity_x'] * ball_unit_x + df['velocity_y'] * ball_unit_y\n    # Alignment with ball path (dot product of heading and ball unit vector)\n    df['heading_alignment_ball'] = df['heading_x'] * ball_unit_x + df['heading_y'] * ball_unit_y\n\n    # --- Other Physics and Role-based Features ---\n    w = df['player_weight'].fillna(200.0) # Impute with average weight\n    df['kinetic_energy'] = 0.5 * w * (s**2)\n    df['momentum'] = w * s\n    \n    df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n    df['is_targeted_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n\n    return df\n\nprint(\"Applying physics and geometry feature engineering...\")\ntrain_input_df = add_physics_features(train_input_df)\nprint(\"Done. New features added.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T12:56:31.028351Z","iopub.execute_input":"2025-10-08T12:56:31.028555Z","iopub.status.idle":"2025-10-08T12:56:37.494326Z","shell.execute_reply.started":"2025-10-08T12:56:31.028539Z","shell.execute_reply":"2025-10-08T12:56:37.493622Z"}},"outputs":[],"execution_count":null},{"id":"31eae49c-599a-4bce-8770-2a425d5a30ab","cell_type":"markdown","source":"3.2. Sequential Features (Lags & Rolling Stats)\n<div style=\"background-color: #f0f8ff; color: #333; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0px; border-radius: 5px;\">\n<p>A single frame is a snapshot in time. To understand a player's trajectory and recent behavior, we need to look at the immediate past. We compute lag features (the value of a variable at a previous timestep) and rolling window statistics (mean/std over the last N frames).</p>\n<p>This gives the model a sense of the player's recent velocity, acceleration changes, and stability of movement, which is crucial for predicting their next move.</p>\n</div>","metadata":{}},{"id":"9c1cc999-b97c-4d70-893a-e18260917e1b","cell_type":"code","source":"def add_sequential_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Computes lag and rolling window features for time-series data.\"\"\"\n    df = df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"]).copy()\n    group_cols = [\"game_id\", \"play_id\", \"nfl_id\"]\n    \n    # Columns for which to create sequential features\n    seq_cols = ['x', 'y', 's', 'a', 'velocity_x', 'velocity_y']\n    \n    print(\"Creating lag features...\")\n    for lag in tqdm([1, 2, 3]):\n        for col in seq_cols:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n\n    print(\"Creating rolling window features...\")\n    for window in tqdm([3, 5]):\n        for col in seq_cols:\n            if col in df.columns:\n                # Use .transform for efficient grouped operations\n                rolling_mean = df.groupby(group_cols)[col].transform(lambda s: s.rolling(window, min_periods=1).mean())\n                rolling_std = df.groupby(group_cols)[col].transform(lambda s: s.rolling(window, min_periods=1).std())\n                df[f'{col}_rolling_mean_{window}'] = rolling_mean\n                df[f'{col}_rolling_std_{window}'] = rolling_std\n    \n    # Calculate change (delta) from the previous frame\n    for col in ['velocity_x', 'velocity_y']:\n        if col in df.columns:\n            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff().fillna(0.0)\n            \n    return df\n\nprint(\"Applying sequential feature engineering...\")\ntrain_input_df = add_sequential_features(train_input_df)\nprint(\"Done. Lag and rolling features added.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T12:56:37.494968Z","iopub.execute_input":"2025-10-08T12:56:37.495148Z","iopub.status.idle":"2025-10-08T13:09:14.725518Z","shell.execute_reply.started":"2025-10-08T12:56:37.495134Z","shell.execute_reply":"2025-10-08T13:09:14.724866Z"}},"outputs":[],"execution_count":null},{"id":"0b567a68-eba5-44df-b4cc-313ed30624c6","cell_type":"markdown","source":"3.3. GNN-lite: Modeling Player Interactions\n<div style=\"background-color: #1F2937; color: #D1D5DB; border-left: 10px solid #F59E0B; padding: 25px; margin: 20px 0px; border-radius: 8px;\">\n<h3 style=\"color: #FBBF24; border-bottom: 2px solid #F59E0B; padding-bottom: 10px;\">Why Model Interactions?</h3>\n<p>A player's movement isn't made in a vacuum. A receiver adjusts their route based on the defender, and a defender reacts to the receiver. A full GNN is complex, but we can capture the essence of this with a \"GNN-lite\" approach. For each player (the \"ego\" player), we will create features that summarize the state of their local neighborhood.</p>\n<h4>Our GNN-lite Process:</h4>\n<ol>\n<li><b>Identify Neighbors:</b> For each player at their last observed frame, find all other players on the field within a certain radius.</li>\n<li><b>Calculate Relative State:</b> For each neighbor, compute their relative position (dx, dy) and velocity (dvx, dvy).</li>\n<li><b>Apply Attention:</b> We can't treat all neighbors equally. A player 2 yards away is more influential than one 20 yards away. We use a softmax function based on distance (<code>exp(-dist / tau)</code>) to create attention weights. Closer players get higher weights.</li>\n<li><b>Aggregate:</b> We compute a weighted average of the relative states, separately for teammates (allies) and opponents. This creates features like gnn_ally_dx_mean (the attention-weighted average x-position of teammates) or gnn_opp_dmin (distance to the nearest opponent).</li>\n</ol>\n<p>This gives the model a compact, powerful summary of the local competitive landscape for each player.</p>\n</div>","metadata":{}},{"id":"947872a9-2ae5-4d94-a5d5-670183749964","cell_type":"code","source":"def compute_neighbor_embeddings(input_df: pd.DataFrame, cfg: CFG) -> pd.DataFrame:\n    \"\"\"\n    Creates GNN-lite features by summarizing the state of nearby players\n    at the last observed frame for each player.\n    \"\"\"\n    print(\"Computing GNN-lite neighbor embeddings...\")\n    \n    # We only need specific columns for this calculation to save memory\n    cols_needed = [\n        \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\",\n        \"velocity_x\", \"velocity_y\", \"player_side\"\n    ]\n    src_df = input_df[cols_needed].copy()\n\n    # Get the state of each player at their last observed frame\n    last_frame_df = (\n        src_df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n              .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n              .tail(1)\n              .rename(columns={\"frame_id\": \"last_frame_id\"})\n              .reset_index(drop=True)\n    )\n\n    # Merge last_frame_df with all players in the same play at that specific frame\n    # This creates pairs of (ego_player, neighbor_player)\n    merged_df = last_frame_df.merge(\n        src_df.rename(columns={\n            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\", \"x\": \"x_nb\", \"y\": \"y_nb\",\n            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\", \"player_side\": \"player_side_nb\"\n        }),\n        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n        how=\"left\",\n    )\n    \n    # Remove self-comparisons\n    merged_df = merged_df[merged_df[\"nfl_id_nb\"] != merged_df[\"nfl_id\"]]\n\n    # Calculate relative vectors and distance\n    merged_df[\"dx\"] = merged_df[\"x_nb\"] - merged_df[\"x\"]\n    merged_df[\"dy\"] = merged_df[\"y_nb\"] - merged_df[\"y\"]\n    merged_df[\"dvx\"] = merged_df[\"vx_nb\"] - merged_df[\"velocity_x\"]\n    merged_df[\"dvy\"] = merged_df[\"vy_nb\"] - merged_df[\"velocity_y\"]\n    merged_df[\"dist\"] = np.sqrt(merged_df[\"dx\"]**2 + merged_df[\"dy\"]**2)\n\n    # Filter out distant neighbors\n    merged_df = merged_df[merged_df[\"dist\"] <= cfg.RADIUS_LIMIT].copy()\n\n    # Identify allies vs opponents\n    merged_df[\"is_ally\"] = (merged_df[\"player_side_nb\"] == merged_df[\"player_side\"]).astype(float)\n\n    # Rank neighbors by distance to find the closest ones\n    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n    merged_df[\"rank\"] = merged_df.groupby(keys)[\"dist\"].rank(method=\"first\")\n    \n    # Keep only the top K neighbors\n    merged_df = merged_df[merged_df[\"rank\"] <= cfg.K_NEIGHBORS].copy()\n\n    # --- Attention Weighting (Softmax) ---\n    merged_df[\"attention\"] = np.exp(-merged_df[\"dist\"] / cfg.TAU)\n    attention_sum = merged_df.groupby(keys)[\"attention\"].transform(\"sum\")\n    merged_df[\"norm_attention\"] = merged_df[\"attention\"] / (attention_sum + 1e-9)\n    \n    merged_df[\"norm_attention_ally\"] = merged_df[\"norm_attention\"] * merged_df[\"is_ally\"]\n    merged_df[\"norm_attention_opp\"] = merged_df[\"norm_attention\"] * (1.0 - merged_df[\"is_ally\"])\n\n    # Pre-multiply features by attention weights for weighted aggregation\n    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n        merged_df[f\"{col}_w_ally\"] = merged_df[col] * merged_df[\"norm_attention_ally\"]\n        merged_df[f\"{col}_w_opp\"] = merged_df[col] * merged_df[\"norm_attention_opp\"]\n    \n    # Separate distances for allies and opponents for min/mean stats\n    merged_df[\"dist_ally\"] = np.where(merged_df[\"is_ally\"] > 0.5, merged_df[\"dist\"], np.nan)\n    merged_df[\"dist_opp\"] = np.where(merged_df[\"is_ally\"] < 0.5, merged_df[\"dist\"], np.nan)\n\n    # --- Aggregation ---\n    agg_dict = {\n        # Weighted means of relative vectors\n        \"gnn_ally_dx_mean\": (\"dx_w_ally\", \"sum\"),\n        \"gnn_ally_dy_mean\": (\"dy_w_ally\", \"sum\"),\n        \"gnn_ally_dvx_mean\": (\"dvx_w_ally\", \"sum\"),\n        \"gnn_ally_dvy_mean\": (\"dvy_w_ally\", \"sum\"),\n        \"gnn_opp_dx_mean\": (\"dx_w_opp\", \"sum\"),\n        \"gnn_opp_dy_mean\": (\"dy_w_opp\", \"sum\"),\n        \"gnn_opp_dvx_mean\": (\"dvx_w_opp\", \"sum\"),\n        \"gnn_opp_dvy_mean\": (\"dvy_w_opp\", \"sum\"),\n        # Counts and distance stats\n        \"gnn_ally_count\": (\"is_ally\", \"sum\"),\n        \"gnn_ally_dist_min\": (\"dist_ally\", \"min\"),\n        \"gnn_ally_dist_mean\": (\"dist_ally\", \"mean\"),\n        \"gnn_opp_dist_min\": (\"dist_opp\", \"min\"),\n        \"gnn_opp_dist_mean\": (\"dist_opp\", \"mean\"),\n    }\n    \n    gnn_features = merged_df.groupby(keys).agg(**agg_dict).reset_index()\n    gnn_features[\"gnn_opp_count\"] = cfg.K_NEIGHBORS - gnn_features[\"gnn_ally_count\"]\n\n    # --- Add distance to N nearest players (regardless of side) ---\n    nearest_dist = merged_df.loc[merged_df['rank'] <= 3].pivot_table(\n        index=keys, columns='rank', values='dist'\n    ).reset_index()\n    nearest_dist.columns = [f\"gnn_dist_rank{int(c)}\" if isinstance(c, float) else c for c in nearest_dist.columns]\n    \n    gnn_features = gnn_features.merge(nearest_dist, on=keys, how=\"left\")\n    \n    # Fill NaNs that occur when a player has no neighbors of a certain type\n    gnn_features = gnn_features.fillna(0)\n    \n    print(\"GNN-lite embeddings computed.\")\n    return gnn_features\n\ngnn_train_features = compute_neighbor_embeddings(train_input_df, CFG)\ndisplay(gnn_train_features.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T13:09:14.726376Z","iopub.execute_input":"2025-10-08T13:09:14.726644Z","iopub.status.idle":"2025-10-08T13:09:19.724896Z","shell.execute_reply.started":"2025-10-08T13:09:14.726617Z","shell.execute_reply":"2025-10-08T13:09:19.724319Z"}},"outputs":[],"execution_count":null},{"id":"b7e062b0-8af5-4105-8e80-133b2abb4bd1","cell_type":"markdown","source":"4. Preparing the Training Dataset\n<div style=\"background-color: #f0f8ff; color: #333; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0px; border-radius: 5px;\">\n<p>Now we assemble our final training dataset. For each row in the output file (which represents a future frame we need to predict), we need to attach the features from the <b>last observed frame</b> of that player. This creates our (features, target) structure.</p>\n<p>After merging, we'll also compute our physics baseline and the residual targets that our model will learn to predict.</p>\n</div>\n\n4.1. Physics Baseline & Residual Targets\n<div style=\"background-color: #1F2937; color: #D1D5DB; border-left: 10px solid #F59E0B; padding: 25px; margin: 20px 0px; border-radius: 8px;\">\n<h3 style=\"color: #FBBF24; border-bottom: 2px solid #F59E0B; padding-bottom: 10px;\">The Power of Residual Modeling</h3>\n<p>Directly predicting coordinates like (110.5, 25.3) is hard. The model has to learn the entire field's coordinate system. A much more effective approach is to give the model a good \"first guess\" and ask it to predict the correction.</p>\n<ul>\n<li><b>Baseline Prediction:</b> Our \"first guess\" is a constant velocity model: <code>future_pos = last_pos + last_velocity * delta_t</code>. This is simple but captures the majority of the movement.</li>\n<li><b>Residual (Target):</b> The value our ML model will actually predict is: <code>residual = true_future_pos - baseline_prediction</code>.</li>\n<li><b>Final Prediction:</b> During inference, our final answer is: <code>Final Prediction = baseline_prediction + predicted_residual</code>.</li>\n</ul>\n<p>This transforms the problem from \"predict where the player will be\" to \"predict how much the player will deviate from a straight line path,\" which is an easier and more stable learning task.</p>\n</div>","metadata":{}},{"id":"0069e793-41d7-442b-98fa-2ba5231db2d7","cell_type":"code","source":"def create_training_dataframe(input_df, output_df, gnn_df):\n    \"\"\"\n    Assembles the final training DataFrame by merging last observed stats\n    with future target frames.\n    \"\"\"\n    print(\"Assembling final training dataframe...\")\n    \n    # Aggregate input_df to get the last observed state for each player in each play\n    last_observed_state = (\n        input_df.sort_values(\"frame_id\")\n                .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n                .tail(1)\n                .rename(columns={\"frame_id\": \"last_frame_id\"})\n    )\n    \n    # Merge the last observed state with the future frames we need to predict\n    train_df = output_df.rename(columns={\"x\": \"target_x\", \"y\": \"target_y\"}).merge(\n        last_observed_state,\n        on=[\"game_id\", \"play_id\", \"nfl_id\"],\n        how=\"left\"\n    )\n    \n    # Merge the GNN features\n    train_df = train_df.merge(\n        gnn_df,\n        on=[\"game_id\", \"play_id\", \"nfl_id\"],\n        how=\"left\"\n    )\n    \n    # --- Calculate time delta and physics baseline ---\n    train_df[\"delta_frames\"] = train_df[\"frame_id\"] - train_df[\"last_frame_id\"]\n    train_df[\"delta_t\"] = train_df[\"delta_frames\"] / 10.0  # Data is at 10Hz\n\n    # Baseline prediction: new_pos = old_pos + velocity * time\n    base_x = train_df[\"x\"] + train_df[\"velocity_x\"] * train_df[\"delta_t\"]\n    base_y = train_df[\"y\"] + train_df[\"velocity_y\"] * train_df[\"delta_t\"]\n    \n    # Clip to be within field boundaries\n    train_df[\"baseline_x\"] = np.clip(base_x, 0.0, 120.0)\n    train_df[\"baseline_y\"] = np.clip(base_y, 0.0, 53.3)\n\n    # --- Calculate residual targets ---\n    train_df[\"residual_x\"] = train_df[\"target_x\"] - train_df[\"baseline_x\"]\n    train_df[\"residual_y\"] = train_df[\"target_y\"] - train_df[\"baseline_y\"]\n    \n    # Evaluate the baseline's performance\n    baseline_rmse = np.sqrt(0.5 * (\n        mean_squared_error(train_df[\"target_x\"], train_df[\"baseline_x\"]) +\n        mean_squared_error(train_df[\"target_y\"], train_df[\"baseline_y\"])\n    ))\n    print(f\"Physics Baseline RMSE: {baseline_rmse:.5f}\")\n    \n    return train_df\n\ntrain_df = create_training_dataframe(train_input_df, train_output_df, gnn_train_features)\n\n# Free up memory\ndel train_input_df, train_output_df, gnn_train_features\ngc.collect()\n\ndisplay(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T13:09:19.725588Z","iopub.execute_input":"2025-10-08T13:09:19.725809Z","iopub.status.idle":"2025-10-08T13:09:27.078593Z","shell.execute_reply.started":"2025-10-08T13:09:19.725783Z","shell.execute_reply":"2025-10-08T13:09:27.077747Z"}},"outputs":[],"execution_count":null},{"id":"e810d896-8085-479b-87c2-31743d3b3002","cell_type":"markdown","source":"5. Model Training with GroupKFold CV\n<div style=\"background-color: #f0f8ff; color: #333; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0px; border-radius: 5px;\">\n<p>We are now ready to train our models. We will train two separate CatBoost models: one to predict residual_x and one for residual_y.</p>\n<h4>Why GroupKFold?</h4>\n<p>Standard KFold cross-validation randomly splits the data. In our case, this is dangerous. A single player's trajectory (game_id, play_id, nfl_id) is split across multiple future frames. If we randomly split, frame 30 might be in the training set while frame 40 (from the same play and player) is in the validation set. The model could \"cheat\" by learning specifics of that single trajectory, leading to an inflated validation score that doesn't reflect true performance on unseen plays.\n\nGroupKFold solves this by ensuring that all rows belonging to the same group (in our case, a unique game_id-play_id-nfl_id combination) are kept together in the same fold. This simulates a more realistic scenario where the model must predict for entirely new plays.</p>\n</div>","metadata":{"execution":{"iopub.status.busy":"2025-10-08T09:19:22.535637Z","iopub.status.idle":"2025-10-08T09:19:22.535868Z","shell.execute_reply.started":"2025-10-08T09:19:22.535753Z","shell.execute_reply":"2025-10-08T09:19:22.535762Z"}}},{"id":"0fe63ad9-eb39-4f03-9db8-98e0657910b6","cell_type":"code","source":"def get_feature_list(df):\n    \"\"\"Defines the list of features to be used for training.\"\"\"\n    # Start with a base list of engineered features\n    base_features = [\n        \"x\", \"y\", \"s\", \"a\", \"o\", \"dir\", \"height_inches\", \"bmi\", \"kinetic_energy\", \"momentum\",\n        \"velocity_x\", \"velocity_y\", \"acceleration_x\", \"acceleration_y\",\n        \"heading_x\", \"heading_y\", \"dist_to_ball\", \"angle_to_ball_rad\",\n        \"velocity_towards_ball\", \"heading_alignment_ball\", \"is_offense\", \"is_targeted_receiver\",\n        \"delta_t\", \"delta_frames\"\n    ]\n    \n    # Add lag, rolling, and delta features dynamically\n    lag_cols = [c for c in df.columns if '_lag' in c]\n    roll_cols = [c for c in df.columns if '_rolling_' in c]\n    delta_cols = [c for c in df.columns if '_delta' in c]\n    \n    # Add GNN features dynamically\n    gnn_cols = [c for c in df.columns if c.startswith('gnn_')]\n    \n    # Combine all feature lists\n    all_features = base_features + lag_cols + roll_cols + delta_cols + gnn_cols\n    \n    # Ensure no duplicates and all columns exist in the DataFrame\n    final_features = sorted(list(set([c for c in all_features if c in df.columns])))\n    \n    print(f\"Total features to be used: {len(final_features)}\")\n    return final_features\n\n# --- Prepare data for training ---\nfeatures = get_feature_list(train_df)\ntrain_df = train_df.dropna(subset=features + [\"residual_x\", \"residual_y\"]).reset_index(drop=True)\n\n# Replace any remaining infs/nans in features with 0\nfor col in features:\n    train_df[col] = train_df[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n\nX = train_df[features].values.astype(np.float32)\ny_x = train_df[\"residual_x\"].values.astype(np.float32)\ny_y = train_df[\"residual_y\"].values.astype(np.float32)\n\n# For validation scoring, we need the original targets and baselines\ny_target_x = train_df[\"target_x\"].values\ny_target_y = train_df[\"target_y\"].values\nbaseline_x = train_df[\"baseline_x\"].values\nbaseline_y = train_df[\"baseline_y\"].values\n\n# Create groups for GroupKFold\ngroups = pd.factorize(\n    train_df[\"game_id\"].astype(str) + \"_\" +\n    train_df[\"play_id\"].astype(str) + \"_\" +\n    train_df[\"nfl_id\"].astype(str)\n)[0]\n\n# --- CV and Training Loop ---\nmodels_x, models_y, oof_rmse_scores = [], [], []\n\nif CFG.USE_GROUP_KFOLD:\n    print(f\"\\nStarting training with GroupKFold (n_splits={CFG.N_FOLDS})...\")\n    cv = GroupKFold(n_splits=CFG.N_FOLDS)\n    folds = cv.split(X, groups=groups)\nelse:\n    print(f\"\\nStarting training with KFold (n_splits={CFG.N_FOLDS})...\")\n    cv = KFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.SEED)\n    folds = cv.split(X)\n\nfor i, (train_idx, val_idx) in enumerate(folds):\n    fold_num = i + 1\n    print(f\"\\n----- Fold {fold_num}/{CFG.N_FOLDS} -----\")\n    \n    # Split data\n    X_train, X_val = X[train_idx], X[val_idx]\n    yx_train, yx_val = y_x[train_idx], y_x[val_idx]\n    yy_train, yy_val = y_y[train_idx], y_y[val_idx]\n\n    # CatBoost parameters\n    params = {\n        'iterations': CFG.ITERATIONS,\n        'learning_rate': CFG.LEARNING_RATE,\n        'depth': CFG.DEPTH,\n        'l2_leaf_reg': CFG.L2_REG,\n        'loss_function': 'RMSE',\n        'random_seed': CFG.SEED + fold_num, # Vary seed per fold\n        'verbose': 500,\n        'early_stopping_rounds': CFG.EARLY_STOPPING_ROUNDS,\n        'task_type': 'GPU' if CFG.USE_GPU else 'CPU',\n        'devices': '0' if CFG.USE_GPU else None,\n    }\n\n    # --- Train X-Model ---\n    print(\"Training model for residual_x...\")\n    model_x = CatBoostRegressor(**params)\n    model_x.fit(X_train, yx_train, eval_set=(X_val, yx_val))\n    \n    # --- Train Y-Model ---\n    print(\"\\nTraining model for residual_y...\")\n    model_y = CatBoostRegressor(**params)\n    model_y.fit(X_train, yy_train, eval_set=(X_val, yy_val))\n\n    # --- Validation ---\n    pred_rx_val = model_x.predict(X_val)\n    pred_ry_val = model_y.predict(X_val)\n    \n    # Add residuals back to baseline to get absolute coordinates\n    pred_x_abs = baseline_x[val_idx] + pred_rx_val\n    pred_y_abs = baseline_y[val_idx] + pred_ry_val\n    \n    # Calculate combined RMSE on absolute coordinates\n    rmse_x = mean_squared_error(y_target_x[val_idx], pred_x_abs, squared=False)\n    rmse_y = mean_squared_error(y_target_y[val_idx], pred_y_abs, squared=False)\n    fold_rmse = np.sqrt(0.5 * (rmse_x**2 + rmse_y**2))\n    \n    print(f\"\\nFold {fold_num} Validation RMSE (absolute): {fold_rmse:.5f}\")\n    \n    models_x.append(model_x)\n    models_y.append(model_y)\n    oof_rmse_scores.append(fold_rmse)\n\nprint(\"\\n--- Training Summary ---\")\nprint(f\"Mean CV RMSE: {np.mean(oof_rmse_scores):.5f}\")\nprint(f\"Std CV RMSE:  {np.std(oof_rmse_scores):.5f}\")\n\n# Save models and features list for inference\nartifacts = {\n    \"models_x\": models_x,\n    \"models_y\": models_y,\n    \"features\": features,\n    \"cv_scores\": oof_rmse_scores\n}\nwith open(CFG.SAVE_DIR / \"model_artifacts.pkl\", \"wb\") as f:\n    pickle.dump(artifacts, f)\n\nprint(f\"\\nModels and artifacts saved to {CFG.SAVE_DIR / 'model_artifacts.pkl'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T13:09:27.079533Z","iopub.execute_input":"2025-10-08T13:09:27.079753Z","iopub.status.idle":"2025-10-08T13:37:49.392625Z","shell.execute_reply.started":"2025-10-08T13:09:27.079736Z","shell.execute_reply":"2025-10-08T13:37:49.391789Z"}},"outputs":[],"execution_count":null},{"id":"66233ec6-5029-4db4-b9f1-8b76d457e059","cell_type":"markdown","source":"6. Inference and Submission\n<div style=\"background-color: #f0f8ff; color: #333; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0px; border-radius: 5px;\">\n<p>The final step is to generate predictions on the test set. The process mirrors our training data preparation exactly:</p>\n<ol>\n<li>Load the test input data.</li>\n<li>Apply the <b>exact same</b> feature engineering pipeline (physics, sequential, GNN-lite).</li>\n<li>Create the test rows by merging the last observed state with the submission template.</li>\n<li>Calculate the physics baseline for the test set.</li>\n<li>Predict the residual_x and residual_y for each test row using our ensemble of trained models (averaging predictions across all folds).</li>\n<li>Add the predicted residuals to the baseline to get the final coordinates.</li>\n<li>Format the results into submission.csv.</li>\n</ol>\n<p>It is absolutely critical that the feature engineering process is identical between training and testing to avoid any data skew.</p>\n</div>","metadata":{}},{"id":"e0147c1c-7a1a-4b9b-b93e-fa80d7e28344","cell_type":"code","source":"print(\"Starting inference process...\")\n\n# --- Load Test Data ---\ntest_input_df = pd.read_csv(CFG.BASE_DIR / \"test_input.csv\")\nsubmission_template_df = pd.read_csv(CFG.BASE_DIR / \"test.csv\")\n\n# --- Apply Feature Engineering Pipeline ---\nprint(\"Applying feature engineering to test data...\")\ntest_input_df = add_physics_features(test_input_df)\ntest_input_df = add_sequential_features(test_input_df)\ngnn_test_features = compute_neighbor_embeddings(test_input_df, CFG)\n\n# --- Assemble Test DataFrame ---\nprint(\"Assembling test dataframe...\")\nlast_observed_test = (\n    test_input_df.sort_values(\"frame_id\")\n             .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n             .tail(1)\n             .rename(columns={\"frame_id\": \"last_frame_id\"})\n)\n\ntest_df = submission_template_df.merge(last_observed_test, on=[\"game_id\", \"play_id\", \"nfl_id\"], how=\"left\")\ntest_df = test_df.merge(gnn_test_features, on=[\"game_id\", \"play_id\", \"nfl_id\"], how=\"left\")\n\n# --- Calculate Baseline and Features ---\ntest_df[\"delta_frames\"] = test_df[\"frame_id\"] - test_df[\"last_frame_id\"]\ntest_df[\"delta_t\"] = test_df[\"delta_frames\"] / 10.0\n\n# Ensure all feature columns from training exist in the test set\nfor col in features:\n    if col not in test_df.columns:\n        test_df[col] = 0.0\n\n# Clean and order columns\ntest_df[features] = test_df[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\nX_test = test_df[features].values.astype(np.float32)\n\n# Calculate physics baseline for test set\ntest_baseline_x = (test_df[\"x\"] + test_df[\"velocity_x\"] * test_df[\"delta_t\"]).values\ntest_baseline_y = (test_df[\"y\"] + test_df[\"velocity_y\"] * test_df[\"delta_t\"]).values\n\n# --- Predict Residuals (Ensemble) ---\nprint(\"Predicting residuals with model ensemble...\")\npreds_rx = np.mean([model.predict(X_test) for model in models_x], axis=0)\npreds_ry = np.mean([model.predict(X_test) for model in models_y], axis=0)\n\n# --- Final Prediction ---\nprint(\"Calculating final coordinates...\")\nfinal_x = np.clip(test_baseline_x + preds_rx, 0.0, 120.0)\nfinal_y = np.clip(test_baseline_y + preds_ry, 0.0, 53.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T13:45:51.674688Z","iopub.execute_input":"2025-10-08T13:45:51.674983Z","iopub.status.idle":"2025-10-08T13:45:59.47401Z","shell.execute_reply.started":"2025-10-08T13:45:51.674963Z","shell.execute_reply":"2025-10-08T13:45:59.473214Z"}},"outputs":[],"execution_count":null},{"id":"a326b417-352d-43d9-9ec9-ee8f6b08ea00","cell_type":"code","source":"print(submission_template_df.columns.tolist())\n\nsubmission_df = pd.DataFrame({\n    \"id\": (\n        submission_template_df[\"game_id\"].astype(str) + \"_\" +\n        submission_template_df[\"play_id\"].astype(str) + \"_\" +\n        submission_template_df[\"nfl_id\"].astype(str) + \"_\" +\n        submission_template_df[\"frame_id\"].astype(str)\n    ),\n    \"x\": final_x,\n    \"y\": final_y\n})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T13:47:09.079989Z","iopub.execute_input":"2025-10-08T13:47:09.08062Z","iopub.status.idle":"2025-10-08T13:47:09.125229Z","shell.execute_reply.started":"2025-10-08T13:47:09.080596Z","shell.execute_reply":"2025-10-08T13:47:09.124651Z"}},"outputs":[],"execution_count":null},{"id":"3b12956c-052e-4c84-9232-c76295c7fc23","cell_type":"markdown","source":"<div style=\"background-color: #111827; color: #F3F4F6; border-left: 10px solid #3B82F6; padding: 25px; margin: 20px 0px; border-radius: 8px; box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2); position: relative;\">\n  <h2 style=\"color: #60A5FA; border-bottom: 2px solid #3B82F6; padding-bottom: 10px;\">Summary of Our Approach</h2>\n  <p>In this notebook, we developed a robust, end-to-end pipeline for the NFL player trajectory prediction task. Our strategy, centered on <b>residual prediction on top of a physics baseline</b>, allowed a powerful model like CatBoost to focus on learning the complex, non-linear interactions and deviations from simple motion.</p>\n  <p>The key components were:</p>\n  <ul>\n    <li><b>Meticulous Feature Engineering:</b> We created a rich feature set covering player physics, target geometry, movement history, and, crucially, local player interactions via a <b>GNN-lite embedding</b>.</li>\n    <li><b>Strong Validation:</b> Using <code>GroupKFold</code> prevented data leakage and gave us a more reliable estimate of our model's true performance.</li>\n    <li><b>Efficient Modeling:</b> CatBoost provided excellent performance with its handling of numerical features and robust implementation, accelerated by GPU usage.</li>\n  </ul>\n\n  <h3 style=\"color: #60A5FA; margin-top: 20px;\">Potential Improvements & Next Steps</h3>\n  <p>While this is a strong baseline, there are many avenues for improvement:</p>\n  <ul>\n    <li><b>More Sophisticated GNNs:</b> Our GNN-lite is effective, but a full graph convolutional network could learn more complex interaction patterns by passing information between players over multiple \"hops.\"</li>\n    <li><b>Different Model Architectures:</b> Transformer-based models, which excel at sequence modeling, could be adapted to treat a player's trajectory as a sequence and potentially capture long-range dependencies better than lags/rolling windows. LSTMs or GRUs are also classic choices for time-series data.</li>\n    <li><b>Advanced Target Engineering:</b> Instead of predicting the residual (dx, dy), one could try predicting changes in velocity and acceleration, then integrate them forward in time.</li>\n    <li><b>Hyperparameter Tuning:</b> A systematic search for the optimal parameters for CatBoost (e.g., using Optuna or Hyperopt) could yield further performance gains.</li>\n    <li><b>Play-level Features:</b> Engineering features that describe the overall state of the play (e.g., time since snap, down, distance to go, type of pass play) could provide valuable context to the model.</li>\n  </ul>\n\n  <p>Thank you for following along! I hope this detailed walkthrough provides a solid foundation for your own experiments and success in the competition.</p>\n\n  <p style=\"position: absolute; bottom: 10px; right: 20px; color: #9CA3AF; font-size: 14px; font-style: italic;\">Created by Ozan M.</p>\n</div>","metadata":{}}]}