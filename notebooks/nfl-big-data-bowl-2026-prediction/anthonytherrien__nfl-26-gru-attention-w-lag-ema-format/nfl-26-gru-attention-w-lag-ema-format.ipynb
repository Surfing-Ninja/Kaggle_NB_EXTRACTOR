{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nIMPROVEMENTS ADDED:\n1. Proper feature engineering (matching baseline)\n2. Correct architecture (matching what worked)\n3. Better validation\n4. Lag features (proven improvement)\n\"\"\"\n\nimport os\nimport torch\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\n\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom datetime import datetime\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import TensorDataset, DataLoader\n\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"e9fafd72-cbfb-47df-b43a-f12528b43dcd","_cell_guid":"453381dd-67e3-43a3-b2dd-e1c1bb3e7df7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CONFIGURATION\n# ============================================================================\nclass Config:\n    # Set data directory path\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n\n    # Define random seed\n    SEED = 42\n\n    # Define model training hyperparameters\n    BATCH_SIZE = 224\n    EPOCHS = 100\n    LEARNING_RATE = 1e-3\n\n    # Define window and model architecture settings\n    WINDOW_SIZE = 8\n    HIDDEN_DIM = 160\n    MAX_FUTURE_HORIZON = 94\n\n    # Define field limits\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n\n    # Define compute device\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"14aa284d-58ed-4d30-a512-292387de0528","_cell_guid":"5fcb473c-82b5-4810-8e48-38bfae0bcd9c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SEEDING\n# ============================================================================\ndef set_seed(seed=42):\n    # Import random library\n    import random\n\n    # Set seeds for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n# Set the global seed\nset_seed(Config.SEED)","metadata":{"_uuid":"f5998943-368a-4c6f-b23f-7b3d0e82256e","_cell_guid":"4d6cf58d-0423-4264-879f-12aff561e434","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# FEATURE ENGINEERING\n# ============================================================================\n# Define function to convert height string (e.g., \"6-2\") to feet\ndef height_to_feet(height_str):\n    try:\n        # Split height into feet and inches, convert to integers\n        ft, inches = map(int, str(height_str).split('-'))\n\n        # Convert to feet\n        return ft + inches / 12\n\n    except:\n        # Return default height if conversion fails\n        return 6.0\n\n\n# Define function to prepare sequential input data for model training or inference\ndef prepare_sequences_fixed(input_df, output_df=None, test_template=None, is_training=True, window_size=8):\n    # Print process status\n    print(f\"Preparing sequences (window_size={window_size})...\")\n\n    # Copy input dataframe to avoid modifying original\n    input_df = input_df.copy()\n\n    # Convert player height from string to numeric feet\n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n\n    # Convert directional angles (degrees) to radians\n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n\n    # Define time delta between frames\n    delta_t = 0.1\n\n    # Compute horizontal velocity components\n    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n\n    # Compute horizontal acceleration components\n    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n\n    # Encode player side and role as binary indicators\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n\n    # Compute physical quantities using player mass\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n\n    # Compute ball-related spatial features if ball coordinates exist\n    if 'ball_land_x' in input_df.columns:\n        # Compute displacement between player and ball\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n\n        # Compute Euclidean distance to ball\n        input_df['distance_to_ball'] = np.sqrt(ball_dx ** 2 + ball_dy ** 2)\n\n        # Compute angular direction toward ball\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n\n        # Normalize direction vector from player to ball\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n\n        # Compute relative closing speed between player and ball\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n\n    # Sort data chronologically by identifiers\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n\n    # Define group columns for player-level operations\n    gcols = ['game_id', 'play_id', 'nfl_id']\n\n    # Add lag features for recent movement history\n    for lag in [1, 2, 3]:\n        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n\n    # Add exponential moving average (EMA) features for smoothed motion\n    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n\n    # Add rolling mean features for local motion stability\n    input_df['velocity_x_roll'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.rolling(window_size, min_periods=1).mean()\n    )\n    input_df['velocity_y_roll'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.rolling(window_size, min_periods=1).mean()\n    )\n\n    # Define final list of model input features\n    feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n        'player_height_feet', 'player_weight',\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y',\n        'closing_speed',\n        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n        'velocity_x_roll', 'velocity_y_roll',\n    ]\n\n    # Filter to only existing columns\n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n\n    # Print feature count for confirmation\n    print(f\"Using {len(feature_cols)} features (baseline + lag + EMA)\")\n\n    # Set index for group-based retrieval\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n\n    # Define target data source depending on mode (training or inference)\n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n\n    # Initialize storage containers for sequences and labels\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n\n    # Iterate through each target player group\n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups)):\n        # Extract key identifiers\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n\n        try:\n            # Retrieve player group data\n            group_df = grouped.get_group(key)\n        except KeyError:\n            # Skip if group not found\n            continue\n\n        # Extract most recent frames for sequence\n        input_window = group_df.tail(window_size)\n\n        # Pad sequence if not enough frames available\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n\n        # Fill missing values using group mean\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n\n        # Extract numerical feature array\n        seq = input_window[feature_cols].values\n\n        # Handle remaining NaNs\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n\n        # Append prepared sequence\n        sequences.append(seq)\n\n        # Prepare target displacement values (dx, dy) for supervised training\n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id'] == row['game_id']) &\n                (output_df['play_id'] == row['play_id']) &\n                (output_df['nfl_id'] == row['nfl_id'])\n            ].sort_values('frame_id')\n\n            # Compute displacement relative to last observed frame\n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n\n            # Store targets\n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n\n        # Store metadata for each sequence\n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id']\n        })\n\n    # Print total number of created sequences\n    print(f\"Created {len(sequences)} sequences\")\n\n    # Return different outputs depending on mode\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n\n    return sequences, sequence_ids","metadata":{"_uuid":"cf5036d9-5151-4c51-b2aa-25508dadcc2e","_cell_guid":"5bc7e2c1-09c8-4c4f-b734-77f0781ff678","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# LOSS FUNCTION\n# ============================================================================\n# Define custom temporal Huber loss with time-based decay weighting\nclass TemporalHuber(nn.Module):\n    # Initialize with Huber delta and temporal decay rate\n    def __init__(self, delta=0.5, time_decay=0.03):\n        super().__init__()\n\n        # Store the Huber delta parameter (controls transition between L1 and L2 regions)\n        self.delta = delta\n\n        # Store exponential time decay parameter for temporal weighting\n        self.time_decay = time_decay\n\n    # Define forward pass to compute loss\n    def forward(self, pred, target, mask):\n        # Compute prediction error\n        err = pred - target\n\n        # Compute absolute error\n        abs_err = torch.abs(err)\n\n        # Compute elementwise Huber loss\n        huber = torch.where(\n            abs_err <= self.delta,\n            0.5 * err * err,\n            self.delta * (abs_err - 0.5 * self.delta)\n        )\n\n        # Apply exponential time decay weighting if enabled\n        if self.time_decay > 0:\n            # Determine sequence length (temporal dimension)\n            L = pred.size(1)\n\n            # Create time index tensor\n            t = torch.arange(L, device=pred.device).float()\n\n            # Compute exponential decay weights for each timestep\n            weight = torch.exp(-self.time_decay * t).view(1, L)\n\n            # Apply temporal weighting to Huber loss and mask\n            huber = huber * weight\n            mask = mask * weight\n\n        # Compute mean weighted loss across valid positions\n        return (huber * mask).sum() / (mask.sum() + 1e-8)","metadata":{"_uuid":"3f302b94-af9c-4e4e-b638-85701004de8f","_cell_guid":"15541f4b-d10d-4a12-88eb-d5dfbe9899d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MODEL ARCHITECTURE\n# ============================================================================\n# Define sequence prediction model with GRU backbone and attention-based pooling\nclass ImprovedSeqModel(nn.Module):\n    # Initialize model layers and parameters\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n\n        # Store prediction horizon (number of future timesteps to predict)\n        self.horizon = horizon\n\n        # Define GRU encoder with two layers and dropout for temporal feature extraction\n        self.gru = nn.GRU(\n            input_size=input_dim,\n            hidden_size=128,\n            num_layers=2,\n            batch_first=True,\n            dropout=0.1\n        )\n\n        # Define layer normalization for post-GRU feature stabilization\n        self.pool_ln = nn.LayerNorm(128)\n\n        # Define multi-head attention for sequence-level context pooling\n        self.pool_attn = nn.MultiheadAttention(\n            embed_dim=128,\n            num_heads=4,\n            batch_first=True\n        )\n\n        # Define learnable query vector for attention-based pooling\n        self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n\n        # Define prediction head to map pooled context to output trajectory deltas\n        self.head = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, horizon)\n        )\n\n    # Define forward computation\n    def forward(self, x):\n        # Pass input sequence through GRU to extract temporal features\n        h, _ = self.gru(x)\n\n        # Get batch size\n        B = h.size(0)\n\n        # Expand query vector across the batch for attention pooling\n        q = self.pool_query.expand(B, -1, -1)\n\n        # Normalize GRU output before applying attention\n        h_norm = self.pool_ln(h)\n\n        # Apply multi-head attention using learned query\n        ctx, _ = self.pool_attn(q, h_norm, h_norm)\n\n        # Remove sequence dimension (collapse pooled vector)\n        ctx = ctx.squeeze(1)\n\n        # Pass pooled context through output head to predict positional deltas\n        out = self.head(ctx)\n\n        # Convert predicted deltas to cumulative displacements across horizon\n        out = torch.cumsum(out, dim=1)\n\n        # Return final trajectory predictions\n        return out","metadata":{"_uuid":"70e105c9-36f3-4200-b60a-c31a83dc4ebc","_cell_guid":"3c9012c8-a563-4dfe-8edf-3fe1027609cc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TARGET PREPARATION\n# ============================================================================\n# Define function to prepare padded target tensors and masks for variable-length sequences\ndef prepare_targets(batch_axis, max_h):\n    # Initialize containers for target tensors and corresponding masks\n    tensors, masks = [], []\n\n    # Iterate through each array in the batch\n    for arr in batch_axis:\n        # Get current sequence length\n        L = len(arr)\n\n        # Pad sequence with zeros to match maximum horizon length\n        padded = np.pad(\n            arr,\n            (0, max_h - L),\n            constant_values=0\n        ).astype(np.float32)\n\n        # Create binary mask indicating valid (non-padded) positions\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n\n        # Convert padded array and mask to PyTorch tensors\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n\n    # Stack all padded tensors and masks into batch tensors\n    return torch.stack(tensors), torch.stack(masks)","metadata":{"_uuid":"3e3e195a-a9b3-43bc-ab31-e465599c8bec","_cell_guid":"6d581f0d-06b6-426b-a290-91c9de37b07a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TRAINING LOOP (NO VALIDATION)\n# ============================================================================\n# Define function to train the model using temporal sequence data\ndef train_model(X_train, y_train, input_dim, horizon, config):\n    # Set computation device (CPU or GPU)\n    device = config.DEVICE\n\n    # Initialize model and move it to the target device\n    model = ImprovedSeqModel(input_dim, horizon).to(device)\n\n    # Initialize loss function with Huber loss and temporal decay\n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n\n    # Initialize optimizer (AdamW) with learning rate and weight decay\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config.LEARNING_RATE,\n        weight_decay=1e-5\n    )\n\n    # Prepare training batches with targets and masks\n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        by, bm = prepare_targets([y_train[j] for j in range(i, end)], horizon)\n        train_batches.append((bx, by, bm))\n\n    # Begin training loop\n    for epoch in range(1, config.EPOCHS + 1):\n        # Set model to training mode\n        model.train()\n        train_losses = []\n\n        # Iterate through all training batches\n        for bx, by, bm in train_batches:\n            # Move batch tensors to device\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n\n            # Forward pass through the model\n            pred = model(bx)\n\n            # Compute loss\n            loss = criterion(pred, by, bm)\n\n            # Reset gradients\n            optimizer.zero_grad()\n\n            # Backpropagate loss\n            loss.backward()\n\n            # Clip gradients to prevent instability\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update model parameters\n            optimizer.step()\n\n            # Store batch loss\n            train_losses.append(loss.item())\n\n        # Compute mean training loss for this epoch\n        train_loss = np.mean(train_losses)\n\n        # Print progress every 10 epochs\n        if epoch % 10 == 0 or epoch == 1 or epoch == config.EPOCHS:\n            print(f\"  Epoch {epoch:03d}: train_loss={train_loss:.4f}\")\n\n    # Return trained model and final training loss\n    return model, train_loss","metadata":{"_uuid":"a83b97f5-9327-4f45-8088-6abcd724bbb0","_cell_guid":"44716cd3-ba9b-4251-b035-e1a086e6f34e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n# Define main function to run the complete NFL player trajectory modeling pipeline\ndef main():\n    # Initialize configuration object\n    config = Config()\n\n    # Print header for visual clarity\n    print(\"=\" * 80)\n    print(\"NFL DEBUGGED PIPELINE (No CV / No Validation)\")\n    print(\"=\" * 80)\n\n    # ------------------------------------------------------------------------\n    # STEP 1: LOAD DATASETS\n    # ------------------------------------------------------------------------\n    print(\"\\n[1/3] Loading...\")\n\n    # Build lists of weekly training input and output files\n    train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n\n    # Concatenate all existing weekly training files\n    train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n\n    # Load test input and submission template\n    test_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(config.DATA_DIR / \"test.csv\")\n\n    # ------------------------------------------------------------------------\n    # STEP 2: FEATURE ENGINEERING & SEQUENCE PREPARATION\n    # ------------------------------------------------------------------------\n    print(\"\\n[2/3] Preparing (with lag + EMA features)...\")\n\n    # Generate sequential training data with engineered features\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = prepare_sequences_fixed(\n        train_input,\n        train_output,\n        is_training=True,\n        window_size=config.WINDOW_SIZE\n    )\n\n    # Convert lists to NumPy object arrays\n    sequences = np.array(sequences, dtype=object)\n    targets_dx = np.array(targets_dx, dtype=object)\n    targets_dy = np.array(targets_dy, dtype=object)\n\n    # ------------------------------------------------------------------------\n    # STEP 3: MODEL TRAINING (FULL DATA, NO VALIDATION)\n    # ------------------------------------------------------------------------\n    print(\"\\n[3/3] Training...\")\n\n    # Use all data for training\n    X_train = sequences\n    y_train_dx = targets_dx\n    y_train_dy = targets_dy\n\n    # Fit StandardScaler using all sequences\n    scaler = StandardScaler()\n    scaler.fit(np.vstack([s for s in X_train]))\n\n    # Apply feature scaling\n    X_train_scaled = np.stack([scaler.transform(s) for s in X_train])\n\n    # Train model for X-coordinate prediction\n    print(\"Training X...\")\n    model_x, _ = train_model(\n        X_train_scaled,\n        y_train_dx,\n        X_train[0].shape[-1],\n        config.MAX_FUTURE_HORIZON,\n        config\n    )\n\n    # Train model for Y-coordinate prediction\n    print(\"Training Y...\")\n    model_y, _ = train_model(\n        X_train_scaled,\n        y_train_dy,\n        X_train[0].shape[-1],\n        config.MAX_FUTURE_HORIZON,\n        config\n    )\n\n    # Store single trained models and scaler for prediction\n    models_x = [model_x]\n    models_y = [model_y]\n    scalers = [scaler]\n\n    # ------------------------------------------------------------------------\n    # STEP 4: PREDICTION ON TEST DATA\n    # ------------------------------------------------------------------------\n    print(\"\\n[4/4] Predicting...\")\n\n    # Prepare test sequences using the same feature logic\n    test_sequences, test_ids = prepare_sequences_fixed(\n        test_input,\n        test_template=test_template,\n        is_training=False,\n        window_size=config.WINDOW_SIZE\n    )\n\n    # Convert to NumPy array format\n    X_test = np.array(test_sequences, dtype=object)\n\n    # Extract last observed player coordinates\n    x_last = np.array([s[-1, 0] for s in X_test])\n    y_last = np.array([s[-1, 1] for s in X_test])\n\n    # Apply feature scaling to test sequences\n    X_scaled = np.stack([scalers[0].transform(s) for s in X_test])\n    X_tensor = torch.tensor(X_scaled.astype(np.float32)).to(config.DEVICE)\n\n    # Run inference\n    model_x.eval()\n    model_y.eval()\n\n    with torch.no_grad():\n        dx = model_x(X_tensor).cpu().numpy()\n        dy = model_y(X_tensor).cpu().numpy()\n\n    # Compute ensemble mean (single model = same)\n    ens_dx = dx\n    ens_dy = dy\n\n    # Build submission\n    rows = []\n    H = ens_dx.shape[1]\n\n    for i, sid in enumerate(test_ids):\n        fids = test_template[\n            (test_template['game_id'] == sid['game_id']) &\n            (test_template['play_id'] == sid['play_id']) &\n            (test_template['nfl_id'] == sid['nfl_id'])\n        ]['frame_id'].sort_values().tolist()\n\n        for t, fid in enumerate(fids):\n            tt = min(t, H - 1)\n            px = np.clip(x_last[i] + ens_dx[i, tt], 0, 120)\n            py = np.clip(y_last[i] + ens_dy[i, tt], 0, 53.3)\n            rows.append({\n                'id': f\"{sid['game_id']}_{sid['play_id']}_{sid['nfl_id']}_{fid}\",\n                'x': px,\n                'y': py\n            })\n\n    # Create and save submission file\n    submission = pd.DataFrame(rows)\n    submission.to_csv(\"submission.csv\", index=False)\n\n    print(f\"\\nâœ“ Saved submission.csv\")\n    print(f\"  Rows: {len(submission)}\")\n    print(f\"  Improvements: lag features + EMA + full-data training\")\n    print(f\"  Expected: ~0.60 RMSE (no validation mode)\")\n\n    return submission","metadata":{"_uuid":"a589b2c0-7b21-419a-a6fd-0dded37810aa","_cell_guid":"9039f464-f6f2-4540-a778-799e5defce4d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# ENTRY POINT\n# ============================================================================\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"62240fdb-e347-4b52-8867-5265ad09b703","_cell_guid":"e1f92f2a-acf8-456f-aa7e-7b03af1df7ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}