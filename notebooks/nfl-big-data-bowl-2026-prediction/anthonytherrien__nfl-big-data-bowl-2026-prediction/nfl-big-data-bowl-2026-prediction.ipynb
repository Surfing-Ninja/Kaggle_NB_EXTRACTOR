{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================================================\n# NFL BIG DATA BOWL 2026 - IMPROVED SOLUTION\n# Enhanced player movement prediction with advanced temporal features and physics\n# ================================================================================\n\n# Imports\nimport os\nimport gc\nimport math\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.auto import tqdm\nfrom scipy.ndimage import gaussian_filter1d\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# CONFIGURATION\n# ================================================================================\n\nclass Config:\n    # Data paths\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n\n    # Random seeds\n    SEED = 42\n    SEEDS = [42, 123, 2024]\n\n    # Cross-validation\n    N_FOLDS = 5\n\n    # Field geometry\n    FIELD_X_MIN = 0.0\n    FIELD_X_MAX = 120.0\n    FIELD_Y_MIN = 0.0\n    FIELD_Y_MAX = 53.3\n\n    # Physical constraints\n    MAX_SPEED = 12.0\n\n    # Neural net training\n    NN_BATCH_SIZE = 2048\n    NN_EPOCHS = 100\n    NN_LEARNING_RATE = 5e-4\n    NN_PATIENCE = 10\n\n    # GPU preferences\n    USE_GPU = True\n    GPU_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    GPU_COUNT = torch.cuda.device_count() if torch.cuda.is_available() else 0\n\n    # Early stopping for tree models\n    ES_ROUNDS = 200\n\n    # Verbosity for third-party libs\n    VERBOSE = False\n\n    # Weeks expected\n    WEEKS = list(range(1, 18))\n\n    # File names\n    TEST_INPUT = \"test_input.csv\"\n    TEST_TEMPLATE = \"test.csv\"\n\n    @classmethod\n    def print_gpu_info(cls):\n        # Header\n        print(\"=\" * 60)\n        print(\"GPU CONFIGURATION\")\n        print(\"=\" * 60)\n\n        # CUDA availability\n        print(f\"CUDA Available: {torch.cuda.is_available()}\")\n\n        # Device and count\n        print(f\"GPU Device: {cls.GPU_DEVICE}\")\n        print(f\"GPU Count: {cls.GPU_COUNT}\")\n\n        # Detailed GPU listing\n        if torch.cuda.is_available():\n            for i in range(cls.GPU_COUNT):\n                name = torch.cuda.get_device_name(i)\n                mem_gb = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f\"GPU {i}: {name} ({mem_gb:.1f} GB)\")\n        else:\n            print(\"No GPU available - using CPU\")\n\n        # Footer\n        print(\"=\" * 60)\n\n    @classmethod\n    def cleanup_gpu_memory(cls):\n        # Clear CUDA cache\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n\n        # Run GC\n        gc.collect()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# UTILS\n# ================================================================================\n\ndef set_all_seeds(seed):\n    # Python RNG\n    random.seed(seed)\n\n    # Numpy RNG\n    np.random.seed(seed)\n\n    # Torch RNG\n    torch.manual_seed(seed)\n\n    # CUDA RNG\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef check_gpu_requirements():\n    # CUDA availability\n    gpu_available = torch.cuda.is_available()\n\n    # CUDA status\n    if gpu_available:\n        print(\"âœ… CUDA is available\")\n    else:\n        print(\"âš ï¸ CUDA not available - will use CPU for all models\")\n\n    # XGBoost availability\n    try:\n        import xgboost  # noqa\n        print(\"âœ… XGBoost installed\")\n    except Exception:\n        print(\"âš ï¸ XGBoost not found - install with: pip install xgboost\")\n\n    # LightGBM availability\n    try:\n        import lightgbm  # noqa\n        print(\"âœ… LightGBM installed\")\n    except Exception:\n        print(\"âš ï¸ LightGBM not found - install with: pip install lightgbm\")\n\n    # CatBoost availability\n    try:\n        import catboost  # noqa\n        print(\"âœ… CatBoost installed\")\n    except Exception:\n        print(\"âš ï¸ CatBoost not found - install with: pip install catboost\")\n\n    # Return status\n    return gpu_available\n\n\ndef to_inches(height_str):\n    # Handle non-string\n    if not isinstance(height_str, str):\n        return 70\n\n    # Handle malformed\n    if '-' not in height_str:\n        return 70\n\n    # Convert ft-in to inches\n    try:\n        feet, inches = map(int, height_str.split('-'))\n        return feet * 12 + inches\n    except Exception:\n        return 70\n\n\ndef safe_cut(series, bins, labels=False):\n    # Robust pd.cut with NaN handling\n    try:\n        return pd.cut(series, bins=bins, labels=labels)\n    except Exception:\n        return pd.Series([np.nan] * len(series), index=series.index)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# SIMPLE NEURAL NETWORK\n# ================================================================================\n\nclass SimpleNN(nn.Module):\n    # Constructor\n    def __init__(self, input_dim):\n        super().__init__()\n\n        # Layers\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.30),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.20),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n        )\n\n    # Forward pass\n    def forward(self, x):\n        return self.layers(x)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# ENCODER WITH UNSEEN HANDLING\n# ================================================================================\n\nclass SafeLabelEncoder:\n    # Constructor\n    def __init__(self):\n        self.classes_ = {}\n        self.fitted = False\n\n    # Fit on training column\n    def fit(self, series):\n        # Fill NaNs\n        ser = series.fillna('Unknown').astype(str)\n\n        # Build classes\n        uniq = sorted(ser.unique().tolist())\n\n        # Ensure Unknown at index 0\n        if 'Unknown' not in uniq:\n            uniq = ['Unknown'] + uniq\n\n        # Map\n        self.classes_ = {c: i for i, c in enumerate(uniq)}\n\n        # Flag\n        self.fitted = True\n\n        # Return self\n        return self\n\n    # Transform any column\n    def transform(self, series):\n        # Precondition\n        if not self.fitted:\n            raise RuntimeError(\"SafeLabelEncoder not fitted\")\n\n        # Fill NaNs\n        ser = series.fillna('Unknown').astype(str)\n\n        # Map with Unknown for unseen\n        return ser.map(lambda x: self.classes_.get(x, self.classes_['Unknown'])).astype(int)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# MAIN PIPELINE\n# ================================================================================\n\nclass EnhancedNFLPlayerMovementPredictor:\n    # Constructor\n    def __init__(self, data_dir, seed=42):\n        # Paths and seed\n        self.data_dir = Path(data_dir)\n        self.seed = seed\n\n        # CV holders\n        self.models_dx = {'xgb': [], 'lgb': [], 'cat': []}\n        self.models_dy = {'xgb': [], 'lgb': [], 'cat': []}\n        self.nn_models_dx = []\n        self.nn_models_dy = []\n\n        # Scalers and encoders\n        self.num_scaler = StandardScaler()\n        self.encoders = {}\n\n        # Feature lists\n        self.numerical_features = []\n        self.categorical_features = []\n\n        # Dataframes\n        self.train_data = None\n        self.test_data = None\n\n    # Load many CSVs\n    def _load_many(self, paths):\n        # Container\n        frames = []\n\n        # Loop\n        for p in tqdm(paths, desc=\"Loading files\"):\n            try:\n                frames.append(pd.read_csv(p))\n            except Exception:\n                pass\n\n        # Concatenate\n        if len(frames) == 0:\n            return pd.DataFrame()\n\n        # Return concat\n        return pd.concat(frames, ignore_index=True)\n\n    # Load all datasets\n    def load_and_combine_datasets(self):\n        # Compose input paths\n        inputs = [self.data_dir / f\"train/input_2023_w{w:02d}.csv\" for w in Config.WEEKS]\n\n        # Compose output paths\n        outputs = [self.data_dir / f\"train/output_2023_w{w:02d}.csv\" for w in Config.WEEKS]\n\n        # Filter existence\n        inputs = [p for p in inputs if p.exists()]\n        outputs = [p for p in outputs if p.exists()]\n\n        # Logs\n        print(f\"Found {len(inputs)} weeks of input and {len(outputs)} weeks of output\")\n\n        # Read train input/output\n        train_input = self._load_many(inputs)\n        train_output = self._load_many(outputs)\n\n        # Read test input/template\n        test_input = pd.read_csv(self.data_dir / Config.TEST_INPUT)\n        test_template = pd.read_csv(self.data_dir / Config.TEST_TEMPLATE)\n\n        # Log sizes\n        print(f\"Loaded train_input: {len(train_input):,} rows\")\n        print(f\"Loaded train_output: {len(train_output):,} rows\")\n        print(f\"Loaded test_input:  {len(test_input):,} rows\")\n        print(f\"Loaded test_template: {len(test_template):,} rows\")\n\n        # Return\n        return train_input, train_output, test_input, test_template\n\n    # Temporal features from tracking\n    def _extract_temporal_features(self, tracking):\n        # Sort and take last frame\n        last = tracking.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']).groupby(\n            ['game_id', 'play_id', 'nfl_id'], as_index=False\n        ).last()\n\n        # Rename last frame columns\n        last = last.rename(columns={'x': 'final_pre_throw_x', 'y': 'final_pre_throw_y'})\n\n        # Aggregate statistics\n        agg = tracking.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n            'x': ['mean', 'std', 'min', 'max'],\n            'y': ['mean', 'std', 'min', 'max'],\n            's': ['mean', 'std', 'max', 'min'],\n            'a': ['mean', 'std', 'max', 'min'],\n            'dir': lambda x: np.std(np.diff(x)) if len(x) > 1 else 0,\n            'o':   lambda x: np.std(np.diff(x)) if len(x) > 1 else 0,\n        }).reset_index()\n\n        # Flatten columns\n        agg.columns = ['_'.join(c).strip('_') for c in agg.columns.to_flat_index()]\n\n        # Rename lambdas\n        agg = agg.rename(columns={\n            'dir_<lambda_0>': 'dir_change_rate',\n            'o_<lambda_0>': 'orientation_change_rate'\n        })\n\n        # Last N frames\n        N = 5\n\n        # Take recent frames\n        recent = tracking.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']).groupby(\n            ['game_id', 'play_id', 'nfl_id']\n        ).tail(N)\n\n        # Trajectory deltas\n        traj = recent.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n            'x': lambda x: (x.iloc[-1] - x.iloc[0]) if len(x) > 1 else 0,\n            'y': lambda x: (x.iloc[-1] - x.iloc[0]) if len(x) > 1 else 0,\n            's': lambda x: x.diff().mean() if len(x) > 1 else 0,\n        }).reset_index()\n\n        # Rename trajectory columns\n        traj.columns = ['game_id', 'play_id', 'nfl_id',\n                        'recent_displacement_x', 'recent_displacement_y', 'acceleration_trend']\n\n        # Merge last + agg + traj\n        out = last.merge(agg, on=['game_id', 'play_id', 'nfl_id'], how='left')\n        out = out.merge(traj, on=['game_id', 'play_id', 'nfl_id'], how='left')\n\n        # Convert height if present\n        if 'player_height' in out.columns:\n            out['height_inches'] = out['player_height'].apply(to_inches)\n\n        # Return features\n        return out\n\n    # Merge targeted receiver coordinates\n    def _incorporate_target_receiver_data(self, df):\n        # Column guard\n        if 'player_role' not in df.columns:\n            out = df.copy()\n            out['target_receiver_x'] = np.nan\n            out['target_receiver_y'] = np.nan\n            return out\n\n        # Extract target receiver per play\n        trg = df[df['player_role'] == \"Targeted Receiver\"][\n            ['game_id', 'play_id', 'final_pre_throw_x', 'final_pre_throw_y']\n        ].drop_duplicates(['game_id', 'play_id'])\n\n        # Rename target columns\n        trg = trg.rename(columns={\n            'final_pre_throw_x': 'target_receiver_x',\n            'final_pre_throw_y': 'target_receiver_y'\n        })\n\n        # Merge back\n        return df.merge(trg, on=['game_id', 'play_id'], how='left')\n\n    # Advanced features\n    def _calculate_advanced_features(self, df_in, training_mode=False):\n        # Copy\n        df = df_in.copy()\n\n        # Time features\n        if 'frame_id' in df.columns:\n            df['time_seconds'] = df['frame_id'] / 10.0\n            df['time_normalized'] = df['frame_id'] / df.groupby(\n                ['game_id', 'play_id', 'nfl_id']\n            )['frame_id'].transform('max')\n            df['time_squared'] = df['time_seconds'] ** 2\n            df['time_cubed'] = df['time_seconds'] ** 3\n            df['sqrt_time'] = np.sqrt(np.clip(df['time_seconds'], 0, None))\n            df['log_time'] = np.log1p(np.clip(df['time_seconds'], 0, None))\n            df['time_sin'] = np.sin(2 * np.pi * df['time_normalized'])\n            df['time_cos'] = np.cos(2 * np.pi * df['time_normalized'])\n            df['time_sin_2'] = np.sin(4 * np.pi * df['time_normalized'])\n            df['time_cos_2'] = np.cos(4 * np.pi * df['time_normalized'])\n            df['is_early_play'] = (df['time_normalized'] < 0.33).astype(int)\n            df['is_mid_play'] = ((df['time_normalized'] >= 0.33) & (df['time_normalized'] < 0.67)).astype(int)\n            df['is_late_play'] = (df['time_normalized'] >= 0.67).astype(int)\n\n        # Velocity and physics\n        if {'s', 'dir'}.issubset(df.columns):\n            rad = np.deg2rad(df['dir'].fillna(0))\n            df['velocity_x'] = df['s'] * np.sin(rad)\n            df['velocity_y'] = df['s'] * np.cos(rad)\n\n        # Momentum\n        if {'player_weight', 's'}.issubset(df.columns):\n            df['momentum_magnitude'] = df['player_weight'] * df['s']\n\n        # Constant velocity expectation\n        if {'time_seconds', 'final_pre_throw_x', 'final_pre_throw_y', 'velocity_x', 'velocity_y'}.issubset(df.columns):\n            df['expected_x_constant_v'] = df['final_pre_throw_x'] + df['velocity_x'] * df['time_seconds']\n            df['expected_y_constant_v'] = df['final_pre_throw_y'] + df['velocity_y'] * df['time_seconds']\n\n        # With acceleration\n        if {'time_seconds', 'time_squared', 'a', 'dir', 'final_pre_throw_x', 'final_pre_throw_y',\n             'velocity_x', 'velocity_y'}.issubset(df.columns):\n            rad = np.deg2rad(df['dir'].fillna(0))\n            df['expected_x_with_accel'] = (\n                df['final_pre_throw_x'] + df['velocity_x'] * df['time_seconds'] + 0.5 * df['a'] * np.sin(rad) * df['time_squared']\n            )\n            df['expected_y_with_accel'] = (\n                df['final_pre_throw_y'] + df['velocity_y'] * df['time_seconds'] + 0.5 * df['a'] * np.cos(rad) * df['time_squared']\n            )\n\n        # Speed/accel consistency\n        if {'s', 's_mean'}.issubset(df.columns):\n            df['speed_consistency'] = df['s'] / (df['s_mean'] + 0.1)\n            df['speed_deviation'] = np.abs(df['s'] - df['s_mean'])\n\n        if {'a', 'a_mean'}.issubset(df.columns):\n            df['acceleration_consistency'] = df['a'] / (df['a_mean'] + 0.1)\n            df['acceleration_deviation'] = np.abs(df['a'] - df['a_mean'])\n\n        # Time interactions\n        if {'time_seconds', 's'}.issubset(df.columns):\n            df['time_x_speed'] = df['time_seconds'] * df['s']\n\n        if {'time_seconds', 'a'}.issubset(df.columns):\n            df['time_x_acceleration'] = df['time_seconds'] * df['a']\n\n        if {'time_squared', 's'}.issubset(df.columns):\n            df['time_squared_x_speed'] = df['time_squared'] * df['s']\n\n        # Ball trajectory features\n        if {'ball_land_x', 'ball_land_y', 'final_pre_throw_x', 'final_pre_throw_y'}.issubset(df.columns):\n            dx = df['ball_land_x'] - df['final_pre_throw_x']\n            dy = df['ball_land_y'] - df['final_pre_throw_y']\n            dist = np.sqrt(dx ** 2 + dy ** 2)\n            df['distance_to_ball_landing'] = dist\n            df['angle_to_ball_landing'] = np.arctan2(dy, dx)\n            df['ball_direction_x'] = dx / (dist + 1e-6)\n            df['ball_direction_y'] = dy / (dist + 1e-6)\n            df['estimated_time_to_ball'] = dist / 20.0\n            if 'time_seconds' in df.columns:\n                df['time_ratio_to_ball'] = df['time_seconds'] / (df['estimated_time_to_ball'] + 0.1)\n            if {'velocity_x', 'velocity_y'}.issubset(df.columns):\n                bux = dx / (dist + 1e-6)\n                buy = dy / (dist + 1e-6)\n                df['closing_speed'] = df['velocity_x'] * bux + df['velocity_y'] * buy\n                df['projected_time_to_ball'] = dist / (np.abs(df['closing_speed']) + 0.1)\n                if 'time_seconds' in df.columns:\n                    df['time_urgency'] = df['time_seconds'] / (df['projected_time_to_ball'] + 0.1)\n            if 'time_seconds' in df.columns:\n                df['distance_to_ball_x_time'] = dist * df['time_seconds']\n            if 'time_squared' in df.columns:\n                df['distance_to_ball_x_time_squared'] = dist * df['time_squared']\n\n        # Target receiver features\n        if {'target_receiver_x', 'target_receiver_y', 'final_pre_throw_x', 'final_pre_throw_y'}.issubset(df.columns):\n            tdx = df['target_receiver_x'] - df['final_pre_throw_x']\n            tdy = df['target_receiver_y'] - df['final_pre_throw_y']\n            tdist = np.sqrt(tdx ** 2 + tdy ** 2)\n            df['distance_to_target'] = tdist\n            df['angle_to_target'] = np.arctan2(tdy, tdx)\n            if 'time_seconds' in df.columns:\n                df['distance_to_target_x_time'] = tdist * df['time_seconds']\n\n        # Target indicator\n        if 'player_role' in df.columns:\n            df['is_target_receiver'] = (df['player_role'] == \"Targeted Receiver\").astype(int)\n        else:\n            df['is_target_receiver'] = 0\n\n        # Field position\n        if 'final_pre_throw_x' in df.columns:\n            df['normalized_x'] = df['final_pre_throw_x'] / Config.FIELD_X_MAX\n            df['field_region_x'] = safe_cut(df['final_pre_throw_x'], bins=6, labels=False)\n            df['distance_from_endzone'] = np.minimum(\n                df['final_pre_throw_x'], Config.FIELD_X_MAX - df['final_pre_throw_x']\n            )\n\n        if 'final_pre_throw_y' in df.columns:\n            df['normalized_y'] = df['final_pre_throw_y'] / Config.FIELD_Y_MAX\n            df['field_region_y'] = safe_cut(df['final_pre_throw_y'], bins=4, labels=False)\n            df['distance_from_sideline'] = np.minimum(\n                df['final_pre_throw_y'], Config.FIELD_Y_MAX - df['final_pre_throw_y']\n            )\n\n        # Game context\n        if 'absolute_yardline_number' in df.columns:\n            df['yards_to_endzone'] = df['absolute_yardline_number']\n            df['is_redzone'] = (df['absolute_yardline_number'] <= 20).astype(int)\n\n        # Team indicators\n        if 'player_side' in df.columns:\n            df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n        else:\n            df['is_offense'] = 0\n\n        if 'player_role' in df.columns:\n            df['is_passer'] = (df['player_role'] == 'Passer').astype(int)\n            df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n        else:\n            df['is_passer'] = 0\n            df['is_coverage'] = 0\n\n        # Player attributes\n        if {'player_weight', 'height_inches'}.issubset(df.columns):\n            ok_h = df['height_inches'] > 0\n            df['bmi'] = np.nan\n            df.loc[ok_h, 'bmi'] = (\n                (df.loc[ok_h, 'player_weight'] * 0.453592)\n                / ((df.loc[ok_h, 'height_inches'] * 0.0254) ** 2)\n            )\n\n        # Motion analysis\n        if {'dir', 'o'}.issubset(df.columns):\n            df['speed_orientation_discrepancy'] = np.abs(df['dir'] - df['o'])\n\n        # Interactions\n        if {'s', 'a'}.issubset(df.columns):\n            df['speed_times_acceleration'] = df['s'] * df['a']\n\n        if {'distance_to_ball_landing', 's'}.issubset(df.columns):\n            df['distance_speed_ratio'] = df['distance_to_ball_landing'] / (df['s'] + 1.0)\n            df['distance_ball_x_speed'] = df['distance_to_ball_landing'] * df['s']\n\n        # Advanced interactions\n        if {'is_target_receiver', 'time_seconds'}.issubset(df.columns):\n            df['is_target_x_time'] = df['is_target_receiver'] * df['time_seconds']\n        if {'is_target_receiver', 'time_squared'}.issubset(df.columns):\n            df['is_target_x_time_squared'] = df['is_target_receiver'] * df['time_squared']\n        if {'is_offense', 'is_early_play'}.issubset(df.columns):\n            df['is_offense_x_early_play'] = df['is_offense'] * df['is_early_play']\n        if {'is_offense', 'is_late_play'}.issubset(df.columns):\n            df['is_offense_x_late_play'] = df['is_offense'] * df['is_late_play']\n        if {'is_target_receiver', 'is_late_play'}.issubset(df.columns):\n            df['is_target_x_late_play'] = df['is_target_receiver'] * df['is_late_play']\n\n        # Additional previously undefined features\n        if {'orientation_change_rate', 'speed_deviation'}.issubset(df.columns):\n            df['motion_consistency'] = 1.0 / (1.0 + df['orientation_change_rate'] + df['speed_deviation'])\n\n        if 'distance_to_ball_landing' in df.columns:\n            diag = math.hypot(Config.FIELD_X_MAX, Config.FIELD_Y_MAX)\n            df['proximity_to_ball_ratio'] = 1.0 - (df['distance_to_ball_landing'] / (diag + 1e-6))\n\n        if {'angle_to_ball_landing'}.issubset(df.columns):\n            df['lateral_position_importance'] = np.abs(np.sin(df['angle_to_ball_landing']))\n\n        if {'velocity_x', 'play_direction'}.issubset(df.columns):\n            sign = df['play_direction'].fillna('right').map({'right': 1.0, 'left': -1.0}).astype(float)\n            df['downfield_progress'] = df['velocity_x'] * sign\n        elif 'velocity_x' in df.columns:\n            df['downfield_progress'] = df['velocity_x']\n\n        # Training targets\n        if training_mode and {'x', 'y', 'final_pre_throw_x', 'final_pre_throw_y'}.issubset(df.columns):\n            df['displacement_x'] = df['x'] - df['final_pre_throw_x']\n            df['displacement_y'] = df['y'] - df['final_pre_throw_y']\n\n        # Return engineered\n        return df\n\n    # Full feature preparation\n    def prepare_features(self, in_df, out_df, training_mode=False):\n        # Temporal aggregation\n        tmp = self._extract_temporal_features(in_df)\n\n        # Target receiver merge\n        tmp = self._incorporate_target_receiver_data(tmp)\n\n        # Merge columns\n        base_keys = ['game_id', 'play_id', 'nfl_id']\n        keep_cols = base_keys + [c for c in tmp.columns if c not in base_keys]\n\n        # Merge output/labels with features\n        merged = out_df.merge(tmp[keep_cols], on=base_keys, how='left')\n\n        # Advanced features\n        return self._calculate_advanced_features(merged, training_mode=training_mode)\n\n    # Fit encoders\n    def _fit_categorical_encoders(self, df, cat_cols):\n        # Loop\n        for c in cat_cols:\n            enc = SafeLabelEncoder()\n            enc.fit(df[c] if c in df.columns else pd.Series(['Unknown'] * len(df)))\n            self.encoders[c] = enc\n\n    # Transform categoricals\n    def _transform_categoricals(self, df, cat_cols):\n        # Copy\n        out = df.copy()\n\n        # Loop\n        for c in cat_cols:\n            if c in out.columns:\n                out[c] = self.encoders[c].transform(out[c])\n            else:\n                out[c] = self.encoders[c].transform(pd.Series(['Unknown'] * len(out)))\n\n        # Return\n        return out\n\n    # Train ensemble\n    def train_models(self):\n        # GPU info\n        Config.print_gpu_info()\n\n        # Seeds\n        set_all_seeds(self.seed)\n\n        # Load datasets\n        train_input, train_output, test_input, test_template = self.load_and_combine_datasets()\n\n        # Prepare train/test features\n        self.train_data = self.prepare_features(train_input, train_output, training_mode=True)\n        self.test_data = self.prepare_features(test_input, test_template, training_mode=False)\n\n        # Feature lists\n        all_cols = self.train_data.columns.tolist()\n\n        # Numerical feature candidates\n        num_candidates = [\n            'final_pre_throw_x', 'final_pre_throw_y', 's', 'a', 'o', 'dir',\n            'time_seconds', 'time_normalized', 'time_squared', 'time_cubed', 'sqrt_time', 'log_time',\n            'time_sin', 'time_cos', 'time_sin_2', 'time_cos_2', 'is_early_play', 'is_mid_play', 'is_late_play',\n            'x_mean', 'x_std', 'x_min', 'x_max', 'y_mean', 'y_std', 'y_min', 'y_max',\n            's_mean', 's_std', 's_max', 's_min', 'a_mean', 'a_std', 'a_max', 'a_min',\n            'dir_change_rate', 'orientation_change_rate', 'recent_displacement_x', 'recent_displacement_y',\n            'acceleration_trend',\n            'speed_consistency', 'speed_deviation', 'acceleration_consistency', 'acceleration_deviation',\n            'velocity_x', 'velocity_y', 'momentum_magnitude', 'expected_x_constant_v', 'expected_y_constant_v',\n            'expected_x_with_accel', 'expected_y_with_accel',\n            'distance_to_ball_landing', 'angle_to_ball_landing', 'closing_speed', 'ball_direction_x', 'ball_direction_y',\n            'estimated_time_to_ball', 'time_ratio_to_ball', 'projected_time_to_ball', 'time_urgency',\n            'distance_to_ball_x_time', 'distance_to_ball_x_time_squared',\n            'distance_to_target', 'is_target_receiver', 'angle_to_target', 'distance_to_target_x_time',\n            'is_target_x_time_squared',\n            'normalized_x', 'normalized_y', 'field_region_x', 'field_region_y',\n            'distance_from_sideline', 'distance_from_endzone',\n            'yards_to_endzone', 'is_offense', 'is_passer', 'is_coverage', 'is_redzone',\n            'height_inches', 'player_weight', 'bmi',\n            'speed_orientation_discrepancy',\n            'motion_consistency', 'proximity_to_ball_ratio', 'lateral_position_importance', 'downfield_progress',\n            'speed_times_acceleration', 'distance_speed_ratio', 'distance_ball_x_speed',\n            'time_x_speed', 'time_x_acceleration', 'time_squared_x_speed', 'is_target_x_time',\n            'is_offense_x_early_play', 'is_offense_x_late_play', 'is_target_x_late_play'\n        ]\n\n        # Categorical feature candidates\n        cat_candidates = ['player_role', 'player_side', 'play_direction']\n\n        # Keep present features\n        self.numerical_features = [c for c in num_candidates if c in all_cols]\n        self.categorical_features = [c for c in cat_candidates if c in all_cols]\n\n        # Target existence check\n        if not {'displacement_x', 'displacement_y'}.issubset(self.train_data.columns):\n            raise KeyError(\"Target variables (displacement_x, displacement_y) not found in training data\")\n\n        # Assemble X/y\n        X = self.train_data[self.numerical_features + self.categorical_features].copy()\n        y_dx = self.train_data['displacement_x'].values\n        y_dy = self.train_data['displacement_y'].values\n\n        # Fit encoders\n        self._fit_categorical_encoders(X, self.categorical_features)\n\n        # Transform categoricals\n        X = self._transform_categoricals(X, self.categorical_features)\n\n        # Fill NaNs\n        X = X.fillna(0)\n\n        # Fit scaler\n        self.num_scaler.fit(X[self.numerical_features])\n\n        # Scale numericals\n        X.loc[:, self.numerical_features] = self.num_scaler.transform(X[self.numerical_features])\n\n        # CV groups\n        groups = self.train_data['game_id'].values\n\n        # Train with CV\n        self._train_with_cv(X, y_dx, y_dy, groups)\n\n        # Return self\n        return self\n\n    # Cross-validation training\n    def _train_with_cv(self, X, y_dx, y_dy, groups):\n        # Splitter\n        gkf = GroupKFold(n_splits=Config.N_FOLDS)\n\n        # Loop folds\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, groups=groups), start=1):\n            # Fold split\n            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n            ydx_tr, ydx_va = y_dx[tr_idx], y_dx[va_idx]\n            ydy_tr, ydy_va = y_dy[tr_idx], y_dy[va_idx]\n\n            # Logs\n            print(f\"Fold {fold}/{Config.N_FOLDS}\")\n\n            # XGBoost params\n            xgb_params = {\n                'n_estimators': 5000,\n                'learning_rate': 0.05,\n                'max_depth': 8,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'random_state': self.seed + fold,\n                'verbosity': 0,\n                'objective': 'reg:squarederror',\n            }\n\n            # XGBoost GPU\n            if Config.USE_GPU and torch.cuda.is_available():\n                xgb_params.update({'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'gpu_id': 0})\n                print(\"  XGBoost: GPU\")\n            else:\n                xgb_params.update({'tree_method': 'hist'})\n                print(\"  XGBoost: CPU\")\n\n            # LightGBM params\n            lgb_params = {\n                'n_estimators': 5000,\n                'learning_rate': 0.05,\n                'max_depth': 8,\n                'num_leaves': 100,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'random_state': self.seed + fold,\n                'verbosity': -1,\n                'objective': 'regression',\n            }\n\n            # LightGBM GPU\n            if Config.USE_GPU and torch.cuda.is_available():\n                lgb_params.update({'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0})\n                print(\"  LightGBM: GPU\")\n            else:\n                print(\"  LightGBM: CPU\")\n\n            # CatBoost params\n            cat_params = {\n                'iterations': 5000,\n                'learning_rate': 0.05,\n                'depth': 8,\n                'random_seed': self.seed + fold,\n                'verbose': False,\n                'loss_function': 'RMSE',\n                'eval_metric': 'RMSE',\n            }\n\n            # CatBoost GPU\n            if Config.USE_GPU and torch.cuda.is_available():\n                cat_params.update({'task_type': 'GPU', 'devices': '0'})\n                print(\"  CatBoost: GPU\")\n            else:\n                print(\"  CatBoost: CPU\")\n\n            # Train XGB dx\n            xgb_dx = XGBRegressor(**xgb_params)\n            xgb_dx.fit(\n                X_tr, ydx_tr,\n                eval_set=[(X_va, ydx_va)],\n                verbose=False,\n                early_stopping_rounds=Config.ES_ROUNDS\n            )\n            self.models_dx['xgb'].append(xgb_dx)\n\n            # Train XGB dy\n            xgb_dy = XGBRegressor(**xgb_params)\n            xgb_dy.fit(\n                X_tr, ydy_tr,\n                eval_set=[(X_va, ydy_va)],\n                verbose=False,\n                early_stopping_rounds=Config.ES_ROUNDS\n            )\n            self.models_dy['xgb'].append(xgb_dy)\n\n            # Train LGB dx\n            lgb_dx = LGBMRegressor(**lgb_params)\n            lgb_dx.fit(\n                X_tr, ydx_tr,\n                eval_set=[(X_va, ydx_va)],\n                eval_metric='rmse',\n                callbacks=[]\n            )\n            self.models_dx['lgb'].append(lgb_dx)\n\n            # Train LGB dy\n            lgb_dy = LGBMRegressor(**lgb_params)\n            lgb_dy.fit(\n                X_tr, ydy_tr,\n                eval_set=[(X_va, ydy_va)],\n                eval_metric='rmse',\n                callbacks=[]\n            )\n            self.models_dy['lgb'].append(lgb_dy)\n\n            # Train Cat dx\n            cat_dx = CatBoostRegressor(**cat_params)\n            cat_dx.fit(\n                X_tr, ydx_tr,\n                eval_set=(X_va, ydx_va),\n                use_best_model=True\n            )\n            self.models_dx['cat'].append(cat_dx)\n\n            # Train Cat dy\n            cat_dy = CatBoostRegressor(**cat_params)\n            cat_dy.fit(\n                X_tr, ydy_tr,\n                eval_set=(X_va, ydy_va),\n                use_best_model=True\n            )\n            self.models_dy['cat'].append(cat_dy)\n\n            # Train NN dx\n            nn_dx = self._train_neural_network(\n                X_tr.values, ydx_tr, X_va.values, ydx_va, seed=self.seed + fold\n            )\n            self.nn_models_dx.append(nn_dx)\n\n            # Train NN dy\n            nn_dy = self._train_neural_network(\n                X_tr.values, ydy_tr, X_va.values, ydy_va, seed=self.seed + fold + 100\n            )\n            self.nn_models_dy.append(nn_dy)\n\n            # Cleanup\n            Config.cleanup_gpu_memory()\n\n    # Train a neural net\n    def _train_neural_network(self, X_tr, y_tr, X_va, y_va, seed=42):\n        # Seeds\n        set_all_seeds(seed)\n\n        # Device\n        device = torch.device(Config.GPU_DEVICE)\n\n        # Datasets\n        ds_tr = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr.reshape(-1, 1)))\n        ds_va = TensorDataset(torch.FloatTensor(X_va), torch.FloatTensor(y_va.reshape(-1, 1)))\n\n        # Loaders\n        ld_tr = DataLoader(ds_tr, batch_size=Config.NN_BATCH_SIZE, shuffle=True)\n        ld_va = DataLoader(ds_va, batch_size=Config.NN_BATCH_SIZE, shuffle=False)\n\n        # Model\n        model = SimpleNN(X_tr.shape[1]).to(device)\n\n        # Loss and optimizer\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=Config.NN_LEARNING_RATE)\n\n        # Early stopping trackers\n        best_loss = float('inf')\n        best_state = model.state_dict()\n        patience = 0\n\n        # Epoch loop\n        for epoch in range(Config.NN_EPOCHS):\n            # Train mode\n            model.train()\n\n            # Loss accumulator\n            train_losses = []\n\n            # Batches\n            for xb, yb in ld_tr:\n                xb = xb.to(device)\n                yb = yb.to(device)\n                optimizer.zero_grad()\n                out = model(xb)\n                loss = criterion(out, yb)\n                loss.backward()\n                optimizer.step()\n                train_losses.append(loss.item())\n\n            # Eval mode\n            model.eval()\n\n            # Validation loss accumulator\n            val_losses = []\n\n            # No grad\n            with torch.no_grad():\n                for xb, yb in ld_va:\n                    xb = xb.to(device)\n                    yb = yb.to(device)\n                    out = model(xb)\n                    loss = criterion(out, yb)\n                    val_losses.append(loss.item())\n\n            # Average validation loss\n            val = float(np.mean(val_losses))\n\n            # Early stopping update\n            if val < best_loss:\n                best_loss = val\n                best_state = model.state_dict()\n                patience = 0\n            else:\n                patience += 1\n                if patience >= Config.NN_PATIENCE:\n                    break\n\n        # Restore best state\n        model.load_state_dict(best_state)\n\n        # Return model\n        return model\n\n    # Generate predictions for test\n    def generate_predictions(self):\n        # Assemble test matrix\n        X_test = self.test_data[self.numerical_features + self.categorical_features].copy()\n\n        # Transform categoricals\n        X_test = self._transform_categoricals(X_test, self.categorical_features)\n\n        # Fill NAs\n        X_test = X_test.fillna(0)\n\n        # Scale numericals\n        X_test.loc[:, self.numerical_features] = self.num_scaler.transform(X_test[self.numerical_features])\n\n        # Ensemble predictions\n        dx = self._ensemble_prediction(X_test.values, self.models_dx, self.nn_models_dx)\n        dy = self._ensemble_prediction(X_test.values, self.models_dy, self.nn_models_dy)\n\n        # Predicted absolute positions\n        self.test_data['predicted_x'] = self.test_data['final_pre_throw_x'] + dx\n        self.test_data['predicted_y'] = self.test_data['final_pre_throw_y'] + dy\n\n        # Constraints\n        self.test_data = self._apply_constraints(self.test_data)\n\n        # Smoothing\n        self.test_data = self._smooth_trajectories(self.test_data)\n\n        # Return predictions\n        return self.test_data\n\n    # Weighted ensemble\n    def _ensemble_prediction(self, X, tree_models, nn_models):\n        # Tree weights\n        weights = {'xgb': 0.34, 'lgb': 0.34, 'cat': 0.22}\n\n        # Bucket\n        preds = []\n\n        # Tree average per family\n        for name, models in tree_models.items():\n            if len(models) == 0:\n                continue\n            fold_preds = [m.predict(X) for m in models]\n            avg = np.mean(fold_preds, axis=0)\n            if name in weights:\n                preds.append(avg * weights[name])\n\n        # Neural nets\n        if len(nn_models) > 0:\n            device = torch.device(Config.GPU_DEVICE)\n            X_tensor = torch.FloatTensor(X).to(device)\n            nn_fold = []\n            for m in nn_models:\n                m.eval()\n                with torch.no_grad():\n                    p = m(X_tensor).cpu().numpy().squeeze()\n                nn_fold.append(p)\n            nn_avg = np.mean(nn_fold, axis=0)\n            preds.append(nn_avg * 0.10)\n\n        # Sum\n        if len(preds) == 0:\n            return np.zeros(X.shape[0])\n        return np.sum(preds, axis=0)\n\n    # Physics constraints\n    def _apply_constraints(self, df):\n        # Deltas\n        dx = df['predicted_x'] - df['final_pre_throw_x']\n        dy = df['predicted_y'] - df['final_pre_throw_y']\n        disp = np.sqrt(dx ** 2 + dy ** 2)\n\n        # Max displacement by time\n        if 'time_seconds' in df.columns:\n            max_disp = Config.MAX_SPEED * df['time_seconds']\n            mask = disp > max_disp\n            if np.any(mask):\n                scale = max_disp[mask] / (disp[mask] + 1e-6)\n                dx.loc[mask] = dx[mask] * scale\n                dy.loc[mask] = dy[mask] * scale\n                df.loc[mask, 'predicted_x'] = df.loc[mask, 'final_pre_throw_x'] + dx[mask]\n                df.loc[mask, 'predicted_y'] = df.loc[mask, 'final_pre_throw_y'] + dy[mask]\n\n        # Clip to field bounds\n        df['predicted_x'] = df['predicted_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n        df['predicted_y'] = df['predicted_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n\n        # Return constrained\n        return df\n\n    # Gaussian smoothing within trajectories\n    def _smooth_trajectories(self, df):\n        # Grouping keys\n        keys = ['game_id', 'play_id', 'nfl_id']\n\n        # Iterate groups\n        for k, g in df.groupby(keys):\n            if len(g) <= 3:\n                continue\n            idx = g.index\n            df.loc[idx, 'predicted_x'] = gaussian_filter1d(g['predicted_x'].values, sigma=0.5)\n            df.loc[idx, 'predicted_y'] = gaussian_filter1d(g['predicted_y'].values, sigma=0.5)\n\n        # Return smoothed\n        return df\n\n    # Submission writer\n    def create_submission_file(self, output_path=\"submission.csv\"):\n        # Guard for predictions present\n        required_cols = ['predicted_x', 'predicted_y']\n        for c in required_cols:\n            if c not in self.test_data.columns:\n                raise KeyError(f\"Missing required prediction column: {c}\")\n    \n        # Prefer the competition-provided id if available\n        if 'id' in self.test_data.columns:\n            # Select and rename\n            sub = self.test_data[['id', 'predicted_x', 'predicted_y']].rename(columns={\n                'predicted_x': 'x',\n                'predicted_y': 'y'\n            })\n        else:\n            # Fall back to constructing an id that includes frame_id if present\n            base_keys = ['game_id', 'play_id', 'nfl_id']\n            for k in base_keys:\n                if k not in self.test_data.columns:\n                    raise KeyError(f\"Missing key column needed to build id: {k}\")\n    \n            # If frame_id exists, include it in the id; if not, build without it\n            if 'frame_id' in self.test_data.columns:\n                # Build id with frame granularity\n                self.test_data['id'] = (\n                    self.test_data['game_id'].astype(str) + \"_\" +\n                    self.test_data['play_id'].astype(str) + \"_\" +\n                    self.test_data['nfl_id'].astype(str) + \"_\" +\n                    self.test_data['frame_id'].astype(str)\n                )\n            else:\n                # Build id without frame granularity\n                self.test_data['id'] = (\n                    self.test_data['game_id'].astype(str) + \"_\" +\n                    self.test_data['play_id'].astype(str) + \"_\" +\n                    self.test_data['nfl_id'].astype(str)\n                )\n    \n            # Select and rename\n            sub = self.test_data[['id', 'predicted_x', 'predicted_y']].rename(columns={\n                'predicted_x': 'x',\n                'predicted_y': 'y'\n            })\n    \n        # Save to disk\n        sub.to_csv(output_path, index=False)\n    \n        # Logs\n        print(f\"Submission file saved to {output_path}\")\n        print(f\"Submission shape: {sub.shape}\")\n    \n        # Return submission\n        return sub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# MAIN\n# ================================================================================\n\ndef main():\n    # RNG seeds\n    set_all_seeds(Config.SEED)\n\n    # GPU check\n    print(\"ðŸ” Checking GPU requirements...\")\n    check_gpu_requirements()\n\n    # GPU info\n    Config.print_gpu_info()\n\n    # Build predictor\n    predictor = EnhancedNFLPlayerMovementPredictor(\n        data_dir=Config.DATA_DIR,\n        seed=Config.SEED\n    )\n\n    # Train models\n    print(\"\\nðŸš€ Training enhanced models with GPU acceleration...\")\n    predictor.train_models()\n\n    # Generate predictions\n    print(\"\\nðŸ“Š Generating predictions...\")\n    preds = predictor.generate_predictions()\n\n    # Create submission\n    print(\"\\nðŸ’¾ Creating submission file...\")\n    sub = predictor.create_submission_file(\"/kaggle/working/submission.csv\")\n\n    # Final log\n    print(\"\\nâœ… Enhanced pipeline completed successfully!\")\n    print(f\"ðŸ“ˆ Final submission shape: {sub.shape}\")\n    print(\"\\nFirst 5 predictions:\")\n    print(sub.head())\n\n    # Cleanup\n    Config.cleanup_gpu_memory()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entry point\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        print(f\"âŒ Error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n        Config.cleanup_gpu_memory()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T03:28:40.524617Z","iopub.execute_input":"2025-09-26T03:28:40.525449Z","iopub.status.idle":"2025-09-26T03:29:44.519801Z","shell.execute_reply.started":"2025-09-26T03:28:40.525421Z","shell.execute_reply":"2025-09-26T03:29:44.518907Z"}},"outputs":[],"execution_count":null}]}