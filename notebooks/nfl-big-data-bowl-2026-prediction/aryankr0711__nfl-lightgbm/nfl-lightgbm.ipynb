{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":158.982855,"end_time":"2025-10-12T21:55:27.064716","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-12T21:52:48.081861","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom pathlib import Path\nimport gc\nimport joblib\nimport os","metadata":{"papermill":{"duration":9.931966,"end_time":"2025-10-12T21:53:03.104575","exception":false,"start_time":"2025-10-12T21:52:53.172609","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:15:07.056766Z","iopub.execute_input":"2025-10-28T18:15:07.058686Z","iopub.status.idle":"2025-10-28T18:15:07.071011Z","shell.execute_reply.started":"2025-10-28T18:15:07.058609Z","shell.execute_reply":"2025-10-28T18:15:07.06932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 1. Configuration","metadata":{"papermill":{"duration":0.003152,"end_time":"2025-10-12T21:53:03.111355","exception":false,"start_time":"2025-10-12T21:53:03.108203","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Config\nDATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction/\"\n\ntest_in = pd.read_csv(os.path.join(DATA_DIR, \"test_input.csv\"))\ntest_template = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n#sample_submission = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n\nTRAIN_PATH = DATA_DIR + '/train'\nWEEKS = list(range(1, 19))\nN_FOLDS = 5\nRANDOM_STATE = 42\n\ngroup_cols = ['game_id', 'play_id', 'nfl_id']\nid_dataset = ['game_id', 'play_id', 'nfl_id', 'frame_id']\n\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 1000,\n    \"random_state\": 42,\n}","metadata":{"papermill":{"duration":0.012359,"end_time":"2025-10-12T21:53:03.12685","exception":false,"start_time":"2025-10-12T21:53:03.114491","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:15:07.260822Z","iopub.execute_input":"2025-10-28T18:15:07.261307Z","iopub.status.idle":"2025-10-28T18:15:07.71103Z","shell.execute_reply.started":"2025-10-28T18:15:07.261273Z","shell.execute_reply":"2025-10-28T18:15:07.709506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 2. Data Loading","metadata":{"papermill":{"duration":0.003061,"end_time":"2025-10-12T21:53:03.133212","exception":false,"start_time":"2025-10-12T21:53:03.130151","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_weeks_parallel(week_nums, train_path, max_workers=8):\n    def load_week(w):\n        inp = pd.read_csv(train_path + f\"/input_2023_w{w:02d}.csv\")\n        out = pd.read_csv(train_path + f\"/output_2023_w{w:02d}.csv\")\n        return w, inp, out\n\n    inputs, outputs = [], []\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = {executor.submit(load_week, w): w for w in week_nums}\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Loading weeks\"):\n            w, inp, out = future.result()\n            inputs.append(inp)\n            outputs.append(out)\n\n    return pd.concat(inputs, ignore_index=True), pd.concat(outputs, ignore_index=True)\n\ndf_in, df_out = load_weeks_parallel(WEEKS, TRAIN_PATH, max_workers=8)\nprint(f\"Data loaded: {len(df_in)} input rows, {df_in['game_id'].nunique()} games\")\nprint(f\"Data loaded: {len(df_out)} output rows, {df_out['game_id'].nunique()} games\")","metadata":{"papermill":{"duration":16.520538,"end_time":"2025-10-12T21:53:19.656816","exception":false,"start_time":"2025-10-12T21:53:03.136278","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:15:07.713216Z","iopub.execute_input":"2025-10-28T18:15:07.714168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 3. Feature Engineering","metadata":{"papermill":{"duration":0.003381,"end_time":"2025-10-12T21:55:02.578507","exception":false,"start_time":"2025-10-12T21:55:02.575126","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches / 12\n    except:\n        return 6.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_opponent_proximity_simple(input_df):\n    \"\"\"Compute simple opponent proximity features.\"\"\"\n    features = []\n    # Note: input_df here is the combined 'train' or 'test_rows' DF\n    \n    # 1. Temporarily rename 'x_last', 'y_last' back to 'x', 'y' for calculation\n    #    This is safer than modifying 'input_df' in place.\n    #    Alternatively, we can use the 'x_last' and 'y_last' columns directly.\n    \n    for (gid, pid), group in input_df.groupby(['game_id', 'play_id']):\n        # We need the last observed frame for proximity calculation\n        # NOTE: group is already reduced to player/play level by the merge logic,\n        # but to be safe, we group by nfl_id and take the last available value.\n        \n        # In the context of prepare_test/prepare_train, the input_df is already \n        # a player-play level DF merged with the last observation features. \n        # We should use the last observation columns.\n        last = group.sort_values('num_frames_output').groupby('nfl_id', dropna=False).last()\n        \n        if len(last) < 2:\n            continue\n            \n        # ðŸ’¥ FIX: Use 'x_last' and 'y_last' instead of 'x' and 'y'\n        positions = last[['x_last', 'y_last']].values \n        \n        sides = last['player_side'].values\n        speeds = last['s'].values\n        directions = last['dir'].values\n        \n        for i, (nid, side) in enumerate(zip(last.index, sides)):\n            opp_mask = sides != side\n            feat = {\n                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n                'nearest_opp_dist': 50.0,\n                'num_nearby_opp_3': 0,\n                'num_nearby_opp_5': 0,\n                'closing_speed_opp': 0.0,\n            }\n            if not opp_mask.any():\n                features.append(feat)\n                continue\n            opp_positions = positions[opp_mask]\n            distances = np.sqrt(((positions[i] - opp_positions) ** 2).sum(axis=1))\n            if len(distances) == 0:\n                features.append(feat)\n                continue\n            nearest_idx = distances.argmin()\n            feat['nearest_opp_dist'] = distances[nearest_idx]\n            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n            features.append(feat)\n    return pd.DataFrame(features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_safe_catboost_features(df):\n    \"\"\"Adds physics-based safe features.\"\"\"\n    df = df.copy()\n    height_parts = df['player_height'].astype(str).str.split('-', expand=True)\n    try:\n        df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    except:\n        df['height_inches'] = np.nan\n    df['bmi'] = (df['player_weight'] / (df['height_inches'] ** 2)) * 703\n    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n    df['speed_squared'] = df['s'] ** 2\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_last_obs(df):\n    \"\"\"\n    Get the last observed position of each player before the pass and convert height to inches.\n    \"\"\"\n    df_last = df.sort_values(['game_id','play_id','nfl_id','frame_id']).groupby(\n        ['game_id','play_id','nfl_id'], as_index=False\n    ).last()\n    df_last = df_last.rename(columns={'x':'x_last','y':'y_last', 'frame_id': 'last_frame_id'}) # <<-- RENAME frame_id HERE\n\n    def height_to_inches(ht):\n        if isinstance(ht, str) and '-' in ht:\n            feet, inches = ht.split('-')\n            return int(feet) * 12 + int(inches)\n        else:\n            return np.nan\n            \n    df_last['player_height'] = df_last['player_height'].apply(height_to_inches)\n    \n    return df_last\n\ndef add_target_info(df_last):\n    \"\"\"\n    Add targeted receiver position. Every player in the same play now knows where the targeted receiver was.\n    \"\"\"\n    \n    targets = df_last[df_last['player_role']==\"Targeted Receiver\"][['game_id','play_id','nfl_id','x_last','y_last']]\n    targets = targets.rename(columns={'nfl_id':'target_nfl_id','x_last':'target_last_x','y_last':'target_last_y'})\n\n    df_last = df_last.merge(\n        targets[['game_id','play_id','target_last_x','target_last_y','target_nfl_id']],\n        on=['game_id','play_id'], how='left'\n    )\n    return df_last\n\ndef create_features(df, is_train=True):\n    \"\"\"\n    Enhanced feature engineering: combines original features + safe physics features\n    \"\"\"\n    df = df.copy()\n    if 'frame_id' in df.columns:\n         df['frame_offset'] = df['frame_id'] # Use frame_id from df_out (if it exists)\n    else:\n         # Use 'num_frames_output' as the time offset if frame_id isn't present in df_out/train\n         df['frame_offset'] = df['num_frames_output'] \n         \n    df['time_offset'] = df['frame_offset'] / 10.0\n\n    # Existing core features (keep them)\n    df['frame_offset'] = df['frame_id']\n    df['time_offset'] = df['frame_offset'] / 10.0\n    df['dist_to_ball_land'] = np.sqrt((df['ball_land_x'] - df['x_last'])**2 +\n                                     (df['ball_land_y'] - df['y_last'])**2)\n    df['angle_to_ball_land'] = np.arctan2(df['ball_land_y'] - df['y_last'],\n                                          df['ball_land_x'] - df['x_last'])\n    df['dist_to_target_last'] = np.sqrt((df['target_last_x'] - df['x_last'])**2 +\n                                       (df['target_last_y'] - df['y_last'])**2)\n    df['is_target'] = (df['nfl_id'] == df['target_nfl_id']).astype(int)\n\n    # âœ… New advanced physics features\n    df = add_safe_catboost_features(df)\n\n    # âœ… Add opponent proximity features\n    opp_feats = get_opponent_proximity_simple(df)\n    df = df.merge(opp_feats, on=['game_id', 'play_id', 'nfl_id'], how='left')\n\n    # âœ… Add field geometry\n    df['dist_from_sideline'] = np.minimum(df['y_last'], 53.3 - df['y_last'])\n    df['dist_from_endzone'] = np.minimum(df['x_last'], 120 - df['x_last'])\n\n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 4. Prepare dataset","metadata":{}},{"cell_type":"code","source":"cols_to_keep = [\n    'game_id', 'play_id', 'nfl_id',\n    'player_to_predict', 'last_frame_id', # <<-- Changed from 'frame_id'\n    'play_direction', 'absolute_yardline_number',\n    # ... (other columns remain the same)\n    'player_name', 'player_height', 'player_weight', 'player_birth_date',\n    'player_position', 'player_side', 'player_role',\n    'x_last', 'y_last', 's', 'a', 'dir', 'o',\n    'num_frames_output', 'ball_land_x', 'ball_land_y',\n    'target_last_x', 'target_last_y', 'target_nfl_id'\n]\n\n\n# factorise into prepare data \ndef prepare_train(df_in, df_out, cols):\n    last_obs = prepare_last_obs(df_in)\n    last_obs = add_target_info(last_obs)\n    train = df_out.merge(last_obs[cols], on=['game_id','play_id','nfl_id'], how='left')\n    train = create_features(train, is_train=True)\n    \n    return train\n\ndef prepare_test(test_in, test_template, cols):\n    last_test = prepare_last_obs(test_in)\n    last_test = add_target_info(last_test)\n    \n    test_rows = test_template.merge(last_test[cols], on=['game_id','play_id','nfl_id'], how='left')\n    test_rows = create_features(test_rows, is_train=False)\n    \n    return test_rows","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_in.columns.tolist())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare datasets\ntrain = prepare_train(df_in, df_out, cols_to_keep)\ntest = prepare_test(test_in, test_template, cols_to_keep)\n\n# Select features to use \nFEATURES = [\n    'x_last','y_last','s','a','o','dir',\n    'frame_offset','time_offset',\n    'dist_to_ball_land','angle_to_ball_land',\n    'dist_to_target_last','is_target',\n    'absolute_yardline_number', 'player_height', 'player_weight',\n    'bmi','orientation_diff','speed_squared',\n    'nearest_opp_dist','num_nearby_opp_3','num_nearby_opp_5',\n    'dist_from_sideline','dist_from_endzone'\n]\n\n#CAT_FEATS = ['player_role','player_side','play_direction']\n#TARGETS = ['dx','dy']\nTARGETS = ['x','y']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train[FEATURES].copy()\n#X = train[FEATURES + CAT_FEATS].copy()\n#for c in CAT_FEATS:\n#    X[c] = X[c].astype('category')\n#y_dx = train['dx'].values\n#y_dy = train['dy'].values\ny_x = train['x'].values\ny_y = train['y'].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 5. Cross-Validation","metadata":{"papermill":{"duration":0.003252,"end_time":"2025-10-12T21:55:05.730306","exception":false,"start_time":"2025-10-12T21:55:05.727054","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nos.makedirs(\"cv_results\", exist_ok=True)\nos.makedirs(\"cv_results/models_x\", exist_ok=True)\nos.makedirs(\"cv_results/models_y\", exist_ok=True)\nfold_importances = []\nbest_rmse = np.inf\nbest_fold = None\nbest_model_dx = None\nbest_model_dy = None\ncv_scores = []\nmodels_dx, models_dy = [], []\n\ngroups = train['game_id'].astype(str) + '_' + train['play_id'].astype(str)\ngkf = GroupKFold(n_splits=N_FOLDS)\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, groups=groups), 1):\n    print(f\"\\n=== Fold {fold}/{N_FOLDS} ===\")\n\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    #y_x_train, y_x_val = y_dx[train_idx], y_dx[val_idx]\n    #y_y_train, y_y_val = y_dy[train_idx], y_dy[val_idx]\n    y_x_train, y_x_val = y_x[train_idx], y_x[val_idx]\n    y_y_train, y_y_val = y_y[train_idx], y_y[val_idx]\n\n    # --- Train model for X coordinate ---\n    model_dx = LGBMRegressor(**params)\n    model_dx.fit(\n        X_train, y_x_train,\n        eval_set=[(X_val, y_x_val)],\n        #categorical_feature=CAT_FEATS,\n        eval_metric=\"rmse\",\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100, verbose=True),\n            lgb.log_evaluation(100),\n        ],\n    )\n\n    # --- Train model for Y coordinate ---\n    model_dy = LGBMRegressor(**params)\n    model_dy.fit(\n        X_train, y_y_train,\n        eval_set=[(X_val, y_y_val)],\n        #categorical_feature=CAT_FEATS,\n        eval_metric=\"rmse\",\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100, verbose=True),\n            lgb.log_evaluation(100),\n        ],\n    )\n\n     # --- Save models ---\n    models_dx.append(model_dx)\n    models_dy.append(model_dy)\n\n    # --- Evaluate fold performance ---\n    pred_x = model_dx.predict(X_val)\n    pred_y = model_dy.predict(X_val)\n\n    rmse_x = np.sqrt(mean_squared_error(y_x_val, pred_x))\n    rmse_y = np.sqrt(mean_squared_error(y_y_val, pred_y))\n    rmse_combined = np.sqrt((rmse_x**2 + rmse_y**2) / 2)\n    cv_scores.append(rmse_combined)\n\n    print(f\"Fold {fold}: RMSE_X={rmse_x:.4f}, RMSE_Y={rmse_y:.4f}, Combined={rmse_combined:.4f}\")\n    \n    imp_x = pd.DataFrame({\n        \"feature\": X.columns,\n        \"importance_split\": model_dx.booster_.feature_importance(importance_type=\"split\"),\n        \"importance_gain\": model_dx.booster_.feature_importance(importance_type=\"gain\"),\n        \"target\": \"x\",\n        \"fold\": fold\n    })\n    imp_y = pd.DataFrame({\n        \"feature\": X.columns,\n        \"importance_split\": model_dy.booster_.feature_importance(importance_type=\"split\"),\n        \"importance_gain\": model_dy.booster_.feature_importance(importance_type=\"gain\"),\n        \"target\": \"y\",\n        \"fold\": fold\n    })\n\n    fold_importances.append(pd.concat([imp_x, imp_y], axis=0))\n\n    if rmse_combined < best_rmse:\n        best_rmse = rmse_combined\n        best_fold = fold\n        best_model_dx = model_dx\n        best_model_dy = model_dy\n\n\nall_importances = pd.concat(fold_importances, axis=0)\nall_importances.to_csv(\"cv_results/all_importances.csv\", index=False)\n\njoblib.dump(best_model_dx, f\"cv_results/best_model_dx_fold{best_fold}.pkl\")\njoblib.dump(best_model_dy, f\"cv_results/best_model_dy_fold{best_fold}.pkl\")\n\nprint(f\"Mean CV RMSE: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")\nprint(f\"Models trained: {len(models_dx)} for X, {len(models_dy)} for Y\")","metadata":{"papermill":{"duration":17.918151,"end_time":"2025-10-12T21:55:23.651838","exception":false,"start_time":"2025-10-12T21:55:05.733687","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 6. Test Prediction","metadata":{"papermill":{"duration":0.00512,"end_time":"2025-10-12T21:55:23.662355","exception":false,"start_time":"2025-10-12T21:55:23.657235","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_test = test[FEATURES].copy()\n#X_test = test[FEATURES + CAT_FEATS].copy()\n#for c in CAT_FEATS:\n#    X_test[c] = X_test[c].astype('category')\n\n# Initialize prediction arrays\npred_dx = np.zeros(len(X_test))\npred_dy = np.zeros(len(X_test))\n\n# Average predictions from all folds\nfor model_dx, model_dy in zip(models_dx, models_dy):\n    pred_dx += model_dx.predict(X_test)\n    pred_dy += model_dy.predict(X_test)\n\npred_dx /= len(models_dx)\npred_dy /= len(models_dy)\n\n# Reconstruct absolute positions\n#test['pred_x'] = test['x_last'] + pred_dx\n#test['pred_y'] = test['y_last'] + pred_dy\ntest['pred_x'] = pred_dx\ntest['pred_y'] = pred_dy","metadata":{"papermill":{"duration":1.881685,"end_time":"2025-10-12T21:55:25.54821","exception":false,"start_time":"2025-10-12T21:55:23.666525","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 7. Submission","metadata":{"papermill":{"duration":0.005007,"end_time":"2025-10-12T21:55:25.559081","exception":false,"start_time":"2025-10-12T21:55:25.554074","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test['id'] = (test['game_id'].astype(str) + \"_\" +\n                   test['play_id'].astype(str) + \"_\" +\n                   test['nfl_id'].astype(str) + \"_\" +\n                   test['frame_id'].astype(str))\n\nsubmission = test[['id','pred_x','pred_y']].rename(columns={'pred_x':'x','pred_y':'y'})\nsubmission.to_csv(\"submission.parquet\", index=False)\nprint(\"Saved submission.parquet, rows:\", submission.shape[0])\nsubmission.head()","metadata":{"papermill":{"duration":0.373562,"end_time":"2025-10-12T21:55:25.936823","exception":false,"start_time":"2025-10-12T21:55:25.563261","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}