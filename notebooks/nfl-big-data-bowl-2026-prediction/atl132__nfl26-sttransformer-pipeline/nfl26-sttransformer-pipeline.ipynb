{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Note\nThe code is mostly based on [NFL Big Data Bowl 2026 - Geometry GNN [LB: .586]](https://www.kaggle.com/code/atl132/nfl-big-data-bowl-2026-geometry-gnn-lb-586), some functions are copied from [NFL2026 Prediction|Openmind on Hsiaosuan Exp‚ù•(^_-)](https://www.kaggle.com/code/atl132/nfl2026-prediction-openmind-on-hsiaosuan-exp)","metadata":{"_uuid":"bf835df1-b9f2-44f1-a20b-1855edf7b7ce","_cell_guid":"8acf3b02-0a63-4476-ad3c-86e4bcdad0c3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import random\nimport os,sys\nimport pickle\nfrom pathlib import Path\nimport warnings\nimport math\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import LRScheduler\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold,StratifiedGroupKFold\nfrom sklearn.cluster import KMeans\n\n# warnings.filterwarnings('ignore')","metadata":{"_uuid":"87661160-cfe0-4464-af5c-abb7714e8993","_cell_guid":"d6216333-0d3c-4879-8876-cce7fc0bf48a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CONFIG\n# ============================================================================\n\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    OUTPUT_DIR = Path(\"./outputs\")\n    OUTPUT_DIR.mkdir(exist_ok=True)\n    DEBUG=True\n\n    SEED = 1\n    N_FOLDS = 5\n    BATCH_SIZE = 256\n    EPOCHS = 2000\n    PATIENCE = 30\n    LEARNING_RATE = 2e-4\n    AUG=False\n    Split=\"sgkf\"\n    scheduler=\"ReduceLROnPlateau\"\n\n    Model_Name=\"STTransformer\" #STSeqModel\n    MAX_PLAYERS=17\n    MAX_PLAYERS2Predict=9\n    WINDOW_SIZE = 10\n    DMODEL = 256\n    MAX_FUTURE_HORIZON = 60\n      \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    Target_Scale_x, Target_Scale_y = 1.,1.\n    \n    K_NEIGH = 6\n    RADIUS = 30.0\n    TAU = 8.0\n    N_ROUTE_CLUSTERS = 7\n    \ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cuda.enable_nested_tensor = False\n# ============================================================================\n# GEOMETRIC BASELINE - THE BREAKTHROUGH\n# ============================================================================\n\n   \ndef compute_geometric_endpoint(df):\n    \"\"\"\n    Compute where each player SHOULD end up based on geometry.\n    This is the deterministic part - no learning needed.\n    \"\"\"\n    df = df.copy()\n    \n    # Time to play end\n    if 'num_frames_output' in df.columns:\n        t_total = df['num_frames_output'] / 10.0\n    else:\n        t_total = 3.0\n    \n    df['time_to_endpoint'] = t_total\n    \n    # Initialize with momentum (default rule)\n    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n    \n    # Rule 1: Targeted Receivers converge to ball\n    if 'ball_land_x' in df.columns:\n        receiver_mask = df['player_role'] == 'Targeted Receiver'\n        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n        \n        # Rule 2: Defenders mirror receivers (maintain offset)\n        defender_mask = df['player_role'] == 'Defensive Coverage'\n        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n        coverage_mask = defender_mask & has_mirror\n        \n        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n            df.loc[coverage_mask, 'ball_land_x'] + \n            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n        )\n        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n            df.loc[coverage_mask, 'ball_land_y'] + \n            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n        )\n    \n    # Clip to field\n    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    return df\n\ndef add_geometric_features(df):\n    \"\"\"Add features that describe the geometric solution\"\"\"\n    df = compute_geometric_endpoint(df)\n    \n    # Vector to geometric endpoint\n    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n    \n    # Required velocity to reach geometric endpoint\n    t = df['time_to_endpoint'] + 0.1\n    df['geo_required_vx'] = df['geo_vector_x'] / t\n    df['geo_required_vy'] = df['geo_vector_y'] / t\n    \n    # Current velocity vs required\n    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n    df['geo_velocity_error'] = np.sqrt(\n        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n    )\n    \n    # Required constant acceleration (a = 2*Œîx/t¬≤)\n    t_sq = t * t\n    df['geo_required_ax'] = 2 * df['geo_vector_x'] / t_sq\n    df['geo_required_ay'] = 2 * df['geo_vector_y'] / t_sq\n    df['geo_required_ax'] = df['geo_required_ax'].clip(-10, 10)\n    df['geo_required_ay'] = df['geo_required_ay'].clip(-10, 10)\n    \n    # Alignment with geometric path\n    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n    df['geo_alignment'] = (\n        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n    ) / (velocity_mag + 0.1)\n    \n    # Role-specific geometric quality\n    df['geo_receiver_urgency'] = df['is_receiver'] * df['geo_distance'] / (t + 0.1)\n    df['geo_defender_coupling'] = df['is_coverage'] * (1.0 / (df.get('mirror_wr_dist', 50) + 1.0))\n    \n    return df\n\n# ============================================================================\n# PROVEN FEATURE ENGINEERING \n# ============================================================================\n\ndef get_velocity(speed, direction_deg):\n    theta = np.deg2rad(direction_deg)\n    return speed * np.sin(theta), speed * np.cos(theta)\n\ndef height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n\ndef get_opponent_features(input_df):\n    \"\"\"Enhanced opponent interaction with MIRROR WR tracking\"\"\"\n    features = []\n    \n    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']), \n                                   desc=\"üèà Opponents\", leave=False):\n        last = group.sort_values('frame_id').groupby('nfl_id').last()\n        \n        if len(last) < 2:\n            continue\n            \n        positions = last[['x', 'y']].values\n        sides = last['player_side'].values\n        speeds = last['s'].values\n        directions = last['dir'].values\n        roles = last['player_role'].values\n        \n        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n        \n        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n            opp_mask = sides != side\n            \n            feat = {\n                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n                'mirror_wr_dist': 50.0,\n            }\n            \n            if not opp_mask.any():\n                features.append(feat)\n                continue\n            \n            opp_positions = positions[opp_mask]\n            distances = np.sqrt(((positions[i] - opp_positions)**2).sum(axis=1))\n            \n            if len(distances) == 0:\n                features.append(feat)\n                continue\n                \n            nearest_idx = distances.argmin()\n            feat['nearest_opp_dist'] = distances[nearest_idx]\n            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n            if role == 'Defensive Coverage' and receiver_mask.any():\n                rec_positions = positions[receiver_mask]\n                rec_distances = np.sqrt(((positions[i] - rec_positions)**2).sum(axis=1))\n                \n                if len(rec_distances) > 0:\n                    closest_rec_idx = rec_distances.argmin()\n                    rec_indices = np.where(receiver_mask)[0]\n                    actual_rec_idx = rec_indices[closest_rec_idx]\n                    \n                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n                    \n                    feat['mirror_wr_vx'] = rec_vx\n                    feat['mirror_wr_vy'] = rec_vy\n                    feat['mirror_wr_dist'] = rec_distances[closest_rec_idx]\n                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n            \n            features.append(feat)\n    \n    return pd.DataFrame(features)\n\ndef extract_route_patterns(input_df, kmeans=None, scaler=None, fit=True):\n    \"\"\"Route clustering\"\"\"\n    route_features = []\n    \n    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']), \n                                        desc=\"üõ£Ô∏è  Routes\", leave=False):\n        traj = group.sort_values('frame_id').tail(5)\n        \n        if len(traj) < 3:\n            continue\n        \n        positions = traj[['x', 'y']].values\n        speeds = traj['s'].values\n        \n        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 + \n                              (positions[-1, 1] - positions[0, 1])**2)\n        straightness = displacement / (total_dist + 0.1)\n        \n        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n        if len(angles) > 1:\n            angle_changes = np.abs(np.diff(angles))\n            max_turn = np.max(angle_changes)\n            mean_turn = np.mean(angle_changes)\n        else:\n            max_turn = mean_turn = 0\n        \n        speed_mean = speeds.mean()\n        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n        dx = positions[-1, 0] - positions[0, 0]\n        dy = positions[-1, 1] - positions[0, 1]\n        \n        route_features.append({\n            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n            'traj_straightness': straightness,\n            'traj_max_turn': max_turn,\n            'traj_mean_turn': mean_turn,\n            'traj_depth': abs(dx),\n            'traj_width': abs(dy),\n            'speed_mean': speed_mean,\n            'speed_change': speed_change,\n        })\n    \n    route_df = pd.DataFrame(route_features)\n    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n    X = route_df[feat_cols].fillna(0)\n    \n    if fit:\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n        kmeans = KMeans(n_clusters=Config.N_ROUTE_CLUSTERS, random_state=Config.SEED, n_init=10)\n        route_df['route_pattern'] = kmeans.fit_predict(X_scaled)\n        return route_df, kmeans, scaler\n    else:\n        X_scaled = scaler.transform(X)\n        route_df['route_pattern'] = kmeans.predict(X_scaled)\n        return route_df\n\ndef compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH, \n                                radius=Config.RADIUS, tau=Config.TAU):\n    \"\"\"graph features\"\"\"\n    print(\"üï∏Ô∏è  graph features...\")\n    \n    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\", \n                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n    src = input_df[cols_needed].copy()\n    \n    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n               .tail(1)\n               .rename(columns={\"frame_id\": \"last_frame_id\"})\n               .reset_index(drop=True))\n    \n    tmp = last.merge(\n        src.rename(columns={\n            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n            \"x\": \"x_nb\", \"y\": \"y_nb\", \n            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\", \n            \"player_side\": \"player_side_nb\"\n        }),\n        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n        how=\"left\"\n    )\n    \n    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n    \n    tmp = tmp[np.isfinite(tmp[\"dist\"]) & (tmp[\"dist\"] > 1e-6)]\n    if radius is not None:\n        tmp = tmp[tmp[\"dist\"] <= radius]\n    \n    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"] == tmp[\"player_side\"]).astype(np.float32)\n    \n    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n    if k_neigh is not None:\n        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n    \n    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n    \n    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n    \n    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n    \n    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n    \n    ag = tmp.groupby(keys).agg(\n        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n        # gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n    ).reset_index()\n    \n    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n    if len(near) > 0:\n        near[\"rnk\"] = near[\"rnk\"].astype(int)\n        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n        dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n        ag = ag.merge(dwide, on=keys, how=\"left\")\n    \n    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n              \"gnn_opp_dx_mean\",  \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\", \n              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n    \n    return ag\n\n# ============================================================================\n# SEQUENCE PREPARATION WITH GEOMETRIC FEATURES\n# ============================================================================\n    \ndef prepare_sequences_geometric(input_df, output_df=None, test_template=None, \n                                is_training=True, window_size=10,\n                                route_kmeans=None, route_scaler=None):\n    \"\"\"YOUR 154 features + 13 geometric features = 167 total\"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"PREPARING GEOMETRIC SEQUENCES\")\n    print(f\"{'='*80}\")\n    \n    input_df = input_df.copy()\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    input_df[\"frames_bin\"] = pd.qcut(input_df[\"num_frames_output\"], q=3, labels=False)\n    input_df[\"stratify_label\"] = (\n        input_df[\"player_role\"].astype(str) + \"_\" +\n        input_df[\"player_side\"].astype(str) + \"_\" +\n        input_df[\"play_direction\"].astype(str) + \"_\" +\n        input_df[\"frames_bin\"].astype(str)\n    )\n    print(\"Step 1: Base features...\")\n    \n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    input_df['velocity_x'] = input_df['s'] * np.sin(dir_rad)\n    input_df['velocity_y'] = input_df['s'] * np.cos(dir_rad)\n    input_df['acceleration_x'] = input_df['a'] * np.cos(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.sin(dir_rad)\n    input_df['speed_squared'] = input_df['s'] ** 2\n\n    input_df['momentum_x'] = input_df['velocity_x'] * input_df['player_weight']\n    input_df['momentum_y'] = input_df['velocity_y'] * input_df['player_weight']\n    input_df['kinetic_energy'] = 0.5 * input_df['player_weight'] * (input_df['s'] ** 2)\n    \n    input_df['orientation_diff'] = np.abs(input_df['o'] - input_df['dir'])\n    input_df['orientation_diff'] = np.minimum(input_df['orientation_diff'], 360 - input_df['orientation_diff'])\n    \n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    input_df['player_to_predict'] = input_df['player_to_predict'].astype(int)\n    \n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['dist_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['dist_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['dist_to_ball'] + 1e-6)\n        input_df['closing_speed_ball'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n        input_df['velocity_toward_ball'] = (\n            input_df['velocity_x'] * np.cos(input_df['angle_to_ball']) + \n            input_df['velocity_y'] * np.sin(input_df['angle_to_ball'])\n        )\n        input_df['velocity_alignment'] = np.cos(input_df['angle_to_ball'] - dir_rad)\n        input_df['angle_diff'] = np.abs(input_df['o'] - np.degrees(input_df['angle_to_ball']))\n        input_df['angle_diff'] = np.minimum(input_df['angle_diff'], 360 - input_df['angle_diff'])\n        input_df['dist_squared'] = input_df['dist_to_ball'] ** 2\n        input_df['is_out'] = ((input_df['ball_land_x'] < 0) | (input_df['ball_land_x'] > 120) |\n                      (input_df['ball_land_y'] < 0) | (input_df['ball_land_y'] > 53.3)).astype(int)\n\n    \n    print(\"Step 2: Advanced features...\")  \n    opp_features = get_opponent_features(input_df)\n    input_df = input_df.merge(opp_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    if is_training:\n        route_features, route_kmeans, route_scaler = extract_route_patterns(input_df)\n    else:\n        route_features = extract_route_patterns(input_df, route_kmeans, route_scaler, fit=False)\n    input_df = input_df.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    gnn_features = compute_neighbor_embeddings(input_df)\n    input_df = input_df.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    if 'nearest_opp_dist' in input_df.columns:\n        input_df['pressure'] = 1 / np.maximum(input_df['nearest_opp_dist'], 0.5)\n        # input_df['under_pressure'] = (input_df['nearest_opp_dist'] < 3).astype(int)\n        input_df['pressure_x_speed'] = input_df['pressure'] * input_df['s']\n    \n    if 'mirror_wr_vx' in input_df.columns:\n        input_df['mirror_offset_dist'] = np.sqrt(\n            input_df['mirror_offset_x']**2 + input_df['mirror_offset_y']**2\n        )\n    \n    print(\"Step 3: Temporal features...\")   \n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in input_df.columns:\n                input_df[f'{col}_lag{lag}'] = input_df.groupby(gcols)[col].shift(lag)\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in input_df.columns:\n                input_df[f'{col}_rolling_mean_{window}'] = (\n                    input_df.groupby(gcols)[col]\n                      .rolling(window, min_periods=1).mean()\n                      .reset_index(level=[0,1,2], drop=True)\n                )\n                input_df[f'{col}_rolling_std_{window}'] = (\n                    input_df.groupby(gcols)[col]\n                      .rolling(window, min_periods=1).std()\n                      .reset_index(level=[0,1,2], drop=True)\n                )\n    \n    for col in ['velocity_x', 'velocity_y']:\n        if col in input_df.columns:\n            input_df[f'{col}_delta'] = input_df.groupby(gcols)[col].diff()\n    \n    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n\n    \n    print(\"Step 4: Time features...\")\n    \n    if 'num_frames_output' in input_df.columns:\n        max_frames = input_df['num_frames_output']\n        \n        input_df['max_play_duration'] = max_frames / 10.0\n        input_df['frame_time'] = input_df['frame_id'] / 10.0\n        input_df['progress_ratio'] = input_df['frame_id'] / np.maximum(max_frames, 1)\n        input_df['time_remaining'] = (max_frames - input_df['frame_id']) / 10.0\n        \n        input_df['expected_x_at_ball'] = input_df['x'] + input_df['velocity_x'] * input_df['frame_time']\n        input_df['expected_y_at_ball'] = input_df['y'] + input_df['velocity_y'] * input_df['frame_time']\n        \n        if 'ball_land_x' in input_df.columns:\n            input_df['error_from_ball_x'] = input_df['expected_x_at_ball'] - input_df['ball_land_x']\n            input_df['error_from_ball_y'] = input_df['expected_y_at_ball'] - input_df['ball_land_y']\n            input_df['error_from_ball'] = np.sqrt(\n                input_df['error_from_ball_x']**2 + input_df['error_from_ball_y']**2\n            )\n            \n            input_df['weighted_dist_by_time'] = input_df['dist_to_ball'] / (input_df['frame_time'] + 0.1)\n            input_df['dist_scaled_by_progress'] = input_df['dist_to_ball'] * (1 - input_df['progress_ratio'])\n        \n\n        input_df['velocity_x_progress'] = input_df['velocity_x'] * input_df['progress_ratio']\n        input_df['velocity_y_progress'] = input_df['velocity_y'] * input_df['progress_ratio']\n        input_df['speed_scaled_by_time_left'] = input_df['s'] * input_df['time_remaining']\n        \n\n        input_df['length_ratio'] = max_frames / 30.0\n    \n    # üéØ THE BREAKTHROUGH: Add geometric features\n    print(\"Step 5: üéØ Geometric endpoint features...\")\n    input_df = add_geometric_features(input_df)\n    \n    print(\"Step 6: Building feature list...\")\n    \n    # Basic features\n    feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n        'player_height_feet', 'bmi',\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n         'orientation_diff',\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        'player_to_predict','is_out',\n        'speed_squared','dist_squared',\n        'dist_to_ball',  'angle_to_ball', \n        'ball_direction_x', 'ball_direction_y', 'closing_speed_ball',\n        'velocity_toward_ball', 'velocity_alignment', 'angle_diff',\n        'nearest_opp_dist',  'num_nearby_opp_3', 'num_nearby_opp_5',\n        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y',\n        'pressure',  'pressure_x_speed', \n        'mirror_offset_dist', 'mirror_alignment',\n        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n        'gnn_opp_dx_mean',  'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n        'gnn_ally_cnt', 'gnn_opp_cnt',\n        'gnn_ally_dmin', 'gnn_ally_dmean', 'gnn_opp_dmin', 'gnn_opp_dmean',\n        'gnn_d1', 'gnn_d2', 'gnn_d3',\n    ]\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            feature_cols.append(f'{col}_lag{lag}')\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            feature_cols.append(f'{col}_rolling_mean_{window}')\n            feature_cols.append(f'{col}_rolling_std_{window}')\n    \n    feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n    feature_cols.extend(['velocity_x_ema', 'velocity_y_ema'])    \n    feature_cols.extend([\n        'max_play_duration', 'frame_time', \n        'expected_x_at_ball', 'expected_y_at_ball', \n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n         'weighted_dist_by_time', \n        'velocity_x_progress', 'velocity_y_progress', 'dist_scaled_by_progress',\n        'speed_scaled_by_time_left', 'actual_play_length', 'length_ratio',\n    ])\n    \n    # üéØ Add 13 geometric features\n    feature_cols.extend([\n        'geo_endpoint_x', 'geo_endpoint_y',\n        'geo_vector_x', 'geo_vector_y', \n        'geo_required_vx', 'geo_required_vy',\n        'geo_velocity_error_x', 'geo_velocity_error_y', 'geo_velocity_error',\n        'geo_required_ax', 'geo_required_ay',\n        'geo_alignment',\n    ])\n    \n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"‚úì Using {len(feature_cols)} features ÔºÅÔºÅ\")\n    \n    print(\"Step 7: Creating sequences...\")\n\n    input_df.set_index(['game_id', 'play_id'], inplace=True)\n\n    # group plays\n    grouped = input_df.groupby(level=['game_id', 'play_id'])\n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id']].drop_duplicates()\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, stratify_labels = [], [], [], [], [], []\n    masks=[]\n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\",dynamic_ncols=True):\n        key = (row['game_id'], row['play_id'])\n        group_df = grouped.get_group(key)\n        nfls=list(group_df[group_df.player_to_predict ==1].nfl_id.unique())\n        nfls_bg=list(group_df[group_df.player_to_predict !=1].nfl_id.unique())\n        nfls_all=nfls+nfls_bg\n        num_players=len(nfls_all)\n        num = group_df['frame_id'].max()\n        start_id = max(1, num - window_size + 1)\n        input_window = group_df[group_df['frame_id'] >= start_id]\n        num_frames = len(input_window) // num_players\n        assert len(input_window) == num_frames * num_players, f\"{len(input_window)}!={num_frames}*{num_players}\"      \n        if num_frames < window_size:\n            padded_dfs=[]\n            pad_len = window_size - num_frames\n            for nfl in nfls_all:\n                group=group_df[group_df.nfl_id==nfl]\n                first_row =group[group['frame_id'] == 1]\n                pad_block = pd.concat([first_row] * pad_len, ignore_index=True)\n                padded_group = pd.concat([pad_block, group], ignore_index=True)\n                padded_dfs.append(padded_group)\n            input_window=pd.concat(padded_dfs, ignore_index=True)\n        else:\n            pass\n        \n        #input_window = input_window.fillna(group_df.mean(numeric_only=True)).infer_objects(copy=False)\n        seq = input_window[feature_cols].values\n        \n        seq_3d = seq.reshape(window_size, num_players, -1)\n        if num_players < Config.MAX_PLAYERS:\n            pad_players = Config.MAX_PLAYERS - num_players\n            last_player = seq_3d[:, -1:, :]                 # (T, 1, F)\n            padding = np.tile(last_player, (1, pad_players, 1))  # (T, pad, F)\n            seq_3d = np.concatenate([seq_3d, padding], axis=1)   # (T, MAX, F)\n        mask = np.zeros(Config.MAX_PLAYERS, dtype=np.int32)\n        mask[:num_players] = 1\n        stratify_label=input_window[\"stratify_label\"].values[0]\n        \n        if np.isnan(seq).any():\n            if is_training:\n                nan_cols = input_window[feature_cols].columns[input_window[feature_cols].isna().any()].tolist()\n                #print(\"features that contain NaN:\", nan_cols)\n                #continue\n            seq_3d = np.nan_to_num(seq_3d, nan=0.0)        \n        sequences.append(seq_3d)\n        masks.append(mask)\n        stratify_labels.append(stratify_label)\n        \n        if is_training:\n            dxs,dys=[],[]\n            for nfl in nfls:\n                input_window=group_df[group_df[\"nfl_id\"]==nfl]\n                out_grp = output_df[\n                    (output_df['game_id']==row['game_id']) &\n                    (output_df['play_id']==row['play_id']) &\n                    (output_df['nfl_id']==nfl)\n                ].sort_values('frame_id')\n            \n                last_x = input_window.iloc[-1]['x']\n                last_y = input_window.iloc[-1]['y']\n                \n                dx = out_grp['x'].values - last_x\n                dy = out_grp['y'].values - last_y\n                dxs.append(dx);dys.append(dy)\n            dxs=np.stack(dxs,axis=0)\n            dys=np.stack(dys,axis=0)\n            targets_dx.append(dxs)\n            targets_dy.append(dys)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n        \n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n        })\n    \n    print(f\"‚úì Created {len(sequences)} sequences\")\n    \n    if is_training:\n        return (sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, \n                 route_kmeans, route_scaler,feature_cols,stratify_labels,masks)\n    return sequences, sequence_ids,masks","metadata":{"_uuid":"9a10a3d3-9f73-48f7-bf44-cf6461571fdf","_cell_guid":"d2637050-2492-4cb9-8c2d-2580ba0bd4d1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MODEL ARCHITECTURE \n# ============================================================================\n\nclass STSeqModel(nn.Module):\n    \"\"\"\n    input:  \n        x -> (B, T, N, input_dim)\n        mask -> (B, N)   # True-> effective playersÔºåFalse -> filled players\n    output:  \n        out -> (B, N, H, 2)\n    structure:  \n        [Frame-Plalyer Mixture] -> TransformerEncoder -> Pool  -> predict x&y\n    \"\"\"\n    def __init__(\n        self,\n        input_dim: int,\n        horizon: int,\n        d_model: int = 128,\n        nhead: int = 4,\n        num_layers: int = 2,\n        ff_multiplier: int = 4,\n        dropout: float = 0.1,\n        max_len: int = 10,       # input window size\n        max_players: int = 17,    # input maxium players\n    ):\n        super().__init__()\n        self.horizon = horizon\n        self.d_model = d_model\n        self.max_players = max_players\n\n        # input projection: (input_dim ‚Üí d_model)\n        self.in_proj = nn.Linear(input_dim, d_model)\n\n        # learnable position embeddings\n        self.time_emb = nn.Parameter(torch.zeros(1, max_len, 1, d_model))\n        self.player_emb = nn.Parameter(torch.zeros(1, 1, max_players, d_model))\n        nn.init.trunc_normal_(self.time_emb, std=0.02)\n        nn.init.trunc_normal_(self.player_emb, std=0.02)\n\n        # Transformer encoder (Token ->T√óN )\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=ff_multiplier * d_model,\n            dropout=dropout,\n            batch_first=True,\n            activation=\"gelu\",\n            norm_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.post_ln = nn.LayerNorm(d_model)\n\n        # predict x&y for players\n        def make_head():\n            return nn.Sequential(\n                nn.Linear(d_model, d_model),\n                nn.GELU(),\n                nn.Dropout(0.2),\n                nn.Linear(d_model, horizon),\n            )\n        self.head_dx = make_head()\n        self.head_dy = make_head()\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n        \"\"\"\n        x: (B, T, N, input_dim)\n        mask: (B, N)  ‚Äî True -> effect players\n        return: out -> (B, M, horizon, 2)\n        \"\"\"\n        B, T, N, _ = x.shape\n\n        # Step 1. projection & position\n        h = self.in_proj(x)  # (B, T, N, d_model)\n        h=h+self.time_emb+self.player_emb\n\n        # Step 2. Flatten as Transformer input: (B, T*N, d_model)\n        h = h.reshape(B, T * N, self.d_model)\n\n        # Step 3. build padding mask (mask -> ~mask)\n        # expand mask to cover all frames\n        # mask_flat shape = (B, T*N)\n        mask_flat = (1-mask).unsqueeze(1).expand(-1, T, -1).reshape(B, T * N)\n\n        # Step 4. encoder\n        h = self.encoder(h, src_key_padding_mask=mask_flat)\n        h = self.post_ln(h)\n\n        # Step 5. reshape (B, T, N, d_model)\n        h = h.view(B, T, N, self.d_model)\n\n        # Step 6. pooling\n        # use last frame as the context\n        ctx = h[:, -1, :, :]   # (B, N, d_model)\n\n        # Step 7. prediction\n        dx = self.head_dx(ctx)              # (B, N, H)\n        dy = self.head_dy(ctx)              # (B, N, H)\n        dx = torch.cumsum(dx, dim=2)\n        dy = torch.cumsum(dy, dim=2)\n\n        # Step 8. to (B, N, H, 2)\n        out = torch.stack([dx, dy], dim=-1)\n        return out\n\nclass STTransformer(nn.Module):\n    \"\"\"\n    input:  \n        x -> (B, T, N, input_dim)\n        mask -> (B, N)   # True-> effective playersÔºåFalse -> filled players\n    output:  \n        out -> (B, N, H, 2)\n    structure:  \n        [Frame-Plalyer Mixture] -> TransformerEncoder -> Pool  -> predict x&y\n    \"\"\"\n    def __init__(\n        self,\n        input_dim: int,\n        horizon: int,\n        d_model: int = 128,\n        nhead: int = 4,\n        num_layers: int = 2,\n        num_decoder_layers: int = 2,  \n        ff_multiplier: int = 4,\n        dropout: float = 0.5,\n        max_len: int = 10,        # input window size\n        max_players: int = 17,    # input maxium players\n    ):\n        super().__init__()\n        self.horizon = horizon\n        self.d_model = d_model\n        self.out_players = Config.MAX_PLAYERS2Predict\n\n        # projection: (input_dim ‚Üí d_model)\n        self.in_proj = nn.Linear(input_dim, d_model)\n\n        # learnable position embeddings\n        self.time_emb = nn.Parameter(torch.zeros(1, max_len, 1, d_model))\n        self.player_emb = nn.Parameter(torch.zeros(1, 1, max_players, d_model))\n        nn.init.trunc_normal_(self.time_emb, std=0.02)\n        nn.init.trunc_normal_(self.player_emb, std=0.02)\n\n        # Transformer Encoder\n        enc_layer1 = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=ff_multiplier * d_model,\n            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n        )\n        self.spatial_encoder = nn.TransformerEncoder(enc_layer1, num_layers=num_layers)\n\n        enc_layer2 = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=ff_multiplier * d_model,\n            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n        )\n        self.temporal_encoder = nn.TransformerEncoder(enc_layer2, num_layers=num_layers)\n\n        # LayerNorms\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n        # ============  Decoder ============\n        # M-players H-horizon\n        self.query_embed = nn.Parameter(torch.zeros(self.out_players, horizon, d_model))\n        nn.init.trunc_normal_(self.query_embed, std=0.02)\n        \n        # fix dimension\n        self.time_emb_dec = nn.Parameter(torch.zeros(1, 1, horizon, d_model))\n        nn.init.trunc_normal_(self.time_emb_dec, std=0.02)\n\n        dec_layer = nn.TransformerDecoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=ff_multiplier * d_model,\n            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n        )\n        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=num_decoder_layers)\n\n        self.out_proj = nn.Linear(d_model, 2)\n        # ===================================================\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n        \"\"\"\n        x: (B, T, N, input_dim)\n        mask: (B, N)  ‚Äî True -> effect players\n        return: out -> (B, M, horizon, 2)\n        \"\"\"\n        B, T, N, _ = x.shape\n\n        # 1. projection\n        h = self.in_proj(x)  # (B, T, N, d_model)\n        h = h + self.player_emb[:, :, :N, :] + self.time_emb[:, :T, :, :]\n\n        # 2. spatial interaction between players\n        h_spatial = h.reshape(B * T, N, self.d_model)\n        spatial_mask_pad = ~mask.bool().unsqueeze(1).repeat(1, T, 1).reshape(B * T, N)\n        \n        h_spatial = self.spatial_encoder(h_spatial, src_key_padding_mask=spatial_mask_pad)\n        h_spatial = self.ln1(h_spatial)\n        h = h_spatial.reshape(B, T, N, self.d_model)\n\n        # 3. predict partial players\n        ctx = h[:, :, :self.out_players, :]  # (B, T, M, D)\n\n        # 4. temporal attention\n        ctx_temporal = ctx.permute(0, 2, 1, 3).reshape(B * self.out_players, T, self.d_model)\n        ctx_temporal = self.temporal_encoder(ctx_temporal)\n        ctx_temporal = self.ln2(ctx_temporal)\n        \n        # 5. prepare memory\n        memory = ctx_temporal  # (B*M, T, D)\n\n        # 6. prepare queries\n        # (M, H, D) -> (B, M, H, D)\n        queries = self.query_embed.unsqueeze(0).expand(B, -1, -1, -1)\n        queries = queries + self.time_emb_dec\n        \n        # (B, M, H, D) -> (B*M, H, D)\n        queries = queries.reshape(B * self.out_players, self.horizon, self.d_model)\n\n        # 7. causal mask\n        causal_mask = torch.triu(\n            torch.ones(self.horizon, self.horizon, dtype=torch.bool, device=x.device), \n            diagonal=1\n        )\n\n        # 8. Transformer Decoder\n        decoded = self.decoder(\n            tgt=queries,  # (B*M, H, D)\n            memory=memory,  # (B*M, T, D)\n            tgt_mask=causal_mask,  # (H, H)\n        )\n\n        # 9. output projection\n        # (B*M, H, D) -> (B, M, H, D)\n        decoded = decoded.reshape(B, self.out_players, self.horizon, self.d_model)\n        # (B, M, H, D) -> (B, M, H, 2)\n        out = self.out_proj(decoded)\n\n        # 10. cumsum\n        dx = out[..., 0]\n        dy = out[..., 1]\n        dx = torch.cumsum(dx, dim=2)\n        dy = torch.cumsum(dy, dim=2)\n        out = torch.stack([dx, dy], dim=-1)\n        return out\n        \n# ============================================================================\n# LOSS (YOUR PROVEN TEMPORAL HUBER)\n# ============================================================================\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n    \n    def forward(self, preds, target, masks):\n        '''\n        preds,targets-->(BS, num_players, num_frames, 2)\n        masks-->(BS, num_players, num_frames, 2)\n        '''\n        n=target.size(1)  # bs,num_players,horizon,2 \n        loss=0\n        for i in range(n):\n            mask=masks[:,i,:,:]\n            if mask.sum()<1:\n                continue\n            pred=preds[:,i,:,:]\n            err = pred - target[:,i,:,:]\n            abs_err = torch.abs(err)\n            huber = torch.where(abs_err <= self.delta, 0.5 * err * err, \n                               self.delta * (abs_err - 0.5 * self.delta))          \n            if self.time_decay > 0:\n                L = pred.size(1) # (B,i,Horizon,2)\n                t = torch.arange(L, device=pred.device).float()\n                weight = torch.exp(-self.time_decay * t).view(1, L, 1)\n                huber = huber * weight\n            loss+=(huber * mask).sum() / (mask.sum())\n        return loss","metadata":{"_uuid":"26bc24b5-ca2d-4573-9175-3c52f27a847d","_cell_guid":"9d3168c3-4b0e-4cdd-b98a-7439955a1804","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TRAINING\n# ============================================================================\ndef random_time_mask(bx, p=0.1, max_width=3):\n    if p <= 0 or max_width <= 0:\n        return bx\n    B, T, _ = bx.shape\n    if T <= 1:\n        return bx\n    for i in range(B):\n        if random.random() < p:\n            w = random.randint(1, max_width)\n            s = random.randint(0, max(0, T - 1 - w))\n            if s > 0:\n                bx[i, s:s+w] = bx[i, s-1].unsqueeze(0)\n            else:\n                bx[i, s:s+w] = bx[i, s+w].unsqueeze(0)\n    return bx\n\ndef flip_context_keep_last(bx, p=0.1):\n    if p <= 0:\n        return bx\n    B, T, _ = bx.shape\n    if T <= 1:\n        return bx\n    mask = torch.rand(B, device=bx.device) < p\n    if mask.any():\n        ctx = bx[mask, :-1].flip(1)\n        bx[mask] = torch.cat([ctx, bx[mask, -1:].clone()], dim=1)\n    return bx\n\ndef add_random_gaussian(bx, sigma_max=0.02):\n    if sigma_max <= 0:\n        return bx\n    sigma = sigma_max * torch.rand(1, device=bx.device)\n    return bx + torch.randn_like(bx) * sigma\n    \ndef prepare_targets(batch_dx, batch_dy, max_h):\n    B = len(batch_dx)\n    padded_dx = torch.zeros(B, Config.MAX_PLAYERS2Predict, max_h)\n    padded_dy = torch.zeros(B, Config.MAX_PLAYERS2Predict, max_h)\n    mask = torch.zeros(B, Config.MAX_PLAYERS2Predict, max_h, dtype=torch.bool)\n\n    for i, (dx, dy) in enumerate(zip(batch_dx, batch_dy)):\n        dx = torch.tensor(dx)  # (num_p, num_f)\n        dy = torch.tensor(dy)\n        \n        num_p, num_f = dx.shape       \n        padded_dx[i, :num_p, :num_f] = dx\n        padded_dy[i, :num_p, :num_f] = dy\n        \n        # ÁîüÊàê maskÔºöÁúüÂÆû‰ΩçÁΩÆ‰∏∫ 1\n        mask[i, :num_p, :num_f] = 1\n    targets = torch.stack([padded_dx, padded_dy],dim=-1)\n    mask = torch.stack([mask, mask],dim=-1)\n    return targets,mask\n\nclass WarmupCosineScheduler(LRScheduler):\n    def __init__(\n        self,\n        optimizer,\n        warmup_steps: int,\n        total_steps: int,\n        min_lr: float = 0.0,\n        last_epoch: int = -1,\n    ):\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self) -> List[float]:\n        if self.last_epoch < self.warmup_steps:\n            # Linear warmup\n            factor = self.last_epoch / max(1, self.warmup_steps)\n            return [base_lr * factor for base_lr in self.base_lrs]\n        else:\n            # Cosine annealing\n            progress = (self.last_epoch - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n            progress = min(progress, 1.0)\n            cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n            return [self.min_lr + (base_lr - self.min_lr) * cosine for base_lr in self.base_lrs]\n        \ndef train_model(X_train, y_train_dx, y_train_dy,mask_tr, X_val, y_val_dx, y_val_dy, mask_va,\n                input_dim, horizon, config):\n\n    if Config.Model_Name==\"STTransformer\":\n        model = STTransformer(input_dim, horizon,Config.DMODEL).to(DEVICE)\n    elif Config.Model_Name==\"STSeqModel\":\n        model = STSeqModel(input_dim, horizon,Config.DMODEL).to(DEVICE)\n    else:\n        raise NotImplementedError\n    \n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    if Config.scheduler==\"ReduceLROnPlateau\":\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5,)\n    elif Config.scheduler==\"WarmupCosineScheduler\":\n        WARMUP_STEPS = 1000\n        TOTAL_STEPS = 50000\n        scheduler = WarmupCosineScheduler(\n            optimizer,\n            warmup_steps=WARMUP_STEPS,\n            total_steps=TOTAL_STEPS,\n            min_lr=1e-6 \n        )\n    else:\n        raise NotImplementedError\n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        bmask = torch.tensor(np.stack(mask_tr[i:end]).astype(np.float32))\n        by, bm = prepare_targets(\n            [y_train_dx[j] for j in range(i, end)],\n            [y_train_dy[j] for j in range(i, end)],\n            horizon\n        )\n        train_batches.append((bx,bmask, by, bm))\n    \n    val_batches = []\n    for i in range(0, len(X_val), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_val))\n        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        bmask = torch.tensor(np.stack(mask_va[i:end]).astype(np.float32))\n        by, bm = prepare_targets(\n            [y_val_dx[j] for j in range(i, end)],\n            [y_val_dy[j] for j in range(i, end)],\n            horizon\n        )\n        val_batches.append((bx,bmask, by, bm))\n    \n    best_loss, best_state, bad = float('inf'), None, 0   \n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for bx,bmask, by, bm in train_batches:\n            bx,bmask, by, bm = bx.to(DEVICE),bmask.to(DEVICE), by.to(DEVICE), bm.to(DEVICE)\n            if Config.AUG and random.random()<0.2:\n                bx = add_random_gaussian(bx, sigma_max=0.01)  \n                bx = random_time_mask(bx, p=0.10, max_width=3)   \n                bx = flip_context_keep_last(bx, p=0.10)  \n            pred = model(bx,bmask)\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n        \n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx,bmask, by, bm in val_batches:\n                bx,bmask, by, bm = bx.to(DEVICE),bmask.to(DEVICE), by.to(DEVICE), bm.to(DEVICE)\n                pred = model(bx,bmask)\n                val_losses.append(criterion(pred, by, bm).item())\n        \n        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n        scheduler.step(val_loss)\n        \n        if epoch % 10 == 0 :\n            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0           \n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n    \n    if best_state:\n        model.load_state_dict(best_state)\n    \n    return model, val_batches\n\n# ============================================================================\n# MAIN\n# ============================================================================\ndef evaluate(model,val_batches):\n    model.eval()\n    with torch.inference_mode():\n        # Iterate through validation batches\n        err=0\n        num=0\n\n        for bx,bmask, by, bm in val_batches:\n            num+=bm.sum()\n            bx,bmask,by,bm = bx.to(DEVICE),bmask.to(DEVICE), by.to(DEVICE), bm.to(DEVICE)\n            pred = model(bx,bmask)\n\n            n=by.size(1)\n            pred=pred[:,:n,:,:]\n            err+=torch.sum((bm*(pred - by))**2)\n\n        #num*=2\n    return err,num","metadata":{"_uuid":"9c52079a-e715-41a8-8617-8d3bf1d8718a","_cell_guid":"e6a101e5-ddb9-4ae8-bbda-669bdf4bc286","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    config = Config()    \n    set_seed(Config.SEED)\n    # Load\n    print(\"[1/4] Loading data...\")\n    \n    if Config.DEBUG:\n        train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 2)]\n        train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1,2)] \n    else:\n        train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n        train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1,19)]\n    train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])   \n    \n    \n    train_input = train_input[(train_input.game_id!=2023091100)&(train_input.play_id!=3167)]\n    train_output = train_output[(train_output.game_id!=2023091100)&(train_output.play_id!=3167)]\n    print(f\"‚úì Train input: {train_input.shape}, Train output: {train_output.shape}\")\n    \n    # Prepare\n    print(\"[2/4] Preparing geometric sequences...\")\n    \n    result = prepare_sequences_geometric(\n        train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n    )\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, route_kmeans, route_scaler,feature_cols,stratify_labels,masks  = result\n    \n    sequences = list(sequences)\n    targets_dx = list(targets_dx)\n    targets_dy = list(targets_dy)\n    \n    # Train\n    print(\"[3/4] Training geometric models...\")\n    groups = np.array([d['game_id'] for d in sequence_ids])\n    if Config.Split==\"gkf\":\n        gkf = GroupKFold(n_splits=config.N_FOLDS)\n        splits=gkf.split(sequences, groups=groups)\n    else:\n        sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=12)\n        splits=sgkf.split(sequences, y=stratify_labels, groups=groups)\n    \n    models, scalers = [], []\n    cv_result=[]\n    cv_err,cv_num=0,0\n    for fold, (tr, va) in enumerate(splits, 1):\n        print(f\"{'='*60}\")\n        print(f\"Fold {fold}/{config.N_FOLDS}\")     \n        X_tr = np.array([sequences[i] for i in tr])\n        X_va = np.array([sequences[i] for i in va])\n        y_tr_dx = [targets_dx[i] for i in tr]\n        y_va_dx = [targets_dx[i] for i in va]\n        y_tr_dy = [targets_dy[i] for i in tr]\n        y_va_dy = [targets_dy[i] for i in va]\n        mask_tr =[masks[i] for i in tr]\n        mask_va =[masks[i] for i in va]\n        scaler = StandardScaler()\n        n_samples_tr = X_tr.shape[0] * X_tr.shape[1] * X_tr.shape[2]\n        n_samples_va = X_va.shape[0] * X_va.shape[1] * X_va.shape[2]\n        X_tr_reshaped = X_tr.reshape(n_samples_tr, -1)\n        X_va_reshaped = X_va.reshape(n_samples_va, -1)\n        scaler.fit(X_tr_reshaped)      \n        X_tr_scaled_reshaped = scaler.transform(X_tr_reshaped)\n        X_tr_sc = X_tr_scaled_reshaped.reshape(X_tr.shape)\n        X_va_scaled_reshaped = scaler.transform(X_va_reshaped)\n        X_va_sc = X_va_scaled_reshaped.reshape(X_va.shape)\n        input_dim=X_tr[0].shape[-1]\n        print(\"input_dim:\",input_dim)\n        model, val_batches = train_model(\n            X_tr_sc, y_tr_dx, y_tr_dy,mask_tr,\n            X_va_sc, y_va_dx, y_va_dy,mask_va,\n            input_dim, config.MAX_FUTURE_HORIZON, config\n        )\n        err,num=evaluate(model,val_batches)\n        rsme=torch.sqrt(err/num).item()\n        cv_result.append(rsme)\n        print(f\"fold_{fold}:{rsme}\")\n        cv_err+=err\n        cv_num+=num\n        models.append(model)\n        scalers.append(scaler)\n        torch.save(model.state_dict(), f'{Config.OUTPUT_DIR}/model_{fold}.pth')\n        torch.save(scaler, f'{Config.OUTPUT_DIR}/scaler_{fold}.pth')\n    cv_rsme=torch.sqrt(cv_err/cv_num).item()\n    cv_results={\"cv_rsme\":cv_rsme,\"avg\":np.mean(cv_result),\"std\":np.std(cv_result),\n        \"folds\":cv_result}\n    print(\"CV Results:\"+\"-+\"*30)\n    print(cv_results)","metadata":{"_uuid":"d57e7fd8-399b-40d0-8457-332c75bd7de2","_cell_guid":"8ee947b2-3921-480a-93a7-12398bf396fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main()","metadata":{"_uuid":"f940dd8e-47b0-493b-8a9d-ea2df517305d","_cell_guid":"efe958cb-3345-47e5-ab5d-52e59a401540","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}