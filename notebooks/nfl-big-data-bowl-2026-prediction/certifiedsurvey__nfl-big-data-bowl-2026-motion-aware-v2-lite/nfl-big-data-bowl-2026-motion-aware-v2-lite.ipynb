{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# NFL Big Data Bowl 2026 ‚Äî Daniel GPU V10.3 (Hybrid Directional Blend)\n#  - Base features: physics + dual rolling (3,7) + QB/Ball relative + bearings\n#  - Two target streams:\n#       ABS: dx = target_x - x_last, dy = target_y - y_last\n#       DIR: dx_dir = dx * sign(play_direction), dy_dir = dy * sign(play_direction)\n#  - Base models (GPU): LGB / XGB / CAT (per stream)\n#  - Meta per stream: Ridge(positive, Œ± tuned) + LGB (nonlinear)\n#  - Final: learn non-negative blend ABS vs DIR using OOF (after inverting DIR‚ÜíABS)\n#  - Role-bias on the final blended OOF (absolute space)\n#  - Output: /kaggle/working/submission.csv\n# ============================================================\n\nimport os, gc, pickle, warnings\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----------------------------\n# Config\n# ----------------------------\nclass Cfg:\n    DATA_DIR  = \"/kaggle/input/nfl-big-data-bowl-2026-prediction/\"\n    OUT_DIR   = \"/kaggle/working/daniel-gpu-v10_3/\"\n    MODEL_DIR = os.path.join(\"/kaggle/working/daniel-gpu-v10_3/\", \"models\")\n    SEED      = 1339\n    N_FOLDS   = 5\n    USE_GROUP_KF = True\n    ROLE_BIAS_SHRINK = 50.0     # shrink group residuals toward global mean\n    META_COMPARE = True          # print meta OOFs for each stream\n\nos.makedirs(Cfg.OUT_DIR, exist_ok=True)\nos.makedirs(Cfg.MODEL_DIR, exist_ok=True)\n\ndef seed_everything(seed=1339):\n    import random\n    random.seed(seed); np.random.seed(seed)\nseed_everything(Cfg.SEED)\n\ndef rmse(a,b): return float(np.sqrt(mean_squared_error(a,b)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:57:55.147021Z","iopub.execute_input":"2025-10-27T08:57:55.147275Z","iopub.status.idle":"2025-10-27T08:58:00.859933Z","shell.execute_reply.started":"2025-10-27T08:57:55.147254Z","shell.execute_reply":"2025-10-27T08:58:00.859356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Helpers\n# ============================================================\ndef detect_target_cols(df_out_pd: pd.DataFrame):\n    cols = set(df_out_pd.columns)\n    if {\"target_x\",\"target_y\"}.issubset(cols): return \"target_x\",\"target_y\"\n    if {\"ball_land_x\",\"ball_land_y\"}.issubset(cols): return \"ball_land_x\",\"ball_land_y\"\n    if {\"x\",\"y\"}.issubset(cols): return \"x\",\"y\"\n    raise RuntimeError(f\"Could not detect targets in output; columns={list(cols)[:20]}\")\n\ndef to_seconds_frames(df: pl.DataFrame) -> pl.DataFrame:\n    if \"frame_offset\" not in df.columns:\n        df = df.with_columns(pl.col(\"frame_id\").cast(pl.Float64).alias(\"frame_offset\"))\n    if \"num_frames_output\" in df.columns:\n        df = df.with_columns(pl.col(\"num_frames_output\").cast(pl.Float64).clip(1.0,None).alias(\"T\"))\n    else:\n        df = df.with_columns(pl.lit(5.0).alias(\"T\"))\n    return df.with_columns([\n        (pl.col(\"frame_offset\")/10.0).alias(\"time_offset\"),\n        (pl.col(\"frame_offset\")/pl.col(\"T\")).alias(\"frame_ratio\"),\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:58:00.861016Z","iopub.execute_input":"2025-10-27T08:58:00.861576Z","iopub.status.idle":"2025-10-27T08:58:00.867506Z","shell.execute_reply.started":"2025-10-27T08:58:00.861551Z","shell.execute_reply":"2025-10-27T08:58:00.866801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Feature Engineering\n# ============================================================\ndef derive_temporal_features(df: pl.DataFrame) -> pl.DataFrame:\n    if {\"x\",\"y\"}.issubset(df.columns):\n        df = df.with_columns([\n            pl.col(\"x\").diff().over([\"game_id\",\"play_id\",\"nfl_id\"]).alias(\"dx_frame\"),\n            pl.col(\"y\").diff().over([\"game_id\",\"play_id\",\"nfl_id\"]).alias(\"dy_frame\"),\n        ])\n        df = df.with_columns([\n            (pl.col(\"dx_frame\")*10.0).alias(\"speed_x_est\"),\n            (pl.col(\"dy_frame\")*10.0).alias(\"speed_y_est\"),\n        ])\n        df = df.with_columns([\n            pl.col(\"speed_x_est\").diff().over([\"game_id\",\"play_id\",\"nfl_id\"]).alias(\"accel_x_est\"),\n            pl.col(\"speed_y_est\").diff().over([\"game_id\",\"play_id\",\"nfl_id\"]).alias(\"accel_y_est\"),\n        ])\n        for c in [\"dx_frame\",\"dy_frame\",\"speed_x_est\",\"speed_y_est\",\"accel_x_est\",\"accel_y_est\"]:\n            df = df.with_columns(pl.col(c).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\"))\n        df = df.with_columns([\n            ( (pl.col(\"speed_x_est\")**2 + pl.col(\"speed_y_est\")**2).sqrt() ).alias(\"v_mag\"),\n            ( (pl.col(\"accel_x_est\")**2 + pl.col(\"accel_y_est\")**2).sqrt() ).alias(\"a_mag_est\"),\n        ])\n    return df\n\ndef add_dual_rolling(df: pl.DataFrame, ks=(3,7)) -> pl.DataFrame:\n    df = df.sort([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"])\n    base = [c for c in [\"x\",\"y\",\"s\",\"a\",\"dir\",\"o\",\"v_mag\",\"a_mag_est\"] if c in df.columns]\n    if not base: return df\n    for k in ks:\n        df = df.with_columns([\n            pl.col(c).rolling_mean(window_size=k, min_periods=1)\n                     .over([\"game_id\",\"play_id\",\"nfl_id\"]).alias(f\"{c}_roll{k}\") for c in base\n        ])\n    return df\n\ndef aggregate_per_player(df: pl.DataFrame) -> pl.DataFrame:\n    if \"s\" in df.columns and \"player_weight\" in df.columns:\n        df = df.with_columns((pl.col(\"s\") * pl.col(\"player_weight\")).alias(\"momentum\"))\n    if \"a\" in df.columns and \"s\" in df.columns:\n        df = df.with_columns((pl.col(\"a\") / (pl.col(\"s\") + 1e-3)).alias(\"accel_ratio\"))\n    if {\"dir\",\"o\"}.issubset(df.columns):\n        df = df.with_columns((pl.col(\"dir\") - pl.col(\"o\")).abs().alias(\"dir_diff\"))\n    if \"dir\" in df.columns:\n        df = df.with_columns(\n            pl.col(\"dir\").diff().over([\"game_id\",\"play_id\",\"nfl_id\"]).abs().alias(\"angular_vel\")\n        ).with_columns(pl.col(\"angular_vel\").fill_null(0.0))\n    if {\"s\",\"player_weight\"}.issubset(df.columns):\n        df = df.with_columns(0.5 * (pl.col(\"player_weight\") * (pl.col(\"s\")**2)).alias(\"kinetic_energy\"))\n    if \"absolute_yardline_number\" in df.columns:\n        df = df.with_columns((pl.col(\"absolute_yardline_number\")/100.0).alias(\"yard_norm\"))\n    if \"player_side\" in df.columns:\n        df = df.with_columns((pl.col(\"player_side\")==\"left\").cast(pl.Int8).alias(\"side_left\"))\n\n    df = derive_temporal_features(df)\n    df = add_dual_rolling(df, ks=(3,7))\n\n    feats = [\"x\",\"y\",\"s\",\"a\",\"o\",\"dir\",\n             \"dx_frame\",\"dy_frame\",\"speed_x_est\",\"speed_y_est\",\n             \"accel_x_est\",\"accel_y_est\",\"v_mag\",\"a_mag_est\",\n             \"x_roll3\",\"y_roll3\",\"s_roll3\",\"a_roll3\",\"dir_roll3\",\"o_roll3\",\"v_mag_roll3\",\"a_mag_est_roll3\",\n             \"x_roll7\",\"y_roll7\",\"s_roll7\",\"a_roll7\",\"dir_roll7\",\"o_roll7\",\"v_mag_roll7\",\"a_mag_est_roll7\",\n             \"momentum\",\"accel_ratio\",\"dir_diff\",\"angular_vel\",\"kinetic_energy\",\n             \"yard_norm\",\"side_left\"]\n    feats = [f for f in feats if f in df.columns]\n\n    agg_exprs = [pl.count().alias(\"num_frames\"), pl.col(\"frame_id\").max().alias(\"max_frame_id\")]\n    for f in feats:\n        agg_exprs += [pl.col(f).mean().alias(f\"{f}_mean\"),\n                      pl.col(f).std().alias(f\"{f}_std\"),\n                      pl.col(f).min().alias(f\"{f}_min\"),\n                      pl.col(f).max().alias(f\"{f}_max\")]\n    out = df.group_by([\"game_id\",\"play_id\",\"nfl_id\"]).agg(agg_exprs)\n\n    last_xy = (df.group_by([\"game_id\",\"play_id\",\"nfl_id\"]).tail(1)\n                 .select([\"game_id\",\"play_id\",\"nfl_id\",\"x\",\"y\"])\n                 .rename({\"x\":\"x_last\",\"y\":\"y_last\"}))\n    return out.join(last_xy, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n\ndef qb_last_xy(df: pl.DataFrame) -> pl.DataFrame:\n    has_pos = \"player_position\" in df.columns\n    has_role = \"player_role\" in df.columns\n    if has_pos and has_role:\n        filt = (pl.col(\"player_position\")==\"QB\") | (pl.col(\"player_role\")==\"passer\")\n    elif has_pos:\n        filt = (pl.col(\"player_position\")==\"QB\")\n    elif has_role:\n        filt = (pl.col(\"player_role\")==\"passer\")\n    else:\n        return pl.DataFrame({\"game_id\":pl.Series([], pl.Int64),\n                             \"play_id\":pl.Series([], pl.Int64),\n                             \"qb_x_last\":pl.Series([], pl.Float64),\n                             \"qb_y_last\":pl.Series([], pl.Float64)})\n    qb = df.filter(filt)\n    if qb.height == 0:\n        return pl.DataFrame({\"game_id\":pl.Series([], pl.Int64),\n                             \"play_id\":pl.Series([], pl.Int64),\n                             \"qb_x_last\":pl.Series([], pl.Float64),\n                             \"qb_y_last\":pl.Series([], pl.Float64)})\n    qb_last = (qb.group_by([\"game_id\",\"play_id\",\"nfl_id\"]).tail(1)\n                 .select([\"game_id\",\"play_id\",\"x\",\"y\"])\n                 .group_by([\"game_id\",\"play_id\"])\n                 .agg([pl.col(\"x\").mean().alias(\"qb_x_last\"),\n                       pl.col(\"y\").mean().alias(\"qb_y_last\")]))\n    return qb_last\n\ndef const_first_per_player(df: pl.DataFrame) -> pl.DataFrame:\n    const_cols = [\"game_id\",\"play_id\",\"nfl_id\",\"absolute_yardline_number\",\"player_weight\",\n                  \"player_position\",\"player_role\",\"player_side\",\"play_direction\",\n                  \"ball_land_x\",\"ball_land_y\",\"num_frames_output\"]\n    keep = [c for c in const_cols if c in df.columns]\n    return df.group_by([\"game_id\",\"play_id\",\"nfl_id\"]).first().select(keep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:58:00.868208Z","iopub.execute_input":"2025-10-27T08:58:00.868487Z","iopub.status.idle":"2025-10-27T08:58:00.99222Z","shell.execute_reply.started":"2025-10-27T08:58:00.868469Z","shell.execute_reply":"2025-10-27T08:58:00.99137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Load Train Data\n# ============================================================\nprint(\"Loading training data with overlap filtering ...\")\ntrain_dir = os.path.join(Cfg.DATA_DIR, \"train\")\ninputs, outputs = [], []\n\nfor w in range(1,19):\n    fi = f\"{train_dir}/input_2023_w{w:02d}.csv\"\n    fo = f\"{train_dir}/output_2023_w{w:02d}.csv\"\n    if not (os.path.exists(fi) and os.path.exists(fo)):\n        print(f\"Week {w:02d}: missing files ‚Äì skip\")\n        continue\n\n    df_i, df_o = pl.read_csv(fi), pd.read_csv(fo)\n    if \"nflId\" in df_i.columns: df_i = df_i.rename({\"nflId\":\"nfl_id\"})\n    if \"nflId\" in df_o.columns: df_o = df_o.rename(columns={\"nflId\":\"nfl_id\"})\n\n    gi_in, gi_out = set(df_i[\"game_id\"].unique()), set(df_o[\"game_id\"].unique())\n    common = gi_in & gi_out\n    if not common:\n        print(f\"Week {w:02d}: no common game_id ‚Äì skip\")\n        continue\n\n    df_i = df_i.filter(pl.col(\"game_id\").is_in(list(common)))\n    df_o = df_o[df_o[\"game_id\"].isin(common)]\n    inputs.append(df_i); outputs.append(df_o)\n    print(f\"Week {w:02d}: input {df_i.shape}, output {df_o.shape}, common={len(common)}\")\n\nif not inputs or not outputs:\n    raise RuntimeError(\"No overlapping weeks found. Check dataset path/files.\")\ndf_in, df_out = pl.concat(inputs), pd.concat(outputs, ignore_index=True)\nprint(f\"‚úÖ Loaded input: {df_in.shape}, output: {df_out.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:58:00.993102Z","iopub.execute_input":"2025-10-27T08:58:00.993339Z","iopub.status.idle":"2025-10-27T08:58:09.623958Z","shell.execute_reply.started":"2025-10-27T08:58:00.993321Z","shell.execute_reply":"2025-10-27T08:58:09.623168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Build Training Table + Features\n# ============================================================\nconst_part = const_first_per_player(df_in)\nagg_feats  = aggregate_per_player(df_in)\nqb_last_tr = qb_last_xy(df_in)\n\ntgt_x, tgt_y = detect_target_cols(df_out)\ndf_out_pl = pl.from_pandas(df_out)\nframe_template = df_out_pl.select([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\",tgt_x,tgt_y]) \\\n                          .rename({tgt_x:\"target_x\",tgt_y:\"target_y\"})\nframe_template = to_seconds_frames(frame_template)\n\nfor c in [\"game_id\",\"play_id\",\"nfl_id\"]:\n    agg_feats      = agg_feats.with_columns(pl.col(c).cast(pl.Int64))\n    const_part     = const_part.with_columns(pl.col(c).cast(pl.Int64))\n    frame_template = frame_template.with_columns(pl.col(c).cast(pl.Int64))\nqb_last_tr = qb_last_tr.with_columns(pl.col(\"game_id\").cast(pl.Int64),\n                                     pl.col(\"play_id\").cast(pl.Int64))\n\nfeat_train = (frame_template\n              .join(agg_feats,  on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"inner\")\n              .join(const_part, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n              .join(qb_last_tr,  on=[\"game_id\",\"play_id\"],        how=\"left\"))\nprint(f\"‚úÖ Joined features shape: {feat_train.shape}\")\nif feat_train.height == 0:\n    raise RuntimeError(\"Joined DataFrame empty after filtering.\")\n\npdf = feat_train.to_pandas()\n\n# Targets\nif not {\"x_last\",\"y_last\"}.issubset(pdf.columns):\n    raise RuntimeError(\"x_last/y_last missing after aggregation.\")\npdf[\"dx\"] = pdf[\"target_x\"] - pdf[\"x_last\"]\npdf[\"dy\"] = pdf[\"target_y\"] - pdf[\"y_last\"]\n\n# Direction sign: left ‚Üí -1, else +1\nplay_dir_sign = np.where(pdf.get(\"play_direction\", \"right\")==\"left\", -1.0, 1.0)\npdf[\"dx_dir\"] = pdf[\"dx\"] * play_dir_sign\npdf[\"dy_dir\"] = pdf[\"dy\"] * play_dir_sign\n\n# Role for calibration\npdf[\"role_raw\"] = pdf.get(\"player_role\", pd.Series([\"UNK\"]*len(pdf))).fillna(\"UNK\").astype(str)\n\n# QB-relative + bearings\npdf[\"qb_x_last\"] = pdf[\"qb_x_last\"].fillna(pdf[\"x_last\"])\npdf[\"qb_y_last\"] = pdf[\"qb_y_last\"].fillna(pdf[\"y_last\"])\npdf[\"rel_x_last_qb\"] = pdf[\"x_last\"] - pdf[\"qb_x_last\"]\npdf[\"rel_y_last_qb\"] = pdf[\"y_last\"] - pdf[\"qb_y_last\"]\npdf[\"dist_to_qb\"] = np.sqrt(pdf[\"rel_x_last_qb\"]**2 + pdf[\"rel_y_last_qb\"]**2)\npdf[\"ang_to_qb\"]  = np.arctan2(pdf[\"rel_y_last_qb\"], pdf[\"rel_x_last_qb\"])\npdf[\"bearing_qb\"] = np.degrees(pdf[\"ang_to_qb\"])\n\n# Ball-relative (+ bearings) if present\nif {\"ball_land_x\",\"ball_land_y\"}.issubset(set(pdf.columns)):\n    pdf[\"rel_x_last_ball\"] = pdf[\"x_last\"] - pdf[\"ball_land_x\"]\n    pdf[\"rel_y_last_ball\"] = pdf[\"y_last\"] - pdf[\"ball_land_y\"]\n    pdf[\"dist_to_ball\"]    = np.sqrt(pdf[\"rel_x_last_ball\"]**2 + pdf[\"rel_y_last_ball\"]**2)\n    pdf[\"ang_to_ball\"]     = np.arctan2(pdf[\"rel_y_last_ball\"], pdf[\"rel_x_last_ball\"])\n    pdf[\"bearing_ball\"]    = np.degrees(pdf[\"ang_to_ball\"])\nelse:\n    for c in [\"rel_x_last_ball\",\"rel_y_last_ball\",\"dist_to_ball\",\"ang_to_ball\",\"bearing_ball\"]:\n        pdf[c] = 0.0\n\n# Context interactions\npdf[\"play_dir_sign_num\"] = play_dir_sign\npdf[\"yard_norm\"] = pdf.get(\"yard_norm\", pdf.get(\"absolute_yardline_number\", 0.0)/100.0)\npdf[\"yard_dir_ctx\"]  = pdf[\"yard_norm\"] * pdf[\"play_dir_sign_num\"]\npdf[\"fratio_ball_ctx\"] = pdf.get(\"frame_ratio\", 0.0) * pdf[\"dist_to_ball\"]\n\n# One-hots\ncat_cols = [c for c in [\"player_position\",\"player_role\",\"play_direction\"] if c in pdf.columns]\nif cat_cols:\n    print(f\"Encoding categorical columns: {cat_cols}\")\n    pdf = pd.get_dummies(pdf, columns=cat_cols, drop_first=True)\n\n# Feature list\ndrop_cols = {'game_id','play_id','nfl_id','frame_id',\n             'player_position','player_role','player_side','play_direction',\n             'target_x','target_y','dx','dy','dx_dir','dy_dir','role_raw'}\nfeat_cols = [c for c in pdf.columns if c not in drop_cols and str(pdf[c].dtype) in ['float64','float32','int64','int32']]\n\n# Standardize numeric features\nscaler = StandardScaler()\npdf[feat_cols] = scaler.fit_transform(pdf[feat_cols])\nwith open(os.path.join(Cfg.MODEL_DIR, \"feature_scaler.pkl\"), \"wb\") as f:\n    pickle.dump({\"feat_cols\": feat_cols, \"scaler\": scaler}, f)\nprint(f\"Standardized {len(feat_cols)} features.\")\n\n# Train arrays\nX        = pdf[feat_cols].values\ny_abs_dx = pdf[\"dx\"].values\ny_abs_dy = pdf[\"dy\"].values\ny_dir_dx = pdf[\"dx_dir\"].values\ny_dir_dy = pdf[\"dy_dir\"].values\nrole_raw_tr = pdf[\"role_raw\"].values\n\nprint(f\"‚úÖ Training rows: {len(pdf):,} | #Features: {len(feat_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:58:09.625881Z","iopub.execute_input":"2025-10-27T08:58:09.626109Z","iopub.status.idle":"2025-10-27T08:58:28.139633Z","shell.execute_reply.started":"2025-10-27T08:58:09.626091Z","shell.execute_reply":"2025-10-27T08:58:28.138889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Base Model Params (GPU)\n# ============================================================\nLGB_PARAMS = dict(objective=\"regression\", metric=\"rmse\", boosting_type=\"gbdt\", device=\"gpu\",\n                  gpu_platform_id=0, gpu_device_id=0, n_estimators=1600, learning_rate=0.0115,\n                  num_leaves=64, subsample=0.9, colsample_bytree=0.9, min_data_in_leaf=20,\n                  lambda_l1=1.0, lambda_l2=1.0, random_state=Cfg.SEED, verbose=-1)\nXGB_PARAMS = dict(objective=\"reg:squarederror\", eval_metric=\"rmse\", n_estimators=1700,\n                  learning_rate=0.011, max_depth=7, colsample_bytree=0.9, subsample=0.9,\n                  gamma=0.05, reg_alpha=0.5, reg_lambda=1.0, random_state=Cfg.SEED,\n                  tree_method=\"gpu_hist\", predictor=\"gpu_predictor\", verbosity=0)\nCAT_PARAMS = dict(loss_function=\"RMSE\", eval_metric=\"RMSE\", iterations=1500, learning_rate=0.012,\n                  depth=6, l2_leaf_reg=3.0, random_seed=Cfg.SEED, task_type=\"GPU\", devices=\"0\", verbose=False)\n\n# Meta LGB (stronger for angles)\nMETA_LGB_PARAMS = dict(objective=\"regression\", metric=\"rmse\", boosting_type=\"gbdt\", device=\"gpu\",\n                       gpu_platform_id=0, gpu_device_id=0, n_estimators=400, learning_rate=0.04,\n                       num_leaves=25, max_depth=4, subsample=1.0, colsample_bytree=1.0,\n                       min_data_in_leaf=20, lambda_l1=0.0, lambda_l2=0.0, random_state=Cfg.SEED, verbose=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:58:28.140342Z","iopub.execute_input":"2025-10-27T08:58:28.140593Z","iopub.status.idle":"2025-10-27T08:58:28.146655Z","shell.execute_reply.started":"2025-10-27T08:58:28.140575Z","shell.execute_reply":"2025-10-27T08:58:28.146054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Cross-Validation with OOF capture (ABS stream)\n# ============================================================\nif Cfg.USE_GROUP_KF:\n    groups = pdf[\"game_id\"].astype(str) + \"_\" + pdf[\"play_id\"].astype(str)\n    cv_iter = list(GroupKFold(n_splits=Cfg.N_FOLDS).split(pdf, groups=groups))\nelse:\n    cv_iter = list(GroupKFold(n_splits=Cfg.N_FOLDS).split(pdf, groups=pdf[\"game_id\"]))\n\noof_abs_dx_lgb = np.zeros_like(y_abs_dx)\noof_abs_dx_xgb = np.zeros_like(y_abs_dx)\noof_abs_dx_cat = np.zeros_like(y_abs_dx)\noof_abs_dy_lgb = np.zeros_like(y_abs_dy)\noof_abs_dy_xgb = np.zeros_like(y_abs_dy)\noof_abs_dy_cat = np.zeros_like(y_abs_dy)\n\nprint(\"Training ABS base models with GroupKFold ...\")\nfor fold,(tr,va) in enumerate(cv_iter,1):\n    X_tr,X_va = X[tr],X[va]\n    ydx_tr,ydx_va = y_abs_dx[tr],y_abs_dx[va]\n    ydy_tr,ydy_va = y_abs_dy[tr],y_abs_dy[va]\n\n    lgb_dx = lgb.LGBMRegressor(**LGB_PARAMS).fit(X_tr,ydx_tr)\n    lgb_dy = lgb.LGBMRegressor(**LGB_PARAMS).fit(X_tr,ydy_tr)\n    xgb_dx = xgb.XGBRegressor(**XGB_PARAMS).fit(X_tr,ydx_tr,verbose=False)\n    xgb_dy = xgb.XGBRegressor(**XGB_PARAMS).fit(X_tr,ydy_tr,verbose=False)\n    cat_dx = CatBoostRegressor(**CAT_PARAMS).fit(X_tr,ydx_tr,verbose=False)\n    cat_dy = CatBoostRegressor(**CAT_PARAMS).fit(X_tr,ydy_tr,verbose=False)\n\n    p_dx_lgb,p_dx_xgb,p_dx_cat = lgb_dx.predict(X_va),xgb_dx.predict(X_va),cat_dx.predict(X_va)\n    p_dy_lgb,p_dy_xgb,p_dy_cat = lgb_dy.predict(X_va),xgb_dy.predict(X_va),cat_dy.predict(X_va)\n\n    oof_abs_dx_lgb[va],oof_abs_dx_xgb[va],oof_abs_dx_cat[va] = p_dx_lgb,p_dx_xgb,p_dx_cat\n    oof_abs_dy_lgb[va],oof_abs_dy_xgb[va],oof_abs_dy_cat[va] = p_dy_lgb,p_dy_xgb,p_dy_cat\n\n    for m,name in [(lgb_dx,\"abs_lgb_dx\"),(lgb_dy,\"abs_lgb_dy\"),\n                   (xgb_dx,\"abs_xgb_dx\"),(xgb_dy,\"abs_xgb_dy\"),\n                   (cat_dx,\"abs_cat_dx\"),(cat_dy,\"abs_cat_dy\")]:\n        with open(os.path.join(Cfg.MODEL_DIR,f\"fold{fold}_{name}.pkl\"),\"wb\") as f:\n            pickle.dump(m,f)\n\n    print(f\"[ABS] Fold {fold}: LGB dx={rmse(ydx_va,p_dx_lgb):.4f} dy={rmse(ydy_va,p_dy_lgb):.4f}\")\n\nprint(\"\\n[ABS] OOF Blend (0.35/0.45/0.20)\")\nabs_blend_dx = 0.35*oof_abs_dx_lgb + 0.45*oof_abs_dx_xgb + 0.20*oof_abs_dx_cat\nabs_blend_dy = 0.35*oof_abs_dy_lgb + 0.45*oof_abs_dy_xgb + 0.20*oof_abs_dy_cat\nprint(f\"ABS OOF RMSE dx: {rmse(y_abs_dx, abs_blend_dx):.4f}\")\nprint(f\"ABS OOF RMSE dy: {rmse(y_abs_dy, abs_blend_dy):.4f}\")\n\n# Meta for ABS\nmetaX_abs = np.vstack([oof_abs_dx_lgb,oof_abs_dx_xgb,oof_abs_dx_cat]).T\nmetaY_abs = np.vstack([oof_abs_dy_lgb,oof_abs_dy_xgb,oof_abs_dy_cat]).T\n\ndef ridge_tuned(Xo, yo):\n    best_a, best = None, 1e9\n    for a in [0.1,0.5,1,2,5]:\n        r = Ridge(alpha=a, fit_intercept=False, positive=True).fit(Xo,yo)\n        sc = rmse(yo, r.predict(Xo))\n        if sc < best: best, best_a = sc, a\n    return Ridge(alpha=best_a, fit_intercept=False, positive=True).fit(Xo,yo), best_a\n\nridge_abs_x, ax = ridge_tuned(metaX_abs, y_abs_dx)\nridge_abs_y, ay = ridge_tuned(metaY_abs, y_abs_dy)\noof_abs_metaR_dx = ridge_abs_x.predict(metaX_abs)\noof_abs_metaR_dy = ridge_abs_y.predict(metaY_abs)\n\nlgb_abs_x = lgb.LGBMRegressor(**META_LGB_PARAMS).fit(metaX_abs, y_abs_dx)\nlgb_abs_y = lgb.LGBMRegressor(**META_LGB_PARAMS).fit(metaY_abs, y_abs_dy)\noof_abs_metaL_dx = lgb_abs_x.predict(metaX_abs)\noof_abs_metaL_dy = lgb_abs_y.predict(metaY_abs)\n\nif Cfg.META_COMPARE:\n    print(f\"[ABS Meta] Ridge(Œ±={ax}) dx: {rmse(y_abs_dx, oof_abs_metaR_dx):.4f}  | LGB dx: {rmse(y_abs_dx, oof_abs_metaL_dx):.4f}\")\n    print(f\"[ABS Meta] Ridge(Œ±={ay}) dy: {rmse(y_abs_dy, oof_abs_metaR_dy):.4f}  | LGB dy: {rmse(y_abs_dy, oof_abs_metaL_dy):.4f}\")\n\n# Blend ABS metas (non-negative)\nblend_abs_Xdx = np.vstack([oof_abs_metaR_dx, oof_abs_metaL_dx]).T\nblend_abs_Xdy = np.vstack([oof_abs_metaR_dy, oof_abs_metaL_dy]).T\nblend_abs_rdx = Ridge(alpha=1e-3, fit_intercept=False, positive=True).fit(blend_abs_Xdx, y_abs_dx)\nblend_abs_rdy = Ridge(alpha=1e-3, fit_intercept=False, positive=True).fit(blend_abs_Xdy, y_abs_dy)\noof_abs_meta_dx = blend_abs_rdx.predict(blend_abs_Xdx)\noof_abs_meta_dy = blend_abs_rdy.predict(blend_abs_Xdy)\nprint(f\"[ABS Meta Blend] OOF RMSE dx: {rmse(y_abs_dx, oof_abs_meta_dx):.4f} | dy: {rmse(y_abs_dy, oof_abs_meta_dy):.4f}\")\n\n# ============================================================\n# üîÑ Save OOF meta predictions for Temporal Clamp refinement\n# ============================================================\nimport pickle, os\n\nOOF_SAVE_PATH = os.path.join(Cfg.MODEL_DIR, \"oof_meta_predictions.pkl\")\n\noof_data = {\n    \"oof_dx_meta_r\": oof_abs_metaR_dx,\n    \"oof_dx_meta_l\": oof_abs_metaL_dx,\n    \"oof_dy_meta_r\": oof_abs_metaR_dy,\n    \"oof_dy_meta_l\": oof_abs_metaL_dy,\n    \"y_dx\": y_abs_dx,\n    \"y_dy\": y_abs_dy,\n    \"pdf\": pdf\n}\n\nwith open(OOF_SAVE_PATH, \"wb\") as f:\n    pickle.dump(oof_data, f)\n\nprint(f\"‚úÖ Saved OOF meta predictions for Temporal Clamp ‚Üí {OOF_SAVE_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:58:28.147454Z","iopub.execute_input":"2025-10-27T08:58:28.14768Z","iopub.status.idle":"2025-10-27T09:30:36.990875Z","shell.execute_reply.started":"2025-10-27T08:58:28.147656Z","shell.execute_reply":"2025-10-27T09:30:36.990238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Cross-Validation with OOF capture (DIR stream)\n# ============================================================\noof_dir_dx_lgb = np.zeros_like(y_dir_dx)\noof_dir_dx_xgb = np.zeros_like(y_dir_dx)\noof_dir_dx_cat = np.zeros_like(y_dir_dx)\noof_dir_dy_lgb = np.zeros_like(y_dir_dy)\noof_dir_dy_xgb = np.zeros_like(y_dir_dy)\noof_dir_dy_cat = np.zeros_like(y_dir_dy)\n\nprint(\"\\nTraining DIR base models with GroupKFold ...\")\nfor fold,(tr,va) in enumerate(cv_iter,1):\n    X_tr,X_va = X[tr],X[va]\n    ydx_tr,ydx_va = y_dir_dx[tr],y_dir_dx[va]\n    ydy_tr,ydy_va = y_dir_dy[tr],y_dir_dy[va]\n\n    lgb_dx = lgb.LGBMRegressor(**LGB_PARAMS).fit(X_tr,ydx_tr)\n    lgb_dy = lgb.LGBMRegressor(**LGB_PARAMS).fit(X_tr,ydy_tr)\n    xgb_dx = xgb.XGBRegressor(**XGB_PARAMS).fit(X_tr,ydx_tr,verbose=False)\n    xgb_dy = xgb.XGBRegressor(**XGB_PARAMS).fit(X_tr,ydy_tr,verbose=False)\n    cat_dx = CatBoostRegressor(**CAT_PARAMS).fit(X_tr,ydx_tr,verbose=False)\n    cat_dy = CatBoostRegressor(**CAT_PARAMS).fit(X_tr,ydy_tr,verbose=False)\n\n    p_dx_lgb,p_dx_xgb,p_dx_cat = lgb_dx.predict(X_va),xgb_dx.predict(X_va),cat_dx.predict(X_va)\n    p_dy_lgb,p_dy_xgb,p_dy_cat = lgb_dy.predict(X_va),xgb_dy.predict(X_va),cat_dy.predict(X_va)\n\n    oof_dir_dx_lgb[va],oof_dir_dx_xgb[va],oof_dir_dx_cat[va] = p_dx_lgb,p_dx_xgb,p_dx_cat\n    oof_dir_dy_lgb[va],oof_dir_dy_xgb[va],oof_dir_dy_cat[va] = p_dy_lgb,p_dy_xgb,p_dy_cat\n\n    for m,name in [(lgb_dx,\"dir_lgb_dx\"),(lgb_dy,\"dir_lgb_dy\"),\n                   (xgb_dx,\"dir_xgb_dx\"),(xgb_dy,\"dir_xgb_dy\"),\n                   (cat_dx,\"dir_cat_dx\"),(cat_dy,\"dir_cat_dy\")]:\n        with open(os.path.join(Cfg.MODEL_DIR,f\"fold{fold}_{name}.pkl\"),\"wb\") as f:\n            pickle.dump(m,f)\n\n    print(f\"[DIR] Fold {fold}: LGB dx={rmse(ydx_va,p_dx_lgb):.4f} dy={rmse(ydy_va,p_dy_lgb):.4f}\")\n\nprint(\"\\n[DIR] OOF Blend (0.35/0.45/0.20)\")\ndir_blend_dx = 0.35*oof_dir_dx_lgb + 0.45*oof_dir_dx_xgb + 0.20*oof_dir_dx_cat\ndir_blend_dy = 0.35*oof_dir_dy_lgb + 0.45*oof_dir_dy_xgb + 0.20*oof_dir_dy_cat\nprint(f\"DIR OOF RMSE dx_dir: {rmse(y_dir_dx, dir_blend_dx):.4f}\")\nprint(f\"DIR OOF RMSE dy_dir: {rmse(y_dir_dy, dir_blend_dy):.4f}\")\n\n\n# ============================================================\n# Meta for DIR\n# ============================================================\n\nmetaX_dir = np.vstack([oof_dir_dx_lgb,oof_dir_dx_xgb,oof_dir_dx_cat]).T\nmetaY_dir = np.vstack([oof_dir_dy_lgb,oof_dir_dy_xgb,oof_dir_dy_cat]).T\n\nridge_dir_x, axd = ridge_tuned(metaX_dir, y_dir_dx)\nridge_dir_y, ayd = ridge_tuned(metaY_dir, y_dir_dy)\noof_dir_metaR_dx = ridge_dir_x.predict(metaX_dir)\noof_dir_metaR_dy = ridge_dir_y.predict(metaY_dir)\n\nlgb_dir_x = lgb.LGBMRegressor(**META_LGB_PARAMS).fit(metaX_dir, y_dir_dx)\nlgb_dir_y = lgb.LGBMRegressor(**META_LGB_PARAMS).fit(metaY_dir, y_dir_dy)\noof_dir_metaL_dx = lgb_dir_x.predict(metaX_dir)\noof_dir_metaL_dy = lgb_dir_y.predict(metaY_dir)\n\nif Cfg.META_COMPARE:\n    print(f\"[DIR Meta] Ridge(Œ±={axd}) dx: {rmse(y_dir_dx, oof_dir_metaR_dx):.4f}  | LGB dx: {rmse(y_dir_dx, oof_dir_metaL_dx):.4f}\")\n    print(f\"[DIR Meta] Ridge(Œ±={ayd}) dy: {rmse(y_dir_dy, oof_dir_metaR_dy):.4f}  | LGB dy: {rmse(y_dir_dy, oof_dir_metaL_dy):.4f}\")\n\n# Blend DIR metas (non-negative)\nblend_dir_Xdx = np.vstack([oof_dir_metaR_dx, oof_dir_metaL_dx]).T\nblend_dir_Xdy = np.vstack([oof_dir_metaR_dy, oof_dir_metaL_dy]).T\nblend_dir_rdx = Ridge(alpha=1e-3, fit_intercept=False, positive=True).fit(blend_dir_Xdx, y_dir_dx)\nblend_dir_rdy = Ridge(alpha=1e-3, fit_intercept=False, positive=True).fit(blend_dir_Xdy, y_dir_dy)\noof_dir_meta_dx = blend_dir_rdx.predict(blend_dir_Xdx)\noof_dir_meta_dy = blend_dir_rdy.predict(blend_dir_Xdy)\nprint(f\"[DIR Meta Blend] OOF RMSE dx_dir: {rmse(y_dir_dx, oof_dir_meta_dx):.4f} | dy_dir: {rmse(y_dir_dy, oof_dir_meta_dy):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T09:30:36.991707Z","iopub.execute_input":"2025-10-27T09:30:36.991965Z","iopub.status.idle":"2025-10-27T10:02:46.033889Z","shell.execute_reply.started":"2025-10-27T09:30:36.991935Z","shell.execute_reply":"2025-10-27T10:02:46.033149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Final Hybrid Blend (ABS vs DIR‚ÜíABS), learn non-negative weights on OOF\n# ============================================================\nprint(\"\\nüîß Learning final ABS vs DIR blend (in ABS space)...\")\n# Convert DIR OOF metas to ABS space using play_dir_sign\ndir2abs_dx = oof_dir_meta_dx * play_dir_sign\ndir2abs_dy = oof_dir_meta_dy * play_dir_sign\n\n# Two-feature OOF design matrix: [ABS_meta, DIR_meta_in_abs]\nfinal_X_dx = np.vstack([oof_abs_meta_dx, dir2abs_dx]).T\nfinal_X_dy = np.vstack([oof_abs_meta_dy, dir2abs_dy]).T\n\nfinal_ridge_dx = Ridge(alpha=1e-3, fit_intercept=False, positive=True).fit(final_X_dx, y_abs_dx)\nfinal_ridge_dy = Ridge(alpha=1e-3, fit_intercept=False, positive=True).fit(final_X_dy, y_abs_dy)\n\nw_abs_x, w_dir_x = final_ridge_dx.coef_\nw_abs_y, w_dir_y = final_ridge_dy.coef_\nsx = w_abs_x + w_dir_x + 1e-12\nsy = w_abs_y + w_dir_y + 1e-12\nprint(f\"Final blend weights ‚Äî X: ABS={w_abs_x/sx:.3f} DIR={w_dir_x/sx:.3f}\")\nprint(f\"Final blend weights ‚Äî Y: ABS={w_abs_y/sy:.3f} DIR={w_dir_y/sy:.3f}\")\n\noof_final_dx = final_ridge_dx.predict(final_X_dx)\noof_final_dy = final_ridge_dy.predict(final_X_dy)\nprint(f\"Final OOF RMSE ‚Äî dx: {rmse(y_abs_dx, oof_final_dx):.4f} | dy: {rmse(y_abs_dy, oof_final_dy):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:02:46.035888Z","iopub.execute_input":"2025-10-27T10:02:46.036114Z","iopub.status.idle":"2025-10-27T10:02:46.244998Z","shell.execute_reply.started":"2025-10-27T10:02:46.036096Z","shell.execute_reply":"2025-10-27T10:02:46.244411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Role-bias from final blended OOF (absolute space)\n# ============================================================\nprint(\"\\nüéØ Learning per-role residual biases (OOF, shrunken, ABS space) ...\")\nres_dx = oof_final_dx - y_abs_dx\nres_dy = oof_final_dy - y_abs_dy\nif role_raw_tr is None or len(role_raw_tr) != len(res_dx):\n    role_raw_tr = np.array([\"UNK\"]*len(res_dx))\n\ndf_res = pd.DataFrame({\"role\": role_raw_tr, \"res_dx\": res_dx, \"res_dy\": res_dy})\nglobal_dx = float(df_res[\"res_dx\"].mean())\nglobal_dy = float(df_res[\"res_dy\"].mean())\n\nrole_bias_dx, role_bias_dy = {}, {}\ncounts = df_res.groupby(\"role\").size()\nmeans_dx = df_res.groupby(\"role\")[\"res_dx\"].mean()\nmeans_dy = df_res.groupby(\"role\")[\"res_dy\"].mean()\n\nfor role, n in counts.items():\n    m_dx = float(means_dx.loc[role])\n    m_dy = float(means_dy.loc[role])\n    shrink = Cfg.ROLE_BIAS_SHRINK\n    role_bias_dx[role] = (n/(n+shrink))*m_dx + (shrink/(n+shrink))*global_dx\n    role_bias_dy[role] = (n/(n+shrink))*m_dy + (shrink/(n+shrink))*global_dy\n\nprint(f\"Computed shrunken biases for {len(role_bias_dx)} roles (examples):\",\n      dict(list(role_bias_dx.items())[:5]))\n\n# Persist meta objects needed at inference\nwith open(os.path.join(Cfg.MODEL_DIR, \"meta_and_blends.pkl\"), \"wb\") as f:\n    pickle.dump({\n        # ABS meta\n        \"ridge_abs_x\": ridge_abs_x, \"ridge_abs_y\": ridge_abs_y,\n        \"lgb_abs_x\": lgb_abs_x, \"lgb_abs_y\": lgb_abs_y,\n        \"blend_abs_rdx\": blend_abs_rdx, \"blend_abs_rdy\": blend_abs_rdy,\n        # DIR meta\n        \"ridge_dir_x\": ridge_dir_x, \"ridge_dir_y\": ridge_dir_y,\n        \"lgb_dir_x\": lgb_dir_x, \"lgb_dir_y\": lgb_dir_y,\n        \"blend_dir_rdx\": blend_dir_rdx, \"blend_dir_rdy\": blend_dir_rdy,\n        # Final blend\n        \"final_ridge_dx\": final_ridge_dx, \"final_ridge_dy\": final_ridge_dy,\n        # Role bias\n        \"role_bias_dx\": role_bias_dx, \"role_bias_dy\": role_bias_dy\n    }, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:02:46.245802Z","iopub.execute_input":"2025-10-27T10:02:46.246126Z","iopub.status.idle":"2025-10-27T10:02:46.441223Z","shell.execute_reply.started":"2025-10-27T10:02:46.246108Z","shell.execute_reply":"2025-10-27T10:02:46.440342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Test Inference\n# ============================================================\nprint(\"\\nPreparing test inference ...\")\ntest_input    = pl.read_csv(os.path.join(Cfg.DATA_DIR,\"test_input.csv\"))\ntest_template = pl.read_csv(os.path.join(Cfg.DATA_DIR,\"test.csv\"))\ntest_template = to_seconds_frames(test_template)\n\nagg_test   = aggregate_per_player(test_input)\nqb_last_te = qb_last_xy(test_input)\nconst_test = const_first_per_player(test_input)\n\nfor c in [\"game_id\",\"play_id\",\"nfl_id\"]:\n    agg_test      = agg_test.with_columns(pl.col(c).cast(pl.Int64))\n    test_template = test_template.with_columns(pl.col(c).cast(pl.Int64))\n    const_test    = const_test.with_columns(pl.col(c).cast(pl.Int64))\nqb_last_te = qb_last_te.with_columns(pl.col(\"game_id\").cast(pl.Int64),\n                                     pl.col(\"play_id\").cast(pl.Int64))\n\ntest_feat = (test_template\n             .join(agg_test,   on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n             .join(const_test, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n             .join(qb_last_te, on=[\"game_id\",\"play_id\"],          how=\"left\"))\n\n# Ensure last positions\nif \"x_last\" not in test_feat.columns:\n    test_feat = test_feat.with_columns(pl.lit(60.0).alias(\"x_last\"))\nif \"y_last\" not in test_feat.columns:\n    test_feat = test_feat.with_columns(pl.lit(26.65).alias(\"y_last\"))\n\n# QB-relative quick fill & rel coords in pandas space\ntest_feat = test_feat.with_columns([\n    pl.col(\"qb_x_last\").fill_null(pl.col(\"x_last\")).alias(\"qb_x_last\"),\n    pl.col(\"qb_y_last\").fill_null(pl.col(\"y_last\")).alias(\"qb_y_last\"),\n    (pl.col(\"x_last\") - pl.col(\"qb_x_last\")).alias(\"rel_x_last_qb\"),\n    (pl.col(\"y_last\") - pl.col(\"qb_y_last\")).alias(\"rel_y_last_qb\"),\n])\n\ntest_pd = test_feat.to_pandas()\ntest_pd[\"dist_to_qb\"] = np.sqrt(test_pd[\"rel_x_last_qb\"]**2 + test_pd[\"rel_y_last_qb\"]**2)\ntest_pd[\"ang_to_qb\"]  = np.arctan2(test_pd[\"rel_y_last_qb\"], test_pd[\"rel_x_last_qb\"])\ntest_pd[\"bearing_qb\"] = np.degrees(test_pd[\"ang_to_qb\"])\n\nif {\"ball_land_x\",\"ball_land_y\"}.issubset(set(test_pd.columns)):\n    test_pd[\"rel_x_last_ball\"] = test_pd[\"x_last\"] - test_pd[\"ball_land_x\"]\n    test_pd[\"rel_y_last_ball\"] = test_pd[\"y_last\"] - test_pd[\"ball_land_y\"]\n    test_pd[\"dist_to_ball\"]    = np.sqrt(test_pd[\"rel_x_last_ball\"]**2 + test_pd[\"rel_y_last_ball\"]**2)\n    test_pd[\"ang_to_ball\"]     = np.arctan2(test_pd[\"rel_y_last_ball\"], test_pd[\"rel_x_last_ball\"])\n    test_pd[\"bearing_ball\"]    = np.degrees(test_pd[\"ang_to_ball\"])\nelse:\n    for c in [\"rel_x_last_ball\",\"rel_y_last_ball\",\"dist_to_ball\",\"ang_to_ball\",\"bearing_ball\"]:\n        test_pd[c] = 0.0\n\n# Direction sign for test\nplay_dir_sign_test = np.where(test_pd.get(\"play_direction\",\"right\")==\"left\", -1.0, 1.0)\ntest_pd[\"play_dir_sign_num\"] = play_dir_sign_test\n\n# Context interactions\nif \"absolute_yardline_number\" in test_pd.columns:\n    test_pd[\"yard_norm\"] = test_pd[\"absolute_yardline_number\"]/100.0\nelse:\n    test_pd[\"yard_norm\"] = 0.0\nif \"frame_ratio\" not in test_pd.columns:\n    test_pd[\"frame_ratio\"] = 0.0\ntest_pd[\"yard_dir_ctx\"]   = test_pd[\"yard_norm\"] * test_pd[\"play_dir_sign_num\"]\ntest_pd[\"fratio_ball_ctx\"] = test_pd[\"frame_ratio\"] * test_pd[\"dist_to_ball\"]\n\n# Align & scale features\nwith open(os.path.join(Cfg.MODEL_DIR, \"feature_scaler.pkl\"), \"rb\") as f:\n    obj = pickle.load(f)\nfeat_cols_saved = obj[\"feat_cols\"]; scaler = obj[\"scaler\"]\nfor c in feat_cols_saved:\n    if c not in test_pd.columns:\n        test_pd[c] = 0.0\ntest_pd = test_pd[feat_cols_saved]\ntest_pd[feat_cols_saved] = scaler.transform(test_pd[feat_cols_saved])\nX_test = test_pd.values\n\n# Load metas & role bias\nwith open(os.path.join(Cfg.MODEL_DIR, \"meta_and_blends.pkl\"), \"rb\") as f:\n    M = pickle.load(f)\n\n# ========== ABS predictions ==========\npred_abs = {\"lgb\":{\"dx\":[],\"dy\":[]}, \"xgb\":{\"dx\":[],\"dy\":[]}, \"cat\":{\"dx\":[],\"dy\":[]}}\nfor fold in range(1, Cfg.N_FOLDS+1):\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_abs_lgb_dx.pkl\"), \"rb\") as f: lgb_dx = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_abs_lgb_dy.pkl\"), \"rb\") as f: lgb_dy = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_abs_xgb_dx.pkl\"), \"rb\") as f: xgb_dx = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_abs_xgb_dy.pkl\"), \"rb\") as f: xgb_dy = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_abs_cat_dx.pkl\"), \"rb\") as f: cat_dx = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_abs_cat_dy.pkl\"), \"rb\") as f: cat_dy = pickle.load(f)\n\n    pred_abs[\"lgb\"][\"dx\"].append(lgb_dx.predict(X_test))\n    pred_abs[\"lgb\"][\"dy\"].append(lgb_dy.predict(X_test))\n    pred_abs[\"xgb\"][\"dx\"].append(xgb_dx.predict(X_test))\n    pred_abs[\"xgb\"][\"dy\"].append(xgb_dy.predict(X_test))\n    pred_abs[\"cat\"][\"dx\"].append(cat_dx.predict(X_test))\n    pred_abs[\"cat\"][\"dy\"].append(cat_dy.predict(X_test))\n\nfor algo in pred_abs:\n    pred_abs[algo][\"dx\"] = np.mean(pred_abs[algo][\"dx\"], axis=0)\n    pred_abs[algo][\"dy\"] = np.mean(pred_abs[algo][\"dy\"], axis=0)\n\nmeta_in_abs_dx = np.vstack([pred_abs[\"lgb\"][\"dx\"], pred_abs[\"xgb\"][\"dx\"], pred_abs[\"cat\"][\"dx\"]]).T\nmeta_in_abs_dy = np.vstack([pred_abs[\"lgb\"][\"dy\"], pred_abs[\"xgb\"][\"dy\"], pred_abs[\"cat\"][\"dy\"]]).T\n\ndx_abs_r = M[\"ridge_abs_x\"].predict(meta_in_abs_dx)\ndy_abs_r = M[\"ridge_abs_y\"].predict(meta_in_abs_dy)\ndx_abs_l = M[\"lgb_abs_x\"].predict(meta_in_abs_dx)\ndy_abs_l = M[\"lgb_abs_y\"].predict(meta_in_abs_dy)\ndx_abs_meta = M[\"blend_abs_rdx\"].predict(np.vstack([dx_abs_r, dx_abs_l]).T)\ndy_abs_meta = M[\"blend_abs_rdy\"].predict(np.vstack([dy_abs_r, dy_abs_l]).T)\n\n# ========== DIR predictions (still directional) ==========\npred_dir = {\"lgb\":{\"dx\":[],\"dy\":[]}, \"xgb\":{\"dx\":[],\"dy\":[]}, \"cat\":{\"dx\":[],\"dy\":[]}}\nfor fold in range(1, Cfg.N_FOLDS+1):\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_dir_lgb_dx.pkl\"), \"rb\") as f: lgb_dx = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_dir_lgb_dy.pkl\"), \"rb\") as f: lgb_dy = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_dir_xgb_dx.pkl\"), \"rb\") as f: xgb_dx = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_dir_xgb_dy.pkl\"), \"rb\") as f: xgb_dy = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_dir_cat_dx.pkl\"), \"rb\") as f: cat_dx = pickle.load(f)\n    with open(os.path.join(Cfg.MODEL_DIR, f\"fold{fold}_dir_cat_dy.pkl\"), \"rb\") as f: cat_dy = pickle.load(f)\n\n    pred_dir[\"lgb\"][\"dx\"].append(lgb_dx.predict(X_test))\n    pred_dir[\"lgb\"][\"dy\"].append(lgb_dy.predict(X_test))\n    pred_dir[\"xgb\"][\"dx\"].append(xgb_dx.predict(X_test))\n    pred_dir[\"xgb\"][\"dy\"].append(xgb_dy.predict(X_test))\n    pred_dir[\"cat\"][\"dx\"].append(cat_dx.predict(X_test))\n    pred_dir[\"cat\"][\"dy\"].append(cat_dy.predict(X_test))\n\nfor algo in pred_dir:\n    pred_dir[algo][\"dx\"] = np.mean(pred_dir[algo][\"dx\"], axis=0)\n    pred_dir[algo][\"dy\"] = np.mean(pred_dir[algo][\"dy\"], axis=0)\n\nmeta_in_dir_dx = np.vstack([pred_dir[\"lgb\"][\"dx\"], pred_dir[\"xgb\"][\"dx\"], pred_dir[\"cat\"][\"dx\"]]).T\nmeta_in_dir_dy = np.vstack([pred_dir[\"lgb\"][\"dy\"], pred_dir[\"xgb\"][\"dy\"], pred_dir[\"cat\"][\"dy\"]]).T\n\ndx_dir_r = M[\"ridge_dir_x\"].predict(meta_in_dir_dx)\ndy_dir_r = M[\"ridge_dir_y\"].predict(meta_in_dir_dy)\ndx_dir_l = M[\"lgb_dir_x\"].predict(meta_in_dir_dx)\ndy_dir_l = M[\"lgb_dir_y\"].predict(meta_in_dir_dy)\ndx_dir_meta = M[\"blend_dir_rdx\"].predict(np.vstack([dx_dir_r, dx_dir_l]).T)\ndy_dir_meta = M[\"blend_dir_rdy\"].predict(np.vstack([dy_dir_r, dy_dir_l]).T)\n\n# Convert DIR meta predictions to ABS using test play_dir_sign\ndx_dir_as_abs = dx_dir_meta * play_dir_sign_test\ndy_dir_as_abs = dy_dir_meta * play_dir_sign_test\n\n# Final learned ABS vs DIR blend on TEST\ndx_hat = M[\"final_ridge_dx\"].predict(np.vstack([dx_abs_meta, dx_dir_as_abs]).T)\ndy_hat = M[\"final_ridge_dy\"].predict(np.vstack([dy_abs_meta, dy_dir_as_abs]).T)\n\n# Per-role bias correction on TEST (ABS space)\nrole_test = (test_feat.get_column(\"player_role\").to_pandas() if \"player_role\" in test_feat.columns\n             else pd.Series([\"UNK\"]*test_feat.height))\nrole_test = role_test.fillna(\"UNK\").astype(str).values\nrole_bias_dx = M[\"role_bias_dx\"]; role_bias_dy = M[\"role_bias_dy\"]\nglobal_bias_dx = float(np.mean(list(role_bias_dx.values()))) if len(role_bias_dx)>0 else 0.0\nglobal_bias_dy = float(np.mean(list(role_bias_dy.values()))) if len(role_bias_dy)>0 else 0.0\nbias_dx_vec = np.array([role_bias_dx.get(r, global_bias_dx) for r in role_test], dtype=float)\nbias_dy_vec = np.array([role_bias_dy.get(r, global_bias_dy) for r in role_test], dtype=float)\ndx_hat = dx_hat - bias_dx_vec\ndy_hat = dy_hat - bias_dy_vec\n\n# Recompose to absolute coordinates\nx_abs = test_feat[\"x_last\"].to_numpy() + dx_hat\ny_abs = test_feat[\"y_last\"].to_numpy() + dy_hat\n\n# Build submission\ntest_df_id = test_feat.select([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"]).to_pandas().astype(str)\nsubmission = pd.DataFrame({\n    \"id\": test_df_id[\"game_id\"] + \"_\" + test_df_id[\"play_id\"] + \"_\" + test_df_id[\"nfl_id\"] + \"_\" + test_df_id[\"frame_id\"],\n    \"x\": x_abs,\n    \"y\": y_abs\n})\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"‚úÖ submission.csv saved:\", submission.shape)\nprint(submission.head())\n\ngc.collect()\n\n# ============================================================\n# ‚ôªÔ∏è Reload OOF meta predictions if training not rerun\n# ============================================================\nimport pickle, os, numpy as np, pandas as pd\n\nOOF_SAVE_PATH = os.path.join(Cfg.MODEL_DIR, \"oof_meta_predictions.pkl\")\nif os.path.exists(OOF_SAVE_PATH):\n    print(\"üîÅ Loading cached OOF meta predictions ...\")\n    with open(OOF_SAVE_PATH, \"rb\") as f:\n        oof_data = pickle.load(f)\n    oof_dx_meta_r = oof_data[\"oof_dx_meta_r\"]\n    oof_dx_meta_l = oof_data[\"oof_dx_meta_l\"]\n    oof_dy_meta_r = oof_data[\"oof_dy_meta_r\"]\n    oof_dy_meta_l = oof_data[\"oof_dy_meta_l\"]\n    y_dx = oof_data[\"y_dx\"]\n    y_dy = oof_data[\"y_dy\"]\n    pdf  = oof_data[\"pdf\"]\n    print(f\"‚úÖ Reloaded OOF data ‚Äî dx:{len(y_dx):,} dy:{len(y_dy):,}\")\nelse:\n    raise RuntimeError(\"Missing OOF cache ‚Äî run full training before Temporal Clamp.\")\n\n# ============================================================\n# NFL Big Data Bowl 2026 ‚Äî Daniel GPU V10.3.1 (Temporal Clamp, Standalone)\n# Safe to run after your trained V10.3 notebook (no retrain needed)\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import ElasticNet\n\nprint(\"\\nüß≠ Running V10.3.1 Temporal Clamp refinement ...\")\n\n# ============================================================\n# Step 1 ‚Äî Temporal-weighted outlier clamp (on existing OOF)\n# ============================================================\n\n# Stack the meta-of-meta input matrices\nblend_X_dx = np.vstack([oof_dx_meta_r, oof_dx_meta_l]).T\nblend_X_dy = np.vstack([oof_dy_meta_r, oof_dy_meta_l]).T\n\n# Temporal + spatial weighting: later frames and ball proximity\ndist = np.asarray(pdf.get(\"dist_to_ball\", 0.0))\nfr   = np.asarray(pdf.get(\"frame_ratio\", 0.0))\nw_base = fr * (1.0 + dist / (dist.mean() + 1e-6))\nw = w_base / (w_base.mean() + 1e-12)\n\n# Probe residuals using LGB meta (most stable)\nres_dx_probe = oof_dx_meta_l - y_dx\nres_dy_probe = oof_dy_meta_l - y_dy\n\n# Compute 99.8th percentile clip thresholds\nq_dx = np.quantile(np.abs(res_dx_probe), 0.998)\nq_dy = np.quantile(np.abs(res_dy_probe), 0.998)\nmask_dx = np.abs(res_dx_probe) <= q_dx\nmask_dy = np.abs(res_dy_probe) <= q_dy\nprint(f\"[Clamp] Kept samples: dx={mask_dx.mean():.4f}, dy={mask_dy.mean():.4f}\")\n\n# ============================================================\n# üßπ Clamp Stability Guard ‚Äî Clean NaNs / Infs / Scaling\n# ============================================================\ndef safe_clean(X):\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n    if np.abs(X).max() > 1e3:\n        X = X / (np.abs(X).max() + 1e-12)\n    return X\n\nblend_X_dx = safe_clean(blend_X_dx)\nblend_X_dy = safe_clean(blend_X_dy)\ny_dx = safe_clean(y_dx)\ny_dy = safe_clean(y_dy)\nw = safe_clean(w)\n\n# Scale target and features to the same magnitude\nfrom sklearn.preprocessing import StandardScaler\n\nsx, sy = StandardScaler(), StandardScaler()\nblend_X_dx_s = sx.fit_transform(blend_X_dx)\nblend_X_dy_s = sy.fit_transform(blend_X_dy)\ny_dx_s = (y_dx - np.mean(y_dx)) / (np.std(y_dx) + 1e-9)\ny_dy_s = (y_dy - np.mean(y_dy)) / (np.std(y_dy) + 1e-9)\n\n# Clip weights to avoid zero or infinity\nw = np.clip(w, 1e-6, np.percentile(w, 99.9))\n\n# ============================================================\n# Step 2 ‚Äî ElasticNet (L1+L2) meta blend with temporal weights\n# ============================================================\nblend_en_x = ElasticNet(\n    alpha=0.01, l1_ratio=0.25, fit_intercept=True,\n    positive=True, max_iter=20000, random_state=Cfg.SEED\n)\nblend_en_y = ElasticNet(\n    alpha=0.01, l1_ratio=0.25, fit_intercept=True,\n    positive=True, max_iter=20000, random_state=Cfg.SEED\n)\n\nprint(f\"Training ElasticNet with {mask_dx.sum():,} valid dx and {mask_dy.sum():,} valid dy samples...\")\n\nblend_en_x.fit(blend_X_dx_s[mask_dx], y_dx_s[mask_dx], sample_weight=w[mask_dx])\nblend_en_y.fit(blend_X_dy_s[mask_dy], y_dy_s[mask_dy], sample_weight=w[mask_dy])\n\n# Reverse scaling to compute actual-weight ratios\nwx_r, wx_l = blend_en_x.coef_\nwy_r, wy_l = blend_en_y.coef_\nnx, ny = wx_r + wx_l + 1e-12, wy_r + wy_l + 1e-12\n\nprint(f\"[Clamp] Final weights X: Ridge={wx_r/nx:.3f}, LGB={wx_l/nx:.3f}\")\nprint(f\"[Clamp] Final weights Y: Ridge={wy_r/ny:.3f}, LGB={wy_l/ny:.3f}\")\n\noof_dx_meta_clamp = blend_en_x.predict(blend_X_dx_s)\noof_dy_meta_clamp = blend_en_y.predict(blend_X_dy_s)\nprint(f\"[Clamp] OOF RMSE dx: {rmse(y_dx_s, oof_dx_meta_clamp):.4f}\")\nprint(f\"[Clamp] OOF RMSE dy: {rmse(y_dy_s, oof_dy_meta_clamp):.4f}\")\n\n# ============================================================\n# Prepare Ridge & LGB meta-level predictions for clamp\n# ============================================================\ndx_hat_r = M[\"ridge_abs_x\"].predict(meta_in_abs_dx)\ndy_hat_r = M[\"ridge_abs_y\"].predict(meta_in_abs_dy)\ndx_hat_l = M[\"lgb_abs_x\"].predict(meta_in_abs_dx)\ndy_hat_l = M[\"lgb_abs_y\"].predict(meta_in_abs_dy)\n\n# ============================================================\n# Step 3 ‚Äî Replace test-time meta blend with clamp blend\n# ============================================================\ndx_hat = blend_en_x.predict(np.vstack([dx_hat_r, dx_hat_l]).T)\ndy_hat = blend_en_y.predict(np.vstack([dy_hat_r, dy_hat_l]).T)\n\n# Apply your existing per-role bias correction & recomposition\ndx_hat = dx_hat - bias_dx_vec\ndy_hat = dy_hat - bias_dy_vec\n\nx_abs = test_feat[\"x_last\"].to_numpy() + dx_hat\ny_abs = test_feat[\"y_last\"].to_numpy() + dy_hat\ntest_df_id = test_feat.select([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"]).to_pandas().astype(str)\nsubmission = pd.DataFrame({\n    \"id\": test_df_id[\"game_id\"] + \"_\" + test_df_id[\"play_id\"] + \"_\" +\n          test_df_id[\"nfl_id\"] + \"_\" + test_df_id[\"frame_id\"],\n    \"x\": x_abs,\n    \"y\": y_abs\n})\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"‚úÖ submission.csv saved:\", submission.shape)\nprint(submission.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:57:59.215273Z","iopub.execute_input":"2025-10-27T10:57:59.215882Z","iopub.status.idle":"2025-10-27T10:58:19.907295Z","shell.execute_reply.started":"2025-10-27T10:57:59.215855Z","shell.execute_reply":"2025-10-27T10:58:19.90632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}