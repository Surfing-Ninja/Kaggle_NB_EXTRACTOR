{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13429029,"sourceType":"datasetVersion","datasetId":8523388},{"sourceId":13441428,"sourceType":"datasetVersion","datasetId":8531647}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -------------------------------\n# Global imports + cuDF accelerator\n# -------------------------------\nimport os\nUSE_CUDF = False\ntry:\n    # zero/low-code GPU acceleration for DataFrame ops\n    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n    import pandas as pd\n    import numpy as np\n    import cupy as cp  # optional (not strictly required below)\n    USE_CUDF = True\n    print(\"using cuda_backend pandas for faster parallel data processing\")\nexcept Exception:\n    print(\"cuda df not used\")\n    import pandas as pd\n    import numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom pathlib import Path\nfrom shutil import make_archive\nimport json\nimport random\nimport joblib\nfrom glob import glob\nfrom tqdm.auto import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# -------------------------------\n# Constants & helpers\n# -------------------------------\nYARDS_TO_METERS = 0.9144\nFPS = 10.0 \nFIELD_LENGTH, FIELD_WIDTH = 120.0, 53.3\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nprint(\"environment set up!\")\ndef wrap_angle_deg(s):\n    # map to (-180, 180]\n    return ((s + 180.0) % 360.0) - 180.0\n\ndef unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Mirror rightward plays so all samples are 'left' oriented (x,y, dir, o, ball_land).\"\"\"\n    if 'play_direction' not in df.columns:\n        return df\n    df = df.copy()\n    right = df['play_direction'].eq('right')\n    # positions\n    if 'x' in df.columns: df.loc[right, 'x'] = FIELD_LENGTH - df.loc[right, 'x']\n    if 'y' in df.columns: df.loc[right, 'y'] = FIELD_WIDTH  - df.loc[right, 'y']\n    # angles in degrees\n    for col in ('dir','o'):\n        if col in df.columns:\n            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n    # ball landing\n    if 'ball_land_x' in df.columns:\n        df.loc[right, 'ball_land_x'] = FIELD_LENGTH - df.loc[right, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[right, 'ball_land_y'] = FIELD_WIDTH  - df.loc[right, 'ball_land_y']\n    return df\n\ndef invert_to_original_direction(x_u, y_u, play_dir_right: bool):\n    \"\"\"Invert unified (left) coordinates back to original play direction.\"\"\"\n    if not play_dir_right:\n        return float(x_u), float(y_u)\n    return float(FIELD_LENGTH - x_u), float(FIELD_WIDTH - y_u)\n\n# -------------------------------\n# Config\n# -------------------------------\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    OUTPUT_DIR = Path(\"./outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n\n    SAVE_DIR = Path(\"./saved_models\")       \n    SAVE_DIR.mkdir(exist_ok=True, parents=True)\n    \n    MODELS_DIR = Path(\"/kaggle/input/hsiaosuan-sttn/saved_models\")\n    MODELS_DIR.mkdir(exist_ok=True, parents=True)\n\n    TRAIN = False\n    SUB   = True\n\n    #是否使用Bi-GRU\n    BIDIRECTIONAL = True\n\n\n\n    #显式指定特征组（确保 train/sub 一致）\n    FEATURE_GROUPS = [\n        'distance_rate','target_alignment','multi_window_rolling','extended_lags',\n        'velocity_changes','field_position','role_specific','time_features','jerk_features','interaction_features_mid','qb_relative',\n        # 'interaction_features', #保守版本对抗特征\n        # 'curvature_land_features',  #若线上无落点，勿开启\n    ]\n\n    #Training Setting\n    SEED = 42\n    SEEDS = [42, 19, 89, 64]   #多种子集成\n    N_FOLDS = 5\n    BATCH_SIZE = 256\n    EPOCHS = 200\n    PATIENCE = 30\n    LEARNING_RATE = 1e-3\n\n    WINDOW_SIZE = 10\n    HIDDEN_DIM = 128\n    MAX_FUTURE_HORIZON = 94  #不要动这个东西\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nset_seed(Config.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:04.216982Z","iopub.execute_input":"2025-10-20T08:51:04.217241Z","iopub.status.idle":"2025-10-20T08:51:10.520827Z","shell.execute_reply.started":"2025-10-20T08:51:04.217221Z","shell.execute_reply":"2025-10-20T08:51:10.519871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_val_rmse(mx, my, X_val_sc, ydx_list, ydy_list, horizon, device, mode=\"per-dim\"):\n    \"\"\"\n    计算验证集轨迹误差。\n    mode:\n      - \"per-dim\"  : sqrt( ( (dx^2+dy^2) 总和 / (2N) ) )  ← 你截图里的公式（推荐用于对齐）\n      - \"2d\"       : sqrt( ( (dx^2+dy^2) 总和 / N ) )     ← 二维欧氏 RMSE（会比 per-dim 大 sqrt(2)）\n      - \"mean-dist\": 平均欧氏距离 E[ sqrt(dx^2+dy^2) ]     ← 某些比赛用这个口径\n    N = 有效的样本×时间步（按 mask 统计）\n    \"\"\"\n    X_t = torch.tensor(X_val_sc.astype(np.float32)).to(device)\n    with torch.no_grad():\n        pdx = mx(X_t).cpu().numpy()   # (N, H)\n        pdy = my(X_t).cpu().numpy()   # (N, H)\n\n    ydx, m = prepare_targets(ydx_list, horizon)  # (N,H), (N,H)\n    ydy, _ = prepare_targets(ydy_list, horizon)\n    ydx, ydy, m = ydx.numpy(), ydy.numpy(), m.numpy()\n\n    se_sum2d = ((pdx - ydx)**2 + (pdy - ydy)**2) * m\n    denom = m.sum() + 1e-8\n\n    if mode == \"per-dim\":\n        return float(np.sqrt(se_sum2d.sum() / (2.0 * denom)))\n    elif mode == \"2d\":\n        return float(np.sqrt(se_sum2d.sum() / denom))\n    elif mode == \"mean-dist\":\n        dist = np.sqrt(se_sum2d)  # 元素级开根号\n        return float(dist.sum() / denom)\n    else:\n        raise ValueError(\"mode must be one of {'per-dim','2d','mean-dist'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.522938Z","iopub.execute_input":"2025-10-20T08:51:10.523458Z","iopub.status.idle":"2025-10-20T08:51:10.535134Z","shell.execute_reply.started":"2025-10-20T08:51:10.523428Z","shell.execute_reply":"2025-10-20T08:51:10.534437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Feature Engineering\n# -------------------------------\nclass FeatureEngineer:\n    \"\"\"\n    Modular, ablation-friendly feature builder (pandas or cuDF pandas-API).\n    \"\"\"\n    def __init__(self, feature_groups_to_create):\n        self.gcols = ['game_id', 'play_id', 'nfl_id']\n        self.active_groups = feature_groups_to_create\n        self.feature_creators = {\n            'distance_rate': self._create_distance_rate_features,\n            'target_alignment': self._create_target_alignment_features,\n            'multi_window_rolling': self._create_multi_window_rolling_features,\n            'extended_lags': self._create_extended_lag_features,\n            'velocity_changes': self._create_velocity_change_features,\n            'field_position': self._create_field_position_features,\n            'role_specific': self._create_role_specific_features,\n            'time_features': self._create_time_features,\n            'jerk_features': self._create_jerk_features,\n            'curvature_land_features': self._create_curvature_land_features,\n            'interaction_features': self._create_interaction_features, #添加,交互对抗特征\n            'interaction_features_mid': self._create_interaction_features_mid,  #激进的交互对抗特征\n            'qb_relative': self._create_qb_relative_features, #相对于四分卫位置的建模\n        }\n        self.created_feature_cols = []\n\n    def _height_to_feet(self, height_str):\n        try:\n            ft, inches = map(int, str(height_str).split('-'))\n            return ft + inches / 12\n        except Exception:\n            return 6.0\n\n    def _create_basic_features(self, df):\n        print(\"Step 1/3: Adding basic features...\")\n        df = df.copy()\n        df['player_height_feet'] = df['player_height'].apply(self._height_to_feet)\n\n        # Correct kinematics: dir is from +x CCW\n        dir_rad = np.deg2rad(df['dir'].fillna(0.0).astype('float32'))\n        df['velocity_x']     = df['s'] * np.cos(dir_rad)\n        df['velocity_y']     = df['s'] * np.sin(dir_rad)\n        df['acceleration_x'] = df['a'] * np.cos(dir_rad)\n        df['acceleration_y'] = df['a'] * np.sin(dir_rad)\n\n        # Roles\n        df['is_offense']  = (df['player_side'] == 'Offense').astype(np.int8)\n        df['is_defense']  = (df['player_side'] == 'Defense').astype(np.int8)\n        df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(np.int8)\n        df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(np.int8)\n        df['is_passer']   = (df['player_role'] == 'Passer').astype(np.int8)\n\n        # Energetics (consistent units)\n        mass_kg = df['player_weight'].fillna(200.0) / 2.20462\n        v_ms = df['s'] * YARDS_TO_METERS\n        df['momentum_x'] = mass_kg * df['velocity_x'] * YARDS_TO_METERS\n        df['momentum_y'] = mass_kg * df['velocity_y'] * YARDS_TO_METERS\n        df['kinetic_energy'] = 0.5 * mass_kg * (v_ms ** 2)\n\n        # Ball landing geometry (static)\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            ball_dx = df['ball_land_x'] - df['x']\n            ball_dy = df['ball_land_y'] - df['y']\n            dist = np.hypot(ball_dx, ball_dy)\n            df['distance_to_ball'] = dist\n            inv = 1.0 / (dist + 1e-6)\n            df['ball_direction_x'] = ball_dx * inv\n            df['ball_direction_y'] = ball_dy * inv\n            df['closing_speed'] = (\n                df['velocity_x'] * df['ball_direction_x'] +\n                df['velocity_y'] * df['ball_direction_y']\n            )\n\n        base = [\n            'x','y','s','a','o','dir','frame_id','ball_land_x','ball_land_y',\n            'player_height_feet','player_weight',\n            'velocity_x','velocity_y','acceleration_x','acceleration_y',\n            'momentum_x','momentum_y','kinetic_energy',\n            'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n            'distance_to_ball','ball_direction_x','ball_direction_y','closing_speed'\n        ]\n        self.created_feature_cols.extend([c for c in base if c in df.columns])\n        return df\n\n    # ---- feature groups ----\n    def _create_distance_rate_features(self, df):\n        new_cols = []\n        if 'distance_to_ball' in df.columns:\n            d = df.groupby(self.gcols)['distance_to_ball'].diff()\n            df['d2ball_dt']  = d.fillna(0.0) * FPS\n            df['d2ball_ddt'] = df.groupby(self.gcols)['d2ball_dt'].diff().fillna(0.0) * FPS\n            df['time_to_intercept'] = (df['distance_to_ball'] /\n                                       (df['d2ball_dt'].abs() + 1e-3)).clip(0, 10)\n            new_cols = ['d2ball_dt','d2ball_ddt','time_to_intercept']\n        return df, new_cols\n\n    def _create_target_alignment_features(self, df):\n        new_cols = []\n        if {'ball_direction_x','ball_direction_y','velocity_x','velocity_y'}.issubset(df.columns):\n            df['velocity_alignment'] = df['velocity_x']*df['ball_direction_x'] + df['velocity_y']*df['ball_direction_y']\n            df['velocity_perpendicular'] = df['velocity_x']*(-df['ball_direction_y']) + df['velocity_y']*df['ball_direction_x']\n            new_cols.extend(['velocity_alignment','velocity_perpendicular'])\n            if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n                df['accel_alignment'] = df['acceleration_x']*df['ball_direction_x'] + df['acceleration_y']*df['ball_direction_y']\n                new_cols.append('accel_alignment')\n        return df, new_cols\n\n    def _create_multi_window_rolling_features(self, df):\n        # keep it simple & compatible (works with cuDF pandas-API); vectorized rolling per group\n        new_cols = []\n        for window in (3, 5, 10):\n            for col in ('velocity_x','velocity_y','s','a'):\n                if col in df.columns:\n                    r_mean = df.groupby(self.gcols)[col].rolling(window, min_periods=1).mean()\n                    r_std  = df.groupby(self.gcols)[col].rolling(window, min_periods=1).std()\n                    # align indices\n                    r_mean = r_mean.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    r_std  = r_std.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    df[f'{col}_roll{window}'] = r_mean\n                    df[f'{col}_std{window}']  = r_std.fillna(0.0)\n                    new_cols.extend([f'{col}_roll{window}', f'{col}_std{window}'])\n        return df, new_cols\n\n    def _create_extended_lag_features(self, df):\n        new_cols = []\n        for lag in (1,2,3,4,5):\n            for col in ('x','y','velocity_x','velocity_y'):\n                if col in df.columns:\n                    g = df.groupby(self.gcols)[col]\n                    lagv = g.shift(lag)\n                    # safe fill for first frames (no \"future\" leakage)\n                    df[f'{col}_lag{lag}'] = lagv.fillna(g.transform('first'))\n                    new_cols.append(f'{col}_lag{lag}')\n        return df, new_cols\n\n    def _create_velocity_change_features(self, df):\n        new_cols = []\n        if 'velocity_x' in df.columns:\n            df['velocity_x_change'] = df.groupby(self.gcols)['velocity_x'].diff().fillna(0.0)\n            df['velocity_y_change'] = df.groupby(self.gcols)['velocity_y'].diff().fillna(0.0)\n            df['speed_change']      = df.groupby(self.gcols)['s'].diff().fillna(0.0)\n            d = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n            df['direction_change']  = wrap_angle_deg(d)\n            new_cols = ['velocity_x_change','velocity_y_change','speed_change','direction_change']\n        return df, new_cols\n\n    def _create_field_position_features(self, df):\n        df['dist_from_left'] = df['y']\n        df['dist_from_right'] = FIELD_WIDTH - df['y']\n        df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n        df['dist_from_endzone']  = np.minimum(df['x'], FIELD_LENGTH - df['x'])\n        return df, ['dist_from_sideline','dist_from_endzone']\n\n    def _create_role_specific_features(self, df):\n        new_cols = []\n        if {'is_receiver','velocity_alignment'}.issubset(df.columns):\n            df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n            df['receiver_deviation']  = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0.0))\n            new_cols.extend(['receiver_optimality','receiver_deviation'])\n        if {'is_coverage','closing_speed'}.issubset(df.columns):\n            df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n            new_cols.append('defender_closing_speed')\n        return df, new_cols\n\n    def _create_time_features(self, df):\n        df['frames_elapsed']  = df.groupby(self.gcols).cumcount()\n        df['normalized_time'] = df.groupby(self.gcols)['frames_elapsed'].transform(\n            lambda x: x / (x.max() + 1e-9)\n        )\n        df['time_sin'] = np.sin(2*np.pi*df['normalized_time'])\n        df['time_cos'] = np.cos(2*np.pi*df['normalized_time'])\n        return df, ['frames_elapsed','normalized_time','time_sin','time_cos']\n\n    def _create_jerk_features(self, df):\n        new_cols = []\n        if 'a' in df.columns:\n            df['jerk'] = df.groupby(self.gcols)['a'].diff().fillna(0.0) * FPS\n            new_cols.append('jerk')\n        if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n            df['jerk_x'] = df.groupby(self.gcols)['acceleration_x'].diff().fillna(0.0) * FPS\n            df['jerk_y'] = df.groupby(self.gcols)['acceleration_y'].diff().fillna(0.0) * FPS\n            new_cols.extend(['jerk_x','jerk_y'])\n        return df, new_cols\n    def _create_curvature_land_features(self, df):\n        \"\"\"\n        -落点侧向偏差（符号）：landing_point 相对“当前运动方向”的左右偏离\n          lateral = cross(u_dir, vector_to_land)（>0 表示落点在运动方向左侧）\n        -bearing_to_land_signed: 运动方向 vs 落点方位角\n        -速度归一化曲率： wrap(Δdir)/ (s*Δt) ，窗口化(3/5) 的均值/绝对值\n        \"\"\"\n        import numpy as np\n        # 侧向偏差 & bearing_to_land\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            dx = df['ball_land_x'] - df['x']\n            dy = df['ball_land_y'] - df['y']\n            bearing = np.arctan2(dy, dx)\n            a_dir = np.deg2rad(df['dir'].fillna(0.0).values)\n            # 有符号方位差\n            df['bearing_to_land_signed'] = np.rad2deg(np.arctan2(np.sin(bearing - a_dir), np.cos(bearing - a_dir)))\n            # 侧向偏差：d × u (2D cross, z 分量)\n            ux, uy = np.cos(a_dir), np.sin(a_dir)\n            df['land_lateral_offset'] = dy*ux - dx*uy  # >0 落点在左侧\n    \n        # 曲率（按序列）\n        ddir = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n        ddir = ((ddir + 180.0) % 360.0) - 180.0\n        curvature = np.deg2rad(ddir).astype('float32') / (df['s'].replace(0, np.nan).astype('float32') * 0.1 + 1e-6)\n        df['curvature_signed'] = curvature.fillna(0.0)\n        df['curvature_abs'] = df['curvature_signed'].abs()\n    \n        # 窗口均值（3/5）\n        for w in (3,5):\n            r = df.groupby(self.gcols)['curvature_signed'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_signed_roll{w}'] = r\n            r2 = df.groupby(self.gcols)['curvature_abs'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_abs_roll{w}'] = r2\n    \n        new_cols = ['bearing_to_land_signed','land_lateral_offset',\n                    'curvature_signed','curvature_abs','curv_signed_roll3','curv_abs_roll3',\n                    'curv_signed_roll5','curv_abs_roll5']\n        return df, [c for c in new_cols if c in df.columns]\n\n    def _create_interaction_features(self, df, speed_eps=0.5):\n        \"\"\"\n        Lite Receiver–Defender interaction features (conservative):\n          - Only K=1 (nearest opponent)\n          - Compute ONLY for target player (player_to_predict==True) if column exists\n          - Features:\n              opp_dmin        : distance to nearest opposite-side opponent (clipped [0,30])\n              opp_close_rate  : (v_opp - v_self) projected onto (opp->self) unit vector (clipped [-10,10])\n              opp_leverage    : sign of cross( self_vel , self->opp ) in 2D ( {-1,0,1} ), gated by speed\n        \"\"\"\n        import numpy as np\n    \n        need = ['x','y','velocity_x','velocity_y','player_side','frame_id']\n        for c in need:\n            if c not in df.columns:\n                return df, []  # missing columns → skip safely\n    \n        out_cols = ['opp_dmin','opp_close_rate','opp_leverage']\n        for c in out_cols:\n            if c not in df.columns:\n                df[c] = np.nan\n    \n        key = ['game_id','play_id','frame_id']\n        use_mask_global = ('player_to_predict' in df.columns)\n    \n        for _, g in df.groupby(key, sort=False):\n            idx = g.index.values\n            if len(g) <= 1:\n                continue\n    \n            pos = g[['x','y']].values.astype('float32')          # (N,2)\n            vel = g[['velocity_x','velocity_y']].values.astype('float32')\n            side_off = (g['player_side'].values == 'Offense')    # (N,)\n            side_def = ~side_off\n    \n            # only compute for target players if available\n            if use_mask_global:\n                tgt_mask = g['player_to_predict'].astype(bool).values\n            else:\n                tgt_mask = np.ones(len(g), dtype=bool)\n    \n            def _assign(A_mask, B_mask):\n                A_mask = A_mask & tgt_mask\n                A_idx = np.where(A_mask)[0]\n                B_idx = np.where(B_mask)[0]\n                if len(A_idx)==0 or len(B_idx)==0:\n                    return\n    \n                Apos, Bpos = pos[A_idx], pos[B_idx]\n                Avel, Bvel = vel[A_idx], vel[B_idx]\n    \n                # pairwise distances (K=1)\n                dx = Apos[:,None,0] - Bpos[None,:,0]\n                dy = Apos[:,None,1] - Bpos[None,:,1]\n                D  = np.sqrt(dx*dx + dy*dy) + 1e-6                 # (Na,Nb)\n                j  = np.argmin(D, axis=1)                          # nearest opponent\n    \n                dmin = D[np.arange(len(A_idx)), j]\n                dmin = np.clip(dmin, 0.0, 30.0)                    # robust clip\n    \n                # closing rate\n                r   = Apos - Bpos[j]                                # opp->self\n                u   = r / (np.linalg.norm(r, axis=1, keepdims=True) + 1e-6)\n                v_rel = Bvel[j] - Avel\n                close = np.einsum('ij,ij->i', v_rel, u)\n                close = np.clip(close, -10.0, 10.0)\n    \n                # leverage sign, gated by own speed\n                speed   = np.linalg.norm(Avel, axis=1)\n                to_opp  = Bpos[j] - Apos                            # self->opp\n                cross_z = to_opp[:,0]*Avel[:,1] - to_opp[:,1]*Avel[:,0]\n                lever   = np.where(speed > speed_eps, np.sign(cross_z), 0).astype('int8')\n    \n                rows = idx[A_idx]\n                df.loc[rows, 'opp_dmin']       = dmin\n                df.loc[rows, 'opp_close_rate'] = close\n                df.loc[rows, 'opp_leverage']   = lever\n    \n            # Offense w.r.t. Defense\n            _assign(side_off, side_def)\n            # Defense w.r.t. Offense\n            _assign(side_def, side_off)\n    \n        return df, out_cols\n\n    def _create_interaction_features_mid(self, df, speed_eps=0.5, k=2, radius=10.0):\n        \"\"\"\n        Medium-aggressive Receiver–Defender interaction:\n          - K=2 nearest opposite-side opponents (still per-frame)\n          - Keep target-only computation if 'player_to_predict' exists\n          - New features (small set):\n              opp_d2                 : 2nd nearest distance (clipped [0,30])\n              opp_dmean_k2           : mean distance of top-2\n              opp_close_rate_min_k2  : min closing rate among top-2 (more threatening)\n              opp_pursuit_error_min_k2: min |angle( v_opp , dir to self )| in degrees\n              opp_density_r10        : #opponents within radius (10 yards)\n          - Light temporal smoothing on previous base features:\n              opp_dmin_roll3, opp_close_rate_roll3\n        \"\"\"\n    \n        need = ['x','y','velocity_x','velocity_y','player_side','frame_id']\n        for c in need:\n            if c not in df.columns:\n                return df, []  # safe skip\n    \n        # 保证保守版的三列存在（以便 rolling）\n        base_cols = ['opp_dmin','opp_close_rate','opp_leverage']\n        for c in base_cols:\n            if c not in df.columns:\n                df[c] = np.nan\n    \n        # 新增列（初始化）\n        new_cols = ['opp_d2','opp_dmean_k2','opp_close_rate_min_k2',\n                    'opp_pursuit_error_min_k2','opp_density_r10','ally_density_r10']\n        for c in new_cols:\n            if c not in df.columns:\n                df[c] = np.nan\n    \n        key = ['game_id','play_id','frame_id']\n        use_mask_global = ('player_to_predict' in df.columns)\n    \n        def angle_between(v1, v2, eps=1e-6):\n            # 返回弧度的夹角 (0..pi)\n            dot = np.einsum('ij,ij->i', v1, v2)\n            n1  = np.linalg.norm(v1, axis=1) + eps\n            n2  = np.linalg.norm(v2, axis=1) + eps\n            cos = np.clip(dot / (n1*n2), -1.0, 1.0)\n            return np.arccos(cos)\n    \n        for _, g in df.groupby(key, sort=False):\n            idx = g.index.values\n            if len(g) <= 1:\n                continue\n    \n            pos = g[['x','y']].values.astype('float32')\n            vel = g[['velocity_x','velocity_y']].values.astype('float32')\n            side_off = (g['player_side'].values == 'Offense')\n            side_def = ~side_off\n    \n            # target-only\n            if use_mask_global:\n                tgt_mask = g['player_to_predict'].astype(bool).values\n            else:\n                tgt_mask = np.ones(len(g), dtype=bool)\n    \n            def _assign(A_mask, B_mask):\n                A_mask = A_mask & tgt_mask\n                A_idx = np.where(A_mask)[0]\n                B_idx = np.where(B_mask)[0]\n                if len(A_idx)==0 or len(B_idx)==0:\n                    return\n    \n                Apos, Bpos = pos[A_idx], pos[B_idx]\n                Avel, Bvel = vel[A_idx], vel[B_idx]\n    \n                # pairwise distances\n                dx = Apos[:,None,0] - Bpos[None,:,0]\n                dy = Apos[:,None,1] - Bpos[None,:,1]\n                D  = np.sqrt(dx*dx + dy*dy) + 1e-6          # (Na,Nb)\n                # Top-2 indices along last axis\n                k_use = min(k, D.shape[1])\n                part = np.argpartition(D, kth=range(k_use), axis=1)[:, :k_use]  # (Na, k_use)\n                # Gather top-2 distances and opponent indices\n                rows = np.arange(len(A_idx))[:,None]\n                d_top = D[rows, part]                 # (Na, k_use)\n                jidx  = part                          # opponent indices for each A\n    \n                # 基于 K=2 推导的派生量\n                d_sorted = np.sort(d_top, axis=1)     # 升序：d1, d2\n                d1 = d_sorted[:,0]\n                d2 = d_sorted[:,1] if k_use >= 2 else d_sorted[:,0]\n    \n                # 关闭率：对每个所选对手计算，再取 min\n                # opp->self 单位向量\n                # 先取最近对手用于 opp_dmin/close_rate（与保守版保持一致含义）\n                j1 = jidx[np.arange(len(A_idx)), np.argmin(d_top, axis=1)]\n                r1 = Apos - Bpos[j1]                         # opp->self\n                u1 = r1 / (np.linalg.norm(r1, axis=1, keepdims=True) + 1e-6)\n                vrel1 = Bvel[j1] - Avel\n                close1 = np.einsum('ij,ij->i', vrel1, u1)\n                close1 = np.clip(close1, -10.0, 10.0)\n    \n                #第二近对手的关闭率\n                if k_use >= 2:\n                    # 取第二近（不是最小值位置）\n                    order = np.argsort(d_top, axis=1)\n                    j2 = jidx[rows[:,0], order[:,1]]\n                    r2 = Apos - Bpos[j2]\n                    u2 = r2 / (np.linalg.norm(r2, axis=1, keepdims=True) + 1e-6)\n                    vrel2 = Bvel[j2] - Avel\n                    close2 = np.einsum('ij,ij->i', vrel2, u2)\n                    close2 = np.clip(close2, -10.0, 10.0)\n                    close_min_k2 = np.minimum(close1, close2)\n                else:\n                    close2 = close1\n                    close_min_k2 = close1\n    \n                #pursuit error：对手速度 vs 指向我方连线（取最小绝对值，单位度）\n                # 角度=atan2(|cross|, dot)≡arccos(dot/(||v||·||u||))\n                ang1 = angle_between(Bvel[j1], u1) * 180.0/np.pi\n                if k_use >= 2:\n                    ang2 = angle_between(Bvel[j2], u2) * 180.0/np.pi\n                    perr_min_k2 = np.minimum(np.abs(ang1), np.abs(ang2))\n                else:\n                    perr_min_k2 = np.abs(ang1)\n    \n                #局部密度（半径内的对手数）\n                density = (D <= radius).sum(axis=1).astype('float32')\n                #同侧密度（A方内的 self-self 距离）；简单起见：用 Apos 两两距离\n                rows_idx = idx[A_idx]  \n                \n                # 同侧密度\n                try:\n                    from scipy.spatial.distance import cdist\n                    A2A = cdist(Apos, Apos) + 1e-6\n                except Exception:\n                    # 无 scipy 的 fallback\n                    diff = Apos[:,None,:] - Apos[None,:,:]\n                    A2A = np.sqrt((diff * diff).sum(-1)) + 1e-6\n                same_density = (A2A <= radius).sum(axis=1) - 1  # 自身-1\n                \n                # 写回\n                df.loc[rows_idx, 'ally_density_r10'] = same_density.astype('float32')\n                df.loc[rows_idx, 'opp_dmin']       = np.clip(d1, 0.0, 30.0)\n                df.loc[rows_idx, 'opp_close_rate'] = close1\n                df.loc[rows_idx, 'opp_d2']                 = np.clip(d2, 0.0, 30.0)\n                df.loc[rows_idx, 'opp_dmean_k2']           = (np.clip(d1,0,30)+np.clip(d2,0,30))/2.0\n                df.loc[rows_idx, 'opp_close_rate_min_k2']  = close_min_k2\n                df.loc[rows_idx, 'opp_pursuit_error_min_k2']= perr_min_k2\n                df.loc[rows_idx, 'opp_density_r10']        = density\n    \n            # Offense vs Defense\n            _assign(side_off, side_def)\n            # Defense vs Offense\n            _assign(side_def, side_off)\n    \n        # 轻量时序平滑（3 帧 rolling）\n        # 注意：以个体维度分组，不跨人\n        for col in ['opp_dmin','opp_close_rate']:\n            if col in df.columns:\n                r = (\n                    df.groupby(self.gcols)[col]\n                      .rolling(3, min_periods=1).mean()\n                      .reset_index(level=list(range(len(self.gcols))), drop=True)\n                )\n                df[f'{col}_roll3'] = r\n    \n        out_cols = base_cols + new_cols + ['opp_dmin_roll3','opp_close_rate_roll3']\n        # 只返回本函数“新增/更新”的列（如果有的来自保守版就不重复计数也没关系）\n        out_cols = [c for c in out_cols if c in df.columns]\n        return df, out_cols\n\n    def _create_qb_relative_features(self, df):\n        \"\"\"\n        QB-relative geometry (per frame):\n          - qb_distance\n          - vel_to_qb_alignment, vel_to_qb_perp\n          - bearing_to_qb_signed (player facing vs vector to QB)\n        仅依赖: x,y,velocity_x,velocity_y,dir,player_role,frame_id\n        \"\"\"\n        need = ['x','y','velocity_x','velocity_y','dir','player_role','frame_id']\n        for c in need:\n            if c not in df.columns:\n                return df, []  # 缺列则安全跳过\n    \n        out_cols = ['qb_distance','vel_to_qb_alignment','vel_to_qb_perp','bearing_to_qb_signed','bearing_to_qb_sin','bearing_to_qb_cos']\n        for c in out_cols:\n            if c not in df.columns:\n                df[c] = np.nan\n    \n        key = ['game_id','play_id','frame_id']\n        for _, g in df.groupby(key, sort=False):\n            idx = g.index.values\n    \n            # 找本帧 QB（通常唯一；若多于1取第一个；找不到则跳过）\n            qb_rows = g[g['player_role'] == 'Passer']\n            if qb_rows.empty:\n                continue\n            qb_x = float(qb_rows.iloc[0]['x'])\n            qb_y = float(qb_rows.iloc[0]['y'])\n    \n            dx = g['x'].values.astype('float32') - qb_x\n            dy = g['y'].values.astype('float32') - qb_y\n            dist = np.sqrt(dx*dx + dy*dy) + 1e-6\n            ux, uy = dx/dist, dy/dist  # QB->player 单位向量\n    \n            vx = g['velocity_x'].values.astype('float32')\n            vy = g['velocity_y'].values.astype('float32')\n    \n            align = vx*ux + vy*uy\n            perp  = vx*(-uy) + vy*ux\n    \n            # bearing 差：玩家朝向 vs 指向 QB 的方向（有符号，(-180,180]）\n            dir_rad = np.deg2rad(g['dir'].fillna(0.0).astype('float32').values)\n            to_qb_angle = np.arctan2(-dy, -dx)  # player->QB\n            bearing = np.rad2deg(np.arctan2(np.sin(to_qb_angle - dir_rad),\n                                            np.cos(to_qb_angle - dir_rad)))\n    \n            df.loc[idx, 'qb_distance'] = dist\n            df.loc[idx, 'vel_to_qb_alignment'] = align\n            df.loc[idx, 'vel_to_qb_perp'] = perp\n            df.loc[idx, 'bearing_to_qb_signed'] = bearing\n            df.loc[idx, 'bearing_to_qb_sin'] = np.sin(np.deg2rad(bearing))\n            df.loc[idx, 'bearing_to_qb_cos'] = np.cos(np.deg2rad(bearing))\n    \n        return df, out_cols\n\n\n\n\n\n    def transform(self, df):\n        df = df.copy().sort_values(['game_id','play_id','nfl_id','frame_id'])\n        df = self._create_basic_features(df)\n\n        print(\"\\nStep 2/3: Adding selected advanced features...\")\n        for group_name in self.active_groups:\n            if group_name in self.feature_creators:\n                creator = self.feature_creators[group_name]\n                df, new_cols = creator(df)\n                self.created_feature_cols.extend(new_cols)\n                print(f\"  [+] Added '{group_name}' ({len(new_cols)} cols)\")\n            else:\n                print(f\"  [!] Unknown feature group: {group_name}\")\n\n        final_cols = sorted(set(self.created_feature_cols))\n        print(f\"\\nTotal features created: {len(final_cols)}\")\n        return df, final_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.536119Z","iopub.execute_input":"2025-10-20T08:51:10.536388Z","iopub.status.idle":"2025-10-20T08:51:10.627742Z","shell.execute_reply.started":"2025-10-20T08:51:10.536364Z","shell.execute_reply":"2025-10-20T08:51:10.626962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Sequence builder (unified frame + safe targets)\n# -------------------------------\ndef build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Return a Series indexed by (game_id, play_id) with values 'left'/'right'.\n    This keeps a clean MultiIndex that works for both pandas and cuDF pandas-API.\n    \"\"\"\n    s = (\n        df_in[['game_id','play_id','play_direction']]\n        .drop_duplicates()\n        .set_index(['game_id','play_id'])['play_direction']\n    )\n    return s  # MultiIndex Series\n\n\ndef apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Attach play_direction (if missing) and then unify to 'left'.\n    dir_map must be the MultiIndex Series produced by build_play_direction_map.\n    \"\"\"\n    if 'play_direction' not in df.columns:\n        dir_df = dir_map.reset_index()  # -> columns: game_id, play_id, play_direction\n        df = df.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n    return unify_left_direction(df)\n\n\n#[A] 统一键类型的辅助函数\ndef _canonicalize_key_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    for c in ('game_id','play_id','nfl_id'):\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors='coerce')\n    # 丢掉缺失键\n    df = df.dropna(subset=['game_id','play_id','nfl_id'])\n    # 统一为 int64\n    df['game_id'] = df['game_id'].astype('int64')\n    df['play_id'] = df['play_id'].astype('int64')\n    df['nfl_id']  = df['nfl_id'].astype('int64')\n    return df\n\ndef prepare_sequences_with_advanced_features(\n        input_df, output_df=None, test_template=None, \n        is_training=True, window_size=10, feature_groups=None):\n\n    print(f\"\\n{'='*80}\")\n    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES (UNIFIED FRAME)\")\n    print(f\"{'='*80}\")\n    print(f\"Window size: {window_size}\")\n\n    # --- [B] 先统一键类型（输入/输出/测试模板都处理） ---\n    input_df  = _canonicalize_key_dtypes(input_df)\n    if is_training:\n        assert output_df is not None\n        output_df = _canonicalize_key_dtypes(output_df)\n    else:\n        assert test_template is not None\n        test_template = _canonicalize_key_dtypes(test_template)\n\n    if feature_groups is None:\n        feature_groups = [\n            'distance_rate','target_alignment','multi_window_rolling','extended_lags',\n            'velocity_changes','field_position','role_specific','time_features',\n            'jerk_features','interaction_features_mid'\n        ]\n\n    # --- Direction map & unify ---\n    dir_map   = build_play_direction_map(input_df)\n    input_df_u= unify_left_direction(input_df)\n\n    if is_training:\n        out_u = apply_direction_to_df(output_df, dir_map)\n        target_rows   = out_u\n        target_groups = out_u[['game_id','play_id','nfl_id']].drop_duplicates()\n    else:\n        if 'play_direction' not in test_template.columns:\n            dir_df = dir_map.reset_index()\n            test_template = test_template.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n        target_rows   = test_template\n        target_groups = test_template[['game_id','play_id','nfl_id','play_direction']].drop_duplicates()\n\n    assert target_rows[['game_id','play_id','play_direction']].isna().sum().sum() == 0, \\\n        \"play_direction merge failed; check (game_id, play_id) coverage\"\n    print(\"play_direction merge OK:\", target_rows['play_direction'].value_counts(dropna=False).to_dict())\n\n    # --- FE ---\n    fe = FeatureEngineer(feature_groups)\n    processed_df, feature_cols = fe.transform(input_df_u)\n\n    # --- Build sequences ---\n    print(\"\\nStep 3/3: Creating sequences...\")\n    processed_df = processed_df.set_index(['game_id','play_id','nfl_id']).sort_index()\n    grouped = processed_df.groupby(level=['game_id','play_id','nfl_id'])\n\n    # [C] 可选：打印键覆盖率，快速定位 miss 的真正原因\n    avail_keys = (\n        processed_df.reset_index()[['game_id','play_id','nfl_id']]\n        .drop_duplicates()\n    )\n    inter = target_groups[['game_id','play_id','nfl_id']].merge(\n        avail_keys, on=['game_id','play_id','nfl_id'], how='inner'\n    )\n    print(f\"[COVERAGE] target_keys={len(target_groups)} | \"\n          f\"input_keys={len(avail_keys)} | \"\n          f\"matched={len(inter)}\")\n\n    # helpful indices\n    idx_x = feature_cols.index('x')\n    idx_y = feature_cols.index('y')\n\n    sequences, targets_dx, targets_dy, targets_fids, seq_meta = [], [], [], [], []\n\n    it = target_groups.itertuples(index=False)\n    it = tqdm(list(it), total=len(target_groups), desc=\"Creating sequences\")\n\n    for row in it:\n        gid = row[0]; pid = row[1]; nid = row[2]\n        play_dir = row[3] if (not is_training and len(row) >= 4) else None\n        key = (gid, pid, nid)\n\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n\n        input_window = group_df.tail(window_size)\n\n        # --- [D] 训练端也允许左侧填充（与测试一致），避免全被 <window_size 丢弃 ---\n        if len(input_window) < window_size:\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n\n        # input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        input_window = input_window.fillna(input_window.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n\n        if np.isnan(seq).any():\n            seq = np.nan_to_num(seq, nan=0.0)\n\n        sequences.append(seq)\n\n        if is_training:\n            out_grp = target_rows[\n                (target_rows['game_id']==gid) &\n                (target_rows['play_id']==pid) &\n                (target_rows['nfl_id']==nid)\n            ].sort_values('frame_id')\n            if len(out_grp)==0:\n                sequences.pop()  # 回滚\n                continue\n\n            last_x = seq[-1, idx_x]\n            last_y = seq[-1, idx_y]\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n\n            targets_dx.append(dx.astype(np.float32))\n            targets_dy.append(dy.astype(np.float32))\n            targets_fids.append(out_grp['frame_id'].values.astype(np.int32))\n\n        seq_meta.append({\n            'game_id': gid,\n            'play_id': pid,\n            'nfl_id': nid,\n            'frame_id': int(input_window.iloc[-1]['frame_id']) if len(input_window) else -1,\n            'play_direction': (None if is_training else play_dir),\n        })\n\n    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_fids, seq_meta, feature_cols, dir_map\n    return sequences, seq_meta, feature_cols, dir_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.628539Z","iopub.execute_input":"2025-10-20T08:51:10.628761Z","iopub.status.idle":"2025-10-20T08:51:10.646841Z","shell.execute_reply.started":"2025-10-20T08:51:10.628744Z","shell.execute_reply":"2025-10-20T08:51:10.646065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Tools for model saving & loading\n# -------------------------------\ndef _seed_dir(base_dir: Path, seed: int) -> Path:\n    d = base_dir / f\"seed_{seed}\"\n    d.mkdir(parents=True, exist_ok=True)\n    return d\n\ndef save_fold_artifacts(seed:int, fold:int, scaler, mx:nn.Module, my:nn.Module, base_dir:Path):\n    sdir = _seed_dir(base_dir, seed)\n    joblib.dump(scaler, sdir / f\"scaler_fold{fold}.pkl\")\n    torch.save(mx.state_dict(), sdir / f\"model_dx_fold{fold}.pt\")\n    torch.save(my.state_dict(), sdir / f\"model_dy_fold{fold}.pt\")\n\ndef write_meta(feature_cols:list, cfg:Config, base_dir:Path):\n    meta = {\n        \"seeds\": cfg.SEEDS,\n        \"n_folds\": cfg.N_FOLDS,\n        \"feature_cols\": feature_cols,\n        \"window_size\": cfg.WINDOW_SIZE,\n        \"max_future_horizon\": cfg.MAX_FUTURE_HORIZON,\n        \"feature_groups\": cfg.FEATURE_GROUPS,\n        \"version\": 1,\n        \"hidden_dim\": cfg.HIDDEN_DIM,\n        \"bidirectional\": getattr(cfg, \"BIDIRECTIONAL\", False),\n    }\n    with open(base_dir / \"meta.json\", \"w\") as f:\n        json.dump(meta, f)\n    print(f\"[META] wrote meta.json to {base_dir}\")\n\ndef load_saved_ensemble(cfg:Config, base_dir:Path):\n    meta_path = base_dir / \"meta.json\"\n    assert meta_path.exists(), f\"meta.json not found: {meta_path}\"\n    with open(meta_path, \"r\") as f:\n        meta = json.load(f)\n\n    feature_cols = meta[\"feature_cols\"]\n    horizon = int(meta[\"max_future_horizon\"])\n    seeds   = meta[\"seeds\"]\n    n_folds = int(meta[\"n_folds\"])\n    hidden_dim   = int(meta.get(\"hidden_dim\", 128))\n    bidirectional= bool(meta.get(\"bidirectional\", False))\n\n    models_x, models_y, scalers = [], [], []\n    for seed in seeds:\n        sdir = base_dir / f\"seed_{seed}\"\n        for fold in range(1, n_folds + 1):\n            sc_path = sdir / f\"scaler_fold{fold}.pkl\"\n            dx_path = sdir / f\"model_dx_fold{fold}.pt\"\n            dy_path = sdir / f\"model_dy_fold{fold}.pt\"\n            if not (sc_path.exists() and dx_path.exists() and dy_path.exists()):\n                print(f\"[WARN] missing seed={seed} fold={fold}, skip\")\n                continue\n            scaler = joblib.load(sc_path)\n            mx = SeqModel(len(feature_cols), horizon,\n                          hidden_dim=hidden_dim, bidirectional=bidirectional\n                          ).to(cfg.DEVICE)\n            mx.load_state_dict(torch.load(dx_path, map_location=cfg.DEVICE)); mx.eval()\n            my = SeqModel(len(feature_cols), horizon,\n                          hidden_dim=hidden_dim, bidirectional=bidirectional\n                          ).to(cfg.DEVICE)\n            my.load_state_dict(torch.load(dy_path, map_location=cfg.DEVICE)); my.eval()\n            scalers.append(scaler); models_x.append(mx); models_y.append(my)\n\n    assert len(models_x) > 0, f\"No models loaded from {base_dir}\"\n    print(f\"[LOAD] loaded {len(models_x)} ΔX & {len(models_y)} ΔY models from {base_dir}\")\n    return models_x, models_y, scalers, meta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.648683Z","iopub.execute_input":"2025-10-20T08:51:10.64893Z","iopub.status.idle":"2025-10-20T08:51:10.66526Z","shell.execute_reply.started":"2025-10-20T08:51:10.648913Z","shell.execute_reply":"2025-10-20T08:51:10.664584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Loss (Huber + time decay + 2nd-order velocity smooth)\n# -------------------------------\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03, lam_smooth=0.01):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n        self.lam_smooth = lam_smooth\n\n    def forward(self, pred, target, mask):\n        # base huber\n        err = pred - target\n        abs_err = torch.abs(err)\n        huber = torch.where(\n            abs_err <= self.delta,\n            0.5 * err * err,\n            self.delta * (abs_err - 0.5 * self.delta)\n        )\n\n        # time decay (keep your logic)\n        if self.time_decay and self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device, dtype=pred.dtype)\n            w = torch.exp(-self.time_decay * t).view(1, L)\n            huber = huber * w\n            mask  = mask  * w\n\n        main_loss = (huber * mask).sum() / (mask.sum() + 1e-8)\n\n        # velocity smooth (2nd difference ≈ jerk), conservative mask对齐\n        if self.lam_smooth and pred.size(1) > 2:\n            d1 = pred[:, 1:] - pred[:, :-1]          # [B, T-1]\n            d2 = d1[:, 1:] - d1[:, :-1]              # [B, T-2]\n            m2 = mask[:, 2:]                         # 对齐长度\n            smooth = (d2 * d2) * m2\n            smooth_loss = smooth.sum() / (m2.sum() + 1e-8)\n        else:\n            smooth_loss = pred.new_tensor(0.0)\n\n        return main_loss + self.lam_smooth * smooth_loss\n\n\n# class SeqModel(nn.Module):\n#     def __init__(self, input_dim, horizon):\n#         super().__init__()\n#         self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1)\n#         self.pool_ln = nn.LayerNorm(128)\n#         self.pool_attn = nn.MultiheadAttention(128, num_heads=4, batch_first=True)\n#         self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n#         self.head = nn.Sequential(\n#             nn.Linear(128, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, horizon)\n#         )\n#     def forward(self, x):\n#         h, _ = self.gru(x)\n#         B = h.size(0)\n#         q = self.pool_query.expand(B, -1, -1)\n#         ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n#         out = self.head(ctx.squeeze(1))\n#         return torch.cumsum(out, dim=1)\n\nclass ResidualMLP(nn.Module):\n    def __init__(self, d_in, d_hidden, horizon, dropout=0.2):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, d_hidden)\n        self.fc2 = nn.Linear(d_hidden, d_hidden)\n        self.proj = nn.Linear(d_in, d_hidden)  # skip\n        self.out = nn.Linear(d_hidden, horizon)\n        self.drop = nn.Dropout(dropout)\n        self.act = nn.GELU()\n    def forward(self, x):\n        y = self.drop(self.act(self.fc1(x)))\n        y = self.drop(self.act(self.fc2(y)) + self.proj(x))  # 残差\n        return self.out(y)\n\nclass SeqModel(nn.Module):\n    def __init__(self, input_dim, horizon, hidden_dim=128, num_layers=2, bidirectional=False,\n                 use_residual=True, n_queries=2):\n        super().__init__()\n        self.bidirectional = bidirectional\n        self.use_residual = use_residual\n        self.gru = nn.GRU(\n            input_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n            dropout=0.1, bidirectional=bidirectional\n        )\n        h_out = hidden_dim * (2 if bidirectional else 1)\n        self.in_proj = nn.Linear(input_dim, h_out) if use_residual else None\n\n        self.pool_ln   = nn.LayerNorm(h_out)\n        self.pool_attn = nn.MultiheadAttention(h_out, num_heads=4, batch_first=True)\n        self.pool_query= nn.Parameter(torch.randn(1, n_queries, h_out))  # 多query汇聚更多上下文\n        self.head      = ResidualMLP(h_out*n_queries, hidden_dim, horizon)\n\n    def forward(self, x):\n        h, _ = self.gru(x)                        # [B,T,h_out]\n        if self.use_residual:\n            h = h + self.in_proj(x)               # 时间维残差\n\n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)     # [B,Q,h_out]\n        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))  # [B,Q,h_out]\n        ctx = ctx.reshape(B, -1)                  # 拼成 [B, Q*h_out]\n        out = self.head(ctx)                      # [B, H]\n        return torch.cumsum(out, dim=1)\n\n\ndef prepare_targets(batch_axis, max_h):\n    tensors, masks = [], []\n    for arr in batch_axis:\n        L = len(arr)\n        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors), torch.stack(masks)\n\ndef train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config, noise_std=0.01, model_kwargs=None):\n    device = config.DEVICE\n    # model = SeqModel(input_dim, horizon).to(device)\n    model = SeqModel(input_dim, horizon, **(model_kwargs or {})).to(device)\n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=False)\n\n    # build batches (keep numpy → torch)\n    def build_batches(X, Y):\n        batches = []\n        B = config.BATCH_SIZE\n        for i in range(0, len(X), B):\n            end = min(i + B, len(X))\n            xs = torch.tensor(np.stack(X[i:end]).astype(np.float32))\n            ys, ms = prepare_targets([Y[j] for j in range(i, end)], horizon)\n            batches.append((xs, ys, ms))\n        return batches\n\n    tr_batches = build_batches(X_train, y_train)\n    va_batches = build_batches(X_val,   y_val)\n\n    best_loss, best_state, bad = float('inf'), None, 0\n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for bx, by, bm in tr_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            # 训练期增强（与 TTA 对齐）\n            bx = add_random_gaussian(bx, sigma_max=noise_std)  # 随机强度噪声\n            bx = random_time_mask(bx, p=0.10, max_width=3)    # 时间mask\n            bx = flip_context_keep_last(bx, p=0.10)           # 反转前T-1，末帧不动\n            pred = model(bx)\n\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by, bm in va_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                val_losses.append(criterion(pred, by, bm).item())\n\n        trl, val = float(np.mean(train_losses)), float(np.mean(val_losses))\n        scheduler.step(val)\n        if epoch % 10 == 0:\n            print(f\"  Epoch {epoch}: train={trl:.4f}, val={val:.4f}\")\n\n        if val < best_loss:\n            best_loss, bad = val, 0\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n\n    if best_state:\n        model.load_state_dict(best_state)\n    return model, best_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.666041Z","iopub.execute_input":"2025-10-20T08:51:10.66628Z","iopub.status.idle":"2025-10-20T08:51:10.688586Z","shell.execute_reply.started":"2025-10-20T08:51:10.666255Z","shell.execute_reply":"2025-10-20T08:51:10.687822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_with_tta_per_model(mx, my, scaler, X_test_raw, device, tta=6, noise_std=0.01, use_flip=True):\n    \"\"\"\n    对单个 (mx,my,scaler) 做 TTA，返回 [N,H] 的 dx,dy 预测。\n    - 在“标准化后的空间”加噪声（与训练一致）\n    - 可选：反转前 T-1 帧（末帧不动），与未反转结果平均\n    - 重复 tta 次取均值\n    \"\"\"\n    mx.eval(); my.eval()\n    outs_dx, outs_dy = [], []\n    base = np.stack([scaler.transform(s) for s in X_test_raw]).astype(np.float32)\n    xt = torch.tensor(base, device=device)\n\n    for _ in range(max(1, tta)):\n        xt_aug = xt\n        if noise_std and noise_std > 0:\n            xt_aug = xt_aug + torch.randn_like(xt_aug) * noise_std\n\n        with torch.no_grad():\n            dx = mx(xt_aug)\n            dy = my(xt_aug)\n            if use_flip and xt_aug.size(1) > 1:\n                ctx = xt_aug[:, :-1].flip(1)\n                xt_flip = torch.cat([ctx, xt_aug[:, -1:].clone()], dim=1)\n                dx = 0.5 * (dx + mx(xt_flip))\n                dy = 0.5 * (dy + my(xt_flip))\n\n        outs_dx.append(dx.detach().cpu().numpy())\n        outs_dy.append(dy.detach().cpu().numpy())\n\n    return np.mean(outs_dx, axis=0), np.mean(outs_dy, axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.689196Z","iopub.execute_input":"2025-10-20T08:51:10.689469Z","iopub.status.idle":"2025-10-20T08:51:10.702302Z","shell.execute_reply.started":"2025-10-20T08:51:10.689448Z","shell.execute_reply":"2025-10-20T08:51:10.701569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random as _py_random\nimport torch\n\ndef random_time_mask(bx, p=0.1, max_width=3):\n    \"\"\"\n    在时间维做短段复制遮挡：随机挑一段 [s, s+w) 用前/后一帧值替换。\n    - bx: [B, T, D] (torch, 支持在 GPU 上原地改)\n    \"\"\"\n    if p <= 0 or max_width <= 0:\n        return bx\n    B, T, D = bx.shape\n    if T <= 1:\n        return bx\n    for i in range(B):\n        if _py_random.random() < p:\n            w = _py_random.randint(1, max_width)\n            s = _py_random.randint(0, max(0, T - 1 - w))\n            if s > 0:\n                bx[i, s:s+w] = bx[i, s-1].unsqueeze(0)\n            else:\n                bx[i, s:s+w] = bx[i, s+w].unsqueeze(0)\n    return bx\n\ndef flip_context_keep_last(bx, p=0.1):\n    \"\"\"\n    仅反转前 T-1 帧（保持最后一帧不动），制造“上下文反转”。\n    \"\"\"\n    if p <= 0:\n        return bx\n    B, T, D = bx.shape\n    if T <= 1:\n        return bx\n    mask = torch.rand(B, device=bx.device) < p\n    if mask.any():\n        ctx = bx[mask, :-1].flip(1)\n        bx[mask] = torch.cat([ctx, bx[mask, -1:].clone()], dim=1)\n    return bx\n\ndef add_random_gaussian(bx, sigma_max=0.02):\n    \"\"\"\n    给整段序列加一次高斯噪声（强度在 [0, sigma_max] 内随机）。\n    \"\"\"\n    if sigma_max <= 0:\n        return bx\n    sigma = sigma_max * torch.rand(1, device=bx.device)\n    return bx + torch.randn_like(bx) * sigma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.703249Z","iopub.execute_input":"2025-10-20T08:51:10.703946Z","iopub.status.idle":"2025-10-20T08:51:10.721495Z","shell.execute_reply.started":"2025-10-20T08:51:10.703923Z","shell.execute_reply":"2025-10-20T08:51:10.72072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------_\n# Main pipeline (MODIFICADO PARA ENSEMBLE DE SEMILLAS)\n# ------------------------------_\nclass CFG(Config):\n    # Añadimos la lista de semillas para el ensemble\n    SEEDS = [42, 19, 89,64,33] # ¡Puedes cambiar o añadir más semillas aquí!\n\ndef main():\n    cfg = CFG()\n    print(\"=\"*80)\n    print(f\"RUN MODE: TRAIN={getattr(cfg, 'TRAIN', False)} | SUB={getattr(cfg, 'SUB', False)}\")\n    print(\"=\"*80)\n    print(f\"cuDF backend active? {USE_CUDF}\")\n\n    # --- sanity checks ---\n    if not cfg.TRAIN and not cfg.SUB:\n        raise ValueError(\"Please set a run mode: TRAIN=True/SUB=False or TRAIN=False/SUB=True\")\n    if cfg.TRAIN and cfg.SUB:\n        raise ValueError(\"TRAIN and SUB cannot both be True\")\n\n    # ---------------------------\n    # TRAIN: train & save to a writable dir\n    # ---------------------------\n    if cfg.TRAIN:\n        # redirect saving path to a writable place\n        save_dir = Path(\"./saved_models\")\n        save_dir.mkdir(parents=True, exist_ok=True)\n        cfg.MODELS_DIR = save_dir   # <<< key line: write into /kaggle/working\n\n        # 1) load training data\n        print(\"\\n[1/4] 加载训练数据…\")\n        train_input_files  = [cfg.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\"  for w in range(1, 19)]\n        train_output_files = [cfg.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n        train_input  = pd.concat([pd.read_csv(f) for f in train_input_files  if f.exists()], ignore_index=True)\n        train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()], ignore_index=True)\n\n        # 2) features + sequences (unified direction)\n        print(\"\\n[2/4] 特征与序列（统一方向）…\")\n        feature_groups = getattr(cfg, \"FEATURE_GROUPS\", None)\n        seqs, tdx, tdy, tfids, seq_meta, feat_cols, dir_map = prepare_sequences_with_advanced_features(\n            train_input, output_df=train_output, is_training=True,\n            window_size=cfg.WINDOW_SIZE, feature_groups=feature_groups\n        )\n        sequences  = list(seqs)\n        targets_dx = list(tdx)\n        targets_dy = list(tdy)\n\n        # 2.5) write meta to the same (writable) dir\n        write_meta(feat_cols, cfg, base_dir=cfg.MODELS_DIR)\n\n\n        # 3) multi-seed × KFold, save per-fold artifacts\n        print(\"\\n[3/4] 多种子 × K 折训练并保存模型…\")\n        # groups = np.array([d['game_id'] for d in seq_meta])\n        groups = np.array([f\"{d['game_id']}_{d['play_id']}\" for d in seq_meta])\n\n        seeds = getattr(cfg, \"SEEDS\", [cfg.SEED])\n        all_rmse = []        # 所有 seed×fold 的 per-dim RMSE\n        cv_log = []          # 也把每折指标放进列表，最后写 json\n\n        for seed in seeds:\n            print(f\"\\n{'='*70}\\n   Seed {seed}\\n{'='*70}\")\n            set_seed(seed)\n            gkf = GroupKFold(n_splits=cfg.N_FOLDS)\n\n            fold_rmses = []  # 当前 seed 的每折 RMSE（per-dim）\n\n            for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n                print(f\"\\n{'-'*60}\\nFold {fold}/{cfg.N_FOLDS} (seed {seed})\\n{'-'*60}\")\n\n                X_tr = [sequences[i] for i in tr]\n                X_va = [sequences[i] for i in va]\n\n                scaler = StandardScaler()\n                scaler.fit(np.vstack([s for s in X_tr]))\n\n                X_tr_sc = np.stack([scaler.transform(s) for s in X_tr]).astype(np.float32)\n                X_va_sc = np.stack([scaler.transform(s) for s in X_va]).astype(np.float32)\n\n\n                # model_kwargs = dict(hidden_dim=cfg.HIDDEN_DIM, bidirectional=getattr(cfg, \"BIDIRECTIONAL\", False))\n                model_kwargs = dict(\n                    hidden_dim=cfg.HIDDEN_DIM,\n                    bidirectional=getattr(cfg, \"BIDIRECTIONAL\", False),\n                    use_residual=True,\n                    n_queries=2\n                )\n                \n                print(\"训练 ΔX …\")\n                mx, loss_x = train_model(\n                    X_tr_sc, [targets_dx[i] for i in tr],\n                    X_va_sc, [targets_dx[i] for i in va],\n                    X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg,\n                    model_kwargs=model_kwargs\n                )\n                \n                print(\"训练 ΔY …\")\n                my, loss_y = train_model(\n                    X_tr_sc, [targets_dy[i] for i in tr],\n                    X_va_sc, [targets_dy[i] for i in va],\n                    X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg,\n                    model_kwargs=model_kwargs\n                )\n\n\n                # --- NEW: 计算三口径，默认用 per-dim 进 CV 汇总 ---\n                rmse_perdim = compute_val_rmse(\n                    mx, my, X_va_sc,\n                    [targets_dx[i] for i in va],\n                    [targets_dy[i] for i in va],\n                    cfg.MAX_FUTURE_HORIZON, cfg.DEVICE, mode=\"per-dim\"\n                )\n                rmse_2d = compute_val_rmse(\n                    mx, my, X_va_sc,\n                    [targets_dx[i] for i in va],\n                    [targets_dy[i] for i in va],\n                    cfg.MAX_FUTURE_HORIZON, cfg.DEVICE, mode=\"2d\"\n                )\n                mean_dist = compute_val_rmse(\n                    mx, my, X_va_sc,\n                    [targets_dx[i] for i in va],\n                    [targets_dy[i] for i in va],\n                    cfg.MAX_FUTURE_HORIZON, cfg.DEVICE, mode=\"mean-dist\"\n                )\n\n                print(f\"[VAL] seed {seed} fold {fold} → \"\n                      f\"Huber dx={loss_x:.5f}, dy={loss_y:.5f} | \"\n                      f\"per-dim RMSE={rmse_perdim:.4f} | 2D RMSE={rmse_2d:.4f} | meanDist={mean_dist:.4f} yards\")\n\n                fold_rmses.append(rmse_perdim)\n                all_rmse.append(rmse_perdim)\n                cv_log.append({\n                    \"seed\": seed, \"fold\": fold,\n                    \"rmse_perdim\": rmse_perdim,\n                    \"rmse_2d\": rmse_2d,\n                    \"mean_dist\": mean_dist,\n                    \"loss_dx\": float(loss_x),\n                    \"loss_dy\": float(loss_y),\n                })\n\n                # 保存模型\n                save_fold_artifacts(seed=seed, fold=fold, scaler=scaler, mx=mx, my=my, base_dir=cfg.MODELS_DIR)\n\n            # --- NEW: 当前 seed 汇总 ---\n            print(f\"[SEED SUMMARY] seed {seed} RMSEs: {[f'{r:.4f}' for r in fold_rmses]} | \"\n                  f\"mean={float(np.mean(fold_rmses)):.4f} yards\")\n\n        # --- NEW: 所有 seeds×folds 的最终汇总 & 落盘 ---\n        print(f\"[CV SUMMARY] all folds RMSEs: {[f'{r:.4f}' for r in all_rmse]}\")\n        print(f\"[CV SUMMARY] overall mean RMSE = {float(np.mean(all_rmse)):.4f} yards\")\n\n        # 写到磁盘（方便回看）\n        try:\n            with open(cfg.MODELS_DIR / \"cv_metrics.json\", \"w\") as f:\n                json.dump({\"per_fold\": cv_log, \"overall_mean_perdim\": float(np.mean(all_rmse))}, f, indent=2)\n            print(f\"✓ CV metrics written to {cfg.MODELS_DIR / 'cv_metrics.json'}\")\n        except Exception as e:\n            print(f\"[WARN] writing cv_metrics.json failed: {e}\")\n\n\n        print(\"\\n\" + \"=\"*80)\n        print(\"COMPLETE (TRAIN)!\")\n        print(\"=\"*80)\n        print(f\"✓ Models saved to: {cfg.MODELS_DIR}\")\n        print(f\"Seeds: {cfg.SEEDS} | Folds: {cfg.N_FOLDS} → checkpoints per axis: {len(cfg.SEEDS)*cfg.N_FOLDS}\")\n        print(f\"Features used: {len(feat_cols)}  (cuDF active: {USE_CUDF})\")\n        return\n\n    # ---------------------------\n    # SUB: load from read-only input dir & infer\n    # ---------------------------\n    if cfg.SUB:\n        # DO NOT change cfg.MODELS_DIR here — keep it as the dataset input path\n        print(\"\\n[1/3] 加载测试数据…\")\n        test_input    = pd.read_csv(cfg.DATA_DIR / \"test_input.csv\")\n        test_template = pd.read_csv(cfg.DATA_DIR / \"test.csv\")\n\n        print(\"\\n[2/3] 读取已保存的模型与元信息…\")\n        # models_x, models_y, scalers, meta = load_saved_ensemble(cfg)\n        models_x, models_y, scalers, meta = load_saved_ensemble(cfg, base_dir=cfg.MODELS_DIR)\n\n        saved_feature_cols = meta[\"feature_cols\"]\n        saved_groups       = meta.get(\"feature_groups\", getattr(cfg, \"FEATURE_GROUPS\", None))\n        saved_window       = int(meta.get(\"window_size\", cfg.WINDOW_SIZE))\n\n        print(\"\\n[3/3] 构建测试序列并推理（统一方向 → 反变换）…\")\n        test_seqs, test_meta, feat_cols_t, _ = prepare_sequences_with_advanced_features(\n            test_input, test_template=test_template, is_training=False,\n            window_size=saved_window, feature_groups=saved_groups\n        )\n        assert feat_cols_t == saved_feature_cols, \\\n            f\"特征列不一致！训练: {len(saved_feature_cols)} vs 测试: {len(feat_cols_t)}\"\n\n        idx_x = feat_cols_t.index('x')\n        idx_y = feat_cols_t.index('y')\n\n        X_test_raw = list(test_seqs)\n        x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n        y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n\n        # TTA across models\n        tta_times   = 6           # 可调：4~8 都行\n        tta_noise   = 0.01        # 与训练同量级或略小\n        use_flip_ta = True\n        \n        all_preds_dx, all_preds_dy = [], []\n        for mx, my, sc in zip(models_x, models_y, scalers):\n            dx_tta, dy_tta = predict_with_tta_per_model(\n                mx, my, sc, X_test_raw, cfg.DEVICE,\n                tta=tta_times, noise_std=tta_noise, use_flip=use_flip_ta\n            )\n            all_preds_dx.append(dx_tta)\n            all_preds_dy.append(dy_tta)\n        \n        ens_dx = np.mean(all_preds_dx, axis=0)\n        ens_dy = np.mean(all_preds_dy, axis=0)\n\n        H = ens_dx.shape[1]\n\n        rows = []\n        tt_idx = test_template.set_index(['game_id','play_id','nfl_id']).sort_index()\n        for i, meta_row in enumerate(test_meta):\n            gid = meta_row['game_id']; pid = meta_row['play_id']; nid = meta_row['nfl_id']\n            play_is_right = (meta_row['play_direction'] == 'right')\n            try:\n                fids = tt_idx.loc[(gid,pid,nid),'frame_id']\n                if isinstance(fids, pd.Series): fids = fids.sort_values().tolist()\n                else: fids = [int(fids)]\n            except KeyError:\n                continue\n\n            for t, fid in enumerate(fids):\n                tt = min(t, H - 1)\n                x_uni = np.clip(x_last_uni[i] + ens_dx[i, tt], 0, FIELD_LENGTH)\n                y_uni = np.clip(y_last_uni[i] + ens_dy[i, tt], 0, FIELD_WIDTH)\n                x_out, y_out = invert_to_original_direction(x_uni, y_uni, play_is_right)\n                rows.append({'id': f\"{gid}_{pid}_{nid}_{int(fid)}\", 'x': x_out, 'y': y_out})\n\n        submission = pd.DataFrame(rows)\n        submission.to_csv(\"submission.csv\", index=False)\n        print(\"\\n\" + \"=\"*80)\n        print(\"COMPLETE (SUBMIT)!\")\n        print(\"=\"*80)\n        print(f\"✓ Submission saved to submission.csv  |  Rows: {len(submission)}\")\n        print(f\"Total models in ensemble: {len(models_x)}\")\n        print(f\"Features used: {len(saved_feature_cols)}  (cuDF active: {USE_CUDF})\")\n        return\n\n    # 如果两个 flag 都没开，给出提醒\n    raise ValueError(\"请在 Config 中设置运行模式：TRAIN=True/SUB=False 或 TRAIN=False/SUB=True\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:51:10.722402Z","iopub.execute_input":"2025-10-20T08:51:10.722638Z","iopub.status.idle":"2025-10-20T08:52:21.029756Z","shell.execute_reply.started":"2025-10-20T08:51:10.722614Z","shell.execute_reply":"2025-10-20T08:52:21.029071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}