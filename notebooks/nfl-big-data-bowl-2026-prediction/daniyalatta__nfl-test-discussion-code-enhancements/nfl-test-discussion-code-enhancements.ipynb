{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nNFL Big Data Bowl 2026 - COMPLETE DEBUGGED PIPELINE (single-file)\nKey: This script combines the earlier pipeline + improvements and\nincorporates Kaggle discussion lessons:\n\n - orientation: 0° = North (+y). Use sin/cos ordering: x += sin(angle), y += cos(angle)\n - input vs output: input = pre-pass frames, output = post-pass frames (frame_id resets)\n - player_to_predict: only those flagged True are scored targets; False players are context\n - frame sequences: GroupKFold on game_id for CV\n - lag features + EMA + rolling + physical features\n - RobustScaler (fit on train only) to avoid train/test distribution mismatch\n - GRU encoder + attention pooling, LayerNorm, dropout\n - Mixed Precision (AMP), gradient clipping, weighted ensemble by val loss\n - target generation: predict displacement relative to last observed input frame\n\"\"\"\n\nimport os\nimport math\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import GroupKFold\n\nwarnings.filterwarnings(\"ignore\")\n\n# -------------------------\n# CONFIG\n# -------------------------\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 256\n    EPOCHS = 150\n    PATIENCE = 25\n    LR = 1e-3\n    WINDOW_SIZE = 8\n    HIDDEN = 128\n    MAX_HORIZON = 94  # maximum frames to predict\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    USE_AMP = True\n\n# reproducibility\ndef set_seed(s=42):\n    import random\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    os.environ[\"PYTHONHASHSEED\"] = str(s)\n\nset_seed(Config.SEED)\n\n# -------------------------\n# HELPERS & NOTES (from Kaggle discussion)\n# -------------------------\n# Noted fixes incorporated:\n#  - Orientation: 0 deg is North (+y). So x component uses sin(angle), y uses cos(angle).\n#  - input_*.csv = pre-throw frames; output_*.csv = post-throw frames. frame_id resets.\n#  - player_to_predict indicates label targets (True) vs background (False)\n#  - Use lag features, EMA, rolling averages\n#  - Scale using train-only scaler to avoid train/test mismatch\n#  - Use GroupKFold by game_id for CV\n#  - Ensure padding & masks for variable-length outputs\n\ndef height_to_feet(h):\n    try:\n        ft, inch = map(int, str(h).split('-'))\n        return ft + inch/12.0\n    except:\n        return 6.0\n\n# safe id builder for Kaggle submission mapping\ndef build_id(game_id, play_id, nfl_id, frame_id):\n    return f\"{int(game_id)}_{int(play_id)}_{int(nfl_id)}_{int(frame_id)}\"\n\n# -------------------------\n# FEATURE ENGINEERING\n# -------------------------\ndef add_features(df):\n    # operate on copy\n    df = df.copy()\n    # player features\n    df['player_height_feet'] = df['player_height'].apply(height_to_feet)\n    df['player_weight'] = df['player_weight'].fillna(200.0)\n    # orientation/dir: 0° = North (+y). Use sin for x, cos for y (discussion correction).\n    # dir column may be NaN -> fill 0\n    dir_deg = df['dir'].fillna(0.0).astype(float)\n    dir_rad = np.deg2rad(dir_deg)\n    # speed and acceleration\n    df['s'] = df['s'].fillna(0.0)\n    df['a'] = df['a'].fillna(0.0)\n    # velocity components (correct trigonometry per discussion)\n    delta_t = 0.1  # assumed per Kaggle data spec\n    df['velocity_x'] = (df['s'] + 0.5 * df['a'] * delta_t) * np.sin(dir_rad)\n    df['velocity_y'] = (df['s'] + 0.5 * df['a'] * delta_t) * np.cos(dir_rad)\n    df['acceleration_x'] = df['a'] * np.sin(dir_rad)\n    df['acceleration_y'] = df['a'] * np.cos(dir_rad)\n    # role booleans (player_role & player_side may need mapping)\n    df['player_role'] = df.get('player_role', pd.Series(['']*len(df)))\n    df['player_side'] = df.get('player_side', pd.Series(['']*len(df)))\n    df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n    df['is_defense'] = (df['player_side'] == 'Defense').astype(int)\n    df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['is_passer'] = (df['player_role'] == 'Passer').astype(int)\n    # physics-derived\n    mass = df['player_weight'].fillna(200.0) / 2.20462\n    df['momentum_x'] = df['velocity_x'] * mass\n    df['momentum_y'] = df['velocity_y'] * mass\n    df['kinetic'] = 0.5 * mass * (df['s']**2)\n    # ball features (if present)\n    if 'ball_land_x' in df.columns and 'ball_land_y' in df.columns:\n        dx = df['ball_land_x'] - df['x']\n        dy = df['ball_land_y'] - df['y']\n        df['dist_to_ball'] = np.sqrt(dx**2 + dy**2)\n        df['angle_to_ball'] = np.arctan2(dy, dx)\n        df['ball_dir_x'] = dx / (df['dist_to_ball'] + 1e-6)\n        df['ball_dir_y'] = dy / (df['dist_to_ball'] + 1e-6)\n        df['closing_speed'] = df['velocity_x'] * df['ball_dir_x'] + df['velocity_y'] * df['ball_dir_y']\n    return df\n\ndef make_lags_and_temporal(df, window_size=8):\n    df = df.copy()\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    df = df.sort_values(gcols + ['frame_id'])\n    for lag in (1,2,3):\n        df[f'x_lag{lag}'] = df.groupby(gcols)['x'].shift(lag)\n        df[f'y_lag{lag}'] = df.groupby(gcols)['y'].shift(lag)\n        df[f'vel_x_lag{lag}'] = df.groupby(gcols)['velocity_x'].shift(lag)\n        df[f'vel_y_lag{lag}'] = df.groupby(gcols)['velocity_y'].shift(lag)\n    # EMA on velocity and speed\n    df['vel_x_ema'] = df.groupby(gcols)['velocity_x'].transform(lambda s: s.ewm(alpha=0.3, adjust=False).mean())\n    df['vel_y_ema'] = df.groupby(gcols)['velocity_y'].transform(lambda s: s.ewm(alpha=0.3, adjust=False).mean())\n    df['speed_ema'] = df.groupby(gcols)['s'].transform(lambda s: s.ewm(alpha=0.3, adjust=False).mean())\n    # rolling mean\n    df['vel_x_roll'] = df.groupby(gcols)['velocity_x'].transform(lambda s: s.rolling(window_size, min_periods=1).mean())\n    df['vel_y_roll'] = df.groupby(gcols)['velocity_y'].transform(lambda s: s.rolling(window_size, min_periods=1).mean())\n    return df\n\n# -------------------------\n# SEQUENCE CREATION (align input/output per Kaggle design)\n# -------------------------\ndef prepare_sequences(input_df, output_df=None, test_template=None, window_size=8, is_training=True):\n    \"\"\"Create sequences per (game, play, nfl_id)\n       - input_df: pre-throw tracking (train_input or test_input)\n       - output_df: post-throw tracking (train_output) when training\n       - test_template: test.csv (used to build sequence ids when is_training=False)\n       - follow discussion guidelines for alignment & player_to_predict\n    \"\"\"\n    print(f\"Preparing sequences (window={window_size})  training={is_training}\")\n    # copy and add features\n    inp = add_features(input_df)\n    inp = make_lags_and_temporal(inp, window_size=window_size)\n\n    # decide feature list (only keep existent)\n    candidate_features = [\n        'x','y','s','a','o','dir','frame_id',\n        'player_height_feet','player_weight',\n        'velocity_x','velocity_y','acceleration_x','acceleration_y',\n        'momentum_x','momentum_y','kinetic',\n        'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n        'dist_to_ball','angle_to_ball','ball_dir_x','ball_dir_y','closing_speed',\n        'x_lag1','y_lag1','vel_x_lag1','vel_y_lag1',\n        'x_lag2','y_lag2','vel_x_lag2','vel_y_lag2',\n        'x_lag3','y_lag3','vel_x_lag3','vel_y_lag3',\n        'vel_x_ema','vel_y_ema','speed_ema','vel_x_roll','vel_y_roll'\n    ]\n    features = [c for c in candidate_features if c in inp.columns]\n\n    # group index\n    gcols = ['game_id','play_id','nfl_id']\n    inp.set_index(gcols, inplace=True, drop=False)\n    grouped = inp.groupby(level=gcols)\n\n    # target groups: training uses output_df players with player_to_predict True\n    if is_training:\n        # ensure output_df sorted by frame\n        out = output_df.copy().sort_values(['game_id','play_id','nfl_id','frame_id'])\n        target_groups = out[['game_id','play_id','nfl_id']].drop_duplicates()\n    else:\n        # test_template contains rows for each id; group by unique players\n        t = test_template[['game_id','play_id','nfl_id']].drop_duplicates()\n        target_groups = t\n\n    sequences = []\n    targets_dx = []\n    targets_dy = []\n    frames_list = []\n    seq_ids = []\n\n    for _, r in tqdm(target_groups.iterrows(), total=len(target_groups)):\n        key = (r['game_id'], r['play_id'], r['nfl_id'])\n        try:\n            group = grouped.get_group(key)\n        except KeyError:\n            # no input data for that key (possible); skip (consistent with Kaggle chat)\n            continue\n\n        # use last 'window_size' frames from input (pre-throw)\n        input_window = group.tail(window_size).reset_index(drop=True)\n\n        # if less than window and training -> skip (we cannot create robust sample)\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            # if test, pad at top\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n\n        # Fill missing numeric with group means (per Kaggle suggestion: group mean > global)\n        numeric_means = group.mean(numeric_only=True)\n        input_window = input_window.fillna(numeric_means)\n\n        seq = input_window[features].values.astype(np.float32)\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n\n        sequences.append(seq)\n        seq_ids.append({\n            'game_id': key[0], 'play_id': key[1], 'nfl_id': key[2],\n            'last_frame_id': int(input_window.iloc[-1]['frame_id'])\n        })\n\n        if is_training:\n            # targets are displacements from last input frame to each output frame (post-throw)\n            out_grp = out[(out['game_id']==key[0]) & (out['play_id']==key[1]) & (out['nfl_id']==key[2])]\n            if out_grp.shape[0] == 0:\n                # no output rows for this player (possible for background players)\n                targets_dx.append(np.array([], dtype=np.float32))\n                targets_dy.append(np.array([], dtype=np.float32))\n                frames_list.append(np.array([], dtype=np.int32))\n            else:\n                last_x = float(input_window.iloc[-1]['x'])\n                last_y = float(input_window.iloc[-1]['y'])\n                dx = (out_grp['x'].values.astype(np.float32) - last_x).astype(np.float32)\n                dy = (out_grp['y'].values.astype(np.float32) - last_y).astype(np.float32)\n                targets_dx.append(dx)\n                targets_dy.append(dy)\n                frames_list.append(out_grp['frame_id'].values.astype(np.int32))\n\n    print(f\"Sequences built: {len(sequences)}\")\n    if is_training:\n        return sequences, targets_dx, targets_dy, frames_list, seq_ids, features\n    else:\n        return sequences, seq_ids, features\n\n# -------------------------\n# LOSS (Temporal Huber with time decay) - consistent with earlier\n# -------------------------\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n\n    def forward(self, pred, target, mask):\n        # pred: [B, T], target: [B, T], mask: [B, T]\n        err = pred - target\n        abs_err = torch.abs(err)\n        huber = torch.where(abs_err <= self.delta, 0.5 * err * err, self.delta * (abs_err - 0.5 * self.delta))\n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            weight = torch.exp(-self.time_decay * t).view(1, L)\n            huber = huber * weight\n            mask = mask * weight\n        loss = (huber * mask).sum() / (mask.sum() + 1e-8)\n        return loss\n\n# -------------------------\n# MODEL (GRU + Attention pooling)\n# -------------------------\nclass SeqModel(nn.Module):\n    def __init__(self, input_dim, horizon, hidden=Config.HIDDEN):\n        super().__init__()\n        self.horizon = horizon\n        self.ln_in = nn.LayerNorm(input_dim)\n        self.gru = nn.GRU(input_dim, hidden, num_layers=2, batch_first=True, dropout=0.1)\n        self.attn_ln = nn.LayerNorm(hidden)\n        self.attn = nn.MultiheadAttention(hidden, num_heads=4, batch_first=True)\n        self.q = nn.Parameter(torch.randn(1, 1, hidden))\n        self.head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.GELU(),\n            nn.Dropout(0.25),\n            nn.Linear(hidden, horizon)\n        )\n\n    def forward(self, x):\n        # x: [B, window, feat]\n        x = self.ln_in(x)\n        h, _ = self.gru(x)  # [B, window, hidden]\n        B = h.size(0)\n        q = self.q.expand(B, -1, -1)\n        h_norm = self.attn_ln(h)\n        ctx, _ = self.attn(q, h_norm, h_norm)  # [B,1,hidden]\n        ctx = ctx.squeeze(1)\n        out = self.head(ctx)  # predicted cumulative deltas steps?\n        out = torch.cumsum(out, dim=1)  # keep cumulative behavior (matches baseline)\n        return out\n\n# -------------------------\n# TRAIN UTILITIES\n# -------------------------\ndef pad_targets(tlist, max_h):\n    # tlist: list of numpy arrays (length L_i each)\n    padded = np.zeros((len(tlist), max_h), dtype=np.float32)\n    mask = np.zeros((len(tlist), max_h), dtype=np.float32)\n    for i, arr in enumerate(tlist):\n        L = len(arr)\n        if L == 0:\n            continue\n        l = min(L, max_h)\n        padded[i, :l] = arr[:l]\n        mask[i, :l] = 1.0\n    return torch.tensor(padded), torch.tensor(mask)\n\ndef train_single(X_tr, y_tr, X_val, y_val, input_dim, horizon, cfg):\n    device = cfg.DEVICE\n    model = SeqModel(input_dim, horizon).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n    loss_fn = TemporalHuber()\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.USE_AMP)\n\n    # prepare batches (pre-batch arrays converted to tensors on the fly to reduce memory)\n    def gen_batches(X, Y, batch_size):\n        n = len(X)\n        for i in range(0, n, batch_size):\n            j = min(n, i+batch_size)\n            bx = torch.tensor(np.stack(X[i:j]), dtype=torch.float32).to(device)\n            by = Y[i:j]\n            yield bx, by\n\n    best_val = 1e9\n    best_state = None\n    bad = 0\n\n    for epoch in range(cfg.EPOCHS):\n        model.train()\n        train_losses = []\n        for bx, by in gen_batches(X_tr, y_tr, cfg.BATCH_SIZE):\n            # by: (list of arrays) -> pad\n            by_padded, mask = pad_targets(by, horizon)\n            by_padded = by_padded.to(device)\n            mask = mask.to(device)\n\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast(enabled=cfg.USE_AMP):\n                pred = model(bx)\n                loss = loss_fn(pred, by_padded, mask)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad(), torch.cuda.amp.autocast(enabled=cfg.USE_AMP):\n            for bx, by in gen_batches(X_val, y_val, cfg.BATCH_SIZE):\n                by_padded, mask = pad_targets(by, horizon)\n                by_padded = by_padded.to(device)\n                mask = mask.to(device)\n                pred = model(bx.to(device))\n                val_losses.append(loss_fn(pred, by_padded, mask).item())\n\n        train_mean = float(np.mean(train_losses)) if train_losses else 0.0\n        val_mean = float(np.mean(val_losses)) if val_losses else 1e9\n        scheduler.step(val_mean)\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch:03d} train={train_mean:.5f} val={val_mean:.5f}\")\n\n        if val_mean < best_val:\n            best_val = val_mean\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= cfg.PATIENCE:\n                print(f\"Early stopping at epoch {epoch} (val did not improve)\")\n                break\n\n    model.load_state_dict(best_state)\n    return model, best_val\n\n# -------------------------\n# PIPELINE: TRAIN + ENSEMBLE\n# -------------------------\ndef run_training(sequences, targets_dx, targets_dy, seq_ids, features, cfg):\n    # sequences: list of [window, feat] arrays\n    # targets_dx/dy: list of arrays (variable lengths)\n    n = len(sequences)\n    groups = np.array([s['game_id'] for s in seq_ids])\n    X = np.array(sequences, dtype=object)  # object dtype array\n    Yx = np.array(targets_dx, dtype=object)\n    Yy = np.array(targets_dy, dtype=object)\n\n    gkf = GroupKFold(n_splits=cfg.N_FOLDS)\n    models_x = []\n    models_y = []\n    scalers = []\n    val_losses = []\n\n    for fold, (tr, va) in enumerate(gkf.split(X, groups=groups), 1):\n        print(f\"\\n---- Fold {fold}/{cfg.N_FOLDS} ----\")\n        X_tr = X[tr]\n        X_va = X[va]\n\n        # fit scaler on X_tr (stack rows)\n        stacked = np.vstack([s for s in X_tr])\n        scaler = RobustScaler().fit(stacked)\n        X_tr_scaled = np.stack([scaler.transform(s) for s in X_tr])\n        X_va_scaled = np.stack([scaler.transform(s) for s in X_va])\n\n        # train model for dx and dy\n        print(\"Training model for DX...\")\n        mx, vx = train_single(X_tr_scaled, Yx[tr], X_va_scaled, Yx[va], X_tr_scaled[0].shape[-1], cfg.MAX_HORIZON, cfg)\n\n        print(\"Training model for DY...\")\n        my, vy = train_single(X_tr_scaled, Yy[tr], X_va_scaled, Yy[va], X_tr_scaled[0].shape[-1], cfg.MAX_HORIZON, cfg)\n\n        models_x.append(mx)\n        models_y.append(my)\n        scalers.append(scaler)\n        val_losses.append((vx + vy) / 2.0)\n\n    # compute weights: better val -> higher weight\n    losses = np.array(val_losses)\n    weights = np.exp(-losses)\n    weights = weights / weights.sum()\n    print(\"Fold weights:\", weights)\n    return models_x, models_y, scalers, weights\n\n# -------------------------\n# PREDICTION & SUBMISSION\n# -------------------------\ndef predict_and_build_submission(models_x, models_y, scalers, weights, test_sequences, test_ids, features, test_template, cfg):\n    # test_sequences: list of [window, feat]\n    # test_ids: list of dicts with game, play, nfl_id, last_frame_id\n    n_models = len(models_x)\n    n_cases = len(test_sequences)\n    H = cfg.MAX_HORIZON\n\n    # prepare last observed x,y for each sequence (to add predicted displacement)\n    last_x = np.array([s[-1, features.index('x')] if 'x' in features else s[-1,0] for s in test_sequences])\n    last_y = np.array([s[-1, features.index('y')] if 'y' in features else s[-1,1] for s in test_sequences])\n\n    all_dx = np.zeros((n_models, n_cases, H), dtype=np.float32)\n    all_dy = np.zeros((n_models, n_cases, H), dtype=np.float32)\n\n    for m_idx, (mx, my, sc) in enumerate(zip(models_x, models_y, scalers)):\n        Xs = np.stack([sc.transform(s) for s in test_sequences])\n        X_tensor = torch.tensor(Xs.astype(np.float32)).to(cfg.DEVICE)\n        mx.eval(); my.eval()\n        with torch.no_grad(), torch.cuda.amp.autocast(enabled=cfg.USE_AMP):\n            dx = mx(X_tensor).cpu().numpy()\n            dy = my(X_tensor).cpu().numpy()\n        # ensure shape [n_cases, H]\n        # if model returns fewer/greater H, clip/pad\n        dx_p = np.zeros((n_cases, H), dtype=np.float32)\n        dy_p = np.zeros((n_cases, H), dtype=np.float32)\n        h = dx.shape[1]\n        copy_h = min(h, H)\n        dx_p[:, :copy_h] = dx[:, :copy_h]\n        dy_p[:, :copy_h] = dy[:, :copy_h]\n        all_dx[m_idx] = dx_p\n        all_dy[m_idx] = dy_p\n\n    # weighted ensemble\n    ens_dx = np.tensordot(weights, all_dx, axes=(0,0))  # shape [n_cases, H]\n    ens_dy = np.tensordot(weights, all_dy, axes=(0,0))\n\n    # build submission rows by iterating test_template frames for each (game, play, nfl_id)\n    rows = []\n    for i, sid in enumerate(test_ids):\n        # get intended frame list from test_template (post-throw frames)\n        fmask = ( (test_template['game_id']==sid['game_id']) &\n                  (test_template['play_id']==sid['play_id']) &\n                  (test_template['nfl_id']==sid['nfl_id']) )\n        fids = test_template.loc[fmask, 'frame_id'].sort_values().tolist()\n        for t, fid in enumerate(fids):\n            tt = min(t, ens_dx.shape[1]-1)\n            px = float(np.clip(last_x[i] + ens_dx[i, tt], 0.0, 120.0))\n            py = float(np.clip(last_y[i] + ens_dy[i, tt], 0.0, 53.3))\n            rows.append({'id': build_id(sid['game_id'], sid['play_id'], sid['nfl_id'], fid), 'x': px, 'y': py})\n\n    sub = pd.DataFrame(rows)\n    return sub\n\n# -------------------------\n# MAIN\n# -------------------------\ndef main():\n    cfg = Config()\n    print(\"=== NFL Big Data Bowl 2026 - Debugged Pipeline ===\")\n    # load train inputs and outputs\n    train_input_files = [cfg.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [cfg.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_inputs = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()], ignore_index=True)\n    train_outputs = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()], ignore_index=True)\n    test_input = pd.read_csv(cfg.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(cfg.DATA_DIR / \"test.csv\")\n\n    # Note: player_to_predict exists in inputs (discussion). We'll build sequences for players present in output set (training)\n    # We still include context/background players via input features (because grouped input may include them),\n    # but for target construction we use output rows (player_to_predict True are scored).\n    print(\"Preparing train sequences...\")\n    seqs, tdx, tdy, tframes, seq_ids, features = prepare_sequences(train_inputs, train_outputs, window_size=cfg.WINDOW_SIZE, is_training=True)\n    print(\"Preparing test sequences...\")\n    test_seqs, test_ids, test_features = prepare_sequences(test_input, test_template=test_template, window_size=cfg.WINDOW_SIZE, is_training=False)\n\n    # Train\n    models_x, models_y, scalers, weights = run_training(seqs, tdx, tdy, seq_ids, features, cfg)\n\n    # Predict & submit\n    submission = predict_and_build_submission(models_x, models_y, scalers, weights, test_seqs, test_ids, features, test_template, cfg)\n\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"Saved submission.csv  Rows:\", len(submission))\n    print(\"Pipeline complete. Notes applied: orientation fix, input/output alignment, player_to_predict handling, lag+EMA features, robust scaling, layernorm+GRU+attention, AMP, weighted ensemble.\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}