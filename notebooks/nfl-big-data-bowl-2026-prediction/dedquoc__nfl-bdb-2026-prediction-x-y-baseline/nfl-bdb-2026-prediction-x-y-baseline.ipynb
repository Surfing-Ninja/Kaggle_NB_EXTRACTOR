{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reference from \nhttps://www.kaggle.com/code/danpietrow/lgbm-baseline-1-00-lb-beginner-friendly-starter/notebook\n\nAnd Improving\n\n## Method  \n- Train two LGBM models: one for `dx`, one for `dy`  \n- Anchor predictions to last observed position (`x_last`, `y_last`)  \n- Clip to field bounds for physical plausibility  ","metadata":{}},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"%%time\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nimport glob\nfrom tqdm.auto import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set data directory\nDATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction/\"\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n\n# Input and output file patterns\ninput_files = sorted(glob.glob(os.path.join(DATA_DIR, \"train/input_2023_w*.csv\")))\noutput_files = sorted(glob.glob(os.path.join(DATA_DIR, \"train/output_2023_w*.csv\")))\n\ninput_files = pd.concat((pd.read_csv(p) for p in tqdm(input_files, desc=\"loading inputs\")), ignore_index=True)\noutput_files = pd.concat((pd.read_csv(p) for p in tqdm(output_files, desc=\"loading outputs\")), ignore_index=True)\n\n#\ntest_in = pd.read_csv(os.path.join(DATA_DIR, \"test_input.csv\"))\ntest_template = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\nsample_submission = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n\nprint(\"Inputs:\", input_files.shape, \"Outputs:\", output_files.shape, \"Test input:\", test_in.shape)\n\nprint(f\"Found {len(input_files)} input files\")\nprint(f\"Found {len(output_files)} output files\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:04.31263Z","iopub.execute_input":"2025-09-27T07:07:04.312956Z","iopub.status.idle":"2025-09-27T07:07:17.782944Z","shell.execute_reply.started":"2025-09-27T07:07:04.312931Z","shell.execute_reply":"2025-09-27T07:07:17.782097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Select Final Features for Modeling\nLetâ€™s build a clean feature set.","metadata":{}},{"cell_type":"code","source":"%%time\nfeature_columns = [\n    # Initial position and motion\n    'start_x', 'start_y', 's', 'a', 'vx', 'vy',\n    \n    # Direction and orientation (radians)\n    'dir_rad', 'o_rad',\n    \n    # Ball landing target (critical!)\n    'ball_land_x', 'ball_land_y',\n    'dx_ball', 'dy_ball', 'dist_to_ball', 'angle_to_ball', 'alignment_to_ball',\n    \n    # Temporal progression in air\n    'frame_id_output', 'frame_norm', 'frame_sq',\n    \n    # Player physical traits\n    'player_weight', 'player_height', 'age',\n    \n    # Role and position flags\n    'is_target', 'is_defender', 'is_WR', 'is_CB', 'is_TE', 'is_RB', 'is_LB', 'is_S', 'is_DL',\n    \n    # Play context\n    'absolute_yardline_number',\n    'play_direction',  # may need encoding\n\n   # CAT_FEATS = ['player_role','player_side','play_direction']\n   # TARGETS = ['dx','dy']\n]   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:17.784315Z","iopub.execute_input":"2025-09-27T07:07:17.784589Z","iopub.status.idle":"2025-09-27T07:07:17.790408Z","shell.execute_reply.started":"2025-09-27T07:07:17.784563Z","shell.execute_reply":"2025-09-27T07:07:17.789459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"%%time\n# Feature Engineering\nimport pandas as pd\nimport numpy as np\n\ndef prepare_last_obs(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Extract the last observed position of each player before the pass.\n    Convert player height from 'feet-inches' format (e.g., '5-11') to inches.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input tracking data with columns: game_id, play_id, nfl_id, frame_id, x, y, etc.\n    \n    Returns:\n    --------\n    pd.DataFrame\n        DataFrame with one row per player per play, containing their last observed state.\n    \"\"\"\n    # Sort once, then group (assumes frame_id increases with time)\n    df_sorted = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    # Get last observation per player per play\n    df_last = (df_sorted\n               .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False)\n               .last())  # This grabs last non-null values across all columns\n\n    # Rename spatial coordinates\n    df_last = df_last.rename(columns={'x': 'x_last', 'y': 'y_last'})\n    \n    # Vectorized height conversion: convert '5-11' â†’ 71\n    def height_str_to_inches(height_series: pd.Series) -> pd.Series:\n        # Split on '-' and handle non-string or missing values\n        split_height = height_series.astype(str).str.split('-', expand=True)\n        feet = pd.to_numeric(split_height[0], errors='coerce')\n        inches = pd.to_numeric(split_height[1], errors='coerce')\n        return feet * 12 + inches\n\n    df_last['player_height'] = height_str_to_inches(df_last['player_height'])\n    \n    return df_last\n\n\ndef add_target_info(df_last: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add targeted receiver's last position to all players in the same play.\n    \n    Parameters:\n    -----------\n    df_last : pd.DataFrame\n        Player-level data with last observed positions.\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Same data, but with target_last_x, target_last_y, and target_nfl_id for each play.\n    \"\"\"\n    # Extract targeted receiver info\n    targets = (df_last[df_last['player_role'] == 'Targeted Receiver']\n               [['game_id', 'play_id', 'nfl_id', 'x_last', 'y_last']]\n               .copy())\n    \n    targets.rename(columns={\n        'nfl_id': 'target_nfl_id',\n        'x_last': 'target_last_x',\n        'y_last': 'target_last_y'\n    }, inplace=True)\n    \n    # Merge target info back to all players in the play\n    df_enhanced = df_last.merge(\n        targets[['game_id', 'play_id', 'target_last_x', 'target_last_y', 'target_nfl_id']],\n        on=['game_id', 'play_id'],\n        how='left'\n    )\n    \n    return df_enhanced\n\n\ndef create_features(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n    \"\"\"\n    Create derived features for modeling.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Merged dataset with last positions and target info.\n    is_train : bool\n        Whether to compute target variables (dx, dy).\n\n    Returns:\n    --------\n    pd.DataFrame\n        Dataset with engineered features.\n    \"\"\"\n    # Avoid modifying the original DataFrame\n    df = df.copy()\n\n    # Time-related features (frame and time since last observation)\n    df['frame_offset'] = df['frame_id']\n    df['time_offset'] = df['frame_offset'] / 10.0  # Assuming 10 Hz sampling rate\n\n    # Distance from player's last position to ball landing point\n    df['dist_to_ball_land'] = np.sqrt(\n        (df['ball_land_x'] - df['x_last'])**2 + \n        (df['ball_land_y'] - df['y_last'])**2\n    )\n\n    # Angle from player to ball landing point (in radians)\n    df['angle_to_ball_land'] = np.arctan2(\n        df['ball_land_y'] - df['y_last'],\n        df['ball_land_x'] - df['x_last']\n    )\n\n    # Distance from player to targeted receiver's last position\n    df['dist_to_target_last'] = np.sqrt(\n        (df['target_last_x'] - df['x_last'])**2 + \n        (df['target_last_y'] - df['y_last'])**2\n    )\n\n    # Binary flag: is this player the targeted receiver?\n    df['is_target'] = (df['nfl_id'] == df['target_nfl_id']).astype(int)\n\n    # Optional: additional useful features (uncomment if needed)\n    # df['speed_to_ball'] = df['dist_to_ball_land'] / (df['time_offset'] + 1e-6)  # avoid div/0\n    # df['relative_dir'] = (df['dir'] - df['angle_to_ball_land']) % (2 * np.pi)\n\n    # Only compute target deltas in training mode\n    if is_train:\n        df['dx'] = df['x'] - df['x_last']  # change in x from last observed\n        df['dy'] = df['y'] - df['y_last']  # change in y from last observed\n\n    return df    \n    \ndef prepare_train(df_in: pd.DataFrame, df_out: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Prepare the training dataset by:\n    - Extracting last observed player state before the pass\n    - Adding targeted receiver position to all players in the play\n    - Merging with future positions (targets)\n    - Engineering motion and spatial features\n\n    Parameters:\n    ----------\n    df_in : pd.DataFrame\n        Tracking data up to the moment of pass (used to get last observation).\n        Must include: game_id, play_id, nfl_id, frame_id, x, y, s, a, o, dir,\n                      player_height, player_weight, player_role, etc.\n    df_out : pd.DataFrame\n        Future positions to predict (output labels).\n        Must include: game_id, play_id, nfl_id, frame_id, x, y\n\n    Returns:\n    -------\n    pd.DataFrame\n        Training-ready DataFrame with features and target deltas (dx, dy).\n    \"\"\"\n    # Validate inputs\n    required_in_cols = {'game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y'}\n    required_out_cols = {'game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y'}\n\n    assert required_in_cols.issubset(df_in.columns), f\"Missing columns in df_in: {required_in_cols - set(df_in.columns)}\"\n    assert required_out_cols.issubset(df_out.columns), f\"Missing columns in df_out: {required_out_cols - set(df_out.columns)}\"\n\n    # Step 1: Get last observed state per player per play\n    last_obs = prepare_last_obs(df_in)\n\n    # Step 2: Add targeted receiver's last position to all players in the same play\n    last_obs = add_target_info(last_obs)\n\n    # Define columns to carry forward (explicit list for clarity and performance)\n    cols_to_keep = [\n        'game_id', 'play_id', 'nfl_id',\n        'x_last', 'y_last', 's', 'a', 'o', 'dir',\n        'player_role', 'player_side', 'num_frames_output',\n        'ball_land_x', 'ball_land_y',\n        'target_last_x', 'target_last_y', 'target_nfl_id',\n        'play_direction', 'absolute_yardline_number',\n        'player_height', 'player_weight'\n    ]\n\n    # Ensure we only select existing columns (defensive programming)\n    available_cols = [c for c in cols_to_keep if c in last_obs.columns]\n    missing_cols = set(cols_to_keep) - set(available_cols)\n    if missing_cols:\n        print(f\"[Warning] Missing columns in last_obs, skipping: {missing_cols}\")\n\n    # Step 3: Merge with future positions (labels)\n    train = df_out.merge(\n        last_obs[available_cols],\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left',\n        suffixes=('', '_drop')\n    )\n\n    # Drop any duplicate columns (in case of suffix issues)\n    train = train.loc[:, ~train.columns.duplicated()]\n\n    # Step 4: Feature engineering\n    train = create_features(train, is_train=True)\n\n    # Optional: Drop rows where last observation is missing (e.g., new players not in df_in)\n    initial_len = len(train)\n    train.dropna(subset=['x_last', 'y_last'], inplace=True)\n    final_len = len(train)\n    if final_len < initial_len:\n        print(f\"[Info] Dropped {initial_len - final_len} rows due to missing last observation.\")\n\n    return train\ndef prepare_test(test_in: pd.DataFrame, test_template: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Prepare the test dataset using the same feature engineering pipeline as training.\n    Does not compute target variables (dx, dy).\n\n    Parameters:\n    ----------\n    test_in : pd.DataFrame\n        Tracking data up to the pass moment (last observed player states).\n        Must include: game_id, play_id, nfl_id, frame_id, x, y, s, a, o, dir, etc.\n    test_template : pd.DataFrame\n        Template of future frames to predict (provided by competition or pipeline).\n        Must include: game_id, play_id, nfl_id, frame_id\n\n    Returns:\n    -------\n    pd.DataFrame\n        Test-ready DataFrame with features, ready for model inference.\n    \"\"\"\n    # Validate inputs\n    required_in_cols = {'game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y'}\n    required_template_cols = {'game_id', 'play_id', 'nfl_id', 'frame_id'}\n\n    assert required_in_cols.issubset(test_in.columns), (\n        f\"Missing required columns in test_in: {required_in_cols - set(test_in.columns)}\"\n    )\n    assert required_template_cols.issubset(test_template.columns), (\n        f\"Missing required columns in test_template: {required_template_cols - set(test_template.columns)}\"\n    )\n\n    # Step 1: Get last observed state per player per play\n    last_test = prepare_last_obs(test_in)\n\n    # Step 2: Add targeted receiver info to all players in the play\n    last_test = add_target_info(last_test)\n\n    # Define feature columns (same as in training for consistency)\n    cols_to_keep = [\n        'game_id', 'play_id', 'nfl_id',\n        'x_last', 'y_last', 's', 'a', 'o', 'dir',\n        'player_role', 'player_side', 'num_frames_output',\n        'ball_land_x', 'ball_land_y',\n        'target_last_x', 'target_last_y', 'target_nfl_id',\n        'play_direction', 'absolute_yardline_number',\n        'player_height', 'player_weight'\n    ]\n\n    # Select only available columns (defensive: in case some metadata is missing)\n    available_cols = [c for c in cols_to_keep if c in last_test.columns]\n    missing_cols = set(cols_to_keep) - set(available_cols)\n    if missing_cols:\n        print(f\"[Warning] Missing in test last_obs, skipping: {missing_cols}\")\n\n    # Step 3: Merge with test template (future frames to predict)\n    test_rows = test_template.merge(\n        last_test[available_cols],\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left',\n        suffixes=('', '_drop')\n    )\n\n    # Drop duplicated columns (in case of merge artifacts)\n    test_rows = test_rows.loc[:, ~test_rows.columns.duplicated()]\n\n    # Step 4: Create features (no dx/dy since no true 'x', 'y' in test output)\n    test_rows = create_features(test_rows, is_train=False)\n\n    # Optional: Warn about missing last observations\n    missing_last = test_rows[['x_last', 'y_last']].isna().any(axis=1).sum()\n    if missing_last > 0:\n        print(f\"[Warning] {missing_last} rows in test have no last observed position. Filled with NaN.\")\n\n    return test_rows   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:17.791362Z","iopub.execute_input":"2025-09-27T07:07:17.791615Z","iopub.status.idle":"2025-09-27T07:07:17.822218Z","shell.execute_reply.started":"2025-09-27T07:07:17.791597Z","shell.execute_reply":"2025-09-27T07:07:17.821257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# trying\n# Prepare datasets\ntrain = prepare_train(input_files, output_files)\ntest = prepare_test(test_in, test_template)\n\n# Select features to use \nFEATURES = [\n    'x_last','y_last','s','a','o','dir',\n    'frame_offset','time_offset',\n    'dist_to_ball_land','angle_to_ball_land',\n    'dist_to_target_last','is_target',\n    'absolute_yardline_number', 'player_height', 'player_weight'\n]\nCAT_FEATS = ['player_role','player_side','play_direction']\nTARGETS = ['dx','dy']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:17.823964Z","iopub.execute_input":"2025-09-27T07:07:17.824237Z","iopub.status.idle":"2025-09-27T07:07:22.591548Z","shell.execute_reply.started":"2025-09-27T07:07:17.824216Z","shell.execute_reply":"2025-09-27T07:07:22.590761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":" ### Peek at Inputs & Outputs","metadata":{}},{"cell_type":"code","source":"%%time\nprint(\"=== INPUT COLUMNS (Sample) ===\")\nprint(input_files.columns.tolist()[:15], \"...\")\n\nprint(\"\\n=== OUTPUT COLUMNS ===\")\nprint(output_files.columns.tolist())\n\nprint(\"\\n=== TEST INPUT SHAPE ===\")\nprint(test_in.shape)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:22.592264Z","iopub.execute_input":"2025-09-27T07:07:22.592451Z","iopub.status.idle":"2025-09-27T07:07:22.597028Z","shell.execute_reply.started":"2025-09-27T07:07:22.592435Z","shell.execute_reply":"2025-09-27T07:07:22.596257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Flag which rows are to be predicted\ntarget_mask = input_files['player_to_predict'] == 1\n\nprint(f\"Total rows: {len(input_files):,}\")\nprint(f\"Target rows (player_to_predict == 1): {target_mask.sum():,}\")\nprint(f\"Target fraction: {target_mask.mean():.1%}\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:22.597933Z","iopub.execute_input":"2025-09-27T07:07:22.598216Z","iopub.status.idle":"2025-09-27T07:07:22.632424Z","shell.execute_reply.started":"2025-09-27T07:07:22.598185Z","shell.execute_reply":"2025-09-27T07:07:22.631531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ“Š Core EDA 1: Target Distribution (if available)","metadata":{}},{"cell_type":"code","source":"%%time\ntargets = input_files[target_mask].copy()\n\nplt.figure(figsize=(10, 4))\nrole_counts = targets['player_role'].value_counts()\nsns.barplot(x=role_counts.index, y=role_counts.values, palette='coolwarm')\nplt.title(\"Player Roles Being Predicted | Core EDA = 1.00\", fontsize=14, weight='bold')\nplt.ylabel(\"Count\")\nplt.xlabel(\"Player Role\")\nplt.xticks(rotation=30)\nfor i, v in enumerate(role_counts.values):\n    plt.text(i, v + 50, str(v), ha='center', va='bottom', fontsize=9)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\nprint(\"Top 5 player roles to predict:\\n\", role_counts.head())   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:22.633299Z","iopub.execute_input":"2025-09-27T07:07:22.633552Z","iopub.status.idle":"2025-09-27T07:07:23.458329Z","shell.execute_reply.started":"2025-09-27T07:07:22.633523Z","shell.execute_reply":"2025-09-27T07:07:23.45746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ“Š Core EDA 2: Input vs Output Position (x, y)","metadata":{}},{"cell_type":"code","source":"%%time\n# Create merge key\ninput_files['key'] = input_files[['game_id', 'play_id', 'nfl_id', 'frame_id']].astype(str).agg('-'.join, axis=1)\noutput_files['key'] = output_files[['game_id', 'play_id', 'nfl_id', 'frame_id']].astype(str).agg('-'.join, axis=1)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:23.459324Z","iopub.execute_input":"2025-09-27T07:07:23.459602Z","iopub.status.idle":"2025-09-27T07:07:52.020567Z","shell.execute_reply.started":"2025-09-27T07:07:23.459576Z","shell.execute_reply":"2025-09-27T07:07:52.019796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now merge input (with x, y) to output (true x, y) on key:","metadata":{}},{"cell_type":"code","source":"%%time\n# Merge input and output on key\nmerged_io = input_files[['key', 'x', 'y', 's', 'a', 'player_position', 'player_role', 'play_direction']].copy()\nmerged_io = merged_io.rename(columns={'x': 'input_x', 'y': 'input_y'})\n\nmerged_io = merged_io.merge(\n    output_files[['key', 'x', 'y']],\n    on='key',\n    how='inner'\n).rename(columns={'x': 'target_x', 'y': 'target_y'})\n\nprint(f\"Merged dataset (input vs target): {len(merged_io):,} frames\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:52.021418Z","iopub.execute_input":"2025-09-27T07:07:52.021699Z","iopub.status.idle":"2025-09-27T07:07:57.846792Z","shell.execute_reply.started":"2025-09-27T07:07:52.021672Z","shell.execute_reply":"2025-09-27T07:07:57.845813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If merge fails â†’ maybe nfl_id missing or type mismatch. Let's ensure dtypes match.","metadata":{}},{"cell_type":"code","source":"%%time\nfor col in ['game_id', 'play_id', 'nfl_id', 'frame_id']:\n    input_files[col] = input_files[col].astype(str)\n    output_files[col] = output_files[col].astype(str)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:07:57.849365Z","iopub.execute_input":"2025-09-27T07:07:57.849587Z","iopub.status.idle":"2025-09-27T07:08:02.689515Z","shell.execute_reply.started":"2025-09-27T07:07:57.849569Z","shell.execute_reply":"2025-09-27T07:08:02.688662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Then recompute key and merge.\n\n### ðŸ“Š Core EDA 3: Position Error Distribution (Input vs Target)\nNow compute the raw displacement error in yards:","metadata":{}},{"cell_type":"code","source":"%%time\nmerged_io['error_x'] = merged_io['target_x'] - merged_io['input_x']\nmerged_io['error_y'] = merged_io['target_y'] - merged_io['input_y']\nmerged_io['error_dist'] = np.sqrt(merged_io['error_x']**2 + merged_io['error_y']**2)\n\nprint(f\"Mean absolute position error: {merged_io['error_dist'].mean():.3f} yards\")\nprint(f\"Median error: {merged_io['error_dist'].median():.3f} yards\")\nprint(f\"95th percentile error: {merged_io['error_dist'].quantile(0.95):.3f} yards\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:02.69037Z","iopub.execute_input":"2025-09-27T07:08:02.690601Z","iopub.status.idle":"2025-09-27T07:08:02.718871Z","shell.execute_reply.started":"2025-09-27T07:08:02.690583Z","shell.execute_reply":"2025-09-27T07:08:02.71801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### ðŸ“ˆ Plot: Error Distribution\nplt.figure(figsize=(10, 4))\nsns.histplot(merged_io['error_dist'], bins=100, kde=True, color='purple')\nplt.title(\"Input vs Target Position Error | Core EDA = 1.00\", fontsize=14, weight='bold')\nplt.xlabel(\"Position Error (yards)\")\nplt.ylabel(\"Density\")\nplt.axvline(merged_io['error_dist'].mean(), color='red', linestyle='--', label=f\"Mean: {merged_io['error_dist'].mean():.3f}\")\nplt.legend()\nplt.grid(alpha=0.2)\nplt.xlim(0, merged_io['error_dist'].quantile(0.99))\nplt.show()  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:02.719852Z","iopub.execute_input":"2025-09-27T07:08:02.720727Z","iopub.status.idle":"2025-09-27T07:08:05.378648Z","shell.execute_reply.started":"2025-09-27T07:08:02.720704Z","shell.execute_reply":"2025-09-27T07:08:05.377895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ“Š Core EDA 4: Frame Density & Temporal Resolution\nCheck how many frames per play:","metadata":{}},{"cell_type":"code","source":"%%time\nframes_per_play = input_files.groupby(['game_id', 'play_id']).size()\nplt.figure(figsize=(10, 4))\nsns.histplot(frames_per_play, bins=50, kde=False, color='teal')\nplt.title(\"Frames per Play | Core EDA = 1.00\", fontsize=14, weight='bold')\nplt.xlabel(\"Number of Frames\")\nplt.ylabel(\"Play Count\")\nplt.axvline(frames_per_play.median(), color='red', linestyle='--', label=f\"Median: {frames_per_play.median()}\")\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n\nprint(f\"Median frames per play: {frames_per_play.median()}\")\nprint(f\"Mean frames per play: {frames_per_play.mean():.1f}\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:05.379493Z","iopub.execute_input":"2025-09-27T07:08:05.379699Z","iopub.status.idle":"2025-09-27T07:08:06.335811Z","shell.execute_reply.started":"2025-09-27T07:08:05.379682Z","shell.execute_reply":"2025-09-27T07:08:06.334935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"%%time\ndef train_directional_models(\n    train: pd.DataFrame,\n    features: list,\n    cat_features: list,\n    target_dx: str = 'dx',\n    target_dy: str = 'dy',\n    lgbm_params: dict = None,\n    valid_size: float = 0.1,\n    use_early_stopping: bool = True,\n    verbose: int = 20\n) -> dict:\n    \"\"\"\n    Train two LightGBM models for dx and dy with proper early stopping support.\n    \"\"\"\n    import lightgbm as lgb\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error\n    import numpy as np\n\n    # Default parameters\n    if lgbm_params is None:\n        lgbm_params = {\n            'objective': 'regression',\n            'boosting_type': 'gbdt',\n            'n_estimators': 1000,\n            'learning_rate': 0.1,\n            'num_leaves': 32,\n            'max_depth': -1,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'reg_alpha': 0.1,\n            'reg_lambda': 0.1,\n            'random_state': 42,\n            'verbosity': -1\n        }\n\n    # Extract features\n    X = train[features].copy()\n    for col in cat_features:\n        if col in X.columns:\n            X[col] = X[col].astype('category')\n        else:\n            raise ValueError(f\"Categorical feature '{col}' not found.\")\n\n    y_dx = train[target_dx].values\n    y_dy = train[target_dy].values\n\n    # Split data for early stopping\n    if use_early_stopping:\n        X_train, X_val, y_train_dx, y_val_dx = train_test_split(\n            X, y_dx, test_size=valid_size, random_state=42\n        )\n        _, _, y_train_dy, y_val_dy = train_test_split(\n            X, y_dy, test_size=valid_size, random_state=42\n        )\n        print(f\"Using train/val split for early stopping (val_size={valid_size})\")\n    else:\n        X_train, y_train_dx, y_train_dy = X, y_dx, y_dy\n        X_val = y_val_dx = y_val_dy = None\n        print(\"Training without validation split or early stopping.\")\n\n    # Train model for dx\n    print(\"â†’ Training model for dx...\")\n    model_dx = LGBMRegressor(**lgbm_params)\n\n    fit_args_dx = {\n        'X': X_train,\n        'y': y_train_dx,\n        'eval_set': [(X_val, y_val_dx)] if use_early_stopping else None,\n        'eval_names': ['val'] if use_early_stopping else None,\n        'categorical_feature': cat_features,\n        #'early_stopping_rounds': 50 if use_early_stopping else None,\n        #'verbose': verbose if use_early_stopping else -1\n    }\n\n    # Only pass early_stopping_rounds if eval_set is provided\n    #f not use_early_stopping:\n    #    fit_args_dx.pop('early_stopping_rounds')\n    #    fit_args_dx.pop('eval_names')\n\n    model_dx.fit(**fit_args_dx)\n\n    # Train model for dy\n    print(\"â†’ Training model for dy...\")\n    model_dy = LGBMRegressor(**lgbm_params)\n\n    fit_args_dy = {\n        'X': X_train,\n        'y': y_train_dy,\n        'eval_set': [(X_val, y_val_dy)] if use_early_stopping else None,\n        'eval_names': ['val'] if use_early_stopping else None,\n        'categorical_feature': cat_features\n    }\n\n    model_dy.fit(**fit_args_dy)   \n    val_rmse = (0.0, 0.0)\n    if use_early_stopping:\n        val_pred_dx = model_dx.predict(X_val)\n        val_pred_dy = model_dy.predict(X_val)\n\n        rmse_dx = np.sqrt(mean_squared_error(y_val_dx, val_pred_dx))\n        rmse_dy = np.sqrt(mean_squared_error(y_val_dy, val_pred_dy))\n        val_rmse = (rmse_dx, rmse_dy)\n\n        print(f\"âœ… Final RMSE | dx: {rmse_dx:.4f}, dy: {rmse_dy:.4f}\") \n        # Feature importance\n        fi = pd.DataFrame({\n        'feature': model_dx.feature_name_,\n        'importance': model_dx.feature_importances_\n    }).sort_values('importance', ascending=False)\n\n    print(\"â†’ Top 10 most important features:\")\n    print(fi.head(10))\n\n    return {\n        'model_dx': model_dx,\n        'model_dy': model_dy,\n        'val_rmse': val_rmse,\n        'feature_importance': fi\n    }   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:06.336855Z","iopub.execute_input":"2025-09-27T07:08:06.337398Z","iopub.status.idle":"2025-09-27T07:08:06.347907Z","shell.execute_reply.started":"2025-09-27T07:08:06.337377Z","shell.execute_reply":"2025-09-27T07:08:06.346919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nmodels = train_directional_models(\n    train=train,\n    features=FEATURES + CAT_FEATS,\n    cat_features=CAT_FEATS,\n    valid_size=0.1\n)\n\nmodel_dx = models['model_dx']\nmodel_dy = models['model_dy']   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:06.348896Z","iopub.execute_input":"2025-09-27T07:08:06.349372Z","iopub.status.idle":"2025-09-27T07:08:46.06429Z","shell.execute_reply.started":"2025-09-27T07:08:06.34935Z","shell.execute_reply":"2025-09-27T07:08:46.063225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediction","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"%%time\n# === PREDICTION: Reconstruct Future (x, y) with Safety Checks ===\n\n# Define features\nPREDICTION_FEATURES = FEATURES + CAT_FEATS\n\n# Safety: Check if all features are present\nmissing_feats = [f for f in PREDICTION_FEATURES if f not in test.columns]\nif missing_feats:\n    raise KeyError(f\"Missing features in test data: {missing_feats}\")\n\n# Create feature matrix\nX_test = test[PREDICTION_FEATURES].copy()\n\n# Handle categorical features safely\nfor col in CAT_FEATS:\n    if col in X_test.columns:\n        # Fill NaN to avoid category conversion error\n        X_test[col] = X_test[col].fillna(\"missing\").astype(\"category\")\n    else:\n        print(f\"âš ï¸ Categorical feature '{col}' not found. Skipping.\")\n\n# Initialize predictions\nprint(f\"Predicting on {len(X_test)} rows...\")\npred_dx = np.zeros(len(X_test), dtype=np.float32)\npred_dy = np.zeros(len(X_test), dtype=np.float32)\n\n# Predict Î”x and Î”y\ntry:\n    pred_dx += model_dx.predict(X_test)\n    pred_dy += model_dy.predict(X_test)\n    print(\"âœ… Prediction complete.\")\nexcept Exception as e:\n    raise RuntimeError(f\"Prediction failed: {e}\")\n\n# Reconstruct absolute coordinates\n# Safety: Ensure x_last and y_last exist\nfor coord in ['x_last', 'y_last']:\n    if coord not in test.columns:\n        raise KeyError(f\"Required column '{coord}' not found in test.\")\n\ntest['pred_x'] = test['x_last'].values + pred_dx\ntest['pred_y'] = test['y_last'].values + pred_dy\n\n# Clip to field boundaries (optional but smart)\ntest['pred_x'] = test['pred_x'].clip(0, 120)  # including end zones\ntest['pred_y'] = test['pred_y'].clip(0, 53.3)  # standard field width\n\nprint(f\"ðŸ“ Final prediction range: x=({test['pred_x'].min():.2f}, {test['pred_x'].max():.2f}), \"\n      f\"y=({test['pred_y'].min():.2f}, {test['pred_y'].max():.2f})\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:46.065249Z","iopub.execute_input":"2025-09-27T07:08:46.065488Z","iopub.status.idle":"2025-09-27T07:08:46.435519Z","shell.execute_reply.started":"2025-09-27T07:08:46.06546Z","shell.execute_reply":"2025-09-27T07:08:46.434594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ndef make_predictions(\n    test: pd.DataFrame,\n    features: list,\n    cat_features: list,\n    model_dx,\n    model_dy,\n    x_last_col: str = 'x_last',\n    y_last_col: str = 'y_last',\n    max_speed_per_sec: float = 10.0  # yards per second (reasonable max sprint)\n) -> pd.DataFrame:\n    \"\"\"\n    Generate future position predictions using trained models.\n    \n    Applies:\n    - Categorical type conversion\n    - Model inference\n    - Movement validation (clip unrealistic deltas)\n    - Safe assignment to copy of input\n    \n    Parameters:\n    -----------\n    test : pd.DataFrame\n        Test data with features and last observed positions.\n    features : list\n        List of feature names used in training.\n    cat_features : list\n        Names of categorical features.\n    model_dx, model_dy : trained models\n        Models predicting displacement in x and y.\n    x_last_col, y_last_col : str\n        Column names for last known x/y.\n    max_speed_per_sec : float\n        Maximum allowed speed to clip implausible predictions.\n\n    Returns:\n    --------\n    pd.DataFrame\n        Copy of `test` with 'pred_x' and 'pred_y' added.\n    \"\"\"\n    # Input validation\n    missing_features = [f for f in features if f not in test.columns]\n    if missing_features:\n        raise ValueError(f\"Missing features in test data: {missing_features}\")\n\n    # Create a clean copy to avoid SettingWithCopyWarning\n    df = test.copy()\n\n    # Prepare feature matrix\n    X_test = df[features].copy()\n\n    # Convert categorical columns\n    for col in cat_features:\n        if col in X_test.columns:\n            X_test[col] = X_test[col].astype('category')\n        else:\n            raise ValueError(f\"Categorical feature '{col}' not found in test set.\")\n\n    # Predict deltas\n    try:\n        pred_dx = model_dx.predict(X_test)\n        pred_dy = model_dy.predict(X_test)\n    except Exception as e:\n        raise RuntimeError(f\"Model prediction failed: {e}\")\n\n    # --- Optional: Clip unrealistic movements ---\n    # Use time_offset (in seconds) to compute max allowed displacement\n    if 'time_offset' in df.columns:\n        time_seconds = df['time_offset'].values  # e.g., frame_id / 10.0\n        time_seconds = np.maximum(time_seconds, 0.1)  # avoid div/0 or instant jumps\n\n        max_speed_per_frame = max_speed_per_sec * time_seconds\n        max_displacement = max_speed_per_frame  # in yards\n\n        total_pred_dist = np.sqrt(pred_dx**2 + pred_dy**2)\n\n        scale = np.where(\n            total_pred_dist > max_displacement,\n            max_displacement / (total_pred_dist + 1e-8),\n            1.0\n        )\n        pred_dx = pred_dx * scale\n        pred_dy = pred_dy * scale\n\n        clipped_count = (scale < 1.0).sum()\n        if clipped_count > 0:\n            print(f\"[!] Clipped {clipped_count} predictions for excessive speed.\")\n\n    # Reconstruct absolute positions\n    pred_x = df[x_last_col].values + pred_dx\n    pred_y = df[y_last_col].values + pred_dy\n\n    # Assign predictions\n    df['pred_x'] = pred_x\n    df['pred_y'] = pred_y\n\n    # Optional: Add raw deltas for debugging\n    # df['pred_dx'] = pred_dx\n    # df['pred_dy'] = pred_dy\n\n    print(f\"âœ… Predictions generated: {len(df):,} rows\")\n    return df\n\n\n# --- Usage ---\ntest_pred = make_predictions(\n    test=test,\n    features=FEATURES + CAT_FEATS,\n    cat_features=CAT_FEATS,\n    model_dx=model_dx,\n    model_dy=model_dy,\n    max_speed_per_sec=12.0  # elite player sprint speed (~27 mph â‰ˆ 12 yd/sec)\n)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:46.43652Z","iopub.execute_input":"2025-09-27T07:08:46.436797Z","iopub.status.idle":"2025-09-27T07:08:46.822556Z","shell.execute_reply.started":"2025-09-27T07:08:46.436771Z","shell.execute_reply":"2025-09-27T07:08:46.821645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create submission","metadata":{}},{"cell_type":"code","source":"%%time\ndef create_submission(\n    df: pd.DataFrame,\n    id_columns: list = ['game_id', 'play_id', 'nfl_id', 'frame_id'],\n    pred_x_col: str = 'pred_x',\n    pred_y_col: str = 'pred_y',\n    output_path: str = \"submission.csv\"\n) -> pd.DataFrame:\n    \"\"\"\n    Create a submission CSV by:\n    - Generating a unique 'id' from key columns\n    - Selecting and renaming predicted coordinates\n    - Saving to CSV\n\n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame containing predictions and ID components.\n    id_columns : list\n        List of column names to join with underscore for 'id'.\n    pred_x_col : str\n        Name of predicted x column.\n    pred_y_col : str\n        Name of predicted y column.\n    output_path : str\n        Path to save the submission CSV.\n\n    Returns:\n    --------\n    pd.DataFrame\n        Submission-ready DataFrame with 'id', 'x', 'y'.\n    \"\"\"\n    required_cols = id_columns + [pred_x_col, pred_y_col]\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        raise ValueError(f\"Missing required columns in DataFrame: {missing_cols}\")\n\n    # Ensure all ID components are string type (avoid 'NaN' or float issues)\n    id_components = df[id_columns].astype(str)\n    \n    # Vectorized: create 'id' using underscore join across columns\n    submission = df.copy()\n    submission['id'] = id_components[id_columns[0]]\n    for col in id_columns[1:]:\n        submission['id'] = submission['id'] + \"_\" + id_components[col]\n    \n    # Select and rename prediction columns\n    submission = submission[['id', pred_x_col, pred_y_col]].rename(\n        columns={pred_x_col: 'x', pred_y_col: 'y'}\n    )\n    \n    # Optional: Validate no missing values in submission\n    if submission.isnull().any().any():\n        null_counts = submission.isnull().sum()\n        print(f\"[Warning] Null values found in submission:\\n{null_counts[null_counts > 0]}\")\n    \n    # Save to CSV\n    submission.to_csv(output_path, index=False)\n    print(f\"âœ… Submission saved to '{output_path}' | Shape: {submission.shape}\")\n    \n    return submission\n\n\n# Usage:\nsubmission = create_submission(\n    df=test,\n    id_columns=['game_id', 'play_id', 'nfl_id', 'frame_id'],\n    pred_x_col='pred_x',\n    pred_y_col='pred_y',\n    output_path=\"submission.csv\"\n)\n\n# Display preview\nsubmission.head()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:08:46.823545Z","iopub.execute_input":"2025-09-27T07:08:46.823869Z","iopub.status.idle":"2025-09-27T07:08:46.881087Z","shell.execute_reply.started":"2025-09-27T07:08:46.823838Z","shell.execute_reply":"2025-09-27T07:08:46.880264Z"}},"outputs":[],"execution_count":null}]}