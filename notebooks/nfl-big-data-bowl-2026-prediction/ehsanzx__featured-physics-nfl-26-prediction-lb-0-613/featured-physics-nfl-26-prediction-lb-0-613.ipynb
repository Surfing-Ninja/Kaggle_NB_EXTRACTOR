{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# ===========================================\n# CONFIGURATION\n# ===========================================\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    OUTPUT_DIR = Path(\"./outputs\")\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n    SEED = 42\n    N_FOLDS = 8\n    BATCH_SIZE = 256\n    EPOCHS = 320\n    PATIENCE = 60\n    LEARNING_RATE = 3e-4\n    WINDOW_SIZE = 12\n    HIDDEN_DIM = 192\n    MAX_FUTURE_HORIZON = 94\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    @staticmethod\n    def set_seed(seed=42):\n        import random\n\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        # Optional determinism for reproducibility\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\nConfig.set_seed(Config.SEED)\n\n\n# ===========================================\n# FEATURE ENGINEERING\n# ===========================================\ndef height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split(\"-\"))\n        return ft + inches / 12\n    except Exception:\n        return 6.0\n\n\ndef add_advanced_features(df):\n    print(\"Adding advanced features...\")\n    df = df.copy()\n    df = df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n    gcols = [\"game_id\", \"play_id\", \"nfl_id\"]\n\n    # Distance Rate Features\n    if \"distance_to_ball\" in df.columns:\n        df[\"distance_to_ball_change\"] = (\n            df.groupby(gcols)[\"distance_to_ball\"].diff().fillna(0)\n        )\n        df[\"distance_to_ball_accel\"] = (\n            df.groupby(gcols)[\"distance_to_ball_change\"].diff().fillna(0)\n        )\n        df[\"time_to_intercept\"] = (\n            df[\"distance_to_ball\"] / (np.abs(df[\"distance_to_ball_change\"]) + 0.1)\n        ).clip(0, 10)\n\n    # Target Alignment\n    if \"ball_direction_x\" in df.columns:\n        dx = df[\"ball_direction_x\"]\n        dy = df[\"ball_direction_y\"]\n        vx = df.get(\"velocity_x\", 0.0)\n        vy = df.get(\"velocity_y\", 0.0)\n        df[\"velocity_alignment\"] = vx * dx + vy * dy\n        df[\"velocity_perpendicular\"] = vx * (-dy) + vy * dx\n        if \"acceleration_x\" in df.columns:\n            ax = df.get(\"acceleration_x\", 0.0)\n            ay = df.get(\"acceleration_y\", 0.0)\n            df[\"accel_alignment\"] = ax * dx + ay * dy\n\n    # Multi-Window Rolling\n    for window in [3, 5, 10]:\n        for col in [\"velocity_x\", \"velocity_y\", \"s\", \"a\"]:\n            if col in df.columns:\n                df[f\"{col}_roll{window}\"] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).mean()\n                )\n                df[f\"{col}_std{window}\"] = (\n                    df.groupby(gcols)[col]\n                    .transform(lambda x: x.rolling(window, min_periods=1).std())\n                    .fillna(0)\n                )\n\n    # Extended Lag (fixed)\n    for lag in [4, 5]:\n        for col in [\"x\", \"y\", \"velocity_x\", \"velocity_y\"]:\n            if col in df.columns:\n                df[f\"{col}_lag{lag}\"] = (\n                    df.groupby(gcols)[col].shift(lag).fillna(0)\n                )\n\n    # Velocity Change\n    if \"velocity_x\" in df.columns:\n        df[\"velocity_x_change\"] = df.groupby(gcols)[\"velocity_x\"].diff().fillna(0)\n        df[\"velocity_y_change\"] = df.groupby(gcols)[\"velocity_y\"].diff().fillna(0)\n    if \"s\" in df.columns:\n        df[\"speed_change\"] = df.groupby(gcols)[\"s\"].diff().fillna(0)\n    if \"dir\" in df.columns:\n        df[\"direction_change\"] = df.groupby(gcols)[\"dir\"].diff().fillna(0)\n        df[\"direction_change\"] = df[\"direction_change\"].apply(\n            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        )\n\n    # Field Position\n    if \"y\" in df.columns:\n        df[\"dist_from_left\"] = df[\"y\"]\n        df[\"dist_from_right\"] = 53.3 - df[\"y\"]\n        df[\"dist_from_sideline\"] = np.minimum(df[\"dist_from_left\"], df[\"dist_from_right\"])\n    if \"x\" in df.columns:\n        df[\"dist_from_endzone\"] = np.minimum(df[\"x\"], 120 - df[\"x\"])\n\n    # Role-Specific\n    if \"is_receiver\" in df.columns and \"velocity_alignment\" in df.columns:\n        df[\"receiver_optimality\"] = df[\"is_receiver\"] * df[\"velocity_alignment\"]\n        df[\"receiver_deviation\"] = df[\"is_receiver\"] * np.abs(\n            df.get(\"velocity_perpendicular\", 0)\n        )\n    if \"is_coverage\" in df.columns and \"closing_speed\" in df.columns:\n        df[\"defender_closing_speed\"] = df[\"is_coverage\"] * df[\"closing_speed\"]\n\n    # Time Features\n    df[\"frames_elapsed\"] = df.groupby(gcols).cumcount()\n    df[\"normalized_time\"] = df.groupby(gcols)[\"frames_elapsed\"].transform(\n        lambda x: x / (x.max() + 1)\n    )\n\n    # Extra: group densities and relative stats\n    if \"is_offense\" in df.columns:\n        df[\"offensive_density\"] = df.groupby([\"game_id\", \"play_id\", \"frame_id\"])[\n            \"is_offense\"\n        ].transform(\"mean\")\n    if \"is_defense\" in df.columns:\n        df[\"defensive_density\"] = df.groupby([\"game_id\", \"play_id\", \"frame_id\"])[\n            \"is_defense\"\n        ].transform(\"mean\")\n\n    for col in [\"s\", \"a\", \"velocity_x\", \"velocity_y\"]:\n        if col in df.columns:\n            df[f\"{col}_group_mean\"] = df.groupby(\n                [\"game_id\", \"play_id\", \"frame_id\"]\n            )[col].transform(\"mean\")\n            df[f\"{col}_group_std\"] = (\n                df.groupby([\"game_id\", \"play_id\", \"frame_id\"])[col]\n                .transform(\"std\")\n                .fillna(0)\n            )\n            df[f\"{col}_rel\"] = df[col] - df[f\"{col}_group_mean\"]\n\n    print(f\"Total features after enhancement: {len(df.columns)}\")\n    return df\n\n\ndef prepare_sequences_with_advanced_features(\n    input_df, output_df=None, test_template=None, is_training=True, window_size=12\n):\n    print(\n        f\"\\n{'='*80}\\nPREPARING SEQUENCES WITH ADVANCED FEATURES\\n{'='*80}\\nWindow size: {window_size}\"\n    )\n    input_df = input_df.copy()\n\n    # Step 1/3: Basic features\n    print(\"Step 1/3: Adding basic features...\")\n    input_df[\"player_height_feet\"] = input_df[\"player_height\"].apply(height_to_feet)\n\n    dir_rad = np.deg2rad(input_df[\"dir\"].fillna(0))\n    delta_t = 0.1\n    input_df[\"velocity_x\"] = (\n        input_df[\"s\"].fillna(0) + 0.5 * input_df[\"a\"].fillna(0) * delta_t\n    ) * np.sin(dir_rad)\n    input_df[\"velocity_y\"] = (\n        input_df[\"s\"].fillna(0) + 0.5 * input_df[\"a\"].fillna(0) * delta_t\n    ) * np.cos(dir_rad)\n    input_df[\"acceleration_x\"] = input_df[\"a\"].fillna(0) * np.sin(dir_rad)\n    input_df[\"acceleration_y\"] = input_df[\"a\"].fillna(0) * np.cos(dir_rad)\n\n    # Roles\n    input_df[\"is_offense\"] = (input_df[\"player_side\"] == \"Offense\").astype(int)\n    input_df[\"is_defense\"] = (input_df[\"player_side\"] == \"Defense\").astype(int)\n    input_df[\"is_receiver\"] = (input_df[\"player_role\"] == \"Targeted Receiver\").astype(int)\n    input_df[\"is_coverage\"] = (input_df[\"player_role\"] == \"Defensive Coverage\").astype(int)\n    input_df[\"is_passer\"] = (input_df[\"player_role\"] == \"Passer\").astype(int)\n\n    # Physics\n    mass_kg = input_df[\"player_weight\"].fillna(200.0) / 2.20462\n    input_df[\"momentum_x\"] = input_df[\"velocity_x\"] * mass_kg\n    input_df[\"momentum_y\"] = input_df[\"velocity_y\"] * mass_kg\n    input_df[\"kinetic_energy\"] = 0.5 * mass_kg * (input_df[\"s\"].fillna(0) ** 2)\n\n    # Ball features\n    if \"ball_land_x\" in input_df.columns and \"ball_land_y\" in input_df.columns:\n        ball_dx = input_df[\"ball_land_x\"] - input_df[\"x\"]\n        ball_dy = input_df[\"ball_land_y\"] - input_df[\"y\"]\n        # Fixed: Euclidean distance (was using *2 instead of **2)\n        input_df[\"distance_to_ball\"] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df[\"angle_to_ball\"] = np.arctan2(ball_dy, ball_dx)\n        input_df[\"ball_direction_x\"] = ball_dx / (input_df[\"distance_to_ball\"] + 1e-6)\n        input_df[\"ball_direction_y\"] = ball_dy / (input_df[\"distance_to_ball\"] + 1e-6)\n        input_df[\"closing_speed\"] = (\n            input_df[\"velocity_x\"] * input_df[\"ball_direction_x\"]\n            + input_df[\"velocity_y\"] * input_df[\"ball_direction_y\"]\n        )\n\n    # Sort temporally\n    input_df = input_df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n    gcols = [\"game_id\", \"play_id\", \"nfl_id\"]\n\n    # Original temporal lags\n    for lag in [1, 2, 3]:\n        input_df[f\"x_lag{lag}\"] = input_df.groupby(gcols)[\"x\"].shift(lag)\n        input_df[f\"y_lag{lag}\"] = input_df.groupby(gcols)[\"y\"].shift(lag)\n        input_df[f\"velocity_x_lag{lag}\"] = input_df.groupby(gcols)[\"velocity_x\"].shift(lag)\n        input_df[f\"velocity_y_lag{lag}\"] = input_df.groupby(gcols)[\"velocity_y\"].shift(lag)\n\n    # EMA features\n    input_df[\"velocity_x_ema\"] = input_df.groupby(gcols)[\"velocity_x\"].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df[\"velocity_y_ema\"] = input_df.groupby(gcols)[\"velocity_y\"].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df[\"speed_ema\"] = input_df.groupby(gcols)[\"s\"].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n\n    # Step 2/3: Advanced features\n    print(\"Step 2/3: Adding advanced features...\")\n    input_df = add_advanced_features(input_df)\n\n    # Step 3/3: Sequence creation\n    print(\"Step 3/3: Creating sequences...\")\n    feature_cols = [\n        # Core\n        \"x\",\n        \"y\",\n        \"s\",\n        \"a\",\n        \"o\",\n        \"dir\",\n        \"frame_id\",\n        \"ball_land_x\",\n        \"ball_land_y\",\n        # Player\n        \"player_height_feet\",\n        \"player_weight\",\n        # Motion\n        \"velocity_x\",\n        \"velocity_y\",\n        \"acceleration_x\",\n        \"acceleration_y\",\n        \"momentum_x\",\n        \"momentum_y\",\n        \"kinetic_energy\",\n        # Roles\n        \"is_offense\",\n        \"is_defense\",\n        \"is_receiver\",\n        \"is_coverage\",\n        \"is_passer\",\n        # Ball\n        \"distance_to_ball\",\n        \"angle_to_ball\",\n        \"ball_direction_x\",\n        \"ball_direction_y\",\n        \"closing_speed\",\n        # Original temporal\n        \"x_lag1\",\n        \"y_lag1\",\n        \"velocity_x_lag1\",\n        \"velocity_y_lag1\",\n        \"x_lag2\",\n        \"y_lag2\",\n        \"velocity_x_lag2\",\n        \"velocity_y_lag2\",\n        \"x_lag3\",\n        \"y_lag3\",\n        \"velocity_x_lag3\",\n        \"velocity_y_lag3\",\n        \"velocity_x_ema\",\n        \"velocity_y_ema\",\n        \"speed_ema\",\n        # Advanced: distance rate\n        \"distance_to_ball_change\",\n        \"distance_to_ball_accel\",\n        \"time_to_intercept\",\n        # Advanced: alignment\n        \"velocity_alignment\",\n        \"velocity_perpendicular\",\n        \"accel_alignment\",\n        # Advanced: rolling\n        \"velocity_x_roll3\",\n        \"velocity_x_std3\",\n        \"velocity_y_roll3\",\n        \"velocity_y_std3\",\n        \"s_roll3\",\n        \"s_std3\",\n        \"a_roll3\",\n        \"a_std3\",\n        \"velocity_x_roll5\",\n        \"velocity_x_std5\",\n        \"velocity_y_roll5\",\n        \"velocity_y_std5\",\n        \"s_roll5\",\n        \"s_std5\",\n        \"a_roll5\",\n        \"a_std5\",\n        \"velocity_x_roll10\",\n        \"velocity_x_std10\",\n        \"velocity_y_roll10\",\n        \"velocity_y_std10\",\n        \"s_roll10\",\n        \"s_std10\",\n        \"a_roll10\",\n        \"a_std10\",\n        # Advanced: extended lags\n        \"x_lag4\",\n        \"y_lag4\",\n        \"velocity_x_lag4\",\n        \"velocity_y_lag4\",\n        \"x_lag5\",\n        \"y_lag5\",\n        \"velocity_x_lag5\",\n        \"velocity_y_lag5\",\n        # Advanced: deltas\n        \"velocity_x_change\",\n        \"velocity_y_change\",\n        \"speed_change\",\n        \"direction_change\",\n        # Field/time/role extras\n        \"dist_from_sideline\",\n        \"dist_from_endzone\",\n        \"receiver_optimality\",\n        \"receiver_deviation\",\n        \"defender_closing_speed\",\n        \"frames_elapsed\",\n        \"normalized_time\",\n        # Group extras\n        \"offensive_density\",\n        \"defensive_density\",\n        \"s_group_mean\",\n        \"s_group_std\",\n        \"s_rel\",\n        \"a_group_mean\",\n        \"a_group_std\",\n        \"a_rel\",\n        \"velocity_x_group_mean\",\n        \"velocity_x_group_std\",\n        \"velocity_x_rel\",\n        \"velocity_y_group_mean\",\n        \"velocity_y_group_std\",\n        \"velocity_y_rel\",\n    ]\n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"Using {len(feature_cols)} features\")\n\n    input_df.set_index([\"game_id\", \"play_id\", \"nfl_id\"], inplace=True)\n    grouped = input_df.groupby(level=[\"game_id\", \"play_id\", \"nfl_id\"])\n\n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[[\"game_id\", \"play_id\", \"nfl_id\"]].drop_duplicates()\n\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = (\n        [],\n        [],\n        [],\n        [],\n        [],\n    )\n\n    for _, row in tqdm(\n        target_groups.iterrows(),\n        total=len(target_groups),\n        desc=\"Creating sequences\",\n    ):\n        key = (row[\"game_id\"], row[\"play_id\"], row[\"nfl_id\"])\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n            input_window = input_window.fillna(group_df.mean(numeric_only=True))\n\n        # Ensure numeric dtype for model input\n        seq = input_window[feature_cols].values.astype(np.float32)\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n\n        sequences.append(seq)\n\n        if is_training:\n            out_grp = output_df[\n                (output_df[\"game_id\"] == row[\"game_id\"])\n                & (output_df[\"play_id\"] == row[\"play_id\"])\n                & (output_df[\"nfl_id\"] == row[\"nfl_id\"])\n            ].sort_values(\"frame_id\")\n\n            last_x = float(input_window.iloc[-1][\"x\"])\n            last_y = float(input_window.iloc[-1][\"y\"])\n            dx = out_grp[\"x\"].values - last_x\n            dy = out_grp[\"y\"].values - last_y\n\n            targets_dx.append(dx.astype(np.float32))\n            targets_dy.append(dy.astype(np.float32))\n            targets_frame_ids.append(out_grp[\"frame_id\"].values.astype(np.int32))\n\n        sequence_ids.append(\n            {\n                \"game_id\": key[0],\n                \"play_id\": key[1],\n                \"nfl_id\": key[2],\n                \"frame_id\": int(input_window.iloc[-1][\"frame_id\"]),\n            }\n        )\n\n    print(f\"Created {len(sequences)} sequences\")\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n    return sequences, sequence_ids\n\n\n# ===========================================\n# MODEL\n# ===========================================\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.04):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n\n    def forward(self, pred, target, mask):\n        err = pred - target\n        abs_err = torch.abs(err)\n        huber = torch.where(\n            abs_err <= self.delta,\n            0.5 * err * err,\n            self.delta * (abs_err - 0.5 * self.delta),\n        )\n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            weight = torch.exp(-self.time_decay * t).view(1, L)\n            huber = huber * weight\n            mask = mask * weight\n        return (huber * mask).sum() / (mask.sum() + 1e-8)\n\n\nclass SeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        # GRU + attention pooling + MLP head\n        self.gru = nn.GRU(\n            input_dim,\n            Config.HIDDEN_DIM,\n            num_layers=2,\n            batch_first=True,\n            dropout=0.18,\n            bidirectional=True,\n        )\n        self.pool_ln = nn.LayerNorm(Config.HIDDEN_DIM * 2)\n        self.pool_attn = nn.MultiheadAttention(\n            Config.HIDDEN_DIM * 2, num_heads=4, batch_first=True\n        )\n        self.pool_query = nn.Parameter(torch.randn(1, 1, Config.HIDDEN_DIM * 2))\n        self.head = nn.Sequential(\n            nn.Linear(Config.HIDDEN_DIM * 2, 128),\n            nn.GELU(),\n            nn.Dropout(0.22),\n            nn.Linear(128, horizon),\n        )\n\n    def forward(self, x):\n        h, _ = self.gru(x)  # [B, T, 2H]\n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)  # [B, 1, 2H]\n        h_ln = self.pool_ln(h)\n        ctx, _ = self.pool_attn(q, h_ln, h_ln)  # [B, 1, 2H]\n        out = self.head(ctx.squeeze(1))  # [B, H]\n        return torch.cumsum(out, dim=1)  # cumulative offsets\n\n\n# ===========================================\n# TRAINING\n# ===========================================\ndef prepare_targets(batch_axis, max_h):\n    tensors, masks = [], []\n    for arr in batch_axis:\n        L = len(arr)\n        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[: L] = 1.0\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors), torch.stack(masks)\n\n\ndef train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config):\n    device = config.DEVICE\n    model = SeqModel(input_dim, horizon).to(device)\n    criterion = TemporalHuber(delta=0.5, time_decay=0.04)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, patience=7, factor=0.5, verbose=False\n    )\n\n    # Batches\n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        by, bm = prepare_targets([y_train[j] for j in range(i, end)], horizon)\n        train_batches.append((bx, by, bm))\n\n    val_batches = []\n    for i in range(0, len(X_val), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_val))\n        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        by, bm = prepare_targets([y_val[j] for j in range(i, end)], horizon)\n        val_batches.append((bx, by, bm))\n\n    best_loss, best_state, bad = float(\"inf\"), None, 0\n\n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for bx, by, bm in train_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            pred = model(bx)\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by, bm in val_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                val_losses.append(criterion(pred, by, bm).item())\n\n        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n        scheduler.step(val_loss)\n\n        if epoch % 10 == 0:\n            print(f\" Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n\n        if bad >= config.PATIENCE:\n            print(f\" Early stop at epoch {epoch}\")\n            break\n\n    if best_state:\n        model.load_state_dict(best_state)\n    return model, best_loss\n\n\n# ===========================================\n# MAIN PIPELINE\n# ===========================================\ndef main():\n    config = Config()\n    print(\"=\" * 80)\n    print(\"STEP 2++: ADVANCED FEATURES PIPELINE (RMSE < 0.50)\")\n    print(\"=\" * 80)\n\n    # Load\n    print(\"\\n[1/4] Loading data...\")\n    train_input_files = [\n        config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)\n    ]\n    train_output_files = [\n        config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)\n    ]\n    train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n    test_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(config.DATA_DIR / \"test.csv\")\n\n    # Prepare with advanced features\n    print(\"\\n[2/4] Preparing with ADVANCED features...\")\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = (\n        prepare_sequences_with_advanced_features(\n            train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n        )\n    )\n    sequences = np.array(sequences, dtype=object)\n    targets_dx = np.array(targets_dx, dtype=object)\n    targets_dy = np.array(targets_dy, dtype=object)\n\n    # Train\n    print(\"\\n[3/4] Training with enhanced features...\")\n    groups = np.array([d[\"game_id\"] for d in sequence_ids])\n    gkf = GroupKFold(n_splits=config.N_FOLDS)\n    models_x, models_y, scalers = [], [], []\n    fold_losses = []\n\n    for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n        print(f\"\\n{'='*60}\\nFold {fold}/{config.N_FOLDS}\\n{'='*60}\")\n        X_tr, X_va = sequences[tr], sequences[va]\n\n        # Fit scaler on training sequences (stack across time)\n        scaler = StandardScaler()\n        scaler.fit(np.vstack([s for s in X_tr]))\n        X_tr_sc = np.stack([scaler.transform(s) for s in X_tr])\n        X_va_sc = np.stack([scaler.transform(s) for s in X_va])\n\n        input_dim = X_tr_sc[0].shape[-1]\n\n        # Train X model\n        print(\"Training X-axis model...\")\n        mx, loss_x = train_model(\n            X_tr_sc,\n            targets_dx[tr],\n            X_va_sc,\n            targets_dx[va],\n            input_dim,\n            config.MAX_FUTURE_HORIZON,\n            config,\n        )\n\n        # Train Y model\n        print(\"Training Y-axis model...\")\n        my, loss_y = train_model(\n            X_tr_sc,\n            targets_dy[tr],\n            X_va_sc,\n            targets_dy[va],\n            input_dim,\n            config.MAX_FUTURE_HORIZON,\n            config,\n        )\n\n        models_x.append(mx)\n        models_y.append(my)\n        scalers.append(scaler)\n        fold_losses.append((loss_x + loss_y) / 2)\n        print(f\"\\nFold {fold} - X loss: {loss_x:.5f}, Y loss: {loss_y:.5f}\")\n\n    # Weighted ensemble (lower CV loss = higher weight)\n    weights = np.exp(-np.array(fold_losses) / (np.min(fold_losses) + 1e-8))\n    weights = weights / weights.sum()\n\n    # Test predictions\n    print(\"\\n[4/4] Creating test predictions...\")\n    test_sequences, test_ids = prepare_sequences_with_advanced_features(\n        test_input, test_template=test_template, is_training=False, window_size=config.WINDOW_SIZE\n    )\n    X_test = np.array(test_sequences, dtype=object)\n\n    # Last known positions from input sequences\n    x_last = np.array([s[-1, 0] for s in X_test])\n    y_last = np.array([s[-1, 1] for s in X_test])\n\n    # Ensemble predictions across folds\n    all_dx, all_dy = [], []\n    for mx, my, sc in zip(models_x, models_y, scalers):\n        X_sc = np.stack([sc.transform(s) for s in X_test]).astype(np.float32)\n        X_t = torch.tensor(X_sc).to(config.DEVICE)\n        mx.eval()\n        my.eval()\n        with torch.no_grad():\n            all_dx.append(mx(X_t).cpu().numpy())\n            all_dy.append(my(X_t).cpu().numpy())\n\n    ens_dx = np.average(np.stack(all_dx), axis=0, weights=weights)\n    ens_dy = np.average(np.stack(all_dy), axis=0, weights=weights)\n\n    # Create submission: match sample_submission id format \"game_play_nfl_index\"\n    rows = []\n    H = ens_dx.shape[1]\n    grouped_template = (\n        test_template[[\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"]]\n        .sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n        .groupby([\"game_id\", \"play_id\", \"nfl_id\"])\n    )\n\n    for i, sid in enumerate(test_ids):\n        key = (sid[\"game_id\"], sid[\"play_id\"], sid[\"nfl_id\"])\n        # Count how many future frames required for this entity (use 1-based sequential index)\n        if key in grouped_template.groups:\n            fids = grouped_template.get_group(key)[\"frame_id\"].tolist()\n        else:\n            fids = []\n\n        for t_idx, _ in enumerate(fids, start=1):\n            tt = min(t_idx - 1, H - 1)\n            px = float(np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX))\n            py = float(np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX))\n            rows.append(\n                {\n                    \"id\": f\"{sid['game_id']}_{sid['play_id']}_{sid['nfl_id']}_{t_idx}\",\n                    \"x\": px,\n                    \"y\": py,\n                }\n            )\n\n    submission = pd.DataFrame(rows, columns=[\"id\", \"x\", \"y\"])\n    submission.to_csv(\"submission.csv\", index=False)\n\n    print(f\"âœ“ Saved submission.csv\")\n    print(f\" Rows: {len(submission)}\")\n    if len(sequences) > 0:\n        print(f\" Features used: {sequences[0].shape[1]}\")\n\n    return submission\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T22:12:28.434082Z","iopub.execute_input":"2025-10-18T22:12:28.434292Z","iopub.status.idle":"2025-10-19T01:28:11.472436Z","shell.execute_reply.started":"2025-10-18T22:12:28.434274Z","shell.execute_reply":"2025-10-19T01:28:11.471475Z"}},"outputs":[],"execution_count":null}]}