{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"STEP 2 — Better Features Pipeline (cuDF-ready)\n- Correct kinematics & angles\n- Unify play direction (and invert at submission time)\n- Fast, modular feature engineering (works with pandas or cuDF pandas-API)\n- Same GRU architecture + GroupKFold CV\n- Safe targets (dx, dy) built in the unified coordinate frame","metadata":{"_uuid":"24f43b8e-0efa-49ea-8dc0-17e1d5861e5e","_cell_guid":"97f685ab-d29d-446f-996b-e1b625520b9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# -------------------------------\n# Global imports + cuDF accelerator\n# -------------------------------\nimport os\nUSE_CUDF = False\ntry:\n    # zero/low-code GPU acceleration for DataFrame ops\n    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n    import pandas as pd\n    import numpy as np\n    import cupy as cp  # optional (not strictly required below)\n    USE_CUDF = True\n    print(\"using cuda_backend pandas for faster parallel data processing\")\nexcept Exception:\n    print(\"cuda df not used\")\n    import pandas as pd\n    import numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# -------------------------------\n# Constants & helpers\n# -------------------------------\nYARDS_TO_METERS = 0.9144\nFPS = 10.0 \nFIELD_LENGTH, FIELD_WIDTH = 120.0, 53.3\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nprint(\"environment set up!\")\ndef wrap_angle_deg(s):\n    # map to (-180, 180]\n    return ((s + 180.0) % 360.0) - 180.0\n\ndef unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Mirror rightward plays so all samples are 'left' oriented (x,y, dir, o, ball_land).\"\"\"\n    if 'play_direction' not in df.columns:\n        return df\n    df = df.copy()\n    right = df['play_direction'].eq('right')\n    # positions\n    if 'x' in df.columns: df.loc[right, 'x'] = FIELD_LENGTH - df.loc[right, 'x']\n    if 'y' in df.columns: df.loc[right, 'y'] = FIELD_WIDTH  - df.loc[right, 'y']\n    # angles in degrees\n    for col in ('dir','o'):\n        if col in df.columns:\n            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n    # ball landing\n    if 'ball_land_x' in df.columns:\n        df.loc[right, 'ball_land_x'] = FIELD_LENGTH - df.loc[right, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[right, 'ball_land_y'] = FIELD_WIDTH  - df.loc[right, 'ball_land_y']\n    return df\n\ndef invert_to_original_direction(x_u, y_u, play_dir_right: bool):\n    \"\"\"Invert unified (left) coordinates back to original play direction.\"\"\"\n    if not play_dir_right:\n        return float(x_u), float(y_u)\n    return float(FIELD_LENGTH - x_u), float(FIELD_WIDTH - y_u)\n\n# -------------------------------\n# Config\n# -------------------------------\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    OUTPUT_DIR = Path(\"./outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n\n    SEED = 42\n    N_FOLDS = 4\n    BATCH_SIZE = 256\n    EPOCHS = 200\n    PATIENCE = 30\n    LEARNING_RATE = 1e-3\n\n    WINDOW_SIZE = 10\n    HIDDEN_DIM = 128\n    MAX_FUTURE_HORIZON = 94  # 不要改动这个！！！\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nset_seed(Config.SEED)","metadata":{"_uuid":"0f17aa9a-6dc5-4a45-8065-5515f9cd9175","_cell_guid":"4f1ee572-ed92-4677-a8a9-9494c909f602","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Sequence builder (unified frame + safe targets)\n# -------------------------------\ndef build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Return a Series indexed by (game_id, play_id) with values 'left'/'right'.\n    This keeps a clean MultiIndex that works for both pandas and cuDF pandas-API.\n    \"\"\"\n    s = (\n        df_in[['game_id','play_id','play_direction']]\n        .drop_duplicates()\n        .set_index(['game_id','play_id'])['play_direction']\n    )\n    return s  # MultiIndex Series\n\n\ndef apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Attach play_direction (if missing) and then unify to 'left'.\n    dir_map must be the MultiIndex Series produced by build_play_direction_map.\n    \"\"\"\n    if 'play_direction' not in df.columns:\n        dir_df = dir_map.reset_index()  # -> columns: game_id, play_id, play_direction\n        df = df.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n    return unify_left_direction(df)\n\ndef prepare_sequences_with_advanced_features(\n        input_df, output_df=None, test_template=None, \n        is_training=True, window_size=10, feature_groups=None):\n\n    print(f\"\\n{'='*80}\")\n    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES (UNIFIED FRAME)\")\n    print(f\"{'='*80}\")\n    print(f\"Window size: {window_size}\")\n\n    if feature_groups is None:\n        feature_groups = [\n            'distance_rate','target_alignment','multi_window_rolling','extended_lags',\n            'velocity_changes','field_position','role_specific','time_features','jerk_features',\n            'player_interaction_distance',\n        ]\n\n    # Direction map and unify\n    dir_map = build_play_direction_map(input_df)\n    input_df_u = unify_left_direction(input_df)\n    \n    if is_training:\n        out_u = apply_direction_to_df(output_df, dir_map)\n        target_rows = out_u\n        target_groups = out_u[['game_id','play_id','nfl_id']].drop_duplicates()\n    else:\n        if 'play_direction' not in test_template.columns:\n            dir_df = dir_map.reset_index()\n            test_template = test_template.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n        target_rows = test_template\n        target_groups = target_rows[['game_id','play_id','nfl_id','play_direction']].drop_duplicates()\n        \n    assert target_rows[['game_id','play_id','play_direction']].isna().sum().sum() == 0, \\\n        \"play_direction merge failed; check (game_id, play_id) coverage\"\n    print(\"play_direction merge OK:\", target_rows['play_direction'].value_counts(dropna=False).to_dict())\n\n    # --- FE ---\n    fe = FeatureEngineer(feature_groups)\n    processed_df, feature_cols = fe.transform(input_df_u)\n\n    # --- Build sequences (OPTIMIZED) ---\n    print(\"\\nStep 3/3: Creating sequences...\")\n    \n    # 优化1: 预先排序并分组（避免重复set_index）\n    processed_df = processed_df.sort_values(['game_id','play_id','nfl_id','frame_id'])\n    \n    # 优化2: 使用字典缓存分组数据（避免重复get_group）\n    grouped_dict = {\n        key: group[feature_cols].values \n        for key, group in processed_df.groupby(['game_id','play_id','nfl_id'])\n    }\n    \n    # 优化3: 预计算分组统计（用于fillna）\n    group_means = processed_df.groupby(['game_id','play_id','nfl_id'])[feature_cols].mean()\n    \n    # 预先获取x,y索引\n    idx_x = feature_cols.index('x')\n    idx_y = feature_cols.index('y')\n    idx_fid = processed_df.columns.get_loc('frame_id')\n    \n    # 预分配列表（减少动态扩容）\n    n_targets = len(target_groups)\n    sequences = []\n    targets_dx = [] if is_training else None\n    targets_dy = [] if is_training else None\n    targets_fids = [] if is_training else None\n    seq_meta = []\n    \n    # 优化4: 转为numpy数组加速迭代\n    target_array = target_groups.values\n    \n    # 优化5: 预处理training目标数据（避免重复筛选）\n    if is_training:\n        # 创建多级索引快速查找\n        target_rows = target_rows.sort_values(['game_id','play_id','nfl_id','frame_id'])\n        target_lookup = {\n            (gid, pid, nid): group[['x','y','frame_id']].values\n            for (gid, pid, nid), group in target_rows.groupby(['game_id','play_id','nfl_id'])\n        }\n    \n    # 优化6: 批量处理（减少tqdm开销）\n    for i in tqdm(range(n_targets), desc=\"Creating sequences\"):\n        if is_training:\n            gid, pid, nid = target_array[i, :3]\n            play_dir = None\n        else:\n            gid, pid, nid, play_dir = target_array[i, :4]\n        \n        key = (gid, pid, nid)\n        \n        # 快速查找\n        if key not in grouped_dict:\n            continue\n        \n        group_data = grouped_dict[key]\n        \n        # 获取窗口\n        if len(group_data) >= window_size:\n            input_window = group_data[-window_size:]\n        else:\n            if is_training:\n                continue\n            # 快速padding（使用numpy）\n            pad_len = window_size - len(group_data)\n            pad_array = np.full((pad_len, len(feature_cols)), np.nan, dtype=np.float32)\n            input_window = np.vstack([pad_array, group_data])\n        \n        # 优化7: 向量化fillna（使用预计算的均值）\n        if key in group_means.index:\n            mean_vals = group_means.loc[key].values\n            nan_mask = np.isnan(input_window)\n            input_window = np.where(nan_mask, mean_vals, input_window)\n        \n        # 最终NaN处理\n        if np.isnan(input_window).any():\n            if is_training:\n                continue\n            input_window = np.nan_to_num(input_window, nan=0.0)\n        \n        sequences.append(input_window.astype(np.float32))\n        \n        # Training targets\n        if is_training:\n            if key not in target_lookup:\n                continue\n            \n            target_data = target_lookup[key]  # [n_frames, 3] -> [x, y, frame_id]\n            \n            last_x = input_window[-1, idx_x]\n            last_y = input_window[-1, idx_y]\n            \n            dx = (target_data[:, 0] - last_x).astype(np.float32)\n            dy = (target_data[:, 1] - last_y).astype(np.float32)\n            fids = target_data[:, 2].astype(np.int32)\n            \n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_fids.append(fids)\n        \n        # Metadata（优化：减少字典创建开销）\n        seq_meta.append({\n            'game_id': int(gid),\n            'play_id': int(pid),\n            'nfl_id': int(nid),\n            'frame_id': int(processed_df[\n                (processed_df['game_id']==gid) & \n                (processed_df['play_id']==pid) & \n                (processed_df['nfl_id']==nid)\n            ]['frame_id'].iloc[-1]),\n            'play_direction': play_dir,\n        })\n    \n    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_fids, seq_meta, feature_cols, dir_map\n    return sequences, seq_meta, feature_cols, dir_map","metadata":{"_uuid":"7ce6ba09-32db-402c-accc-4dd05ccba59a","_cell_guid":"4b958933-d29e-4865-9595-be8100ede912","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Feature Engineering\n# -------------------------------\nclass FeatureEngineer:\n    \"\"\"\n    Modular, ablation-friendly feature builder (pandas or cuDF pandas-API).\n    \"\"\"\n    def __init__(self, feature_groups_to_create):\n        self.gcols = ['game_id', 'play_id', 'nfl_id']\n        self.active_groups = feature_groups_to_create\n        self.feature_creators = {\n            'distance_rate': self._create_distance_rate_features,\n            'target_alignment': self._create_target_alignment_features,\n            'multi_window_rolling': self._create_multi_window_rolling_features,\n            'extended_lags': self._create_extended_lag_features,\n            'velocity_changes': self._create_velocity_change_features,\n            'field_position': self._create_field_position_features,\n            'role_specific': self._create_role_specific_features,\n            'time_features': self._create_time_features,\n            'jerk_features': self._create_jerk_features,\n            'curvature_land_features': self._create_curvature_land_features,\n            'player_interaction_distance': self._create_player_interaction_distance_features,\n        }\n        self.created_feature_cols = []\n\n    def _height_to_feet(self, height_str):\n        try:\n            ft, inches = map(int, str(height_str).split('-'))\n            return ft + inches / 12\n        except Exception:\n            return 6.0\n\n    def _create_basic_features(self, df):\n        print(\"Step 1/3: Adding basic features...\")\n        df = df.copy()\n        df['player_height_feet'] = df['player_height'].apply(self._height_to_feet)\n\n        # Correct kinematics: dir is from +x CCW\n        dir_rad = np.deg2rad(df['dir'].fillna(0.0).astype('float32'))\n        df['velocity_x']     = df['s'] * np.cos(dir_rad)\n        df['velocity_y']     = df['s'] * np.sin(dir_rad)\n        df['acceleration_x'] = df['a'] * np.cos(dir_rad)\n        df['acceleration_y'] = df['a'] * np.sin(dir_rad)\n\n        # Roles\n        df['is_offense']  = (df['player_side'] == 'Offense').astype(np.int8)\n        df['is_defense']  = (df['player_side'] == 'Defense').astype(np.int8)\n        df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(np.int8)\n        df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(np.int8)\n        df['is_passer']   = (df['player_role'] == 'Passer').astype(np.int8)\n\n        # Energetics (consistent units)\n        mass_kg = df['player_weight'].fillna(200.0) / 2.20462\n        v_ms = df['s'] * YARDS_TO_METERS\n        df['momentum_x'] = mass_kg * df['velocity_x'] * YARDS_TO_METERS\n        df['momentum_y'] = mass_kg * df['velocity_y'] * YARDS_TO_METERS\n        df['kinetic_energy'] = 0.5 * mass_kg * (v_ms ** 2)\n\n        # Ball landing geometry (static)\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            ball_dx = df['ball_land_x'] - df['x']\n            ball_dy = df['ball_land_y'] - df['y']\n            dist = np.hypot(ball_dx, ball_dy)\n            df['distance_to_ball'] = dist\n            inv = 1.0 / (dist + 1e-6)\n            df['ball_direction_x'] = ball_dx * inv\n            df['ball_direction_y'] = ball_dy * inv\n            df['closing_speed'] = (\n                df['velocity_x'] * df['ball_direction_x'] +\n                df['velocity_y'] * df['ball_direction_y']\n            )\n\n        base = [\n            'x','y','s','a','o','dir','frame_id','ball_land_x','ball_land_y',\n            'player_height_feet','player_weight',\n            'velocity_x','velocity_y','acceleration_x','acceleration_y',\n            'momentum_x','momentum_y','kinetic_energy',\n            'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n            'distance_to_ball','ball_direction_x','ball_direction_y','closing_speed'\n        ]\n        self.created_feature_cols.extend([c for c in base if c in df.columns])\n        return df\n\n    # ---- feature groups ----\n    def _create_distance_rate_features(self, df):\n        new_cols = []\n        if 'distance_to_ball' in df.columns:\n            d = df.groupby(self.gcols)['distance_to_ball'].diff()\n            df['d2ball_dt']  = d.fillna(0.0) * FPS\n            df['d2ball_ddt'] = df.groupby(self.gcols)['d2ball_dt'].diff().fillna(0.0) * FPS\n            df['time_to_intercept'] = (df['distance_to_ball'] /\n                                       (df['d2ball_dt'].abs() + 1e-3)).clip(0, 10)\n            new_cols = ['d2ball_dt','d2ball_ddt','time_to_intercept']\n        return df, new_cols\n\n    def _create_target_alignment_features(self, df):\n        new_cols = []\n        if {'ball_direction_x','ball_direction_y','velocity_x','velocity_y'}.issubset(df.columns):\n            df['velocity_alignment'] = df['velocity_x']*df['ball_direction_x'] + df['velocity_y']*df['ball_direction_y']\n            df['velocity_perpendicular'] = df['velocity_x']*(-df['ball_direction_y']) + df['velocity_y']*df['ball_direction_x']\n            new_cols.extend(['velocity_alignment','velocity_perpendicular'])\n            if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n                df['accel_alignment'] = df['acceleration_x']*df['ball_direction_x'] + df['acceleration_y']*df['ball_direction_y']\n                new_cols.append('accel_alignment')\n        return df, new_cols\n\n    def _create_multi_window_rolling_features(self, df):\n        # keep it simple & compatible (works with cuDF pandas-API); vectorized rolling per group\n        new_cols = []\n        for window in (3, 5, 10):\n            for col in ('velocity_x','velocity_y','s','a'):\n                if col in df.columns:\n                    r_mean = df.groupby(self.gcols)[col].rolling(window, min_periods=1).mean()\n                    r_std  = df.groupby(self.gcols)[col].rolling(window, min_periods=1).std()\n                    # align indices\n                    r_mean = r_mean.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    r_std  = r_std.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    df[f'{col}_roll{window}'] = r_mean\n                    df[f'{col}_std{window}']  = r_std.fillna(0.0)\n                    new_cols.extend([f'{col}_roll{window}', f'{col}_std{window}'])\n        return df, new_cols\n\n    def _create_extended_lag_features(self, df):\n        new_cols = []\n        for lag in (1,2,3,4,5):\n            for col in ('x','y','velocity_x','velocity_y'):\n                if col in df.columns:\n                    g = df.groupby(self.gcols)[col]\n                    lagv = g.shift(lag)\n                    # safe fill for first frames (no \"future\" leakage)\n                    df[f'{col}_lag{lag}'] = lagv.fillna(g.transform('first'))\n                    new_cols.append(f'{col}_lag{lag}')\n        return df, new_cols\n\n    def _create_velocity_change_features(self, df):\n        new_cols = []\n        if 'velocity_x' in df.columns:\n            df['velocity_x_change'] = df.groupby(self.gcols)['velocity_x'].diff().fillna(0.0)\n            df['velocity_y_change'] = df.groupby(self.gcols)['velocity_y'].diff().fillna(0.0)\n            df['speed_change']      = df.groupby(self.gcols)['s'].diff().fillna(0.0)\n            d = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n            df['direction_change']  = wrap_angle_deg(d)\n            new_cols = ['velocity_x_change','velocity_y_change','speed_change','direction_change']\n        return df, new_cols\n\n    def _create_field_position_features(self, df):\n        df['dist_from_left'] = df['y']\n        df['dist_from_right'] = FIELD_WIDTH - df['y']\n        df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n        df['dist_from_endzone']  = np.minimum(df['x'], FIELD_LENGTH - df['x'])\n        return df, ['dist_from_sideline','dist_from_endzone']\n\n    def _create_role_specific_features(self, df):\n        new_cols = []\n        if {'is_receiver','velocity_alignment'}.issubset(df.columns):\n            df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n            df['receiver_deviation']  = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0.0))\n            new_cols.extend(['receiver_optimality','receiver_deviation'])\n        if {'is_coverage','closing_speed'}.issubset(df.columns):\n            df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n            new_cols.append('defender_closing_speed')\n        return df, new_cols\n\n    def _create_time_features(self, df):\n        df['frames_elapsed']  = df.groupby(self.gcols).cumcount()\n        df['normalized_time'] = df.groupby(self.gcols)['frames_elapsed'].transform(\n            lambda x: x / (x.max() + 1e-9)\n        )\n        return df, ['frames_elapsed','normalized_time']\n\n    def _create_jerk_features(self, df):\n        new_cols = []\n        if 'a' in df.columns:\n            df['jerk'] = df.groupby(self.gcols)['a'].diff().fillna(0.0) * FPS\n            new_cols.append('jerk')\n        if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n            df['jerk_x'] = df.groupby(self.gcols)['acceleration_x'].diff().fillna(0.0) * FPS\n            df['jerk_y'] = df.groupby(self.gcols)['acceleration_y'].diff().fillna(0.0) * FPS\n            new_cols.extend(['jerk_x','jerk_y'])\n        return df, new_cols\n        \n    def _create_curvature_land_features(self, df):\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            dx = df['ball_land_x'] - df['x']\n            dy = df['ball_land_y'] - df['y']\n            bearing = np.arctan2(dy, dx)\n            a_dir = np.deg2rad(df['dir'].fillna(0.0).values)\n            # 有符号方位差\n            df['bearing_to_land_signed'] = np.rad2deg(np.arctan2(np.sin(bearing - a_dir), np.cos(bearing - a_dir)))\n            # 侧向偏差：d × u (2D cross, z 分量)\n            ux, uy = np.cos(a_dir), np.sin(a_dir)\n            df['land_lateral_offset'] = dy*ux - dx*uy  # >0 落点在左侧\n    \n        # 曲率（按序列）\n        ddir = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n        ddir = ((ddir + 180.0) % 360.0) - 180.0\n        curvature = np.deg2rad(ddir).astype('float32') / (df['s'].replace(0, np.nan).astype('float32') * 0.1 + 1e-6)\n        df['curvature_signed'] = curvature.fillna(0.0)\n        df['curvature_abs'] = df['curvature_signed'].abs()\n    \n        # 窗口均值（3/5）\n        for w in (3,5):\n            r = df.groupby(self.gcols)['curvature_signed'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_signed_roll{w}'] = r\n            r2 = df.groupby(self.gcols)['curvature_abs'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_abs_roll{w}'] = r2\n    \n        new_cols = ['bearing_to_land_signed','land_lateral_offset',\n                    'curvature_signed','curvature_abs','curv_signed_roll3','curv_abs_roll3',\n                    'curv_signed_roll5','curv_abs_roll5']\n        return df, [c for c in new_cols if c in df.columns]\n        \n    def _create_player_interaction_distance_features(self, df):\n        new_cols = []\n        \n        # 确保必要列存在\n        if not {'x', 'y', 'velocity_x', 'velocity_y', 'is_offense'}.issubset(df.columns):\n            return df, new_cols\n        \n        # 按play分组计算\n        grouped_features = []\n        for (gid, pid, fid), frame_df in df.groupby(['game_id', 'play_id', 'frame_id']):\n            frame_df = frame_df.copy()\n            \n            # 提取坐标和速度（float32降低内存）\n            positions = frame_df[['x', 'y']].values.astype(np.float32)\n            velocities = frame_df[['velocity_x', 'velocity_y']].values.astype(np.float32)\n            is_offense = frame_df['is_offense'].values\n            \n            n_players = len(frame_df)\n            \n            # 预计算距离矩阵（使用scipy优化）\n            from scipy.spatial.distance import cdist\n            dist_matrix = cdist(positions, positions, metric='euclidean').astype(np.float32)\n            np.fill_diagonal(dist_matrix, np.inf)\n            \n            # 预计算队友/对手掩码\n            is_opponent_matrix = is_offense[:, np.newaxis] != is_offense[np.newaxis, :]\n            is_teammate_matrix = (is_offense[:, np.newaxis] == is_offense[np.newaxis, :]) & (np.arange(n_players)[:, None] != np.arange(n_players))\n            \n            # 向量化计算最小距离\n            opponent_dists = np.where(is_opponent_matrix, dist_matrix, np.inf)\n            teammate_dists = np.where(is_teammate_matrix, dist_matrix, np.inf)\n            \n            min_opponent_dist = np.minimum(np.min(opponent_dists, axis=1), 999.0)\n            min_teammate_dist = np.minimum(np.min(teammate_dists, axis=1), 999.0)\n            \n            # 向量化密度计算（5码内）\n            opponent_density = np.sum((opponent_dists < 5.0), axis=1).astype(np.int8)\n            teammate_density = np.sum((teammate_dists < 5.0), axis=1).astype(np.int8)\n            \n            # 简化相对速度计算（使用近似）\n            nearest_opponent_idx = np.argmin(opponent_dists, axis=1)\n            valid_mask = min_opponent_dist < 20.0\n            \n            # 向量化速度计算\n            vel_diff = velocities - velocities[nearest_opponent_idx]  # (n, 2)\n            pos_diff = positions[nearest_opponent_idx] - positions  # (n, 2)\n            \n            # 点积投影（向量化）\n            relative_velocity = np.sum(vel_diff * pos_diff, axis=1) / (min_opponent_dist + 1e-6)\n            relative_velocity = np.where(valid_mask, relative_velocity, 0.0).astype(np.float32)\n            \n            # 批量赋值\n            frame_df['min_opponent_distance'] = min_opponent_dist\n            frame_df['min_teammate_distance'] = min_teammate_dist\n            frame_df['relative_velocity_to_nearest_opponent'] = relative_velocity\n            frame_df['opponent_density_5yd'] = opponent_density\n            frame_df['teammate_density_5yd'] = teammate_density\n            \n            grouped_features.append(frame_df)\n        \n        # 合并结果\n        result_df = pd.concat(grouped_features, ignore_index=True)\n        \n        new_cols = [\n            'min_opponent_distance',\n            'min_teammate_distance', \n            'relative_velocity_to_nearest_opponent',\n            'opponent_density_5yd',\n            'teammate_density_5yd'\n        ]\n        \n        return result_df, new_cols\n\n    def transform(self, df):\n        df = df.copy().sort_values(['game_id','play_id','nfl_id','frame_id'])\n        df = self._create_basic_features(df)\n\n        print(\"\\nStep 2/3: Adding selected advanced features...\")\n        for group_name in self.active_groups:\n            if group_name in self.feature_creators:\n                creator = self.feature_creators[group_name]\n                df, new_cols = creator(df)\n                self.created_feature_cols.extend(new_cols)\n                print(f\"  [+] Added '{group_name}' ({len(new_cols)} cols)\")\n            else:\n                print(f\"  [!] Unknown feature group: {group_name}\")\n\n        final_cols = sorted(set(self.created_feature_cols))\n        print(f\"\\nTotal features created: {len(final_cols)}\")\n        return df, final_cols","metadata":{"_uuid":"e5c16db3-f9a3-4576-937c-a051125c999e","_cell_guid":"b6992e5d-b44e-4bd7-982e-69dff1718229","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03, velocity_penalty_weight=0.01,\n                 acceleration_penalty_weight=0.0, use_huber_for_penalty=False):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n        self.velocity_penalty_weight = velocity_penalty_weight\n        self.acceleration_penalty_weight = acceleration_penalty_weight\n        self.use_huber_for_penalty = use_huber_for_penalty\n        \n        # 预计算标志位（避免每次forward都判断）\n        self.has_velocity_penalty = velocity_penalty_weight > 0\n        self.has_acceleration_penalty = acceleration_penalty_weight > 0\n        self.has_time_decay = time_decay > 0\n        \n        # 缓存时间衰减权重（对于固定长度序列）\n        self._cached_time_weights = {}\n    \n    def _get_time_weights(self, length, device):\n        \"\"\"缓存时间衰减权重以避免重复计算\"\"\"\n        key = (length, device)\n        if key not in self._cached_time_weights:\n            t = torch.arange(length, device=device, dtype=torch.float32)\n            self._cached_time_weights[key] = torch.exp(-self.time_decay * t).view(1, -1)\n        return self._cached_time_weights[key]\n    \n    def _huber_loss(self, err):\n        \"\"\"统一的Huber损失计算\"\"\"\n        abs_err = torch.abs(err)\n        return torch.where(abs_err <= self.delta,\n                          0.5 * err.square(),  # 使用.square()代替 * 运算\n                          self.delta * (abs_err - 0.5 * self.delta))\n    \n    def forward(self, pred, target, mask):\n        err = pred - target\n        \n        # ===== 主Huber损失 =====\n        huber = self._huber_loss(err)\n        \n        # 时间衰减权重（一次性计算，复用于所有项）\n        if self.has_time_decay:\n            time_weights = self._get_time_weights(pred.size(1), pred.device)\n            mask_weighted = mask * time_weights\n            huber = huber * time_weights\n        else:\n            mask_weighted = mask\n            time_weights = None\n        \n        main_loss = (huber * mask_weighted).sum() / (mask_weighted.sum() + 1e-8)\n        \n        # 早期退出：如果没有正则项，直接返回\n        if not (self.has_velocity_penalty or self.has_acceleration_penalty):\n            return main_loss\n        \n        total_loss = main_loss\n        \n        # ===== 速度平滑正则项 =====\n        if self.has_velocity_penalty and pred.size(1) > 1:\n            # 一阶差分（速度变化）\n            velocity_diff = pred[:, 1:] - pred[:, :-1]\n            mask_vel = mask[:, 1:]\n            \n            # 选择损失函数（减少分支）\n            if self.use_huber_for_penalty:\n                vel_loss = self._huber_loss(velocity_diff)\n            else:\n                vel_loss = velocity_diff.square()\n            \n            # 应用时间衰减（复用已计算的权重）\n            if self.has_time_decay:\n                time_weights_vel = time_weights[:, 1:]  # 切片比重新计算快\n                vel_loss = vel_loss * time_weights_vel\n                mask_vel = mask_vel * time_weights_vel\n            \n            velocity_penalty = (vel_loss * mask_vel).sum() / (mask_vel.sum() + 1e-8)\n            total_loss = total_loss + self.velocity_penalty_weight * velocity_penalty\n        \n        # ===== 加速度平滑正则项 =====\n        if self.has_acceleration_penalty and pred.size(1) > 2:\n            # 二阶差分（加速度变化）- 复用velocity_diff避免重复计算\n            if not self.has_velocity_penalty:\n                velocity_diff = pred[:, 1:] - pred[:, :-1]\n            \n            acceleration = velocity_diff[:, 1:] - velocity_diff[:, :-1]\n            mask_acc = mask[:, 2:]\n            \n            # 选择损失函数\n            if self.use_huber_for_penalty:\n                acc_loss = self._huber_loss(acceleration)\n            else:\n                acc_loss = acceleration.square()\n            \n            # 应用时间衰减\n            if self.has_time_decay:\n                time_weights_acc = time_weights[:, 2:]\n                acc_loss = acc_loss * time_weights_acc\n                mask_acc = mask_acc * time_weights_acc\n            \n            acceleration_penalty = (acc_loss * mask_acc).sum() / (mask_acc.sum() + 1e-8)\n            total_loss = total_loss + self.acceleration_penalty_weight * acceleration_penalty\n        \n        return total_loss\n\nclass SeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        # 投影到可被num_heads整除的维度\n        self.hidden_dim = 128\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, self.hidden_dim)\n        \n        # 位置编码（假设序列长度最大为10）- 注册为buffer避免梯度计算\n        pos_encoding = torch.randn(1, 10, self.hidden_dim) * 0.02\n        self.register_buffer('pos_encoding', pos_encoding)\n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.hidden_dim,\n            nhead=4,\n            dim_feedforward=256,\n            dropout=0.1,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True  # Pre-LN更稳定\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        # 简化Pooling层 - 合并LayerNorm到注意力后\n        self.pool_attn = nn.MultiheadAttention(\n            self.hidden_dim, \n            num_heads=4, \n            batch_first=True,\n            dropout=0.1\n        )\n        self.pool_query = nn.Parameter(torch.randn(1, 1, self.hidden_dim))\n        self.pool_ln = nn.LayerNorm(self.hidden_dim)  # 移到注意力后\n        \n        # 输出头 - 合并部分操作\n        self.head_linear1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.head_prelu = nn.PReLU(num_parameters=self.hidden_dim)\n        self.head_dropout = nn.Dropout(0.2)\n        self.head_linear2 = nn.Linear(self.hidden_dim, horizon)\n        \n        # 预分配pool_query以避免每次forward都expand\n        self._pool_query_cache = {}\n    \n    def _get_pool_query(self, batch_size):\n        \"\"\"缓存不同batch_size的pool_query\"\"\"\n        if batch_size not in self._pool_query_cache:\n            self._pool_query_cache[batch_size] = self.pool_query.expand(batch_size, -1, -1)\n        return self._pool_query_cache[batch_size]\n    \n    def forward(self, x):\n        # x: (B, seq_len, input_dim)\n        B, seq_len, _ = x.shape\n        \n        # 投影输入 + 位置编码（合并操作）\n        x = self.input_proj(x)\n        x = x + self.pos_encoding[:, :seq_len, :]\n        \n        # Transformer编码\n        h = self.transformer(x)  # (B, seq_len, hidden_dim)\n        \n        # 注意力池化（优化版本）\n        q = self._get_pool_query(B)  # 复用缓存\n        ctx, _ = self.pool_attn(q, h, h)  # 直接对h操作，不预先LayerNorm\n        ctx = self.pool_ln(ctx.squeeze(1))  # LayerNorm放到注意力后，同时squeeze\n        \n        # 预测（展开Sequential避免列表迭代）\n        out = self.head_linear1(ctx)\n        out = self.head_prelu(out)\n        out = self.head_dropout(out)\n        out = self.head_linear2(out)  # (B, horizon)\n        \n        # 累积和（使用inplace操作不可行，保持原样）\n        return torch.cumsum(out, dim=1)  # (B, horizon)\n\ndef prepare_targets(batch_axis, max_h):\n    tensors, masks = [], []\n    for arr in batch_axis:\n        L = len(arr)\n        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors), torch.stack(masks)\n\ndef train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config):\n    device = config.DEVICE\n    model = SeqModel(input_dim, horizon).to(device)\n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=False)\n    \n    # 初始化混合精度训练\n    scaler = GradScaler()\n\n    # build batches (keep numpy → torch)\n    def build_batches(X, Y):\n        batches = []\n        B = config.BATCH_SIZE\n        for i in range(0, len(X), B):\n            end = min(i + B, len(X))\n            xs = torch.tensor(np.stack(X[i:end]).astype(np.float32))\n            ys, ms = prepare_targets([Y[j] for j in range(i, end)], horizon)\n            batches.append((xs, ys, ms))\n        return batches\n\n    tr_batches = build_batches(X_train, y_train)\n    va_batches = build_batches(X_val,   y_val)\n\n    best_loss, best_state, bad = float('inf'), None, 0\n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for bx, by, bm in tr_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            \n            optimizer.zero_grad()\n            \n            # 混合精度前向传播\n            with autocast():\n                pred = model(bx)\n                loss = criterion(pred, by, bm)\n            \n            # 混合精度反向传播\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_losses.append(loss.item())\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by, bm in va_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                \n                # 验证阶段混合精度\n                with autocast():\n                    pred = model(bx)\n                    val_losses.append(criterion(pred, by, bm).item())\n\n        trl, val = float(np.mean(train_losses)), float(np.mean(val_losses))\n        scheduler.step(val)\n        if epoch % 10 == 0:\n            print(f\"  Epoch {epoch}: train={trl:.4f}, val={val:.4f}\")\n\n        if val < best_loss:\n            best_loss, bad = val, 0\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n\n    if best_state:\n        model.load_state_dict(best_state)\n    return model, best_loss","metadata":{"_uuid":"158e79c8-cacf-4d2b-bbfc-378c46386b84","_cell_guid":"82b97ef0-b19a-44a5-ac09-83100a5a1882","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------_\n# Main pipeline (MODIFICADO PARA ENSEMBLE DE SEMILLAS)\n# ------------------------------_\nclass CFG(Config):\n    # Añadimos la lista de semillas para el ensemble\n    SEEDS = [7] # ¡Puedes cambiar o añadir más semillas aquí!\n\ndef main():\n    cfg = CFG()\n    print(\"=\"*80)\n    print(f\"PASO 2: PIPELINE MEJORADO CON ENSEMBLE DE {len(cfg.SEEDS)} SEMILLAS\")\n    print(\"=\"*80)\n    print(f\"Semillas a utilizar: {cfg.SEEDS}\")\n    print(f\"cuDF backend activo? {USE_CUDF}\")\n\n    # [1/4] Carga de datos (se hace una sola vez)\n    print(\"\\n[1/4] Cargando datos...\")\n    train_input_files  = [cfg.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\"  for w in range(1, 19)]\n    train_output_files = [cfg.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_input  = pd.concat([pd.read_csv(f) for f in train_input_files  if f.exists()], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()], ignore_index=True)\n    test_input     = pd.read_csv(cfg.DATA_DIR / \"test_input.csv\")\n    test_template  = pd.read_csv(cfg.DATA_DIR / \"test.csv\")\n\n    # [2/4] Preparación de secuencias (se hace una sola vez)\n    print(\"\\n[2/4] Construyendo secuencias con características AVANZADAS...\")\n    seqs, tdx, tdy, tfids, seq_meta, feat_cols, dir_map = prepare_sequences_with_advanced_features(\n        train_input, output_df=train_output, is_training=True,\n        window_size=cfg.WINDOW_SIZE\n    )\n\n    # numpy object arrays a listas para un manejo más fácil\n    sequences = list(seqs)\n    targets_dx = list(tdx)\n    targets_dy = list(tdy)\n\n    # [3/4] Entrenamiento con GroupKFold sobre múltiples semillas\n    print(\"\\n[3/4] Iniciando entrenamiento de ensemble...\")\n    \n    # Contenedores para todos los modelos y escaladores de todas las ejecuciones\n    all_models_x, all_models_y, all_scalers = [], [], []\n    fold_rmse_list = []  # Para almacenar RMSE de cada fold\n    \n    groups = np.array([d['game_id'] for d in seq_meta])\n    \n    for seed in cfg.SEEDS:\n        print(f\"\\n{'='*70}\\n   Entrenando con Semilla (Seed): {seed}\\n{'='*70}\")\n        set_seed(seed) # <--- ¡IMPORTANTE! Establecer la semilla para esta ejecución\n        \n        gkf = GroupKFold(n_splits=cfg.N_FOLDS)\n\n        for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n            print(f\"\\n{'-'*60}\\nFold {fold}/{cfg.N_FOLDS} para la semilla {seed}\\n{'-'*60}\")\n\n            X_tr = [sequences[i] for i in tr]\n            X_va = [sequences[i] for i in va]\n\n            # Estandarización por fold\n            scaler = StandardScaler()\n            scaler.fit(np.vstack([s for s in X_tr]))\n\n            X_tr_sc = np.stack([scaler.transform(s) for s in X_tr]).astype(np.float32)\n            X_va_sc = np.stack([scaler.transform(s) for s in X_va]).astype(np.float32)\n\n            # Entrenar modelo para X\n            print(\"Entrenando modelo ΔX...\")\n            mx, loss_x = train_model(\n                X_tr_sc, [targets_dx[i] for i in tr],\n                X_va_sc, [targets_dx[i] for i in va],\n                X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg\n          )\n\n            # Entrenar modelo para Y\n            print(\"Entrenando modelo ΔY...\")\n            my, loss_y = train_model(\n                X_tr_sc, [targets_dy[i] for i in tr],\n                X_va_sc, [targets_dy[i] for i in va],\n                X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg\n            )\n            \n            # Guardar los modelos y el escalador de este fold\n            all_models_x.append(mx)\n            all_models_y.append(my)\n            all_scalers.append(scaler)\n            \n            # Calcular RMSE en el conjunto de validación\n            mx.eval()\n            my.eval()\n            with torch.no_grad():\n                X_va_t = torch.tensor(X_va_sc).to(cfg.DEVICE)\n                pred_dx = mx(X_va_t).cpu().numpy()\n                pred_dy = my(X_va_t).cpu().numpy()\n            \n            # Preparar targets de validación para RMSE\n            y_va_dx = [targets_dx[i] for i in va]\n            y_va_dy = [targets_dy[i] for i in va]\n            \n            # Calcular RMSE: sqrt(mean((x_pred - x_true)^2 + (y_pred - y_true)^2))\n            squared_errors = []\n            for i in range(len(pred_dx)):\n                # Obtener targets reales con padding\n                target_dx_full, mask_dx = prepare_targets([y_va_dx[i]], cfg.MAX_FUTURE_HORIZON)\n                target_dy_full, mask_dy = prepare_targets([y_va_dy[i]], cfg.MAX_FUTURE_HORIZON)\n                \n                target_dx_arr = target_dx_full[0].cpu().numpy()\n                target_dy_arr = target_dy_full[0].cpu().numpy()\n                mask_arr = mask_dx[0].cpu().numpy()\n                \n                # Solo calcular error en posiciones válidas (mask == 1)\n                valid_indices = mask_arr > 0\n                if valid_indices.sum() > 0:\n                    dx_error = (pred_dx[i][valid_indices] - target_dx_arr[valid_indices]) ** 2\n                    dy_error = (pred_dy[i][valid_indices] - target_dy_arr[valid_indices]) ** 2\n                    squared_errors.extend(dx_error + dy_error)\n            \n            fold_rmse = np.sqrt(np.mean(squared_errors) / 2)\n            fold_rmse_list.append(fold_rmse)\n            \n            print(f\"Fold {fold} (semilla {seed}) — val loss: dx={loss_x:.5f}, dy={loss_y:.5f} | RMSE={fold_rmse:.5f}\")\n\n    # Calcular estadísticas de RMSE\n    rmse_mean = np.mean(fold_rmse_list)\n    rmse_std = np.std(fold_rmse_list)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ESTADÍSTICAS DE RMSE POR FOLD\")\n    print(\"=\"*80)\n    for i, rmse in enumerate(fold_rmse_list, 1):\n        print(f\"Fold {i}: RMSE = {rmse:.5f}\")\n    print(\"-\"*80)\n    print(f\"RMSE Promedio: {rmse_mean:.5f}\")\n    print(f\"RMSE Desviación Estándar: {rmse_std:.5f}\")\n    print(\"=\"*80)\n\n    # [4/4] Inferencia sobre el test usando todos los modelos entrenados\n    print(f\"\\n[4/4] Inferencia y submission con ensemble de {len(all_models_x)} modelos...\")\n    test_seqs, test_meta, feat_cols_t, dir_map_test = prepare_sequences_with_advanced_features(\n        test_input, test_template=test_template, is_training=False,\n        window_size=cfg.WINDOW_SIZE\n    )\n    assert feat_cols_t == feat_cols, \"¡Las columnas de características de Train/Test no coinciden!\"\n\n    idx_x = feat_cols.index('x')\n    idx_y = feat_cols.index('y')\n\n    X_test_raw = list(test_seqs)\n    x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n    y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n\n    # Ensemble a través de todos los modelos de todos los folds y todas las semillas\n    all_preds_dx, all_preds_dy = [], []\n    \n    # Iteramos sobre todos los modelos y escaladores guardados\n    for mx, my, sc in zip(all_models_x, all_models_y, all_scalers):\n        X_sc = np.stack([sc.transform(s) for s in X_test_raw]).astype(np.float32)\n        X_t = torch.tensor(X_sc).to(cfg.DEVICE)\n        mx.eval()\n        my.eval()\n        with torch.no_grad():\n            all_preds_dx.append(mx(X_t).cpu().numpy())\n            all_preds_dy.append(my(X_t).cpu().numpy())\n\n    # Promediamos todas las predicciones\n    ens_dx = np.mean(all_preds_dx, axis=0)\n    ens_dy = np.mean(all_preds_dy, axis=0)\n    H = ens_dx.shape[1]\n\n    # Construcción de las filas para la submission, con inversión para jugadas a la derecha\n    rows = []\n    tt_idx = test_template.set_index(['game_id','play_id','nfl_id']).sort_index()\n\n    for i, meta in enumerate(test_meta):\n        gid = meta['game_id']; pid = meta['play_id']; nid = meta['nfl_id']\n        play_dir = meta['play_direction']\n        play_is_right = (play_dir == 'right')\n\n        try:\n            fids = tt_idx.loc[(gid,pid,nid),'frame_id']\n            if isinstance(fids, pd.Series):\n                fids = fids.sort_values().tolist()\n            else:\n                fids = [int(fids)]\n        except KeyError:\n            continue\n\n        for t, fid in enumerate(fids):\n            tt = min(t, H - 1)\n            x_uni = np.clip(x_last_uni[i] + ens_dx[i, tt], 0, FIELD_LENGTH)\n            y_uni = np.clip(y_last_uni[i] + ens_dy[i, tt], 0, FIELD_WIDTH)\n            x_out, y_out = invert_to_original_direction(x_uni, y_uni, play_is_right)\n\n            rows.append({\n                'id': f\"{gid}_{pid}_{nid}_{int(fid)}\",\n                'x': x_out,\n                'y': y_out\n            })\n\n    submission = pd.DataFrame(rows)\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"\\n\" + \"=\"*80)\n    print(\"¡PASO 2 COMPLETO!\")\n    print(\"=\"*80)\n    print(f\"✓ Submission guardada en submission.csv  |  Filas: {len(submission)}\")\n    print(f\"Total de modelos en ensemble: {len(all_models_x)}\")\n    print(f\"Características utilizadas: {len(feat_cols)}  (cuDF activo: {USE_CUDF})\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"3ca303d2-ec2b-4690-a186-3f93c8638529","_cell_guid":"725dcc39-caed-4588-a453-ab09e6294699","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}