{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# é€šè¿‡add_advanced_featureså‡½æ•°æ·»åŠ äº†ä¸€äº›ç‰¹å¾","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:08:53.776778Z","iopub.execute_input":"2025-10-28T07:08:53.777176Z","iopub.status.idle":"2025-10-28T07:08:53.783278Z","shell.execute_reply.started":"2025-10-28T07:08:53.777142Z","shell.execute_reply":"2025-10-28T07:08:53.782204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nimport os\nimport pickle\nimport joblib\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom torch.utils.data import TensorDataset, DataLoader\n\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_debug = True\n# ============================================================================\n# CONFIG\n# ============================================================================\n\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    \n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 256\n    EPOCHS = 2 if is_debug else 200\n    NFILE = 2 if is_debug else 19\n    PATIENCE = 30\n    LEARNING_RATE = 1e-3\n    \n    WINDOW_SIZE = 10  # ä¸æ¨ç†ä¿æŒä¸€è‡´\n    HIDDEN_DIM = 128\n    MAX_FUTURE_HORIZON = 94  # ä¸æ¨ç†ä¿æŒä¸€è‡´\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(Config.SEED)\n\n# ============================================================================\n# FEATURE ENGINEERING (ä¸æ¨ç†å®Œå…¨ä¸€è‡´)\n# ============================================================================\n\ndef height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n    \ndef add_advanced_features(df):\n    \"\"\"Original nflnn.py features\"\"\"\n    print(\"Adding advanced features...\")\n    df = df.copy()\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # ä½ç½®å·®åˆ†\n    if 'distance_to_ball' in df.columns:\n        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n        df['time_to_intercept'] = (df['distance_to_ball'] / \n                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n    \n    # æœå‘çƒçš„ç‰¹å¾\n    if 'ball_direction_x' in df.columns:\n        df['velocity_alignment'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_perpendicular'] = (\n            df['velocity_x'] * (-df['ball_direction_y']) +\n            df['velocity_y'] * df['ball_direction_x']\n        )\n        if 'acceleration_x' in df.columns:\n            df['accel_alignment'] = (\n                df['acceleration_x'] * df['ball_direction_x'] +\n                df['acceleration_y'] * df['ball_direction_y']\n            )\n    \n    # é€Ÿåº¦å·®åˆ†\n    if 'velocity_x' in df.columns:\n        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n        df['direction_change'] = df['direction_change'].apply(\n            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        )\n    \n    # è·ç¦»è¾¹ç•Œä½ç½®\n    df['dist_from_left'] = df['y']\n    df['dist_from_right'] = 53.3 - df['y']\n    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n    \n    # çƒå‘˜èŒä½ç‰¹å¾\n    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n    \n    # æ—¶é—´ç‰¹å¾\n    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n        lambda x: x / (x.max() + 1)\n    )\n    \n    print(f\"Total features after enhancement: {len(df.columns)}\")\n    \n    return df\n\n\ndef prepare_sequences_fixed(input_df, output_df=None, test_template=None, is_training=True, window_size=8):\n    input_df = input_df.copy()\n    \n    # Basic features\n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n    \n    # Velocity\n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    delta_t = 0.1\n    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n    \n    # Acceleration\n    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n    \n    # Roles\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    \n    # Physics\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n    \n    # Ball features\n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n    \n    # Sort\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # Lag & EMA Features\n    for lag in [1, 2, 3]:\n        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n    \n    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_x_roll'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.rolling(window_size, min_periods=1).mean()\n    )\n    input_df['velocity_y_roll'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.rolling(window_size, min_periods=1).mean()\n    )\n    \n    print(\"Step 2/4: Adding advanced features...\")\n    input_df = add_advanced_features(input_df)\n\n    feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id',\n        'ball_land_x', 'ball_land_y',\n        'player_height_feet', 'player_weight',\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n        'velocity_x_roll', 'velocity_y_roll',\n\n        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n        'dist_from_sideline', 'dist_from_endzone',\n        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n        'frames_elapsed', 'normalized_time',\n    ]\n    \n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"ä½¿ç”¨ {len(feature_cols)} ä¸ªç‰¹å¾\" if not is_training else f\"ä½¿ç”¨ {len(feature_cols)} ä¸ªç‰¹å¾ (è®­ç»ƒ)\")\n    \n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    \n    if is_training:\n        target_rows = output_df\n        target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n        sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n        \n        for _, row in tqdm(target_groups.iterrows()):\n            key = (row['game_id'], row['play_id'], row['nfl_id'])\n            try:\n                group_df = grouped.get_group(key)\n            except KeyError:\n                continue\n            \n            input_window = group_df.tail(window_size)\n            if len(input_window) < window_size:\n                if is_training:\n                    continue\n                pad_len = window_size - len(input_window)\n                pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n                input_window = pd.concat([pad_df, input_window], ignore_index=True)\n            \n            input_window = input_window.fillna(group_df.mean(numeric_only=True))\n            seq = input_window[feature_cols].values\n            if np.isnan(seq).any():\n                if is_training:\n                    continue\n                seq = np.nan_to_num(seq, nan=0.0)\n            \n            sequences.append(seq)\n            \n            out_grp = target_rows[\n                (target_rows['game_id']==row['game_id']) &\n                (target_rows['play_id']==row['play_id']) &\n                (target_rows['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            \n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n            \n            sequence_ids.append({\n                'game_id': key[0],\n                'play_id': key[1],\n                'nfl_id': key[2],\n                'frame_id': input_window.iloc[-1]['frame_id']\n            })\n        \n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n    else:\n        target_groups = test_template[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n        sequences, sequence_ids = [], []\n        for _, row in target_groups.iterrows():\n            key = (row['game_id'], row['play_id'], row['nfl_id'])\n            try:\n                group_df = grouped.get_group(key)\n            except KeyError:\n                continue\n            \n            input_window = group_df.tail(window_size)\n            if len(input_window) < window_size:\n                pad_len = window_size - len(input_window)\n                pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n                input_window = pd.concat([pad_df, input_window], ignore_index=True)\n            \n            input_window = input_window.fillna(group_df.mean(numeric_only=True))\n            seq = input_window[feature_cols].values\n            if np.isnan(seq).any():\n                seq = np.nan_to_num(seq, nan=0.0)\n            \n            sequences.append(seq)\n            sequence_ids.append({\n                'game_id': key[0],\n                'play_id': key[1],\n                'nfl_id': key[2],\n                'frame_id': input_window.iloc[-1]['frame_id']\n            })\n        \n        return sequences, sequence_ids\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# LOSS (ä¸è®­ç»ƒä¸€è‡´)\n# ============================================================================\n\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n    \n    def forward(self, pred, target, mask):\n        err = pred - target\n        abs_err = torch.abs(err)\n        \n        huber = torch.where(\n            abs_err <= self.delta,\n            0.5 * err * err,\n            self.delta * (abs_err - 0.5 * self.delta)\n        )\n        \n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            weight = torch.exp(-self.time_decay * t).view(1, L)\n            huber = huber * weight\n            mask = mask * weight\n        \n        return (huber * mask).sum() / (mask.sum() + 1e-8)\n\n# ============================================================================\n# MODEL (ä¸è®­ç»ƒä¸€è‡´)\n# ============================================================================\n\nclass ImprovedSeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        self.horizon = horizon\n        self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1)\n        self.pool_ln = nn.LayerNorm(128)\n        self.pool_attn = nn.MultiheadAttention(128, num_heads=4, batch_first=True)\n        self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n        self.head = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, horizon)\n        )\n    \n    def forward(self, x):\n        h, _ = self.gru(x)\n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)\n        h_norm = self.pool_ln(h)\n        ctx, _ = self.pool_attn(q, h_norm, h_norm)\n        ctx = ctx.squeeze(1)\n        out = self.head(ctx)\n        out = torch.cumsum(out, dim=1)\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# TRAINING (æ ¸å¿ƒè®­ç»ƒé€»è¾‘)\n# ============================================================================\n\ndef prepare_targets(batch_axis, max_h):\n    tensors, masks = [], []\n    for arr in batch_axis:\n        L = len(arr)\n        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors), torch.stack(masks)\n\ndef train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config):\n    device = config.DEVICE\n    model = ImprovedSeqModel(input_dim, horizon).to(device)\n    \n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=True)\n    \n    # Batches\n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        by, bm = prepare_targets([y_train[j] for j in range(i, end)], horizon)\n        train_batches.append((bx, by, bm))\n    \n    val_batches = []\n    for i in range(0, len(X_val), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_val))\n        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        by, bm = prepare_targets([y_val[j] for j in range(i, end)], horizon)\n        val_batches.append((bx, by, bm))\n    \n    best_loss, best_state, bad = float('inf'), None, 0\n    \n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        \n        for bx, by, bm in train_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            pred = model(bx)\n            loss = criterion(pred, by, bm)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n        \n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by, bm in val_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                loss = criterion(pred, by, bm)\n                val_losses.append(loss.item())\n        \n        train_loss = np.mean(train_losses)\n        val_loss = np.mean(val_losses)\n        scheduler.step(val_loss)\n        \n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}/{config.EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n            print(f\"  âœ… Best val loss improved to {best_loss:.4f}, saving model\")\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  âŒ Early stopping at epoch {epoch} (no improvement for {bad} epochs)\")\n                break\n    \n    if best_state:\n        model.load_state_dict(best_state)\n    \n    return model, best_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# MAIN TRAINING FLOW\n# ============================================================================\n\n\nprint(\"=\"*80)\nprint(\"NFL Big Data Bowl 2026 - TRAINING PIPELINE\")\nprint(\"=\"*80)\n\n# ===== 1. åŠ è½½æ•°æ® =====\nprint(\"\\n[1/5] åŠ è½½æ•°æ®...\")\ntrain_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, Config.NFILE)]  # åªç”¨ç¬¬1å‘¨æ•°æ®\ntrain_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, Config.NFILE)]\n\ntrain_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\ntrain_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n\ntest_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\ntest_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n\nprint(f\"è®­ç»ƒæ•°æ®: {len(train_input):,} è¡Œ, {len(train_output):,} è¡Œè¾“å‡º\")\nprint(f\"æµ‹è¯•æ¨¡æ¿: {len(test_input):,} è¡Œ\")\n\n# ===== 2. ç‰¹å¾å·¥ç¨‹ =====\nprint(\"\\n[2/5] ç‰¹å¾å·¥ç¨‹...\")\nsequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = prepare_sequences_fixed(\n    train_input, train_output, is_training=True, window_size=Config.WINDOW_SIZE\n)\n\nsequences = np.array(sequences, dtype=object)\ntargets_dx = np.array(targets_dx, dtype=object)\ntargets_dy = np.array(targets_dy, dtype=object)\n\nprint(f\"ç”Ÿæˆåºåˆ—æ•°é‡: {len(sequences):,}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ===== 3. è®­ç»ƒæ¨¡å‹ (5æŠ˜äº¤å‰éªŒè¯) =====\nprint(\"\\n[3/5] å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\ngroups = np.array([d['game_id'] for d in sequence_ids])\ngkf = GroupKFold(n_splits=Config.N_FOLDS)\n\nmodels_x, models_y, scalers = [], [], []\n\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(sequences, groups=groups), 1):\n    print(f\"\\n{'='*40}\")\n    print(f\"Fold {fold}/{Config.N_FOLDS}\")\n    print(f\"{'='*40}\")\n    \n    X_tr = sequences[tr_idx]\n    X_va = sequences[va_idx]\n    y_tr_dx = [targets_dx[i] for i in tr_idx]\n    y_va_dx = [targets_dx[i] for i in va_idx]\n    y_tr_dy = [targets_dy[i] for i in tr_idx]\n    y_va_dy = [targets_dy[i] for i in va_idx]\n    \n    # ç‰¹å¾æ ‡å‡†åŒ– (åªç”¨è®­ç»ƒé›†fit)\n    scaler = StandardScaler()\n    scaler.fit(np.vstack([s for s in X_tr]))\n    \n    # æ ‡å‡†åŒ–æ•°æ®\n    X_tr_scaled = np.stack([scaler.transform(s) for s in X_tr])\n    X_va_scaled = np.stack([scaler.transform(s) for s in X_va])\n    \n    input_dim = X_tr[0].shape[1]  # ç‰¹å¾ç»´åº¦\n    \n    # ===== è®­ç»ƒ X æ–¹å‘æ¨¡å‹ =====\n    print(\"\\nğŸ”µ è®­ç»ƒ X æ–¹å‘æ¨¡å‹...\")\n    model_x, best_loss_x = train_model(\n        X_tr_scaled, y_tr_dx, X_va_scaled, y_va_dx,\n        input_dim, Config.MAX_FUTURE_HORIZON, Config\n    )\n    models_x.append(model_x)\n    scalers.append(scaler)\n    print(f\"âœ… X æ–¹å‘æœ€ä½³ Val Loss: {best_loss_x:.4f}\")\n    \n    # ===== è®­ç»ƒ Y æ–¹å‘æ¨¡å‹ =====\n    print(\"\\nğŸ”´ è®­ç»ƒ Y æ–¹å‘æ¨¡å‹...\")\n    model_y, best_loss_y = train_model(\n        X_tr_scaled, y_tr_dy, X_va_scaled, y_va_dy,\n        input_dim, Config.MAX_FUTURE_HORIZON, Config\n    )\n    models_y.append(model_y)\n    print(f\"âœ… Y æ–¹å‘æœ€ä½³ Val Loss: {best_loss_y:.4f}\")\n\n# ===== 4. ä¿å­˜æ¨¡å‹ =====\nprint(\"\\n[4/5] ä¿å­˜æ¨¡å‹...\")\nMODEL_SAVE_DIR = Path(\"./models/\")\nMODEL_SAVE_DIR.mkdir(exist_ok=True)\n\nfor fold in range(Config.N_FOLDS):\n    # ä¿å­˜æ¨¡å‹X\n    torch.save(models_x[fold].state_dict(), MODEL_SAVE_DIR / f\"model_x_fold{fold+1}.pth\")\n    # ä¿å­˜æ¨¡å‹Y\n    torch.save(models_y[fold].state_dict(), MODEL_SAVE_DIR / f\"model_y_fold{fold+1}.pth\")\n    # ä¿å­˜Scaler\n    joblib.dump(scalers[fold], MODEL_SAVE_DIR / f\"scaler_fold{fold+1}.pkl\")\n    \n    print(f\"å·²ä¿å­˜ Fold {fold+1} æ¨¡å‹åˆ° {MODEL_SAVE_DIR}\")\n\nprint(f\"\\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° {MODEL_SAVE_DIR}\")\nprint(f\"ğŸ“‚ è¯·å°†æ­¤æ–‡ä»¶å¤¹å‹ç¼©å¹¶ä¸Šä¼ ä¸º Kaggle Datasetï¼Œå‘½åä¸ºå¦‚: nfl-models\")\nprint(f\"ğŸ”— æ¨ç†æ—¶è¯·å°†æ¨¡å‹ä¸Šä¼ åˆ° /kaggle/input/nfl-models/ ç›®å½•\")\nprint(f\"ğŸ“ æ¨ç†notebookä¼šè‡ªåŠ¨ä»è¯¥ç›®å½•åŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf /kaggle/working/models/\n# !rm /kaggle/working/nfl_models.zip\n\n# æ‰“åŒ…æ¨¡å‹æƒé‡\n# ä¸‹è½½åˆ°æœ¬åœ°å†å¯¼å…¥æ¨ç†Notebook\n!zip -r nfl_models.zip /kaggle/working/models","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:42:45.276Z"}},"outputs":[],"execution_count":null}]}