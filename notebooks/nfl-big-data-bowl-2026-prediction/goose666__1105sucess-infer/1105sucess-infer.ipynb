{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"},{"sourceId":13527658,"sourceType":"datasetVersion","datasetId":8589472},{"sourceId":13529204,"sourceType":"datasetVersion","datasetId":8590590},{"sourceId":13543975,"sourceType":"datasetVersion","datasetId":8601411},{"sourceId":13573350,"sourceType":"datasetVersion","datasetId":8622640},{"sourceId":13645042,"sourceType":"datasetVersion","datasetId":8674000},{"sourceId":13648589,"sourceType":"datasetVersion","datasetId":8676640},{"sourceId":624487,"sourceType":"modelInstanceVersion","modelInstanceId":469942,"modelId":485817},{"sourceId":624836,"sourceType":"modelInstanceVersion","modelInstanceId":470214,"modelId":486107},{"sourceId":625042,"sourceType":"modelInstanceVersion","modelInstanceId":470394,"modelId":486290},{"sourceId":625077,"sourceType":"modelInstanceVersion","modelInstanceId":470425,"modelId":486320},{"sourceId":626032,"sourceType":"modelInstanceVersion","modelInstanceId":471249,"modelId":487154},{"sourceId":626426,"sourceType":"modelInstanceVersion","modelInstanceId":471583,"modelId":487487},{"sourceId":626796,"sourceType":"modelInstanceVersion","modelInstanceId":471881,"modelId":487784},{"sourceId":626808,"sourceType":"modelInstanceVersion","modelInstanceId":471892,"modelId":487795},{"sourceId":627985,"sourceType":"modelInstanceVersion","modelInstanceId":472923,"modelId":488792},{"sourceId":628006,"sourceType":"modelInstanceVersion","modelInstanceId":472943,"modelId":488812}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"my work based on these excellent notebooks:\n\nhttps://www.kaggle.com/code/yusufsertkayaysk/nfl-big-data-bowl-2026-lb-0-604\n\nhttps://www.kaggle.com/code/ryanadamsai/nfl-big-data-bowl-geometric-gnn-lb-586\n\nhttps://www.kaggle.com/code/muran169633/baseline-0-583\n\nhuge respect to the authors.\nI was also insipired by the ideas on the discussions.","metadata":{}},{"cell_type":"markdown","source":"My train notebook is https://www.kaggle.com/code/goose666/1109mytrain0579\n\nThe score after inference using the trained weight file is 0.579.\nBut I lost my 0.577 train notebook.I only adjusted some parameters here, but I've forgotten the details.","metadata":{}},{"cell_type":"markdown","source":"# **About**","metadata":{}},{"cell_type":"markdown","source":"This solution integrates a variety of effective feature engineering techniques, which I've encapsulated into functions and called during the creation of frame sequences, resulting in a total of 187 features. The model employs a STtransformer with residual MLP blocks, and has undergone some hyperparameter tuning. It predicts positional residuals. Training takes approximately one and a half hours, and inference followed by evaluation requires about two hours.","metadata":{}},{"cell_type":"markdown","source":"# **Future** ","metadata":{}},{"cell_type":"markdown","source":"The current model is overfitting. My friend suggested that this is because we are predicting the residual of the position rather than the position itself. Also, some of the features created during the feature engineering phase may be redundant and ineffective. I will spend more time exploring data preprocessing and feature engineering next. Perhaps this task does not require a very complex model, we need to refocus on the data itself.(my English is not good)","metadata":{}},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nimport os\nimport pickle\nimport joblib\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom torch.utils.data import TensorDataset, DataLoader\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:43:44.414375Z","iopub.execute_input":"2025-11-08T16:43:44.415325Z","iopub.status.idle":"2025-11-08T16:43:50.593173Z","shell.execute_reply.started":"2025-11-08T16:43:44.415234Z","shell.execute_reply":"2025-11-08T16:43:50.59243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    \n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 256\n    WINDOW_SIZE = 12\n    HIDDEN_DIM = 128\n    MAX_FUTURE_HORIZON = 94\n\n    K_NEIGH = 6\n    RADIUS = 30.0\n    TAU = 8.0\n    N_ROUTE_CLUSTERS = 7\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(Config.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:43:50.594597Z","iopub.execute_input":"2025-11-08T16:43:50.595063Z","iopub.status.idle":"2025-11-08T16:43:50.608572Z","shell.execute_reply.started":"2025-11-08T16:43:50.595034Z","shell.execute_reply":"2025-11-08T16:43:50.607747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Part","metadata":{}},{"cell_type":"code","source":"class ResidualMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):\n        super().__init__()\n        layers = []\n        \n        layers.append(nn.Linear(input_dim, hidden_dim))\n        layers.append(nn.LayerNorm(hidden_dim))\n        layers.append(nn.GELU())\n        layers.append(nn.Dropout(dropout))\n        \n        for _ in range(num_layers - 2):\n            layers.append(ResidualBlock(hidden_dim, hidden_dim, dropout))\n        \n        layers.append(nn.Linear(hidden_dim, output_dim))\n        \n        self.net = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.net(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(dropout),\n        )\n        self.activation = nn.GELU()\n    \n    def forward(self, x):\n        return self.activation(self.net(x) + x)\n\nclass SpatioTemporalTransformer(nn.Module):\n    def __init__(self, input_dim, horizon, hidden_dim=256, num_heads=8, num_layers=4, dropout=0.1):\n        super().__init__()\n        self.horizon = horizon\n        self.hidden_dim = hidden_dim\n        \n        self.input_projection = nn.Linear(input_dim, hidden_dim)\n        self.temporal_pos_encoding = nn.Parameter(torch.randn(1, Config.WINDOW_SIZE, hidden_dim))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            batch_first=True,\n            norm_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.prediction_head = ResidualMLP(\n            input_dim=hidden_dim,\n            hidden_dim=hidden_dim * 2,\n            output_dim=horizon,\n            num_layers=3,\n            dropout=dropout\n        )\n        \n        self.output_norm = nn.LayerNorm(horizon)\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                torch.nn.init.constant_(module.bias, 0.0)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.constant_(module.bias, 0.0)\n            torch.nn.init.constant_(module.weight, 1.0)\n    \n    def forward(self, x):\n        batch_size, window_size, _ = x.shape\n        \n        x = self.input_projection(x)\n        x = x + self.temporal_pos_encoding[:, :window_size, :]\n        x = self.transformer_encoder(x)\n        \n        attention_weights = torch.softmax(torch.mean(x, dim=-1), dim=-1)\n        x_pooled = torch.sum(x * attention_weights.unsqueeze(-1), dim=1)\n        \n        pred = self.prediction_head(x_pooled)\n        pred = self.output_norm(pred)\n        pred = torch.cumsum(pred, dim=1)\n        \n        return pred\n\nclass ImprovedSeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        self.horizon = horizon\n        self.model = SpatioTemporalTransformer(\n            input_dim=input_dim,\n            horizon=horizon,\n            hidden_dim=256,\n            num_heads=8,\n            num_layers=4,\n            dropout=0.1\n        )\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:43:50.609518Z","iopub.execute_input":"2025-11-08T16:43:50.609767Z","iopub.status.idle":"2025-11-08T16:43:50.635266Z","shell.execute_reply.started":"2025-11-08T16:43:50.609748Z","shell.execute_reply":"2025-11-08T16:43:50.6343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature","metadata":{}},{"cell_type":"code","source":"def height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n\ndef get_velocity(speed, direction_deg):\n    theta = np.deg2rad(direction_deg)\n    return speed * np.sin(theta), speed * np.cos(theta)\n\ndef create_base_features(input_df):\n    df = input_df.copy()\n    \n    df['player_height_feet'] = df['player_height'].apply(height_to_feet)\n\n    height_parts = df['player_height'].str.split('-', expand=True)\n    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n\n    dir_rad = np.deg2rad(df['dir'].fillna(0))\n    df['velocity_x'] = df['s'] * np.sin(dir_rad)\n    df['velocity_y'] = df['s'] * np.cos(dir_rad)\n    df['acceleration_x'] = df['a'] * np.cos(dir_rad)\n    df['acceleration_y'] = df['a'] * np.sin(dir_rad)\n\n    df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n    df['is_defense'] = (df['player_side'] == 'Defense').astype(int)\n    df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['is_passer'] = (df['player_role'] == 'Passer').astype(int)\n\n    df['role_targeted_receiver'] = df['is_receiver']\n    df['role_defensive_coverage'] = df['is_coverage']\n    df['role_passer'] = df['is_passer']\n    df['side_offense'] = df['is_offense']\n\n    mass_kg = df['player_weight'].fillna(200.0) / 2.20462\n    df['momentum_x'] = df['velocity_x'] * df['player_weight']\n    df['momentum_y'] = df['velocity_y'] * df['player_weight']\n    df['kinetic_energy'] = 0.5 * df['player_weight'] * (df['s'] ** 2)\n\n    df['speed_squared'] = df['s'] ** 2\n    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n\n    if 'ball_land_x' in df.columns:\n        ball_dx = df['ball_land_x'] - df['x']\n        ball_dy = df['ball_land_y'] - df['y']\n        df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        df['dist_to_ball'] = df['distance_to_ball']\n        df['dist_squared'] = df['distance_to_ball'] ** 2\n        df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        df['ball_direction_x'] = ball_dx / (df['distance_to_ball'] + 1e-6)\n        df['ball_direction_y'] = ball_dy / (df['distance_to_ball'] + 1e-6)\n        df['closing_speed_ball'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_toward_ball'] = (\n            df['velocity_x'] * np.cos(df['angle_to_ball']) + \n            df['velocity_y'] * np.sin(df['angle_to_ball'])\n        )\n        df['velocity_alignment'] = np.cos(df['angle_to_ball'] - dir_rad)\n        df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n        df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n    \n    return df\n\ndef create_lag_features(df, window_size=8):\n    df = df.copy()\n    \n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n\n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag)\n\n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in df.columns:\n                df[f'{col}_rolling_mean_{window}'] = (\n                    df.groupby(gcols)[col]\n                    .rolling(window, min_periods=1).mean()\n                    .reset_index(level=[0,1,2], drop=True)\n                )\n                df[f'{col}_rolling_std_{window}'] = (\n                    df.groupby(gcols)[col]\n                    .rolling(window, min_periods=1).std()\n                    .reset_index(level=[0,1,2], drop=True)\n                )\n\n    for col in ['velocity_x', 'velocity_y']:\n        if col in df.columns:\n            df[f'{col}_delta'] = df.groupby(gcols)[col].diff()\n\n    df['velocity_x_ema'] = df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    df['velocity_y_ema'] = df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    df['speed_ema'] = df.groupby(gcols)['s'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    \n    return df\n\ndef get_opponent_features(input_df):\n    features = []\n    \n    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']),\n                                desc=\"üèà Opponents\", leave=False):\n        last = group.sort_values('frame_id').groupby('nfl_id').last()\n\n        if len(last) < 2:\n            continue\n\n        positions = last[['x', 'y']].values\n        sides = last['player_side'].values\n        speeds = last['s'].values\n        directions = last['dir'].values\n        roles = last['player_role'].values\n\n        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n\n        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n            opp_mask = sides != side\n\n            feat = {\n                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n                'mirror_wr_dist': 50.0,\n            }\n\n            if not opp_mask.any():\n                features.append(feat)\n                continue\n\n            opp_positions = positions[opp_mask]\n            distances = np.sqrt(((positions[i] - opp_positions) ** 2).sum(axis=1))\n\n            if len(distances) == 0:\n                features.append(feat)\n                continue\n\n            nearest_idx = distances.argmin()\n            feat['nearest_opp_dist'] = distances[nearest_idx]\n            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n\n            my_vx, my_vy = get_velocity(speeds[i], directions[i])\n            opp_speeds = speeds[opp_mask]\n            opp_dirs = directions[opp_mask]\n            opp_vx, opp_vy = get_velocity(opp_speeds[nearest_idx], opp_dirs[nearest_idx])\n\n            rel_vx = my_vx - opp_vx\n            rel_vy = my_vy - opp_vy\n            to_me = positions[i] - opp_positions[nearest_idx]\n            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n            feat['closing_speed'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n\n            if role == 'Defensive Coverage' and receiver_mask.any():\n                rec_positions = positions[receiver_mask]\n                rec_distances = np.sqrt(((positions[i] - rec_positions) ** 2).sum(axis=1))\n\n                if len(rec_distances) > 0:\n                    closest_rec_idx = rec_distances.argmin()\n                    rec_indices = np.where(receiver_mask)[0]\n                    actual_rec_idx = rec_indices[closest_rec_idx]\n\n                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n\n                    feat['mirror_wr_vx'] = rec_vx\n                    feat['mirror_wr_vy'] = rec_vy\n                    feat['mirror_wr_dist'] = rec_distances[closest_rec_idx]\n                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n\n            features.append(feat)\n\n    return pd.DataFrame(features)\n\ndef extract_route_patterns(input_df, kmeans=None, scaler=None, fit=False):\n    route_features = []\n    \n    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']), \n                                      desc=\"üõ£Ô∏è Routes\", leave=False):\n        traj = group.sort_values('frame_id').tail(5)\n        \n        if len(traj) < 3:\n            continue\n        \n        positions = traj[['x', 'y']].values\n        speeds = traj['s'].values\n        \n        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 + \n                               (positions[-1, 1] - positions[0, 1])**2)\n        straightness = displacement / (total_dist + 0.1)\n        \n        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n        if len(angles) > 1:\n            angle_changes = np.abs(np.diff(angles))\n            max_turn = np.max(angle_changes)\n            mean_turn = np.mean(angle_changes)\n        else:\n            max_turn = mean_turn = 0\n        \n        speed_mean = speeds.mean()\n        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n        dx = positions[-1, 0] - positions[0, 0]\n        dy = positions[-1, 1] - positions[0, 1]\n        \n        route_features.append({\n            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n            'traj_straightness': straightness,\n            'traj_max_turn': max_turn,\n            'traj_mean_turn': mean_turn,\n            'traj_depth': abs(dx),\n            'traj_width': abs(dy),\n            'speed_mean': speed_mean,\n            'speed_change': speed_change,\n        })\n    \n    route_df = pd.DataFrame(route_features)\n    if route_df.empty or 'traj_straightness' not in route_df.columns:\n        return pd.DataFrame()\n            \n    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n    X = route_df[feat_cols].fillna(0)\n    \n    if kmeans is None or scaler is None:\n        return route_df\n    \n    X_scaled = scaler.transform(X)\n    route_df['route_pattern'] = kmeans.predict(X_scaled)\n    return route_df\n\ndef compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH, \n                                radius=Config.RADIUS, tau=Config.TAU):\n    \n    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\", \n                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n    src = input_df[cols_needed].copy()\n    \n    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n               .tail(1)\n               .rename(columns={\"frame_id\": \"last_frame_id\"})\n               .reset_index(drop=True))\n    \n    tmp = last.merge(\n        src.rename(columns={\n            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n            \"x\": \"x_nb\", \"y\": \"y_nb\", \n            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\", \n            \"player_side\": \"player_side_nb\"\n        }),\n        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n        how=\"left\"\n    )\n    \n    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n    \n    tmp = tmp[np.isfinite(tmp[\"dist\"]) & (tmp[\"dist\"] > 1e-6)]\n    if radius is not None:\n        tmp = tmp[tmp[\"dist\"] <= radius]\n    \n    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"] == tmp[\"player_side\"]).astype(np.float32)\n    \n    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n    if k_neigh is not None:\n        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n    \n    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n    \n    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n    \n    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n    \n    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n    \n    ag = tmp.groupby(keys).agg(\n        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n        gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n    ).reset_index()\n    \n    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n    if len(near) > 0:\n        near[\"rnk\"] = near[\"rnk\"].astype(int)\n        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n        dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n        ag = ag.merge(dwide, on=keys, how=\"left\")\n    \n    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n              \"gnn_opp_dx_mean\", \"gnn_opp_dy_mean\", \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\", \n              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n    \n    return ag\n\ndef compute_geometric_endpoint(df):\n    \"\"\"Âü∫‰∫éÂá†‰ΩïËßÑÂàôËÆ°ÁÆóÊØè‰∏™ÁêÉÂëòÁöÑÁªàÁÇπ‰ΩçÁΩÆ\"\"\"\n    df = df.copy()\n\n    if 'num_frames_output' in df.columns:\n        t_total = df['num_frames_output'] / 10.0\n    else:\n        t_total = 3.0\n    \n    df['time_to_endpoint'] = t_total\n\n    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n\n    if 'ball_land_x' in df.columns:\n        receiver_mask = df['player_role'] == 'Targeted Receiver'\n        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n\n        defender_mask = df['player_role'] == 'Defensive Coverage'\n        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n        coverage_mask = defender_mask & has_mirror\n        \n        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n            df.loc[coverage_mask, 'ball_land_x'] + \n            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n        )\n        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n            df.loc[coverage_mask, 'ball_land_y'] + \n            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n        )\n\n    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    return df\n\ndef add_geometric_features(df):\n    df = compute_geometric_endpoint(df)\n\n    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n\n    t = df['time_to_endpoint'] + 0.1\n    df['geo_required_vx'] = df['geo_vector_x'] / t\n    df['geo_required_vy'] = df['geo_vector_y'] / t\n\n    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n    df['geo_velocity_error'] = np.sqrt(\n        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n    )\n\n    t_sq = t * t\n    df['geo_required_ax'] = 2 * df['geo_vector_x'] / t_sq\n    df['geo_required_ay'] = 2 * df['geo_vector_y'] / t_sq\n    df['geo_required_ax'] = df['geo_required_ax'].clip(-10, 10)\n    df['geo_required_ay'] = df['geo_required_ay'].clip(-10, 10)\n\n    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n    df['geo_alignment'] = (\n        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n    ) / (velocity_mag + 0.1)\n\n    df['geo_receiver_urgency'] = df['is_receiver'] * df['geo_distance'] / (t + 0.1)\n    df['geo_defender_coupling'] = df['is_coverage'] * (1.0 / (df.get('mirror_wr_dist', 50) + 1.0))\n    \n    return df\n\ndef add_advanced_features(df):\n    df = df.copy()\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n\n    if 'distance_to_ball' in df.columns:\n        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n        df['time_to_intercept'] = (df['distance_to_ball'] / \n                                  (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n\n    if 'ball_direction_x' in df.columns:\n        df['velocity_alignment'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_perpendicular'] = (\n            df['velocity_x'] * (-df['ball_direction_y']) +\n            df['velocity_y'] * df['ball_direction_x']\n        )\n        if 'acceleration_x' in df.columns:\n            df['accel_alignment'] = (\n                df['acceleration_x'] * df['ball_direction_x'] +\n                df['acceleration_y'] * df['ball_direction_y']\n            )\n\n    if 'velocity_x' in df.columns:\n        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n        df['direction_change'] = df['direction_change'].apply(\n            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        )\n\n    df['dist_from_left'] = df['y']\n    df['dist_from_right'] = 53.3 - df['y']\n    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n\n    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n\n    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n        lambda x: x / (x.max() + 1)\n    )\n\n    if 'nearest_opp_dist' in df.columns:\n        df['pressure'] = 1 / np.maximum(df['nearest_opp_dist'], 0.5)\n        df['under_pressure'] = (df['nearest_opp_dist'] < 3).astype(int)\n        df['pressure_x_speed'] = df['pressure'] * df['s']\n        \n    if 'mirror_wr_vx' in df.columns:\n        s_safe = np.maximum(df['s'], 0.1)\n        df['mirror_similarity'] = (\n                df['velocity_x'] * df['mirror_wr_vx'] +\n                df['velocity_y'] * df['mirror_wr_vy']\n        ) / s_safe\n        df['mirror_offset_dist'] = np.sqrt(\n            df['mirror_offset_x'] ** 2 + df['mirror_offset_y'] ** 2\n        )\n        df['mirror_alignment'] = df['mirror_similarity'] * df['is_coverage']\n\n    return df\n\ndef add_time_features(df):\n    if 'num_frames_output' not in df.columns:\n        return df\n        \n    max_frames = df['num_frames_output']\n    \n    df['max_play_duration'] = max_frames / 10.0\n    df['frame_time'] = df['frame_id'] / 10.0\n    df['progress_ratio'] = df['frame_id'] / np.maximum(max_frames, 1)\n    df['time_remaining'] = (max_frames - df['frame_id']) / 10.0\n    df['frames_remaining'] = max_frames - df['frame_id']\n    \n    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['frame_time']\n    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['frame_time']\n    \n    if 'ball_land_x' in df.columns:\n        df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n        df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n        df['error_from_ball'] = np.sqrt(\n            df['error_from_ball_x']**2 + df['error_from_ball_y']**2\n        )\n        \n        df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['frame_time'] + 0.1)\n        df['dist_scaled_by_progress'] = df['dist_to_ball'] * (1 - df['progress_ratio'])\n    \n    df['time_squared'] = df['frame_time'] ** 2\n    df['velocity_x_progress'] = df['velocity_x'] * df['progress_ratio']\n    df['velocity_y_progress'] = df['velocity_y'] * df['progress_ratio']\n    df['speed_scaled_by_time_left'] = df['s'] * df['time_remaining']\n    \n    df['actual_play_length'] = max_frames\n    df['length_ratio'] = max_frames / 30.0\n    \n    return df\n\ndef get_feature_columns(df):\n    base_feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id',\n        'ball_land_x', 'ball_land_y',\n        'player_height_feet', 'player_weight', 'height_inches', 'bmi',\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'speed_squared', 'accel_magnitude', 'orientation_diff',\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer', 'side_offense',\n        'distance_to_ball', 'dist_to_ball', 'dist_squared', 'angle_to_ball', \n        'ball_direction_x', 'ball_direction_y', 'closing_speed_ball',\n        'velocity_toward_ball', 'velocity_alignment', 'angle_diff',\n    ]\n    \n    opponent_cols = [\n        'nearest_opp_dist', 'closing_speed', 'num_nearby_opp_3', 'num_nearby_opp_5',\n        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y', 'mirror_wr_dist',\n    ]\n    \n    route_cols = [\n        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n    ]\n    \n    gnn_cols = [\n        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n        'gnn_opp_dx_mean', 'gnn_opp_dy_mean', 'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n        'gnn_ally_cnt', 'gnn_opp_cnt', 'gnn_ally_dmin', 'gnn_ally_dmean', \n        'gnn_opp_dmin', 'gnn_opp_dmean', 'gnn_d1', 'gnn_d2', 'gnn_d3',\n    ]\n    \n    temporal_cols = []\n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            temporal_cols.append(f'{col}_lag{lag}')\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            temporal_cols.append(f'{col}_rolling_mean_{window}')\n            temporal_cols.append(f'{col}_rolling_std_{window}')\n    \n    temporal_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n    temporal_cols.extend(['velocity_x_ema', 'velocity_y_ema', 'speed_ema'])\n    \n    time_cols = [\n        'max_play_duration', 'frame_time', 'progress_ratio', 'time_remaining', 'frames_remaining',\n        'expected_x_at_ball', 'expected_y_at_ball', \n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n        'time_squared', 'weighted_dist_by_time', \n        'velocity_x_progress', 'velocity_y_progress', 'dist_scaled_by_progress',\n        'speed_scaled_by_time_left', 'actual_play_length', 'length_ratio',\n    ]\n    \n    advanced_cols = [\n        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n        'dist_from_sideline', 'dist_from_endzone',\n        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n        'frames_elapsed', 'normalized_time',\n        'pressure', 'under_pressure', 'pressure_x_speed',\n        'mirror_similarity', 'mirror_offset_dist', 'mirror_alignment'\n    ]\n    \n    geometric_cols = [\n        'geo_endpoint_x', 'geo_endpoint_y',\n        'geo_vector_x', 'geo_vector_y', 'geo_distance',\n        'geo_required_vx', 'geo_required_vy',\n        'geo_velocity_error_x', 'geo_velocity_error_y', 'geo_velocity_error',\n        'geo_required_ax', 'geo_required_ay',\n        'geo_alignment', 'geo_receiver_urgency', 'geo_defender_coupling'\n    ]\n    \n    all_feature_cols = (base_feature_cols + opponent_cols + route_cols + gnn_cols + \n                       temporal_cols + time_cols + advanced_cols + geometric_cols)\n    \n    return [c for c in all_feature_cols if c in df.columns]\n\ndef wrap_angle_deg(s):\n    return ((s + 180.0) % 360.0) - 180.0\n\ndef unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n    if 'play_direction' not in df.columns:\n        return df\n    df = df.copy()\n    right = df['play_direction'].eq('right')\n    if 'x' in df.columns: df.loc[right, 'x'] = Config.FIELD_X_MAX - df.loc[right, 'x']\n    if 'y' in df.columns: df.loc[right, 'y'] = Config.FIELD_Y_MAX - df.loc[right, 'y']\n    for col in ('dir','o'):\n        if col in df.columns:\n            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n    if 'ball_land_x' in df.columns:\n        df.loc[right, 'ball_land_x'] = Config.FIELD_X_MAX - df.loc[right, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[right, 'ball_land_y'] = Config.FIELD_Y_MAX - df.loc[right, 'ball_land_y']\n    return df\n\ndef build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n    s = (\n        df_in[['game_id','play_id','play_direction']]\n        .drop_duplicates()\n        .set_index(['game_id','play_id'])['play_direction']\n    )\n    return s\n\ndef apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n    if 'play_direction' not in df.columns:\n        dir_df = dir_map.reset_index()\n        df = df.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n    return unify_left_direction(df)\n\ndef invert_to_original_direction(x_u, y_u, play_dir_right: bool):\n    if not play_dir_right:\n        return float(x_u), float(y_u)\n    return float(Config.FIELD_X_MAX - x_u), float(Config.FIELD_Y_MAX - y_u)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:43:50.637175Z","iopub.execute_input":"2025-11-08T16:43:50.637532Z","iopub.status.idle":"2025-11-08T16:43:50.719119Z","shell.execute_reply.started":"2025-11-08T16:43:50.637509Z","shell.execute_reply":"2025-11-08T16:43:50.718316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Seqences","metadata":{}},{"cell_type":"code","source":"def prepare_sequences_fixed(input_df, output_df=None, test_template=None, \n                           is_training=False, window_size=Config.WINDOW_SIZE,\n                           route_kmeans=None, route_scaler=None):\n    dir_map = build_play_direction_map(input_df)\n    input_df_u = apply_direction_to_df(input_df, dir_map)\n    \n    if is_training:\n        out_u = apply_direction_to_df(output_df, dir_map)\n        target_rows = out_u\n        target_groups = out_u[['game_id','play_id','nfl_id']].drop_duplicates()\n    else:\n        if 'play_direction' not in test_template.columns:\n            dir_df = dir_map.reset_index()\n            test_template = test_template.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n        target_rows = test_template\n        target_groups = target_rows[['game_id','play_id','nfl_id']].drop_duplicates()\n\n    input_df_u = create_base_features(input_df_u)\n    input_df_u = create_lag_features(input_df_u, window_size)\n    opponent_features = get_opponent_features(input_df_u)\n    input_df_u = input_df_u.merge(opponent_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    route_features = extract_route_patterns(input_df_u, route_kmeans, route_scaler, fit=False)\n    if not route_features.empty:\n        input_df_u = input_df_u.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    gnn_features = compute_neighbor_embeddings(input_df_u)\n    input_df_u = input_df_u.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    input_df_u = add_advanced_features(input_df_u)\n    input_df_u = add_time_features(input_df_u)\n    input_df_u = add_geometric_features(input_df_u)\n    feature_cols = get_feature_columns(input_df_u)\n\n    input_df_u.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df_u.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    \n    sequences, sequence_ids = [], []\n    geo_endpoints_x, geo_endpoints_y = [], []\n    \n    for _, row in tqdm(target_groups.iterrows(), desc=\"sequences ing\"):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n\n        if np.isnan(seq).any():\n            seq = np.nan_to_num(seq, nan=0.0)\n        \n        sequences.append(seq)\n\n        geo_x = input_window.iloc[-1]['geo_endpoint_x']\n        geo_y = input_window.iloc[-1]['geo_endpoint_y']\n        geo_endpoints_x.append(geo_x)\n        geo_endpoints_y.append(geo_y)\n        \n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id'],\n            'play_direction': input_window.iloc[-1]['play_direction'],\n            'last_x': input_window.iloc[-1]['x'],\n            'last_y': input_window.iloc[-1]['y']\n        })\n    \n    return sequences, sequence_ids, geo_endpoints_x, geo_endpoints_y, feature_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:43:53.247893Z","iopub.execute_input":"2025-11-08T16:43:53.24894Z","iopub.status.idle":"2025-11-08T16:43:53.261417Z","shell.execute_reply.started":"2025-11-08T16:43:53.24891Z","shell.execute_reply":"2025-11-08T16:43:53.260398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n\n    test_pd = test.to_pandas()\n    test_input_pd = test_input.to_pandas()\n    \n    MODEL_DIR = Path(\"/kaggle/input/1103new-all-all-all/pytorch/default/1/1103new_all_all_all\") \n    try:\n        route_kmeans = joblib.load(MODEL_DIR / \"route_kmeans.pkl\")\n        route_scaler = joblib.load(MODEL_DIR / \"route_scaler.pkl\")\n    except:\n        route_kmeans = None\n        route_scaler = None\n\n    sequences, sequence_ids, geo_endpoints_x, geo_endpoints_y, feature_cols = prepare_sequences_fixed(\n        test_input_pd, test_template=test_pd, is_training=False, \n        window_size=Config.WINDOW_SIZE, route_kmeans=route_kmeans, route_scaler=route_scaler\n    )\n    \n    if not sequences:\n        return pl.DataFrame({\"x\": [], \"y\": []})\n    \n    X_test = np.array(sequences, dtype=object)\n\n    x_last_u = np.array([s[-1, 0] for s in X_test]) \n    y_last_u = np.array([s[-1, 1] for s in X_test]) \n\n    model_x_paths = [MODEL_DIR / f\"model_x_fold{i+1}.pth\" for i in range(Config.N_FOLDS)]\n    model_y_paths = [MODEL_DIR / f\"model_y_fold{i+1}.pth\" for i in range(Config.N_FOLDS)]\n    scaler_paths = [MODEL_DIR / f\"scaler_fold{i+1}.pkl\" for i in range(Config.N_FOLDS)]\n\n    for p in model_x_paths + model_y_paths + scaler_paths:\n        if not p.exists():\n            raise FileNotFoundError(f\"loss: {p.name}\")\n\n    models_x = []\n    models_y = []\n    scalers = []\n    \n    for i in range(Config.N_FOLDS):\n\n        model_x = ImprovedSeqModel(input_dim=X_test[0].shape[1], horizon=Config.MAX_FUTURE_HORIZON).to(Config.DEVICE)\n        model_x.load_state_dict(torch.load(model_x_paths[i], map_location=Config.DEVICE))\n        model_x.eval()\n        models_x.append(model_x)\n\n        model_y = ImprovedSeqModel(input_dim=X_test[0].shape[1], horizon=Config.MAX_FUTURE_HORIZON).to(Config.DEVICE)\n        model_y.load_state_dict(torch.load(model_y_paths[i], map_location=Config.DEVICE))\n        model_y.eval()\n        models_y.append(model_y)\n\n        scaler = joblib.load(scaler_paths[i])\n        scalers.append(scaler)\n\n    all_dx, all_dy = [], []\n    for mx, my, sc in zip(models_x, models_y, scalers):\n        X_scaled = np.stack([sc.transform(s) for s in X_test])\n        X_tensor = torch.tensor(X_scaled.astype(np.float32)).to(Config.DEVICE)\n        \n        mx.eval()\n        my.eval()\n        with torch.no_grad():\n            dx = mx(X_tensor).cpu().numpy()\n            dy = my(X_tensor).cpu().numpy()\n        all_dx.append(dx)\n        all_dy.append(dy)\n    \n    ens_dx = np.mean(all_dx, axis=0)\n    ens_dy = np.mean(all_dy, axis=0)\n\n    rows = []\n    H = ens_dx.shape[1]\n    \n    for i, sid in enumerate(sequence_ids):\n        fids = test_pd[\n            (test_pd['game_id'] == sid['game_id']) &\n            (test_pd['play_id'] == sid['play_id']) &\n            (test_pd['nfl_id'] == sid['nfl_id'])\n        ]['frame_id'].sort_values().tolist()\n        \n        play_dir_right = (sid['play_direction'] == 'right')\n        \n        for t, fid in enumerate(fids):\n            tt = min(t, H - 1)\n\n            x_u = np.clip(x_last_u[i] + ens_dx[i, tt], 0, 120)\n            y_u = np.clip(y_last_u[i] + ens_dy[i, tt], 0, 53.3)\n            \n            x_orig, y_orig = invert_to_original_direction(x_u, y_u, play_dir_right)\n            \n            rows.append({\n                'x': float(x_orig),\n                'y': float(y_orig)\n            })\n    \n    return pl.DataFrame(rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:43:54.699881Z","iopub.execute_input":"2025-11-08T16:43:54.700716Z","iopub.status.idle":"2025-11-08T16:43:54.715077Z","shell.execute_reply.started":"2025-11-08T16:43:54.700687Z","shell.execute_reply":"2025-11-08T16:43:54.714093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Infer","metadata":{}},{"cell_type":"code","source":"try:\n    from kaggle_evaluation.nfl_inference_server import NFLInferenceServer\n    inference_server = NFLInferenceServer(predict)\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        inference_server.serve()\n    else:\n        inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))\nexcept ImportError as e:\n    if __name__ == \"__main__\":\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T16:43:56.45902Z","iopub.execute_input":"2025-11-08T16:43:56.459344Z","iopub.status.idle":"2025-11-08T16:46:47.786063Z","shell.execute_reply.started":"2025-11-08T16:43:56.459321Z","shell.execute_reply":"2025-11-08T16:46:47.785319Z"}},"outputs":[],"execution_count":null}]}