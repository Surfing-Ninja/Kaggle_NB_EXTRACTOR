{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T14:57:15.279534Z","iopub.execute_input":"2025-09-30T14:57:15.279712Z","iopub.status.idle":"2025-09-30T14:57:15.597377Z","shell.execute_reply.started":"2025-09-30T14:57:15.279696Z","shell.execute_reply":"2025-09-30T14:57:15.59565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom multiprocessing import Pool, cpu_count\nfrom tqdm.auto import tqdm\nimport pickle\n\nbasedir = '/kaggle/input/nfl-big-data-bowl-2026-prediction'\ndef load_weekly_data(week_num):\n    input_df = pd.read_csv(f'{basedir}/train/input_2023_w{week_num:02d}.csv')\n    output_df = pd.read_csv(f'{basedir}/train/output_2023_w{week_num:02d}.csv')\n    return input_df, output_df\n\ndef load_all_train_data():\n    print(\"Loading training data...\")\n    with Pool(min(cpu_count(), 18)) as pool:\n        results = list(tqdm(pool.imap(load_weekly_data, range(1, 19)), total=18))\n    \n    input_dfs = [r[0] for r in results]\n    output_dfs = [r[1] for r in results]\n    \n    input_data = pd.concat(input_dfs, ignore_index=True)\n    output_data = pd.concat(output_dfs, ignore_index=True)\n    \n    print(f\"Input data shape: {input_data.shape}\")\n    print(f\"Output data shape: {output_data.shape}\")\n    \n    return input_data, output_data\n\ndef engineer_advanced_features(df):\n    \"\"\"Advanced feature engineering with sequence and interaction features\"\"\"\n    df = df.copy()\n    \n    df['velocity_x'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['velocity_y'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    df['dist_to_ball'] = np.sqrt(\n        (df['x'] - df['ball_land_x'])**2 + \n        (df['y'] - df['ball_land_y'])**2\n    )\n    \n    df['angle_to_ball'] = np.arctan2(\n        df['ball_land_y'] - df['y'],\n        df['ball_land_x'] - df['x']\n    )\n    \n    df['velocity_toward_ball'] = (\n        df['velocity_x'] * np.cos(df['angle_to_ball']) + \n        df['velocity_y'] * np.sin(df['angle_to_ball'])\n    )\n    \n    df['time_to_ball'] = df['num_frames_output'] / 10.0\n    \n    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n    \n    df['role_targeted_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    df['role_defensive_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['role_passer'] = (df['player_role'] == 'Passer').astype(int)\n    df['side_offense'] = (df['player_side'] == 'Offense').astype(int)\n    \n    height_parts = df['player_height'].str.split('-', expand=True)\n    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n    \n    df['acceleration_x'] = df['a'] * np.cos(np.radians(df['dir']))\n    df['acceleration_y'] = df['a'] * np.sin(np.radians(df['dir']))\n    \n    df['distance_to_target_x'] = df['ball_land_x'] - df['x']\n    df['distance_to_target_y'] = df['ball_land_y'] - df['y']\n    \n    df['speed_squared'] = df['s'] ** 2\n    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n    \n    df['velocity_alignment'] = np.cos(df['angle_to_ball'] - np.radians(df['dir']))\n    \n    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['time_to_ball']\n    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['time_to_ball']\n    \n    df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n    df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n    df['error_from_ball'] = np.sqrt(df['error_from_ball_x']**2 + df['error_from_ball_y']**2)\n    \n    df['momentum_x'] = df['player_weight'] * df['velocity_x']\n    df['momentum_y'] = df['player_weight'] * df['velocity_y']\n    \n    df['kinetic_energy'] = 0.5 * df['player_weight'] * df['speed_squared']\n    \n    df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n    df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n    \n    df['time_squared'] = df['time_to_ball'] ** 2\n    df['dist_squared'] = df['dist_to_ball'] ** 2\n    \n    df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['time_to_ball'] + 0.1)\n    \n    return df\n\ndef add_sequence_features(df):\n    \"\"\"Add temporal lag and rolling features\"\"\"\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in df.columns:\n                df[f'{col}_rolling_mean_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n                df[f'{col}_rolling_std_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n    \n    for col in ['velocity_x', 'velocity_y']:\n        if col in df.columns:\n            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff()\n    \n    return df\n\ndef create_training_dataset(input_df, output_df):\n    output_df = output_df.copy()\n    output_df['id'] = (output_df['game_id'].astype(str) + '_' + \n                       output_df['play_id'].astype(str) + '_' + \n                       output_df['nfl_id'].astype(str) + '_' + \n                       output_df['frame_id'].astype(str))\n    \n    output_df = output_df.rename(columns={'x': 'target_x', 'y': 'target_y'})\n    \n    input_agg = input_df.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    \n    if 'frame_id' in input_agg.columns:\n        input_agg = input_agg.drop('frame_id', axis=1)\n    \n    merged = output_df.merge(\n        input_agg,\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left',\n        suffixes=('', '_input')\n    )\n    \n    return merged\n\ndef physics_baseline_prediction(x, y, velocity_x, velocity_y, frame_id):\n    time_delta = frame_id / 10.0\n    pred_x = x + velocity_x * time_delta\n    pred_y = y + velocity_y * time_delta\n    pred_x = np.clip(pred_x, 0, 120)\n    pred_y = np.clip(pred_y, 0, 53.3)\n    return pred_x, pred_y\n\ndef main():\n    print(f\"CPU cores: {cpu_count()}\")\n    \n    input_data, output_data = load_all_train_data()\n    \n    print(\"\\n=== Advanced Feature Engineering ===\")\n    print(\"Step 1: Engineering advanced physics features...\")\n    input_features = engineer_advanced_features(input_data)\n    \n    print(\"Step 2: Adding sequence and rolling features...\")\n    input_features = add_sequence_features(input_features)\n    \n    print(f\"Feature engineered data shape: {input_features.shape}\")\n    print(f\"Total features: {input_features.shape[1]}\")\n    \n    print(\"\\nStep 3: Creating training dataset...\")\n    train_df = create_training_dataset(input_features, output_data)\n    print(f\"Training dataset shape: {train_df.shape}\")\n    \n    feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir',\n        'velocity_x', 'velocity_y', 'dist_to_ball', 'angle_to_ball',\n        'velocity_toward_ball', 'time_to_ball', 'orientation_diff',\n        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer',\n        'side_offense', 'height_inches', 'player_weight', 'bmi',\n        'ball_land_x', 'ball_land_y', 'num_frames_output', 'frame_id',\n        'acceleration_x', 'acceleration_y', 'distance_to_target_x', 'distance_to_target_y',\n        'speed_squared', 'accel_magnitude', 'velocity_alignment',\n        'expected_x_at_ball', 'expected_y_at_ball',\n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'angle_diff', 'time_squared', 'dist_squared', 'weighted_dist_by_time'\n    ]\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            feature_cols.append(f'{col}_lag{lag}')\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            feature_cols.append(f'{col}_rolling_mean_{window}')\n            feature_cols.append(f'{col}_rolling_std_{window}')\n    \n    feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n    \n    available_features = [col for col in feature_cols if col in train_df.columns]\n    print(f\"Available features: {len(available_features)}\")\n    \n    train_df = train_df.dropna(subset=available_features + ['target_x', 'target_y'])\n    print(f\"Training data after removing NaNs: {train_df.shape}\")\n    \n    print(\"\\n=== Physics Baseline ===\")\n    baseline_x, baseline_y = physics_baseline_prediction(\n        train_df['x'].values,\n        train_df['y'].values,\n        train_df['velocity_x'].values,\n        train_df['velocity_y'].values,\n        train_df['frame_id'].values\n    )\n    \n    baseline_rmse = np.sqrt(\n        0.5 * (mean_squared_error(train_df['target_x'], baseline_x) +\n               mean_squared_error(train_df['target_y'], baseline_y))\n    )\n    print(f\"Physics Baseline RMSE: {baseline_rmse:.4f}\")\n    \n    from lightgbm import LGBMRegressor\n    import lightgbm as lgb\n    \n    X = train_df[available_features].values\n    y_x = train_df['target_x'].values\n    y_y = train_df['target_y'].values\n    \n    split_idx = int(len(train_df) * 0.90)\n    X_train, X_val = X[:split_idx], X[split_idx:]\n    y_x_train, y_x_val = y_x[:split_idx], y_x[split_idx:]\n    y_y_train, y_y_val = y_y[:split_idx], y_y[split_idx:]\n    \n    print(f\"\\nTraining set: {X_train.shape}, Validation set: {X_val.shape}\")\n    \n    print(\"\\n=== Training Ultra-Optimized LightGBM ===\")\n    print(\"Training X coordinate model...\")\n    model_x = LGBMRegressor(\n        n_estimators=2000,\n        learning_rate=0.02,\n        max_depth=15,\n        num_leaves=200,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        min_child_samples=50,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n        verbose=-1\n    )\n    \n    model_x.fit(\n        X_train, y_x_train,\n        eval_set=[(X_val, y_x_val)],\n        eval_metric='rmse',\n        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n    )\n    \n    print(\"\\nTraining Y coordinate model...\")\n    model_y = LGBMRegressor(\n        n_estimators=2000,\n        learning_rate=0.02,\n        max_depth=15,\n        num_leaves=200,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        min_child_samples=50,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n        verbose=-1\n    )\n    \n    model_y.fit(\n        X_train, y_y_train,\n        eval_set=[(X_val, y_y_val)],\n        eval_metric='rmse',\n        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n    )\n    \n    pred_x = model_x.predict(X_val)\n    pred_y = model_y.predict(X_val)\n    \n    pred_x = np.clip(pred_x, 0, 120)\n    pred_y = np.clip(pred_y, 0, 53.3)\n    \n    lgbm_rmse = np.sqrt(\n        0.5 * (mean_squared_error(y_x_val, pred_x) +\n               mean_squared_error(y_y_val, pred_y))\n    )\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ULTRA-OPTIMIZED MODEL PERFORMANCE\")\n    print(f\"{'='*60}\")\n    print(f\"Physics Baseline RMSE:     {baseline_rmse:.4f}\")\n    print(f\"Ultra-Optimized LightGBM:  {lgbm_rmse:.4f}\")\n    print(f\"Improvement:               {((baseline_rmse - lgbm_rmse) / baseline_rmse * 100):.2f}%\")\n    print(f\"Target RMSE:               0.9000\")\n    target_met = 'YES - TARGET ACHIEVED!' if lgbm_rmse < 0.9 else 'NO - Continuing optimization...'\n    print(f\"Target Met:                {target_met}\")\n    print(f\"{'='*60}\")\n    \n    feature_importance = pd.DataFrame({\n        'feature': available_features,\n        'importance_x': model_x.feature_importances_,\n        'importance_y': model_y.feature_importances_\n    })\n    feature_importance['importance_avg'] = (feature_importance['importance_x'] + \n                                            feature_importance['importance_y']) / 2\n    feature_importance = feature_importance.sort_values('importance_avg', ascending=False)\n    \n    print(\"\\nTop 30 Most Important Features:\")\n    print(feature_importance.head(30).to_string())\n    \n    with open('ultra_models.pkl', 'wb') as f:\n        pickle.dump({\n            'model_x': model_x, \n            'model_y': model_y, \n            'features': available_features,\n            'rmse': lgbm_rmse\n        }, f)\n    print(\"\\nModels saved to ultra_models.pkl\")\n    \n    print(\"\\n=== Generating Submission ===\")\n    test_input = pd.read_csv(f'{basedir}/test_input.csv')\n    test_data = pd.read_csv(f'{basedir}/test.csv')\n    \n    print(\"Engineering features for test data...\")\n    test_features = engineer_advanced_features(test_input)\n    test_features = add_sequence_features(test_features)\n    \n    test_agg = test_features.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    \n    if 'frame_id' in test_agg.columns:\n        test_agg = test_agg.drop('frame_id', axis=1)\n    \n    test_merged = test_data.merge(\n        test_agg,\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left'\n    )\n    \n    test_merged['id'] = (test_merged['game_id'].astype(str) + '_' + \n                         test_merged['play_id'].astype(str) + '_' + \n                         test_merged['nfl_id'].astype(str) + '_' + \n                         test_merged['frame_id'].astype(str))\n    \n    for col in available_features:\n        if col not in test_merged.columns:\n            test_merged[col] = 0\n    \n    X_test = test_merged[available_features].fillna(0).values\n    \n    pred_x_test = model_x.predict(X_test)\n    pred_y_test = model_y.predict(X_test)\n    \n    pred_x_test = np.clip(pred_x_test, 0, 120)\n    pred_y_test = np.clip(pred_y_test, 0, 53.3)\n    \n    submission = pd.DataFrame({\n        'id': test_merged['id'],\n        'x': pred_x_test,\n        'y': pred_y_test\n    })\n    \n    submission.to_csv('submission.csv', index=False)\n    print(f\"\\n[SUCCESS] Submission saved: submission_ultra.csv\")\n    print(f\"Shape: {submission.shape}\")\n    \n    print(\"\\n=== Submission Validation ===\")\n    print(f\"No NaN values: {submission.isnull().sum().sum() == 0}\")\n    print(f\"X range: [{submission['x'].min():.2f}, {submission['x'].max():.2f}]\")\n    print(f\"Y range: [{submission['y'].min():.2f}, {submission['y'].max():.2f}]\")\n    print(f\"Unique IDs: {submission['id'].nunique()}\")\n    \n    return lgbm_rmse\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T14:57:15.598832Z","iopub.execute_input":"2025-09-30T14:57:15.599179Z","iopub.status.idle":"2025-09-30T14:57:16.823193Z","shell.execute_reply.started":"2025-09-30T14:57:15.599158Z","shell.execute_reply":"2025-09-30T14:57:16.822447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport lightgbm as lgb\nfinal_rmse = main()\nprint(f\"\\n[FINAL] Validation RMSE: {final_rmse:.4f}\")\nachievement = 'ACHIEVED!' if final_rmse < 0.9 else 'Not yet - need further optimization'\nprint(f\"[FINAL] Target RMSE < 0.9: {achievement}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T14:43:05.513939Z","iopub.execute_input":"2025-09-30T14:43:05.514251Z","iopub.status.idle":"2025-09-30T14:53:05.335805Z","shell.execute_reply.started":"2025-09-30T14:43:05.514229Z","shell.execute_reply":"2025-09-30T14:53:05.335145Z"}},"outputs":[],"execution_count":null}]}