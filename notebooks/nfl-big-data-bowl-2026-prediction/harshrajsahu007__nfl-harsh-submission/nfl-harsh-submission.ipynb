{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-28T17:31:31.357945Z","iopub.execute_input":"2025-09-28T17:31:31.358247Z","iopub.status.idle":"2025-09-28T17:31:33.885995Z","shell.execute_reply.started":"2025-09-28T17:31:31.358187Z","shell.execute_reply":"2025-09-28T17:31:33.885419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\nfrom sklearn.model_selection import GroupKFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import Ridge, ElasticNet\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass NFLPlayerMovementPredictor:\n    \"\"\"Advanced NFL Player Movement Prediction with comprehensive feature engineering and ensemble methods\"\"\"\n    \n    def __init__(self, data_dir, seed=42, optimize_hyperparams=False):\n        self.data_dir = Path(data_dir)\n        self.seed = seed\n        self.optimize_hyperparams = optimize_hyperparams\n        self.weeks = list(range(1, 18))\n        self.models_dx = {}\n        self.models_dy = {}\n        self.scalers = {}\n        self.label_encoders = {}\n        self.feature_importance = {}\n        \n        # Set random seeds for reproducibility\n        np.random.seed(seed)\n        \n    def load_and_combine_datasets(self):\n        \"\"\"Load and combine weekly training data with enhanced error handling\"\"\"\n        try:\n            input_paths = [self.data_dir / f\"train/input_2023_w{w:02d}.csv\" for w in self.weeks]\n            output_paths = [self.data_dir / f\"train/output_2023_w{w:02d}.csv\" for w in self.weeks]\n            \n            # Load only existing files\n            existing_input_paths = [p for p in input_paths if p.exists()]\n            existing_output_paths = [p for p in output_paths if p.exists()]\n            \n            if not existing_input_paths or not existing_output_paths:\n                raise FileNotFoundError(\"No training files found\")\n            \n            print(f\"Loading {len(existing_input_paths)} input files and {len(existing_output_paths)} output files\")\n            \n            train_input = self._load_multiple_csv_files(existing_input_paths)\n            train_output = self._load_multiple_csv_files(existing_output_paths)\n            \n            # Load test data\n            test_input = pd.read_csv(self.data_dir / \"test_input.csv\")\n            test_template = pd.read_csv(self.data_dir / \"test.csv\")\n            \n            print(f\"Training input shape: {train_input.shape}\")\n            print(f\"Training output shape: {train_output.shape}\")\n            print(f\"Test input shape: {test_input.shape}\")\n            print(f\"Test template shape: {test_template.shape}\")\n            \n            return train_input, train_output, test_input, test_template\n            \n        except Exception as e:\n            print(f\"Error loading datasets: {e}\")\n            raise\n    \n    def _load_multiple_csv_files(self, file_paths):\n        \"\"\"Load and concatenate multiple CSV files with memory optimization\"\"\"\n        data_frames = []\n        for path in file_paths:\n            try:\n                df = pd.read_csv(path, low_memory=False)\n                data_frames.append(df)\n            except Exception as e:\n                print(f\"Warning: Could not load {path}: {e}\")\n                continue\n        \n        if not data_frames:\n            raise ValueError(\"No valid CSV files could be loaded\")\n            \n        return pd.concat(data_frames, ignore_index=True)\n    \n    def _convert_height_to_inches(self, height_str):\n        \"\"\"Convert height from 'ft-in' format to total inches with robust parsing\"\"\"\n        if pd.isna(height_str) or not isinstance(height_str, str):\n            return np.nan\n        \n        try:\n            if '-' in height_str:\n                feet, inches = map(int, height_str.split('-'))\n                return feet * 12 + inches\n            elif \"'\" in height_str:\n                # Handle format like 6'2\"\n                height_str = height_str.replace('\"', '').replace(\"'\", '-')\n                feet, inches = map(int, height_str.split('-'))\n                return feet * 12 + inches\n            else:\n                # Assume it's already in inches\n                return float(height_str)\n        except (ValueError, AttributeError):\n            return np.nan\n    \n    def _extract_final_pre_throw_observation(self, tracking_data):\n        \"\"\"Extract the last tracking frame before pass for each player with enhanced preprocessing\"\"\"\n        print(\"Available columns in tracking data:\", tracking_data.columns.tolist())\n        \n        # Ensure we have the required grouping columns\n        required_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id']\n        missing_cols = [col for col in required_cols if col not in tracking_data.columns]\n        if missing_cols:\n            raise KeyError(f\"Missing required columns: {missing_cols}\")\n        \n        # Sort and get final observation\n        sorted_data = tracking_data.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n        final_observation = sorted_data.groupby(['game_id', 'play_id', 'nfl_id'], as_index=False).last()\n        \n        # Rename position columns for clarity\n        column_mapping = {\n            'x': 'final_pre_throw_x',\n            'y': 'final_pre_throw_y',\n            'player_height': 'player_height_raw'\n        }\n        \n        for old_col, new_col in column_mapping.items():\n            if old_col in final_observation.columns:\n                final_observation = final_observation.rename(columns={old_col: new_col})\n        \n        # Convert height to inches if available\n        height_columns = ['player_height_raw', 'player_height', 'height']\n        for col in height_columns:\n            if col in final_observation.columns:\n                final_observation['height_inches'] = final_observation[col].apply(\n                    self._convert_height_to_inches\n                )\n                break\n        \n        return final_observation\n    \n    def _incorporate_target_receiver_data(self, player_data):\n        \"\"\"Add target receiver position data to all players in the same play\"\"\"\n        if 'player_role' not in player_data.columns:\n            print(\"Warning: 'player_role' column not found. Creating placeholder target receiver data.\")\n            player_data['target_receiver_x'] = np.nan\n            player_data['target_receiver_y'] = np.nan\n            return player_data\n        \n        # Get target receiver positions\n        target_receivers = player_data[player_data['player_role'] == \"Targeted Receiver\"][\n            ['game_id', 'play_id', 'final_pre_throw_x', 'final_pre_throw_y']\n        ].rename(columns={\n            'final_pre_throw_x': 'target_receiver_x', \n            'final_pre_throw_y': 'target_receiver_y'\n        })\n        \n        # Handle multiple target receivers per play (take first one)\n        target_receivers = target_receivers.drop_duplicates(['game_id', 'play_id'], keep='first')\n        \n        return player_data.merge(target_receivers, on=['game_id', 'play_id'], how='left')\n    \n    def _calculate_advanced_features(self, data_frame, training_mode=False):\n        \"\"\"Create comprehensive feature set with advanced football analytics and physics-based features\"\"\"\n        df = data_frame.copy()\n        available_columns = df.columns.tolist()\n        \n        print(f\"Calculating advanced features for {len(df)} records...\")\n        \n        # Basic position and time features\n        if 'frame_id' in df.columns:\n            df['time_seconds'] = df['frame_id'] / 10.0  # 10 FPS\n            \n            # Calculate relative frame position within each play\n            play_stats = df.groupby(['game_id', 'play_id'])['frame_id'].agg(['min', 'max', 'count'])\n            df = df.merge(play_stats, left_on=['game_id', 'play_id'], right_index=True, suffixes=('', '_play'))\n            \n            df['normalized_frame'] = (df['frame_id'] - df['min']) / (df['max'] - df['min'] + 1e-8)\n            df['frames_remaining'] = df['max'] - df['frame_id']\n            df['play_duration'] = df['count'] / 10.0  # in seconds\n        \n        # Ball trajectory and physics features\n        if all(col in df.columns for col in ['ball_land_x', 'ball_land_y', 'final_pre_throw_x', 'final_pre_throw_y']):\n            ball_dx = df['ball_land_x'] - df['final_pre_throw_x']\n            ball_dy = df['ball_land_y'] - df['final_pre_throw_y']\n            df['distance_to_ball_landing'] = np.sqrt(ball_dx**2 + ball_dy**2)\n            df['angle_to_ball_landing'] = np.arctan2(ball_dy, ball_dx)\n            \n            # Ball trajectory features\n            df['ball_trajectory_angle'] = np.rad2deg(df['angle_to_ball_landing'])\n            df['ball_horizontal_distance'] = np.abs(ball_dx)\n            df['ball_vertical_distance'] = np.abs(ball_dy)\n        \n        # Target receiver analysis\n        if all(col in df.columns for col in ['target_receiver_x', 'target_receiver_y', 'final_pre_throw_x', 'final_pre_throw_y']):\n            target_dx = df['target_receiver_x'] - df['final_pre_throw_x']\n            target_dy = df['target_receiver_y'] - df['final_pre_throw_y']\n            df['distance_to_target'] = np.sqrt(target_dx**2 + target_dy**2)\n            df['angle_to_target'] = np.arctan2(target_dy, target_dx)\n            \n            # Alignment with target receiver\n            if 'angle_to_ball_landing' in df.columns:\n                df['target_ball_alignment'] = np.abs(df['angle_to_target'] - df['angle_to_ball_landing'])\n        \n        # Player role indicators\n        if 'player_role' in df.columns:\n            df['is_target_receiver'] = (df['player_role'] == \"Targeted Receiver\").astype(int)\n            df['is_quarterback'] = (df['player_role'] == \"Quarterback\").astype(int)\n            df['is_receiver'] = df['player_role'].str.contains('Receiver|receiver', na=False).astype(int)\n            df['is_defender'] = df['player_role'].str.contains('Defense|defender', na=False).astype(int)\n        else:\n            for col in ['is_target_receiver', 'is_quarterback', 'is_receiver', 'is_defender']:\n                df[col] = 0\n        \n        # Advanced velocity and acceleration features\n        if all(col in df.columns for col in ['s', 'dir']):\n            direction_radians = np.deg2rad(df['dir'])\n            df['velocity_x'] = df['s'] * np.sin(direction_radians)\n            df['velocity_y'] = df['s'] * np.cos(direction_radians)\n            df['speed_squared'] = df['s'] ** 2\n            \n            # Velocity towards target and ball\n            if 'distance_to_target' in df.columns and 'angle_to_target' in df.columns:\n                df['velocity_towards_target'] = df['s'] * np.cos(direction_radians - df['angle_to_target'])\n            \n            if 'distance_to_ball_landing' in df.columns and 'angle_to_ball_landing' in df.columns:\n                df['velocity_towards_ball'] = df['s'] * np.cos(direction_radians - df['angle_to_ball_landing'])\n        \n        # Enhanced acceleration features\n        if 'a' in df.columns:\n            df['acceleration_magnitude'] = np.abs(df['a'])\n            df['acceleration_squared'] = df['a'] ** 2\n            \n            # Combine with speed for momentum-like features\n            if 's' in df.columns:\n                df['momentum_indicator'] = df['s'] * df['acceleration_magnitude']\n        \n        # Orientation and body positioning\n        if 'o' in df.columns:\n            df['orientation_radians'] = np.deg2rad(df['o'])\n            \n            # Body orientation relative to movement and targets\n            if 'dir' in df.columns:\n                df['body_movement_alignment'] = np.abs(np.deg2rad(df['dir']) - df['orientation_radians'])\n                df['body_movement_alignment'] = np.minimum(df['body_movement_alignment'], \n                                                         2*np.pi - df['body_movement_alignment'])\n            \n            if 'angle_to_target' in df.columns:\n                df['body_target_alignment'] = np.abs(df['orientation_radians'] - df['angle_to_target'])\n        \n        # Field position and spatial analysis\n        if 'final_pre_throw_x' in df.columns:\n            df['normalized_x'] = df['final_pre_throw_x'] / 120.0\n            df['distance_from_sideline_x'] = np.minimum(df['final_pre_throw_x'], 120.0 - df['final_pre_throw_x'])\n            df['field_third_x'] = pd.cut(df['final_pre_throw_x'], bins=3, labels=[0, 1, 2]).astype(float)\n        \n        if 'final_pre_throw_y' in df.columns:\n            df['normalized_y'] = df['final_pre_throw_y'] / 53.3\n            df['distance_from_sideline_y'] = np.minimum(df['final_pre_throw_y'], 53.3 - df['final_pre_throw_y'])\n            df['field_hash_position'] = np.abs(df['final_pre_throw_y'] - 26.65) / 26.65  # Normalized distance from center\n        \n        # Game context and situational features\n        if 'absolute_yardline_number' in df.columns:\n            df['yards_to_endzone'] = df['absolute_yardline_number']\n            df['red_zone_indicator'] = (df['yards_to_endzone'] <= 20).astype(int)\n            df['goal_line_indicator'] = (df['yards_to_endzone'] <= 5).astype(int)\n        \n        # Team and formation features\n        if 'player_side' in df.columns:\n            df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n            df['is_defense'] = (df['player_side'] == 'Defense').astype(int)\n        else:\n            df['is_offense'] = 0\n            df['is_defense'] = 0\n        \n        # Physical attributes and derived metrics\n        physical_features = self._calculate_physical_features(df)\n        df = pd.concat([df, physical_features], axis=1)\n        \n        # Time-based and motion consistency features\n        motion_features = self._calculate_motion_consistency_features(df)\n        df = pd.concat([df, motion_features], axis=1)\n        \n        # Interaction and ratio features\n        interaction_features = self._calculate_interaction_features(df)\n        df = pd.concat([df, interaction_features], axis=1)\n        \n        # Training targets (for training mode only)\n        if training_mode and all(col in df.columns for col in ['x', 'final_pre_throw_x', 'y', 'final_pre_throw_y']):\n            df['displacement_x'] = df['x'] - df['final_pre_throw_x']\n            df['displacement_y'] = df['y'] - df['final_pre_throw_y']\n            df['total_displacement'] = np.sqrt(df['displacement_x']**2 + df['displacement_y']**2)\n            df['displacement_angle'] = np.arctan2(df['displacement_y'], df['displacement_x'])\n        \n        print(f\"Feature engineering completed. Final shape: {df.shape}\")\n        return df\n    \n    def _calculate_physical_features(self, df):\n        \"\"\"Calculate physical attribute features\"\"\"\n        physical_df = pd.DataFrame(index=df.index)\n        \n        # BMI and physical ratios\n        if all(col in df.columns for col in ['player_weight', 'height_inches']):\n            valid_height = df['height_inches'] > 0\n            physical_df['bmi'] = np.nan\n            physical_df.loc[valid_height, 'bmi'] = (df.loc[valid_height, 'player_weight'] * 0.453592) / (\n                (df.loc[valid_height, 'height_inches'] * 0.0254) ** 2\n            )\n            \n            # Weight-to-height ratio\n            physical_df.loc[valid_height, 'weight_height_ratio'] = (\n                df.loc[valid_height, 'player_weight'] / df.loc[valid_height, 'height_inches']\n            )\n        \n        # Age-based features (if available)\n        if 'player_age' in df.columns:\n            physical_df['age_category'] = pd.cut(df['player_age'], \n                                               bins=[0, 23, 27, 30, 50], \n                                               labels=[0, 1, 2, 3]).astype(float)\n        \n        return physical_df.fillna(0)\n    \n    def _calculate_motion_consistency_features(self, df):\n        \"\"\"Calculate motion consistency and temporal features\"\"\"\n        motion_df = pd.DataFrame(index=df.index)\n        \n        # Group-based statistics for motion consistency\n        if all(col in df.columns for col in ['game_id', 'play_id', 'nfl_id']):\n            group_cols = ['game_id', 'play_id', 'nfl_id']\n            \n            for feature in ['s', 'a', 'dir', 'o']:\n                if feature in df.columns:\n                    # Statistical measures within each player's play\n                    motion_df[f'{feature}_std'] = df.groupby(group_cols)[feature].transform('std')\n                    motion_df[f'{feature}_mean'] = df.groupby(group_cols)[feature].transform('mean')\n                    motion_df[f'{feature}_min'] = df.groupby(group_cols)[feature].transform('min')\n                    motion_df[f'{feature}_max'] = df.groupby(group_cols)[feature].transform('max')\n                    \n                    # Range and coefficient of variation\n                    motion_df[f'{feature}_range'] = motion_df[f'{feature}_max'] - motion_df[f'{feature}_min']\n                    motion_df[f'{feature}_cv'] = motion_df[f'{feature}_std'] / (motion_df[f'{feature}_mean'] + 1e-8)\n        \n        return motion_df.fillna(0)\n    \n    def _calculate_interaction_features(self, df):\n        \"\"\"Calculate interaction and ratio features\"\"\"\n        interaction_df = pd.DataFrame(index=df.index)\n        \n        # Speed and distance interactions\n        if all(col in df.columns for col in ['s', 'distance_to_ball_landing']):\n            interaction_df['speed_distance_ratio'] = df['s'] / (df['distance_to_ball_landing'] + 1.0)\n            interaction_df['time_to_ball_estimate'] = df['distance_to_ball_landing'] / (df['s'] + 0.1)\n        \n        # Acceleration and speed combinations\n        if all(col in df.columns for col in ['s', 'a']):\n            interaction_df['speed_acceleration_product'] = df['s'] * np.abs(df['a'])\n            interaction_df['kinetic_energy_estimate'] = 0.5 * df['s'] ** 2  # Assuming unit mass\n        \n        # Position and velocity combinations\n        if all(col in df.columns for col in ['normalized_x', 'normalized_y', 'velocity_x', 'velocity_y']):\n            interaction_df['position_velocity_x'] = df['normalized_x'] * df['velocity_x']\n            interaction_df['position_velocity_y'] = df['normalized_y'] * df['velocity_y']\n        \n        # Role-based feature interactions\n        if 'is_target_receiver' in df.columns:\n            for feature in ['s', 'distance_to_ball_landing', 'normalized_x']:\n                if feature in df.columns:\n                    interaction_df[f'target_receiver_{feature}'] = df['is_target_receiver'] * df[feature]\n        \n        return interaction_df.fillna(0)\n    \n    def _encode_categorical_features(self, data_frame, categorical_columns):\n        \"\"\"Encode categorical variables with robust handling\"\"\"\n        encoded_df = data_frame.copy()\n        \n        for col in categorical_columns:\n            if col in encoded_df.columns:\n                # Handle missing values first\n                encoded_df[col] = encoded_df[col].fillna('Unknown')\n                \n                if col not in self.label_encoders:\n                    # Fit new encoder\n                    self.label_encoders[col] = LabelEncoder()\n                    encoded_df[col] = self.label_encoders[col].fit_transform(encoded_df[col].astype(str))\n                else:\n                    # Transform using existing encoder\n                    # Handle unseen categories\n                    unique_vals = set(encoded_df[col].astype(str).unique())\n                    trained_vals = set(self.label_encoders[col].classes_)\n                    \n                    if not unique_vals.issubset(trained_vals):\n                        # Map unseen categories to 'Unknown'\n                        encoded_df[col] = encoded_df[col].astype(str).apply(\n                            lambda x: x if x in trained_vals else 'Unknown'\n                        )\n                    \n                    encoded_df[col] = self.label_encoders[col].transform(encoded_df[col].astype(str))\n            else:\n                print(f\"Warning: Categorical column '{col}' not found in data. Adding as constant.\")\n                encoded_df[col] = 0\n        \n        return encoded_df\n    \n    def prepare_features(self, input_data, output_data, training_mode=False):\n        \"\"\"Complete feature engineering pipeline with enhanced error handling\"\"\"\n        try:\n            print(\"Extracting final pre-throw observations...\")\n            final_observations = self._extract_final_pre_throw_observation(input_data)\n            \n            print(\"Incorporating target receiver data...\")\n            final_observations = self._incorporate_target_receiver_data(final_observations)\n            \n            # Determine merge columns based on available data\n            base_merge_cols = ['game_id', 'play_id', 'nfl_id']\n            optional_merge_cols = [\n                'final_pre_throw_x', 'final_pre_throw_y', 's', 'a', 'o', 'dir',\n                'player_role', 'player_side', 'num_frames_output', 'ball_land_x', \n                'ball_land_y', 'target_receiver_x', 'target_receiver_y',\n                'play_direction', 'absolute_yardline_number', 'height_inches', \n                'player_weight', 'player_age', 'frame_id'\n            ]\n            \n            available_cols = final_observations.columns.tolist()\n            merge_cols = base_merge_cols + [col for col in optional_merge_cols if col in available_cols]\n            \n            print(f\"Merging datasets with {len(merge_cols)} columns...\")\n            \n            # Perform merge\n            merged_data = output_data.merge(\n                final_observations[merge_cols],\n                on=base_merge_cols,\n                how='left'\n            )\n            \n            print(f\"Merged data shape: {merged_data.shape}\")\n            \n            # Calculate advanced features\n            print(\"Calculating advanced features...\")\n            processed_data = self._calculate_advanced_features(merged_data, training_mode=training_mode)\n            \n            return processed_data\n            \n        except Exception as e:\n            print(f\"Error in feature preparation: {e}\")\n            raise\n    \n    def _optimize_hyperparameters(self, X_train, y_train, model_type='xgb'):\n        \"\"\"Optimize hyperparameters using Optuna\"\"\"\n        def objective(trial):\n            if model_type == 'xgb':\n                params = {\n                    'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n                    'max_depth': trial.suggest_int('max_depth', 6, 12),\n                    'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n                    'random_state': self.seed,\n                    'tree_method': 'gpu_hist',\n                    'predictor': 'gpu_predictor',\n                }\n                model = XGBRegressor(**params)\n            \n            elif model_type == 'lgb':\n                params = {\n                    'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n                    'max_depth': trial.suggest_int('max_depth', 6, 12),\n                    'num_leaves': trial.suggest_int('num_leaves', 50, 200),\n                    'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n                    'random_state': self.seed,\n                    'device': 'gpu',\n                    'verbosity': -1,\n                }\n                model = LGBMRegressor(**params)\n            \n            # Cross-validation\n            scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error')\n            return -scores.mean()\n        \n        study = optuna.create_study(direction='minimize')\n        study.optimize(objective, n_trials=50)\n        return study.best_params\n    \n    def train_models(self):\n        \"\"\"Train ensemble models with advanced techniques\"\"\"\n        try:\n            # Load and prepare data\n            print(\"Loading datasets...\")\n            train_input, train_output, test_input, test_template = self.load_and_combine_datasets()\n            \n            print(\"Preparing training features...\")\n            self.train_data = self.prepare_features(train_input, train_output, training_mode=True)\n            \n            print(\"Preparing test features...\")\n            self.test_data = self.prepare_features(test_input, test_template, training_mode=False)\n            \n            # Define feature sets\n            self._define_feature_sets()\n            \n            # Prepare training matrices\n            print(\"Preparing training matrices...\")\n            X_train = self._prepare_training_matrix(self.train_data)\n            \n            if not all(col in self.train_data.columns for col in ['displacement_x', 'displacement_y']):\n                raise KeyError(\"Target variables not found in training data\")\n            \n            y_dx = self.train_data['displacement_x'].values\n            y_dy = self.train_data['displacement_y'].values\n            \n            print(f\"Training data shape: {X_train.shape}\")\n            print(f\"Number of features: {len(self.all_features)}\")\n            \n            # Train models for both displacement dimensions\n            self._train_model_ensemble(X_train, y_dx, y_dy)\n            \n            print(\"Model training completed successfully!\")\n            return self\n            \n        except Exception as e:\n            print(f\"Error in model training: {e}\")\n            raise\n    \n    def _define_feature_sets(self):\n        \"\"\"Define comprehensive feature sets\"\"\"\n        available_columns = set(self.train_data.columns)\n        \n        # Numerical features\n        potential_numerical = [\n            'final_pre_throw_x', 'final_pre_throw_y', 's', 'a', 'o', 'dir',\n            'time_seconds', 'normalized_frame', 'frames_remaining', 'play_duration',\n            'distance_to_ball_landing', 'angle_to_ball_landing', 'ball_trajectory_angle',\n            'ball_horizontal_distance', 'ball_vertical_distance',\n            'distance_to_target', 'angle_to_target', 'target_ball_alignment',\n            'is_target_receiver', 'is_quarterback', 'is_receiver', 'is_defender',\n            'velocity_x', 'velocity_y', 'speed_squared', 'velocity_towards_target',\n            'velocity_towards_ball', 'acceleration_magnitude', 'acceleration_squared',\n            'momentum_indicator', 'body_movement_alignment', 'body_target_alignment',\n            'normalized_x', 'normalized_y', 'distance_from_sideline_x', 'distance_from_sideline_y',\n            'field_third_x', 'field_hash_position', 'yards_to_endzone', 'red_zone_indicator',\n            'goal_line_indicator', 'is_offense', 'is_defense', 'height_inches', 'player_weight',\n            'bmi', 'weight_height_ratio', 'age_category'\n        ]\n        \n        # Add motion consistency features\n        for feature in ['s', 'a', 'dir', 'o']:\n            for stat in ['std', 'mean', 'min', 'max', 'range', 'cv']:\n                potential_numerical.append(f'{feature}_{stat}')\n        \n        # Add interaction features\n        interaction_features = [\n            'speed_distance_ratio', 'time_to_ball_estimate', 'speed_acceleration_product',\n            'kinetic_energy_estimate', 'position_velocity_x', 'position_velocity_y'\n        ]\n        potential_numerical.extend(interaction_features)\n        \n        # Target receiver interaction features\n        for feature in ['s', 'distance_to_ball_landing', 'normalized_x']:\n            potential_numerical.append(f'target_receiver_{feature}')\n        \n        # Categorical features\n        potential_categorical = ['player_role', 'player_side', 'play_direction']\n        \n        # Select available features\n        self.numerical_features = [f for f in potential_numerical if f in available_columns]\n        self.categorical_features = [f for f in potential_categorical if f in available_columns]\n        self.all_features = self.numerical_features + self.categorical_features\n        \n        print(f\"Selected {len(self.numerical_features)} numerical features\")\n        print(f\"Selected {len(self.categorical_features)} categorical features\")\n        print(f\"Total features: {len(self.all_features)}\")\n    \n    def _prepare_training_matrix(self, data):\n        \"\"\"Prepare training matrix with preprocessing\"\"\"\n        X = data[self.all_features].copy()\n        \n        # Encode categorical features\n        X = self._encode_categorical_features(X, self.categorical_features)\n        \n        # Handle missing values with advanced imputation\n        X = self._handle_missing_values(X)\n        \n        # Scale numerical features using RobustScaler (less sensitive to outliers)\n        self.scalers['numerical'] = RobustScaler()\n        X[self.numerical_features] = self.scalers['numerical'].fit_transform(X[self.numerical_features])\n        \n        # Remove outliers using IQR method\n        X = self._remove_outliers(X, data)\n        \n        return X\n    \n    def _handle_missing_values(self, X):\n        \"\"\"Advanced missing value handling\"\"\"\n        # For numerical features, use median imputation\n        for col in self.numerical_features:\n            if col in X.columns:\n                median_val = X[col].median()\n                X[col] = X[col].fillna(median_val)\n        \n        # For categorical features, use mode or 'Unknown'\n        for col in self.categorical_features:\n            if col in X.columns:\n                X[col] = X[col].fillna(0)  # Already encoded, so use 0\n        \n        return X.fillna(0)\n    \n    def _remove_outliers(self, X, original_data):\n        \"\"\"Remove extreme outliers using IQR method\"\"\"\n        if 'displacement_x' in original_data.columns and 'displacement_y' in original_data.columns:\n            # Calculate displacement magnitude for outlier detection\n            displacement_mag = np.sqrt(original_data['displacement_x']**2 + original_data['displacement_y']**2)\n            \n            # Use IQR method\n            Q1 = displacement_mag.quantile(0.25)\n            Q3 = displacement_mag.quantile(0.75)\n            IQR = Q3 - Q1\n            \n            # Define outlier bounds (more conservative)\n            lower_bound = Q1 - 2.0 * IQR\n            upper_bound = Q3 + 2.0 * IQR\n            \n            # Keep only non-outliers\n            mask = (displacement_mag >= lower_bound) & (displacement_mag <= upper_bound)\n            print(f\"Removing {(~mask).sum()} outliers ({(~mask).mean()*100:.2f}%)\")\n            \n            return X[mask]\n        \n        return X\n    \n    def _train_model_ensemble(self, X_train, y_dx, y_dy):\n        \"\"\"Train comprehensive ensemble of models\"\"\"\n        # Remove outliers from targets as well\n        valid_indices = X_train.index\n        y_dx = y_dx[valid_indices] if hasattr(y_dx, '__getitem__') else y_dx\n        y_dy = y_dy[valid_indices] if hasattr(y_dy, '__getitem__') else y_dy\n        \n        # Define model configurations\n        model_configs = self._get_model_configurations()\n        \n        # Train models for X displacement\n        print(\"Training models for X displacement...\")\n        for name, config in model_configs.items():\n            print(f\"Training {name}...\")\n            if self.optimize_hyperparams and name in ['xgb', 'lgb']:\n                # Optimize hyperparameters\n                best_params = self._optimize_hyperparameters(X_train, y_dx, name)\n                config.update(best_params)\n            \n            model = self._create_model(name, config)\n            model.fit(X_train, y_dx)\n            self.models_dx[name] = model\n            \n            # Store feature importance if available\n            if hasattr(model, 'feature_importances_'):\n                self.feature_importance[f'{name}_x'] = model.feature_importances_\n        \n        # Train models for Y displacement\n        print(\"Training models for Y displacement...\")\n        for name, config in model_configs.items():\n            print(f\"Training {name}...\")\n            if self.optimize_hyperparams and name in ['xgb', 'lgb']:\n                # Optimize hyperparameters\n                best_params = self._optimize_hyperparameters(X_train, y_dy, name)\n                config.update(best_params)\n            \n            model = self._create_model(name, config)\n            model.fit(X_train, y_dy)\n            self.models_dy[name] = model\n            \n            # Store feature importance if available\n            if hasattr(model, 'feature_importances_'):\n                self.feature_importance[f'{name}_y'] = model.feature_importances_\n        \n        # Print feature importance summary\n        self._print_feature_importance_summary()\n    \n    def _get_model_configurations(self):\n        \"\"\"Get model configurations with optimal hyperparameters\"\"\"\n        return {\n            'xgb': {\n                'n_estimators': 2500,\n                'learning_rate': 0.05,\n                'max_depth': 8,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'reg_alpha': 0.1,\n                'reg_lambda': 0.1,\n                'random_state': self.seed,\n                'tree_method': 'gpu_hist',\n                'predictor': 'gpu_predictor',\n                'objective': 'reg:squarederror'\n            },\n            'lgb': {\n                'n_estimators': 2500,\n                'learning_rate': 0.045,\n                'max_depth': 8,\n                'num_leaves': 120,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'reg_alpha': 0.1,\n                'reg_lambda': 0.1,\n                'random_state': self.seed,\n                'verbosity': -1,\n                'device': 'gpu',\n                'objective': 'regression'\n            },\n            'cat': {\n                'iterations': 2500,\n                'learning_rate': 0.035,\n                'depth': 8,\n                'l2_leaf_reg': 3.0,\n                'random_seed': self.seed,\n                'verbose': False,\n                'task_type': 'GPU',\n                'loss_function': 'RMSE'\n            },\n            'rf': {\n                'n_estimators': 300,\n                'max_depth': 12,\n                'min_samples_split': 5,\n                'min_samples_leaf': 2,\n                'max_features': 'sqrt',\n                'random_state': self.seed,\n                'n_jobs': -1\n            },\n            'et': {\n                'n_estimators': 300,\n                'max_depth': 12,\n                'min_samples_split': 5,\n                'min_samples_leaf': 2,\n                'max_features': 'sqrt',\n                'random_state': self.seed,\n                'n_jobs': -1\n            },\n            'ridge': {\n                'alpha': 1.0,\n                'random_state': self.seed\n            },\n            'elastic': {\n                'alpha': 1.0,\n                'l1_ratio': 0.5,\n                'random_state': self.seed\n            }\n        }\n    \n    def _create_model(self, name, config):\n        \"\"\"Create model instance based on name and configuration\"\"\"\n        model_map = {\n            'xgb': XGBRegressor,\n            'lgb': LGBMRegressor,\n            'cat': CatBoostRegressor,\n            'rf': RandomForestRegressor,\n            'et': ExtraTreesRegressor,\n            'ridge': Ridge,\n            'elastic': ElasticNet\n        }\n        \n        if name not in model_map:\n            raise ValueError(f\"Unknown model type: {name}\")\n        \n        return model_map[name](**config)\n    \n    def _print_feature_importance_summary(self):\n        \"\"\"Print feature importance summary from tree-based models\"\"\"\n        if not self.feature_importance:\n            return\n        \n        print(\"\\nFeature Importance Summary (Top 15 features):\")\n        print(\"-\" * 60)\n        \n        # Average feature importance across models\n        all_importances = {}\n        for model_key, importance in self.feature_importance.items():\n            for i, feature in enumerate(self.all_features):\n                if feature not in all_importances:\n                    all_importances[feature] = []\n                all_importances[feature].append(importance[i])\n        \n        # Calculate average importance\n        avg_importance = {\n            feature: np.mean(importance_list) \n            for feature, importance_list in all_importances.items()\n        }\n        \n        # Sort by importance\n        sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n        \n        for feature, importance in sorted_features[:15]:\n            print(f\"{feature:40s}: {importance:.4f}\")\n    \n    def generate_predictions(self):\n        \"\"\"Generate ensemble predictions for test data\"\"\"\n        try:\n            print(\"Preparing test features...\")\n            X_test = self.test_data[self.all_features].copy()\n            \n            # Apply same preprocessing as training\n            X_test = self._encode_categorical_features(X_test, self.categorical_features)\n            X_test = self._handle_missing_values(X_test)\n            X_test[self.numerical_features] = self.scalers['numerical'].transform(X_test[self.numerical_features])\n            \n            # Generate ensemble predictions\n            print(\"Generating ensemble predictions...\")\n            pred_dx = self._ensemble_prediction(X_test, self.models_dx)\n            pred_dy = self._ensemble_prediction(X_test, self.models_dy)\n            \n            # Calculate final positions\n            self.test_data['predicted_x'] = self.test_data['final_pre_throw_x'] + pred_dx\n            self.test_data['predicted_y'] = self.test_data['final_pre_throw_y'] + pred_dy\n            \n            # Apply field boundary constraints with soft clipping\n            self.test_data['predicted_x'] = self._soft_clip(self.test_data['predicted_x'], 0.0, 120.0)\n            self.test_data['predicted_y'] = self._soft_clip(self.test_data['predicted_y'], 0.0, 53.3)\n            \n            # Post-processing adjustments based on player roles\n            self._apply_role_based_adjustments()\n            \n            print(f\"Generated predictions for {len(self.test_data)} samples\")\n            return self.test_data\n            \n        except Exception as e:\n            print(f\"Error generating predictions: {e}\")\n            raise\n    \n    def _soft_clip(self, values, min_val, max_val):\n        \"\"\"Apply soft clipping to avoid hard boundaries\"\"\"\n        # Use sigmoid-like function near boundaries\n        margin = 2.0  # yards\n        \n        # Soft lower bound\n        lower_mask = values < (min_val + margin)\n        values[lower_mask] = min_val + margin * (1 / (1 + np.exp(-(values[lower_mask] - min_val))))\n        \n        # Soft upper bound\n        upper_mask = values > (max_val - margin)\n        values[upper_mask] = max_val - margin * (1 / (1 + np.exp(values[upper_mask] - max_val)))\n        \n        # Hard clip for extreme cases\n        return np.clip(values, min_val, max_val)\n    \n    def _apply_role_based_adjustments(self):\n        \"\"\"Apply post-processing adjustments based on player roles\"\"\"\n        if 'is_target_receiver' in self.test_data.columns:\n            # Target receivers likely move towards ball landing position\n            target_mask = self.test_data['is_target_receiver'] == 1\n            if target_mask.any() and 'ball_land_x' in self.test_data.columns:\n                adjustment_factor = 0.1  # 10% adjustment towards ball\n                \n                ball_dx = self.test_data['ball_land_x'] - self.test_data['predicted_x']\n                ball_dy = self.test_data['ball_land_y'] - self.test_data['predicted_y']\n                \n                self.test_data.loc[target_mask, 'predicted_x'] += adjustment_factor * ball_dx[target_mask]\n                self.test_data.loc[target_mask, 'predicted_y'] += adjustment_factor * ball_dy[target_mask]\n        \n        # Ensure defenders don't move too far from their zones (conservative adjustment)\n        if 'is_defense' in self.test_data.columns:\n            defense_mask = self.test_data['is_defense'] == 1\n            if defense_mask.any():\n                # Reduce displacement magnitude for defensive players\n                dx = self.test_data['predicted_x'] - self.test_data['final_pre_throw_x']\n                dy = self.test_data['predicted_y'] - self.test_data['final_pre_throw_y']\n                \n                # Apply conservative factor\n                conservative_factor = 0.9\n                self.test_data.loc[defense_mask, 'predicted_x'] = (\n                    self.test_data.loc[defense_mask, 'final_pre_throw_x'] + \n                    conservative_factor * dx[defense_mask]\n                )\n                self.test_data.loc[defense_mask, 'predicted_y'] = (\n                    self.test_data.loc[defense_mask, 'final_pre_throw_y'] + \n                    conservative_factor * dy[defense_mask]\n                )\n    \n    def _ensemble_prediction(self, X, models):\n        \"\"\"Generate sophisticated weighted ensemble predictions\"\"\"\n        predictions = []\n        \n        # Dynamic weights based on model performance\n        weights = {\n            'xgb': 0.30,\n            'lgb': 0.25,\n            'cat': 0.20,\n            'rf': 0.10,\n            'et': 0.08,\n            'ridge': 0.04,\n            'elastic': 0.03\n        }\n        \n        for model_name, model in models.items():\n            if model_name in weights:\n                pred = model.predict(X)\n                predictions.append(pred * weights[model_name])\n                print(f\"{model_name} prediction range: [{pred.min():.2f}, {pred.max():.2f}]\")\n        \n        ensemble_pred = np.sum(predictions, axis=0)\n        \n        # Apply ensemble post-processing (outlier reduction)\n        ensemble_pred = self._reduce_prediction_outliers(ensemble_pred)\n        \n        return ensemble_pred\n    \n    def _reduce_prediction_outliers(self, predictions):\n        \"\"\"Reduce extreme predictions using percentile capping\"\"\"\n        lower_percentile = np.percentile(predictions, 2)\n        upper_percentile = np.percentile(predictions, 98)\n        \n        # Soft capping using tanh function for extreme values\n        extreme_low = predictions < lower_percentile\n        extreme_high = predictions > upper_percentile\n        \n        predictions[extreme_low] = lower_percentile + (predictions[extreme_low] - lower_percentile) * 0.5\n        predictions[extreme_high] = upper_percentile + (predictions[extreme_high] - upper_percentile) * 0.5\n        \n        return predictions\n    \n    def create_submission_file(self, output_path=\"submission.csv\"):\n        \"\"\"Create submission file in required format with validation\"\"\"\n        try:\n            # Check available columns for ID creation\n            available_columns = self.test_data.columns.tolist()\n            print(f\"Available columns in test_data: {available_columns}\")\n            \n            # Create unique identifier - handle missing frame_id gracefully\n            id_components = []\n            \n            # Required components\n            for col in ['game_id', 'play_id', 'nfl_id']:\n                if col in self.test_data.columns:\n                    id_components.append(self.test_data[col].astype(str))\n                else:\n                    raise KeyError(f\"Required column '{col}' not found in test data\")\n            \n            # Optional frame_id component\n            if 'frame_id' in self.test_data.columns:\n                id_components.append(self.test_data['frame_id'].astype(str))\n                print(\"Using frame_id in unique ID creation\")\n            else:\n                print(\"Warning: frame_id not found, using sequential numbering\")\n                # Create sequential frame numbers for each play\n                self.test_data['synthetic_frame_id'] = (\n                    self.test_data.groupby(['game_id', 'play_id', 'nfl_id']).cumcount() + 1\n                )\n                id_components.append(self.test_data['synthetic_frame_id'].astype(str))\n            \n            # Join all components with underscores\n            self.test_data['unique_id'] = id_components[0]\n            for component in id_components[1:]:\n                self.test_data['unique_id'] = self.test_data['unique_id'] + \"_\" + component\n            \n            # Prepare submission DataFrame\n            submission_df = self.test_data[['unique_id', 'predicted_x', 'predicted_y']].rename(\n                columns={'predicted_x': 'x', 'predicted_y': 'y', 'unique_id': 'id'}\n            )\n            \n            # Validation checks\n            print(\"Performing validation checks...\")\n            \n            # Check for missing values\n            if submission_df.isnull().any().any():\n                print(\"Warning: Found missing values in submission\")\n                submission_df = submission_df.fillna(method='bfill').fillna(method='ffill')\n            \n            # Check for duplicate IDs\n            if submission_df['id'].duplicated().any():\n                print(\"Warning: Found duplicate IDs in submission\")\n                submission_df = submission_df.drop_duplicates('id', keep='first')\n            \n            # Validate coordinate ranges\n            x_out_of_bounds = (submission_df['x'] < 0) | (submission_df['x'] > 120)\n            y_out_of_bounds = (submission_df['y'] < 0) | (submission_df['y'] > 53.3)\n            \n            if x_out_of_bounds.any():\n                print(f\"Warning: {x_out_of_bounds.sum()} x coordinates out of bounds\")\n                submission_df['x'] = np.clip(submission_df['x'], 0, 120)\n            \n            if y_out_of_bounds.any():\n                print(f\"Warning: {y_out_of_bounds.sum()} y coordinates out of bounds\")\n                submission_df['y'] = np.clip(submission_df['y'], 0, 53.3)\n            \n            # Final validation\n            print(f\"Final submission statistics:\")\n            print(f\"Shape: {submission_df.shape}\")\n            print(f\"X range: [{submission_df['x'].min():.2f}, {submission_df['x'].max():.2f}]\")\n            print(f\"Y range: [{submission_df['y'].min():.2f}, {submission_df['y'].max():.2f}]\")\n            print(f\"Missing values: {submission_df.isnull().sum().sum()}\")\n            print(f\"Duplicate IDs: {submission_df['id'].duplicated().sum()}\")\n            \n            # Save submission file\n            submission_df.to_csv(output_path, index=False)\n            print(f\"Submission file saved to {output_path}\")\n            \n            return submission_df\n            \n        except Exception as e:\n            print(f\"Error creating submission file: {e}\")\n            raise\n    \n    def evaluate_model_performance(self):\n        \"\"\"Evaluate model performance using cross-validation\"\"\"\n        if not hasattr(self, 'train_data'):\n            print(\"No training data available for evaluation\")\n            return\n        \n        try:\n            print(\"Evaluating model performance...\")\n            \n            # Prepare data\n            X = self.train_data[self.all_features].copy()\n            X = self._encode_categorical_features(X, self.categorical_features)\n            X = self._handle_missing_values(X)\n            X[self.numerical_features] = self.scalers['numerical'].transform(X[self.numerical_features])\n            \n            y_dx = self.train_data['displacement_x'].values\n            y_dy = self.train_data['displacement_y'].values\n            \n            # Cross-validation setup\n            gkf = GroupKFold(n_splits=3)\n            groups = self.train_data['game_id'].values\n            \n            # Evaluate each model\n            results = {}\n            for model_name in self.models_dx.keys():\n                print(f\"Evaluating {model_name}...\")\n                \n                dx_scores = []\n                dy_scores = []\n                \n                for train_idx, val_idx in gkf.split(X, y_dx, groups):\n                    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n                    y_dx_train, y_dx_val = y_dx[train_idx], y_dx[val_idx]\n                    y_dy_train, y_dy_val = y_dy[train_idx], y_dy[val_idx]\n                    \n                    # Train and predict\n                    model_dx = self._create_model(model_name, self._get_model_configurations()[model_name])\n                    model_dy = self._create_model(model_name, self._get_model_configurations()[model_name])\n                    \n                    model_dx.fit(X_train_fold, y_dx_train)\n                    model_dy.fit(X_train_fold, y_dy_train)\n                    \n                    pred_dx = model_dx.predict(X_val_fold)\n                    pred_dy = model_dy.predict(X_val_fold)\n                    \n                    dx_scores.append(mean_squared_error(y_dx_val, pred_dx))\n                    dy_scores.append(mean_squared_error(y_dy_val, pred_dy))\n                \n                results[model_name] = {\n                    'dx_rmse': np.sqrt(np.mean(dx_scores)),\n                    'dy_rmse': np.sqrt(np.mean(dy_scores)),\n                    'combined_rmse': np.sqrt(np.mean(dx_scores) + np.mean(dy_scores))\n                }\n            \n            # Print results\n            print(\"\\nModel Performance Summary:\")\n            print(\"-\" * 60)\n            print(f\"{'Model':<10} {'X RMSE':<10} {'Y RMSE':<10} {'Combined':<10}\")\n            print(\"-\" * 60)\n            \n            for model_name, scores in results.items():\n                print(f\"{model_name:<10} {scores['dx_rmse']:<10.4f} {scores['dy_rmse']:<10.4f} {scores['combined_rmse']:<10.4f}\")\n            \n            return results\n            \n        except Exception as e:\n            print(f\"Error in model evaluation: {e}\")\n            return None\n\n# Main execution function\ndef main():\n    \"\"\"Main execution function with comprehensive error handling\"\"\"\n    try:\n        print(\"=\"*80)\n        print(\"NFL PLAYER MOVEMENT PREDICTION - ENHANCED VERSION\")\n        print(\"=\"*80)\n        \n        # Initialize predictor with hyperparameter optimization disabled for faster execution\n        predictor = NFLPlayerMovementPredictor(\n            data_dir=\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\",\n            seed=42,\n            optimize_hyperparams=False  # Set to True for hyperparameter optimization\n        )\n        \n        # Train models\n        print(\"\\nSTEP 1: Training ensemble models...\")\n        predictor.train_models()\n        \n        # Evaluate model performance (optional)\n        print(\"\\nSTEP 2: Evaluating model performance...\")\n        performance_results = predictor.evaluate_model_performance()\n        \n        # Generate predictions\n        print(\"\\nSTEP 3: Generating predictions...\")\n        predictions = predictor.generate_predictions()\n        \n        # Create submission file\n        print(\"\\nSTEP 4: Creating submission file...\")\n        submission = predictor.create_submission_file(\"/kaggle/working/submission.csv\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n        print(\"=\"*80)\n        \n        # Display sample predictions\n        print(\"\\nSample predictions:\")\n        print(submission.head(10))\n        \n        print(f\"\\nSubmission summary:\")\n        print(f\"Total predictions: {len(submission)}\")\n        print(f\"Unique IDs: {submission['id'].nunique()}\")\n        print(f\"X coordinate range: [{submission['x'].min():.2f}, {submission['x'].max():.2f}]\")\n        print(f\"Y coordinate range: [{submission['y'].min():.2f}, {submission['y'].max():.2f}]\")\n        \n        return submission\n        \n    except Exception as e:\n        print(f\"\\nERROR: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\nif __name__ == \"__main__\":\n    submission_result = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T17:31:38.206767Z","iopub.execute_input":"2025-09-28T17:31:38.207151Z","iopub.status.idle":"2025-09-28T18:10:41.444643Z","shell.execute_reply.started":"2025-09-28T17:31:38.207129Z","shell.execute_reply":"2025-09-28T18:10:41.444021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}