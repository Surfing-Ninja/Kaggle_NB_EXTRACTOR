{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================\n# セル1: 基本セットアップ\n# - 必要ライブラリのimport\n# - フィールド定数、学習ハイパーパラメータ\n# - 方向正規化や物理モデルなどの基本ユーティリティ関数\n# =========================================================\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------------------------\n# Try cuDF pandas backend for speed (GPU上でのpandasアクセラレーション)\n# 成功したらUSE_CUDF=Trueになる\n# --------------------------------------\nUSE_CUDF = False\ntry:\n    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n    import pandas as pd\n    import numpy as np\n    import cupy as cp  # optional\n    USE_CUDF = True\n    print(\"✅ cuDF pandas backend ENABLED (GPU DataFrame ops)\")\nexcept Exception:\n    import pandas as pd\n    import numpy as np\n    print(\"cuDF backend not available -> using pandas (CPU)\")\n\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom multiprocessing import Pool as MP_Pool, cpu_count\n\nimport pickle\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor, Pool as CatBoostPool\n\n# -------------------\n# データパス・定数\n# -------------------\nBASEDIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n\nFIELD_LENGTH = 120.0\nFIELD_WIDTH  = 53.3\nFPS = 10.0  # frames/sec\n\nN_FRAMES_KEEP = 10      # スナップ前の最後の最大frame数だけ残す\nWINDOW_SIZE   = 3       # ラグ/ローリングの窓幅\nSHORT_TIME_THRESHOLD = 1.0  # time_to_ball<1.0秒を\"ショートパス系\"モデルにする境界\n\n# CatBoostハイパーパラメータ\nITERATIONS = 20000\nLEARNING_RATE = 0.045\nDEPTH = 9\nL2_LEAF_REG = 3.0\nBOOTSTRAP_TYPE = \"Bayesian\"\nBAGGING_TEMPERATURE = 0.7\nEARLY_STOPPING = 400\nVERBOSE_EVAL = 200\n\nSEEDS = [42, 2025, 7]\nN_FOLDS = 2  # CVはweek17/18ブロック\n\ndef set_seed(seed=42):\n    \"\"\"できる範囲で乱数を固定する\"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    try:\n        import torch\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    except Exception:\n        pass\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ndef normalize_orientation_inplace(df: pd.DataFrame) -> None:\n    \"\"\"\n    攻撃方向を常に右(エンドゾーンx=120方向)にそろえる。\n    left方向のプレイは左右反転+角度180度回転。\n    \"\"\"\n    if 'play_direction' not in df.columns:\n        return\n    mask_left = df['play_direction'].astype(str).str.lower().eq('left')\n\n    # 座標を左右反転\n    if 'x' in df.columns:\n        df.loc[mask_left, 'x'] = FIELD_LENGTH - df.loc[mask_left, 'x']\n    if 'y' in df.columns:\n        df.loc[mask_left, 'y'] = FIELD_WIDTH  - df.loc[mask_left, 'y']\n\n    # 向き(dir,o)を180度ずらす\n    for ang in ['dir', 'o']:\n        if ang in df.columns:\n            df.loc[mask_left, ang] = (df.loc[mask_left, ang] + 180) % 360\n\n    # ボール落下地点も反転\n    if 'ball_land_x' in df.columns:\n        df.loc[mask_left, 'ball_land_x'] = FIELD_LENGTH - df.loc[mask_left, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[mask_left, 'ball_land_y'] = FIELD_WIDTH  - df.loc[mask_left, 'ball_land_y']\n\n\ndef compute_physics_xy(x, y, vx, vy, future_frame_id):\n    \"\"\"\n    等速運動での単純な将来位置予測。\n    frame_id(=何フレーム先か) / FPS で時間[s]に変換し、x+vx*t, y+vy*t を返す。\n    \"\"\"\n    t = np.nan_to_num(pd.to_numeric(future_frame_id, errors='coerce')) / FPS\n    px = x + vx * t\n    py = y + vy * t\n    px = np.clip(px, 0, FIELD_LENGTH)\n    py = np.clip(py, 0, FIELD_WIDTH)\n    return px, py\n\n\ndef role_bucket_id(player_role: str) -> int:\n    \"\"\"\n    役割を3クラスにまとめる(バケット補正用)\n      0: Targeted Receiver\n      1: Defensive Coverage\n      2: その他\n    \"\"\"\n    if player_role == \"Targeted Receiver\":\n        return 0\n    if player_role == \"Defensive Coverage\":\n        return 1\n    return 2\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T01:13:49.049459Z","iopub.execute_input":"2025-11-02T01:13:49.04983Z","iopub.status.idle":"2025-11-02T01:13:49.06238Z","shell.execute_reply.started":"2025-11-02T01:13:49.049806Z","shell.execute_reply":"2025-11-02T01:13:49.061767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# セル2: 特徴量エンジニアリング & 前処理ユーティリティ\n# - 全週の読み込み\n# - 最終10フレーム抽出\n# - 1フレームあたりの物理系/幾何系特徴量付与\n# - ラグ/ローリング統計 (WINDOW_SIZE=3)\n# - スナップ直前フレーム(最終フレーム)におけるQB/DFコンテキスト追加\n# - 学習用テーブル(train_tbl)と推論用テーブル(test_tbl)を組み立てる\n# - モデルに入れる特徴カラム選定\n# - CV分割(week17/18)\n# - バケット補正テーブルを作る下準備\n# =========================================================\n\ndef load_week(week:int):\n    df_in  = pd.read_csv(BASEDIR / f\"train/input_2023_w{week:02d}.csv\")\n    df_out = pd.read_csv(BASEDIR / f\"train/output_2023_w{week:02d}.csv\")\n    df_in['week']  = week\n    df_out['week'] = week\n    return df_in, df_out\n\ndef load_all_train():\n    \"\"\"1〜18週のinput/outputを全部読み込んで結合\"\"\"\n    weeks = list(range(1,19))\n    with MP_Pool(min(cpu_count(), 16)) as pool:\n        res = list(tqdm(pool.imap(load_week, weeks), total=len(weeks)))\n    ins  = [r[0] for r in res]\n    outs = [r[1] for r in res]\n    inp  = pd.concat(ins , ignore_index=True)\n    outp = pd.concat(outs, ignore_index=True)\n    return inp, outp\n\ndef load_test():\n    \"\"\"公開テスト(test_input/test)読み込み\"\"\"\n    test_input = pd.read_csv(BASEDIR / \"test_input.csv\")\n    test_df    = pd.read_csv(BASEDIR / \"test.csv\")\n    return test_input, test_df\n\n\ndef keep_last_n_frames(df: pd.DataFrame, n_last=N_FRAMES_KEEP) -> pd.DataFrame:\n    \"\"\"\n    各(game_id,play_id,nfl_id)ごとにフレームIDでソートし、末尾n_last個だけ残す。\n    \"\"\"\n    if len(df) == 0:\n        return df.copy()\n    df = df.sort_values(['game_id','play_id','nfl_id','frame_id'])\n    tail_idx = df.groupby(['game_id','play_id','nfl_id']).tail(n_last).index\n    return df.loc[tail_idx].copy()\n\n\ndef add_perframe_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    各フレーム行に対して物理量・プレイヤー体格・ボール/エンドゾーン距離などの特徴量を付与。\n    \"\"\"\n    if len(df) == 0:\n        return df.copy()\n\n    out = df.copy()\n\n    # 欠損フラグ\n    for col in ['s','a','o','dir','ball_land_x','ball_land_y','player_height']:\n        if col in out.columns:\n            out[f'isna_{col}'] = out[col].isna().astype(np.int8)\n\n    # 速度・加速度ベクトル (dirに沿う)\n    dir_rad = np.radians(out['dir'].fillna(0.0))\n    spd = out['s'].fillna(0.0)\n    acc = out['a'].fillna(0.0)\n\n    out['velocity_x']     = spd * np.cos(dir_rad)\n    out['velocity_y']     = spd * np.sin(dir_rad)\n    out['acceleration_x'] = acc * np.cos(dir_rad)\n    out['acceleration_y'] = acc * np.sin(dir_rad)\n\n    # ボール着地点との距離・方向・ボール方向への接近速度\n    if {'ball_land_x','ball_land_y','x','y'}.issubset(out.columns):\n        dx = out['ball_land_x'] - out['x']\n        dy = out['ball_land_y'] - out['y']\n        dist = np.sqrt(dx*dx + dy*dy)\n        out['dist_to_ball'] = dist\n        out['angle_to_ball'] = np.arctan2(dy, dx)\n        out['velocity_toward_ball'] = (\n            out['velocity_x'] * np.cos(out['angle_to_ball']) +\n            out['velocity_y'] * np.sin(out['angle_to_ball'])\n        )\n    else:\n        out['dist_to_ball'] = np.nan\n        out['angle_to_ball'] = np.nan\n        out['velocity_toward_ball'] = np.nan\n\n    # time_to_ball (フレーム数→秒)\n    if 'num_frames_output' in out.columns:\n        out['time_to_ball'] = out['num_frames_output'] / FPS\n    else:\n        out['time_to_ball'] = np.nan\n\n    # 向き(o)と移動方向(dir)のズレ\n    diff = np.abs(out['o'].fillna(0.0) - out['dir'].fillna(0.0))\n    out['orientation_diff'] = np.minimum(diff, 360 - diff)\n\n    # 役割フラグ\n    out['role_targeted_receiver']  = (out['player_role'] == 'Targeted Receiver').astype(np.int8)\n    out['role_defensive_coverage'] = (out['player_role'] == 'Defensive Coverage').astype(np.int8)\n    out['role_passer']             = (out['player_role'] == 'Passer').astype(np.int8)\n    out['side_offense']            = (out['player_side'] == 'Offense').astype(np.int8)\n\n    # 体格/BMI\n    ht = out['player_height'].fillna('0-0').astype(str).str.split('-', n=1, expand=True)\n    ft  = pd.to_numeric(ht[0], errors='coerce').fillna(0)\n    inc = pd.to_numeric(ht[1], errors='coerce').fillna(0)\n    height_in = ft * 12 + inc\n    out['height_inches'] = height_in\n    with np.errstate(divide='ignore', invalid='ignore'):\n        out['bmi'] = (out['player_weight'] / (np.maximum(height_in,1e-6)**2)) * 703\n\n    # 運動エネルギーっぽい指標やフィールド上の位置関係\n    out['speed_squared'] = spd**2\n    out['accel_magnitude'] = np.sqrt(out['acceleration_x']**2 + out['acceleration_y']**2)\n    out['velocity_alignment'] = np.cos(out['angle_to_ball'] - np.radians(out['dir'].fillna(0.0)))\n\n    t_ball = out['time_to_ball'].fillna(0.0)\n    out['expected_x_at_ball'] = out['x'] + out['velocity_x'] * t_ball\n    out['expected_y_at_ball'] = out['y'] + out['velocity_y'] * t_ball\n    out['error_from_ball_x'] = out['expected_x_at_ball'] - out['ball_land_x']\n    out['error_from_ball_y'] = out['expected_y_at_ball'] - out['ball_land_y']\n    out['error_from_ball']   = np.sqrt(out['error_from_ball_x']**2 + out['error_from_ball_y']**2)\n\n    out['momentum_x'] = out['player_weight'].fillna(200.0) * out['velocity_x']\n    out['momentum_y'] = out['player_weight'].fillna(200.0) * out['velocity_y']\n    out['kinetic_energy'] = 0.5 * out['player_weight'].fillna(200.0) * out['speed_squared']\n\n    ang_ball_deg = np.degrees(out['angle_to_ball'])\n    ad = np.abs(out['o'].fillna(0.0) - ang_ball_deg)\n    out['angle_diff'] = np.minimum(ad, 360 - ad)\n\n    out['time_squared'] = out['time_to_ball']**2\n    out['dist_squared'] = out['dist_to_ball']**2\n    out['weighted_dist_by_time'] = out['dist_to_ball'] / (out['time_to_ball'] + 0.1)\n\n    out['dist_to_sideline'] = np.minimum(out['y'], FIELD_WIDTH - out['y'])\n    out['dist_to_endzone']  = FIELD_LENGTH - out['x']\n\n    # 角度特徴のsin/cos展開\n    for col in ['dir','o']:\n        rad = np.radians(out[col].fillna(0.0))\n        out[f'{col}_sin'] = np.sin(rad)\n        out[f'{col}_cos'] = np.cos(rad)\n    rad_ball = out['angle_to_ball'].fillna(0.0)\n    out['angle_to_ball_sin'] = np.sin(rad_ball)\n    out['angle_to_ball_cos'] = np.cos(rad_ball)\n\n    return out\n\n\ndef add_sequence_features(df: pd.DataFrame, window=WINDOW_SIZE) -> pd.DataFrame:\n    \"\"\"\n    各(game,play,nfl_id)の時系列内でラグ・ローリング平均/分散・速度変化量などを付与。\n    \"\"\"\n    if len(df) == 0:\n        return df.copy()\n\n    out = df.sort_values(['game_id','play_id','nfl_id','frame_id']).copy()\n    gcols = ['game_id','play_id','nfl_id']\n\n    # ラグ\n    for lag in [1,2,3][:window]:\n        for col in ['x','y','velocity_x','velocity_y','s','a']:\n            if col in out.columns:\n                out[f'{col}_lag{lag}'] = out.groupby(gcols)[col].shift(lag)\n\n    # ローリング平均/分散\n    for col in ['x','y','velocity_x','velocity_y','s']:\n        if col in out.columns:\n            roll_mean = out.groupby(gcols)[col].rolling(window, min_periods=1).mean()\n            roll_std  = out.groupby(gcols)[col].rolling(window, min_periods=1).std()\n            roll_mean = roll_mean.reset_index(level=list(range(len(gcols))), drop=True)\n            roll_std  = roll_std.reset_index(level=list(range(len(gcols))), drop=True)\n            out[f'{col}_roll{window}'] = roll_mean\n            out[f'{col}_std{window}']  = roll_std.fillna(0.0)\n\n    # 速度変化(差分)\n    for col in ['velocity_x','velocity_y']:\n        if col in out.columns:\n            out[f'{col}_delta'] = out.groupby(gcols)[col].diff()\n\n    # 新規列のNaN埋め\n    seq_cols = [\n        c for c in out.columns\n        if ('_lag' in c) or (f'_roll{window}' in c) or (f'_std{window}' in c) or c.endswith('_delta')\n    ]\n    out[seq_cols] = out[seq_cols].fillna(0.0)\n\n    return out\n\n\ndef add_context_on_finalframe(final_snap: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    最終フレーム(投げる直前)で:\n    - QBとの距離/相対速度\n    - 最も近いDFとの距離/相対速度\n    - 半径3/5/7ydでのDF密度\n    \"\"\"\n    snap = final_snap.copy()\n    if len(snap) == 0:\n        for c in [\n            'dist_to_qb','rel_speed_to_qb',\n            'nearest_defender_dist','nearest_defender_rel_speed',\n            'def_count_r3','def_count_r5','def_count_r7'\n        ]:\n            snap[c] = 0.0\n        return snap\n\n    # 足りない列は作っておく\n    need_cols = [\n        'game_id','play_id','nfl_id','x','y',\n        'velocity_x','velocity_y','player_role','player_side'\n    ]\n    for c in need_cols:\n        if c not in snap.columns:\n            snap[c] = np.nan\n\n    new_cols = [\n        'dist_to_qb','rel_speed_to_qb',\n        'nearest_defender_dist','nearest_defender_rel_speed',\n        'def_count_r3','def_count_r5','def_count_r7'\n    ]\n    for c in new_cols:\n        snap[c] = np.nan\n\n    # プレイごとにまとめて計算(ループOK: プレイ数はそこまで巨大じゃない)\n    for (gid,pid), g in snap.groupby(['game_id','play_id']):\n        idxs = g.index.values\n\n        # QBを特定\n        qb_mask = (g['player_role'].values == 'Passer')\n        if qb_mask.any():\n            qb_row = g[qb_mask].iloc[0]\n            qb_x, qb_y = qb_row['x'], qb_row['y']\n            qb_vx, qb_vy = qb_row['velocity_x'], qb_row['velocity_y']\n        else:\n            qb_x = qb_y = qb_vx = qb_vy = np.nan\n\n        # DF集合 (ディフェンス側 or Coverage指定)\n        def_mask = (g['player_side'].values == 'Defense') | \\\n                   (g['player_role'].values == 'Defensive Coverage')\n        defenders = g[def_mask]\n\n        def_xy  = defenders[['x','y']].to_numpy(dtype=float) if len(defenders)>0 else np.zeros((0,2))\n        def_vel = defenders[['velocity_x','velocity_y']].to_numpy(dtype=float) if len(defenders)>0 else np.zeros((0,2))\n        def_ids = defenders['nfl_id'].to_numpy(dtype=float) if len(defenders)>0 else np.zeros((0,))\n\n        p_xy   = g[['x','y']].to_numpy(dtype=float)\n        p_vel  = g[['velocity_x','velocity_y']].to_numpy(dtype=float)\n        p_ids  = g['nfl_id'].to_numpy(dtype=float)\n\n        # QBとの距離・相対接近速度\n        if not np.isnan(qb_x):\n            vec_qb = np.stack([qb_x - p_xy[:,0], qb_y - p_xy[:,1]], axis=1)\n            dist_qb = np.linalg.norm(vec_qb, axis=1)\n            u_qb = np.zeros_like(vec_qb)\n            nz = dist_qb > 1e-6\n            u_qb[nz] = vec_qb[nz] / dist_qb[nz,None]\n            v_rel_qb = p_vel - np.array([qb_vx,qb_vy])\n            rel_speed_qb = (v_rel_qb * u_qb).sum(axis=1)\n        else:\n            dist_qb = np.zeros(len(idxs))\n            rel_speed_qb = np.zeros(len(idxs))\n\n        # 最も近いDFの距離・相対接近速度・DF密度\n        if def_xy.shape[0] > 0:\n            diff = p_xy[:,None,:] - def_xy[None,:,:]   # [Nplayers,Ndef,2]\n            dists = np.sqrt((diff**2).sum(axis=2))     # [Nplayers,Ndef]\n\n            # 自分自身(同一nfl_id)を除外\n            for ii, pidv in enumerate(p_ids):\n                same = (def_ids == pidv)\n                if same.any():\n                    dists[ii, same] = np.inf\n\n            min_idx = np.argmin(dists, axis=1)\n            min_dist = dists[np.arange(len(min_idx)), min_idx]\n\n            nn_vel = def_vel[min_idx] if def_vel.shape[0] else np.zeros_like(p_vel)\n\n            vec_df = def_xy[min_idx] - p_xy\n            norm_df= np.linalg.norm(vec_df, axis=1)\n            u_df   = np.zeros_like(vec_df)\n            nz2    = norm_df > 1e-6\n            u_df[nz2] = vec_df[nz2] / norm_df[nz2,None]\n\n            v_rel_df = p_vel - nn_vel\n            rel_speed_df = (v_rel_df * u_df).sum(axis=1)\n\n            def_counts_r3 = (dists <= 3.0).sum(axis=1)\n            def_counts_r5 = (dists <= 5.0).sum(axis=1)\n            def_counts_r7 = (dists <= 7.0).sum(axis=1)\n        else:\n            min_dist      = np.zeros(len(idxs))\n            rel_speed_df  = np.zeros(len(idxs))\n            def_counts_r3 = np.zeros(len(idxs))\n            def_counts_r5 = np.zeros(len(idxs))\n            def_counts_r7 = np.zeros(len(idxs))\n\n        snap.loc[idxs, 'dist_to_qb']                 = dist_qb\n        snap.loc[idxs, 'rel_speed_to_qb']            = rel_speed_qb\n        snap.loc[idxs, 'nearest_defender_dist']      = min_dist\n        snap.loc[idxs, 'nearest_defender_rel_speed'] = rel_speed_df\n        snap.loc[idxs, 'def_count_r3']               = def_counts_r3\n        snap.loc[idxs, 'def_count_r5']               = def_counts_r5\n        snap.loc[idxs, 'def_count_r7']               = def_counts_r7\n\n    fill_cols = [\n        'dist_to_qb','rel_speed_to_qb',\n        'nearest_defender_dist','nearest_defender_rel_speed',\n        'def_count_r3','def_count_r5','def_count_r7'\n    ]\n    for c in fill_cols:\n        snap[c] = snap[c].fillna(0.0)\n\n    return snap\n\n\ndef build_training_table(frame_df: pd.DataFrame,\n                         out_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    学習セットを作る:\n    - 各選手の「投球直前の最終フレーム」を1行にする\n    - そのスナップショットに対し、将来フレーム(target_x,target_y)を紐づける\n    - ターゲットも右方向座標系に正規化\n    - role_bucket（Targeted/DefCoverage/Other）を付与\n    \"\"\"\n    tmp = frame_df.copy()\n    tmp['__fid__'] = pd.to_numeric(tmp['frame_id'], errors='coerce')\n    idx_last = tmp.groupby(['game_id','play_id','nfl_id'])['__fid__'].idxmax()\n\n    final_snap = frame_df.loc[idx_last].copy().reset_index(drop=True)\n\n    # スコア対象の選手だけ\n    if 'player_to_predict' in final_snap.columns:\n        final_snap = final_snap[final_snap['player_to_predict'] == True].copy()\n\n    # 上で定義したDF/QBコンテキストを付与\n    final_snap = add_context_on_finalframe(final_snap)\n\n    # 未来の実測トラッキング(out_df)と結合 (x,y -> target_x,target_y)\n    outc = out_df.copy()\n    outc = outc.rename(columns={'x':'target_x','y':'target_y'})\n    merged = outc.merge(\n        final_snap,\n        on=['game_id','play_id','nfl_id'],\n        how='left',\n        suffixes=('','_snap')\n    )\n\n    # ターゲット座標も右方向化(=train_outputは左右混在なので左のやつは反転)\n    if 'play_direction' in merged.columns:\n        mask_left = merged['play_direction'].astype(str).str.lower().eq('left')\n        merged['target_x'] = np.where(mask_left, FIELD_LENGTH - merged['target_x'], merged['target_x'])\n        merged['target_y'] = np.where(mask_left, FIELD_WIDTH  - merged['target_y'], merged['target_y'])\n\n    # role_bucket (バケット補正キーで使う)\n    merged['role_bucket'] = merged['player_role'].apply(role_bucket_id).astype(np.int8)\n\n    return merged.reset_index(drop=True)\n\n\ndef build_test_snapshot(test_input_df: pd.DataFrame,\n                        test_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    推論用スナップショットを作る(公開テストでの事前デバッグ用)。\n    - train側と似た処理。ただしターゲット列はまだ無い。\n    \"\"\"\n    ti = test_input_df.copy()\n    normalize_orientation_inplace(ti)\n\n    ti = keep_last_n_frames(ti, N_FRAMES_KEEP)\n    ti_feat = add_perframe_features(ti)\n    ti_feat = add_sequence_features(ti_feat, window=WINDOW_SIZE)\n\n    # 各選手の最終フレーム行だけ抜き出す\n    tmp = ti_feat.copy()\n    tmp['__fid__'] = pd.to_numeric(tmp['frame_id'], errors='coerce')\n    idx_last = tmp.groupby(['game_id','play_id','nfl_id'])['__fid__'].idxmax()\n    final_snap = ti_feat.loc[idx_last].copy().reset_index(drop=True)\n\n    # カラム名の整合性調整\n    final_snap = final_snap.rename(columns={\n        'frame_id': 'frame_id_y',\n        'play_direction': 'play_direction_snap',\n    })\n\n    # DF/QBコンテキスト付与\n    final_snap = add_context_on_finalframe(final_snap)\n\n    # 公開test側のメタ(test.csv)と結合。frame_id_xが未来側フレームID\n    test_local = test_df.copy().rename(columns={'frame_id':'frame_id_x'})\n    merged = test_local.merge(\n        final_snap,\n        on=['game_id','play_id','nfl_id'],\n        how='left'\n    )\n\n    # role_bucket\n    merged['role_bucket'] = merged['player_role'].apply(role_bucket_id).astype(np.int8)\n\n    # 元の方向を保持(あとで左右反転を戻すのに使う)\n    merged['play_direction'] = merged['play_direction_snap']\n\n    for c in ['time_to_ball','nearest_defender_dist']:\n        if c in merged.columns:\n            merged[c] = merged[c].fillna(0.0)\n\n    return merged\n\n\ndef get_feature_columns(df_cols):\n    \"\"\"\n    モデルに入れる説明変数カラム一覧を返す。\n    \"\"\"\n    base_cols = [\n        'x','y','s','a','o','dir','velocity_x','velocity_y','acceleration_x','acceleration_y',\n        'dist_to_ball','angle_to_ball','velocity_toward_ball','time_to_ball','orientation_diff',\n        'role_targeted_receiver','role_defensive_coverage','role_passer',\n        'side_offense','height_inches','player_weight','bmi',\n        'ball_land_x','ball_land_y','num_frames_output','frame_id',\n        'speed_squared','accel_magnitude','velocity_alignment',\n        'expected_x_at_ball','expected_y_at_ball',\n        'error_from_ball_x','error_from_ball_y','error_from_ball',\n        'momentum_x','momentum_y','kinetic_energy',\n        'angle_diff','time_squared','dist_squared','weighted_dist_by_time',\n        'dist_to_sideline','dist_to_endzone',\n        'dist_to_qb','rel_speed_to_qb',\n        'nearest_defender_dist','nearest_defender_rel_speed',\n        'def_count_r3','def_count_r5','def_count_r7',\n        'dir_sin','dir_cos','o_sin','o_cos',\n        'angle_to_ball_sin','angle_to_ball_cos',\n    ]\n\n    seq_cols = []\n    for lag in [1,2,3][:WINDOW_SIZE]:\n        for c in ['x','y','velocity_x','velocity_y','s','a']:\n            seq_cols.append(f'{c}_lag{lag}')\n    for c in ['x','y','velocity_x','velocity_y','s']:\n        seq_cols.append(f'{c}_roll{WINDOW_SIZE}')\n        seq_cols.append(f'{c}_std{WINDOW_SIZE}')\n    seq_cols += ['velocity_x_delta','velocity_y_delta']\n\n    naflag_cols = [\n        'isna_s','isna_a','isna_o','isna_dir',\n        'isna_ball_land_x','isna_ball_land_y','isna_player_height'\n    ]\n\n    all_cols = base_cols + seq_cols + naflag_cols\n    return [c for c in all_cols if c in df_cols]\n\n\ndef build_week_block_2fold(df_local: pd.DataFrame):\n    \"\"\"\n    CV分割:\n    - Fold1: week17をバリデーション\n    - Fold2: week18をバリデーション\n    同じプレイ(game_id,play_id)がtrainとvalに跨らないようグループ化。\n    \"\"\"\n    df_local = df_local.reset_index(drop=True).copy()\n    df_local['__group__'] = (\n        df_local['game_id'].astype(str) + '_' + df_local['play_id'].astype(str)\n    )\n\n    grp_week = df_local.groupby('__group__')['week'].agg(lambda x: int(x.iloc[0]))\n    grp_week = grp_week.to_dict()\n\n    fold_weeks = [{17}, {18}]\n    idx_all = np.arange(len(df_local))\n\n    folds = []\n    for wset in fold_weeks:\n        val_groups = {g for g,wk in grp_week.items() if wk in wset}\n        val_mask = df_local['__group__'].isin(val_groups).values\n        val_idx  = idx_all[val_mask]\n        tr_idx   = idx_all[~val_mask]\n        folds.append((tr_idx, val_idx))\n    return folds\n\n\ndef build_bucket_offset(oof_df: pd.DataFrame):\n    \"\"\"\n    OOF(検証データでの予測)から、\n    (time_to_ball_bin, nearest_defender_dist_bin, role_bucket)ごとの\n    系統的なズレ(平均誤差ベクトル)を学習し、補正テーブルとして返す。\n    \"\"\"\n    tbins = [0.0, 0.6, 1.2, 2.0, np.inf]\n    dbins = [0.0, 2.0, 4.0, 6.0, np.inf]\n\n    d = oof_df.copy()\n    d['t_bin'] = pd.cut(d['time_to_ball'].fillna(0), tbins,\n                        labels=False, include_lowest=True)\n    d['d_bin'] = pd.cut(d['nearest_defender_dist'].fillna(99), dbins,\n                        labels=False, include_lowest=True)\n\n    d['err_x'] = d['target_x'] - d['pred_x']\n    d['err_y'] = d['target_y'] - d['pred_y']\n\n    grp = d.groupby(['t_bin','d_bin','role_bucket'])\n    tab = grp[['err_x','err_y']].mean().reset_index()\n\n    offset_dict = {}\n    for _, r in tab.iterrows():\n        key = (int(r.t_bin), int(r.d_bin), int(r.role_bucket))\n        offset_dict[key] = (float(r.err_x), float(r.err_y))\n\n    return offset_dict, tbins, dbins\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T01:13:49.236687Z","iopub.execute_input":"2025-11-02T01:13:49.237149Z","iopub.status.idle":"2025-11-02T01:13:49.283086Z","shell.execute_reply.started":"2025-11-02T01:13:49.237126Z","shell.execute_reply":"2025-11-02T01:13:49.282419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# セル3: 学習本体\n# - 物理モデル(等速予測)との差分(残差)をCatBoostで学習\n# - time_to_ball<1秒をSHORTモデル、それ以外をLONGモデルとして別々に学習\n# - CVはweek17/18ブロック×複数seed、OOFからバケット補正テーブル作成\n# - すべてまとめてbundle.pklに保存 (推論ノートブックで読む)\n# =========================================================\n\ndef train_models_for_split(df_split: pd.DataFrame,\n                           feature_cols: list):\n    \"\"\"\n    指定された(ショートorロング)データでCatBoostアンサンブルを学習。\n    物理的な等速予測→残差を学習し、OOFでバケット補正のための誤差統計も集計。\n    \"\"\"\n    # 等速(physics)ベースライン\n    bx  = df_split['x'].fillna(0).values\n    by  = df_split['y'].fillna(0).values\n    bvx = df_split['velocity_x'].fillna(0).values\n    bvy = df_split['velocity_y'].fillna(0).values\n    fr_future = pd.to_numeric(df_split['frame_id'], errors='coerce').fillna(0).values\n\n    phys_x, phys_y = compute_physics_xy(bx, by, bvx, bvy, fr_future)\n\n    tx = df_split['target_x'].values\n    ty = df_split['target_y'].values\n\n    yres_x = tx - phys_x\n    yres_y = ty - phys_y\n\n    base_rmse = np.sqrt(0.5 * (\n        mean_squared_error(tx, phys_x) + mean_squared_error(ty, phys_y)\n    ))\n    print(f\"[split] Physics baseline RMSE: {base_rmse:.4f}\")\n\n    Xmat = df_split[feature_cols].fillna(0).to_numpy(dtype=np.float32)\n\n    folds = build_week_block_2fold(df_split)\n\n    models_x_all = []\n    models_y_all = []\n    val_scores = []\n    oof_parts = []\n\n    for seed in SEEDS:\n        print(f\"  >> SEED {seed}\")\n        set_seed(seed)\n\n        for fi, (tr_idx, va_idx) in enumerate(folds, start=1):\n            print(f\"     Fold {fi}/{len(folds)} tr={len(tr_idx)} va={len(va_idx)}\")\n\n            Xtr, Xva = Xmat[tr_idx], Xmat[va_idx]\n            yx_tr, yx_va = yres_x[tr_idx], yres_x[va_idx]\n            yy_tr, yy_va = yres_y[tr_idx], yres_y[va_idx]\n\n            pool_tr_x = CatBoostPool(Xtr, yx_tr)\n            pool_va_x = CatBoostPool(Xva, yx_va)\n            pool_tr_y = CatBoostPool(Xtr, yy_tr)\n            pool_va_y = CatBoostPool(Xva, yy_va)\n\n            # x座標残差モデル\n            model_x = CatBoostRegressor(\n                iterations=ITERATIONS,\n                learning_rate=LEARNING_RATE,\n                depth=DEPTH,\n                l2_leaf_reg=L2_LEAF_REG,\n                random_seed=seed,\n                task_type='GPU', devices='0',\n                bootstrap_type=BOOTSTRAP_TYPE,\n                bagging_temperature=BAGGING_TEMPERATURE,\n                loss_function='RMSE',\n                early_stopping_rounds=EARLY_STOPPING,\n                verbose=VERBOSE_EVAL\n            )\n            model_x.fit(pool_tr_x, eval_set=pool_va_x, verbose=VERBOSE_EVAL)\n            models_x_all.append(model_x)\n\n            # y座標残差モデル\n            model_y = CatBoostRegressor(\n                iterations=ITERATIONS,\n                learning_rate=LEARNING_RATE,\n                depth=DEPTH,\n                l2_leaf_reg=L2_LEAF_REG,\n                random_seed=seed,\n                task_type='GPU', devices='0',\n                bootstrap_type=BOOTSTRAP_TYPE,\n                bagging_temperature=BAGGING_TEMPERATURE,\n                loss_function='RMSE',\n                early_stopping_rounds=EARLY_STOPPING,\n                verbose=VERBOSE_EVAL\n            )\n            model_y.fit(pool_tr_y, eval_set=pool_va_y, verbose=VERBOSE_EVAL)\n            models_y_all.append(model_y)\n\n            # 検証(va_idx)での予測 → 等速＋残差\n            pred_x_resid = model_x.predict(Xva)\n            pred_y_resid = model_y.predict(Xva)\n            pred_x_full  = np.clip(phys_x[va_idx] + pred_x_resid, 0, FIELD_LENGTH)\n            pred_y_full  = np.clip(phys_y[va_idx] + pred_y_resid, 0, FIELD_WIDTH)\n\n            fold_rmse = np.sqrt(0.5 * (\n                mean_squared_error(tx[va_idx], pred_x_full) +\n                mean_squared_error(ty[va_idx], pred_y_full)\n            ))\n            print(f\"        Fold {fi} RMSE={fold_rmse:.4f}\")\n            val_scores.append(fold_rmse)\n\n            # OOFを蓄積して後でバケット補正テーブルを作る\n            part = df_split.iloc[va_idx][[\n                'time_to_ball','nearest_defender_dist','role_bucket',\n                'target_x','target_y'\n            ]].copy()\n            part['pred_x'] = pred_x_full\n            part['pred_y'] = pred_y_full\n            oof_parts.append(part)\n\n    cv_rmse = float(np.mean(val_scores))\n    print(f\"[split] CV RMSE folds×seeds: {cv_rmse:.4f} (physics {base_rmse:.4f})\")\n\n    oof_df = pd.concat(oof_parts, ignore_index=True)\n    offset_dict, tbins, dbins = build_bucket_offset(oof_df)\n    print(f\"[split] bucket_offset entries: {len(offset_dict)}\")\n\n    bundle_piece = {\n        \"models_x\": models_x_all,\n        \"models_y\": models_y_all,\n        \"cv_rmse\": cv_rmse,\n        \"physics_rmse\": base_rmse,\n        \"bucket_offset\": {\n            \"dict\": offset_dict,\n            \"tbins\": tbins,\n            \"dbins\": dbins\n        }\n    }\n    return bundle_piece\n\n\ndef create_and_save_bundle(output_path=\"/kaggle/working/bundle.pkl\"):\n    \"\"\"\n    学習のフルパイプライン:\n      1. train/test入力読み込み\n      2. 進行方向を右方向に正規化\n      3. 最終Nフレーム抽出+特徴量生成(物理+ラグ/ローリング)\n      4. 学習テーブル(train_tbl)作成\n      5. time_to_ballでSHORT/LONGに分割してCatBoost学習\n      6. SHORT/LONG両方のバケット補正をマージ\n      7. bundle.pklとして保存（提出ノートで使う）\n    \"\"\"\n    print(f\"CPU cores visible: {cpu_count()} | cuDF backend: {USE_CUDF}\")\n\n    # --- データ読み込み ---\n    print(\"Loading train & test...\")\n    train_input, train_output = load_all_train()\n    test_input,  test_meta    = load_test()\n    print(\"Shapes:\")\n    print(\"  train_input :\", train_input.shape)\n    print(\"  train_output:\", train_output.shape)\n    print(\"  test_input  :\", test_input.shape)\n    print(\"  test_meta   :\", test_meta.shape)\n\n    # 攻撃方向を右にそろえる\n    normalize_orientation_inplace(train_input)\n    normalize_orientation_inplace(test_input)\n\n    # 最終Nフレームだけ残す(軽量化＋局所の動きに集中)\n    train_cut = keep_last_n_frames(train_input, N_FRAMES_KEEP)\n    test_cut  = keep_last_n_frames(test_input , N_FRAMES_KEEP)\n    print(\"[CUT] train_cut:\", train_cut.shape, \" test_cut:\", test_cut.shape)\n\n    # 1フレーム特徴量+時系列特徴量\n    train_feat = add_perframe_features(train_cut)\n    train_feat = add_sequence_features(train_feat, window=WINDOW_SIZE)\n\n    test_feat  = add_perframe_features(test_cut)\n    test_feat  = add_sequence_features(test_feat, window=WINDOW_SIZE)\n\n    # 学習テーブル(スナップショット+将来ターゲット)\n    train_tbl = build_training_table(train_feat, train_output)\n    print(\"[TRAIN_TBL]\", train_tbl.shape)\n\n    # テスト用スナップショット(公開データでの動作確認用)\n    test_tbl  = build_test_snapshot(test_input, test_meta)\n    print(\"[TEST_TBL ]\", test_tbl.shape)\n\n    # 使用特徴量カラムの抽出\n    feature_cols = get_feature_columns(train_tbl.columns)\n    print(f\"feature_cols: {len(feature_cols)} cols\")\n\n    # time_to_ballでSHORT/LONGに分割\n    short_mask = train_tbl['time_to_ball'] < SHORT_TIME_THRESHOLD\n    train_short = train_tbl[short_mask].reset_index(drop=True)\n    train_long  = train_tbl[~short_mask].reset_index(drop=True)\n    print(\"[SPLIT] short:\", len(train_short), \" long:\", len(train_long))\n\n    # SHORTモデル学習\n    print(\"\\n=== TRAIN SHORT (time_to_ball < 1.0s) ===\")\n    short_bundle = train_models_for_split(train_short, feature_cols)\n\n    # LONGモデル学習\n    print(\"\\n=== TRAIN LONG (time_to_ball >= 1.0s) ===\")\n    long_bundle  = train_models_for_split(train_long , feature_cols)\n\n    # 推論用bundleを組み立て\n    final_bundle = {\n        \"feature_cols\": feature_cols,\n\n        # 学習済みCatBoostモデル群(ショート/ロング、X/Yそれぞれ)\n        \"short_models_x\": short_bundle[\"models_x\"],\n        \"short_models_y\": short_bundle[\"models_y\"],\n        \"long_models_x\":  long_bundle[\"models_x\"],\n        \"long_models_y\":  long_bundle[\"models_y\"],\n\n        # バケット補正テーブル(ショート/ロング両方を統合)\n        \"bucket_offset_3d\": {},  # 後でマージ\n        \"tbins\": short_bundle[\"bucket_offset\"][\"tbins\"],\n        \"dbins\": short_bundle[\"bucket_offset\"][\"dbins\"],\n\n        # 役割→role_bucketIDのマッピング (提出側でも使う)\n        \"role_map\": {\n            \"Targeted Receiver\": 0,\n            \"Defensive Coverage\": 1,\n            \"__other__\": 2\n        }\n    }\n\n    # バケット補正dictを統合\n    off_s = short_bundle[\"bucket_offset\"][\"dict\"]\n    off_l = long_bundle[\"bucket_offset\"][\"dict\"]\n    merged_off = {}\n    merged_off.update(off_l)\n    merged_off.update(off_s)\n    final_bundle[\"bucket_offset_3d\"] = merged_off\n\n    # bundleを保存\n    with open(output_path, \"wb\") as f:\n        pickle.dump(final_bundle, f)\n\n    print(f\"[SAVED] bundle -> {output_path}\")\n    return final_bundle\n\n\n# 実行: 学習して /kaggle/working/bundle.pkl を作る\nBUNDLE = create_and_save_bundle(\"/kaggle/working/bundle.pkl\")\nprint(\"Done. Bundle keys:\", list(BUNDLE.keys()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T01:13:49.28421Z","iopub.execute_input":"2025-11-02T01:13:49.284444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# セル4: ローカル検証(任意)\n# - 学習済みbundleを用いて、推論の後処理(バケット補正・左右反転戻し)まで\n#   1プレー分で通す検証ヘルパー。\n# - 提出セルと同じロジックをここでもう一度再現して\n#   \"少なくともローカルで動く\"ことを確認する目的。\n# - 本番Submitには不要。\n# =========================================================\n\ndef apply_bucket_offset_batch(px_raw, py_raw,\n                              t_ball_arr, def_dist_arr, role_b_arr,\n                              offset_dict, tbins, dbins):\n    adj_x = np.empty_like(px_raw, dtype=float)\n    adj_y = np.empty_like(py_raw, dtype=float)\n    for i in range(len(px_raw)):\n        tval = float(np.nan_to_num(t_ball_arr[i]))\n        dval = float(np.nan_to_num(def_dist_arr[i], nan=99.0))\n        rb   = int(role_b_arr[i])\n        tbin = np.digitize([tval], tbins, right=False)[0] - 1\n        dbin = np.digitize([dval], dbins, right=False)[0] - 1\n        tbin = int(np.clip(tbin, 0, len(tbins)-2))\n        dbin = int(np.clip(dbin, 0, len(dbins)-2))\n        off_x, off_y = offset_dict.get((tbin, dbin, rb), (0.0, 0.0))\n        adj_x[i] = px_raw[i] + off_x\n        adj_y[i] = py_raw[i] + off_y\n    return adj_x, adj_y\n\n\ndef run_catboost_ensemble_for_rows(df_rows: pd.DataFrame,\n                                   bundle,\n                                   is_short_split: bool):\n    \"\"\"\n    SHORT or LONG のどちらかのサブセットをまとめて推論する。\n    \"\"\"\n    if len(df_rows) == 0:\n        return np.array([],dtype=float), np.array([],dtype=float)\n\n    feat_cols = bundle[\"feature_cols\"]\n    local = df_rows.copy()\n    for c in feat_cols:\n        if c not in local.columns:\n            local[c] = 0.0\n    Xtest = local[feat_cols].fillna(0.0).to_numpy(dtype=np.float32)\n\n    bx  = local['x'].fillna(0.0).to_numpy()\n    by  = local['y'].fillna(0.0).to_numpy()\n    bvx = local['velocity_x'].fillna(0.0).to_numpy()\n    bvy = local['velocity_y'].fillna(0.0).to_numpy()\n\n    fr_future = pd.to_numeric(local['frame_id'], errors='coerce').fillna(0.0).to_numpy()\n    phys_x, phys_y = compute_physics_xy(bx, by, bvx, bvy, fr_future)\n\n    if is_short_split:\n        mx_list = bundle[\"short_models_x\"]\n        my_list = bundle[\"short_models_y\"]\n    else:\n        mx_list = bundle[\"long_models_x\"]\n        my_list = bundle[\"long_models_y\"]\n\n    pred_x_res_list = [m.predict(Xtest) for m in mx_list]\n    pred_y_res_list = [m.predict(Xtest) for m in my_list]\n    pred_x_resid = np.mean(pred_x_res_list, axis=0)\n    pred_y_resid = np.mean(pred_y_res_list, axis=0)\n\n    pred_x_raw = np.clip(phys_x + pred_x_resid, 0, FIELD_LENGTH)\n    pred_y_raw = np.clip(phys_y + pred_y_resid, 0, FIELD_WIDTH)\n\n    off_dict = bundle[\"bucket_offset_3d\"]\n    tbins    = bundle[\"tbins\"]\n    dbins    = bundle[\"dbins\"]\n\n    ttb = local['time_to_ball'].fillna(0.0).to_numpy()\n    ndd = local['nearest_defender_dist'].fillna(99.0).to_numpy()\n    rbk = local['role_bucket'].fillna(bundle[\"role_map\"][\"__other__\"]).to_numpy()\n\n    adj_x, adj_y = apply_bucket_offset_batch(\n        pred_x_raw, pred_y_raw, ttb, ndd, rbk, off_dict, tbins, dbins\n    )\n\n    return adj_x, adj_y\n\nprint(\"セル4: 検証用ヘルパー定義完了（任意で利用）\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# セル5: 提出用ノートブック\n# - Kaggleの評価APIに合わせてpredict()とサーバを定義するセル\n# - 学習済みbundle.pklを読み込み、time_to_ballでSHORT/LONGに分岐して予測\n# - 最後に攻撃方向を元に戻して(x,y)を返す\n#\n# 実際のSubmitノートブックでは、このセルだけを使用する想定。\n# あなたの自作dataset(=bundle.pklを含む)をAdd dataして、\n# BUNDLE_CANDIDATESパスを合わせておくこと。\n# run_local_gateway() でsubmission.csvをその場で生成できる。\n# =========================================================\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# cuDFアクセル(なくてもOKだが速いことがある)\nUSE_CUDF = False\ntry:\n    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n    import pandas as pd\n    import numpy as np\n    import cupy as cp\n    USE_CUDF = True\n    print(\"✅ cuDF pandas backend ENABLED (GPU DataFrame ops)\")\nexcept Exception:\n    import pandas as pd\n    import numpy as np\n    print(\"cuDF backend not available -> using pandas (CPU)\")\n\nimport pickle\nimport polars as pl\nfrom kaggle_evaluation import nfl_inference_server\n\nFIELD_LENGTH = 120.0\nFIELD_WIDTH  = 53.3\nFPS = 10.0\nN_FRAMES_KEEP = 10\nWINDOW_SIZE   = 3\nSHORT_TIME_THRESHOLD = 1.0\n\n# 学習済みbundle.pklの候補パス\nBUNDLE_CANDIDATES = [\n    \"/kaggle/input/nfl-bdb2026-bundle/bundle.pkl\",   # ←自分のDataset名に合わせて更新\n    \"/kaggle/input/nfl-bdb2026-final/bundle.pkl\",\n    \"/kaggle/input/bundle/bundle.pkl\",\n    \"/kaggle/input/bundle.pkl\",\n    \"/kaggle/working/bundle.pkl\",  # ローカル動作テスト向け\n]\n\n_BUNDLE_CACHE = None\n\n\ndef _find_bundle_path():\n    \"\"\"bundle.pklを探す。見つからなければエラー。\"\"\"\n    for cand in BUNDLE_CANDIDATES:\n        if os.path.exists(cand):\n            print(f\"[INFO] Found bundle at: {cand}\")\n            return cand\n    raise RuntimeError(\n        \"bundle.pkl が見つからない。'Add data' で学習済みモデル束Datasetを追加し、\"\n        \"BUNDLE_CANDIDATES を正しいパスにして下さい。\"\n    )\n\n\ndef load_bundle_once():\n    \"\"\"bundle.pklを一度だけロードし、グローバルキャッシュに保持。\"\"\"\n    global _BUNDLE_CACHE\n    if _BUNDLE_CACHE is None:\n        bp = _find_bundle_path()\n        with open(bp, \"rb\") as f:\n            _BUNDLE_CACHE = pickle.load(f)\n        print(\"[INFO] bundle loaded. keys:\", list(_BUNDLE_CACHE.keys()))\n        req = [\n            \"feature_cols\",\n            \"short_models_x\",\"short_models_y\",\n            \"long_models_x\",\"long_models_y\",\n            \"bucket_offset_3d\",\"tbins\",\"dbins\",\n            \"role_map\"\n        ]\n        for k in req:\n            if k not in _BUNDLE_CACHE:\n                raise RuntimeError(f\"Missing key '{k}' in bundle.pkl\")\n    return _BUNDLE_CACHE\n\n\ndef normalize_orientation_inplace(df: pd.DataFrame):\n    \"\"\"\n    推論時もtrainと同じく攻撃方向を右向きに正規化する。\n    \"\"\"\n    if 'play_direction' not in df.columns:\n        return\n    mask_left = df['play_direction'].astype(str).str.lower().eq('left')\n    if 'x' in df.columns:\n        df.loc[mask_left, 'x'] = FIELD_LENGTH - df.loc[mask_left, 'x']\n    if 'y' in df.columns:\n        df.loc[mask_left, 'y'] = FIELD_WIDTH  - df.loc[mask_left, 'y']\n    for ang in ['dir','o']:\n        if ang in df.columns:\n            df.loc[mask_left, ang] = (df.loc[mask_left, ang] + 180) % 360\n    if 'ball_land_x' in df.columns:\n        df.loc[mask_left, 'ball_land_x'] = FIELD_LENGTH - df.loc[mask_left, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[mask_left, 'ball_land_y'] = FIELD_WIDTH  - df.loc[mask_left, 'ball_land_y']\n\n\ndef compute_physics_xy(x, y, vx, vy, future_frame_id):\n    \"\"\"\n    等速モデル(学習時と同じ)\n    \"\"\"\n    t = np.nan_to_num(pd.to_numeric(future_frame_id, errors='coerce')) / FPS\n    px = x + vx * t\n    py = y + vy * t\n    px = np.clip(px, 0, FIELD_LENGTH)\n    py = np.clip(py, 0, FIELD_WIDTH)\n    return px, py\n\n\ndef role_bucket_id_inference(player_role: str, role_map: dict) -> int:\n    \"\"\"\n    提出サイドでも役割を3分類に落とす\n    \"\"\"\n    if player_role == \"Targeted Receiver\":\n        return role_map.get(\"Targeted Receiver\", 0)\n    if player_role == \"Defensive Coverage\":\n        return role_map.get(\"Defensive Coverage\", 1)\n    return role_map.get(\"__other__\", 2)\n\n\ndef keep_last_n_frames(df: pd.DataFrame, n_last=N_FRAMES_KEEP) -> pd.DataFrame:\n    \"\"\"\n    (game,play,nfl_id)ごとに末尾nフレームのみ保持\n    \"\"\"\n    if len(df) == 0:\n        return df.copy()\n    df = df.sort_values(['game_id','play_id','nfl_id','frame_id'])\n    tail_idx = df.groupby(['game_id','play_id','nfl_id']).tail(n_last).index\n    return df.loc[tail_idx].copy()\n\n\ndef add_perframe_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    学習時と同等の単発フレーム特徴量を付与\n    \"\"\"\n    out = df.copy()\n    if len(out)==0:\n        return out\n\n    for col in ['s','a','o','dir','ball_land_x','ball_land_y','player_height']:\n        if col in out.columns:\n            out[f'isna_{col}'] = out[col].isna().astype(np.int8)\n\n    dir_rad = np.radians(out['dir'].fillna(0.0))\n    spd = out['s'].fillna(0.0)\n    acc = out['a'].fillna(0.0)\n\n    out['velocity_x']     = spd * np.cos(dir_rad)\n    out['velocity_y']     = spd * np.sin(dir_rad)\n    out['acceleration_x'] = acc * np.cos(dir_rad)\n    out['acceleration_y'] = acc * np.sin(dir_rad)\n\n    if {'ball_land_x','ball_land_y','x','y'}.issubset(out.columns):\n        dx = out['ball_land_x'] - out['x']\n        dy = out['ball_land_y'] - out['y']\n        dist = np.sqrt(dx*dx + dy*dy)\n        out['dist_to_ball'] = dist\n        out['angle_to_ball'] = np.arctan2(dy, dx)\n        out['velocity_toward_ball'] = (\n            out['velocity_x'] * np.cos(out['angle_to_ball']) +\n            out['velocity_y'] * np.sin(out['angle_to_ball'])\n        )\n    else:\n        out['dist_to_ball'] = np.nan\n        out['angle_to_ball'] = np.nan\n        out['velocity_toward_ball'] = np.nan\n\n    if 'num_frames_output' in out.columns:\n        out['time_to_ball'] = out['num_frames_output'] / FPS\n    else:\n        out['time_to_ball'] = np.nan\n\n    diff = np.abs(out['o'].fillna(0.0) - out['dir'].fillna(0.0))\n    out['orientation_diff'] = np.minimum(diff, 360 - diff)\n\n    out['role_targeted_receiver']  = (out['player_role'] == 'Targeted Receiver').astype(np.int8)\n    out['role_defensive_coverage'] = (out['player_role'] == 'Defensive Coverage').astype(np.int8)\n    out['role_passer']             = (out['player_role'] == 'Passer').astype(np.int8)\n    out['side_offense']            = (out['player_side'] == 'Offense').astype(np.int8)\n\n    ht = out['player_height'].fillna('0-0').astype(str).str.split('-', n=1, expand=True)\n    ft  = pd.to_numeric(ht[0], errors='coerce').fillna(0)\n    inc = pd.to_numeric(ht[1], errors='coerce').fillna(0)\n    height_in = ft * 12 + inc\n    out['height_inches'] = height_in\n    with np.errstate(divide='ignore', invalid='ignore'):\n        out['bmi'] = (out['player_weight'] / (np.maximum(height_in,1e-6)**2)) * 703\n\n    out['speed_squared'] = spd**2\n    out['accel_magnitude'] = np.sqrt(out['acceleration_x']**2 + out['acceleration_y']**2)\n    out['velocity_alignment'] = np.cos(out['angle_to_ball'] - np.radians(out['dir'].fillna(0.0)))\n\n    t_ball = out['time_to_ball'].fillna(0.0)\n    out['expected_x_at_ball'] = out['x'] + out['velocity_x'] * t_ball\n    out['expected_y_at_ball'] = out['y'] + out['velocity_y'] * t_ball\n    out['error_from_ball_x'] = out['expected_x_at_ball'] - out['ball_land_x']\n    out['error_from_ball_y'] = out['expected_y_at_ball'] - out['ball_land_y']\n    out['error_from_ball']   = np.sqrt(out['error_from_ball_x']**2 + out['error_from_ball_y']**2)\n\n    out['momentum_x'] = out['player_weight'].fillna(200.0) * out['velocity_x']\n    out['momentum_y'] = out['player_weight'].fillna(200.0) * out['velocity_y']\n    out['kinetic_energy'] = 0.5 * out['player_weight'].fillna(200.0) * out['speed_squared']\n\n    ang_ball_deg = np.degrees(out['angle_to_ball'])\n    ad = np.abs(out['o'].fillna(0.0) - ang_ball_deg)\n    out['angle_diff'] = np.minimum(ad, 360 - ad)\n\n    out['time_squared'] = out['time_to_ball']**2\n    out['dist_squared'] = out['dist_to_ball']**2\n    out['weighted_dist_by_time'] = out['dist_to_ball'] / (out['time_to_ball'] + 0.1)\n\n    out['dist_to_sideline'] = np.minimum(out['y'], FIELD_WIDTH - out['y'])\n    out['dist_to_endzone']  = FIELD_LENGTH - out['x']\n\n    for col in ['dir','o']:\n        rad = np.radians(out[col].fillna(0.0))\n        out[f'{col}_sin'] = np.sin(rad)\n        out[f'{col}_cos'] = np.cos(rad)\n    rad_ball = out['angle_to_ball'].fillna(0.0)\n    out['angle_to_ball_sin'] = np.sin(rad_ball)\n    out['angle_to_ball_cos'] = np.cos(rad_ball)\n\n    return out\n\n\ndef add_sequence_features(df: pd.DataFrame, window=WINDOW_SIZE) -> pd.DataFrame:\n    \"\"\"\n    学習時と同じラグ/ローリング/デルタ特徴を付与\n    \"\"\"\n    if len(df)==0:\n        return df.copy()\n\n    out = df.sort_values(['game_id','play_id','nfl_id','frame_id']).copy()\n    gcols = ['game_id','play_id','nfl_id']\n\n    for lag in [1,2,3][:window]:\n        for col in ['x','y','velocity_x','velocity_y','s','a']:\n            if col in out.columns:\n                out[f'{col}_lag{lag}'] = out.groupby(gcols)[col].shift(lag)\n\n    for col in ['x','y','velocity_x','velocity_y','s']:\n        if col in out.columns:\n            roll_mean = out.groupby(gcols)[col].rolling(window, min_periods=1).mean()\n            roll_std  = out.groupby(gcols)[col].rolling(window, min_periods=1).std()\n            roll_mean = roll_mean.reset_index(level=list(range(len(gcols))), drop=True)\n            roll_std  = roll_std.reset_index(level=list(range(len(gcols))), drop=True)\n            out[f'{col}_roll{window}'] = roll_mean\n            out[f'{col}_std{window}']  = roll_std.fillna(0.0)\n\n    for col in ['velocity_x','velocity_y']:\n        if col in out.columns:\n            out[f'{col}_delta'] = out.groupby(gcols)[col].diff()\n\n    seq_cols = [\n        c for c in out.columns\n        if ('_lag' in c) or (f'_roll{window}' in c) or (f'_std{window}' in c) or c.endswith('_delta')\n    ]\n    out[seq_cols] = out[seq_cols].fillna(0.0)\n\n    return out\n\n\ndef add_context_on_finalframe(final_snap: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    スナップ直前フレームでQB/DFの空間コンテキストを付与(学習時と同じ)\n    \"\"\"\n    snap = final_snap.copy()\n    if len(snap)==0:\n        for c in [\n            'dist_to_qb','rel_speed_to_qb',\n            'nearest_defender_dist','nearest_defender_rel_speed',\n            'def_count_r3','def_count_r5','def_count_r7'\n        ]:\n            snap[c]=0.0\n        return snap\n\n    need_cols = ['game_id','play_id','nfl_id','x','y',\n                 'velocity_x','velocity_y','player_role','player_side']\n    for c in need_cols:\n        if c not in snap.columns:\n            snap[c]=np.nan\n\n    for c in [\n        'dist_to_qb','rel_speed_to_qb',\n        'nearest_defender_dist','nearest_defender_rel_speed',\n        'def_count_r3','def_count_r5','def_count_r7'\n    ]:\n        snap[c]=np.nan\n\n    for (gid,pid), g in snap.groupby(['game_id','play_id']):\n        idxs = g.index.values\n\n        qb_mask = (g['player_role'].values=='Passer')\n        if qb_mask.any():\n            qb_row = g[qb_mask].iloc[0]\n            qb_x,qb_y = qb_row['x'],qb_row['y']\n            qb_vx,qb_vy = qb_row['velocity_x'],qb_row['velocity_y']\n        else:\n            qb_x=qb_y=qb_vx=qb_vy=np.nan\n\n        def_mask = (g['player_side'].values=='Defense') | (g['player_role'].values=='Defensive Coverage')\n        defenders = g[def_mask]\n\n        def_xy  = defenders[['x','y']].to_numpy(dtype=float) if len(defenders)>0 else np.zeros((0,2))\n        def_vel = defenders[['velocity_x','velocity_y']].to_numpy(dtype=float) if len(defenders)>0 else np.zeros((0,2))\n        def_ids = defenders['nfl_id'].to_numpy(dtype=float) if len(defenders)>0 else np.zeros((0,))\n\n        p_xy   = g[['x','y']].to_numpy(dtype=float)\n        p_vel  = g[['velocity_x','velocity_y']].to_numpy(dtype=float)\n        p_ids  = g['nfl_id'].to_numpy(dtype=float)\n\n        if not np.isnan(qb_x):\n            vec_qb = np.stack([qb_x - p_xy[:,0], qb_y - p_xy[:,1]], axis=1)\n            dist_qb = np.linalg.norm(vec_qb, axis=1)\n            u_qb = np.zeros_like(vec_qb)\n            nz = dist_qb>1e-6\n            u_qb[nz] = vec_qb[nz]/dist_qb[nz,None]\n            v_rel_qb = p_vel - np.array([qb_vx,qb_vy])\n            rel_speed_qb = (v_rel_qb * u_qb).sum(axis=1)\n        else:\n            dist_qb = np.zeros(len(idxs))\n            rel_speed_qb = np.zeros(len(idxs))\n\n        if len(defenders)>0:\n            diff = p_xy[:,None,:] - def_xy[None,:,:]\n            dists= np.sqrt((diff**2).sum(axis=2))\n\n            for ii,pidv in enumerate(p_ids):\n                same=(def_ids==pidv)\n                if same.any():\n                    dists[ii, same]=np.inf\n\n            min_idx = np.argmin(dists,axis=1)\n            min_dist= dists[np.arange(len(min_idx)),min_idx]\n\n            nn_vel = def_vel[min_idx] if def_vel.shape[0] else np.zeros_like(p_vel)\n\n            vec_df = def_xy[min_idx]-p_xy\n            norm_df= np.linalg.norm(vec_df,axis=1)\n            u_df   = np.zeros_like(vec_df)\n            nz2    = norm_df>1e-6\n            u_df[nz2]=vec_df[nz2]/norm_df[nz2,None]\n\n            v_rel_df = p_vel-nn_vel\n            rel_speed_df = (v_rel_df*u_df).sum(axis=1)\n\n            def_counts_r3 = (dists<=3.0).sum(axis=1)\n            def_counts_r5 = (dists<=5.0).sum(axis=1)\n            def_counts_r7 = (dists<=7.0).sum(axis=1)\n        else:\n            min_dist      = np.zeros(len(idxs))\n            rel_speed_df  = np.zeros(len(idxs))\n            def_counts_r3 = np.zeros(len(idxs))\n            def_counts_r5 = np.zeros(len(idxs))\n            def_counts_r7 = np.zeros(len(idxs))\n\n        snap.loc[idxs,'dist_to_qb']                 = dist_qb\n        snap.loc[idxs,'rel_speed_to_qb']            = rel_speed_qb\n        snap.loc[idxs,'nearest_defender_dist']      = min_dist\n        snap.loc[idxs,'nearest_defender_rel_speed'] = rel_speed_df\n        snap.loc[idxs,'def_count_r3']               = def_counts_r3\n        snap.loc[idxs,'def_count_r5']               = def_counts_r5\n        snap.loc[idxs,'def_count_r7']               = def_counts_r7\n\n    for c in [\n        'dist_to_qb','rel_speed_to_qb',\n        'nearest_defender_dist','nearest_defender_rel_speed',\n        'def_count_r3','def_count_r5','def_count_r7'\n    ]:\n        snap[c]=snap[c].fillna(0.0)\n\n    return snap\n\n\ndef build_snapshot_and_merge(test_df: pd.DataFrame,\n                             test_input_df: pd.DataFrame,\n                             role_map: dict) -> pd.DataFrame:\n    \"\"\"\n    ゲートウェイから受け取ったバッチ(test, test_input)を、\n    学習時と同じスナップショット特徴量の形式にまとめる。\n    \"\"\"\n    # 攻撃方向を右向きへ正規化した上で、最後のNフレームだけ残す\n    ti = test_input_df.copy()\n    normalize_orientation_inplace(ti)\n    ti = keep_last_n_frames(ti, N_FRAMES_KEEP)\n    ti_feat = add_perframe_features(ti)\n    ti_feat = add_sequence_features(ti_feat, window=WINDOW_SIZE)\n\n    # 各選手について「最終フレーム(投球直前)」のみ残す\n    tmp = ti_feat.copy()\n    tmp['__fid__'] = pd.to_numeric(tmp['frame_id'], errors='coerce')\n    idx_last = tmp.groupby(['game_id','play_id','nfl_id'])['__fid__'].idxmax()\n    final_snap = ti_feat.loc[idx_last].copy().reset_index(drop=True)\n\n    # 後で未来フレームIDと区別しやすいように名称を分ける\n    final_snap = final_snap.rename(columns={\n        'frame_id': 'frame_id_y',\n        'play_direction': 'play_direction_snap',\n    })\n\n    # スナップ直前フレームにコンテキスト特徴(DF密度など)を注入\n    final_snap = add_context_on_finalframe(final_snap)\n\n    # Gatewayのtest(未来フレームごとの行)と結合\n    test_local = test_df.copy().rename(columns={'frame_id':'frame_id_x'})\n    merged = test_local.merge(\n        final_snap,\n        on=['game_id','play_id','nfl_id'],\n        how='left'\n    )\n\n    # role_bucket(補正テーブルのキー)\n    merged['role_bucket'] = merged['player_role'].apply(\n        lambda r: role_bucket_id_inference(r, role_map)\n    ).astype(np.int8)\n\n    # 元の方向(左右)を保持しておく。最後に戻すため\n    merged['play_direction'] = merged['play_direction_snap']\n\n    # いくつかの特徴はNaNを0埋め\n    for c in ['time_to_ball','nearest_defender_dist']:\n        if c in merged.columns:\n            merged[c] = merged[c].fillna(0.0)\n\n    return merged\n\n\ndef apply_bucket_offset_batch(px_raw, py_raw,\n                              t_ball_arr, def_dist_arr, role_b_arr,\n                              offset_dict, tbins, dbins):\n    \"\"\"\n    (time_to_ball_bin, defender_dist_bin, role_bucket)ごとの\n    系統誤差オフセットを加えて最終補正する。\n    \"\"\"\n    adj_x = np.empty_like(px_raw, dtype=float)\n    adj_y = np.empty_like(py_raw, dtype=float)\n    for i in range(len(px_raw)):\n        tval = float(np.nan_to_num(t_ball_arr[i]))\n        dval = float(np.nan_to_num(def_dist_arr[i], nan=99.0))\n        rb   = int(role_b_arr[i])\n        tbin = np.digitize([tval], tbins, right=False)[0]-1\n        dbin = np.digitize([dval], dbins, right=False)[0]-1\n        tbin = int(np.clip(tbin,0,len(tbins)-2))\n        dbin = int(np.clip(dbin,0,len(dbins)-2))\n        off_x, off_y = offset_dict.get((tbin,dbin,rb),(0.0,0.0))\n        adj_x[i] = px_raw[i] + off_x\n        adj_y[i] = py_raw[i] + off_y\n    return adj_x, adj_y\n\n\ndef run_catboost_ensemble_for_rows(df_rows: pd.DataFrame,\n                                   bundle,\n                                   is_short_split: bool):\n    \"\"\"\n    SHORT行 or LONG行 だけをまとめて推論する。\n    \"\"\"\n    if len(df_rows)==0:\n        return np.array([],dtype=float), np.array([],dtype=float)\n\n    feat_cols = bundle[\"feature_cols\"]\n    local = df_rows.copy()\n    for c in feat_cols:\n        if c not in local.columns:\n            local[c] = 0.0\n    Xtest = local[feat_cols].fillna(0.0).to_numpy(dtype=np.float32)\n\n    # 等速物理ベースライン\n    bx  = local['x'].fillna(0.0).to_numpy()\n    by  = local['y'].fillna(0.0).to_numpy()\n    bvx = local['velocity_x'].fillna(0.0).to_numpy()\n    bvy = local['velocity_y'].fillna(0.0).to_numpy()\n\n    # 未来フレームオフセット(frame_id_y=スナップ時; frame_id_x=予測対象フレーム)\n    if 'frame_id_y' in local.columns:\n        horizon_frames = pd.to_numeric(local['frame_id_y'], errors='coerce').fillna(0.0).to_numpy()\n    else:\n        horizon_frames = pd.to_numeric(local.get('frame_id_x', 0), errors='coerce').fillna(0.0).to_numpy()\n\n    phys_x, phys_y = compute_physics_xy(bx, by, bvx, bvy, horizon_frames)\n\n    if is_short_split:\n        models_x = bundle[\"short_models_x\"]\n        models_y = bundle[\"short_models_y\"]\n    else:\n        models_x = bundle[\"long_models_x\"]\n        models_y = bundle[\"long_models_y\"]\n\n    # CatBoostアンサンブルの平均\n    pred_x_res_list = [m.predict(Xtest) for m in models_x]\n    pred_y_res_list = [m.predict(Xtest) for m in models_y]\n    pred_x_resid = np.mean(pred_x_res_list, axis=0)\n    pred_y_resid = np.mean(pred_y_res_list, axis=0)\n\n    pred_x_raw = np.clip(phys_x + pred_x_resid, 0, FIELD_LENGTH)\n    pred_y_raw = np.clip(phys_y + pred_y_resid, 0, FIELD_WIDTH)\n\n    # バケット補正で系統バイアス修正\n    off_dict = bundle[\"bucket_offset_3d\"]\n    tbins    = bundle[\"tbins\"]\n    dbins    = bundle[\"dbins\"]\n\n    ttb = local['time_to_ball'].fillna(0.0).to_numpy()\n    ndd = local['nearest_defender_dist'].fillna(99.0).to_numpy()\n    rbk = local['role_bucket'].fillna(bundle[\"role_map\"][\"__other__\"]).to_numpy()\n\n    adj_x, adj_y = apply_bucket_offset_batch(\n        pred_x_raw, pred_y_raw,\n        ttb, ndd, rbk,\n        off_dict, tbins, dbins\n    )\n\n    return adj_x, adj_y\n\n\ndef infer_batch(test_pl: pl.DataFrame,\n                test_input_pl: pl.DataFrame,\n                bundle) -> pd.DataFrame:\n    \"\"\"\n    Kaggle Gatewayから渡される1バッチ(test, test_input)に対して、\n    最終的な(x,y)予測を返す。\n    \"\"\"\n    test_df = test_pl.to_pandas()\n    test_input_df = test_input_pl.to_pandas()\n\n    merged = build_snapshot_and_merge(\n        test_df=test_df,\n        test_input_df=test_input_df,\n        role_map=bundle[\"role_map\"]\n    )\n\n    # time_to_ballでSHORT/LONGに分岐\n    short_mask = merged['time_to_ball'] < SHORT_TIME_THRESHOLD\n    long_mask  = ~short_mask\n\n    merged_short = merged[short_mask].copy()\n    merged_long  = merged[long_mask].copy()\n\n    pred_sx, pred_sy = run_catboost_ensemble_for_rows(\n        merged_short, bundle, is_short_split=True\n    )\n    pred_lx, pred_ly = run_catboost_ensemble_for_rows(\n        merged_long,  bundle, is_short_split=False\n    )\n\n    merged['pred_x_right'] = np.nan\n    merged['pred_y_right'] = np.nan\n    merged.loc[short_mask,'pred_x_right'] = pred_sx\n    merged.loc[short_mask,'pred_y_right'] = pred_sy\n    merged.loc[long_mask ,'pred_x_right'] = pred_lx\n    merged.loc[long_mask ,'pred_y_right'] = pred_ly\n\n    # 攻撃方向を元(左/右)に戻す：学習時は右正規化なので、\n    # leftプレイは反転して提出\n    mask_left_orig = merged['play_direction'].astype(str).str.lower().eq('left')\n    sub_x = np.where(mask_left_orig,\n                     FIELD_LENGTH - merged['pred_x_right'],\n                     merged['pred_x_right'])\n    sub_y = np.where(mask_left_orig,\n                     FIELD_WIDTH  - merged['pred_y_right'],\n                     merged['pred_y_right'])\n\n    out_df = pd.DataFrame({\n        \"x\": sub_x,\n        \"y\": sub_y\n    }, index=merged.index)\n\n    # Kaggle APIは test(=test_df)の順序と同じ長さを要求するので整形して返す\n    out_df = out_df.reindex(test_df.index)\n    return out_df\n\n\ndef predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"\n    Kaggle評価サーバから呼ばれるエントリポイント。\n    test/test_inputはPolars DataFrame。\n    同じ並び順でx,y列を返す必要がある。\n    \"\"\"\n    bundle = load_bundle_once()\n    preds_pd = infer_batch(test, test_input, bundle)\n    assert isinstance(preds_pd, pd.DataFrame)\n    assert len(preds_pd) == len(test)\n    return preds_pd\n\n\n# Kaggleが要求するインターフェース:\n# - コンテスト本番環境では inference_server.serve() が呼ばれる\n# - Notebook上でのローカルテストは run_local_gateway() で submission.csv を吐き出す\ninference_server = nfl_inference_server.NFLInferenceServer(predict)\n\n# 起動テスト:\n_ = load_bundle_once()\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    # 本番評価モード\n    inference_server.serve()\nelse:\n    # ローカルテストモード: 公開データに対してpredict()を叩いて\n    # submission.csv がワーキングディレクトリに生成される\n    inference_server.run_local_gateway((\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\",))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}