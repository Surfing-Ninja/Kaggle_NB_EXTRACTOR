{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"},{"sourceId":13421249,"sourceType":"datasetVersion","datasetId":8517257,"isSourceIdPinned":true},{"sourceId":13429964,"sourceType":"datasetVersion","datasetId":8512056}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================================================================\n# ÂçïÂÖÉ 3Ôºö‰ΩøÁî®ÊúÄ‰ºòÂèÇÊï∞ËÆ≠ÁªÉÂπ∂Êé®ÁêÜÁîüÊàê submission.csv\n# ======================================================================\n\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"19\n167.8s 118 model_x_val_list:[0.6287498120885685]\n\n466.1s 342 model_x_val_list:[0.6876166711048914]\n\n726.6s 538 model_x_val_list:[0.6191256934265665]\n\n1066.2s 790 model_x_val_list:[0.6408800487589146]\n\n1305.3s 971 model_x_val_list:[0.655929250511712]\n\n\n42\n1628.7s 1194 model_x_val_list:[0.6230685653714565]\n\n1946.9s 1397 model_x_val_list:[0.6816828236837671]\n\n2255.7s 1594 model_x_val_list:[0.6380264214753033]\n\n2580.3s 1801 model_x_val_list:[0.634864329873299]\n\n2924.6s 2020 model_x_val_list:[0.6709610554462989]\n\n\n64\n3383.7s 2263 model_x_val_list:[0.6499298537217154]\n\n3844.3s 2455 model_x_val_list:[0.6796746843223926]\n\n4334.1s 2660 model_x_val_list:[0.6356689155008671]\n\n5013.2s 2941 model_x_val_list:[0.622604983644486]\n\n5565.2s 3172 model_x_val_list:[0.6291349753625153]\n\n\n\n364.2s 263 model_y_val_list: [0.561663060031395]\n\n610.0s 450 model_y_val_list: [0.6099795133957178]\n\n882.5s 653 model_y_val_list: [0.5821781166620897]\n\n1194.2s 885 model_y_val_list: [0.5797950492099755]\n\n1458.6s 1083 model_y_val_list: [0.6141496390569589]\n\n\n1814.7s 1311 model_y_val_list: [0.5869238502917573]\n\n2108.5s 1499 model_y_val_list: [0.6196303800034224]\n\n2436.7s 1708 model_y_val_list: [0.5611591410229677]\n\n2797.3s 1937 model_y_val_list: [0.58890986012815]\n\n3141.5s 2156 model_y_val_list: [0.5733821356221309]\n\n\n3647.3s 2371 model_y_val_list: [0.5816903462069517]\n\n4140.2s 2578 model_y_val_list: [0.6027354114904219]\n\n4627.9s 2782 model_y_val_list: [0.5441324146492953]\n\n5284.7s 3054 model_y_val_list: [0.6025653593135782]\n\n5806.7s 3271 model_y_val_list: [0.6014968684030821]\n\n# Now all params are found with opt \n\nSeed 19: X values: [0.6682479069545363, 0.6271058951198512, 0.6420836529418633, 0.6315098601225932, 0.6088514363599971], Y values: [0.6105412800407741, 0.5685900346321365, 0.5800642574688976, 0.5424466438549035, 0.6065578838845658]\nSeed 42: X values: [0.635550935708804, 0.6618808185229955, 0.6601690848073783, 0.655013706522254, 0.6353804208791102], Y values: [0.5996811457375933, 0.5767373171936827, 0.5709060747287746, 0.5777591002546802, 0.5833306324308463]\nSeed 64: X values: [0.6522520906518646, 0.6523542004264561, 0.6150006706401064, 0.6201418607790589, 0.654220539020383], Y values: [0.5700290755981909, 0.5508604222290143, 0.5747010325064147, 0.5947516842287439, 0.620652449164692]\nSeed 89: X values: [0.6522301035613204, 0.6347463148530911, 0.6482107260680275, 0.6229904400103146, 0.6171141605262076], Y values: [0.5757633951407591, 0.5740469305432565, 0.5831753725979005, 0.5951615811987084, 0.6084288929242135]\n","metadata":{}},{"cell_type":"code","source":"# -------------------------------\n# ÂÆö‰πâÊ®°Âûã‰∏éÊçüÂ§±ÂáΩÊï∞Ôºà‰∏éÂçïÂÖÉ 2 Áõ∏ÂêåÔºâ\n# -------------------------------\nclass ResidualGRU(nn.Module):\n    def __init__(self, input_dim, horizon, hidden_size=256, num_layers=2, dropout=0.1, nheads=4):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n        self.ln = nn.LayerNorm(hidden_size)\n        self.attn = nn.MultiheadAttention(hidden_size, nheads, batch_first=True)\n        self.q = nn.Parameter(torch.randn(1,1,hidden_size))\n        self.head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, horizon)\n        )\n    def forward(self, x):\n        h, _ = self.gru(x)\n        h = self.ln(h)\n        B = h.size(0)\n        q = self.q.expand(B,-1,-1)\n        ctx, _ = self.attn(q, h, h)\n        out = self.head(ctx.squeeze(1))\n        return torch.cumsum(out, dim=1)\n\nclass TemporalHuberLoss(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.02, alpha=0.0):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n        self.alpha = alpha\n    def forward(self, pred, target, mask):\n        err = pred - target\n        abs_err = torch.abs(err)\n        huber = torch.where(abs_err <= self.delta,\n                            0.5*err*err,\n                            self.delta*(abs_err - 0.5*self.delta))\n        L = pred.size(1)\n        t = torch.arange(L, device=pred.device).float()\n        decay = torch.exp(-self.time_decay * t).view(1,L)\n        weights = (1.0 + t)**self.alpha * decay\n        huber = huber * weights\n        mask  = mask  * weights\n        return (huber * mask).sum() / (mask.sum() + 1e-8)\n\n# ÊûÑÂª∫ÊâπÊ¨°ÂáΩÊï∞Ôºà‰ΩøÁî®Â∑≤Â°´ÂÖÖÊï∞ÊçÆÔºâ\ndef build_batches_padded(X, y, mask, batch_size):\n    batches = []\n    n = len(X)\n    for i in range(0, n, batch_size):\n        end = min(i+batch_size, n)\n        xs = torch.tensor(np.stack(X[i:end]), dtype=torch.float32)\n        ys = torch.tensor(np.stack(y[i:end]), dtype=torch.float32)\n        ms = torch.tensor(np.stack(mask[i:end]), dtype=torch.float32)\n        batches.append((xs, ys, ms))\n    return batches\nfrom tqdm.auto import tqdm\nfrom tqdm.auto import tqdm\n\ndef train_residual_model_padded(\n    X_tr, y_tr, mask_tr,\n    X_va, y_va, mask_va,\n    input_dim, horizon,\n    hidden_size, num_layers,\n    dropout, lr, batch_size,\n    delta, time_decay, alpha,\n    max_epochs=150, patience=20\n):\n    \"\"\"\n    Â∏¶ tqdm + official RMSE È™åËØÅÊòæÁ§∫ÁöÑÁâàÊú¨\n    ËæìÂÖ•ËæìÂá∫Êé•Âè£‰∏éÂéüÂáΩÊï∞ÂÆåÂÖ®‰∏ÄËá¥\n    \"\"\"\n    import torch\n    import torch.nn as nn\n    import numpy as np\n    from tqdm import tqdm\n    official_rmse_list = []\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ResidualGRU(input_dim, horizon, hidden_size, num_layers, dropout).to(device)\n    criterion = TemporalHuberLoss(delta=delta, time_decay=time_decay, alpha=alpha)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n\n    tr_batches = build_batches_padded(X_tr, y_tr, mask_tr, batch_size)\n    va_batches = build_batches_padded(X_va, y_va, mask_va, batch_size)\n\n    best_loss, best_state, bad = float('inf'), None, 0\n\n    for epoch in tqdm(range(1, max_epochs + 1), desc=f\"Training GRU ({hidden_size})\"):\n        # ---------------------\n        # ËÆ≠ÁªÉÈò∂ÊÆµ\n        # ---------------------\n        model.train()\n        train_losses = []\n        for bx, by, bm in tr_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            pred = model(bx)\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n        mean_tr = float(np.mean(train_losses))\n\n        # ---------------------\n        # È™åËØÅÈò∂ÊÆµ\n        # ---------------------\n        model.eval()\n        val_losses, preds_list, trues_list, masks_list = [], [], [], []\n        with torch.no_grad():\n            for bx, by, bm in va_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                val_losses.append(criterion(pred, by, bm).item())\n                preds_list.append(pred.cpu().numpy())\n                trues_list.append(by.cpu().numpy())\n                masks_list.append(bm.cpu().numpy())\n\n        val_mean = float(np.mean(val_losses))\n        scheduler.step(val_mean)\n\n        # ---------------------\n        # È¢ùÂ§ñÔºöËÆ°ÁÆó Official RMSE\n        # ---------------------\n        try:\n            preds_all = np.concatenate(preds_list, axis=0)\n            trues_all = np.concatenate(trues_list, axis=0)\n            masks_all = np.concatenate(masks_list, axis=0)\n\n            mask_sum = np.sum(masks_all)\n            if mask_sum > 0:\n                mse = np.sum(((preds_all - trues_all) ** 2) * masks_all) / (mask_sum + 1e-8)\n                official_rmse = float(np.sqrt(mse))\n            else:\n                official_rmse = np.nan\n        except Exception as e:\n            print(f\"[Warning] Official RMSEËÆ°ÁÆóÂá∫Èîô: {e}\")\n            official_rmse = np.nan\n\n        # ---------------------\n        # Early Stopping\n        # ---------------------\n        if val_mean < best_loss:\n            best_loss, bad = val_mean, 0\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            bad += 1\n            if bad >= patience:\n                print(f\"Early stop at epoch {epoch}\")\n                break\n\n        # ---------------------\n        # ËæìÂá∫‰ø°ÊÅØ\n        # ---------------------\n        tqdm.write(f\"Epoch {epoch:03d} | train={mean_tr:.4f} | val={val_mean:.4f} | offRMSE={official_rmse:.4f}\")\n\n    # ---------------------\n    # ÊÅ¢Â§çÊúÄ‰Ω≥ÊùÉÈáç\n    # ---------------------\n    if best_state:\n        official_rmse_list.append(official_rmse)\n        model.load_state_dict(best_state)\n    print(f\"Official RMSE: {official_rmse_list} for this kfold model\")\n    return model,official_rmse_list   ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# ÂçïÂÖÉ 2ÔºöÂ§öÁßçÂ≠ê„ÄÅÂ§öÊäòÊ®°ÂûãËÆ≠ÁªÉ‰∏é‰øùÂ≠ò\n# ======================================================================\n\nimport pickle\nimport json\nimport re\nimport torch\nfrom pathlib import Path\n\n# --- 1. Setup and Seed Discovery ---\n\n# Define the cache directory where data and parameter files are stored\nCACHE_DIR = Path(\"/kaggle/input/cache-feature-engineer-nfl\")\nSEED_DIR = Path(\"/kaggle/input/packages-json-params-nfl\")\n# Use regular expressions to extract seed numbers from filenames\nseed_pattern = re.compile(r'seed_(\\d+)')\n\n# Find all parameter files and extract their seed numbers\nparam_files = {p.name: seed_pattern.search(p.name).group(1)\n               for p in SEED_DIR.glob(\"best_params_seed_*.json\") if seed_pattern.search(p.name)}\n\n# Find all data files and extract their seed numbers\ndata_files = {p.name: seed_pattern.search(p.name).group(1)\n              for p in CACHE_DIR.glob(\"folds_data_seed_*.pkl\") if seed_pattern.search(p.name)}\n\n# Identify the seeds that have BOTH a parameter file and a data file\nvalid_seeds = sorted(list(set(param_files.values()) & set(data_files.values())), key=int)\n\nif not valid_seeds:\n    raise FileNotFoundError(\"No matching seed files found for both params (*.json) and data (*.pkl). Please check the filenames in CACHE_DIR.\")\n\nprint(f\"‚úÖ Found {len(valid_seeds)} valid seeds to process: {valid_seeds}\\n\")\n\n\n# --- 2. Main Training Loop for Each Seed ---\n\n# Dictionaries to store the trained models from all seeds\nall_models_x = {}\nall_models_y = {} # CORRECTED: Initialized all_models_y\n\n# Define the main output directory for saved models\noutput_dir = Path(\"/kaggle/working/saved_models/\")\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Iterate through each valid seed\nfor seed in valid_seeds:\n    print(f\"--- Starting training for SEED: {seed} ---\")\n    # Load the specific parameter file for the current seed\n    param_filepath = SEED_DIR / f\"best_params_seed_{seed}.json\"\n    with open(param_filepath, 'r') as f:\n        best_params = json.load(f)\n    print(f\"Loaded parameters from: {param_filepath.name}\")\n\n    # Load the specific data file for the current seed\n    data_filepath = CACHE_DIR / f\"folds_data_seed_{seed}.pkl\"\n    with open(data_filepath, 'rb') as f:\n        data = pickle.load(f)\n    print(f\"Loaded data from: {data_filepath.name}\")\n\n    # Unpack the data dictionary\n    folds_data = data['folds_data']\n    horizon    = data['horizon']\n    feature_cols = data['feature_cols']\n    idx_x = data['idx_x']\n    idx_y = data['idx_y']\n\n    # Lists to hold the models for the current seed's folds\n    models_x_for_seed, models_y_for_seed = [], []\n\n    # --- Inner loop to train on each fold ---\n    for i, fd in enumerate(folds_data):\n        print(f\"\\n--- Training Fold {i+1}/{len(folds_data)} for Seed {seed} ---\")\n        X_tr    = fd['X_train']\n        X_va    = fd['X_val']\n        ydx_tr  = fd['y_dx_train']\n        ydx_va  = fd['y_dx_val']\n        ydy_tr  = fd['y_dy_train']\n        ydy_va  = fd['y_dy_val']\n        m_tr    = fd['mask_train']\n        m_va    = fd['mask_val']\n        input_dim = X_tr[0].shape[-1]\n\n        # Train Œîx model\n        model_x, model_x_val_list = train_residual_model_padded(\n            X_tr, ydx_tr, m_tr, X_va, ydx_va, m_va,\n            input_dim=input_dim, horizon=horizon, **best_params,\n            max_epochs=180, patience=25\n        )\n        print(f\"model_x_val_list:{model_x_val_list}\")\n\n        # Train Œîy model\n        model_y, model_y_val_list = train_residual_model_padded(\n            X_tr, ydy_tr, m_tr, X_va, ydy_va, m_va,\n            input_dim=input_dim, horizon=horizon, **best_params,\n            max_epochs=180, patience=25\n        )\n        print(f\"model_y_val_list: {model_y_val_list}\")\n\n        models_x_for_seed.append(model_x)\n        models_y_for_seed.append(model_y)\n\n    # Store the list of trained models for the current seed in the main dictionary\n    all_models_x[seed] = models_x_for_seed\n    all_models_y[seed] = models_y_for_seed\n    print(f\"\\n‚úÖ Finished training for Seed {seed}. Stored {len(models_x_for_seed)} fold models.\")\n\n    # --- 3. ADDED: Save the trained models for the current seed ---\n    print(f\"üíæ Saving models for Seed {seed}...\")\n    seed_output_dir = output_dir / f\"seed_{seed}\"\n    seed_output_dir.mkdir(exist_ok=True)\n\n    for i, model in enumerate(models_x_for_seed):\n        save_path = seed_output_dir / f\"model_x_fold_{i}.pth\"\n        torch.save(model.state_dict(), save_path)\n\n    for i, model in enumerate(models_y_for_seed):\n        save_path = seed_output_dir / f\"model_y_fold_{i}.pth\"\n        torch.save(model.state_dict(), save_path)\n\n    print(f\"Models saved to: {seed_output_dir}\")\n\n\nprint(\"\\n\\nüéâ All training cycles complete for all seeds. üéâ\")\nprint(f\"Total seeds processed: {list(all_models_x.keys())}\")\nprint(f\"All model files saved in: {output_dir}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# ÈáçÊñ∞ÁîüÊàêÊµãËØïÈõÜÊï∞ÊçÆÂπ∂È¢ÑÊµã\n# -------------------------------\n# Feature engineering helpers are needed here\ndef unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n    if 'play_direction' not in df.columns:\n        return df\n    df = df.copy()\n    right = df['play_direction'].eq('right')\n    if 'x' in df.columns: df.loc[right, 'x'] = FIELD_LENGTH - df.loc[right, 'x']\n    if 'y' in df.columns: df.loc[right, 'y'] = FIELD_WIDTH  - df.loc[right, 'y']\n    for col in ('dir','o'):\n        if col in df.columns:\n            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n    if 'ball_land_x' in df.columns:\n        df.loc[right, 'ball_land_x'] = FIELD_LENGTH - df.loc[right, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[right, 'ball_land_y'] = FIELD_WIDTH  - df.loc[right, 'ball_land_y']\n    return df\n\n# ËØªÂèñ CSV\nDATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\ntest_input  = pd.read_csv(DATA_DIR / \"test_input.csv\")\ntest_template = pd.read_csv(DATA_DIR / \"test.csv\")\n\n# %% [code]\n# ======================================================================\n# ÂçïÂÖÉ 1ÔºöÊûÑÂª∫ÊäòÊï∞ÊçÆÂπ∂ÁºìÂ≠ò\n# ======================================================================\n\nimport os\nimport json\nimport hashlib\nimport pickle\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\n\n# -------------------------------\n# ÈÖçÁΩÆÂèäÂ∏∏Èáè\n# -------------------------------\nclass Config:\n    # ‰øÆÊîπ‰∏∫ Kaggle Êï∞ÊçÆÁõÆÂΩï\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    CACHE_DIR = Path(\"/kaggle/working/\")\n    CACHE_DIR.mkdir(exist_ok=True, parents=True)\n\n    WINDOW_SIZE = 10\n    MAX_FUTURE_HORIZON = 94\n    CV_BASELINE_L = 4\n    N_FOLDS = 5\n    FEATURE_GROUPS = [\n        'distance_rate','target_alignment','multi_window_rolling','extended_lags',\n        'velocity_changes','field_position','role_specific','time_features',\n        'jerk_features',#'curvature_land_features'\n    ]\n\n# -------------------------------\n# Âü∫Êú¨ËæÖÂä©ÂáΩÊï∞\n# -------------------------------\nYARDS_TO_METERS = 0.9144\nFPS            = 10.0\nFIELD_LENGTH   = 120.0\nFIELD_WIDTH    = 53.3\n\ndef wrap_angle_deg(s):\n    return ((s + 180.0) % 360.0) - 180.0\n\ndef unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n    if 'play_direction' not in df.columns:\n        return df\n    df = df.copy()\n    right = df['play_direction'].eq('right')\n    if 'x' in df.columns: df.loc[right, 'x'] = FIELD_LENGTH - df.loc[right, 'x']\n    if 'y' in df.columns: df.loc[right, 'y'] = FIELD_WIDTH  - df.loc[right, 'y']\n    for col in ('dir','o'):\n        if col in df.columns:\n            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n    if 'ball_land_x' in df.columns:\n        df.loc[right, 'ball_land_x'] = FIELD_LENGTH - df.loc[right, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[right, 'ball_land_y'] = FIELD_WIDTH  - df.loc[right, 'ball_land_y']\n    return df\n\n# %% [code]\nclass FeatureEngineer:\n    \"\"\"\n    Modular, ablation-friendly feature builder (pandas or cuDF pandas-API).\n    \"\"\"\n    def __init__(self, feature_groups_to_create):\n        self.gcols = ['game_id', 'play_id', 'nfl_id']\n        self.active_groups = feature_groups_to_create\n        self.feature_creators = {\n            'distance_rate': self._create_distance_rate_features,\n            'target_alignment': self._create_target_alignment_features,\n            'multi_window_rolling': self._create_multi_window_rolling_features,\n            'extended_lags': self._create_extended_lag_features,\n            'velocity_changes': self._create_velocity_change_features,\n            'field_position': self._create_field_position_features,\n            'role_specific': self._create_role_specific_features,\n            'time_features': self._create_time_features,\n            'jerk_features': self._create_jerk_features,\n            'curvature_land_features': self._create_curvature_land_features,\n        }\n        self.created_feature_cols = []\n\n    def _height_to_feet(self, height_str):\n        try:\n            ft, inches = map(int, str(height_str).split('-'))\n            return ft + inches / 12\n        except Exception:\n            return 6.0\n\n    def _create_basic_features(self, df):\n        print(\"Step 1/3: Adding basic features...\")\n        df = df.copy()\n        df['player_height_feet'] = df['player_height'].apply(self._height_to_feet)\n\n        # Correct kinematics: dir is from +x CCW\n        dir_rad = np.deg2rad(df['dir'].fillna(0.0).astype('float32'))\n        df['velocity_x']     = df['s'] * np.cos(dir_rad)\n        df['velocity_y']     = df['s'] * np.sin(dir_rad)\n        df['acceleration_x'] = df['a'] * np.cos(dir_rad)\n        df['acceleration_y'] = df['a'] * np.sin(dir_rad)\n\n        # Roles\n        df['is_offense']  = (df['player_side'] == 'Offense').astype(np.int8)\n        df['is_defense']  = (df['player_side'] == 'Defense').astype(np.int8)\n        df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(np.int8)\n        df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(np.int8)\n        df['is_passer']   = (df['player_role'] == 'Passer').astype(np.int8)\n\n        # Energetics (consistent units)\n        mass_kg = df['player_weight'].fillna(200.0) / 2.20462\n        v_ms = df['s'] * YARDS_TO_METERS\n        df['momentum_x'] = mass_kg * df['velocity_x'] * YARDS_TO_METERS\n        df['momentum_y'] = mass_kg * df['velocity_y'] * YARDS_TO_METERS\n        df['kinetic_energy'] = 0.5 * mass_kg * (v_ms ** 2)\n\n        # Ball landing geometry (static)\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            ball_dx = df['ball_land_x'] - df['x']\n            ball_dy = df['ball_land_y'] - df['y']\n            dist = np.hypot(ball_dx, ball_dy)\n            df['distance_to_ball'] = dist\n            inv = 1.0 / (dist + 1e-6)\n            df['ball_direction_x'] = ball_dx * inv\n            df['ball_direction_y'] = ball_dy * inv\n            df['closing_speed'] = (\n                df['velocity_x'] * df['ball_direction_x'] +\n                df['velocity_y'] * df['ball_direction_y']\n            )\n\n        base = [\n            'x','y','s','a','o','dir','frame_id','ball_land_x','ball_land_y',\n            'player_height_feet','player_weight',\n            'velocity_x','velocity_y','acceleration_x','acceleration_y',\n            'momentum_x','momentum_y','kinetic_energy',\n            'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n            'distance_to_ball','ball_direction_x','ball_direction_y','closing_speed'\n        ]\n        self.created_feature_cols.extend([c for c in base if c in df.columns])\n        return df\n\n    # ---- feature groups ----\n    def _create_distance_rate_features(self, df):\n        new_cols = []\n        if 'distance_to_ball' in df.columns:\n            d = df.groupby(self.gcols)['distance_to_ball'].diff()\n            df['d2ball_dt']  = d.fillna(0.0) * FPS\n            df['d2ball_ddt'] = df.groupby(self.gcols)['d2ball_dt'].diff().fillna(0.0) * FPS\n            df['time_to_intercept'] = (df['distance_to_ball'] /\n                                       (df['d2ball_dt'].abs() + 1e-3)).clip(0, 10)\n            new_cols = ['d2ball_dt','d2ball_ddt','time_to_intercept']\n        return df, new_cols\n\n    def _create_target_alignment_features(self, df):\n        new_cols = []\n        if {'ball_direction_x','ball_direction_y','velocity_x','velocity_y'}.issubset(df.columns):\n            df['velocity_alignment'] = df['velocity_x']*df['ball_direction_x'] + df['velocity_y']*df['ball_direction_y']\n            df['velocity_perpendicular'] = df['velocity_x']*(-df['ball_direction_y']) + df['velocity_y']*df['ball_direction_x']\n            new_cols.extend(['velocity_alignment','velocity_perpendicular'])\n            if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n                df['accel_alignment'] = df['acceleration_x']*df['ball_direction_x'] + df['acceleration_y']*df['ball_direction_y']\n                new_cols.append('accel_alignment')\n        return df, new_cols\n\n    def _create_multi_window_rolling_features(self, df):\n        # keep it simple & compatible (works with cuDF pandas-API); vectorized rolling per group\n        new_cols = []\n        for window in (3, 5, 10):\n            for col in ('velocity_x','velocity_y','s','a'):\n                if col in df.columns:\n                    r_mean = df.groupby(self.gcols)[col].rolling(window, min_periods=1).mean()\n                    r_std  = df.groupby(self.gcols)[col].rolling(window, min_periods=1).std()\n                    # align indices\n                    r_mean = r_mean.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    r_std  = r_std.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    df[f'{col}_roll{window}'] = r_mean\n                    df[f'{col}_std{window}']  = r_std.fillna(0.0)\n                    new_cols.extend([f'{col}_roll{window}', f'{col}_std{window}'])\n        return df, new_cols\n\n    def _create_extended_lag_features(self, df):\n        new_cols = []\n        for lag in (1,2,3,4,5):\n            for col in ('x','y','velocity_x','velocity_y'):\n                if col in df.columns:\n                    g = df.groupby(self.gcols)[col]\n                    lagv = g.shift(lag)\n                    # safe fill for first frames (no \"future\" leakage)\n                    df[f'{col}_lag{lag}'] = lagv.fillna(g.transform('first'))\n                    new_cols.append(f'{col}_lag{lag}')\n        return df, new_cols\n\n    def _create_velocity_change_features(self, df):\n        new_cols = []\n        if 'velocity_x' in df.columns:\n            df['velocity_x_change'] = df.groupby(self.gcols)['velocity_x'].diff().fillna(0.0)\n            df['velocity_y_change'] = df.groupby(self.gcols)['velocity_y'].diff().fillna(0.0)\n            df['speed_change']      = df.groupby(self.gcols)['s'].diff().fillna(0.0)\n            d = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n            df['direction_change']  = wrap_angle_deg(d)\n            new_cols = ['velocity_x_change','velocity_y_change','speed_change','direction_change']\n        return df, new_cols\n\n    def _create_field_position_features(self, df):\n        df['dist_from_left'] = df['y']\n        df['dist_from_right'] = FIELD_WIDTH - df['y']\n        df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n        df['dist_from_endzone']  = np.minimum(df['x'], FIELD_LENGTH - df['x'])\n        return df, ['dist_from_sideline','dist_from_endzone']\n\n    def _create_role_specific_features(self, df):\n        new_cols = []\n        if {'is_receiver','velocity_alignment'}.issubset(df.columns):\n            df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n            df['receiver_deviation']  = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0.0))\n            new_cols.extend(['receiver_optimality','receiver_deviation'])\n        if {'is_coverage','closing_speed'}.issubset(df.columns):\n            df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n            new_cols.append('defender_closing_speed')\n        return df, new_cols\n\n    def _create_time_features(self, df):\n        df['frames_elapsed']  = df.groupby(self.gcols).cumcount()\n        df['normalized_time'] = df.groupby(self.gcols)['frames_elapsed'].transform(\n            lambda x: x / (x.max() + 1e-9)\n        )\n        return df, ['frames_elapsed','normalized_time']\n\n    def _create_jerk_features(self, df):\n        new_cols = []\n        if 'a' in df.columns:\n            df['jerk'] = df.groupby(self.gcols)['a'].diff().fillna(0.0) * FPS\n            new_cols.append('jerk')\n        if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n            df['jerk_x'] = df.groupby(self.gcols)['acceleration_x'].diff().fillna(0.0) * FPS\n            df['jerk_y'] = df.groupby(self.gcols)['acceleration_y'].diff().fillna(0.0) * FPS\n            new_cols.extend(['jerk_x','jerk_y'])\n        return df, new_cols\n    def _create_curvature_land_features(self, df):\n        \"\"\"\n        -ËêΩÁÇπ‰æßÂêëÂÅèÂ∑ÆÔºàÁ¨¶Âè∑ÔºâÔºölanding_point Áõ∏ÂØπ‚ÄúÂΩìÂâçËøêÂä®ÊñπÂêë‚ÄùÁöÑÂ∑¶Âè≥ÂÅèÁ¶ª\n          lateral = cross(u_dir, vector_to_land)Ôºà>0 Ë°®Á§∫ËêΩÁÇπÂú®ËøêÂä®ÊñπÂêëÂ∑¶‰æßÔºâ\n        -bearing_to_land_signed: ËøêÂä®ÊñπÂêë vs ËêΩÁÇπÊñπ‰ΩçËßí\n        -ÈÄüÂ∫¶ÂΩí‰∏ÄÂåñÊõ≤ÁéáÔºö wrap(Œîdir)/ (s*Œît) ÔºåÁ™óÂè£Âåñ(3/5) ÁöÑÂùáÂÄº/ÁªùÂØπÂÄº\n        \"\"\"\n        import numpy as np\n        # ‰æßÂêëÂÅèÂ∑Æ & bearing_to_land\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            dx = df['ball_land_x'] - df['x']\n            dy = df['ball_land_y'] - df['y']\n            bearing = np.arctan2(dy, dx)\n            a_dir = np.deg2rad(df['dir'].fillna(0.0).values)\n            # ÊúâÁ¨¶Âè∑Êñπ‰ΩçÂ∑Æ\n            df['bearing_to_land_signed'] = np.rad2deg(np.arctan2(np.sin(bearing - a_dir), np.cos(bearing - a_dir)))\n            # ‰æßÂêëÂÅèÂ∑ÆÔºöd √ó u (2D cross, z ÂàÜÈáè)\n            ux, uy = np.cos(a_dir), np.sin(a_dir)\n            df['land_lateral_offset'] = dy*ux - dx*uy  # >0 ËêΩÁÇπÂú®Â∑¶‰æß\n    \n        # Êõ≤ÁéáÔºàÊåâÂ∫èÂàóÔºâ\n        ddir = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n        ddir = ((ddir + 180.0) % 360.0) - 180.0\n        curvature = np.deg2rad(ddir).astype('float32') / (df['s'].replace(0, np.nan).astype('float32') * 0.1 + 1e-6)\n        df['curvature_signed'] = curvature.fillna(0.0)\n        df['curvature_abs'] = df['curvature_signed'].abs()\n    \n        # Á™óÂè£ÂùáÂÄºÔºà3/5Ôºâ\n        for w in (3,5):\n            r = df.groupby(self.gcols)['curvature_signed'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_signed_roll{w}'] = r\n            r2 = df.groupby(self.gcols)['curvature_abs'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_abs_roll{w}'] = r2\n    \n        new_cols = ['bearing_to_land_signed','land_lateral_offset',\n                    'curvature_signed','curvature_abs','curv_signed_roll3','curv_abs_roll3',\n                    'curv_signed_roll5','curv_abs_roll5']\n        return df, [c for c in new_cols if c in df.columns]\n\n    def transform(self, df):\n        df = df.copy().sort_values(['game_id','play_id','nfl_id','frame_id'])\n        df = self._create_basic_features(df)\n\n        print(\"\\nStep 2/3: Adding selected advanced features...\")\n        for group_name in self.active_groups:\n            if group_name in self.feature_creators:\n                creator = self.feature_creators[group_name]\n                df, new_cols = creator(df)\n                self.created_feature_cols.extend(new_cols)\n                print(f\"  [+] Added '{group_name}' ({len(new_cols)} cols)\")\n            else:\n                print(f\"  [!] Unknown feature group: {group_name}\")\n\n        final_cols = sorted(set(self.created_feature_cols))\n        print(f\"\\nTotal features created: {len(final_cols)}\")\n        return df, final_cols\n\n# %% [code]\n# -------------------------------\n# Â∫èÂàóÁîüÊàêÂáΩÊï∞\n# -------------------------------\ndef build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n    s = (df_in[['game_id','play_id','play_direction']]\n         .drop_duplicates()\n         .set_index(['game_id','play_id'])['play_direction'])\n    return s\n\ndef apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n    if 'play_direction' not in df.columns:\n        dir_df = dir_map.reset_index()\n        df = df.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n    return unify_left_direction(df)\n\ndef prepare_sequences_with_advanced_features(\n    input_df, output_df=None, test_template=None, is_training=True,\n    window_size=10, feature_groups=None\n):\n    if feature_groups is None:\n        feature_groups = Config.FEATURE_GROUPS\n    dir_map = build_play_direction_map(input_df)\n    input_df_u = unify_left_direction(input_df)\n    if is_training:\n        out_u = apply_direction_to_df(output_df, dir_map)\n        target_rows  = out_u\n        target_groups = out_u[['game_id','play_id','nfl_id']].drop_duplicates()\n    else:\n        if 'play_direction' not in test_template.columns:\n            dir_df = dir_map.reset_index()\n            test_template = test_template.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n        target_rows  = test_template\n        target_groups = test_template[['game_id','play_id','nfl_id','play_direction']].drop_duplicates()\n\n    assert target_rows[['game_id','play_id','play_direction']].isna().sum().sum() == 0\n\n    fe = FeatureEngineer(feature_groups)\n    processed_df, feature_cols = fe.transform(input_df_u)\n    processed_df = processed_df.set_index(['game_id','play_id','nfl_id']).sort_index()\n    grouped = processed_df.groupby(level=['game_id','play_id','nfl_id'])\n    idx_x = feature_cols.index('x')\n    idx_y = feature_cols.index('y')\n\n    sequences, targets_dx, targets_dy, seq_meta = [], [], [], []\n    for row in tqdm(list(target_groups.itertuples(index=False)), total=len(target_groups), desc=\"Creating sequences\"):\n        gid = row[0]; pid = row[1]; nid = row[2]\n        play_dir = row[3] if (not is_training and len(row) >= 4) else None\n        key = (gid,pid,nid)\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n        sequences.append(seq)\n        if is_training:\n            out_grp = target_rows[(target_rows['game_id']==gid) & (target_rows['play_id']==pid) & (target_rows['nfl_id']==nid)].sort_values('frame_id')\n            if len(out_grp)==0:\n                continue\n            last_x = seq[-1, idx_x]\n            last_y = seq[-1, idx_y]\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            targets_dx.append(dx.astype(np.float32))\n            targets_dy.append(dy.astype(np.float32))\n        seq_meta.append({\n            'game_id': gid,\n            'play_id': pid,\n            'nfl_id': nid,\n            'frame_id': int(input_window.iloc[-1]['frame_id']),\n            'play_direction': (None if is_training else play_dir)\n        })\n    if is_training:\n        return sequences, targets_dx, targets_dy, seq_meta, feature_cols, idx_x, idx_y\n    return sequences, seq_meta, feature_cols, idx_x, idx_y\n    sequences, targets_dx, targets_dy, seq_meta = [], [], [], []\n    for row in tqdm(list(target_groups.itertuples(index=False)), total=len(target_groups), desc=\"Creating sequences\"):\n        gid = row[0]; pid = row[1]; nid = row[2]\n        play_dir = row[3] if (not is_training and len(row) >= 4) else None\n        key = (gid,pid,nid)\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n        sequences.append(seq)\n        if is_training:\n            out_grp = target_rows[(target_rows['game_id']==gid) & (target_rows['play_id']==pid) & (target_rows['nfl_id']==nid)].sort_values('frame_id')\n            if len(out_grp)==0:\n                continue\n            last_x = seq[-1, idx_x]\n            last_y = seq[-1, idx_y]\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            targets_dx.append(dx.astype(np.float32))\n            targets_dy.append(dy.astype(np.float32))\n        seq_meta.append({\n            'game_id': gid,\n            'play_id': pid,\n            'nfl_id': nid,\n            'frame_id': int(input_window.iloc[-1]['frame_id']),\n            'play_direction': (None if is_training else play_dir)\n        })\n    if is_training:\n        return sequences, targets_dx, targets_dy, seq_meta, feature_cols, idx_x, idx_y\n    return sequences, seq_meta, feature_cols, idx_x, idx_y\n    sequences, targets_dx, targets_dy, seq_meta = [], [], [], []\n    for row in tqdm(list(target_groups.itertuples(index=False)), total=len(target_groups), desc=\"Creating sequences\"):\n        gid = row[0]; pid = row[1]; nid = row[2]\n        play_dir = row[3] if (not is_training and len(row) >= 4) else None\n        key = (gid,pid,nid)\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n        sequences.append(seq)\n        if is_training:\n            out_grp = target_rows[(target_rows['game_id']==gid) & (target_rows['play_id']==pid) & (target_rows['nfl_id']==nid)].sort_values('frame_id')\n            if len(out_grp)==0:\n                continue\n            last_x = seq[-1, idx_x]\n            last_y = seq[-1, idx_y]\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            targets_dx.append(dx.astype(np.float32))\n            targets_dy.append(dy.astype(np.float32))\n        seq_meta.append({\n            'game_id': gid,\n            'play_id': pid,\n            'nfl_id': nid,\n            'frame_id': int(input_window.iloc[-1]['frame_id']),\n            'play_direction': (None if is_training else play_dir)\n        })\n    if is_training:\n        return sequences, targets_dx, targets_dy, seq_meta, feature_cols, idx_x, idx_y\n    return sequences, seq_meta, feature_cols, idx_x, idx_y\n\n    # -------------------------------\n# Â∏∏ÈÄüÂü∫Á∫øÂáΩÊï∞\n# -------------------------------\ndef cv_baseline_from_seq(seq_xy: np.ndarray, idx_x: int, idx_y: int, horizon: int, L: int = 4):\n    x = seq_xy[:, idx_x].astype(np.float32)\n    y = seq_xy[:, idx_y].astype(np.float32)\n    T = len(x)\n    L = max(2, min(L, T))\n    if T >= 2:\n        vx = np.diff(x)[-L+1:].mean()\n        vy = np.diff(y)[-L+1:].mean()\n    else:\n        vx = 0.0\n        vy = 0.0\n    steps = np.arange(1, horizon+1, dtype=np.float32)\n    dx_cv = vx * steps\n    dy_cv = vy * steps\n    return dx_cv, dy_cv\n# pasted directly from the first notebook","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# ÂçïÂÖÉ 4ÔºöÂ§öÁßçÂ≠ê„ÄÅÂ§öÊäòÊ®°ÂûãÊé®ÁêÜ‰∏éÊèê‰∫§\n# ======================================================================\n\nprint(\"üöÄ Starting inference process...\")\n\n# --- 1. Generate Test Sequences (run once) ---\n# This part is the same as before.\ndir_map = build_play_direction_map(test_input)\ntest_input_u = unify_left_direction(test_input)\n\ntest_seqs, test_meta, feature_cols, idx_x, idx_y = prepare_sequences_with_advanced_features(\n    test_input,\n    output_df=None,\n    test_template=test_template,\n    is_training=False,\n    window_size=Config.WINDOW_SIZE,\n    feature_groups=Config.FEATURE_GROUPS\n)\n\n# --- 2. Calculate Constant Velocity Baseline (run once) ---\ndx_cv_test = []\ndy_cv_test = []\n# Note: The 'horizon' variable is loaded within the loop, so we need to get it from the last loaded data.\n# It should be the same for all seeds.\nfor seq in test_seqs:\n    dx_cv, dy_cv = cv_baseline_from_seq(seq, idx_x, idx_y, horizon, L=Config.CV_BASELINE_L)\n    dx_cv_test.append(dx_cv)\n    dy_cv_test.append(dy_cv)\n\n# --- 3. Scale Sequences and Predict with All Models ---\nall_dx_res, all_dy_res = [], []\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Outer loop: Iterate through each seed\nfor seed in valid_seeds:\n    print(f\"\\\\n--- Inferring with models from Seed {seed} ---\")\n\n    # Load the data file for the current seed to get scaler info\n    data_filepath = CACHE_DIR / f\"folds_data_seed_{seed}.pkl\"\n    with open(data_filepath, 'rb') as f:\n        data = pickle.load(f)\n    folds_data = data['folds_data']  # This has the scalers for this seed\n\n    # Retrieve the models trained for this specific seed\n    models_for_this_seed_x = all_models_x[seed]\n    models_for_this_seed_y = all_models_y[seed]\n\n    # Inner loop: Iterate through each fold within the seed\n    for fold_idx, fd in enumerate(folds_data):\n        print(f\"  -> Using Fold {fold_idx+1}/{len(folds_data)} scaler and model\")\n\n        # Reconstruct the scaler for this specific fold\n        scaler = StandardScaler()\n        scaler.mean_ = fd['scaler_mean']\n        scaler.scale_ = fd['scaler_scale']\n        scaler.var_ = scaler.scale_**2\n        scaler.n_features_in_ = len(scaler.mean_)\n\n        # Scale the test sequences with this fold's scaler\n        X_test_scaled = [scaler.transform(s).astype(np.float32) for s in test_seqs]\n\n        # Get the corresponding model for this fold\n        model_x = models_for_this_seed_x[fold_idx].eval().to(device)\n        model_y = models_for_this_seed_y[fold_idx].eval().to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            X_t = torch.tensor(np.stack(X_test_scaled), dtype=torch.float32).to(device)\n            dx_res = model_x(X_t).cpu().numpy()\n            dy_res = model_y(X_t).cpu().numpy()\n\n        # Append the results from this single model to the master list\n        all_dx_res.append(dx_res)\n        all_dy_res.append(dy_res)\n\n# --- 4. Average Predictions and Generate Submission ---\nprint(f\"\\\\n‚úÖ Inference complete. Averaging results from {len(all_dx_res)} models...\")\n\n# Average residuals from ALL models (all folds across all seeds)\navg_dx_res = np.mean(all_dx_res, axis=0)\navg_dy_res = np.mean(all_dy_res, axis=0)\n\n# Add the baseline back to the averaged residuals\ndx_full = avg_dx_res + np.stack(dx_cv_test)\ndy_full = avg_dy_res + np.stack(dy_cv_test)\n\n# Generate submission file (this part is the same as before)\nrows = []\nFIELD_LENGTH = 120.0\nFIELD_WIDTH  = 53.3\ntt_idx = test_template.set_index(['game_id','play_id','nfl_id']).sort_index()\n\nfor i, meta in enumerate(test_meta):\n    gid, pid, nid = meta['game_id'], meta['play_id'], meta['nfl_id']\n    play_dir = meta.get('play_direction')\n    play_is_right = (play_dir == 'right')\n\n    try:\n        fids = tt_idx.loc[(gid,pid,nid),'frame_id']\n        if isinstance(fids, pd.Series):\n            fids = fids.sort_values().tolist()\n        else:\n            fids = [int(fids)]\n    except KeyError:\n        continue\n\n    last_x = test_seqs[i][-1, idx_x]\n    last_y = test_seqs[i][-1, idx_y]\n\n    for t, fid in enumerate(fids):\n        tt = min(t, horizon - 1)\n        x_uni = np.clip(last_x + dx_full[i, tt], 0, FIELD_LENGTH)\n        y_uni = np.clip(last_y + dy_full[i, tt], 0, FIELD_WIDTH)\n\n        if play_is_right:\n            x_out = FIELD_LENGTH - x_uni\n            y_out = FIELD_WIDTH  - y_uni\n        else:\n            x_out = x_uni\n            y_out = y_uni\n\n        rows.append({\n            'id': f\"{gid}_{pid}_{nid}_{int(fid)}\",\n            'x': float(x_out),\n            'y': float(y_out)\n        })\n\nsubmission = pd.DataFrame(rows)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(f\"\\\\nüéâ Saved submission.csv with {len(submission)} rows. üéâ\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}