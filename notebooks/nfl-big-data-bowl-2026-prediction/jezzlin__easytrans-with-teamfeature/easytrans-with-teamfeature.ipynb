{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"My Work:\n1 I use a easy trans model to replace the gru model\n2 I use a new loss function with a speed limitation\n3 I use a TTA in best seed model and multi-seed SWA\n4 I use a team-base feature to enhance model performing\n\nSTEP 2 — Better Features Pipeline (cuDF-ready)\n- Correct kinematics & angles\n- Unify play direction (and invert at submission time)\n- Fast, modular feature engineering (works with pandas or cuDF pandas-API)\n- Same GRU architecture + GroupKFold CV\n- Safe targets (dx, dy) built in the unified coordinate frame","metadata":{"_uuid":"24f43b8e-0efa-49ea-8dc0-17e1d5861e5e","_cell_guid":"97f685ab-d29d-446f-996b-e1b625520b9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# -------------------------------\n# Global imports + cuDF accelerator\n# -------------------------------\nimport os\nUSE_CUDF = False\ntry:\n    # zero/low-code GPU acceleration for DataFrame ops\n    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n    import pandas as pd\n    import numpy as np\n    import cupy as cp  # optional (not strictly required below)\n    USE_CUDF = True\n    print(\"using cuda_backend pandas for faster parallel data processing\")\nexcept Exception:\n    print(\"cuda df not used\")\n    import pandas as pd\n    import numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# -------------------------------\n# Constants & helpers\n# -------------------------------\nYARDS_TO_METERS = 0.9144\nFPS = 10.0 \nFIELD_LENGTH, FIELD_WIDTH = 120.0, 53.3\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nprint(\"environment set up!\")\ndef wrap_angle_deg(s):\n    # map to (-180, 180]\n    return ((s + 180.0) % 360.0) - 180.0\n\ndef unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Mirror rightward plays so all samples are 'left' oriented (x,y, dir, o, ball_land).\"\"\"\n    if 'play_direction' not in df.columns:\n        return df\n    df = df.copy()\n    right = df['play_direction'].eq('right')\n    # positions\n    if 'x' in df.columns: df.loc[right, 'x'] = FIELD_LENGTH - df.loc[right, 'x']\n    if 'y' in df.columns: df.loc[right, 'y'] = FIELD_WIDTH  - df.loc[right, 'y']\n    # angles in degrees\n    for col in ('dir','o'):\n        if col in df.columns:\n            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n    # ball landing\n    if 'ball_land_x' in df.columns:\n        df.loc[right, 'ball_land_x'] = FIELD_LENGTH - df.loc[right, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[right, 'ball_land_y'] = FIELD_WIDTH  - df.loc[right, 'ball_land_y']\n    return df\n\ndef invert_to_original_direction(x_u, y_u, play_dir_right: bool):\n    \"\"\"Invert unified (left) coordinates back to original play direction.\"\"\"\n    if not play_dir_right:\n        return float(x_u), float(y_u)\n    return float(FIELD_LENGTH - x_u), float(FIELD_WIDTH - y_u)\n\n# -------------------------------\n# Config\n# -------------------------------\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    OUTPUT_DIR = Path(\"./outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n\n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 256\n    EPOCHS = 200\n    PATIENCE = 30\n    LEARNING_RATE = 1e-3\n\n    WINDOW_SIZE = 10\n    HIDDEN_DIM = 128\n    MAX_FUTURE_HORIZON = 94  # 不要改动这个！！！\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nset_seed(Config.SEED)","metadata":{"_uuid":"0f17aa9a-6dc5-4a45-8065-5515f9cd9175","_cell_guid":"4f1ee572-ed92-4677-a8a9-9494c909f602","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Sequence builder (unified frame + safe targets)\n# -------------------------------\ndef build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Return a Series indexed by (game_id, play_id) with values 'left'/'right'.\n    This keeps a clean MultiIndex that works for both pandas and cuDF pandas-API.\n    \"\"\"\n    s = (\n        df_in[['game_id','play_id','play_direction']]\n        .drop_duplicates()\n        .set_index(['game_id','play_id'])['play_direction']\n    )\n    return s  # MultiIndex Series\n\n\ndef apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Attach play_direction (if missing) and then unify to 'left'.\n    dir_map must be the MultiIndex Series produced by build_play_direction_map.\n    \"\"\"\n    if 'play_direction' not in df.columns:\n        dir_df = dir_map.reset_index()  # -> columns: game_id, play_id, play_direction\n        df = df.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n    return unify_left_direction(df)\n\ndef prepare_sequences_with_advanced_features(\n        input_df, output_df=None, test_template=None, \n        is_training=True, window_size=10, feature_groups=None):\n\n    print(f\"\\n{'='*80}\")\n    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES (UNIFIED FRAME)\")\n    print(f\"{'='*80}\")\n    print(f\"Window size: {window_size}\")\n\n    if feature_groups is None:\n        feature_groups = [\n            'distance_rate','target_alignment','multi_window_rolling','extended_lags',\n            'velocity_changes','field_position','role_specific','time_features','jerk_features',\n            'player_interaction_distance',#\"curvature_land_features\"\n        ]\n\n    # Direction map and unify\n    # inside prepare_sequences_with_advanced_features(...)\n    dir_map = build_play_direction_map(input_df)\n    input_df_u = unify_left_direction(input_df)\n    \n    if is_training:\n        out_u = apply_direction_to_df(output_df, dir_map)  # <-- 用新的函数\n        target_rows = out_u\n        target_groups = out_u[['game_id','play_id','nfl_id']].drop_duplicates()\n    else:\n        # ensure test_template has play_direction via safe merge\n        if 'play_direction' not in test_template.columns:\n            dir_df = dir_map.reset_index()\n            test_template = test_template.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n        target_rows = test_template\n        target_groups = target_rows[['game_id','play_id','nfl_id','play_direction']].drop_duplicates()\n        \n    #after merging play_direction into outputs / test_template:\n    assert target_rows[['game_id','play_id','play_direction']].isna().sum().sum() == 0, \\\n        \"play_direction merge failed; check (game_id, play_id) coverage\"\n    print(\"play_direction merge OK:\", target_rows['play_direction'].value_counts(dropna=False).to_dict())\n    # --- FE ---\n\n    fe = FeatureEngineer(feature_groups)\n    processed_df, feature_cols = fe.transform(input_df_u)\n\n    # --- Build sequences ---\n    print(\"\\nStep 3/3: Creating sequences...\")\n    processed_df = processed_df.set_index(['game_id','play_id','nfl_id']).sort_index()\n    grouped = processed_df.groupby(level=['game_id','play_id','nfl_id'])\n\n    # helpful indices for last x,y in unified frame\n    idx_x = feature_cols.index('x')\n    idx_y = feature_cols.index('y')\n\n    sequences, targets_dx, targets_dy, targets_fids, seq_meta = [], [], [], [], []\n\n    it = target_groups.itertuples(index=False)\n    it = tqdm(list(it), total=len(target_groups), desc=\"Creating sequences\")\n\n    for row in it:\n        gid = row[0]; pid = row[1]; nid = row[2]\n        play_dir = row[3] if (not is_training and len(row) >= 4) else None\n        key = (gid, pid, nid)\n\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n\n        # simple impute with group means\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n\n        sequences.append(seq)\n\n        # training targets from unified outputs (dx, dy from last unified x,y)\n        if is_training:\n            out_grp = target_rows[\n                (target_rows['game_id']==gid) &\n                (target_rows['play_id']==pid) &\n                (target_rows['nfl_id']==nid)\n            ].sort_values('frame_id')\n            if len(out_grp)==0:\n                continue\n\n            last_x = seq[-1, idx_x]\n            last_y = seq[-1, idx_y]\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n\n            targets_dx.append(dx.astype(np.float32))\n            targets_dy.append(dy.astype(np.float32))\n            targets_fids.append(out_grp['frame_id'].values.astype(np.int32))\n\n        seq_meta.append({\n            'game_id': gid,\n            'play_id': pid,\n            'nfl_id': nid,\n            'frame_id': int(input_window.iloc[-1]['frame_id']),\n            'play_direction': (None if is_training else play_dir),\n        })\n\n    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_fids, seq_meta, feature_cols, dir_map\n    return sequences, seq_meta, feature_cols, dir_map","metadata":{"_uuid":"7ce6ba09-32db-402c-accc-4dd05ccba59a","_cell_guid":"4b958933-d29e-4865-9595-be8100ede912","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Feature Engineering\n# -------------------------------\nclass FeatureEngineer:\n    \"\"\"\n    Modular, ablation-friendly feature builder (pandas or cuDF pandas-API).\n    \"\"\"\n    def __init__(self, feature_groups_to_create):\n        self.gcols = ['game_id', 'play_id', 'nfl_id']\n        self.active_groups = feature_groups_to_create\n        self.feature_creators = {\n            'distance_rate': self._create_distance_rate_features,\n            'target_alignment': self._create_target_alignment_features,\n            'multi_window_rolling': self._create_multi_window_rolling_features,\n            'extended_lags': self._create_extended_lag_features,\n            'velocity_changes': self._create_velocity_change_features,\n            'field_position': self._create_field_position_features,\n            'role_specific': self._create_role_specific_features,\n            'time_features': self._create_time_features,\n            'jerk_features': self._create_jerk_features,\n            'curvature_land_features': self._create_curvature_land_features,\n            'player_interaction_distance': self._create_player_interaction_distance_features,\n        }\n        self.created_feature_cols = []\n\n    def _height_to_feet(self, height_str):\n        try:\n            ft, inches = map(int, str(height_str).split('-'))\n            return ft + inches / 12\n        except Exception:\n            return 6.0\n\n    def _create_basic_features(self, df):\n        print(\"Step 1/3: Adding basic features...\")\n        df = df.copy()\n        df['player_height_feet'] = df['player_height'].apply(self._height_to_feet)\n\n        # Correct kinematics: dir is from +x CCW\n        dir_rad = np.deg2rad(df['dir'].fillna(0.0).astype('float32'))\n        df['velocity_x']     = df['s'] * np.cos(dir_rad)\n        df['velocity_y']     = df['s'] * np.sin(dir_rad)\n        df['acceleration_x'] = df['a'] * np.cos(dir_rad)\n        df['acceleration_y'] = df['a'] * np.sin(dir_rad)\n\n        # Roles\n        df['is_offense']  = (df['player_side'] == 'Offense').astype(np.int8)\n        df['is_defense']  = (df['player_side'] == 'Defense').astype(np.int8)\n        df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(np.int8)\n        df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(np.int8)\n        df['is_passer']   = (df['player_role'] == 'Passer').astype(np.int8)\n\n        # Energetics (consistent units)\n        mass_kg = df['player_weight'].fillna(200.0) / 2.20462\n        v_ms = df['s'] * YARDS_TO_METERS\n        df['momentum_x'] = mass_kg * df['velocity_x'] * YARDS_TO_METERS\n        df['momentum_y'] = mass_kg * df['velocity_y'] * YARDS_TO_METERS\n        df['kinetic_energy'] = 0.5 * mass_kg * (v_ms ** 2)\n\n        # Ball landing geometry (static)\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            ball_dx = df['ball_land_x'] - df['x']\n            ball_dy = df['ball_land_y'] - df['y']\n            dist = np.hypot(ball_dx, ball_dy)\n            df['distance_to_ball'] = dist\n            inv = 1.0 / (dist + 1e-6)\n            df['ball_direction_x'] = ball_dx * inv\n            df['ball_direction_y'] = ball_dy * inv\n            df['closing_speed'] = (\n                df['velocity_x'] * df['ball_direction_x'] +\n                df['velocity_y'] * df['ball_direction_y']\n            )\n\n        base = [\n            'x','y','s','a','o','dir','frame_id','ball_land_x','ball_land_y',\n            'player_height_feet','player_weight',\n            'velocity_x','velocity_y','acceleration_x','acceleration_y',\n            'momentum_x','momentum_y','kinetic_energy',\n            'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n            'distance_to_ball','ball_direction_x','ball_direction_y','closing_speed'\n        ]\n        self.created_feature_cols.extend([c for c in base if c in df.columns])\n        return df\n\n    # ---- feature groups ----\n    def _create_distance_rate_features(self, df):\n        new_cols = []\n        if 'distance_to_ball' in df.columns:\n            d = df.groupby(self.gcols)['distance_to_ball'].diff()\n            df['d2ball_dt']  = d.fillna(0.0) * FPS\n            df['d2ball_ddt'] = df.groupby(self.gcols)['d2ball_dt'].diff().fillna(0.0) * FPS\n            df['time_to_intercept'] = (df['distance_to_ball'] /\n                                       (df['d2ball_dt'].abs() + 1e-3)).clip(0, 10)\n            new_cols = ['d2ball_dt','d2ball_ddt','time_to_intercept']\n        return df, new_cols\n\n    def _create_target_alignment_features(self, df):\n        new_cols = []\n        if {'ball_direction_x','ball_direction_y','velocity_x','velocity_y'}.issubset(df.columns):\n            df['velocity_alignment'] = df['velocity_x']*df['ball_direction_x'] + df['velocity_y']*df['ball_direction_y']\n            df['velocity_perpendicular'] = df['velocity_x']*(-df['ball_direction_y']) + df['velocity_y']*df['ball_direction_x']\n            new_cols.extend(['velocity_alignment','velocity_perpendicular'])\n            if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n                df['accel_alignment'] = df['acceleration_x']*df['ball_direction_x'] + df['acceleration_y']*df['ball_direction_y']\n                new_cols.append('accel_alignment')\n        return df, new_cols\n\n    def _create_multi_window_rolling_features(self, df):\n        # keep it simple & compatible (works with cuDF pandas-API); vectorized rolling per group\n        new_cols = []\n        for window in (3, 5, 10):\n            for col in ('velocity_x','velocity_y','s','a'):\n                if col in df.columns:\n                    r_mean = df.groupby(self.gcols)[col].rolling(window, min_periods=1).mean()\n                    r_std  = df.groupby(self.gcols)[col].rolling(window, min_periods=1).std()\n                    # align indices\n                    r_mean = r_mean.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    r_std  = r_std.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    df[f'{col}_roll{window}'] = r_mean\n                    df[f'{col}_std{window}']  = r_std.fillna(0.0)\n                    new_cols.extend([f'{col}_roll{window}', f'{col}_std{window}'])\n        return df, new_cols\n\n    def _create_extended_lag_features(self, df):\n        new_cols = []\n        for lag in (1,2,3,4,5):\n            for col in ('x','y','velocity_x','velocity_y'):\n                if col in df.columns:\n                    g = df.groupby(self.gcols)[col]\n                    lagv = g.shift(lag)\n                    # safe fill for first frames (no \"future\" leakage)\n                    df[f'{col}_lag{lag}'] = lagv.fillna(g.transform('first'))\n                    new_cols.append(f'{col}_lag{lag}')\n        return df, new_cols\n\n    def _create_velocity_change_features(self, df):\n        new_cols = []\n        if 'velocity_x' in df.columns:\n            df['velocity_x_change'] = df.groupby(self.gcols)['velocity_x'].diff().fillna(0.0)\n            df['velocity_y_change'] = df.groupby(self.gcols)['velocity_y'].diff().fillna(0.0)\n            df['speed_change']      = df.groupby(self.gcols)['s'].diff().fillna(0.0)\n            d = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n            df['direction_change']  = wrap_angle_deg(d)\n            new_cols = ['velocity_x_change','velocity_y_change','speed_change','direction_change']\n        return df, new_cols\n\n    def _create_field_position_features(self, df):\n        df['dist_from_left'] = df['y']\n        df['dist_from_right'] = FIELD_WIDTH - df['y']\n        df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n        df['dist_from_endzone']  = np.minimum(df['x'], FIELD_LENGTH - df['x'])\n        return df, ['dist_from_sideline','dist_from_endzone']\n\n    def _create_role_specific_features(self, df):\n        new_cols = []\n        if {'is_receiver','velocity_alignment'}.issubset(df.columns):\n            df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n            df['receiver_deviation']  = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0.0))\n            new_cols.extend(['receiver_optimality','receiver_deviation'])\n        if {'is_coverage','closing_speed'}.issubset(df.columns):\n            df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n            new_cols.append('defender_closing_speed')\n        return df, new_cols\n\n    def _create_time_features(self, df):\n        df['frames_elapsed']  = df.groupby(self.gcols).cumcount()\n        df['normalized_time'] = df.groupby(self.gcols)['frames_elapsed'].transform(\n            lambda x: x / (x.max() + 1e-9)\n        )\n        return df, ['frames_elapsed','normalized_time']\n\n    def _create_jerk_features(self, df):\n        new_cols = []\n        if 'a' in df.columns:\n            df['jerk'] = df.groupby(self.gcols)['a'].diff().fillna(0.0) * FPS\n            new_cols.append('jerk')\n        if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n            df['jerk_x'] = df.groupby(self.gcols)['acceleration_x'].diff().fillna(0.0) * FPS\n            df['jerk_y'] = df.groupby(self.gcols)['acceleration_y'].diff().fillna(0.0) * FPS\n            new_cols.extend(['jerk_x','jerk_y'])\n        return df, new_cols\n    def _create_curvature_land_features(self, df):\n        \"\"\"\n        -落点侧向偏差（符号）：landing_point 相对“当前运动方向”的左右偏离\n          lateral = cross(u_dir, vector_to_land)（>0 表示落点在运动方向左侧）\n        -bearing_to_land_signed: 运动方向 vs 落点方位角\n        -速度归一化曲率： wrap(Δdir)/ (s*Δt) ，窗口化(3/5) 的均值/绝对值\n        \"\"\"\n        import numpy as np\n        # 侧向偏差 & bearing_to_land\n        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n            dx = df['ball_land_x'] - df['x']\n            dy = df['ball_land_y'] - df['y']\n            bearing = np.arctan2(dy, dx)\n            a_dir = np.deg2rad(df['dir'].fillna(0.0).values)\n            # 有符号方位差\n            df['bearing_to_land_signed'] = np.rad2deg(np.arctan2(np.sin(bearing - a_dir), np.cos(bearing - a_dir)))\n            # 侧向偏差：d × u (2D cross, z 分量)\n            ux, uy = np.cos(a_dir), np.sin(a_dir)\n            df['land_lateral_offset'] = dy*ux - dx*uy  # >0 落点在左侧\n    \n        # 曲率（按序列）\n        ddir = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n        ddir = ((ddir + 180.0) % 360.0) - 180.0\n        curvature = np.deg2rad(ddir).astype('float32') / (df['s'].replace(0, np.nan).astype('float32') * 0.1 + 1e-6)\n        df['curvature_signed'] = curvature.fillna(0.0)\n        df['curvature_abs'] = df['curvature_signed'].abs()\n    \n        # 窗口均值（3/5）\n        for w in (3,5):\n            r = df.groupby(self.gcols)['curvature_signed'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_signed_roll{w}'] = r\n            r2 = df.groupby(self.gcols)['curvature_abs'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n            df[f'curv_abs_roll{w}'] = r2\n    \n        new_cols = ['bearing_to_land_signed','land_lateral_offset',\n                    'curvature_signed','curvature_abs','curv_signed_roll3','curv_abs_roll3',\n                    'curv_signed_roll5','curv_abs_roll5']\n        return df, [c for c in new_cols if c in df.columns]\n        \n    def _create_player_interaction_distance_features(self, df):\n        new_cols = []\n        \n        if not {'x', 'y', 'velocity_x', 'velocity_y', 'is_offense'}.issubset(df.columns):\n            return df, new_cols\n        \n        # 按play分组，确保时间连续性\n        all_frames = []\n        \n        for (gid, pid), play_df in df.groupby(['game_id', 'play_id']):\n            play_df = play_df.sort_values('frame_id').copy()\n            \n            # 每帧的交互特征\n            frame_features = []\n            \n            for fid, frame_df in play_df.groupby('frame_id', sort=True):\n                frame_df = frame_df.copy()\n                positions = frame_df[['x', 'y']].values\n                velocities = frame_df[['velocity_x', 'velocity_y']].values\n                is_offense = frame_df['is_offense'].values\n                nfl_ids = frame_df['nfl_id'].values  # 用于跟踪球员\n                \n                n_players = len(frame_df)\n                \n                # === 原有特征 ===\n                min_opponent_dist = np.full(n_players, 999.0, dtype=np.float32)\n                min_teammate_dist = np.full(n_players, 999.0, dtype=np.float32)\n                relative_velocity_to_opponent = np.zeros(n_players, dtype=np.float32)\n                opponent_density = np.zeros(n_players, dtype=np.int8)\n                teammate_density = np.zeros(n_players, dtype=np.int8)\n                \n                # === 新增特征 ===\n                time_to_closest_opponent = np.full(n_players, 999.0, dtype=np.float32)  # TTC\n                nearest_opponent_acceleration = np.zeros(n_players, dtype=np.float32)\n                directional_threat_score = np.zeros(n_players, dtype=np.float32)  # 方向威胁\n                voronoi_area = np.full(n_players, 100.0, dtype=np.float32)  # 控制区域\n                \n                # 计算交互特征\n                for i in range(n_players):\n                    pos_i = positions[i]\n                    vel_i = velocities[i]\n                    side_i = is_offense[i]\n                    speed_i = np.linalg.norm(vel_i)\n                    \n                    opponent_dists = []\n                    teammate_dists = []\n                    closest_opponent_idx = -1\n                    min_opp_dist = 999.0\n                    \n                    for j in range(n_players):\n                        if i == j:\n                            continue\n                        \n                        pos_j = positions[j]\n                        vel_j = velocities[j]\n                        side_j = is_offense[j]\n                        \n                        dist = np.sqrt((pos_i[0] - pos_j[0])**2 + (pos_i[1] - pos_j[1])**2)\n                        is_opponent = (side_i != side_j)\n                        \n                        if is_opponent:\n                            opponent_dists.append(dist)\n                            \n                            # 跟踪最近对手\n                            if dist < min_opp_dist:\n                                min_opp_dist = dist\n                                closest_opponent_idx = j\n                            \n                            # === 相对速度（改进版） ===\n                            if dist < 20.0:\n                                rel_vel = vel_i - vel_j\n                                pos_diff = pos_j - pos_i\n                                if dist > 1e-6:\n                                    closing_vel = np.dot(rel_vel, pos_diff) / dist\n                                    if dist == min(opponent_dists):\n                                        relative_velocity_to_opponent[i] = closing_vel\n                                        \n                                        # === TTC (Time To Closest) ===\n                                        if closing_vel > 0.1:  # 正在接近\n                                            time_to_closest_opponent[i] = dist / closing_vel\n                                        \n                                        # === 方向威胁分数 ===\n                                        # 考虑对手速度方向与我方位置的夹角\n                                        speed_j = np.linalg.norm(vel_j)\n                                        if speed_j > 0.5:\n                                            vel_j_norm = vel_j / speed_j\n                                            pos_diff_norm = pos_diff / (dist + 1e-6)\n                                            alignment = np.dot(vel_j_norm, pos_diff_norm)\n                                            # alignment接近1表示对手直冲我方\n                                            threat = max(0, alignment) * speed_j / (dist + 1.0)\n                                            directional_threat_score[i] = threat\n                            \n                            # 密度统计\n                            if dist < 5.0:\n                                opponent_density[i] += 1\n                        else:\n                            teammate_dists.append(dist)\n                            if dist < 5.0:\n                                teammate_density[i] += 1\n                    \n                    # 记录最小距离\n                    if opponent_dists:\n                        min_opponent_dist[i] = min(opponent_dists)\n                    if teammate_dists:\n                        min_teammate_dist[i] = min(teammate_dists)\n                    \n                    # === Voronoi 近似（简化版）===\n                    # 控制区域 ≈ 到最近对手和队友距离的加权和\n                    avg_neighbor_dist = 0.0\n                    count = 0\n                    for dist in opponent_dists[:3] + teammate_dists[:3]:  # 最近3个\n                        if dist < 50:\n                            avg_neighbor_dist += dist\n                            count += 1\n                    if count > 0:\n                        voronoi_area[i] = (avg_neighbor_dist / count) ** 2 * 3.14159\n                \n                # 添加到DataFrame\n                frame_df['min_opponent_distance'] = min_opponent_dist\n                frame_df['min_teammate_distance'] = min_teammate_dist\n                frame_df['relative_velocity_to_nearest_opponent'] = relative_velocity_to_opponent\n                frame_df['opponent_density_5yd'] = opponent_density\n                frame_df['teammate_density_5yd'] = teammate_density\n                frame_df['time_to_closest_opponent'] = np.clip(time_to_closest_opponent, 0, 10)\n                frame_df['directional_threat_score'] = directional_threat_score\n                frame_df['voronoi_control_area'] = np.clip(voronoi_area, 0, 500)\n                \n                frame_features.append(frame_df)\n            \n            # 合并该play的所有帧\n            play_combined = pd.concat(frame_features, ignore_index=True)\n            \n            # === 时序特征（跨帧计算）===\n            play_combined = play_combined.sort_values(['nfl_id', 'frame_id'])\n            \n            # 对每个球员计算时序变化\n            temporal_cols = ['opponent_density_change', 'threat_acceleration']\n            for col in temporal_cols:\n                play_combined[col] = 0.0\n            \n            for nfl_id in play_combined['nfl_id'].unique():\n                mask = play_combined['nfl_id'] == nfl_id\n                player_data = play_combined[mask].copy()\n                \n                if len(player_data) > 1:\n                    # 对手密度变化率\n                    density_diff = player_data['opponent_density_5yd'].diff().fillna(0)\n                    play_combined.loc[mask, 'opponent_density_change'] = density_diff.values\n                    \n                    # 威胁加速度\n                    threat_diff = player_data['directional_threat_score'].diff().fillna(0)\n                    play_combined.loc[mask, 'threat_acceleration'] = threat_diff.values\n            \n            all_frames.append(play_combined)\n        \n        df = pd.concat(all_frames, ignore_index=True)\n        \n        new_cols = [\n            'min_opponent_distance',\n            'min_teammate_distance',\n            'relative_velocity_to_nearest_opponent',\n            'opponent_density_5yd',\n            'teammate_density_5yd',\n            'time_to_closest_opponent',           # 新增\n            'directional_threat_score',           # 新增\n            'voronoi_control_area',               # 新增\n            'opponent_density_change',            # 新增（时序）\n            'threat_acceleration'                 # 新增（时序）\n        ]\n        \n        return df, new_cols\n\n    def transform(self, df):\n        df = df.copy().sort_values(['game_id','play_id','nfl_id','frame_id'])\n        df = self._create_basic_features(df)\n\n        print(\"\\nStep 2/3: Adding selected advanced features...\")\n        for group_name in self.active_groups:\n            if group_name in self.feature_creators:\n                creator = self.feature_creators[group_name]\n                df, new_cols = creator(df)\n                self.created_feature_cols.extend(new_cols)\n                print(f\"  [+] Added '{group_name}' ({len(new_cols)} cols)\")\n            else:\n                print(f\"  [!] Unknown feature group: {group_name}\")\n\n        final_cols = sorted(set(self.created_feature_cols))\n        print(f\"\\nTotal features created: {len(final_cols)}\")\n        return df, final_cols","metadata":{"_uuid":"e5c16db3-f9a3-4576-937c-a051125c999e","_cell_guid":"b6992e5d-b44e-4bd7-982e-69dff1718229","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Model & training (same spirit as your version)\n# -------------------------------\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03, velocity_penalty_weight=0.01,\n                 acceleration_penalty_weight=0.0, use_huber_for_penalty=True):\n        \"\"\"\n        参数:\n        delta: Huber损失的阈值\n        time_decay: 时间衰减系数，越大则未来时刻权重越小\n        velocity_penalty_weight: 速度变化惩罚权重（一阶差分）\n        acceleration_penalty_weight: 加速度惩罚权重（二阶差分）\n        use_huber_for_penalty: 是否对正则项也使用Huber损失\n        \"\"\"\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n        self.velocity_penalty_weight = velocity_penalty_weight\n        self.acceleration_penalty_weight = acceleration_penalty_weight\n        self.use_huber_for_penalty = use_huber_for_penalty\n    \n    def forward(self, pred, target, mask):\n        err = pred - target\n        abs_err = torch.abs(err)\n        \n        # ===== 主Huber损失 =====\n        huber = torch.where(abs_err <= self.delta,\n                           0.5 * err * err,\n                           self.delta * (abs_err - 0.5 * self.delta))\n        \n        # 时间衰减权重\n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            w = torch.exp(-self.time_decay * t).view(1, L)\n            huber = huber * w\n            mask_weighted = mask * w\n        else:\n            mask_weighted = mask\n        \n        main_loss = (huber * mask_weighted).sum() / (mask_weighted.sum() + 1e-8)\n        \n        # ===== 速度平滑正则项 =====\n        velocity_penalty = 0.0\n        if self.velocity_penalty_weight > 0 and pred.size(1) > 1:\n            # 一阶差分（速度变化）\n            velocity_diff = pred[:, 1:] - pred[:, :-1]\n            mask_vel = mask[:, 1:]\n            \n            if self.use_huber_for_penalty:\n                vel_abs = torch.abs(velocity_diff)\n                vel_loss = torch.where(vel_abs <= self.delta,\n                                      0.5 * velocity_diff * velocity_diff,\n                                      self.delta * (vel_abs - 0.5 * self.delta))\n            else:\n                vel_loss = velocity_diff * velocity_diff\n            \n            # 应用时间衰减（可选）\n            if self.time_decay > 0:\n                L_vel = vel_loss.size(1)\n                t_vel = torch.arange(L_vel, device=pred.device).float()\n                w_vel = torch.exp(-self.time_decay * t_vel).view(1, L_vel)\n                vel_loss = vel_loss * w_vel\n                mask_vel = mask_vel * w_vel\n            \n            velocity_penalty = (vel_loss * mask_vel).sum() / (mask_vel.sum() + 1e-8)\n        \n        # ===== 加速度平滑正则项 =====\n        acceleration_penalty = 0.0\n        if self.acceleration_penalty_weight > 0 and pred.size(1) > 2:\n            # 二阶差分（加速度变化）\n            velocity_diff = pred[:, 1:] - pred[:, :-1]\n            acceleration = velocity_diff[:, 1:] - velocity_diff[:, :-1]\n            mask_acc = mask[:, 2:]\n            \n            if self.use_huber_for_penalty:\n                acc_abs = torch.abs(acceleration)\n                acc_loss = torch.where(acc_abs <= self.delta,\n                                      0.5 * acceleration * acceleration,\n                                      self.delta * (acc_abs - 0.5 * self.delta))\n            else:\n                acc_loss = acceleration * acceleration\n            \n            # 应用时间衰减（可选）\n            if self.time_decay > 0:\n                L_acc = acc_loss.size(1)\n                t_acc = torch.arange(L_acc, device=pred.device).float()\n                w_acc = torch.exp(-self.time_decay * t_acc).view(1, L_acc)\n                acc_loss = acc_loss * w_acc\n                mask_acc = mask_acc * w_acc\n            \n            acceleration_penalty = (acc_loss * mask_acc).sum() / (mask_acc.sum() + 1e-8)\n        \n        # ===== 组合损失 =====\n        total_loss = (main_loss +\n                     self.velocity_penalty_weight * velocity_penalty +\n                     self.acceleration_penalty_weight * acceleration_penalty)\n        \n        return total_loss\n\nclass SeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        # 投影到可被num_heads整除的维度\n        self.hidden_dim = 128\n        self.input_proj = nn.Linear(input_dim, self.hidden_dim)\n        \n        # 位置编码（假设序列长度最大为10）\n        self.pos_encoding = nn.Parameter(torch.randn(1, 10, self.hidden_dim) * 0.02)\n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.hidden_dim,\n            nhead=4,\n            dim_feedforward=256,\n            dropout=0.1,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True  # Pre-LN更稳定\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        # Pooling层\n        self.pool_ln = nn.LayerNorm(self.hidden_dim)\n        self.pool_attn = nn.MultiheadAttention(\n            self.hidden_dim, \n            num_heads=4, \n            batch_first=True,\n            dropout=0.1\n        )\n        self.pool_query = nn.Parameter(torch.randn(1, 1, self.hidden_dim))\n        \n        # 输出头 - 使用PReLU替代GELU\n        self.head = nn.Sequential(\n            nn.Linear(self.hidden_dim, self.hidden_dim),\n            nn.PReLU(num_parameters=self.hidden_dim),  # 每个通道独立学习负斜率\n            nn.Dropout(0.2),\n            nn.Linear(self.hidden_dim, horizon)\n        )\n    \n    def forward(self, x):\n        # x: (B, seq_len, input_dim)\n        B, seq_len, _ = x.shape\n        \n        # 投影输入\n        x = self.input_proj(x)  # (B, seq_len, hidden_dim)\n        \n        # 添加位置编码\n        x = x + self.pos_encoding[:, :seq_len, :]\n        \n        # Transformer编码\n        h = self.transformer(x)  # (B, seq_len, hidden_dim)\n        \n        # 注意力池化\n        q = self.pool_query.expand(B, -1, -1)  # (B, 1, hidden_dim)\n        h_norm = self.pool_ln(h)  # (B, seq_len, hidden_dim)\n        ctx, _ = self.pool_attn(q, h_norm, h_norm)  # (B, 1, hidden_dim)\n        \n        # 预测\n        out = self.head(ctx.squeeze(1))  # (B, horizon)\n        \n        # 累积和\n        return torch.cumsum(out, dim=1)  # (B, horizon)\n\ndef prepare_targets(batch_axis, max_h):\n    tensors, masks = [], []\n    for arr in batch_axis:\n        L = len(arr)\n        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors), torch.stack(masks)\n\ndef train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config):\n    device = config.DEVICE\n    model = SeqModel(input_dim, horizon).to(device)\n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=False)\n\n    # build batches (keep numpy → torch)\n    def build_batches(X, Y):\n        batches = []\n        B = config.BATCH_SIZE\n        for i in range(0, len(X), B):\n            end = min(i + B, len(X))\n            xs = torch.tensor(np.stack(X[i:end]).astype(np.float32))\n            ys, ms = prepare_targets([Y[j] for j in range(i, end)], horizon)\n            batches.append((xs, ys, ms))\n        return batches\n\n    tr_batches = build_batches(X_train, y_train)\n    va_batches = build_batches(X_val,   y_val)\n\n    best_loss, best_state, bad = float('inf'), None, 0\n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for bx, by, bm in tr_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            pred = model(bx)\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by, bm in va_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                val_losses.append(criterion(pred, by, bm).item())\n\n        trl, val = float(np.mean(train_losses)), float(np.mean(val_losses))\n        scheduler.step(val)\n        if epoch % 10 == 0:\n            print(f\"  Epoch {epoch}: train={trl:.4f}, val={val:.4f}\")\n\n        if val < best_loss:\n            best_loss, bad = val, 0\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n\n    if best_state:\n        model.load_state_dict(best_state)\n    return model, best_loss","metadata":{"_uuid":"158e79c8-cacf-4d2b-bbfc-378c46386b84","_cell_guid":"82b97ef0-b19a-44a5-ac09-83100a5a1882","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------_\n# Main pipeline (MODIFICADO PARA ENSEMBLE DE SEMILLAS)\n# ------------------------------_\nclass CFG(Config):\n    # Añadimos la lista de semillas para el ensemble\n    SEEDS = [42, 19, 89,64] # ¡Puedes cambiar o añadir más semillas aquí!\n\ndef main():\n    cfg = CFG()\n    print(\"=\"*80)\n    print(f\"PASO 2: PIPELINE MEJORADO CON ENSEMBLE DE {len(cfg.SEEDS)} SEMILLAS + TTA SELECTIVO\")\n    print(\"=\"*80)\n    print(f\"Semillas a utilizar: {cfg.SEEDS}\")\n    print(f\"cuDF backend activo? {USE_CUDF}\")\n\n    # [1/4] Carga de datos (se hace una sola vez)\n    print(\"\\n[1/4] Cargando datos...\")\n    train_input_files  = [cfg.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\"  for w in range(1, 19)]\n    train_output_files = [cfg.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_input  = pd.concat([pd.read_csv(f) for f in train_input_files  if f.exists()], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()], ignore_index=True)\n    test_input     = pd.read_csv(cfg.DATA_DIR / \"test_input.csv\")\n    test_template  = pd.read_csv(cfg.DATA_DIR / \"test.csv\")\n\n    # [2/4] Preparación de secuencias (se hace una sola vez)\n    print(\"\\n[2/4] Construyendo secuencias con características AVANZADAS...\")\n    seqs, tdx, tdy, tfids, seq_meta, feat_cols, dir_map = prepare_sequences_with_advanced_features(\n        train_input, output_df=train_output, is_training=True,\n        window_size=cfg.WINDOW_SIZE\n    )\n\n    # numpy object arrays a listas para un manejo más fácil\n    sequences = list(seqs)\n    targets_dx = list(tdx)\n    targets_dy = list(tdy)\n\n    # [3/4] Entrenamiento con GroupKFold sobre múltiples semillas\n    print(\"\\n[3/4] Iniciando entrenamiento de ensemble...\")\n    \n    # Contenedores para todos los modelos y escaladores de todas las ejecuciones\n    all_models_x, all_models_y, all_scalers = [], [], []\n    fold_rmse_list = []  # Para almacenar RMSE de cada fold\n    \n    # Diccionario para rastrear modelos por semilla\n    seed_models = {}  # {seed: {'models_x': [], 'models_y': [], 'scalers': [], 'rmse_list': []}}\n    \n    groups = np.array([d['game_id'] for d in seq_meta])\n    \n    for seed in cfg.SEEDS:\n        print(f\"\\n{'='*70}\\n   Entrenando con Semilla (Seed): {seed}\\n{'='*70}\")\n        set_seed(seed)\n        \n        # Inicializar contenedores para esta semilla\n        seed_models[seed] = {\n            'models_x': [],\n            'models_y': [],\n            'scalers': [],\n            'rmse_list': []\n        }\n        \n        gkf = GroupKFold(n_splits=cfg.N_FOLDS)\n\n        for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n            print(f\"\\n{'-'*60}\\nFold {fold}/{cfg.N_FOLDS} para la semilla {seed}\\n{'-'*60}\")\n\n            X_tr = [sequences[i] for i in tr]\n            X_va = [sequences[i] for i in va]\n\n            # Estandarización por fold\n            scaler = StandardScaler()\n            scaler.fit(np.vstack([s for s in X_tr]))\n\n            X_tr_sc = np.stack([scaler.transform(s) for s in X_tr]).astype(np.float32)\n            X_va_sc = np.stack([scaler.transform(s) for s in X_va]).astype(np.float32)\n\n            # Entrenar modelo para X\n            print(\"Entrenando modelo ΔX...\")\n            mx, loss_x = train_model(\n                X_tr_sc, [targets_dx[i] for i in tr],\n                X_va_sc, [targets_dx[i] for i in va],\n                X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg\n            )\n\n            # Entrenar modelo para Y\n            print(\"Entrenando modelo ΔY...\")\n            my, loss_y = train_model(\n                X_tr_sc, [targets_dy[i] for i in tr],\n                X_va_sc, [targets_dy[i] for i in va],\n                X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg\n            )\n            \n            # Guardar los modelos y el escalador de este fold\n            all_models_x.append(mx)\n            all_models_y.append(my)\n            all_scalers.append(scaler)\n            \n            # También guardar en el diccionario de la semilla\n            seed_models[seed]['models_x'].append(mx)\n            seed_models[seed]['models_y'].append(my)\n            seed_models[seed]['scalers'].append(scaler)\n            \n            # Calcular RMSE en el conjunto de validación\n            mx.eval()\n            my.eval()\n            with torch.no_grad():\n                X_va_t = torch.tensor(X_va_sc).to(cfg.DEVICE)\n                pred_dx = mx(X_va_t).cpu().numpy()\n                pred_dy = my(X_va_t).cpu().numpy()\n            \n            # Preparar targets de validación para RMSE\n            y_va_dx = [targets_dx[i] for i in va]\n            y_va_dy = [targets_dy[i] for i in va]\n            \n            # Calcular RMSE: sqrt(mean((x_pred - x_true)^2 + (y_pred - y_true)^2))\n            squared_errors = []\n            for i in range(len(pred_dx)):\n                # Obtener targets reales con padding\n                target_dx_full, mask_dx = prepare_targets([y_va_dx[i]], cfg.MAX_FUTURE_HORIZON)\n                target_dy_full, mask_dy = prepare_targets([y_va_dy[i]], cfg.MAX_FUTURE_HORIZON)\n                \n                target_dx_arr = target_dx_full[0].cpu().numpy()\n                target_dy_arr = target_dy_full[0].cpu().numpy()\n                mask_arr = mask_dx[0].cpu().numpy()\n                \n                # Solo calcular error en posiciones válidas (mask == 1)\n                valid_indices = mask_arr > 0\n                if valid_indices.sum() > 0:\n                    dx_error = (pred_dx[i][valid_indices] - target_dx_arr[valid_indices]) ** 2\n                    dy_error = (pred_dy[i][valid_indices] - target_dy_arr[valid_indices]) ** 2\n                    squared_errors.extend(dx_error + dy_error)\n            \n            fold_rmse = np.sqrt(np.mean(squared_errors))\n            fold_rmse_list.append(fold_rmse)\n            seed_models[seed]['rmse_list'].append(fold_rmse)\n            \n            print(f\"Fold {fold} (semilla {seed}) — val loss: dx={loss_x:.5f}, dy={loss_y:.5f} | RMSE={fold_rmse:.5f}\")\n\n    # Calcular RMSE promedio por semilla e identificar la mejor\n    print(\"\\n\" + \"=\"*80)\n    print(\"ESTADÍSTICAS DE RMSE POR SEMILLA\")\n    print(\"=\"*80)\n    seed_avg_rmse = {}\n    for seed in cfg.SEEDS:\n        avg_rmse = np.mean(seed_models[seed]['rmse_list'])\n        std_rmse = np.std(seed_models[seed]['rmse_list'])\n        seed_avg_rmse[seed] = avg_rmse\n        print(f\"Semilla {seed}: RMSE Promedio = {avg_rmse:.5f} ± {std_rmse:.5f}\")\n    \n    best_seed = min(seed_avg_rmse, key=seed_avg_rmse.get)\n    print(\"-\"*80)\n    print(f\"✓ MEJOR SEMILLA: {best_seed} con RMSE = {seed_avg_rmse[best_seed]:.5f}\")\n    print(\"  (Se aplicará TTA solo a los modelos de esta semilla)\")\n    print(\"=\"*80)\n    \n    # Calcular estadísticas generales de RMSE\n    rmse_mean = np.mean(fold_rmse_list)\n    rmse_std = np.std(fold_rmse_list)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ESTADÍSTICAS DE RMSE POR FOLD (TODOS)\")\n    print(\"=\"*80)\n    for i, rmse in enumerate(fold_rmse_list, 1):\n        print(f\"Fold {i}: RMSE = {rmse:.5f}\")\n    print(\"-\"*80)\n    print(f\"RMSE Promedio Global: {rmse_mean:.5f}\")\n    print(f\"RMSE Desviación Estándar Global: {rmse_std:.5f}\")\n    print(\"=\"*80)\n\n    # [4/4] Inferencia sobre el test usando todos los modelos entrenados + TTA selectivo\n    print(f\"\\n[4/4] Inferencia y submission con ensemble de {len(all_models_x)} modelos + TTA en mejor semilla...\")\n    test_seqs, test_meta, feat_cols_t, dir_map_test = prepare_sequences_with_advanced_features(\n        test_input, test_template=test_template, is_training=False,\n        window_size=cfg.WINDOW_SIZE\n    )\n    assert feat_cols_t == feat_cols, \"¡Las columnas de características de Train/Test no coinciden!\"\n\n    idx_x = feat_cols.index('x')\n    idx_y = feat_cols.index('y')\n\n    X_test_raw = list(test_seqs)\n    x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n    y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n\n    # Predicciones normales de todos los modelos (peso 1.0 cada uno)\n    all_preds_dx, all_preds_dy = [], []\n    \n    print(f\"\\n--- Generando predicciones normales de {len(all_models_x)} modelos...\")\n    for mx, my, sc in zip(all_models_x, all_models_y, all_scalers):\n        X_sc = np.stack([sc.transform(s) for s in X_test_raw]).astype(np.float32)\n        X_t = torch.tensor(X_sc).to(cfg.DEVICE)\n        mx.eval()\n        my.eval()\n        with torch.no_grad():\n            all_preds_dx.append(mx(X_t).cpu().numpy())\n            all_preds_dy.append(my(X_t).cpu().numpy())\n\n    # TTA solo para la mejor semilla (peso reducido)\n    tta_weight = 0.15  # Peso por cada aumentación de TTA (ajustable)\n    tta_augmentations = [\n        ('noise', 0.01),      # Ruido gaussiano pequeño\n        ('temporal_shift', 1), # Desplazamiento temporal de 1 frame\n        ('speed_scale', 1.05)  # Escala de velocidad 5%\n    ]\n    \n    print(f\"\\n--- Aplicando TTA a modelos de la mejor semilla ({best_seed})...\")\n    print(f\"    Peso de cada aumentación TTA: {tta_weight:.3f}\")\n    print(f\"    Aumentaciones: {len(tta_augmentations)}\")\n    \n    best_models_x = seed_models[best_seed]['models_x']\n    best_models_y = seed_models[best_seed]['models_y']\n    best_scalers = seed_models[best_seed]['scalers']\n    \n    tta_preds_dx, tta_preds_dy = [], []\n    \n    for aug_name, aug_param in tta_augmentations:\n        print(f\"    Procesando aumentación: {aug_name} (param={aug_param})...\")\n        \n        # Aplicar aumentación a las secuencias de test\n        X_test_aug = []\n        for seq in X_test_raw:\n            if aug_name == 'noise':\n                # Añadir ruido gaussiano\n                noise = np.random.randn(*seq.shape) * aug_param\n                aug_seq = seq + noise\n            elif aug_name == 'temporal_shift':\n                # Desplazar temporalmente (eliminar primeros frames, duplicar últimos)\n                shift = int(aug_param)\n                aug_seq = np.vstack([seq[shift:], np.tile(seq[-1:], (shift, 1))])\n            elif aug_name == 'speed_scale':\n                # Escalar componentes de velocidad\n                aug_seq = seq.copy()\n                if 'vx' in feat_cols and 'vy' in feat_cols:\n                    idx_vx = feat_cols.index('vx')\n                    idx_vy = feat_cols.index('vy')\n                    aug_seq[:, idx_vx] *= aug_param\n                    aug_seq[:, idx_vy] *= aug_param\n                if 'ax' in feat_cols and 'ay' in feat_cols:\n                    idx_ax = feat_cols.index('ax')\n                    idx_ay = feat_cols.index('ay')\n                    aug_seq[:, idx_ax] *= aug_param\n                    aug_seq[:, idx_ay] *= aug_param\n            else:\n                aug_seq = seq\n            \n            X_test_aug.append(aug_seq)\n        \n        # Generar predicciones con TTA para cada modelo de la mejor semilla\n        for mx, my, sc in zip(best_models_x, best_models_y, best_scalers):\n            X_sc = np.stack([sc.transform(s) for s in X_test_aug]).astype(np.float32)\n            X_t = torch.tensor(X_sc).to(cfg.DEVICE)\n            mx.eval()\n            my.eval()\n            with torch.no_grad():\n                tta_preds_dx.append(mx(X_t).cpu().numpy())\n                tta_preds_dy.append(my(X_t).cpu().numpy())\n\n    # Ensemble ponderado: predicciones normales (peso 1.0) + TTA (peso reducido)\n    print(f\"\\n--- Combinando predicciones...\")\n    print(f\"    Predicciones normales: {len(all_preds_dx)} modelos × peso 1.0\")\n    print(f\"    Predicciones TTA: {len(tta_preds_dx)} aumentaciones × peso {tta_weight:.3f}\")\n    \n    # Calcular suma ponderada\n    total_weight = len(all_preds_dx) * 1.0 + len(tta_preds_dx) * tta_weight\n    \n    # Sumar predicciones normales\n    ens_dx = np.sum(all_preds_dx, axis=0) * 1.0\n    ens_dy = np.sum(all_preds_dy, axis=0) * 1.0\n    \n    # Sumar predicciones TTA\n    if len(tta_preds_dx) > 0:\n        ens_dx += np.sum(tta_preds_dx, axis=0) * tta_weight\n        ens_dy += np.sum(tta_preds_dy, axis=0) * tta_weight\n    \n    # Normalizar por peso total\n    ens_dx /= total_weight\n    ens_dy /= total_weight\n    \n    H = ens_dx.shape[1]\n\n    # Construcción de las filas para la submission, con inversión para jugadas a la derecha\n    rows = []\n    tt_idx = test_template.set_index(['game_id','play_id','nfl_id']).sort_index()\n\n    for i, meta in enumerate(test_meta):\n        gid = meta['game_id']; pid = meta['play_id']; nid = meta['nfl_id']\n        play_dir = meta['play_direction']\n        play_is_right = (play_dir == 'right')\n\n        try:\n            fids = tt_idx.loc[(gid,pid,nid),'frame_id']\n            if isinstance(fids, pd.Series):\n                fids = fids.sort_values().tolist()\n            else:\n                fids = [int(fids)]\n        except KeyError:\n            continue\n\n        for t, fid in enumerate(fids):\n            tt = min(t, H - 1)\n            x_uni = np.clip(x_last_uni[i] + ens_dx[i, tt], 0, FIELD_LENGTH)\n            y_uni = np.clip(y_last_uni[i] + ens_dy[i, tt], 0, FIELD_WIDTH)\n            x_out, y_out = invert_to_original_direction(x_uni, y_uni, play_is_right)\n\n            rows.append({\n                'id': f\"{gid}_{pid}_{nid}_{int(fid)}\",\n                'x': x_out,\n                'y': y_out\n            })\n\n    submission = pd.DataFrame(rows)\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"\\n\" + \"=\"*80)\n    print(\"¡PASO 2 COMPLETO CON TTA SELECTIVO!\")\n    print(\"=\"*80)\n    print(f\"✓ Submission guardada en submission.csv  |  Filas: {len(submission)}\")\n    print(f\"Total de modelos base en ensemble: {len(all_models_x)}\")\n    print(f\"Modelos con TTA (semilla {best_seed}): {len(best_models_x)} × {len(tta_augmentations)} aumentaciones\")\n    print(f\"Peso efectivo TTA vs normal: {tta_weight:.3f} : 1.0\")\n    print(f\"Características utilizadas: {len(feat_cols)}  (cuDF activo: {USE_CUDF})\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"3ca303d2-ec2b-4690-a186-3f93c8638529","_cell_guid":"725dcc39-caed-4588-a453-ab09e6294699","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}