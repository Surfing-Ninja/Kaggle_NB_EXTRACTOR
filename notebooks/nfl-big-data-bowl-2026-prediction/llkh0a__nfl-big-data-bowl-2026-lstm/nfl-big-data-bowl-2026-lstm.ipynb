{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"},{"sourceId":265179171,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":5900.241149,"end_time":"2025-10-04T07:43:09.106289","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-04T06:04:48.86514","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"862f0ee9-78d0-4956-9c7e-35d80038d669","cell_type":"markdown","source":"note: lastest version added GRU, switch to version 15 for pure LSTM with LB: 0 .69\n\ncan change to GPU for faster submission time","metadata":{}},{"id":"f9c2ed6c","cell_type":"code","source":"# ================================================================================\n# NFL BIG DATA BOWL 2026 - COMPLETE WORKING SOLUTION\n# Predicting player movement during pass plays with temporal features\n# ================================================================================\nimport torch\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport gc\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nimport joblib\nfrom datetime import datetime\n# Machine Learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom tqdm import tqdm\n# Deep Learning\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\nwarnings.filterwarnings('ignore')\n\n# ================================================================================\n# CONFIGURATION\n# ================================================================================\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:04:53.73491Z","iopub.status.busy":"2025-10-04T06:04:53.734071Z","iopub.status.idle":"2025-10-04T06:05:01.841499Z","shell.execute_reply":"2025-10-04T06:05:01.840591Z"},"papermill":{"duration":8.118876,"end_time":"2025-10-04T06:05:01.843169","exception":false,"start_time":"2025-10-04T06:04:53.724293","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"9375c527","cell_type":"markdown","source":"# Config","metadata":{"papermill":{"duration":0.006588,"end_time":"2025-10-04T06:05:01.856968","exception":false,"start_time":"2025-10-04T06:05:01.85038","status":"completed"},"tags":[]}},{"id":"8822dca6","cell_type":"code","source":"\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    PRETRAIN_DIR = None\n    SEED = 42\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    MAX_SPEED = 12.0\n    N_FOLDS = 5\n    \n    # LSTM_DATA_DIR = '/kaggle/input/prepare-lstm'\n    LSTM_DATA_DIR = None\n    HIDDEN_DIM = 128\n    NUM_LAYERS = 2\n    DROPOUT = 0.3\n    MAX_FUTURE_HORIZON = 94 #unchangable\n    \n    WINDOW_SIZE = 6 # aware of high value. 6,7 are safer for submission\n    BATCH_SIZE = 256\n    LEARNING_RATE = 1e-3\n    PATIENCE = 30\n    EPOCHS = 200\n    DEBUG_FRACTION = 1.0\n    # Set to low value if need to debug\n    # EPOCHS = 1\n    # DEBUG_FRACTION = 0.01","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:01.871558Z","iopub.status.busy":"2025-10-04T06:05:01.871169Z","iopub.status.idle":"2025-10-04T06:05:01.876499Z","shell.execute_reply":"2025-10-04T06:05:01.875867Z"},"papermill":{"duration":0.014176,"end_time":"2025-10-04T06:05:01.877768","exception":false,"start_time":"2025-10-04T06:05:01.863592","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"3960d38a","cell_type":"code","source":"def set_global_seeds(seed: int = 42):\n    \"\"\"Set seeds for reproducibility.\"\"\"\n    import random, os\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nset_global_seeds(Config.SEED)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:01.893627Z","iopub.status.busy":"2025-10-04T06:05:01.892851Z","iopub.status.idle":"2025-10-04T06:05:01.904225Z","shell.execute_reply":"2025-10-04T06:05:01.903261Z"},"papermill":{"duration":0.020781,"end_time":"2025-10-04T06:05:01.906042","exception":false,"start_time":"2025-10-04T06:05:01.885261","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"88038ef2","cell_type":"code","source":"\n# ================================================================================\n# DATA LOADING\n# ================================================================================\n\ndef load_data(debug_fraction=1.0):\n    \"\"\"Load all training and test data with an option to use a fraction for debugging.\"\"\"\n    print(\"Loading data...\")\n    \n    # Training data\n    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    # Filter existing files\n    train_input_files = [f for f in train_input_files if f.exists()]\n    train_output_files = [f for f in train_output_files if f.exists()]\n    \n    print(f\"Found {len(train_input_files)} weeks of data\")\n    \n    # Load and concatenate\n    train_input = pd.concat([pd.read_csv(f) for f in tqdm(train_input_files, desc=\"Input\")], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in tqdm(train_output_files, desc=\"Output\")], ignore_index=True)\n    \n    # Test data\n    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n    \n    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n    \n    # Use only a fraction of the games for debugging (select entire games)\n    if debug_fraction < 1.0:\n        unique_game_ids = train_input['game_id'].unique()\n        sampled_game_ids = pd.Series(unique_game_ids).sample(frac=debug_fraction, random_state=42).values\n        train_input = train_input[train_input['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n        train_output = train_output[train_output['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n        print(f\"Using {len(train_input):,} input records from {len(sampled_game_ids)} games for debugging\")\n    \n    return train_input, train_output, test_input, test_template\n# ================================================================================","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:01.921281Z","iopub.status.busy":"2025-10-04T06:05:01.920562Z","iopub.status.idle":"2025-10-04T06:05:01.92956Z","shell.execute_reply":"2025-10-04T06:05:01.928797Z"},"papermill":{"duration":0.018053,"end_time":"2025-10-04T06:05:01.930944","exception":false,"start_time":"2025-10-04T06:05:01.912891","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"e1d04d05","cell_type":"markdown","source":"# Metric","metadata":{"papermill":{"duration":0.006381,"end_time":"2025-10-04T06:05:01.944224","exception":false,"start_time":"2025-10-04T06:05:01.937843","status":"completed"},"tags":[]}},{"id":"1ac45ad5","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute RMSE for NFL competition.\n    Expected input:\n      - solution and submission as pandas.DataFrame\n      - Column 'id': unique identifier for each (game_id, play_id, nfl_id, frame_id)\n      - Column 'x'\n      - Column 'y'\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> row_id_column_name = 'id'\n    >>> solution = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,3], 'y':[4,2,3]})\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1.1,2,3], 'y':[4,2.2,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    0.0913\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [0,2,3], 'y':[4,2.2,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    0.4163\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,1], 'y':[4,0,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    1.1547\n    \"\"\"\n\n    TARGET = ['x', 'y']\n    if row_id_column_name not in solution.columns:\n        raise ParticipantVisibleError(f\"Solution file missing required column: '{row_id_column_name}'\")\n    if row_id_column_name not in submission.columns:\n        raise ParticipantVisibleError(f\"Submission file missing required column: '{row_id_column_name}'\")\n\n    missing_in_solution = set(TARGET) - set(solution.columns)\n    missing_in_submission = set(TARGET) - set(submission.columns)\n\n    if missing_in_solution:\n        raise ParticipantVisibleError(f'Solution file missing required columns: {missing_in_solution}')\n    if missing_in_submission:\n        raise ParticipantVisibleError(f'Submission file missing required columns: {missing_in_submission}')\n\n    submission = submission[['id'] + TARGET]\n    merged_df = pd.merge(solution, submission, on=row_id_column_name, suffixes=('_true', '_pred'))\n    #log NaN\n    nanx_in_pred = merged_df['x_pred'].isna().sum()\n    nany_in_pred = merged_df['y_pred'].isna().sum()\n    if nanx_in_pred > 0:\n        print(f\"WARNING: Found {nanx_in_pred} NaN predictions in merged results\")\n    if nany_in_pred > 0:\n        print(f\"WARNING: Found {nany_in_pred} NaN predictions in merged results\")\n    nanx_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['x_true'].isna().sum()\n    nany_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['y_true'].isna().sum()\n    if nanx_in_true > 0:\n        print(f\"WARNING: Found {nanx_in_true} NaN true values corresponding to NaN predictions\")\n    if nany_in_true > 0:\n        print(f\"WARNING: Found {nany_in_true} NaN true values corresponding to NaN predictions\")\n    rmse = np.sqrt(\n        0.5 * (mean_squared_error(merged_df['x_true'], merged_df['x_pred']) + mean_squared_error(merged_df['y_true'], merged_df['y_pred']))\n    )\n    return float(rmse)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:01.958953Z","iopub.status.busy":"2025-10-04T06:05:01.958636Z","iopub.status.idle":"2025-10-04T06:05:01.967909Z","shell.execute_reply":"2025-10-04T06:05:01.967135Z"},"papermill":{"duration":0.018555,"end_time":"2025-10-04T06:05:01.969278","exception":false,"start_time":"2025-10-04T06:05:01.950723","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"36bba9c8","cell_type":"markdown","source":"# Prepare features for LSTM","metadata":{"papermill":{"duration":0.006531,"end_time":"2025-10-04T06:05:01.982672","exception":false,"start_time":"2025-10-04T06:05:01.976141","status":"completed"},"tags":[]}},{"id":"9ac43150","cell_type":"code","source":"def height_to_feet(height_str):\n    \"\"\"Convert height from 'ft-in' format to feet\"\"\"\n    try:\n        ft, inches = map(int, height_str.split('-'))\n        return ft + inches/12\n    except:\n        return None\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:01.997699Z","iopub.status.busy":"2025-10-04T06:05:01.997126Z","iopub.status.idle":"2025-10-04T06:05:02.001534Z","shell.execute_reply":"2025-10-04T06:05:02.000731Z"},"papermill":{"duration":0.013469,"end_time":"2025-10-04T06:05:02.002963","exception":false,"start_time":"2025-10-04T06:05:01.989494","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"7b523e17","cell_type":"code","source":"def prepare_sequences_for_lstm(input_df, output_df=None, test_template=None, is_training=True,\n                               window_size=Config.WINDOW_SIZE, cache_dir=\"cache\", save_to_disk=True):\n    \"\"\"(UPDATED) Prepare sequences; now always records last observed frame_id.\"\"\"\n    print(\"Preparing sequences for LSTM...\")\n    print('Using window size = ',window_size)\n    input_df = input_df.copy()\n    input_df['player_height_feet'] = input_df['player_height'].map(height_to_feet)\n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    delta_t = 0.1\n    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    # add age\n    current_date = datetime.now()\n    input_df['age'] = input_df['player_birth_date'].apply(\n        lambda x: (current_date - datetime.strptime(x, '%Y-%m-%d')).days // 365 if pd.notnull(x) else None\n    )\n    # add kenetic energy and force\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n    input_df['force'] = mass_kg * input_df['a']\n    # Add rolling statistics\n    input_df['rolling_mean_velocity_x'] = input_df.groupby(['game_id', 'play_id', 'nfl_id'])['velocity_x'].transform(\n        lambda x: x.rolling(window=window_size, min_periods=1).mean()\n    )\n    input_df['rolling_std_acceleration'] = input_df.groupby(['game_id', 'play_id', 'nfl_id'])['a'].transform(\n        lambda x: x.rolling(window=window_size, min_periods=1).std()\n    )\n    # Ball related features\n    if all(col in input_df.columns for col in ['ball_land_x', 'ball_land_y']):\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (input_df['velocity_x'] * input_df['ball_direction_x'] +\n                                     input_df['velocity_y'] * input_df['ball_direction_y'])\n        input_df['estimated_time_to_ball'] = input_df['distance_to_ball'] / 20.0\n        input_df['projected_time_to_ball'] = input_df['distance_to_ball'] / (np.abs(input_df['closing_speed']) + 0.1)\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    input_df['is_right'] = (input_df['play_direction'] == 'right').astype(int)\n    input_df['is_left']  = (input_df['play_direction'] == 'left').astype(int)\n\n    target_rows = output_df if is_training else test_template\n    grouped_input = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n\n    feature_cols = [\n        'x','y','s','a','o','dir',\n        'absolute_yardline_number',\n        'player_height_feet','player_weight',\n        'is_right','is_left',\n        'velocity_x','velocity_y',\n        'momentum_x','momentum_y',\n        'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n        # New features\n        'age',\n        'kinetic_energy','force',\n        'rolling_mean_velocity_x','rolling_std_acceleration'\n    ]\n    if 'distance_to_ball' in input_df.columns:\n        feature_cols += [\n            'distance_to_ball','angle_to_ball','ball_direction_x','ball_direction_y',\n            'closing_speed','estimated_time_to_ball','projected_time_to_ball'\n        ]\n\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups)):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        try:\n            group_df = grouped_input.get_group(key)\n        except KeyError:\n            continue\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            # Option: pad instead of skip\n            # pad for test\n            if is_training:\n                continue\n            # pad for test\n            pad_length = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_length), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True).reset_index(drop=True)\n        seq = input_window[feature_cols].values\n        if np.isnan(seq.astype(np.float32)).any():\n            if is_training:\n                print(f\"Skipping sequence with NaNs for key {key}\")\n                continue\n            else:\n            # For test, we can pad NaNs with zeros (or mean values)\n                print(f\"Found NaNs in test sequence for key {key}, padding with mean values\")\n                seq = np.nan_to_num(seq, nan=0.0)\n                seq = np.where(seq == 0, np.mean(seq), seq)\n        sequences.append(seq)\n\n        last_frame_id = input_window['frame_id'].iloc[-1]\n        if is_training:\n            output_group = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            last_input_x = input_window.iloc[-1]['x']\n            last_input_y = input_window.iloc[-1]['y']\n            dx = output_group['x'].values - last_input_x  # cumulative displacement\n            dy = output_group['y'].values - last_input_y\n            future_frame_ids = output_group['frame_id'].values\n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(future_frame_ids)\n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': last_frame_id  # now included\n        })\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids,sequence_ids\n    return sequences, sequence_ids","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:02.018082Z","iopub.status.busy":"2025-10-04T06:05:02.01771Z","iopub.status.idle":"2025-10-04T06:05:02.037918Z","shell.execute_reply":"2025-10-04T06:05:02.037062Z"},"papermill":{"duration":0.029554,"end_time":"2025-10-04T06:05:02.039306","exception":false,"start_time":"2025-10-04T06:05:02.009752","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"d4829f4e","cell_type":"code","source":"print(\"Loading and preparing data...\")\ntrain_input, train_output, test_input, test_template = load_data(debug_fraction=Config.DEBUG_FRACTION)\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:02.054438Z","iopub.status.busy":"2025-10-04T06:05:02.053714Z","iopub.status.idle":"2025-10-04T06:05:22.609678Z","shell.execute_reply":"2025-10-04T06:05:22.608727Z"},"papermill":{"duration":20.565258,"end_time":"2025-10-04T06:05:22.611299","exception":false,"start_time":"2025-10-04T06:05:02.046041","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"07c5045d","cell_type":"code","source":"sequences, targets_dx, targets_dy,targets_frame_ids,ids = prepare_sequences_for_lstm(\n    input_df=train_input,\n    output_df=train_output,\n    is_training=True,\n    window_size=Config.WINDOW_SIZE,\n)\n# save to /kaggle/working\njoblib.dump({\n    'sequences': sequences,\n    'targets_dx': targets_dx,\n    'targets_dy': targets_dy,\n    'targets_frame_ids': targets_frame_ids,\n    'ids': ids\n}, 'lstm_sequences_targets_ids.joblib')\n\nprint(\"Saved sequences, targets_dx, targets_dy, targets_frame_ids, ids to lstm_sequences_targets_ids.joblib\")\n\n# Prepare 3D sequences for LSTM\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:05:22.628954Z","iopub.status.busy":"2025-10-04T06:05:22.628625Z","iopub.status.idle":"2025-10-04T06:11:21.130004Z","shell.execute_reply":"2025-10-04T06:11:21.128994Z"},"papermill":{"duration":358.618061,"end_time":"2025-10-04T06:11:21.237836","exception":false,"start_time":"2025-10-04T06:05:22.619775","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"0800a89c","cell_type":"code","source":"len(sequences),sequences[0].shape,len(targets_dx),targets_dx[0].shape,targets_dy[0].shape","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:21.453608Z","iopub.status.busy":"2025-10-04T06:11:21.452936Z","iopub.status.idle":"2025-10-04T06:11:21.459564Z","shell.execute_reply":"2025-10-04T06:11:21.458777Z"},"papermill":{"duration":0.117651,"end_time":"2025-10-04T06:11:21.461098","exception":false,"start_time":"2025-10-04T06:11:21.343447","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"d991f96a","cell_type":"code","source":"targets_dx[0]","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:21.678028Z","iopub.status.busy":"2025-10-04T06:11:21.677701Z","iopub.status.idle":"2025-10-04T06:11:21.683371Z","shell.execute_reply":"2025-10-04T06:11:21.682637Z"},"papermill":{"duration":0.113586,"end_time":"2025-10-04T06:11:21.684572","exception":false,"start_time":"2025-10-04T06:11:21.570986","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"ed89b9e4","cell_type":"code","source":"def create_oof_predictions(model, scaler, X_val_unscaled, val_ids, y_val_dx, y_val_dy, y_val_frame_ids, val_data):\n    \"\"\"\n    Build per-frame OOF predictions using ALL models (no exclusion).\n    Returns pred_df, true_df with real frame_ids.\n    \"\"\"\n    pred_rows, true_rows = [], []\n    for i, seq_info in enumerate(val_ids):\n        game_id = seq_info['game_id']\n        play_id = seq_info['play_id']\n        nfl_id = seq_info['nfl_id']\n        x_last = val_data.iloc[i]['x_last']\n        y_last = val_data.iloc[i]['y_last']\n        dx_true = y_val_dx[i]\n        dy_true = y_val_dy[i]\n        frame_ids_future = y_val_frame_ids[i]  # real future frame_ids\n        # True rows\n        for t in range(len(dx_true)):\n            true_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_ids_future[t]}\",\n                'x': x_last + dx_true[t],\n                'y': y_last + dy_true[t]\n            })\n        # Ensemble predictions\n        per_model_dx, per_model_dy = [], []\n        \n            \n        scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n        inp = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n        model.eval()\n        with torch.no_grad():\n            out = model(inp).cpu().numpy()[0]  # (H,2) cumulative dx,dy\n        per_model_dx.append(out[:,0])\n        per_model_dy.append(out[:,1])\n        ens_dx = np.mean(per_model_dx, axis=0)\n        ens_dy = np.mean(per_model_dy, axis=0)\n        # Use only required length\n        for t in range(len(dx_true)):\n            pred_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_ids_future[t]}\",\n                'x': np.clip(x_last + ens_dx[t], Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n                'y': np.clip(y_last + ens_dy[t], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX),\n            })\n    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:21.898217Z","iopub.status.busy":"2025-10-04T06:11:21.897504Z","iopub.status.idle":"2025-10-04T06:11:21.906789Z","shell.execute_reply":"2025-10-04T06:11:21.906018Z"},"papermill":{"duration":0.117424,"end_time":"2025-10-04T06:11:21.908229","exception":false,"start_time":"2025-10-04T06:11:21.790805","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"bb0c0d30","cell_type":"code","source":"# ================================================================================\n# PREDICTION UTILITIES\n# ================================================================================\n\ndef displacement_to_position(displacement_dx, displacement_dy, x_last, y_last):\n    \"\"\"\n    Convert displacement predictions to absolute positions.\n    \n    Args:\n        displacement_dx: Predicted displacement in x direction\n        displacement_dy: Predicted displacement in y direction  \n        x_last: Last known x position\n        y_last: Last known y position\n        \n    Returns:\n        pred_x, pred_y: Absolute predicted positions\n    \"\"\"\n    pred_x = x_last + displacement_dx\n    pred_y = y_last + displacement_dy\n    \n    # Apply field constraints\n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    return pred_x, pred_y\n\n\ndef predict_with_lstm(model, X_test, test_data):\n    \"\"\"\n    Make predictions with trained LSTM model.\n    \n    Args:\n        model: Trained LSTM model\n        X_test: Test sequences (batch, sequence_length, features)\n        test_data: Test dataframe for position conversion\n        \n    Returns:\n        pred_x, pred_y: Absolute predicted positions\n    \"\"\"\n    device = next(model.parameters()).device\n    model.eval()\n    \n    predictions_dx = []\n    predictions_dy = []\n    \n    # Predict in batches\n    batch_size = 1024\n    test_dataset = TensorDataset(torch.FloatTensor(X_test))\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    with torch.no_grad():\n        for batch_X, in test_loader:\n            batch_X = batch_X.to(device)\n            outputs = model(batch_X)\n            \n            predictions_dx.extend(outputs[:, 0].cpu().numpy())\n            predictions_dy.extend(outputs[:, 1].cpu().numpy())\n    \n    # Convert to absolute positions\n    pred_x, pred_y = displacement_to_position(\n        np.array(predictions_dx), \n        np.array(predictions_dy),\n        test_data['x_last'].values,\n        test_data['y_last'].values\n    )\n    \n    return pred_x, pred_y","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:22.122128Z","iopub.status.busy":"2025-10-04T06:11:22.12175Z","iopub.status.idle":"2025-10-04T06:11:22.129143Z","shell.execute_reply":"2025-10-04T06:11:22.128343Z"},"papermill":{"duration":0.115353,"end_time":"2025-10-04T06:11:22.130588","exception":false,"start_time":"2025-10-04T06:11:22.015235","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"5d0d9c20","cell_type":"markdown","source":"# Predict function","metadata":{"papermill":{"duration":0.105606,"end_time":"2025-10-04T06:11:22.344811","exception":false,"start_time":"2025-10-04T06:11:22.239205","status":"completed"},"tags":[]}},{"id":"f388bf28","cell_type":"code","source":"def make_test_predictions_lstm(models, X_test, test_seq_ids, test_input):\n    \"\"\"\n    Make predictions on test data using ensemble of trained LSTM models.\n    \n    Args:\n        models: List of trained LSTM models\n        X_test: Test sequences (batch, sequence_length, features)\n        test_seq_ids: Mapping info for test sequences\n        test_input: Original test input dataframe\n        \n    Returns:\n        submission: DataFrame with id, x, y columns\n    \"\"\"\n    print(\"Making test predictions...\")\n    \n    if len(X_test) == 0:\n        print(\"WARNING: No test sequences provided. Using fallback predictions.\")\n        # Fallback: use last known positions\n        submission = pd.DataFrame({\n            'id': (test_input['game_id'].astype(str) + '_' + \n                  test_input['play_id'].astype(str) + '_' + \n                  test_input['nfl_id'].astype(str) + '_' + \n                  test_input['frame_id'].astype(str)),\n            'x': test_input['x'].values,\n            'y': test_input['y'].values\n        })\n        return submission\n    \n    print(f\"Test sequences shape: {X_test.shape}\")\n    \n    # Get ensemble predictions\n    all_predictions_dx = []\n    all_predictions_dy = []\n    \n    for i, model in enumerate(models):\n        print(f\"Predicting with model {i+1}/{len(models)}...\")\n        \n        device = next(model.parameters()).device\n        model.eval()\n        \n        predictions_dx = []\n        predictions_dy = []\n        \n        # Predict in batches\n        batch_size = 512\n        test_dataset = TensorDataset(torch.FloatTensor(X_test))\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n        \n        with torch.no_grad():\n            for batch_X, in test_loader:\n                batch_X = batch_X.to(device)\n                outputs = model(batch_X)\n                \n                predictions_dx.extend(outputs[:, 0].cpu().numpy())\n                predictions_dy.extend(outputs[:, 1].cpu().numpy())\n        \n        all_predictions_dx.append(np.array(predictions_dx))\n        all_predictions_dy.append(np.array(predictions_dy))\n    \n    # Ensemble average\n    ensemble_dx = np.mean(all_predictions_dx, axis=0)\n    ensemble_dy = np.mean(all_predictions_dy, axis=0)\n    \n    # Initialize output arrays with NaN\n    final_pred_x = np.full(len(test_input), np.nan)\n    final_pred_y = np.full(len(test_input), np.nan)\n    \n    # Map predictions back to original test rows\n    for i, seq_info in enumerate(test_seq_ids):\n        # Find corresponding row in test_input\n        mask = ((test_input['game_id'] == seq_info['game_id']) &\n               (test_input['play_id'] == seq_info['play_id']) &\n               (test_input['nfl_id'] == seq_info['nfl_id']) &\n               (test_input['frame_id'] == seq_info['frame_id']))\n        \n        if mask.any():\n            # Get reference position\n            ref_x = test_input.loc[mask, 'x'].iloc[0]\n            ref_y = test_input.loc[mask, 'y'].iloc[0]\n            \n            # Convert displacement to absolute position\n            pred_x = ref_x + ensemble_dx[i]\n            pred_y = ref_y + ensemble_dy[i]\n            \n            # Store predictions\n            final_pred_x[mask] = pred_x\n            final_pred_y[mask] = pred_y\n    \n    # Fill any remaining NaN with original positions\n    nan_mask = np.isnan(final_pred_x)\n    final_pred_x[nan_mask] = test_input.loc[nan_mask, 'x'].values\n    final_pred_y[nan_mask] = test_input.loc[nan_mask, 'y'].values\n    \n    # Create submission DataFrame\n    submission = pd.DataFrame({\n        'id': (test_input['game_id'].astype(str) + '_' + \n              test_input['play_id'].astype(str) + '_' + \n              test_input['nfl_id'].astype(str) + '_' + \n              test_input['frame_id'].astype(str)),\n        'x': final_pred_x,\n        'y': final_pred_y\n    })\n    \n    # Final validation\n    submission['x'] = np.clip(submission['x'], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    submission['y'] = np.clip(submission['y'], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    print(f\"Created submission with {len(submission)} predictions\")\n    print(f\"X range: [{submission['x'].min():.2f}, {submission['x'].max():.2f}]\")\n    print(f\"Y range: [{submission['y'].min():.2f}, {submission['y'].max():.2f}]\")\n    \n    return submission","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:22.560329Z","iopub.status.busy":"2025-10-04T06:11:22.560052Z","iopub.status.idle":"2025-10-04T06:11:22.573146Z","shell.execute_reply":"2025-10-04T06:11:22.572217Z"},"papermill":{"duration":0.123208,"end_time":"2025-10-04T06:11:22.574588","exception":false,"start_time":"2025-10-04T06:11:22.45138","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"69f99011","cell_type":"markdown","source":"# Model definition","metadata":{"papermill":{"duration":0.104093,"end_time":"2025-10-04T06:11:22.849516","exception":false,"start_time":"2025-10-04T06:11:22.745423","status":"completed"},"tags":[]}},{"id":"a4ae4f0c","cell_type":"code","source":"class CombinedLSTMGRURegressor(nn.Module):\n    \"\"\"Parallel LSTM+GRU encoders on same input; concatenate last states.\"\"\"\n    def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.3, max_frames_output=10):\n        super().__init__()\n        self.max_frames_output = max_frames_output\n        branch_h = max(16, hidden_dim // 2)  # keep total ~hidden_dim\n\n        self.lstm = nn.LSTM(\n            input_size=input_dim, hidden_size=branch_h,\n            num_layers=num_layers, batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        self.gru = nn.GRU(\n            input_size=input_dim, hidden_size=branch_h,\n            num_layers=num_layers, batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(2 * branch_h, 128),\n            nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(64, 2 * max_frames_output)\n        )\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        gru_out, _ = self.gru(x)\n        last_lstm = lstm_out[:, -1, :]\n        last_gru  = gru_out[:, -1, :]\n        combined = torch.cat([last_lstm, last_gru], dim=-1)\n        all_outputs = self.fc(combined)\n        B = all_outputs.shape[0]\n        return all_outputs.view(B, self.max_frames_output, 2)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:23.061289Z","iopub.status.busy":"2025-10-04T06:11:23.060676Z","iopub.status.idle":"2025-10-04T06:11:23.068613Z","shell.execute_reply":"2025-10-04T06:11:23.067802Z"},"papermill":{"duration":0.115593,"end_time":"2025-10-04T06:11:23.069979","exception":false,"start_time":"2025-10-04T06:11:22.954386","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"c2668078","cell_type":"markdown","source":"# Train function","metadata":{"papermill":{"duration":0.128067,"end_time":"2025-10-04T06:11:23.305063","exception":false,"start_time":"2025-10-04T06:11:23.176996","status":"completed"},"tags":[]}},{"id":"b681bb00","cell_type":"code","source":"# Add these imports if needed\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch\n\n\ndef train_improved_lstm_model(\n    X_train, y_train_dx, y_train_dy,\n    X_val, y_val_dx, y_val_dy,\n    val_data, input_dim,\n    epochs=50, batch_size=512, learning_rate=0.001,patience = 10,\n    eval_all_frames=True,print_score_every=10  # set False to score only first frame\n):\n    X_train = np.array(X_train, dtype=np.float32)\n    X_val   = np.array(X_val, dtype=np.float32)\n\n    # Max future horizon\n    max_frames_output = Config.MAX_FUTURE_HORIZON\n    print(f\"Maximum output frames: {max_frames_output}\")\n\n    def prepare_targets_batch(batch_dx, batch_dy):\n        output_lengths = [len(dx) for dx in batch_dx]\n        tensors_dx, tensors_dy, masks = [], [], []\n        for i in range(len(batch_dx)):\n            dx_padded = np.pad(batch_dx[i], (0, max_frames_output - len(batch_dx[i])), constant_values=0)\n            dy_padded = np.pad(batch_dy[i], (0, max_frames_output - len(batch_dy[i])), constant_values=0)\n            tensors_dx.append(torch.tensor(dx_padded, dtype=torch.float32))\n            tensors_dy.append(torch.tensor(dy_padded, dtype=torch.float32))\n            mask = torch.zeros(max_frames_output)\n            mask[:len(batch_dx[i])] = 1\n            masks.append(mask)\n        batch_dx_tensor = torch.stack(tensors_dx).unsqueeze(-1)  # (B, L, 1)\n        batch_dy_tensor = torch.stack(tensors_dy).unsqueeze(-1)  # (B, L, 1)\n        batch_targets = torch.cat([batch_dx_tensor, batch_dy_tensor], dim=-1)  # (B, L, 2)\n        batch_mask = torch.stack(masks)  # (B, L)\n        return batch_targets, batch_mask, output_lengths\n\n    # Pre-batch (kept same style as original)\n    train_batches = []\n    for i in range(0, len(X_train), batch_size):\n        end = min(i + batch_size, len(X_train))\n        batch_y, batch_mask, lengths = prepare_targets_batch(\n            [y_train_dx[j] for j in range(i, end)],\n            [y_train_dy[j] for j in range(i, end)]\n        )\n        train_batches.append((torch.tensor(X_train[i:end]), batch_y, batch_mask, lengths))\n\n    val_batches = []\n    for i in range(0, len(X_val), batch_size):\n        end = min(i + batch_size, len(X_val))\n        batch_y, batch_mask, lengths = prepare_targets_batch(\n            [y_val_dx[j] for j in range(i, end)],\n            [y_val_dy[j] for j in range(i, end)]\n        )\n        val_batches.append((torch.tensor(X_val[i:end]), batch_y, batch_mask, lengths))\n\n    model = CombinedLSTMGRURegressor(\n        input_dim=input_dim,\n        hidden_dim=Config.HIDDEN_DIM,\n        num_layers=Config.NUM_LAYERS,\n        dropout=Config.DROPOUT,\n        max_frames_output=Config.MAX_FUTURE_HORIZON\n    )\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"Using device:\", device)\n    model.to(device)\n    print('sucessfully moved model to device')\n\n    criterion = nn.MSELoss(reduction='none')\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', patience=5, factor=0.5, verbose=True\n    )\n\n    best_score = float('inf')\n    best_competition_score = float('inf')\n    best_state = None\n    \n    patience_ctr = 0\n\n    # Sequence-level metadata (one row per sequence) from val_data\n    # Ensure order matches X_val / y_val arrays\n    val_meta = val_data.reset_index(drop=True)\n    assert len(val_meta) == len(X_val), \"val_data length mismatch with validation sequences\"\n\n    x_last = val_meta['x_last'].values\n    y_last = val_meta['y_last'].values\n    seq_game = val_meta['game_id'].values\n    seq_play = val_meta['play_id'].values\n    seq_nfl  = val_meta['nfl_id'].values\n\n    for epoch in range(epochs):\n        # -------- Train --------\n        model.train()\n        train_losses = []\n        for batch_X, batch_y, batch_mask, lengths in train_batches:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            batch_mask = batch_mask.to(device).unsqueeze(-1)  # (B,L,1)\n\n            outputs = model(batch_X)  # (B, max_frames_output, 2)\n\n            loss_all = criterion(outputs, batch_y)  # (B,L,2)\n            mask_expanded = batch_mask.expand_as(loss_all)\n            masked_loss = (loss_all * mask_expanded).sum() / mask_expanded.sum()\n\n            optimizer.zero_grad()\n            masked_loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(masked_loss.item())\n\n        # -------- Validate (build flattened per-frame DataFrames) --------\n        model.eval()\n        val_losses = []\n\n        pred_rows = []\n        true_rows = []\n\n        seq_cursor = 0  # global sequence index across val batches\n\n        with torch.no_grad():\n            for batch_X, batch_y, batch_mask, lengths in val_batches:\n                B = batch_X.size(0)\n                batch_X = batch_X.to(device)\n                batch_y = batch_y.to(device)\n                batch_mask_device = batch_mask.to(device).unsqueeze(-1)\n\n                outputs = model(batch_X)\n                loss_all = criterion(outputs, batch_y)\n                mask_expanded = batch_mask_device.expand_as(loss_all)\n                masked_loss = (loss_all * mask_expanded).sum() / mask_expanded.sum()\n                val_losses.append(masked_loss.item())\n\n                outputs_cpu = outputs.cpu()\n                targets_cpu = batch_y.cpu()\n\n                for b in range(B):\n                    L = lengths[b]  # valid future frames\n                    game_id = seq_game[seq_cursor]\n                    play_id = seq_play[seq_cursor]\n                    nfl_id  = seq_nfl[seq_cursor]\n                    base_x  = x_last[seq_cursor]\n                    base_y  = y_last[seq_cursor]\n\n                    valid_pred = outputs_cpu[b, :L, :]   # (L,2) dx,dy\n                    valid_true = targets_cpu[b, :L, :]   # (L,2)\n\n                    for t in range(L):\n                        frame_rel = t + 1  # start at 1\n                        dx_pred, dy_pred = valid_pred[t].tolist()\n                        dx_true, dy_true = valid_true[t].tolist()\n\n                        pred_rows.append({\n                            'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n                            'x': base_x + dx_pred,\n                            'y': base_y + dy_pred\n                        })\n                        true_rows.append({\n                            'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n                            'x': base_x + dx_true,\n                            'y': base_y + dy_true\n                        })\n\n                    seq_cursor += 1\n        mean_vloss = np.mean(val_losses)\n        print(f\"Epoch {epoch+1}/{epochs} - TrainLoss {np.mean(train_losses):.4f} ValLoss {mean_vloss:.4f}\")\n        # Build DataFrames\n        val_submission_full = pd.DataFrame(pred_rows)\n        val_truth_full = pd.DataFrame(true_rows)\n        \n        if not eval_all_frames:\n            # Keep only first-frame per id group (frame_id_rel==1)\n            val_submission_eval = val_submission_full[val_submission_full['id'].str.endswith('_1')].copy()\n            val_truth_eval = val_truth_full[val_truth_full['id'].str.endswith('_1')].copy()\n        else:\n            val_submission_eval = val_submission_full\n            val_truth_eval = val_truth_full\n        competition_score = score(val_truth_eval, val_submission_eval, 'id')\n        print(f\"Val RMSE ({'all' if eval_all_frames else 'first'} frames): {competition_score:.5f}\")\n            \n        if competition_score < best_competition_score:\n            best_competition_score = competition_score\n        \n        # Early stopping logic\n        if mean_vloss < best_score:\n            best_score = mean_vloss\n            best_state = model.state_dict()\n            patience_ctr = 0\n            print(f\" New best model found at epoch {epoch+1} with ValLoss {best_score:.4f}\")\n        else:\n            patience_ctr += 1\n\n        scheduler.step(np.mean(val_losses))\n        if patience_ctr >= patience:\n            print(f\"Early stopping epoch {epoch+1}\")\n            break\n\n    if best_state:\n        model.load_state_dict(best_state)\n\n    return model, best_score, best_competition_score\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:23.516154Z","iopub.status.busy":"2025-10-04T06:11:23.515665Z","iopub.status.idle":"2025-10-04T06:11:23.539579Z","shell.execute_reply":"2025-10-04T06:11:23.538877Z"},"papermill":{"duration":0.130385,"end_time":"2025-10-04T06:11:23.540846","exception":false,"start_time":"2025-10-04T06:11:23.410461","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"bcceba51","cell_type":"markdown","source":"# Train 1 fold","metadata":{"papermill":{"duration":0.104026,"end_time":"2025-10-04T06:11:23.752134","exception":false,"start_time":"2025-10-04T06:11:23.648108","status":"completed"},"tags":[]}},{"id":"acd3fd39","cell_type":"code","source":"\n# Train 1 fold using GroupKFold\n\nfrom sklearn.model_selection import KFold, GroupKFold\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\nprint(f\"Sequences shape: {len(sequences)}\")  # Already an object array\nprint(f\"Targets_dx: {len(targets_dx)} sequences, lengths: {[len(dx) for dx in targets_dx[:5]]}...\")  # Show first 5 lengths\nprint(f\"Targets_dy: {len(targets_dy)} sequences, lengths: {[len(dy) for dy in targets_dy[:5]]}...\")\n\n# Convert lists to numpy arrays for consistent handling\nsequences = np.array(sequences, dtype=object) \ntargets_dx = np.array(targets_dx, dtype=object)\ntargets_dy = np.array(targets_dy, dtype=object)\n\n# Get number of output frames from the targets\nnum_frames_output = [targets_dx[i].shape for i in range(len(targets_dx))]\n# print(f\"Number of output frames to predict: {num_frames_output}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:23.964102Z","iopub.status.busy":"2025-10-04T06:11:23.963733Z","iopub.status.idle":"2025-10-04T06:11:24.35843Z","shell.execute_reply":"2025-10-04T06:11:24.357721Z"},"papermill":{"duration":0.503071,"end_time":"2025-10-04T06:11:24.359919","exception":false,"start_time":"2025-10-04T06:11:23.856848","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"30e480cf","cell_type":"code","source":"# # Prepare the data for training using GroupKFold\n# groups = [d['play_id'] for d in ids]\n# gkf = GroupKFold(n_splits=5)\n# folds = list(gkf.split(sequences, groups=groups))\n\n# # Use the first fold\n# train_idx, val_idx = folds[0]\n\n# X_train_unscaled = sequences[train_idx]\n# X_val_unscaled = sequences[val_idx]\n# y_train_dx_fold = targets_dx[train_idx]\n# y_train_dy_fold = targets_dy[train_idx]\n# y_val_dx_fold = targets_dx[val_idx]\n# y_val_dy_fold = targets_dy[val_idx]\n\n# # Validation metadata (use unscaled last positions)\n# val_ids = [ids[i] for i in val_idx]\n# val_data = pd.DataFrame(val_ids)\n# val_data['x_last'] = np.array([s[-1, 0] for s in X_val_unscaled])\n# val_data['y_last'] = np.array([s[-1, 1] for s in X_val_unscaled])\n\n# # Fit scaler on training-fold frames only (no leakage)\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# train_frames = np.vstack([s for s in X_train_unscaled])  # (n_train*window, feat)\n# scaler.fit(train_frames)\n\n# # Helper to apply scaler to each sequence\n# def apply_scaler_to_sequences(seq_array, scaler):\n#     scaled = []\n#     for s in seq_array:\n#         scaled.append(scaler.transform(s))\n#     return np.array(scaled, dtype=object)\n\n# # Produce scaled arrays used for training\n# X_train_fold = apply_scaler_to_sequences(X_train_unscaled, scaler)\n# X_val_fold = apply_scaler_to_sequences(X_val_unscaled, scaler)\n\n# # Optionally persist scaler for inference\n# joblib.dump(scaler, 'lstm_feature_scaler.joblib')\n\n# # REMOVE the block that overwrote game_id/play_id/nfl_id using train_output:\n# # (Delete these lines if present)\n# # val_data['x'] = train_output.iloc[val_idx]['x'].values\n# # val_data['y'] = train_output.iloc[val_idx]['y'].values\n# # val_data['game_id'] = train_output.iloc[val_idx]['game_id'].values\n# # val_data['play_id'] = train_output.iloc[val_idx]['play_id'].values\n# # val_data['nfl_id'] = train_output.iloc[val_idx]['nfl_id'].values\n# # val_data['frame_id'] = train_output.iloc[val_idx]['frame_id'].values\n# # val_data['id'] = ...\n\n# # Alignment sanity check (optional)\n# # print(\"Alignment check (first 3):\")\n# # for k in range(min(3, len(X_val_fold))):\n# #     seq_id = val_ids[k]\n# #     dx_seq = y_val_dx_fold[k]\n# #     if len(dx_seq) == 0: \n# #         continue\n# #     approx_first_abs = val_data.loc[k, 'x_last'] + dx_seq[0]\n# #     print(seq_id, \"first_pred_abs_x_example:\", approx_first_abs)\n# # Train the model - no need to provide num_frames_output\n# input_dim = X_train_fold[0].shape[-1]\n# model, best_score = train_improved_lstm_model(\n#     X_train_fold, y_train_dx_fold, y_train_dy_fold,\n#     X_val_fold, y_val_dx_fold, y_val_dy_fold,\n#     val_data, input_dim=input_dim,\n#     epochs=200, batch_size=512, learning_rate=0.001,eval_all_frames=True\n# )\n\n# print(f\"Best validation RMSE (1st fold): {best_score:.5f}\")\n\n# # Save the model\n# torch.save(model.state_dict(), 'lstm_model.pt')\n# print(\"Model saved to 'lstm_model.pt'\")","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:24.572168Z","iopub.status.busy":"2025-10-04T06:11:24.571533Z","iopub.status.idle":"2025-10-04T06:11:24.576624Z","shell.execute_reply":"2025-10-04T06:11:24.57581Z"},"papermill":{"duration":0.113519,"end_time":"2025-10-04T06:11:24.578148","exception":false,"start_time":"2025-10-04T06:11:24.464629","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"7df0d36f","cell_type":"markdown","source":"# Submission maker","metadata":{"papermill":{"duration":0.104865,"end_time":"2025-10-04T06:11:24.789086","exception":false,"start_time":"2025-10-04T06:11:24.684221","status":"completed"},"tags":[]}},{"id":"7c9b5c5c","cell_type":"code","source":"def predict_with_improved_lstm(model, X_test, test_data,test_template=None, return_all=True):\n    \"\"\"\n    Predict cumulative displacements for each horizon.\n    Returns:\n      pred_first_x, pred_first_y, dx_cum, dy_cum, (optional) abs_all_x, abs_all_y\n    \"\"\"\n    device = next(model.parameters()).device\n    model.eval()\n    X = np.array(X_test, dtype=np.float32)\n    test_dataset = TensorDataset(torch.from_numpy(X))\n    loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    dx_list, dy_list = [], []\n    with torch.no_grad():\n        for (batch,) in loader:\n            batch = batch.to(device)\n            out = model(batch)  # (B, H, 2) cumulative displacements\n            # print(f\"Predicted batch shape: {out.shape}\")\n            dx_list.append(out[:, :, 0].cpu().numpy())\n            dy_list.append(out[:, :, 1].cpu().numpy())\n    # print(f\"Predicted {len(dx_list)} batches\")\n    if not dx_list:\n        print(\"WARNING: No predictions made. Using fallback.\")\n        empty = np.zeros((0, getattr(model, \"max_frames_output\", 1)))\n        return empty, empty, empty, empty, empty, empty\n    dx_cum = np.vstack(dx_list)\n    dy_cum = np.vstack(dy_list)\n    x_last = test_data['x_last'].values\n    y_last = test_data['y_last'].values\n    abs_all_x = x_last[:, None] + dx_cum\n    abs_all_y = y_last[:, None] + dy_cum\n    abs_all_x = np.clip(abs_all_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    abs_all_y = np.clip(abs_all_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    pred_first_x = abs_all_x[:, 0]\n    pred_first_y = abs_all_y[:, 0]\n    # print(pred_first_x.shape, pred_first_y.shape, dx_cum.shape, dy_cum.shape, abs_all_x.shape, abs_all_y.shape)\n    # print(abs_all_x[0])\n    if return_all:\n        return pred_first_x, pred_first_y, dx_cum, dy_cum, abs_all_x, abs_all_y\n    # print(pred_first_x.shape, pred_first_y.shape, dx_cum.shape, dy_cum.shape)\n    return pred_first_x, pred_first_y, dx_cum, dy_cum","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:25.008013Z","iopub.status.busy":"2025-10-04T06:11:25.007436Z","iopub.status.idle":"2025-10-04T06:11:25.015789Z","shell.execute_reply":"2025-10-04T06:11:25.015223Z"},"papermill":{"duration":0.12206,"end_time":"2025-10-04T06:11:25.017092","exception":false,"start_time":"2025-10-04T06:11:24.895032","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"479c11e1","cell_type":"code","source":"def create_ensemble_predictions(models, scalers, X_test_unscaled, test_seq_ids, test_template):\n    \"\"\"\n    Ensemble cumulative displacement predictions across models/folds.\n    Produces one row per (game_id, play_id, nfl_id, frame_id) present in test_template.\n    \n    Args:\n        models: list[ImprovedLSTMRegressor]\n        scalers: list[StandardScaler] (same length as models) or None\n        X_test_unscaled: list/array of shape (N, seq_len, feats)\n        test_seq_ids: list of dicts including metadata for each sequence\n        test_template: DataFrame with required submission rows\n    \n    Returns:\n        submission: DataFrame with id, x, y columns for all frames in test_template.\n    \"\"\"\n    if len(models) == 0:\n        print(\"No models supplied.\")\n        return None\n    if scalers is not None and len(scalers) != len(models):\n        raise ValueError(\"Length of scalers must match models or be None.\")\n\n    # Convert test sequences to numpy array\n    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n    test_meta = pd.DataFrame(test_seq_ids)\n\n    # Prepare last observed positions\n    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n    test_meta['x_last'] = x_last\n    test_meta['y_last'] = y_last\n\n    # Ensemble predictions\n    per_model_abs = []\n    max_h = 0\n    for i, model in enumerate(models):\n        scaler = scalers[i] if scalers is not None else None\n        if scaler is not None:\n            scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n        else:\n            scaled = np.array(X_test_unscaled, dtype=object)\n        # Convert object -> stacked float tensor\n        stacked = np.stack(scaled.astype(np.float32))\n        _, _, _, _, abs_all_x, abs_all_y = predict_with_improved_lstm(\n            model, stacked, test_meta, return_all=True\n        )\n        per_model_abs.append((abs_all_x, abs_all_y))\n        max_h = max(max_h, abs_all_x.shape[1])\n\n    # Pad & average predictions across models\n    M = len(per_model_abs)\n    N = len(test_meta)\n    pad_x = np.full((M, N, max_h), np.nan, dtype=np.float32)\n    pad_y = np.full((M, N, max_h), np.nan, dtype=np.float32)\n    for m, (ax, ay) in enumerate(per_model_abs):\n        h = ax.shape[1]\n        pad_x[m, :, :h] = ax\n        pad_y[m, :, :h] = ay\n    ens_x = np.nanmean(pad_x, axis=0)\n    ens_y = np.nanmean(pad_y, axis=0)\n\n    # Create submission DataFrame\n    out_rows = []\n    for i, seq_info in test_meta.iterrows():\n        # game_id, play_id, nfl_id = seq_info['game_id'], seq_info['play_id'], seq_info['nfl_id']\n        game_id = int(seq_info['game_id'])\n        play_id = int(seq_info['play_id'])\n        nfl_id = int(seq_info['nfl_id'])\n        frame_ids = test_template[\n            (test_template['game_id'] == game_id) &\n            (test_template['play_id'] == play_id) &\n            (test_template['nfl_id'] == nfl_id)\n        ]['frame_id'].sort_values().tolist()\n\n        for t, frame_id in enumerate(frame_ids):\n            if t < ens_x.shape[1]:  # Ensure we don't exceed the predicted horizon\n                px = ens_x[i, t]\n                py = ens_y[i, t]\n            else:\n                # If predictions are shorter than required frames, use the last prediction\n                px = ens_x[i, -1]\n                py = ens_y[i, -1]\n\n            out_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n                'x': np.clip(px, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n                'y': np.clip(py, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX),\n            })\n\n    submission = pd.DataFrame(out_rows)\n    return submission","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:25.23006Z","iopub.status.busy":"2025-10-04T06:11:25.229591Z","iopub.status.idle":"2025-10-04T06:11:25.24198Z","shell.execute_reply":"2025-10-04T06:11:25.241251Z"},"papermill":{"duration":0.120905,"end_time":"2025-10-04T06:11:25.243239","exception":false,"start_time":"2025-10-04T06:11:25.122334","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"5adea99a","cell_type":"code","source":"def create_ensemble_val_predictions(models, scalers, X_val_unscaled, val_ids, y_val_dx_fold, y_val_dy_fold, val_data, exclude_fold=None):\n    \"\"\"\n    Generate ensemble predictions for validation data and prepare for scoring.\n    Excludes the model from the same fold to prevent potential overfitting/leakage.\n    \n    Args:\n        models: List of trained models\n        scalers: List of scalers (one per model)\n        X_val_unscaled: Validation sequences (unscaled)\n        val_ids: List of dicts with sequence metadata\n        y_val_dx_fold, y_val_dy_fold: Ground truth displacements\n        val_data: DataFrame with x_last, y_last\n        exclude_fold: Index of the fold to exclude (0-based)\n    \n    Returns:\n        ensemble_pred_df, ensemble_true_df: DataFrames for scoring\n    \"\"\"\n    pred_rows = []\n    true_rows = []\n    \n    for i, seq_info in enumerate(val_ids):\n        game_id = seq_info['game_id']\n        play_id = seq_info['play_id']\n        nfl_id = seq_info['nfl_id']\n        x_last = val_data.iloc[i]['x_last']\n        y_last = val_data.iloc[i]['y_last']\n        \n        # Ground truth\n        dx_true = y_val_dx_fold[i]\n        dy_true = y_val_dy_fold[i]\n        for t in range(len(dx_true)):\n            frame_rel = t + 1\n            true_x = x_last + dx_true[t]\n            true_y = y_last + dy_true[t]\n            true_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n                'x': true_x,\n                'y': true_y\n            })\n        \n        # Ensemble predictions (exclude the model from the same fold)\n        per_model_dx = []\n        per_model_dy = []\n        for j, model in enumerate(models):\n            if exclude_fold is not None and j == exclude_fold:\n                continue  # Skip the model trained on this fold\n            scaler = scalers[j]\n            scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n            scaled_seq = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n            model.eval()\n            with torch.no_grad():\n                output = model(scaled_seq).cpu().numpy()[0]  # (max_frames_output, 2)\n            per_model_dx.append(output[:, 0])\n            per_model_dy.append(output[:, 1])\n        \n        # Average across remaining models\n        if per_model_dx:  # Ensure there are models to average\n            ens_dx = np.mean(per_model_dx, axis=0)\n            ens_dy = np.mean(per_model_dy, axis=0)\n        else:\n            # Fallback: use the last known position (though this shouldn't happen with n_folds > 1)\n            ens_dx = np.zeros(len(dx_true))\n            ens_dy = np.zeros(len(dy_true))\n        \n        # Generate predictions for each frame\n        for t in range(len(dx_true)):\n            pred_x = x_last + ens_dx[t]\n            pred_y = y_last + ens_dy[t]\n            pred_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{t+1}\",\n                'x': np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n                'y': np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            })\n    \n    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:25.51835Z","iopub.status.busy":"2025-10-04T06:11:25.517675Z","iopub.status.idle":"2025-10-04T06:11:25.528438Z","shell.execute_reply":"2025-10-04T06:11:25.527506Z"},"papermill":{"duration":0.179862,"end_time":"2025-10-04T06:11:25.529846","exception":false,"start_time":"2025-10-04T06:11:25.349984","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"9cb64f30","cell_type":"markdown","source":"# 5folds training","metadata":{"papermill":{"duration":0.105078,"end_time":"2025-10-04T06:11:25.741719","exception":false,"start_time":"2025-10-04T06:11:25.636641","status":"completed"},"tags":[]}},{"id":"8736ecdb","cell_type":"code","source":"def run_multi_fold_training(sequences, targets_dx, targets_dy, targets_frame_ids, ids,\n                            lr=Config.LEARNING_RATE, n_folds=5, epochs=15, patience=5):\n    # Ensure numpy object arrays for advanced indexing\n    if not isinstance(sequences, np.ndarray):\n        sequences = np.array(sequences, dtype=object)\n    if not isinstance(targets_dx, np.ndarray):\n        targets_dx = np.array(targets_dx, dtype=object)\n    if not isinstance(targets_dy, np.ndarray):\n        targets_dy = np.array(targets_dy, dtype=object)\n    if not isinstance(targets_frame_ids, np.ndarray):\n        targets_frame_ids = np.array(targets_frame_ids, dtype=object)\n    groups = [d['play_id'] for d in ids]\n    gkf = GroupKFold(n_splits=n_folds)\n    folds = list(gkf.split(sequences, groups=groups))\n    input_dim = sequences[0].shape[-1]\n    print(f\"Input feature dimension: {input_dim}\")\n    models, scalers, fold_scores = [], [], []\n    # Store per-fold validation pieces for OOF\n    oof_pred_parts, oof_true_parts = [], []\n    for fold, (train_idx, val_idx) in enumerate(folds):\n        print(f\"\\n--- Training Fold {fold+1}/{n_folds} ---\")\n        X_train_unscaled = sequences[train_idx]\n        X_val_unscaled = sequences[val_idx]\n        y_train_dx_fold = targets_dx[train_idx]\n        y_train_dy_fold = targets_dy[train_idx]\n        y_val_dx_fold = targets_dx[val_idx]\n        y_val_dy_fold = targets_dy[val_idx]\n        y_val_frame_ids_fold = targets_frame_ids[val_idx]\n        # Meta\n        val_ids = [ids[i] for i in val_idx]\n        val_data = pd.DataFrame(val_ids)\n        val_data['x_last'] = np.array([s[-1,0] for s in X_val_unscaled])\n        val_data['y_last'] = np.array([s[-1,1] for s in X_val_unscaled])\n        # Scaler\n        scaler = StandardScaler()\n        scaler.fit(np.vstack(X_train_unscaled))\n        def apply_scaler(arr):\n            return np.array([scaler.transform(s) for s in arr], dtype=object)\n        X_train_fold = apply_scaler(X_train_unscaled)\n        X_val_fold = apply_scaler(X_val_unscaled)\n        model, best_loss, best_metric = train_improved_lstm_model(\n            X_train_fold, y_train_dx_fold, y_train_dy_fold,\n            X_val_fold, y_val_dx_fold, y_val_dy_fold,\n            val_data, input_dim=input_dim,\n            epochs=epochs, batch_size=Config.BATCH_SIZE, learning_rate=lr,\n            eval_all_frames=True, patience=patience\n        )\n        fold_scores.append(best_metric)\n        models.append(model); scalers.append(scaler)\n        # save scaler for this fold\n        # make directory if not exists\n        import os\n        if not os.path.exists(f'fold_{fold+1}'):\n            os.makedirs(f'fold_{fold+1}')\n        joblib.dump(scaler, f'fold_{fold+1}/lstm_feature_scaler_fold.joblib')\n        print(f\"Scaler for fold {fold+1} saved to 'fold_{fold+1}/lstm_feature_scaler_fold.joblib'\")\n        # Save model for this fold\n        torch.save({\n            'state_dict': model.state_dict(),\n            'config': {\n                'input_dim': input_dim,\n                'hidden_dim': Config.HIDDEN_DIM,\n                'num_layers': Config.NUM_LAYERS,\n                'dropout': Config.DROPOUT,\n                'max_frames_output': Config.MAX_FUTURE_HORIZON\n            }\n        }, f'fold_{fold+1}/lstm_model_fold.pt')\n        print(f\"Model for fold {fold+1} saved to 'fold_{fold+1}/lstm_model_fold.pt'\")\n        oof_pred_fold, oof_true_fold = create_oof_predictions(\n            model=model, scaler=scaler,\n            X_val_unscaled=X_val_unscaled,\n            val_ids=val_ids,\n            y_val_dx=y_val_dx_fold,\n            y_val_dy=y_val_dy_fold,\n            y_val_frame_ids=y_val_frame_ids_fold,\n            val_data=val_data\n        )\n        print('shape of oof_pred_fold:', oof_pred_fold.shape)\n        oof_pred_parts.append(oof_pred_fold)\n        oof_true_parts.append(oof_true_fold)\n    oof_pred_df = pd.concat(oof_pred_parts, ignore_index=True)\n    oof_true_df = pd.concat(oof_true_parts, ignore_index=True)\n    # Deduplicate if any (shouldn't but safe)\n    oof_pred_df = oof_pred_df.drop_duplicates('id')\n    oof_true_df = oof_true_df.drop_duplicates('id')\n    print(f\"OOF predictions example:\\n{oof_pred_df.head()}\")\n    print(f\"OOF truth example:\\n{oof_true_df.head()}\")\n    print(f\"\\nOOF predictions shape: {oof_pred_df.shape}, OOF truth shape: {oof_true_df.shape}\")\n    cv_score = score(oof_true_df, oof_pred_df, 'id')\n    print(\"\\n--- Multi-Fold Training Summary ---\")\n    for i, fs in enumerate(fold_scores):\n        print(f\"Fold {i+1}: {fs:.5f}\")\n    print(f\"Mean Single-Model Fold Score: {np.mean(fold_scores):.5f}  {np.std(fold_scores):.5f}\")\n    print(f\"OOF CV Score: {cv_score:.5f}\")\n    return models, fold_scores, scalers, cv_score, oof_pred_df\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:25.954646Z","iopub.status.busy":"2025-10-04T06:11:25.95434Z","iopub.status.idle":"2025-10-04T06:11:25.968899Z","shell.execute_reply":"2025-10-04T06:11:25.968228Z"},"papermill":{"duration":0.122843,"end_time":"2025-10-04T06:11:25.970176","exception":false,"start_time":"2025-10-04T06:11:25.847333","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"20ce9484","cell_type":"code","source":"def load_trained_models(num_models, input_dim=None, max_frames_output=None, models_dir=None,\n                        default_hidden=Config.HIDDEN_DIM, default_layers=Config.NUM_LAYERS, default_dropout=Config.DROPOUT):\n    \"\"\"\n    Robust loader: handles (a) new checkpoints with config dict, (b) old state_dict-only files.\n    \"\"\"\n    models = []\n    scalers = []\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    for fold in range(num_models):\n\n        model_path = f'fold_{fold+1}/lstm_model_fold.pt'\n        scaler_path = f'fold_{fold+1}/lstm_feature_scaler_fold.joblib'\n        if models_dir is not None:\n            model_path  = f\"{models_dir}/fold_{fold+1}/lstm_model_fold.pt\"\n            scaler_path = f\"{models_dir}/fold_{fold+1}/lstm_feature_scaler_fold.joblib\"\n        try:\n            ckpt = torch.load(model_path, map_location=device)\n            if isinstance(ckpt, dict) and 'state_dict' in ckpt:\n                print(f\"Loading fold {fold+1} from checkpoint with config...\")\n                state_dict = ckpt['state_dict']\n                cfg = ckpt.get('config', {})\n                _input_dim = cfg.get('input_dim', input_dim)\n                _hidden = cfg.get('hidden_dim', default_hidden)\n                _layers = cfg.get('num_layers', default_layers)\n                _dropout = cfg.get('dropout', default_dropout)\n                _max_out = cfg.get('max_frames_output', max_frames_output)\n            else:\n                # Old format: only state_dict\n                state_dict = ckpt\n                _input_dim = input_dim\n                _hidden = default_hidden\n                _layers = default_layers\n                _dropout = default_dropout\n                _max_out = max_frames_output\n            if _input_dim is None or _max_out is None:\n                print(f\"[Fold {fold+1}] Missing input_dim or max_frames_output; cannot load.\")\n                continue\n            model = CombinedLSTMGRURegressor(\n                input_dim=input_dim,\n                hidden_dim=Config.HIDDEN_DIM,           # will be split across branches\n                num_layers=Config.NUM_LAYERS,\n                dropout=Config.DROPOUT,\n                max_frames_output=Config.MAX_FUTURE_HORIZON\n            )\n            model.load_state_dict(state_dict, strict=True)\n            model.to(device)\n            model.eval()\n            models.append(model)\n            print(f\"Loaded fold {fold+1} (hidden={_hidden}, layers={_layers}, max_out={_max_out})\")\n        except FileNotFoundError:\n            print(f\"Checkpoint not found: {model_path}\")\n        except Exception as e:\n            print(f\"Error loading {model_path}: {e}\")\n        # Load scaler\n        try:\n            scaler = joblib.load(scaler_path)\n            scalers.append(scaler)\n            print(f\"Loaded scaler for fold {fold+1}\")\n        except FileNotFoundError:\n            print(f\"Scaler not found: {scaler_path}\")\n            scalers.append(None)\n        except Exception as e:\n            print(f\"Error loading scaler {scaler_path}: {e}\")\n            scalers.append(None)\n    return models, scalers\n# ...existing code...","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:26.182419Z","iopub.status.busy":"2025-10-04T06:11:26.182103Z","iopub.status.idle":"2025-10-04T06:11:26.192245Z","shell.execute_reply":"2025-10-04T06:11:26.191442Z"},"papermill":{"duration":0.118178,"end_time":"2025-10-04T06:11:26.193773","exception":false,"start_time":"2025-10-04T06:11:26.075595","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"e40fdc49","cell_type":"markdown","source":"## Train","metadata":{"papermill":{"duration":0.106406,"end_time":"2025-10-04T06:11:26.406555","exception":false,"start_time":"2025-10-04T06:11:26.300149","status":"completed"},"tags":[]}},{"id":"bf6df7f1","cell_type":"code","source":"# Check NaN in sequences robustly\nnan_count = 0\nfor i, seq in enumerate(sequences):\n    try:\n        arr = np.array(seq, dtype=np.float32)\n        if np.isnan(arr).any():\n            nan_mask = np.isnan(arr)\n            nan_features = np.where(nan_mask.any(axis=0))[0]\n            print(f\"WARNING: NaN values found in sequence index {i}, feature columns: {nan_features}\")\n            nan_count += 1\n    except Exception as e:\n        print(f\"Could not check sequence {i}: {e}\")\nprint(f\"Total sequences with NaN: {nan_count}\")","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:26.621806Z","iopub.status.busy":"2025-10-04T06:11:26.62112Z","iopub.status.idle":"2025-10-04T06:11:27.083592Z","shell.execute_reply":"2025-10-04T06:11:27.082636Z"},"papermill":{"duration":0.573392,"end_time":"2025-10-04T06:11:27.085043","exception":false,"start_time":"2025-10-04T06:11:26.511651","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"7d3e76ce","cell_type":"code","source":"models,scores,scalers,ensemble_score,oof_pred_df=run_multi_fold_training(sequences, targets_dx, targets_dy,targets_frame_ids, ids,n_folds=Config.N_FOLDS,epochs=Config.EPOCHS,patience=Config.PATIENCE)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T06:11:27.29871Z","iopub.status.busy":"2025-10-04T06:11:27.297992Z","iopub.status.idle":"2025-10-04T07:43:01.197921Z","shell.execute_reply":"2025-10-04T07:43:01.196984Z"},"papermill":{"duration":5494.009627,"end_time":"2025-10-04T07:43:01.199485","exception":false,"start_time":"2025-10-04T06:11:27.189858","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"73c19620","cell_type":"markdown","source":"# Infer","metadata":{"papermill":{"duration":0.17515,"end_time":"2025-10-04T07:43:01.550251","exception":false,"start_time":"2025-10-04T07:43:01.375101","status":"completed"},"tags":[]}},{"id":"7d7464d2","cell_type":"code","source":"test_sequences, test_seq_ids = prepare_sequences_for_lstm(test_input,test_template=test_template,is_training=False,)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T07:43:01.903922Z","iopub.status.busy":"2025-10-04T07:43:01.902672Z","iopub.status.idle":"2025-10-04T07:43:03.843408Z","shell.execute_reply":"2025-10-04T07:43:03.842518Z"},"papermill":{"duration":2.119841,"end_time":"2025-10-04T07:43:03.84458","exception":false,"start_time":"2025-10-04T07:43:01.724739","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"fb009add","cell_type":"code","source":"len(test_sequences), len(test_seq_ids)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T07:43:04.194487Z","iopub.status.busy":"2025-10-04T07:43:04.193682Z","iopub.status.idle":"2025-10-04T07:43:04.199841Z","shell.execute_reply":"2025-10-04T07:43:04.199216Z"},"papermill":{"duration":0.183489,"end_time":"2025-10-04T07:43:04.20101","exception":false,"start_time":"2025-10-04T07:43:04.017521","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"aeee545d","cell_type":"code","source":"\n# Example of making ensemble predictions:\nloaded_models, scalers = load_trained_models(\n    num_models=Config.N_FOLDS,\n    input_dim=sequences.shape[-1],\n    # max_frames_output=targets_dx[0].shape[0]\n    max_frames_output=94,\n    models_dir=Config.PRETRAIN_DIR\n)\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T07:43:04.55295Z","iopub.status.busy":"2025-10-04T07:43:04.5526Z","iopub.status.idle":"2025-10-04T07:43:04.595145Z","shell.execute_reply":"2025-10-04T07:43:04.594257Z"},"papermill":{"duration":0.219808,"end_time":"2025-10-04T07:43:04.596516","exception":false,"start_time":"2025-10-04T07:43:04.376708","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"b7defd46","cell_type":"code","source":"\nensemble_submission = create_ensemble_predictions(\n    models=loaded_models,\n    scalers=scalers,                 # previously returned\n    X_test_unscaled=test_sequences,\n    test_seq_ids=test_seq_ids,\n    test_template=test_template,\n)\nensemble_submission.to_csv('submission.csv', index=False)\nprint(\"Ensemble predictions saved to 'submission.csv'\")","metadata":{"execution":{"iopub.execute_input":"2025-10-04T07:43:05.022492Z","iopub.status.busy":"2025-10-04T07:43:05.021874Z","iopub.status.idle":"2025-10-04T07:43:05.94061Z","shell.execute_reply":"2025-10-04T07:43:05.939833Z"},"papermill":{"duration":1.094455,"end_time":"2025-10-04T07:43:05.941995","exception":false,"start_time":"2025-10-04T07:43:04.84754","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"ab049b56","cell_type":"code","source":"ensemble_submission","metadata":{"execution":{"iopub.execute_input":"2025-10-04T07:43:06.289997Z","iopub.status.busy":"2025-10-04T07:43:06.289403Z","iopub.status.idle":"2025-10-04T07:43:06.30681Z","shell.execute_reply":"2025-10-04T07:43:06.306003Z"},"papermill":{"duration":0.191485,"end_time":"2025-10-04T07:43:06.308277","exception":false,"start_time":"2025-10-04T07:43:06.116792","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}