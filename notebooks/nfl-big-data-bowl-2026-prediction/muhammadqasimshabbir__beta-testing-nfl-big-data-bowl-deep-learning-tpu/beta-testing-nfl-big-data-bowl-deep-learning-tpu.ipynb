{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":4029121,"sourceType":"datasetVersion","datasetId":2286611}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile /kaggle/working/nfl_big_data_bowl_2026_optimiezed_train.py\n# =============================================================================\n# NFL Big Data Bowl 2026 - RMSE-Optimized Deep Learning Solution\n# =============================================================================\n\"\"\"\nDirect RMSE optimization for NFL trajectory prediction.\nDesigned for TPU training with single core.\n\nDependencies:\n-------------\nRequired for training:\n    - torch, pandas, numpy, sklearn, tqdm\n\nRequired for API predictions:\n    - polars (install with: pip install polars)\n\nTPU Usage in Kaggle:\n--------------------\nMethod 1 - Environment variable (recommended):\n    !USE_TPU=True python /kaggle/working/nfl_big_data_bowl_2026_optimiezed_train.py\n\nMethod 2 - In Python code:\n    import os\n    os.environ['USE_TPU'] = 'True'\n    exec(open('/kaggle/working/nfl_big_data_bowl_2026_optimiezed_train.py').read())\n\nFor TPU runtime setup:\n    1. Enable TPU in Kaggle notebook settings (Settings → Accelerator → TPU v3-8)\n    2. torch_xla is usually pre-installed in TPU runtime\n    3. If needed, install: \n       !pip install cloud-tpu-client==0.10 torch-xla[tpu]==2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n\nSingle core TPU usage:\n    The code automatically detects and uses TPU when available and USE_TPU=True.\n    For single core, use: XLA_USE_BF16=1 to enable bfloat16 precision.\n\nNote: polars is only required when running in competition mode (API predictions).\nTraining can proceed without polars, but prediction API requires it.\n\"\"\"\n\nimport os\nUSE_CUDF = False\ntry:\n    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n    import pandas as pd\n    import numpy as np\n    USE_CUDF = True\n    print(\"Using cuda_backend pandas for faster parallel data processing\")\nexcept Exception:\n    print(\"CUDA DF not available, using standard pandas\")\n    import pandas as pd\n    import numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import autocast, GradScaler\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# =============================================================================\n# COMPETITION METRIC - DIRECT RMSE LOSS\n# =============================================================================\n\nclass RMSELoss(nn.Module):\n    \"\"\"\n    Direct RMSE loss matching competition evaluation metric.\n    Computes: sqrt(0.5 * (MSE_x + MSE_y))\n    This directly optimizes the competition metric.\n    \"\"\"\n    \n    def __init__(self, reduction='mean'):\n        super().__init__()\n        self.reduction = reduction\n        self.mse_loss = nn.MSELoss(reduction='none')\n    \n    def forward(self, pred_x, pred_y, target_x, target_y, mask):\n        \"\"\"\n        Args:\n            pred_x: (batch, horizon) predicted x coordinates\n            pred_y: (batch, horizon) predicted y coordinates\n            target_x: (batch, horizon) target x coordinates\n            target_y: (batch, horizon) target y coordinates\n            mask: (batch, horizon) valid prediction mask\n        \"\"\"\n        # Compute MSE for x and y separately\n        mse_x = self.mse_loss(pred_x, target_x)\n        mse_y = self.mse_loss(pred_y, target_y)\n        \n        # Apply mask (only valid positions contribute)\n        mse_x = (mse_x * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n        mse_y = (mse_y * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n        \n        # Competition metric: sqrt(0.5 * (MSE_x + MSE_y))\n        combined_mse = 0.5 * (mse_x + mse_y)\n        rmse = torch.sqrt(combined_mse + 1e-8)\n        \n        if self.reduction == 'mean':\n            return rmse.mean()\n        elif self.reduction == 'sum':\n            return rmse.sum()\n        else:\n            return rmse\n\nclass TemporalRMSELoss(nn.Module):\n    \"\"\"\n    RMSE loss with temporal weighting for trajectory prediction.\n    Gives more weight to earlier predictions (ball closer to release).\n    \"\"\"\n    \n    def __init__(self, time_decay=0.02, reduction='mean'):\n        super().__init__()\n        self.time_decay = time_decay\n        self.reduction = reduction\n        self.mse_loss = nn.MSELoss(reduction='none')\n        \n    def forward(self, pred_x, pred_y, target_x, target_y, mask):\n        \"\"\"Forward pass with temporal weighting\"\"\"\n        # Compute MSE\n        mse_x = self.mse_loss(pred_x, target_x)\n        mse_y = self.mse_loss(pred_y, target_y)\n        \n        # Temporal weights (decay over time)\n        horizon = pred_x.size(1)\n        time_weights = torch.exp(-self.time_decay * torch.arange(horizon, device=pred_x.device, dtype=torch.float32))\n        time_weights = time_weights.view(1, -1)\n        \n        # Apply temporal weights and mask\n        weighted_mse_x = (mse_x * mask * time_weights).sum(dim=1) / ((mask * time_weights).sum(dim=1) + 1e-8)\n        weighted_mse_y = (mse_y * mask * time_weights).sum(dim=1) / ((mask * time_weights).sum(dim=1) + 1e-8)\n        \n        # Competition metric\n        combined_mse = 0.5 * (weighted_mse_x + weighted_mse_y)\n        rmse = torch.sqrt(combined_mse + 1e-8)\n        \n        if self.reduction == 'mean':\n            return rmse.mean()\n        elif self.reduction == 'sum':\n            return rmse.sum()\n        else:\n            return rmse\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\nclass GlobalConfig:\n    \"\"\"Global configuration constants\"\"\"\n    YARDS_TO_METERS = 0.9144\n    FPS = 10.0\n    FIELD_LENGTH = 120.0\n    FIELD_WIDTH = 53.3\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    OUTPUT_DIR = Path(\"./outputs\")\n\nclass ModelConfig:\n    \"\"\"Model training configuration\"\"\"\n    SEED = 42\n    SEEDS = [7, 42, 123]  # Ensemble seeds\n    N_FOLDS = 4\n    BATCH_SIZE = 256\n    EPOCHS = 30  # Increased for better convergence\n    PATIENCE = 10\n    LEARNING_RATE = 1e-3\n    WINDOW_SIZE = 10\n    HIDDEN_DIM = 256  # Increased capacity\n    MAX_FUTURE_HORIZON = 94\n    \n    # Device setup - auto-detect TPU\n    _check_tpu = False\n    try:\n        import torch_xla\n        _check_tpu = True\n    except ImportError:\n        pass\n    \n    USE_TPU = _check_tpu and os.getenv('USE_TPU', 'False').lower() == 'true'\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # RMSE optimization specific\n    USE_TEMPORAL_WEIGHTING = True\n    TIME_DECAY = 0.02\n\n# =============================================================================\n# SPECIALIZED ARCHITECTURE FOR TRAJECTORY PREDICTION\n# =============================================================================\n\nclass TrajectoryPredictionModel(nn.Module):\n    \"\"\"\n    Specialized neural network for NFL player trajectory prediction.\n    Architecture:\n    1. Input projection + positional encoding\n    2. Bidirectional LSTM for temporal context\n    3. Transformer encoder for spatial-temporal attention\n    4. Multi-head attention pooling\n    5. Residual connections\n    6. Dual prediction heads (x and y)\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=256, num_layers=3, num_heads=8, dropout=0.1, horizon=94):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.horizon = horizon\n        \n        # Input processing\n        self.input_projection = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.PReLU(),\n            nn.Dropout(dropout * 0.5)\n        )\n        \n        # Positional encoding for sequence\n        self.pos_encoding = nn.Parameter(torch.randn(1, 10, hidden_dim) * 0.02)\n        \n        # Bidirectional LSTM for temporal modeling\n        self.lstm = nn.LSTM(\n            hidden_dim, \n            hidden_dim // 2, \n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Transformer encoder layers for attention\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        # Multi-head attention pooling\n        self.attention_pool = nn.MultiheadAttention(\n            hidden_dim, \n            num_heads=num_heads,\n            batch_first=True,\n            dropout=dropout\n        )\n        self.pool_query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n        self.pool_norm = nn.LayerNorm(hidden_dim)\n        \n        # Residual connection\n        self.residual_proj = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Dual prediction heads with shared base\n        self.shared_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.PReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LayerNorm(hidden_dim // 2),\n            nn.PReLU(),\n            nn.Dropout(dropout * 0.5)\n        )\n        \n        # Separate heads for x and y (allows different learning rates)\n        self.head_x = nn.Sequential(\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.PReLU(),\n            nn.Dropout(dropout * 0.25),\n            nn.Linear(hidden_dim // 4, horizon)\n        )\n        \n        self.head_y = nn.Sequential(\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.PReLU(),\n            nn.Dropout(dropout * 0.25),\n            nn.Linear(hidden_dim // 4, horizon)\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (batch, sequence_len, input_dim) input sequences\n        Returns:\n            pred_x: (batch, horizon) predicted x deltas\n            pred_y: (batch, horizon) predicted y deltas\n        \"\"\"\n        batch_size, seq_len, _ = x.shape\n        \n        # Input projection\n        x_proj = self.input_projection(x)\n        \n        # Add positional encoding\n        x_proj = x_proj + self.pos_encoding[:, :seq_len, :]\n        \n        # LSTM encoding\n        lstm_out, _ = self.lstm(x_proj)\n        \n        # Transformer encoding with residual\n        transformer_out = self.transformer(lstm_out)\n        transformer_out = transformer_out + self.residual_proj(lstm_out)\n        \n        # Attention pooling\n        query = self.pool_query.expand(batch_size, -1, -1)\n        pooled, _ = self.attention_pool(query, transformer_out, transformer_out)\n        pooled = self.pool_norm(pooled.squeeze(1))\n        \n        # Shared representation\n        shared_repr = self.shared_head(pooled)\n        \n        # Dual predictions\n        delta_x = self.head_x(shared_repr)\n        delta_y = self.head_y(shared_repr)\n        \n        # Cumulative sum for absolute positions (if needed)\n        # For this model, we predict deltas directly\n        return delta_x, delta_y\n\n# =============================================================================\n# UTILITY FUNCTIONS (from original solution)\n# =============================================================================\n\ndef set_random_seed(seed=42):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ndef wrap_angle_degrees(angle_series):\n    \"\"\"Wrap angles to (-180, 180] range\"\"\"\n    return ((angle_series + 180.0) % 360.0) - 180.0\n\ndef unify_field_direction(dataframe):\n    \"\"\"Mirror rightward plays so all samples are left-oriented\"\"\"\n    if 'play_direction' not in dataframe.columns:\n        return dataframe\n    \n    df = dataframe.copy()\n    right_plays = df['play_direction'].eq('right')\n    \n    if 'x' in df.columns:\n        df.loc[right_plays, 'x'] = GlobalConfig.FIELD_LENGTH - df.loc[right_plays, 'x']\n    if 'y' in df.columns:\n        df.loc[right_plays, 'y'] = GlobalConfig.FIELD_WIDTH - df.loc[right_plays, 'y']\n    \n    for angle_col in ('dir', 'o'):\n        if angle_col in df.columns:\n            df.loc[right_plays, angle_col] = (df.loc[right_plays, angle_col] + 180.0) % 360.0\n    \n    if 'ball_land_x' in df.columns:\n        df.loc[right_plays, 'ball_land_x'] = GlobalConfig.FIELD_LENGTH - df.loc[right_plays, 'ball_land_x']\n    if 'ball_land_y' in df.columns:\n        df.loc[right_plays, 'ball_land_y'] = GlobalConfig.FIELD_WIDTH - df.loc[right_plays, 'ball_land_y']\n    \n    return df\n\ndef revert_to_original_direction(unified_x, unified_y, is_right_play):\n    \"\"\"Convert unified coordinates back to original field direction\"\"\"\n    if not is_right_play:\n        return float(unified_x), float(unified_y)\n    return float(GlobalConfig.FIELD_LENGTH - unified_x), float(GlobalConfig.FIELD_WIDTH - unified_y)\n\ndef create_play_direction_mapping(input_dataframe):\n    \"\"\"Create play direction mapping for coordinate unification\"\"\"\n    direction_series = (\n        input_dataframe[['game_id', 'play_id', 'play_direction']]\n        .drop_duplicates()\n        .set_index(['game_id', 'play_id'])['play_direction']\n    )\n    return direction_series\n\ndef apply_direction_unification(dataframe, direction_mapping):\n    \"\"\"Apply direction unification to dataframe\"\"\"\n    if 'play_direction' not in dataframe.columns:\n        direction_df = direction_mapping.reset_index()\n        dataframe = dataframe.merge(direction_df, on=['game_id', 'play_id'], how='left', validate='many_to_one')\n    return unify_field_direction(dataframe)\n\n# =============================================================================\n# FEATURE ENGINEERING (simplified version)\n# =============================================================================\n\nclass FeatureEngineer:\n    \"\"\"Simplified feature engineering for faster training\"\"\"\n    \n    def __init__(self):\n        self.grouping = ['game_id', 'play_id', 'nfl_id']\n    \n    def create_features(self, df):\n        \"\"\"Create essential features\"\"\"\n        df = df.copy()\n        \n        # Basic kinematics\n        dir_rad = np.deg2rad(df['dir'].fillna(0.0).astype('float32'))\n        df['vx'] = df['s'] * np.cos(dir_rad)\n        df['vy'] = df['s'] * np.sin(dir_rad)\n        df['ax'] = df['a'] * np.cos(dir_rad)\n        df['ay'] = df['a'] * np.sin(dir_rad)\n        \n        # Role indicators\n        df['is_offense'] = (df['player_side'] == 'Offense').astype(np.int8)\n        df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(np.int8)\n        df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(np.int8)\n        \n        # Ball distance\n        if {'ball_land_x', 'ball_land_y'}.issubset(df.columns):\n            dx = df['ball_land_x'] - df['x']\n            dy = df['ball_land_y'] - df['y']\n            df['ball_dist'] = np.hypot(dx, dy)\n            inv_dist = 1.0 / (df['ball_dist'] + 1e-6)\n            df['ball_dir_x'] = dx * inv_dist\n            df['ball_dir_y'] = dy * inv_dist\n        \n        # Rolling features\n        for col in ['vx', 'vy', 's', 'a']:\n            if col in df.columns:\n                for w in [3, 5]:\n                    rolling = df.groupby(self.grouping)[col].rolling(w, min_periods=1).mean()\n                    df[f'{col}_roll{w}'] = rolling.reset_index(level=[0,1,2], drop=True)\n        \n        # Lag features\n        for col in ['x', 'y', 'vx', 'vy']:\n            if col in df.columns:\n                for lag in [1, 2]:\n                    lagged = df.groupby(self.grouping)[col].shift(lag)\n                    df[f'{col}_lag{lag}'] = lagged.fillna(df.groupby(self.grouping)[col].transform('first'))\n        \n        return df\n\n# =============================================================================\n# SEQUENCE BUILDING\n# =============================================================================\n\ndef build_sequences(input_df, output_df=None, test_template=None, is_training=True, window_size=10):\n    \"\"\"Build sequences for training/inference\"\"\"\n    \n    # Unify directions\n    direction_map = create_play_direction_mapping(input_df)\n    unified_input = unify_field_direction(input_df)\n    \n    if is_training:\n        unified_output = apply_direction_unification(output_df, direction_map)\n        target_data = unified_output\n        target_groups = unified_output[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n    else:\n        if 'play_direction' not in test_template.columns:\n            direction_df = direction_map.reset_index()\n            test_template = test_template.merge(direction_df, on=['game_id', 'play_id'], how='left')\n        target_data = test_template\n        target_groups = target_data[['game_id', 'play_id', 'nfl_id', 'play_direction']].drop_duplicates()\n    \n    # Feature engineering\n    engineer = FeatureEngineer()\n    processed = engineer.create_features(unified_input)\n    \n    # Get feature columns - only numeric columns\n    exclude_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'play_direction']\n    \n    # Filter to only numeric columns\n    numeric_cols = []\n    for col in processed.columns:\n        if col in exclude_cols:\n            continue\n        \n        # Check if column is numeric type\n        dtype_name = str(processed[col].dtype)\n        is_numeric = (\n            'float' in dtype_name or \n            'int' in dtype_name or \n            'uint' in dtype_name or\n            dtype_name == 'bool' or\n            dtype_name == 'bool_' or\n            np.issubdtype(processed[col].dtype, np.number)\n        )\n        \n        if is_numeric:\n            numeric_cols.append(col)\n        else:\n            # Try to convert to numeric - if fails, skip (strings like player names)\n            try:\n                # Test if we can convert a sample to numeric\n                sample = processed[col].dropna().iloc[0] if len(processed[col].dropna()) > 0 else None\n                if sample is not None:\n                    pd.to_numeric([sample], errors='raise')\n                    # If conversion works, it's numeric\n                    numeric_cols.append(col)\n            except (ValueError, TypeError, IndexError):\n                # Skip non-numeric columns (like player names, team names, etc.)\n                pass\n    \n    feature_cols = sorted(numeric_cols)  # Sort for consistency\n    \n    # Log excluded columns for debugging\n    excluded_cols = [c for c in processed.columns if c not in exclude_cols and c not in feature_cols]\n    if excluded_cols:\n        print(f\"Excluded non-numeric columns: {excluded_cols[:10]}{'...' if len(excluded_cols) > 10 else ''}\")\n    print(f\"Using {len(feature_cols)} numeric feature columns\")\n    \n    # Build sequences\n    sequences = []\n    targets_x = [] if is_training else None\n    targets_y = [] if is_training else None\n    frame_ids_list = [] if is_training else None\n    metadata = []\n    \n    x_idx = feature_cols.index('x')\n    y_idx = feature_cols.index('y')\n    \n    # Cache grouped data - ensure all values are numeric\n    grouped = {}\n    for key, group in processed.groupby(['game_id', 'play_id', 'nfl_id']):\n        # Select only numeric feature columns and convert to numeric\n        group_data = group[feature_cols].copy()\n        # Convert any object columns to numeric\n        for col in feature_cols:\n            if group_data[col].dtype == 'object':\n                group_data[col] = pd.to_numeric(group_data[col], errors='coerce')\n        grouped[key] = group_data.values.astype(np.float32)\n    \n    if is_training:\n        target_lookup = {\n            (g, p, n): group[['x', 'y', 'frame_id']].values\n            for (g, p, n), group in unified_output.groupby(['game_id', 'play_id', 'nfl_id'])\n        }\n    \n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Building sequences\"):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        if key not in grouped:\n            continue\n        \n        data = grouped[key]\n        \n        # Create window\n        if len(data) >= window_size:\n            window = data[-window_size:]\n        else:\n            if is_training:\n                continue\n            padding = np.full((window_size - len(data), len(feature_cols)), np.nan)\n            window = np.vstack([padding, data])\n        \n        # Fill NaN and ensure all values are numeric\n        window = np.nan_to_num(window, nan=0.0)\n        \n        # Safeguard: ensure all values are numeric\n        try:\n            window = window.astype(np.float32)\n        except (ValueError, TypeError) as e:\n            # If conversion fails, try to convert each column individually\n            window_fixed = []\n            for i in range(window.shape[1]):\n                col_data = window[:, i]\n                try:\n                    # Try to convert to float\n                    col_float = pd.to_numeric(col_data, errors='coerce')\n                    window_fixed.append(col_float.fillna(0.0).values)\n                except:\n                    # If still fails, fill with zeros\n                    window_fixed.append(np.zeros(len(col_data), dtype=np.float32))\n            window = np.column_stack(window_fixed).astype(np.float32)\n        \n        sequences.append(window)\n        \n        if is_training:\n            if key not in target_lookup:\n                continue\n            \n            targets = target_lookup[key]\n            last_x = window[-1, x_idx]\n            last_y = window[-1, y_idx]\n            \n            dx = (targets[:, 0] - last_x).astype(np.float32)\n            dy = (targets[:, 1] - last_y).astype(np.float32)\n            fids = targets[:, 2].astype(np.int32)\n            \n            targets_x.append(dx)\n            targets_y.append(dy)\n            frame_ids_list.append(fids)\n        \n        metadata.append({\n            'game_id': int(row['game_id']),\n            'play_id': int(row['play_id']),\n            'nfl_id': int(row['nfl_id']),\n            'play_direction': row.get('play_direction'),\n        })\n    \n    if is_training:\n        return sequences, targets_x, targets_y, frame_ids_list, metadata, feature_cols, direction_map\n    return sequences, metadata, feature_cols, direction_map\n\n# =============================================================================\n# TRAINING FUNCTION\n# =============================================================================\n\ndef prepare_targets(target_list, max_horizon):\n    \"\"\"Prepare target tensors with padding\"\"\"\n    targets = []\n    masks = []\n    \n    for arr in target_list:\n        length = len(arr)\n        padded = np.pad(arr, (0, max_horizon - length), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_horizon, dtype=np.float32)\n        mask[:length] = 1.0\n        targets.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    \n    return torch.stack(targets), torch.stack(masks)\n\ndef train_model(config, train_seqs, train_tx, train_ty, val_seqs, val_tx, val_ty, input_dim):\n    \"\"\"Train trajectory prediction model with RMSE loss\"\"\"\n    \n    # Setup device\n    xm = None\n    if config.USE_TPU:\n        try:\n            import torch_xla.core.xla_model as xm\n            device = xm.xla_device()\n            print(f\"Using TPU device: {device}\")\n        except ImportError:\n            print(\"TPU requested but torch_xla not available, falling back to CPU/GPU\")\n            device = config.DEVICE\n            config.USE_TPU = False\n            xm = None\n    else:\n        device = config.DEVICE\n    \n    # Create model\n    model = TrajectoryPredictionModel(\n        input_dim, \n        hidden_dim=config.HIDDEN_DIM,\n        horizon=config.MAX_FUTURE_HORIZON\n    ).to(device)\n    \n    # Loss function - direct RMSE optimization\n    if config.USE_TEMPORAL_WEIGHTING:\n        criterion = TemporalRMSELoss(time_decay=config.TIME_DECAY)\n    else:\n        criterion = RMSELoss()\n    \n    # Optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(), \n        lr=config.LEARNING_RATE, \n        weight_decay=1e-4\n    )\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=config.EPOCHS, eta_min=1e-6\n    )\n    \n    scaler = GradScaler() if not config.USE_TPU else None\n    \n    # Create batches\n    def make_batches(seqs, tx, ty):\n        batches = []\n        for i in range(0, len(seqs), config.BATCH_SIZE):\n            end = min(i + config.BATCH_SIZE, len(seqs))\n            seq_batch = torch.tensor(np.stack(seqs[i:end])).to(device)\n            tx_batch, tx_mask = prepare_targets([tx[j] for j in range(i, end)], config.MAX_FUTURE_HORIZON)\n            ty_batch, ty_mask = prepare_targets([ty[j] for j in range(i, end)], config.MAX_FUTURE_HORIZON)\n            tx_batch, tx_mask = tx_batch.to(device), tx_mask.to(device)\n            ty_batch, ty_mask = ty_batch.to(device), ty_mask.to(device)\n            batches.append((seq_batch, tx_batch, ty_batch, tx_mask, ty_mask))\n        return batches\n    \n    train_batches = make_batches(train_seqs, train_tx, train_ty)\n    val_batches = make_batches(val_seqs, val_tx, val_ty)\n    \n    best_rmse = float('inf')\n    best_state = None\n    patience = 0\n    \n    for epoch in range(1, config.EPOCHS + 1):\n        # Training\n        model.train()\n        train_losses = []\n        \n        for seq_batch, tx_batch, ty_batch, tx_mask, ty_mask in train_batches:\n            optimizer.zero_grad()\n            \n            if config.USE_TPU and xm is not None:\n                pred_x, pred_y = model(seq_batch)\n                loss = criterion(pred_x, pred_y, tx_batch, ty_batch, tx_mask * ty_mask)\n                loss.backward()\n                # XLA requires explicit gradient marking and optimizer step\n                xm.optimizer_step(optimizer)  # Includes mark_step internally\n                train_losses.append(float(loss.cpu().item()))  # Move to CPU for item()\n            else:\n                with autocast():\n                    pred_x, pred_y = model(seq_batch)\n                    loss = criterion(pred_x, pred_y, tx_batch, ty_batch, tx_mask * ty_mask)\n                \n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            \n            train_losses.append(loss.item())\n        \n        # Validation\n        model.eval()\n        val_losses = []\n        \n        with torch.no_grad():\n            for seq_batch, tx_batch, ty_batch, tx_mask, ty_mask in val_batches:\n                if config.USE_TPU and xm is not None:\n                    pred_x, pred_y = model(seq_batch)\n                    loss = criterion(pred_x, pred_y, tx_batch, ty_batch, tx_mask * ty_mask)\n                    xm.mark_step()  # Mark computation boundary for XLA\n                    val_losses.append(float(loss.cpu().item()))  # Move to CPU for item()\n                else:\n                    with autocast():\n                        pred_x, pred_y = model(seq_batch)\n                        loss = criterion(pred_x, pred_y, tx_batch, ty_batch, tx_mask * ty_mask)\n                    val_losses.append(loss.item())\n        \n        train_rmse = np.mean(train_losses)\n        val_rmse = np.mean(val_losses)\n        \n        scheduler.step()\n        \n        if epoch % 5 == 0:\n            print(f\"Epoch {epoch}: Train RMSE={train_rmse:.5f}, Val RMSE={val_rmse:.5f}\")\n        \n        if val_rmse < best_rmse:\n            best_rmse = val_rmse\n            patience = 0\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            patience += 1\n            if patience >= config.PATIENCE:\n                print(f\"Early stopping at epoch {epoch}\")\n                break\n    \n    if best_state:\n        model.load_state_dict(best_state)\n    \n    return model, best_rmse\n\n# =============================================================================\n# MAIN TRAINING PIPELINE\n# =============================================================================\n\ndef main():\n    \"\"\"Main training pipeline\"\"\"\n    config = ModelConfig()\n    \n    print(\"=\"*80)\n    print(\"NFL BIG DATA BOWL 2026 - RMSE-OPTIMIZED TRAINING\")\n    print(\"=\"*80)\n    print(f\"Device: {config.DEVICE}\")\n    print(f\"TPU Mode: {config.USE_TPU}\")\n    print(f\"Ensemble Seeds: {config.SEEDS}\")\n    \n    # Load data\n    print(\"\\n[1/4] Loading data...\")\n    train_input_files = [GlobalConfig.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [GlobalConfig.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()], ignore_index=True)\n    \n    # Build sequences\n    print(\"\\n[2/4] Building sequences...\")\n    seqs, tx, ty, fids, meta, features, dir_map = build_sequences(\n        train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n    )\n    \n    print(f\"Created {len(seqs)} sequences with {len(features)} features\")\n    \n    # Training with ensemble\n    print(\"\\n[3/4] Training ensemble...\")\n    all_models_x, all_models_y = [], []\n    \n    group_ids = np.array([m['game_id'] for m in meta])\n    \n    for seed in config.SEEDS:\n        print(f\"\\n{'='*70}\\nSeed: {seed}\\n{'='*70}\")\n        set_random_seed(seed)\n        \n        kfold = GroupKFold(n_splits=config.N_FOLDS)\n        \n        for fold, (train_idx, val_idx) in enumerate(kfold.split(seqs, groups=group_ids), 1):\n            print(f\"\\nFold {fold}/{config.N_FOLDS}\")\n            \n            # Feature scaling\n            scaler = StandardScaler()\n            X_train = np.stack([seqs[i] for i in train_idx])\n            X_val = np.stack([seqs[i] for i in val_idx])\n            \n            scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n            \n            X_train_scaled = np.stack([scaler.transform(s) for s in X_train]).astype(np.float32)\n            X_val_scaled = np.stack([scaler.transform(s) for s in X_val]).astype(np.float32)\n            \n            # Train X model\n            print(\"Training X model...\")\n            model_x, rmse_x = train_model(\n                config,\n                X_train_scaled,\n                [tx[i] for i in train_idx],\n                [ty[i] for i in train_idx],\n                X_val_scaled,\n                [tx[i] for i in val_idx],\n                [ty[i] for i in val_idx],\n                X_train_scaled.shape[-1]\n            )\n            \n            # Train Y model\n            print(\"Training Y model...\")\n            model_y, rmse_y = train_model(\n                config,\n                X_train_scaled,\n                [tx[i] for i in train_idx],\n                [ty[i] for i in train_idx],\n                X_val_scaled,\n                [tx[i] for i in val_idx],\n                [ty[i] for i in val_idx],\n                X_train_scaled.shape[-1]\n            )\n            \n            all_models_x.append((model_x, scaler))\n            all_models_y.append((model_y, scaler))\n            \n            print(f\"Fold {fold} complete: X RMSE={rmse_x:.5f}, Y RMSE={rmse_y:.5f}\")\n    \n    print(f\"\\n[4/4] Training complete!\")\n    print(f\"Total models trained: {len(all_models_x)} X models, {len(all_models_y)} Y models\")\n    print(\"=\"*80)\n    \n    # Save models\n    save_dir = Path(\"/kaggle/working/models\")\n    save_dir.mkdir(exist_ok=True)\n    \n    for i, ((mx, sx), (my, sy)) in enumerate(zip(all_models_x, all_models_y)):\n        torch.save({\n            'model_x': mx.state_dict(),\n            'model_y': my.state_dict(),\n            'scaler': sx,\n            'features': features\n        }, save_dir / f\"ensemble_model_{i}.pt\")\n    \n    print(f\"Models saved to {save_dir}\")\n\n# =============================================================================\n# KAGGLE EVALUATION API INTEGRATION\n# =============================================================================\n\n# Polars is only needed for the API prediction function\n# Import it conditionally to avoid errors during training\ntry:\n    import polars as pl\n    POLARS_AVAILABLE = True\nexcept ImportError:\n    POLARS_AVAILABLE = False\n    # Create a dummy class for type hints\n    class pl:\n        @staticmethod\n        def from_pandas(df):\n            raise ImportError(\"polars is required for API predictions. Install with: pip install polars\")\n        class DataFrame:\n            pass\n\n# Global variables for trained models\n_trained_models_x = None\n_trained_models_y = None\n_feature_columns = None\n_direction_mapping = None\n_model_config = None\n\ndef load_ensemble_models(models_dir=\"/kaggle/working/models\"):\n    \"\"\"Load all ensemble models from disk\"\"\"\n    global _trained_models_x, _trained_models_y, _feature_columns, _model_config\n    \n    if _trained_models_x is not None:\n        return _trained_models_x, _trained_models_y, _feature_columns\n    \n    print(\"Loading ensemble models...\")\n    models_dir = Path(models_dir)\n    model_files = sorted(models_dir.glob(\"ensemble_model_*.pt\"))\n    \n    if not model_files:\n        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n    \n    _trained_models_x = []\n    _trained_models_y = []\n    \n    config = ModelConfig()\n    device = config.DEVICE\n    if config.USE_TPU:\n        try:\n            import torch_xla.core.xla_model as xm\n            device = xm.xla_device()\n        except ImportError:\n            pass\n    \n    for model_file in model_files:\n        checkpoint = torch.load(model_file, map_location=device, weights_only=False)\n        \n        # Create models\n        input_dim = len(checkpoint['features'])\n        model_x = TrajectoryPredictionModel(\n            input_dim,\n            hidden_dim=config.HIDDEN_DIM,\n            horizon=config.MAX_FUTURE_HORIZON\n        ).to(device)\n        model_y = TrajectoryPredictionModel(\n            input_dim,\n            hidden_dim=config.HIDDEN_DIM,\n            horizon=config.MAX_FUTURE_HORIZON\n        ).to(device)\n        \n        model_x.load_state_dict(checkpoint['model_x'])\n        model_y.load_state_dict(checkpoint['model_y'])\n        \n        model_x.eval()\n        model_y.eval()\n        \n        _trained_models_x.append((model_x, checkpoint['scaler']))\n        _trained_models_y.append((model_y, checkpoint['scaler']))\n        \n        if _feature_columns is None:\n            _feature_columns = checkpoint['features']\n    \n    print(f\"Loaded {len(_trained_models_x)} ensemble models\")\n    return _trained_models_x, _trained_models_y, _feature_columns\n\ndef predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Main prediction function for Kaggle evaluation API.\n    \n    This function must return predictions within 5 minutes for each batch.\n    The first call can take longer to load models (no 5-minute deadline).\n    \n    Args:\n        test: DataFrame with columns (game_id, play_id, nfl_id, frame_id)\n        test_input: DataFrame with tracking data before pass is thrown\n    \n    Returns:\n        DataFrame with columns (id, x, y) where id = \"{game_id}_{play_id}_{nfl_id}_{frame_id}\"\n    \"\"\"\n    # Ensure polars is available\n    if not POLARS_AVAILABLE:\n        raise ImportError(\n            \"polars is required for API predictions. \"\n            \"Please install it: pip install polars\"\n        )\n    \n    global _trained_models_x, _trained_models_y, _feature_columns, _direction_mapping, _model_config\n    \n    # Convert to pandas for processing\n    test_pd = test.to_pandas()\n    test_input_pd = test_input.to_pandas()\n    \n    # Initialize models on first call\n    if _trained_models_x is None:\n        print(\"[First call] Loading ensemble models and preparing for inference...\")\n        _trained_models_x, _trained_models_y, _feature_columns = load_ensemble_models()\n        _model_config = ModelConfig()\n        print(f\"Loaded {len(_trained_models_x)} ensemble models with {len(_feature_columns)} features\")\n    \n    print(f\"Making predictions for {len(test_pd)} targets...\")\n    \n    # Build test sequences using the same pipeline as training\n    test_template = test_pd[['game_id', 'play_id', 'nfl_id', 'frame_id']].copy()\n    \n    # Create direction mapping\n    _direction_mapping = create_play_direction_mapping(test_input_pd)\n    \n    # Build sequences (same as training pipeline)\n    test_sequences, test_metadata, test_features, _ = build_sequences(\n        test_input_pd,\n        test_template=test_template,\n        is_training=False,\n        window_size=_model_config.WINDOW_SIZE\n    )\n    \n    if len(test_sequences) == 0:\n        print(\"Warning: No sequences created, returning default predictions\")\n        # Return default predictions (center of field)\n        result_df = pd.DataFrame({\n            'id': test_pd.apply(lambda r: f\"{r['game_id']}_{r['play_id']}_{r['nfl_id']}_{r['frame_id']}\", axis=1),\n            'x': GlobalConfig.FIELD_LENGTH / 2,\n            'y': GlobalConfig.FIELD_WIDTH / 2\n        })\n        return pl.from_pandas(result_df)\n    \n    # Verify feature alignment\n    if test_features != _feature_columns:\n        print(f\"Warning: Feature mismatch! Expected {len(_feature_columns)}, got {len(test_features)}\")\n        # Try to align features\n        missing_features = set(_feature_columns) - set(test_features)\n        if missing_features:\n            print(f\"Missing features: {missing_features}\")\n    \n    # Get last positions from sequences\n    x_idx = test_features.index('x') if 'x' in test_features else None\n    y_idx = test_features.index('y') if 'y' in test_features else None\n    \n    if x_idx is None or y_idx is None:\n        print(\"Error: Could not find x or y in feature columns\")\n        result_df = pd.DataFrame({\n            'id': test_pd.apply(lambda r: f\"{r['game_id']}_{r['play_id']}_{r['nfl_id']}_{r['frame_id']}\", axis=1),\n            'x': GlobalConfig.FIELD_LENGTH / 2,\n            'y': GlobalConfig.FIELD_WIDTH / 2\n        })\n        return pl.from_pandas(result_df)\n    \n    test_x_last = np.array([seq[-1, x_idx] for seq in test_sequences], dtype=np.float32)\n    test_y_last = np.array([seq[-1, y_idx] for seq in test_sequences], dtype=np.float32)\n    \n    # Setup device\n    device = _model_config.DEVICE\n    if _model_config.USE_TPU:\n        try:\n            import torch_xla.core.xla_model as xm\n            device = xm.xla_device()\n        except ImportError:\n            pass\n    \n    # Ensemble predictions\n    all_pred_dx = []\n    all_pred_dy = []\n    \n    for (model_x, scaler), (model_y, _) in zip(_trained_models_x, _trained_models_y):\n        # Scale sequences using the same scaler\n        X_test_scaled = np.stack([scaler.transform(seq) for seq in test_sequences]).astype(np.float32)\n        X_test_tensor = torch.tensor(X_test_scaled).to(device)\n        \n        # Predict\n        model_x.eval()\n        model_y.eval()\n        with torch.no_grad():\n            pred_dx = model_x(X_test_tensor).cpu().numpy()  # Shape: [batch, horizon]\n            pred_dy = model_y(X_test_tensor).cpu().numpy()\n        \n        all_pred_dx.append(pred_dx)\n        all_pred_dy.append(pred_dy)\n    \n    # Average ensemble predictions\n    ensemble_dx = np.mean(all_pred_dx, axis=0)  # [batch, horizon]\n    ensemble_dy = np.mean(all_pred_dy, axis=0)\n    horizon = ensemble_dx.shape[1]\n    \n    # Build submission DataFrame\n    submission_rows = []\n    test_template_indexed = test_pd.set_index(['game_id', 'play_id', 'nfl_id']).sort_index()\n    \n    # Create mapping from (game_id, play_id, nfl_id) to sequence index\n    seq_index_map = {}\n    for idx, metadata in enumerate(test_metadata):\n        key = (int(metadata['game_id']), int(metadata['play_id']), int(metadata['nfl_id']))\n        if key not in seq_index_map:\n            seq_index_map[key] = []\n        seq_index_map[key].append((idx, metadata))\n    \n    # Process each test row\n    for _, test_row in test_pd.iterrows():\n        game_id = int(test_row['game_id'])\n        play_id = int(test_row['play_id'])\n        player_id = int(test_row['nfl_id'])\n        frame_id = int(test_row['frame_id'])\n        \n        key = (game_id, play_id, player_id)\n        \n        if key not in seq_index_map:\n            # No sequence for this player, use default position\n            x_default = GlobalConfig.FIELD_LENGTH / 2\n            y_default = GlobalConfig.FIELD_WIDTH / 2\n            submission_rows.append({\n                'id': f\"{game_id}_{play_id}_{player_id}_{frame_id}\",\n                'x': x_default,\n                'y': y_default\n            })\n            continue\n        \n        # Get the sequence metadata (use first match)\n        seq_idx, metadata = seq_index_map[key][0]\n        is_right_play = (metadata.get('play_direction') == 'right')\n        \n        # Determine time step (how many frames into the future)\n        # We need to find the frame_id in the target frames\n        try:\n            frame_ids = test_template_indexed.loc[key, 'frame_id']\n            if isinstance(frame_ids, pd.Series):\n                frame_ids = frame_ids.sort_values().tolist()\n            else:\n                frame_ids = [int(frame_ids)]\n            \n            # Find which time step this frame_id corresponds to\n            time_step = frame_ids.index(frame_id) if frame_id in frame_ids else 0\n            prediction_step = min(time_step, horizon - 1)\n        except (KeyError, ValueError, AttributeError):\n            prediction_step = 0\n        \n        # Get predicted deltas\n        dx = float(ensemble_dx[seq_idx, prediction_step])\n        dy = float(ensemble_dy[seq_idx, prediction_step])\n        \n        # Convert to absolute positions in unified coordinates\n        x_unified = float(np.clip(test_x_last[seq_idx] + dx, 0, GlobalConfig.FIELD_LENGTH))\n        y_unified = float(np.clip(test_y_last[seq_idx] + dy, 0, GlobalConfig.FIELD_WIDTH))\n        \n        # Revert to original field direction\n        if is_right_play:\n            x_original = float(GlobalConfig.FIELD_LENGTH - x_unified)\n            y_original = float(GlobalConfig.FIELD_WIDTH - y_unified)\n        else:\n            x_original = x_unified\n            y_original = y_unified\n        \n        submission_rows.append({\n            'id': f\"{game_id}_{play_id}_{player_id}_{frame_id}\",\n            'x': x_original,\n            'y': y_original\n        })\n    \n    result_df = pd.DataFrame(submission_rows)\n    \n    # Verify we have all predictions\n    if len(result_df) != len(test_pd):\n        print(f\"Warning: Prediction count mismatch. Expected {len(test_pd)}, got {len(result_df)}\")\n        # Fill missing predictions\n        test_ids = set(test_pd.apply(lambda r: f\"{r['game_id']}_{r['play_id']}_{r['nfl_id']}_{r['frame_id']}\", axis=1))\n        result_ids = set(result_df['id'])\n        missing_ids = test_ids - result_ids\n        for missing_id in missing_ids:\n            result_df = pd.concat([result_df, pd.DataFrame({\n                'id': [missing_id],\n                'x': [GlobalConfig.FIELD_LENGTH / 2],\n                'y': [GlobalConfig.FIELD_WIDTH / 2]\n            })], ignore_index=True)\n    \n    # Sort by id to match test order\n    result_df = result_df.sort_values('id').reset_index(drop=True)\n    \n    print(f\"Generated {len(result_df)} predictions\")\n    \n    # Convert back to polars\n    return pl.from_pandas(result_df)\n\n# =============================================================================\n# EXECUTION\n# =============================================================================\n\ndef check_models_exist(models_dir=\"/kaggle/working/models\"):\n    \"\"\"Check if trained models exist\"\"\"\n    models_dir = Path(models_dir)\n    model_files = list(models_dir.glob(\"ensemble_model_*.pt\"))\n    return len(model_files) > 0\n\nif __name__ == \"__main__\":\n    # Check if we're in competition mode (Kaggle evaluation API)\n    is_competition_mode = os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n    \n    # Check if models already exist\n    models_exist = check_models_exist()\n    \n    # Step 1: Train models if they don't exist\n    if not models_exist:\n        print(\"=\"*80)\n        print(\"STEP 1: TRAINING MODELS\")\n        print(\"=\"*80)\n        print(\"No trained models found. Starting training pipeline...\")\n        print(\"=\"*80)\n        \n        try:\n            main()\n            print(\"\\n\" + \"=\"*80)\n            print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n            print(\"=\"*80)\n            print(f\"Models saved to: /kaggle/working/models/\")\n            \n            # Verify models were created\n            if check_models_exist():\n                model_count = len(list(Path(\"/kaggle/working/models\").glob(\"ensemble_model_*.pt\")))\n                print(f\"✓ {model_count} model files created\")\n            else:\n                print(\"⚠ WARNING: Models may not have been saved correctly\")\n                \n        except Exception as e:\n            print(f\"\\n❌ ERROR during training: {e}\")\n            import traceback\n            traceback.print_exc()\n            raise\n    else:\n        model_count = len(list(Path(\"/kaggle/working/models\").glob(\"ensemble_model_*.pt\")))\n        print(\"=\"*80)\n        print(\"MODELS ALREADY TRAINED\")\n        print(\"=\"*80)\n        print(f\"Found {model_count} trained model files. Skipping training.\")\n        print(\"=\"*80)\n    \n    # Step 2: Start inference server if in competition mode\n    if is_competition_mode:\n        print(\"\\n\" + \"=\"*80)\n        print(\"STEP 2: STARTING INFERENCE SERVER\")\n        print(\"=\"*80)\n        print(\"Kaggle competition mode detected.\")\n        print(\"Starting inference server for API predictions...\")\n        print(\"=\"*80)\n        \n        try:\n            import kaggle_evaluation.nfl_inference_server\n            inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n            print(\"✓ Inference server initialized successfully\")\n            print(\"✓ Waiting for API prediction requests...\")\n            print(\"\\nServer is ready to handle predictions!\")\n            print(\"=\"*80)\n            inference_server.serve()\n        except ImportError:\n            print(\"\\n❌ ERROR: kaggle_evaluation package not found!\")\n            print(\"This script must be run in Kaggle competition environment.\")\n            print(\"Falling back to local mode...\")\n            \n            # Test prediction locally\n            print(\"\\nTesting prediction function locally...\")\n            try:\n                # Load test data if available\n                test_input_file = GlobalConfig.DATA_DIR / \"test_input.csv\"\n                test_file = GlobalConfig.DATA_DIR / \"test.csv\"\n                \n                if test_input_file.exists() and test_file.exists():\n                    if not POLARS_AVAILABLE:\n                        print(\"⚠ Polars not available. Skipping local prediction test.\")\n                    else:\n                        print(\"Loading test data for local prediction test...\")\n                        test_input_df = pd.read_csv(test_input_file)\n                        test_df = pd.read_csv(test_file)\n                        \n                        # Convert to polars\n                        test_input_pl = pl.from_pandas(test_input_df)\n                        test_pl = pl.from_pandas(test_df[['game_id', 'play_id', 'nfl_id', 'frame_id']])\n                        \n                        # Run prediction\n                        print(\"Running prediction...\")\n                        predictions = predict(test_pl, test_input_pl)\n                        print(f\"✓ Prediction successful! Generated {len(predictions)} predictions\")\n                        \n                        # Save locally\n                        predictions_pd = predictions.to_pandas()\n                        predictions_pd.to_csv(\"/kaggle/working/local_predictions.csv\", index=False)\n                        print(f\"✓ Saved predictions to: /kaggle/working/local_predictions.csv\")\n                else:\n                    print(\"⚠ Test data files not found. Skipping local prediction test.\")\n            except Exception as e:\n                print(f\"⚠ Local prediction test failed: {e}\")\n            raise\n        except Exception as e:\n            print(f\"\\n❌ ERROR starting inference server: {e}\")\n            import traceback\n            traceback.print_exc()\n            raise\n    else:\n        print(\"\\n\" + \"=\"*80)\n        print(\"LOCAL MODE - TRAINING COMPLETE\")\n        print(\"=\"*80)\n        print(\"To use prediction API, set environment variable:\")\n        print(\"  export KAGGLE_IS_COMPETITION_RERUN=true\")\n        print(\"=\"*80)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-10-31T11:04:00.556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/polars/polars-0.13.60-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n!KAGGLE_IS_COMPETITION_RERUN=true python /kaggle/working/nfl_big_data_bowl_2026_optimiezed_train.py","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-31T11:04:00.556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models already exist, just serve API\n!KAGGLE_IS_COMPETITION_RERUN=true python /kaggle/working/nfl_big_data_bowl_2026_optimiezed_train.py","metadata":{}}]}