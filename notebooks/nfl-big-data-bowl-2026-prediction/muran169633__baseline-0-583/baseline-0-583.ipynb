{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13530200,"sourceType":"datasetVersion","datasetId":8591302},{"sourceId":13576573,"sourceType":"datasetVersion","datasetId":8624935},{"sourceId":625294,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":470610,"modelId":486505}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport polars as pl  # å¯¼å…¥ Polars\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nimport os\nimport pickle # torch.load å†…éƒ¨éœ€è¦ pickle\n\n# --- æ¨ç†æœåŠ¡å™¨ Imports ---\ntry:\n    import kaggle_evaluation.nfl_inference_server\nexcept ImportError:\n    print(\"Warning: kaggle_evaluation not found. Server will not start in local test.\")\n    # åœ¨æœ¬åœ°æµ‹è¯•æ—¶ï¼Œå®šä¹‰ä¸€ä¸ªæ¨¡æ‹Ÿçš„ nfl_inference_server ä»¥é¿å… NameError\n    if 'kaggle_evaluation' not in globals():\n        class MockNFLInferenceServer:\n            def __init__(self, predict_fn):\n                print(\"[MockServer] Initialized.\")\n            def serve(self):\n                print(\"[MockServer] serve() called.\")\n            def run_local_gateway(self, data_path):\n                print(f\"[MockServer] run_local_gateway() called with {data_path}.\")\n        \n        # æ¨¡æ‹Ÿ kaggle_evaluation æ¨¡å—\n        class MockKaggleEvaluation:\n            class nfl_inference_server:\n                NFLInferenceServer = MockNFLInferenceServer\n        \n        kaggle_evaluation = MockKaggleEvaluation()\n\n\nfrom sklearn.preprocessing import StandardScaler\n# å¯¼å…¥ StratifiedKFold å’Œ GroupKFold\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold \nfrom sklearn.cluster import KMeans\nfrom multiprocessing import Pool as MultiprocessingPool, cpu_count\n\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIG\n# ============================================================================\n\nclass Config:\n    # !!! åˆ‡æ¢è¿™é‡Œ: True ç”¨äºè®­ç»ƒ, False ç”¨äºæ¨ç†æäº¤ !!!\n    IS_TRAINING = False\n    \n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n    OUTPUT_DIR = Path(\"/kaggle/input/ubantu11/outputs\") # æ¨¡å‹å’Œç¼©æ”¾å™¨å°†ä¿å­˜åˆ°è¿™é‡Œ\n    OUTPUT_DIR.mkdir(exist_ok=True)\n    \n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 256\n    EPOCHS = 200\n    PATIENCE = 30\n    LEARNING_RATE = 1e-3\n    \n    WINDOW_SIZE = 10\n    HIDDEN_DIM = 128\n    MAX_FUTURE_HORIZON = 94\n    \n    # === Transformer è¶…å‚æ•° ===\n    N_HEADS = 4  # Transformer çš„æ³¨æ„åŠ›å¤´æ•°\n    N_LAYERS = 2 # Transformer ç¼–ç å™¨çš„å±‚æ•°\n    \n    # === æ–°å¢ï¼šResidualMLP Head è¶…å‚æ•° ===\n    MLP_HIDDEN_DIM = 256 # MLP å¤´çš„å†…éƒ¨éšè—ç»´åº¦\n    N_RES_BLOCKS = 2     # æ®‹å·®å—çš„æ•°é‡\n    # ==================================\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    \n    K_NEIGH = 6\n    RADIUS = 30.0\n    TAU = 8.0\n    N_ROUTE_CLUSTERS = 7\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(Config.SEED)\n\n# ============================================================================\n# GEOMETRIC BASELINE - THE BREAKTHROUGH\n# (æ­¤éƒ¨åˆ†åŠä»¥ä¸‹ç‰¹å¾å·¥ç¨‹å‡½æ•°æœªæ›´æ”¹... ä¸ºç®€æ´èµ·è§å¯æŠ˜å )\n# ============================================================================\n\ndef compute_geometric_endpoint(df):\n    \"\"\"\n    Compute where each player SHOULD end up based on geometry.\n    This is the deterministic part - no learning needed.\n    \"\"\"\n    df = df.copy()\n    \n    # Time to play end\n    if 'num_frames_output' in df.columns:\n        t_total = df['num_frames_output'] / 10.0\n    else:\n        t_total = 3.0\n    \n    df['time_to_endpoint'] = t_total\n    \n    # Initialize with momentum (default rule)\n    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n    \n    # Rule 1: Targeted Receivers converge to ball\n    if 'ball_land_x' in df.columns:\n        receiver_mask = df['player_role'] == 'Targeted Receiver'\n        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n        \n        # Rule 2: Defenders mirror receivers (maintain offset)\n        defender_mask = df['player_role'] == 'Defensive Coverage'\n        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n        coverage_mask = defender_mask & has_mirror\n        \n        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n            df.loc[coverage_mask, 'ball_land_x'] + \n            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n        )\n        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n            df.loc[coverage_mask, 'ball_land_y'] + \n            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n        )\n    \n    # Clip to field\n    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    return df\n\ndef add_geometric_features(df):\n    \"\"\"Add features that describe the geometric solution\"\"\"\n    df = compute_geometric_endpoint(df)\n    \n    # Vector to geometric endpoint\n    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n    \n    # Required velocity to reach geometric endpoint\n    t = df['time_to_endpoint'] + 0.1\n    df['geo_required_vx'] = df['geo_vector_x'] / t\n    df['geo_required_vy'] = df['geo_vector_y'] / t\n    \n    # Current velocity vs required\n    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n    df['geo_velocity_error'] = np.sqrt(\n        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n    )\n    \n    # Required constant acceleration (a = 2*Î”x/tÂ²)\n    t_sq = t * t\n    df['geo_required_ax'] = 2 * df['geo_vector_x'] / t_sq\n    df['geo_required_ay'] = 2 * df['geo_vector_y'] / t_sq\n    df['geo_required_ax'] = df['geo_required_ax'].clip(-10, 10)\n    df['geo_required_ay'] = df['geo_required_ay'].clip(-10, 10)\n    \n    # Alignment with geometric path\n    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n    df['geo_alignment'] = (\n        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n    ) / (velocity_mag + 0.1)\n    \n    # Role-specific geometric quality\n    df['geo_receiver_urgency'] = df['is_receiver'] * df['geo_distance'] / (t + 0.1)\n    df['geo_defender_coupling'] = df['is_coverage'] * (1.0 / (df.get('mirror_wr_dist', 50) + 1.0))\n    \n    return df\n\n# ============================================================================\n# PROVEN FEATURE ENGINEERING (YOUR 0.59 BASE)\n# (æ­¤éƒ¨åˆ†æœªæ›´æ”¹)\n# ============================================================================\n\ndef get_velocity(speed, direction_deg):\n    theta = np.deg2rad(direction_deg)\n    return speed * np.sin(theta), speed * np.cos(theta)\n\ndef height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n\ndef get_opponent_features(input_df):\n    \"\"\"Enhanced opponent interaction with MIRROR WR tracking\"\"\"\n    features = []\n    # (åœ¨æ¨ç†æ—¶å…³é—­ tqdm)\n    disable_tqdm = not Config.IS_TRAINING\n    \n    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']), \n                                  desc=\"ğŸˆ Opponents\", leave=False, disable=disable_tqdm):\n        last = group.sort_values('frame_id').groupby('nfl_id').last()\n        \n        if len(last) < 2:\n            continue\n            \n        positions = last[['x', 'y']].values\n        sides = last['player_side'].values\n        speeds = last['s'].values\n        directions = last['dir'].values\n        roles = last['player_role'].values\n        \n        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n        \n        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n            opp_mask = sides != side\n            \n            feat = {\n                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n                'mirror_wr_dist': 50.0,\n            }\n            \n            if not opp_mask.any():\n                features.append(feat)\n                continue\n            \n            opp_positions = positions[opp_mask]\n            distances = np.sqrt(((positions[i] - opp_positions)**2).sum(axis=1))\n            \n            if len(distances) == 0:\n                features.append(feat)\n                continue\n                \n            nearest_idx = distances.argmin()\n            feat['nearest_opp_dist'] = distances[nearest_idx]\n            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n            \n            my_vx, my_vy = get_velocity(speeds[i], directions[i])\n            opp_speeds = speeds[opp_mask]\n            opp_dirs = directions[opp_mask]\n            opp_vx, opp_vy = get_velocity(opp_speeds[nearest_idx], opp_dirs[nearest_idx])\n            \n            rel_vx = my_vx - opp_vx\n            rel_vy = my_vy - opp_vy\n            to_me = positions[i] - opp_positions[nearest_idx]\n            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n            feat['closing_speed'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n            \n            if role == 'Defensive Coverage' and receiver_mask.any():\n                rec_positions = positions[receiver_mask]\n                rec_distances = np.sqrt(((positions[i] - rec_positions)**2).sum(axis=1))\n                \n                if len(rec_distances) > 0:\n                    closest_rec_idx = rec_distances.argmin()\n                    rec_indices = np.where(receiver_mask)[0]\n                    actual_rec_idx = rec_indices[closest_rec_idx]\n                    \n                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n                    \n                    feat['mirror_wr_vx'] = rec_vx\n                    feat['mirror_wr_vy'] = rec_vy\n                    feat['mirror_wr_dist'] = rec_distances[closest_rec_idx]\n                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n            \n            features.append(feat)\n    \n    return pd.DataFrame(features)\n\ndef extract_route_patterns(input_df, kmeans=None, scaler=None, fit=True):\n    \"\"\"Route clustering\"\"\"\n    route_features = []\n    # (åœ¨æ¨ç†æ—¶å…³é—­ tqdm)\n    disable_tqdm = not Config.IS_TRAINING\n    \n    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']), \n                                      desc=\"ğŸ›£ï¸  Routes\", leave=False, disable=disable_tqdm):\n        traj = group.sort_values('frame_id').tail(5)\n        \n        if len(traj) < 3:\n            continue\n        \n        positions = traj[['x', 'y']].values\n        speeds = traj['s'].values\n        \n        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 + \n                               (positions[-1, 1] - positions[0, 1])**2)\n        straightness = displacement / (total_dist + 0.1)\n        \n        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n        if len(angles) > 1:\n            angle_changes = np.abs(np.diff(angles))\n            max_turn = np.max(angle_changes)\n            mean_turn = np.mean(angle_changes)\n        else:\n            max_turn = mean_turn = 0\n        \n        speed_mean = speeds.mean()\n        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n        dx = positions[-1, 0] - positions[0, 0]\n        dy = positions[-1, 1] - positions[0, 1]\n        \n        route_features.append({\n            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n            'traj_straightness': straightness,\n            'traj_max_turn': max_turn,\n            'traj_mean_turn': mean_turn,\n            'traj_depth': abs(dx),\n            'traj_width': abs(dy),\n            'speed_mean': speed_mean,\n            'speed_change': speed_change,\n        })\n    \n    route_df = pd.DataFrame(route_features)\n    if 'traj_straightness' not in route_df.columns:\n        # å¦‚æœæ²¡æœ‰åˆ›å»ºç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œæ‰€æœ‰åºåˆ—éƒ½å¤ªçŸ­ï¼‰\n        if fit:\n            return pd.DataFrame(), KMeans(n_clusters=Config.N_ROUTE_CLUSTERS), StandardScaler()\n        else:\n            return pd.DataFrame()\n            \n    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n    X = route_df[feat_cols].fillna(0)\n    \n    if fit:\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n        kmeans = KMeans(n_clusters=Config.N_ROUTE_CLUSTERS, random_state=Config.SEED, n_init=10)\n        route_df['route_pattern'] = kmeans.fit_predict(X_scaled)\n        return route_df, kmeans, scaler\n    else:\n        # åœ¨æ¨ç†æ—¶ï¼Œç¡®ä¿ kmeans å’Œ scaler å·²ç»è¢«åŠ è½½\n        if kmeans is None or scaler is None:\n            raise ValueError(\"KMeans and Scaler must be provided during inference (fit=False)\")\n        X_scaled = scaler.transform(X)\n        route_df['route_pattern'] = kmeans.predict(X_scaled)\n        return route_df\n\ndef compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH, \n                                radius=Config.RADIUS, tau=Config.TAU):\n    \"\"\"GNN-lite embeddings\"\"\"\n    # (åœ¨æ¨ç†æ—¶å…³é—­æ‰“å°)\n    if Config.IS_TRAINING:\n        print(\"ğŸ•¸ï¸  GNN embeddings...\")\n    \n    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\", \n                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n    src = input_df[cols_needed].copy()\n    \n    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n               .tail(1)\n               .rename(columns={\"frame_id\": \"last_frame_id\"})\n               .reset_index(drop=True))\n    \n    tmp = last.merge(\n        src.rename(columns={\n            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n            \"x\": \"x_nb\", \"y\": \"y_nb\", \n            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\", \n            \"player_side\": \"player_side_nb\"\n        }),\n        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n        how=\"left\"\n    )\n    \n    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n    \n    tmp = tmp[np.isfinite(tmp[\"dist\"]) & (tmp[\"dist\"] > 1e-6)]\n    if radius is not None:\n        tmp = tmp[tmp[\"dist\"] <= radius]\n    \n    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"] == tmp[\"player_side\"]).astype(np.float32)\n    \n    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n    if k_neigh is not None:\n        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n    \n    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n    \n    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n    \n    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n    \n    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n    \n    ag = tmp.groupby(keys).agg(\n        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n        gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n    ).reset_index()\n    \n    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n    if len(near) > 0:\n        near[\"rnk\"] = near[\"rnk\"].astype(int)\n        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n        dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n        ag = ag.merge(dwide, on=keys, how=\"left\")\n    \n    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n              \"gnn_opp_dx_mean\", \"gnn_opp_dy_mean\", \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n        ag[c] = ag[c].fillna(0.0)\n    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\", \n              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n    \n    return ag\n\n# ============================================================================\n# SEQUENCE PREPARATION WITH GEOMETRIC FEATURES\n# (æ­¤éƒ¨åˆ†æœªæ›´æ”¹)\n# ============================================================================\n\ndef prepare_sequences_geometric(input_df, output_df=None, test_template=None, \n                                is_training=True, window_size=10,\n                                route_kmeans=None, route_scaler=None):\n    \"\"\"YOUR 154 features + 13 geometric features = 167 total\"\"\"\n    \n    # åœ¨æ¨ç†æ—¶ï¼Œå…³é—­ tqdm è¿›åº¦æ¡ï¼Œå› ä¸ºå®ƒåœ¨æœåŠ¡å™¨æ—¥å¿—ä¸­å¾ˆåµ\n    disable_tqdm = not is_training\n    \n    if is_training:\n        print(f\"\\n{'='*80}\")\n        print(f\"PREPARING GEOMETRIC SEQUENCES (is_training={is_training})\")\n        print(f\"{'='*80}\")\n    \n    input_df = input_df.copy()\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    if not disable_tqdm: print(\"Step 1: Base features...\")\n    \n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n    height_parts = input_df['player_height'].str.split('-', expand=True)\n    input_df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    input_df['bmi'] = (input_df['player_weight'] / (input_df['height_inches']**2)) * 703\n    \n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    input_df['velocity_x'] = input_df['s'] * np.sin(dir_rad)\n    input_df['velocity_y'] = input_df['s'] * np.cos(dir_rad)\n    input_df['acceleration_x'] = input_df['a'] * np.cos(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.sin(dir_rad)\n    \n    input_df['speed_squared'] = input_df['s'] ** 2\n    input_df['accel_magnitude'] = np.sqrt(input_df['acceleration_x']**2 + input_df['acceleration_y']**2)\n    input_df['momentum_x'] = input_df['velocity_x'] * input_df['player_weight']\n    input_df['momentum_y'] = input_df['velocity_y'] * input_df['player_weight']\n    input_df['kinetic_energy'] = 0.5 * input_df['player_weight'] * input_df['speed_squared']\n    \n    input_df['orientation_diff'] = np.abs(input_df['o'] - input_df['dir'])\n    input_df['orientation_diff'] = np.minimum(input_df['orientation_diff'], 360 - input_df['orientation_diff'])\n    \n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    input_df['role_targeted_receiver'] = input_df['is_receiver']\n    input_df['role_defensive_coverage'] = input_df['is_coverage']\n    input_df['role_passer'] = input_df['is_passer']\n    input_df['side_offense'] = input_df['is_offense']\n    \n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['dist_to_ball'] = input_df['distance_to_ball']\n        input_df['dist_squared'] = input_df['distance_to_ball'] ** 2\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed_ball'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n        input_df['velocity_toward_ball'] = (\n            input_df['velocity_x'] * np.cos(input_df['angle_to_ball']) + \n            input_df['velocity_y'] * np.sin(input_df['angle_to_ball'])\n        )\n        input_df['velocity_alignment'] = np.cos(input_df['angle_to_ball'] - dir_rad)\n        input_df['angle_diff'] = np.abs(input_df['o'] - np.degrees(input_df['angle_to_ball']))\n        input_df['angle_diff'] = np.minimum(input_df['angle_diff'], 360 - input_df['angle_diff'])\n    \n    if not disable_tqdm: print(\"Step 2: Advanced features...\")\n    \n    opp_features = get_opponent_features(input_df) # TQDM åœ¨å‡½æ•°å†…éƒ¨\n    input_df = input_df.merge(opp_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    if is_training:\n        route_features, route_kmeans, route_scaler = extract_route_patterns(input_df, fit=True)\n    else:\n        # åœ¨æ¨ç†æ—¶ï¼Œroute_kmeans å’Œ route_scaler å¿…é¡»è¢«ä¼ å…¥\n        route_features = extract_route_patterns(input_df, route_kmeans, route_scaler, fit=False)\n    \n    if not route_features.empty:\n        input_df = input_df.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    gnn_features = compute_neighbor_embeddings(input_df) # æ‰“å°åœ¨å‡½æ•°å†…éƒ¨\n    input_df = input_df.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    if 'nearest_opp_dist' in input_df.columns:\n        input_df['pressure'] = 1 / np.maximum(input_df['nearest_opp_dist'], 0.5)\n        input_df['under_pressure'] = (input_df['nearest_opp_dist'] < 3).astype(int)\n        input_df['pressure_x_speed'] = input_df['pressure'] * input_df['s']\n    \n    if 'mirror_wr_vx' in input_df.columns:\n        s_safe = np.maximum(input_df['s'], 0.1)\n        input_df['mirror_similarity'] = (\n            input_df['velocity_x'] * input_df['mirror_wr_vx'] + \n            input_df['velocity_y'] * input_df['mirror_wr_vy']\n        ) / s_safe\n        input_df['mirror_offset_dist'] = np.sqrt(\n            input_df['mirror_offset_x']**2 + input_df['mirror_offset_y']**2\n        )\n        input_df['mirror_alignment'] = input_df['mirror_similarity'] * input_df['role_defensive_coverage']\n    \n    if not disable_tqdm: print(\"Step 3: Temporal features...\")\n    \n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in input_df.columns:\n                input_df[f'{col}_lag{lag}'] = input_df.groupby(gcols)[col].shift(lag)\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in input_df.columns:\n                input_df[f'{col}_rolling_mean_{window}'] = (\n                    input_df.groupby(gcols)[col]\n                            .rolling(window, min_periods=1).mean()\n                            .reset_index(level=[0,1,2], drop=True)\n                )\n                input_df[f'{col}_rolling_std_{window}'] = (\n                    input_df.groupby(gcols)[col]\n                            .rolling(window, min_periods=1).std()\n                            .reset_index(level=[0,1,2], drop=True)\n                )\n    \n    for col in ['velocity_x', 'velocity_y']:\n        if col in input_df.columns:\n            input_df[f'{col}_delta'] = input_df.groupby(gcols)[col].diff()\n    \n    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    \n    if not disable_tqdm: print(\"Step 4: Time features...\")\n    \n    if 'num_frames_output' in input_df.columns:\n        max_frames = input_df['num_frames_output']\n        \n        input_df['max_play_duration'] = max_frames / 10.0\n        input_df['frame_time'] = input_df['frame_id'] / 10.0\n        input_df['progress_ratio'] = input_df['frame_id'] / np.maximum(max_frames, 1)\n        input_df['time_remaining'] = (max_frames - input_df['frame_id']) / 10.0\n        input_df['frames_remaining'] = max_frames - input_df['frame_id']\n        \n        input_df['expected_x_at_ball'] = input_df['x'] + input_df['velocity_x'] * input_df['frame_time']\n        input_df['expected_y_at_ball'] = input_df['y'] + input_df['velocity_y'] * input_df['frame_time']\n        \n        if 'ball_land_x' in input_df.columns:\n            input_df['error_from_ball_x'] = input_df['expected_x_at_ball'] - input_df['ball_land_x']\n            input_df['error_from_ball_y'] = input_df['expected_y_at_ball'] - input_df['ball_land_y']\n            input_df['error_from_ball'] = np.sqrt(\n                input_df['error_from_ball_x']**2 + input_df['error_from_ball_y']**2\n            )\n            \n            input_df['weighted_dist_by_time'] = input_df['dist_to_ball'] / (input_df['frame_time'] + 0.1)\n            input_df['dist_scaled_by_progress'] = input_df['dist_to_ball'] * (1 - input_df['progress_ratio'])\n        \n        input_df['time_squared'] = input_df['frame_time'] ** 2\n        input_df['velocity_x_progress'] = input_df['velocity_x'] * input_df['progress_ratio']\n        input_df['velocity_y_progress'] = input_df['velocity_y'] * input_df['progress_ratio']\n        input_df['speed_scaled_by_time_left'] = input_df['s'] * input_df['time_remaining']\n        \n        input_df['actual_play_length'] = max_frames\n        input_df['length_ratio'] = max_frames / 30.0\n    \n    # ğŸ¯ THE BREAKTHROUGH: Add geometric features\n    if not disable_tqdm: print(\"Step 5: ğŸ¯ Geometric endpoint features...\")\n    input_df = add_geometric_features(input_df)\n    \n    if not disable_tqdm: print(\"Step 6: Building feature list...\")\n    \n    # ä½ çš„ 154 ä¸ªæˆç†Ÿç‰¹å¾\n    feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n        'player_height_feet', 'player_weight', 'height_inches', 'bmi',\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'speed_squared', 'accel_magnitude', 'orientation_diff',\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer', 'side_offense',\n        'distance_to_ball', 'dist_to_ball', 'dist_squared', 'angle_to_ball', \n        'ball_direction_x', 'ball_direction_y', 'closing_speed_ball',\n        'velocity_toward_ball', 'velocity_alignment', 'angle_diff',\n        'nearest_opp_dist', 'closing_speed', 'num_nearby_opp_3', 'num_nearby_opp_5',\n        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y',\n        'pressure', 'under_pressure', 'pressure_x_speed', \n        'mirror_similarity', 'mirror_offset_dist', 'mirror_alignment',\n        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n        'gnn_opp_dx_mean', 'gnn_opp_dy_mean', 'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n        'gnn_ally_cnt', 'gnn_opp_cnt',\n        'gnn_ally_dmin', 'gnn_ally_dmean', 'gnn_opp_dmin', 'gnn_opp_dmean',\n        'gnn_d1', 'gnn_d2', 'gnn_d3',\n    ]\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            feature_cols.append(f'{col}_lag{lag}')\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            feature_cols.append(f'{col}_rolling_mean_{window}')\n            feature_cols.append(f'{col}_rolling_std_{window}')\n    \n    feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n    feature_cols.extend(['velocity_x_ema', 'velocity_y_ema', 'speed_ema'])\n    \n    feature_cols.extend([\n        'max_play_duration', 'frame_time', 'progress_ratio', 'time_remaining', 'frames_remaining',\n        'expected_x_at_ball', 'expected_y_at_ball', \n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n        'time_squared', 'weighted_dist_by_time', \n        'velocity_x_progress', 'velocity_y_progress', 'dist_scaled_by_progress',\n        'speed_scaled_by_time_left', 'actual_play_length', 'length_ratio',\n    ])\n    \n    # ğŸ¯ Add 13 geometric features\n    feature_cols.extend([\n        'geo_endpoint_x', 'geo_endpoint_y',\n        'geo_vector_x', 'geo_vector_y', 'geo_distance',\n        'geo_required_vx', 'geo_required_vy',\n        'geo_velocity_error_x', 'geo_velocity_error_y', 'geo_velocity_error',\n        'geo_required_ax', 'geo_required_ay',\n        'geo_alignment',\n    ])\n    \n    # ç¡®ä¿ç‰¹å¾å­˜åœ¨\n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    \n    if not disable_tqdm: print(f\"âœ“ Using {len(feature_cols)} features\")\n    \n    if not disable_tqdm: print(\"Step 7: Creating sequences...\")\n    \n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    \n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n    \n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n    geo_endpoints_x, geo_endpoints_y = [], []\n    \n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\", disable=disable_tqdm):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        \n        input_window = group_df.tail(window_size)\n        \n        if len(input_window) < window_size:\n            # ä»…åœ¨è®­ç»ƒæ—¶è·³è¿‡ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¡«å……ä»¥è¿›è¡Œé¢„æµ‹ã€‚\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        \n        # å¡«å……å‡å€¼\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n        \n        if np.isnan(seq).any():\n            # å†æ¬¡æ£€æŸ¥ï¼Œå¦‚æœä»ç„¶æœ‰ NaNï¼ˆä¾‹å¦‚ï¼Œå¦‚æœ group_df ä¸ºç©ºæˆ–å…¨ä¸º NaNï¼‰\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0) # åœ¨æ¨ç†æ—¶å¡«å…… 0\n        \n        sequences.append(seq)\n        \n        # Store geometric endpoint for this player\n        geo_x = input_window.iloc[-1]['geo_endpoint_x']\n        geo_y = input_window.iloc[-1]['geo_endpoint_y']\n        geo_endpoints_x.append(geo_x)\n        geo_endpoints_y.append(geo_y)\n        \n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            \n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n            \n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            \n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n        \n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id']\n        })\n    \n    if not disable_tqdm: print(f\"âœ“ Created {len(sequences)} sequences\")\n    \n    if is_training:\n        return (sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, \n                geo_endpoints_x, geo_endpoints_y, route_kmeans, route_scaler, feature_cols)\n    \n    # åœ¨æ¨ç†æ¨¡å¼ä¸‹\n    return sequences, sequence_ids, geo_endpoints_x, geo_endpoints_y, feature_cols\n\n# ============================================================================\n# MODEL ARCHITECTURE (ST-TRANSFORMER with ResidualMLP Head)\n# ============================================================================\n\nclass ResidualBlock(nn.Module):\n    \"\"\"\n    ä¸€ä¸ªæ ‡å‡†çš„æ®‹å·®å—ï¼šFFN + å¿«æ·è¿æ¥\n    \"\"\"\n    def __init__(self, dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n        self.norm = nn.LayerNorm(dim)\n    \n    def forward(self, x):\n        # Pre-normalization\n        return x + self.ffn(self.norm(x))\n\nclass ResidualMLPHead(nn.Module):\n    \"\"\"\n    æ›¿æ¢åŸæœ‰çš„ nn.Sequential Head\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, n_res_blocks=2, dropout=0.2):\n        super().__init__()\n        # 1. ä» context_dim (128) æŠ•å½±åˆ° mlp_hidden_dim (256)\n        self.input_layer = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.GELU()\n        )\n        \n        # 2. ä¸€ç³»åˆ—çš„æ®‹å·®å— (åœ¨ 256 ç»´åº¦ä¸Šæ“ä½œ)\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(hidden_dim, hidden_dim * 2, dropout) for _ in range(n_res_blocks)]\n        )\n        \n        # 3. æœ€åçš„ LayerNorm å’Œè¾“å‡ºæŠ•å½±\n        self.output_norm = nn.LayerNorm(hidden_dim)\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.residual_blocks(x)\n        x = self.output_norm(x)\n        x = self.output_layer(x)\n        return x\n\nclass STTransformer(nn.Module):\n    \"\"\"\n    Spatio-Temporal Transformer\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, horizon, window_size, n_heads, n_layers, dropout=0.1):\n        super().__init__()\n        config = Config() # è·å– MLP çš„è¶…å‚æ•°\n        self.horizon = horizon\n        self.hidden_dim = hidden_dim\n\n        # 1. Spatio: ç‰¹å¾åµŒå…¥\n        self.input_projection = nn.Linear(input_dim, hidden_dim)\n        \n        # 2. Temporal: å¯å­¦ä¹ çš„ä½ç½®ç¼–ç \n        self.pos_embed = nn.Parameter(torch.randn(1, window_size, hidden_dim)) \n        self.embed_dropout = nn.Dropout(dropout)\n\n        # 3. Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=n_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            batch_first=True,\n            activation='gelu'\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=n_layers\n        )\n\n        # 4. æ± åŒ– (å¤ç”¨ä½ æˆç†Ÿçš„ Attention Pooling æœºåˆ¶)\n        self.pool_ln = nn.LayerNorm(hidden_dim)\n        self.pool_attn = nn.MultiheadAttention(hidden_dim, num_heads=n_heads, batch_first=True)\n        self.pool_query = nn.Parameter(torch.randn(1, 1, hidden_dim)) \n\n        # 5. è¾“å‡º Head (!!! å·²æ›¿æ¢ä¸º ResidualMLPHead !!!)\n        self.head = ResidualMLPHead(\n            input_dim=hidden_dim,                   # 128\n            hidden_dim=config.MLP_HIDDEN_DIM,       # 256\n            output_dim=horizon * 2,                 # 188\n            n_res_blocks=config.N_RES_BLOCKS,       # 2\n            dropout=0.2\n        )\n\n    def forward(self, x):\n        B, S, _ = x.shape\n        x_embed = self.input_projection(x) \n        x = x_embed + self.pos_embed[:, :S, :] \n        x = self.embed_dropout(x)\n        \n        h = self.transformer_encoder(x) \n\n        q = self.pool_query.expand(B, -1, -1)\n        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n        ctx = ctx.squeeze(1) \n\n        out = self.head(ctx) # <--- ä½¿ç”¨æ–°çš„ Head\n        out = out.view(B, self.horizon, 2)\n        \n        out = torch.cumsum(out, dim=1)\n        \n        return out\n\n# ============================================================================\n# LOSS (YOUR PROVEN TEMPORAL HUBER)\n# (æ­¤éƒ¨åˆ†æœªæ›´æ”¹)\n# ============================================================================\n\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n    \n    def forward(self, pred, target, mask):\n        err = pred - target\n        abs_err = torch.abs(err)\n        huber = torch.where(abs_err <= self.delta, 0.5 * err * err, \n                            self.delta * (abs_err - 0.5 * self.delta))\n        \n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            weight = torch.exp(-self.time_decay * t).view(1, L, 1)\n            huber = huber * weight\n            mask = mask.unsqueeze(-1) * weight\n        \n        return (huber * mask).sum() / (mask.sum() + 1e-8)\n\n# ============================================================================\n# TRAINING\n# (æ­¤éƒ¨åˆ†æœªæ›´æ”¹)\n# ============================================================================\n\ndef prepare_targets(batch_dx, batch_dy, max_h):\n    tensors_x, tensors_y, masks = [], [], []\n    \n    for dx, dy in zip(batch_dx, batch_dy):\n        L = len(dx)\n        padded_x = np.pad(dx, (0, max_h - L), constant_values=0).astype(np.float32)\n        padded_y = np.pad(dy, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        \n        tensors_x.append(torch.tensor(padded_x))\n        tensors_y.append(torch.tensor(padded_y))\n        masks.append(torch.tensor(mask))\n    \n    targets = torch.stack([torch.stack(tensors_x), torch.stack(tensors_y)], dim=-1)\n    return targets, torch.stack(masks)\n\ndef train_model(X_train, y_train_dx, y_train_dy, X_val, y_val_dx, y_val_dy, \n                input_dim, horizon, config):\n    device = config.DEVICE\n    \n    # (æ¨¡å‹å®ä¾‹åŒ–ä¼šè‡ªåŠ¨ä½¿ç”¨æ–°çš„ ResidualMLPHead)\n    model = STTransformer(\n        input_dim=input_dim,\n        hidden_dim=config.HIDDEN_DIM,\n        horizon=horizon,\n        window_size=config.WINDOW_SIZE,\n        n_heads=config.N_HEADS,\n        n_layers=config.N_LAYERS\n    ).to(device)\n    \n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n    \n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        by, bm = prepare_targets(\n            [y_train_dx[j] for j in range(i, end)],\n            [y_train_dy[j] for j in range(i, end)],\n            horizon\n        )\n        train_batches.append((bx, by, bm))\n    \n    val_batches = []\n    for i in range(0, len(X_val), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_val))\n        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        by, bm = prepare_targets(\n            [y_val_dx[j] for j in range(i, end)],\n            [y_val_dy[j] for j in range(i, end)],\n            horizon\n        )\n        val_batches.append((bx, by, bm))\n    \n    best_loss, best_state, bad = float('inf'), None, 0\n    \n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for bx, by, bm in train_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            pred = model(bx)\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n        \n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by, bm in val_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                val_losses.append(criterion(pred, by, bm).item())\n        \n        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n        scheduler.step(val_loss)\n        \n        if epoch % 10 == 0:\n            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n    \n    if best_state:\n        model.load_state_dict(best_state)\n    \n    return model, best_loss\n\n# ============================================================================\n# MAIN (å·²ä¿®æ”¹ä¸ºä½¿ç”¨ GroupKFold)\n# ============================================================================\n\ndef main():\n    config = Config()\n    \n    # è¿™ä¸ªå‡½æ•°ç°åœ¨åªåœ¨ IS_TRAINING = True æ—¶è¢«è°ƒç”¨\n    print(\"ğŸ†  NFL BDB 2026 - GEOMETRIC ST-TRANSFORMER (TRAINING) ğŸ†\")\n    \n    # [1/4] åŠ è½½è®­ç»ƒæ•°æ®\n    print(\"\\n[1/4] Loading training data...\")\n    train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n    print(f\"âœ“ Train input: {train_input.shape}, Train output: {train_output.shape}\")\n\n    # [2/4] å‡†å¤‡è®­ç»ƒåºåˆ—\n    print(\"\\n[2/4] Preparing geometric sequences...\")\n    result = prepare_sequences_geometric(\n        train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n    )\n    # æ³¨æ„ï¼šfeature_cols åˆ—è¡¨ç°åœ¨ä¹Ÿè¢«è¿”å›\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, geo_x, geo_y, route_kmeans, route_scaler, feature_cols = result\n    \n    # è·å– input_dim ä»¥ä¾¿ä¿å­˜\n    input_dim = sequences[0].shape[1]\n    \n    sequences = list(sequences)\n    targets_dx = list(targets_dx)\n    targets_dy = list(targets_dy)\n    \n    # --- ä¿å­˜ç‰¹å¾å·¥ç¨‹å¯¹è±¡ (ä½¿ç”¨ .pth) ---\n    torch.save(route_kmeans, config.OUTPUT_DIR / \"route_kmeans.pth\")\n    torch.save(route_scaler, config.OUTPUT_DIR / \"route_scaler.pth\")\n    print(f\"âœ“ Saved route K-Means and scaler to {config.OUTPUT_DIR} (as .pth)\")\n\n    # [3/4] è®­ç»ƒæ¨¡å‹ (!!! å·²æ›´æ”¹ä¸º GroupKFold !!!)\n    print(\"\\n[3/4] Training geometric models with GroupKFold...\")\n    \n    # --- GroupKFold é€»è¾‘ ---\n    # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå”¯ä¸€çš„ ID æ¥ä»£è¡¨æ¯ä¸ª \"play\"\n    # 'play_id' æœ¬èº«å¯èƒ½ä¸æ˜¯å…¨å±€å”¯ä¸€çš„ï¼ˆä¾‹å¦‚ï¼Œä¸åŒ game ä¹‹é—´ï¼‰\n    groups_raw = [f\"{sid['game_id']}_{sid['play_id']}\" for sid in sequence_ids]\n    # å°†å­—ç¬¦ä¸²ç»„ ID è½¬æ¢ä¸ºæ•´æ•°\n    unique_groups, groups = np.unique(groups_raw, return_inverse=True)\n    print(f\"âœ“ Created {len(unique_groups)} unique groups (game_play_id) for GroupKFold.\")\n    \n    gkf = GroupKFold(n_splits=config.N_FOLDS)\n    # -------------------------\n    \n    oof_predictions = {}\n    fold_rmses = []\n    \n    # (y=None, groups=groups)\n    for fold, (tr, va) in enumerate(gkf.split(sequences, y=None, groups=groups), 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"Fold {fold}/{config.N_FOLDS}\")\n        print(f\"{'='*60}\")\n        \n        X_tr = [sequences[i] for i in tr]\n        X_va = [sequences[i] for i in va]\n        y_tr_dx = [targets_dx[i] for i in tr]\n        y_va_dx = [targets_dx[i] for i in va]\n        y_tr_dy = [targets_dy[i] for i in tr]\n        y_va_dy = [targets_dy[i] for i in va]\n        \n        scaler = StandardScaler()\n        scaler.fit(np.vstack([s for s in X_tr]))\n        \n        X_tr_sc = [scaler.transform(s) for s in X_tr]\n        X_va_sc = [scaler.transform(s) for s in X_va]\n        \n        model, loss = train_model(\n            X_tr_sc, y_tr_dx, y_tr_dy,\n            X_va_sc, y_va_dx, y_va_dy,\n            input_dim, config.MAX_FUTURE_HORIZON, config # ä½¿ç”¨æ­£ç¡®çš„ input_dim\n        )\n        \n        # --- ä¿å­˜æ¨¡å‹å’Œç¼©æ”¾å™¨ (ä½¿ç”¨ .pth) ---\n        model_path = config.OUTPUT_DIR / f\"st_transformer_fold_{fold}.pth\"\n        scaler_path = config.OUTPUT_DIR / f\"scaler_fold_{fold}.pth\"\n        \n        torch.save(model.state_dict(), model_path)\n        torch.save(scaler, scaler_path)\n        \n        print(f\"âœ“ Saved model to {model_path}\")\n        print(f\"âœ“ Saved scaler to {scaler_path}\")\n        \n        # --- OOF é¢„æµ‹ (ç”¨äº CV éªŒè¯) ---\n        print(f\"  Generating OOF predictions for fold {fold}...\")\n        model.eval()\n        oof_preds_fold = []\n        with torch.no_grad():\n            for i in range(0, len(X_va_sc), config.BATCH_SIZE):\n                end = min(i + config.BATCH_SIZE, len(X_va_sc))\n                batch_x = torch.tensor(np.stack(X_va_sc[i:end]).astype(np.float32)).to(config.DEVICE)\n                batch_preds = model(batch_x).cpu().numpy()\n                oof_preds_fold.append(batch_preds)\n        \n        oof_preds_fold = np.vstack(oof_preds_fold)\n\n        pred_coords, true_coords = [], []\n        for i in range(len(oof_preds_fold)):\n            original_idx = va[i]\n            seq_id_info = sequence_ids[original_idx]\n            sample_id = f\"{seq_id_info['game_id']}_{seq_id_info['play_id']}_{seq_id_info['nfl_id']}\"\n            \n            true_len = len(y_va_dx[i])\n            pred_dx = oof_preds_fold[i, :true_len, 0]\n            pred_dy = oof_preds_fold[i, :true_len, 1]\n            \n            oof_predictions[sample_id] = {\n                'pred_dx': pred_dx, 'pred_dy': pred_dy,\n                'true_dx': y_va_dx[i], 'true_dy': y_va_dy[i]\n            }\n            \n            pred_coords.extend(list(zip(pred_dx, pred_dy)))\n            true_coords.extend(list(zip(y_va_dx[i], y_va_dy[i])))\n        \n        pred_coords = np.array(pred_coords)\n        true_coords = np.array(true_coords)\n        \n        # (pred_coords - true_coords)**2 -> (N, 2)\n        # np.mean(...) -> è®¡ç®—æ‰€æœ‰ N*2 ä¸ªè¯¯å·®çš„å‡å€¼\n        # np.sqrt(...) -> æ±‚å¹³æ–¹æ ¹\n        rmse = np.sqrt(np.mean((pred_coords - true_coords)**2))\n        fold_rmses.append(rmse)\n        print(f\"\\nâœ“ Fold {fold} - Loss: {loss:.5f}, RMSE: {rmse:.5f}\")\n\n    # --- ä¿å­˜å…ƒæ•°æ® (åœ¨å¾ªç¯ç»“æŸå, ä½¿ç”¨ .pth) ---\n    metadata = {\n        'input_dim': input_dim,\n        'feature_cols': feature_cols, # ä¿å­˜ç‰¹å¾åˆ—è¡¨\n        'window_size': config.WINDOW_SIZE,\n        'n_folds': config.N_FOLDS,\n    }\n    torch.save(metadata, config.OUTPUT_DIR / \"metadata.pth\")\n    print(f\"\\nâœ“ Saved metadata (input_dim={input_dim}) to {config.OUTPUT_DIR} (as .pth)\")\n\n\n    # [4/4] CV æ‘˜è¦\n    print(\"\\n[4/4] Calculating CV Summary...\")\n    all_pred_coords, all_true_coords = [], []\n    for sample_id, data in oof_predictions.items():\n        all_pred_coords.extend(list(zip(data['pred_dx'], data['pred_dy'])))\n        all_true_coords.extend(list(zip(data['true_dx'], data['true_dy'])))\n        \n    all_pred_coords = np.array(all_pred_coords)\n    all_true_coords = np.array(all_true_coords)\n    \n    # å†æ¬¡è®¡ç®—æ€» OOF RMSE\n    oof_rmse = np.sqrt(np.mean((all_pred_coords - all_true_coords)**2))\n    mean_rmse = np.mean(fold_rmses)\n    std_rmse = np.std(fold_rmses)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"CV Summary\")\n    print(\"=\"*60)\n    print(f\"Per-fold RMSEs: [{', '.join([f'{r:.5f}' for r in fold_rmses])}]\")\n    print(f\"æ¯æŠ˜ RMSEï¼š[{', '.join([f'{r:.5f}' for r in fold_rmses])}]\")\n    print(f\"Mean CV RMSE: {mean_rmse:.4f} Â± {std_rmse:.4f}\")\n    print(f\"å¹³å‡ CV RMSEï¼š{mean_rmse:.4f} Â± {std_rmse:.4f}\")\n    print(f\"OOF CV RMSE: {oof_rmse:.4f} (æŒ‰æŒ‡æ ‡è®¡ç®—çš„æœ€ç»ˆ OOF)\")\n    print(\"=\"*60 + \"\\n\")\n    \n    print(\"\\n\" + \"ğŸ†\"*40)\n    print(\"ğŸ†  TRAINING COMPLETE! ğŸ†\")\n    print(f\"âœ“ All models and artifacts saved to {config.OUTPUT_DIR} (as .pth)\")\n    print(\"ğŸ†\"*40 + \"\\n\")\n\n\n# ============================================================================\n# --- è„šæœ¬æ‰§è¡Œæ§åˆ¶å™¨ ---\n# ============================================================================\n\nif __name__ == \"__main__\":\n    config = Config()\n    \n    if config.IS_TRAINING:\n        # -----------------\n        #  æ¨¡ å¼ 1: è®­ ç»ƒ\n        # -----------------\n        # åœ¨æœ¬åœ°è¿è¡Œæˆ–åœ¨ Kaggle Notebook ä¸Š\"ä¿å­˜ç‰ˆæœ¬\"æ—¶æ‰§è¡Œ\n        main()\n    \n    # ----------------------------------------------------------------------\n    #  æ¨¡ å¼ 2: æ¨ ç†\n    #  å½“ IS_TRAINING = False æ—¶ï¼Œ`main()` ä¸ä¼šè¿è¡Œï¼Œ\n    #  Python è§£é‡Šå™¨å°†ç»§ç»­æ‰§è¡Œåˆ°æ–‡ä»¶æœ«å°¾çš„æ­¤ `else` å—å’Œ `predict` å®šä¹‰\n    # ----------------------------------------------------------------------\n    else:\n        print(\"[INFERENCE MODE] Script loaded. Defining predict() and starting server...\")\n        \n        # åœ¨ `else` å—ä¸­å®šä¹‰ predict å‡½æ•°ï¼Œ\n        # è¿™æ ·å®ƒåªåœ¨æ¨ç†æ¨¡å¼ä¸‹å­˜åœ¨\n        \n        def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n            \"\"\"\n            Inference function: process each batch of data\n            (ç”± Kaggle æ¨ç†æœåŠ¡å™¨è°ƒç”¨)\n            \n            Args:\n                test: Frames to predict (contains game_id, play_id, nfl_id, frame_id, etc.)\n                test_input: Available input data (historical frames)\n            \n            Returns:\n                DataFrame with x, y coordinates\n            \"\"\"\n            config = Config()\n            \n            # --- 1. é¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½æ¨¡å‹ (ä½¿ç”¨å‡½æ•°å±æ€§ä½œä¸ºç¼“å­˜) ---\n            if not hasattr(predict, \"models_cache\"):\n                print(\"[INFERENCE] First call, loading models and artifacts...\")\n                predict.models_cache = []\n                predict.scalers_cache = []\n                \n                try:\n                    # åŠ è½½å…ƒæ•°æ®\n                    metadata_path = config.OUTPUT_DIR / \"metadata.pth\"\n                    predict.metadata = torch.load(metadata_path, map_location=config.DEVICE,weights_only=False)\n                    \n                    # åŠ è½½è·¯ç”±èšç±»å·¥ä»¶\n                    predict.route_kmeans = torch.load(config.OUTPUT_DIR / \"route_kmeans.pth\", map_location=config.DEVICE,weights_only=False)\n                    predict.route_scaler = torch.load(config.OUTPUT_DIR / \"route_scaler.pth\", map_location=config.DEVICE,weights_only=False)\n                    \n                    print(\"âœ“ Loaded metadata, route K-Means, and route scaler\")\n\n                    input_dim = predict.metadata['input_dim']\n                    n_folds = predict.metadata['n_folds']\n                    print(f\"âœ“ Metadata: input_dim={input_dim}, n_folds={n_folds}\")\n\n                    # åŠ è½½æ‰€æœ‰æŠ˜å çš„æ¨¡å‹å’Œç¼©æ”¾å™¨\n                    for fold in range(1, n_folds + 1):\n                        model_path = config.OUTPUT_DIR / f\"st_transformer_fold_{fold}.pth\"\n                        scaler_path = config.OUTPUT_DIR / f\"scaler_fold_{fold}.pth\"\n                        \n                        # åŠ è½½ scaler\n                        scaler = torch.load(scaler_path, map_location=config.DEVICE,weights_only=False)\n                        predict.scalers_cache.append(scaler)\n                        \n                        # åŠ è½½ model\n                        model = STTransformer(\n                            input_dim=input_dim,\n                            hidden_dim=config.HIDDEN_DIM,\n                            horizon=config.MAX_FUTURE_HORIZON,\n                            window_size=config.WINDOW_SIZE,\n                            n_heads=config.N_HEADS,\n                            n_layers=config.N_LAYERS\n                        )\n                        model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))\n                        model.to(config.DEVICE)\n                        model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n                        predict.models_cache.append(model)\n                        \n                        print(f\"âœ“ Loaded fold {fold} model and scaler\")\n                    \n                    print(f\"âœ“ All {n_folds} models loaded successfully.\")\n                \n                except FileNotFoundError as e:\n                    print(f\"Error: Missing artifact file! {e}\")\n                    print(\"è¯·é¦–å…ˆè¿è¡Œè®­ç»ƒ (è®¾ç½® IS_TRAINING = True) å¹¶ç¡®ä¿æ‰€æœ‰ .pth æ–‡ä»¶éƒ½åœ¨ ./outputs/ ä¸­ã€‚\")\n                    # åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬åº”è¯¥å¼•å‘å¼‚å¸¸æ¥åœæ­¢\n                    raise\n            \n            # --- 2. è½¬æ¢ä¸º Pandas ---\n            test_pd = test.to_pandas()\n            test_input_pd = test_input.to_pandas()\n            \n            # --- 3. æ„å»ºåºåˆ— ---\n            # (æˆ‘ä»¬ä»ç¼“å­˜ä¸­è·å–ç‰¹å¾å·¥ç¨‹å¯¹è±¡)\n            test_seq, test_ids, _, _, feature_cols = prepare_sequences_geometric(\n                test_input_pd, test_template=test_pd, is_training=False, \n                window_size=config.WINDOW_SIZE,\n                route_kmeans=predict.route_kmeans, \n                route_scaler=predict.route_scaler\n            )\n            \n            if not test_seq:\n                # å¦‚æœæ²¡æœ‰ç”Ÿæˆåºåˆ—ï¼ˆä¾‹å¦‚ï¼Œæ‰€æœ‰è¾“å…¥éƒ½å¤ªçŸ­ï¼‰\n                print(\"Warning: No sequences generated for this batch.\")\n                # è¿”å›ä¸€ä¸ªå…·æœ‰æ­£ç¡®å½¢çŠ¶ä½†ä¸ºç©ºï¼ˆæˆ–å…¨ä¸º0ï¼‰çš„ DataFrame\n                return pl.DataFrame({'x': np.zeros(len(test_pd)), 'y': np.zeros(len(test_pd))})\n\n            X_test = list(test_seq)\n            \n            # è·å–æœ€åä¸€ä¸ª x, y åæ ‡\n            try:\n                idx_x = feature_cols.index('x')\n                idx_y = feature_cols.index('y')\n            except ValueError:\n                print(\"Error: 'x' or 'y' not in feature_cols. Assuming 0 and 1.\")\n                idx_x, idx_y = 0, 1\n                \n            x_last = np.array([s[-1, idx_x] for s in X_test], dtype=np.float32)\n            y_last = np.array([s[-1, idx_y] for s in X_test], dtype=np.float32)\n\n            # --- 4. é›†æˆé¢„æµ‹ ---\n            all_preds_ens = []\n            for model, sc in zip(predict.models_cache, predict.scalers_cache):\n                X_sc = [sc.transform(s) for s in X_test]\n                \n                fold_preds_batches = []\n                with torch.no_grad():\n                    for i in range(0, len(X_sc), config.BATCH_SIZE):\n                        end = min(i + config.BATCH_SIZE, len(X_sc))\n                        batch_x = torch.tensor(np.stack(X_sc[i:end]).astype(np.float32)).to(config.DEVICE)\n                        batch_preds = model(batch_x).cpu().numpy()\n                        fold_preds_batches.append(batch_preds)\n                \n                all_preds_ens.append(np.vstack(fold_preds_batches))\n            \n            # å¯¹æ‰€æœ‰æŠ˜å è¿›è¡Œå¹³å‡\n            ens_preds = np.mean(all_preds_ens, axis=0)\n            \n            # --- 5. æ„å»ºä¸è¾“å…¥ `test` DataFrame é¡ºåºç›¸åŒçš„é¢„æµ‹ ---\n            rows = []\n            H = ens_preds.shape[1] # é¢„æµ‹è§†ç•Œ (Horizon)\n            \n            player_pred_map = {}\n            for i, sid in enumerate(test_ids):\n                key = (sid['game_id'], sid['play_id'], sid['nfl_id'])\n                player_pred_map[key] = {\n                    'x_last': x_last[i],\n                    'y_last': y_last[i],\n                    'preds_dx': ens_preds[i, :, 0],\n                    'preds_dy': ens_preds[i, :, 1]\n                }\n            \n            # è®¡ç®—æ¯ä¸ª (gid, pid, nid) ç»„å†…çš„å¸§åº (t)\n            test_pd['t'] = test_pd.groupby(['game_id', 'play_id', 'nfl_id'])['frame_id'].rank(method='first').astype(int) - 1\n\n            for _, row in test_pd.iterrows():\n                key = (row['game_id'], row['play_id'], row['nfl_id'])\n                t = row['t']\n                \n                if key in player_pred_map:\n                    player_data = player_pred_map[key]\n                    tt = min(t, H - 1) # ç¡®ä¿æ—¶é—´æ­¥ä¸è¶Šç•Œ\n                    \n                    px = np.clip(player_data['x_last'] + player_data['preds_dx'][tt], 0, 120)\n                    py = np.clip(player_data['y_last'] + player_data['preds_dy'][tt], 0, 53.3)\n                    \n                    rows.append({'x': px, 'y': py})\n                else:\n                    # Fallback\n                    # è¿™å¯èƒ½å‘ç”Ÿåœ¨ prepare_sequences_geometric ä¸¢å¼ƒäº†æ— æ•ˆåºåˆ—æ—¶\n                    print(f\"Warning: Missing prediction for {key} at t={t}. Appending (0, 0).\")\n                    rows.append({'x': 0.0, 'y': 0.0}) \n                    \n            # è½¬æ¢ä¸º Polars DataFrame\n            predictions = pl.DataFrame(rows)\n            \n            assert len(predictions) == len(test), f\"Prediction length ({len(predictions)}) != Test length ({len(test)})\"\n            \n            return predictions\n\n        # --- æœåŠ¡å™¨å¯åŠ¨ä»£ç  ---\n        # (è¿™éƒ¨åˆ†ä»£ç åœ¨ IS_TRAINING = False æ—¶æ‰ä¼šè¢«æ‰§è¡Œ)\n        \n        # åˆå§‹åŒ–æ¨ç†æœåŠ¡å™¨\n        inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n        \n        # Start server in competition environment\n        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n            print(\"[SERVER] Starting inference server...\")\n            inference_server.serve()\n        else:\n            # æœ¬åœ°æµ‹è¯•ç½‘å…³ (å¦‚æœ KAGGLE_IS_COMPETITION_RERUN æœªè®¾ç½®)\n            print(\"[SERVER] Running local gateway for testing...\")\n            # ç¡®ä¿ä½ çš„æœ¬åœ°æ•°æ®è·¯å¾„æ˜¯æ­£ç¡®çš„\n            local_test_path = str(config.DATA_DIR)\n            if not Path(local_test_path).exists():\n                local_test_path = './' # Fallback\n                print(f\"[SERVER] Warning: DATA_DIR not found, using fallback './'\")\n            \n            print(f\"[SERVER] Using data from: {local_test_path}\")\n            inference_server.run_local_gateway((local_test_path,))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}