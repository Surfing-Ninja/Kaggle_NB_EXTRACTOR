{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13467154,"sourceType":"datasetVersion","datasetId":8548783}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Sequence, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.auto import tqdm\n\n# Optional GPU tabular libs\ntry:\n    import xgboost as xgb  # type: ignore\nexcept Exception:  # pragma: no cover\n    xgb = None\ntry:\n    from catboost import CatBoostRegressor  # type: ignore\nexcept Exception:  # pragma: no cover\n    CatBoostRegressor = None\n\nimport joblib\n\n# ---------------------------------------------------------------------------\n# Logging & constants\n# ---------------------------------------------------------------------------\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"[%(levelname)s] %(message)s\",\n)\nlog = logging.getLogger(\"BDB2026\")\n\nYARDS_TO_METERS = 0.9144\nFPS = 10.0\nFIELD_LENGTH, FIELD_WIDTH = 120.0, 53.3\n\n# ---------------------------------------------------------------------------\n# Config\n# ---------------------------------------------------------------------------\n@dataclass\nclass Config:\n    # paths\n    DATA_DIR: Path = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")  #按照你们各自的数据路径调整下，这个路径是kaggle数据集的root路径\n    OUTPUT_DIR: Path = Path(\"/kaggle/working/\")\n    MODELS_DIR: Path = Path(\"/kaggle/input/model-cv5/ref_models\")\n\n    # run mode\n    TRAIN: bool = False\n    SUB: bool = True\n\n    # model choice: GRU_RES | DYN | XGB | CAT\n    MODEL_NAME: str = \"GRU_RES\"\n\n    # feature groups (lean default)\n    FEATURE_GROUPS: List[str] = field(\n        default_factory=lambda: [\n            \"distance_rate\",\n            \"target_alignment\",\n            \"multi_window_rolling\",\n            \"extended_lags\",\n            \"velocity_changes\",\n            \"field_position\",\n            \"time_features\",\n            # NOTE: removed by default: role_specific, jerk_features, curvature_land_features\n        ]\n    )\n\n    # learning\n    SEED: int = 42\n    N_FOLDS: int = 5\n    BATCH_SIZE: int = 256\n    EPOCHS: int = 125\n    PATIENCE: int = 30\n    LEARNING_RATE: float = 1e-3\n\n    # sequence\n    WINDOW_SIZE: int = 10\n    HIDDEN_DIM: int = 128\n    MAX_FUTURE_HORIZON: int = 94  # do not change\n\n    # torch\n    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def __post_init__(self):\n        self.OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n        self.MODELS_DIR.mkdir(exist_ok=True, parents=True)\n\n\n# ---------------------------------------------------------------------------\n# Utilities\n# ---------------------------------------------------------------------------\n\ndef set_seed(seed: int = 42) -> None:\n    import random\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n\ndef wrap_angle_deg(s: pd.Series | np.ndarray) -> pd.Series | np.ndarray:\n    return ((s + 180.0) % 360.0) - 180.0\n\n\ndef unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Mirror rightward plays so all samples are 'left' oriented (x,y, dir, o, ball_land).\"\"\"\n    if \"play_direction\" not in df.columns:\n        return df\n    out = df.copy()\n    right = out[\"play_direction\"].eq(\"right\")\n    if \"x\" in out.columns:\n        out.loc[right, \"x\"] = FIELD_LENGTH - out.loc[right, \"x\"]\n    if \"y\" in out.columns:\n        out.loc[right, \"y\"] = FIELD_WIDTH - out.loc[right, \"y\"]\n    for col in (\"dir\", \"o\"):\n        if col in out.columns:\n            out.loc[right, col] = (out.loc[right, col] + 180.0) % 360.0\n    if \"ball_land_x\" in out.columns:\n        out.loc[right, \"ball_land_x\"] = FIELD_LENGTH - out.loc[right, \"ball_land_x\"]\n    if \"ball_land_y\" in out.columns:\n        out.loc[right, \"ball_land_y\"] = FIELD_WIDTH - out.loc[right, \"ball_land_y\"]\n    return out\n\n\ndef invert_to_original_direction(x_u: float, y_u: float, play_dir_right: bool) -> Tuple[float, float]:\n    if not play_dir_right:\n        return float(x_u), float(y_u)\n    return float(FIELD_LENGTH - x_u), float(FIELD_WIDTH - y_u)\n\n\n# ---------------------------------------------------------------------------\n# Feature engineering (leaned down)\n# ---------------------------------------------------------------------------\nclass FeatureEngineer:\n    \"\"\"Modular, ablation-friendly feature builder.\"\"\"\n\n    def __init__(self, feature_groups_to_create: Sequence[str]):\n        self.gcols = [\"game_id\", \"play_id\", \"nfl_id\"]\n        self.active_groups = list(feature_groups_to_create)\n        self.feature_creators = {\n            \"distance_rate\": self._create_distance_rate_features,\n            \"target_alignment\": self._create_target_alignment_features,\n            \"multi_window_rolling\": self._create_multi_window_rolling_features,\n            \"extended_lags\": self._create_extended_lag_features,\n            \"velocity_changes\": self._create_velocity_change_features,\n            \"field_position\": self._create_field_position_features,\n            \"time_features\": self._create_time_features,\n        }\n        self.created_feature_cols: List[str] = []\n\n    @staticmethod\n    def _height_to_feet(height_str) -> float:\n        try:\n            ft, inches = map(int, str(height_str).split(\"-\"))\n            return ft + inches / 12\n        except Exception:\n            return 6.0\n\n    def _create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        log.info(\"[FE] Base kinematics & roles…\")\n        out = df.copy()\n        out[\"player_height_feet\"] = out[\"player_height\"].apply(self._height_to_feet)\n\n        # Direction: dir from +x CCW\n        dir_rad = np.deg2rad(out[\"dir\"].fillna(0.0).astype(\"float32\"))\n        out[\"velocity_x\"] = out[\"s\"] * np.cos(dir_rad)\n        out[\"velocity_y\"] = out[\"s\"] * np.sin(dir_rad)\n        out[\"acceleration_x\"] = out[\"a\"] * np.cos(dir_rad)\n        out[\"acceleration_y\"] = out[\"a\"] * np.sin(dir_rad)\n\n        # Roles (kept as raw flags; no engineered role-specific features by default)\n        out[\"is_offense\"] = (out[\"player_side\"] == \"Offense\").astype(np.int8)\n        out[\"is_defense\"] = (out[\"player_side\"] == \"Defense\").astype(np.int8)\n        out[\"is_receiver\"] = (out[\"player_role\"] == \"Targeted Receiver\").astype(np.int8)\n        out[\"is_coverage\"] = (out[\"player_role\"] == \"Defensive Coverage\").astype(np.int8)\n        out[\"is_passer\"] = (out[\"player_role\"] == \"Passer\").astype(np.int8)\n\n        # Ball landing geometry\n        if {\"ball_land_x\", \"ball_land_y\"}.issubset(out.columns):\n            dx = out[\"ball_land_x\"] - out[\"x\"]\n            dy = out[\"ball_land_y\"] - out[\"y\"]\n            dist = np.hypot(dx, dy)\n            out[\"distance_to_ball\"] = dist\n            inv = 1.0 / (dist + 1e-6)\n            out[\"ball_direction_x\"] = dx * inv\n            out[\"ball_direction_y\"] = dy * inv\n            out[\"closing_speed\"] = (\n                out[\"velocity_x\"] * out[\"ball_direction_x\"]\n                + out[\"velocity_y\"] * out[\"ball_direction_y\"]\n            )\n\n        base = [\n            \"x\",\n            \"y\",\n            \"s\",\n            \"a\",\n            \"o\",\n            \"dir\",\n            \"frame_id\",\n            \"ball_land_x\",\n            \"ball_land_y\",\n            \"player_height_feet\",\n            \"player_weight\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"acceleration_x\",\n            \"acceleration_y\",\n            \"is_offense\",\n            \"is_defense\",\n            \"is_receiver\",\n            \"is_coverage\",\n            \"is_passer\",\n            \"distance_to_ball\",\n            \"ball_direction_x\",\n            \"ball_direction_y\",\n            \"closing_speed\",\n        ]\n        self.created_feature_cols.extend([c for c in base if c in out.columns])\n        return out\n\n    # ---- feature groups ----\n    def _create_distance_rate_features(self, df: pd.DataFrame):\n        new_cols: List[str] = []\n        if \"distance_to_ball\" in df.columns:\n            d = df.groupby(self.gcols)[\"distance_to_ball\"].diff()\n            df[\"d2ball_dt\"] = d.fillna(0.0) * FPS\n            df[\"d2ball_ddt\"] = df.groupby(self.gcols)[\"d2ball_dt\"].diff().fillna(0.0) * FPS\n            df[\"time_to_intercept\"] = (\n                (df[\"distance_to_ball\"] / (df[\"d2ball_dt\"].abs() + 1e-3)).clip(0, 10)\n            )\n            new_cols = [\"d2ball_dt\", \"d2ball_ddt\", \"time_to_intercept\"]\n        return df, new_cols\n\n    def _create_target_alignment_features(self, df: pd.DataFrame):\n        new_cols: List[str] = []\n        need = {\"ball_direction_x\", \"ball_direction_y\", \"velocity_x\", \"velocity_y\"}\n        if need.issubset(df.columns):\n            df[\"velocity_alignment\"] = (\n                df[\"velocity_x\"] * df[\"ball_direction_x\"]\n                + df[\"velocity_y\"] * df[\"ball_direction_y\"]\n            )\n            df[\"velocity_perpendicular\"] = (\n                df[\"velocity_x\"] * (-df[\"ball_direction_y\"]) + df[\"velocity_y\"] * df[\"ball_direction_x\"]\n            )\n            new_cols.extend([\"velocity_alignment\", \"velocity_perpendicular\"])\n        return df, new_cols\n\n    def _create_multi_window_rolling_features(self, df: pd.DataFrame):\n        new_cols: List[str] = []\n        for window in (3, 5, 10):\n            for col in (\"velocity_x\", \"velocity_y\", \"s\", \"a\"):\n                if col in df.columns:\n                    r_mean = df.groupby(self.gcols)[col].rolling(window, min_periods=1).mean()\n                    r_std = df.groupby(self.gcols)[col].rolling(window, min_periods=1).std()\n                    r_mean = r_mean.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    r_std = r_std.reset_index(level=list(range(len(self.gcols))), drop=True)\n                    df[f\"{col}_roll{window}\"] = r_mean\n                    df[f\"{col}_std{window}\"] = r_std.fillna(0.0)\n                    new_cols.extend([f\"{col}_roll{window}\", f\"{col}_std{window}\"])\n        return df, new_cols\n\n    def _create_extended_lag_features(self, df: pd.DataFrame):\n        new_cols: List[str] = []\n        for lag in (1, 2, 3, 4, 5):\n            for col in (\"x\", \"y\", \"velocity_x\", \"velocity_y\"):\n                if col in df.columns:\n                    g = df.groupby(self.gcols)[col]\n                    lagv = g.shift(lag)\n                    df[f\"{col}_lag{lag}\"] = lagv.fillna(g.transform(\"first\"))\n                    new_cols.append(f\"{col}_lag{lag}\")\n        return df, new_cols\n\n    def _create_velocity_change_features(self, df: pd.DataFrame):\n        new_cols: List[str] = []\n        if \"velocity_x\" in df.columns:\n            df[\"velocity_x_change\"] = df.groupby(self.gcols)[\"velocity_x\"].diff().fillna(0.0)\n            df[\"velocity_y_change\"] = df.groupby(self.gcols)[\"velocity_y\"].diff().fillna(0.0)\n            df[\"speed_change\"] = df.groupby(self.gcols)[\"s\"].diff().fillna(0.0)\n            d = df.groupby(self.gcols)[\"dir\"].diff().fillna(0.0)\n            df[\"direction_change\"] = wrap_angle_deg(d)\n            new_cols = [\n                \"velocity_x_change\",\n                \"velocity_y_change\",\n                \"speed_change\",\n                \"direction_change\",\n            ]\n        return df, new_cols\n\n    def _create_field_position_features(self, df: pd.DataFrame):\n        df[\"dist_from_left\"] = df[\"y\"]\n        df[\"dist_from_right\"] = FIELD_WIDTH - df[\"y\"]\n        df[\"dist_from_sideline\"] = np.minimum(df[\"dist_from_left\"], df[\"dist_from_right\"])\n        df[\"dist_from_endzone\"] = np.minimum(df[\"x\"], FIELD_LENGTH - df[\"x\"])\n        return df, [\"dist_from_sideline\", \"dist_from_endzone\"]\n\n    def _create_time_features(self, df: pd.DataFrame):\n        df[\"frames_elapsed\"] = df.groupby(self.gcols).cumcount()\n        df[\"normalized_time\"] = df.groupby(self.gcols)[\"frames_elapsed\"].transform(lambda x: x / (x.max() + 1e-9))\n        return df, [\"frames_elapsed\", \"normalized_time\"]\n\n    def transform(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n        df = df.copy().sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n        df = self._create_basic_features(df)\n\n        log.info(\"[FE] Adding selected groups…\")\n        for group_name in self.active_groups:\n            creator = self.feature_creators.get(group_name)\n            if creator is None:\n                log.warning(\"[FE] Unknown group: %s\", group_name)\n                continue\n            df, new_cols = creator(df)\n            self.created_feature_cols.extend(new_cols)\n            log.info(\"  [+] %s (+%d cols)\", group_name, len(new_cols))\n\n        final_cols = sorted(set(self.created_feature_cols))\n        log.info(\"[FE] Total features: %d\", len(final_cols))\n        return df, final_cols\n\n\n# ---------------------------------------------------------------------------\n# Sequence builder\n# ---------------------------------------------------------------------------\n\ndef build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n    s = (\n        df_in[[\"game_id\", \"play_id\", \"play_direction\"]]\n        .drop_duplicates()\n        .set_index([\"game_id\", \"play_id\"])[\"play_direction\"]\n    )\n    return s\n\n\ndef apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n    if \"play_direction\" not in df.columns:\n        dir_df = dir_map.reset_index()\n        df = df.merge(dir_df, on=[\"game_id\", \"play_id\"], how=\"left\", validate=\"many_to_one\")\n    return unify_left_direction(df)\n\n\ndef prepare_sequences(\n    input_df: pd.DataFrame,\n    output_df: pd.DataFrame | None = None,\n    test_template: pd.DataFrame | None = None,\n    is_training: bool = True,\n    window_size: int = 10,\n    feature_groups: Sequence[str] | None = None,\n) -> Tuple:\n    log.info(\"=\" * 80)\n    log.info(\"PREPARING SEQUENCES (unified left frame)\")\n    log.info(\"=\" * 80)\n    log.info(\"window_size=%d\", window_size)\n\n    if feature_groups is None:\n        feature_groups = [\n            \"distance_rate\",\n            \"target_alignment\",\n            \"multi_window_rolling\",\n            \"extended_lags\",\n            \"velocity_changes\",\n            \"field_position\",\n            \"time_features\",\n        ]\n\n    dir_map = build_play_direction_map(input_df)\n    input_df_u = unify_left_direction(input_df)\n\n    if is_training:\n        assert output_df is not None\n        out_u = apply_direction_to_df(output_df, dir_map)\n        target_rows = out_u\n        target_groups = out_u[[\"game_id\", \"play_id\", \"nfl_id\"]].drop_duplicates()\n    else:\n        assert test_template is not None\n        if \"play_direction\" not in test_template.columns:\n            dir_df = dir_map.reset_index()\n            test_template = test_template.merge(dir_df, on=[\"game_id\", \"play_id\"], how=\"left\", validate=\"many_to_one\")\n        target_rows = test_template\n        target_groups = target_rows[[\"game_id\", \"play_id\", \"nfl_id\", \"play_direction\"]].drop_duplicates()\n\n    assert (\n        target_rows[[\"game_id\", \"play_id\", \"play_direction\"]].isna().sum().sum() == 0\n    ), \"play_direction merge failed\"\n    log.info(\"play_direction merge OK: %s\", target_rows[\"play_direction\"].value_counts(dropna=False).to_dict())\n\n    fe = FeatureEngineer(feature_groups)\n    processed_df, feature_cols = fe.transform(input_df_u)\n\n    # ---- build sliding window per (gid,pid,nid)\n    processed_df = processed_df.set_index([\"game_id\", \"play_id\", \"nfl_id\"]).sort_index()\n    grouped = processed_df.groupby(level=[\"game_id\", \"play_id\", \"nfl_id\"])\n\n    idx_x = feature_cols.index(\"x\")\n    idx_y = feature_cols.index(\"y\")\n\n    sequences: List[np.ndarray] = []\n    targets_dx: List[np.ndarray] = []\n    targets_dy: List[np.ndarray] = []\n    targets_fids: List[np.ndarray] = []\n    seq_meta: List[Dict] = []\n\n    it = tqdm(list(target_groups.itertuples(index=False)), total=len(target_groups), desc=\"Create seqs\")\n    for row in it:\n        gid = row[0]\n        pid = row[1]\n        nid = row[2]\n        play_dir = row[3] if (not is_training and len(row) >= 4) else None\n        key = (gid, pid, nid)\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n\n        sequences.append(seq)\n\n        if is_training:\n            out_grp = target_rows[\n                (target_rows[\"game_id\"] == gid)\n                & (target_rows[\"play_id\"] == pid)\n                & (target_rows[\"nfl_id\"] == nid)\n            ].sort_values(\"frame_id\")\n            if len(out_grp) == 0:\n                continue\n            last_x = seq[-1, idx_x]\n            last_y = seq[-1, idx_y]\n            dx = out_grp[\"x\"].values - last_x\n            dy = out_grp[\"y\"].values - last_y\n            targets_dx.append(dx.astype(np.float32))\n            targets_dy.append(dy.astype(np.float32))\n            targets_fids.append(out_grp[\"frame_id\"].values.astype(np.int32))\n\n        seq_meta.append(\n            {\n                \"game_id\": gid,\n                \"play_id\": pid,\n                \"nfl_id\": nid,\n                \"frame_id\": int(input_window.iloc[-1][\"frame_id\"]),\n                \"play_direction\": (None if is_training else play_dir),\n            }\n        )\n\n    log.info(\"Created %d sequences with %d features\", len(sequences), len(feature_cols))\n\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_fids, seq_meta, feature_cols, dir_map\n    return sequences, seq_meta, feature_cols, dir_map\n\n\n# ---------------------------------------------------------------------------\n# Torch models & training\n# ---------------------------------------------------------------------------\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta: float = 0.5, time_decay: float = 0.03):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n\n    def forward(self, pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        err = pred - target\n        abs_err = torch.abs(err)\n        huber = torch.where(\n            abs_err <= self.delta, 0.5 * err * err, self.delta * (abs_err - 0.5 * self.delta)\n        )\n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            w = torch.exp(-self.time_decay * t).view(1, L)\n            huber, mask = huber * w, mask * w\n        return (huber * mask).sum() / (mask.sum() + 1e-8)\n\n\nclass Residual(nn.Module):\n    def __init__(self, mod: nn.Module, dim_in: int, dim_out: int, drop_prob: float = 0.0):\n        super().__init__()\n        self.mod = mod\n        self.proj = nn.Identity() if dim_in == dim_out else nn.Linear(dim_in, dim_out)\n        self.dropout = nn.Dropout(drop_prob)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        y = self.mod(x)\n        x_proj = self.proj(x)\n        return self.dropout(y) + x_proj\n\n\nclass GRUResidualModel(nn.Module):\n    \"\"\"GRU + learned query attention pooling + residual head (predict cumulative Δ).\"\"\"\n\n    def __init__(self, input_dim: int, hidden: int, horizon: int):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden, num_layers=2, batch_first=True, dropout=0.1)\n        self.ln = nn.LayerNorm(hidden)\n        self.attn = nn.MultiheadAttention(hidden, num_heads=4, batch_first=True)\n        self.q = nn.Parameter(torch.randn(1, 1, hidden))\n        self.head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden, horizon),\n        )\n        # residual shortcut from simple linear readout of last step features\n        self.res_proj = nn.Linear(input_dim, horizon)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        h, _ = self.gru(x)  # (B,T,H)\n        B = h.size(0)\n        q = self.q.expand(B, -1, -1)\n        ctx, _ = self.attn(q, self.ln(h), self.ln(h))\n        core = self.head(ctx.squeeze(1))  # (B,H)\n        # residual: map last-step raw features to horizon and add\n        skip = self.res_proj(x[:, -1, :])\n        out = core + skip\n        return torch.cumsum(out, dim=1)\n\n\n# --- Flexible dynamic-space NN (inspired by your baseline2) ---\nclass RNNBlock(nn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int, rnn: str = \"gru\", num_layers: int = 1, dropout: float = 0.1, bidirectional: bool = False):\n        super().__init__()\n        rnn_cls = nn.GRU if rnn.lower() == \"gru\" else nn.LSTM\n        self.rnn = rnn_cls(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0,\n            bidirectional=bidirectional,\n        )\n        self.out_dim = hidden_dim * (2 if bidirectional else 1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        y, _ = self.rnn(x)\n        return y\n\n\nclass Conv1DBlock(nn.Module):\n    def __init__(self, dim: int, kernel_size: int = 3, dilation: int = 1, dropout: float = 0.1):\n        super().__init__()\n        pad = (kernel_size - 1) * dilation // 2\n        self.pre_ln = nn.LayerNorm(dim)\n        self.dw = nn.Conv1d(dim, dim, kernel_size, padding=pad, dilation=dilation, groups=dim)\n        self.pw = nn.Conv1d(dim, dim, 1)\n        self.act = nn.GELU()\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        y = self.pre_ln(x)\n        y = y.transpose(1, 2)\n        y = self.dw(y)\n        y = self.act(y)\n        y = self.pw(y)\n        y = self.drop(y)\n        return y.transpose(1, 2)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim: int, nhead: int = 4, ff_mult: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads=nhead, dropout=dropout, batch_first=True)\n        self.ln2 = nn.LayerNorm(dim)\n        self.ff = nn.Sequential(\n            nn.Linear(dim, ff_mult * dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_mult * dim, dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        h = self.ln1(x)\n        y, _ = self.attn(h, h, h)\n        x = x + y\n        h = self.ln2(x)\n        h = x + self.ff(h)\n        return h\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, dim: int, r: int = 4):\n        super().__init__()\n        hidden = max(1, dim // r)\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden), nn.ReLU(), nn.Linear(hidden, dim), nn.Sigmoid()\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        s = x.mean(dim=1)\n        g = self.net(s).unsqueeze(1)\n        return x * g\n\n\nclass TransformerBlockWrapper(nn.Module):\n    def __init__(self, block: nn.Module):\n        super().__init__()\n        self.block = block\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.block(x)\n\n\nclass FlexibleSeqModel(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        horizon: int,\n        block_specs: List[Dict],\n        dropout: float = 0.2,\n        pooling: str = \"attn\",\n        predict_mode: str = \"steps\",\n        attn_pool_heads: int = 4,\n    ):\n        super().__init__()\n        self.horizon = horizon\n        self.predict_mode = predict_mode\n        self.pooling = pooling\n\n        dim = input_dim\n        blocks: List[nn.Module] = []\n        for spec in block_specs:\n            t = spec[\"type\"].lower()\n            if t == \"rnn\":\n                blk = RNNBlock(\n                    input_dim=dim,\n                    hidden_dim=spec.get(\"hidden\", 128),\n                    rnn=spec.get(\"rnn\", \"gru\"),\n                    num_layers=spec.get(\"layers\", 1),\n                    dropout=spec.get(\"dropout\", 0.1),\n                    bidirectional=spec.get(\"bidirectional\", False),\n                )\n                blocks.append(Residual(blk, dim, blk.out_dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n                dim = blk.out_dim\n            elif t == \"tcn\":\n                blk = Conv1DBlock(\n                    dim, kernel_size=spec.get(\"kernel\", 3), dilation=spec.get(\"dilation\", 1), dropout=spec.get(\"dropout\", 0.1)\n                )\n                blocks.append(Residual(blk, dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n            elif t == \"transformer\":\n                blk = TransformerBlock(\n                    dim, nhead=spec.get(\"nhead\", 4), ff_mult=spec.get(\"ff_mult\", 4), dropout=spec.get(\"dropout\", 0.1)\n                )\n                blocks.append(Residual(TransformerBlockWrapper(blk), dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n            elif t == \"se\":\n                blk = SEBlock(dim, r=spec.get(\"r\", 4))\n                blocks.append(Residual(blk, dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n            else:\n                raise ValueError(f\"Unknown block type: {t}\")\n        self.blocks = nn.ModuleList(blocks)\n\n        if pooling == \"attn\":\n            self.pool_ln = nn.LayerNorm(dim)\n            self.pool_attn = nn.MultiheadAttention(dim, num_heads=attn_pool_heads, batch_first=True)\n            self.pool_vec = nn.Parameter(torch.randn(1, 1, dim))\n        else:\n            self.pool_ln = nn.LayerNorm(dim)\n\n        self.head = nn.Sequential(nn.Linear(dim, 128), nn.GELU(), nn.Dropout(dropout), nn.Linear(128, horizon))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        h = x\n        for blk in self.blocks:\n            h = blk(h)\n        if self.pooling == \"attn\":\n            B, T, D = h.shape\n            q = self.pool_vec.expand(B, -1, -1)\n            k = v = self.pool_ln(h)\n            ctx, _ = self.pool_attn(q, k, v)\n            ctx = ctx.squeeze(1)\n        elif self.pooling == \"mean\":\n            ctx = self.pool_ln(h).mean(dim=1)\n        else:\n            ctx = self.pool_ln(h[:, -1, :])\n        out = self.head(ctx)\n        if self.predict_mode == \"steps\":\n            out = torch.cumsum(out, dim=1)\n        return out\n\n\n# ---------------------------------------------------------------------------\n# Training helpers\n# ---------------------------------------------------------------------------\ndef _masked_mse(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> float:\n    # pred/target/mask: (B, L)\n    se = (pred - target) ** 2\n    se = se * mask\n    denom = mask.sum().clamp_min(1e-8)\n    return (se.sum() / denom).item()\n\ndef masked_rmse(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> float:\n    return float(np.sqrt(_masked_mse(pred, target, mask)))\n\ndef kaggle_combo_rmse(\n    pred_dx: torch.Tensor, pred_dy: torch.Tensor,\n    true_dx: torch.Tensor, true_dy: torch.Tensor,\n    mask: torch.Tensor\n) -> float:\n    # 官方評分：sqrt( 0.5 * (MSE_x + MSE_y) )，注意是先合併 MSE 再開根號\n    mse_x = _masked_mse(pred_dx, true_dx, mask)\n    mse_y = _masked_mse(pred_dy, true_dy, mask)\n    return float(np.sqrt(0.5 * (mse_x + mse_y)))\n\n\ndef prepare_targets(batch_axis: Sequence[np.ndarray], max_h: int) -> Tuple[torch.Tensor, torch.Tensor]:\n    tensors, masks = [], []\n    for arr in batch_axis:\n        L = len(arr)\n        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors), torch.stack(masks)\n\n\ndef build_batches(X: List[np.ndarray], Y: List[np.ndarray], batch_size: int, horizon: int) -> List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n    batches = []\n    for i in range(0, len(X), batch_size):\n        end = min(i + batch_size, len(X))\n        xs = torch.tensor(np.stack(X[i:end]).astype(np.float32))\n        ys, ms = prepare_targets([Y[j] for j in range(i, end)], horizon)\n        batches.append((xs, ys, ms))\n    return batches\n\n\ndef train_torch_axis_model(\n    X_tr: np.ndarray,\n    y_tr: List[np.ndarray],\n    X_va: np.ndarray,\n    y_va: List[np.ndarray],\n    input_dim: int,\n    horizon: int,\n    cfg: Config,\n    model_name: str,\n    axis_name: str = \"dx\",   # 用於日誌標識\n) -> Tuple[nn.Module, float, float, float]:\n    device = cfg.DEVICE\n    if model_name == \"GRU_RES\":\n        model = GRUResidualModel(input_dim=input_dim, hidden=cfg.HIDDEN_DIM, horizon=horizon).to(device)\n    elif model_name == \"DYN\":\n        block_specs = [\n            {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1},\n            {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 1},\n            {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4},\n            {\"type\": \"se\", \"r\": 4},\n        ]\n        model = FlexibleSeqModel(input_dim=input_dim, horizon=horizon, block_specs=block_specs, pooling=\"attn\").to(device)\n    else:\n        raise ValueError(\"Torch training only for GRU_RES or DYN\")\n\n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)\n\n    tr_batches = build_batches(X_tr, y_tr, cfg.BATCH_SIZE, horizon)\n    va_batches = build_batches(X_va, y_va, cfg.BATCH_SIZE, horizon)\n\n    best_loss, best_state, bad = float(\"inf\"), None, 0\n    last_train_loss, last_lr = float(\"inf\"), cfg.LEARNING_RATE\n\n    for epoch in range(1, cfg.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for step, (bx, by, bm) in enumerate(tr_batches):\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            pred = model(bx)\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            # per-batch cosine\n            scheduler.step(epoch - 1 + step / max(1, len(tr_batches)))\n            train_losses.append(loss.item())\n\n        # ---- validation: Huber + masked RMSE（該軸）\n        model.eval()\n        val_losses, val_rmses = [], []\n        with torch.no_grad():\n            for bx, by, bm in va_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                val_losses.append(criterion(pred, by, bm).item())\n                val_rmses.append(masked_rmse(pred, by, bm))\n\n        trl = float(np.mean(train_losses))\n        val = float(np.mean(val_losses))\n        vrmse = float(np.mean(val_rmses))\n        last_train_loss = trl\n        last_lr = optimizer.param_groups[0][\"lr\"]\n\n        # ✅ 每個 epoch 都打印\n        log.info(\"  [%s] Epoch %03d | Train Loss=%.4f | Val Huber=%.4f | Val RMSE=%.4f | LR=%.2e\",\n                 axis_name, epoch, trl, val, vrmse, last_lr)\n\n        # 早停\n        if val < best_loss:\n            best_loss, bad = val, 0\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            bad += 1\n            if bad >= cfg.PATIENCE:\n                log.info(\"  [%s] Early stop at epoch %d\", axis_name, epoch)\n                break\n\n    if best_state:\n        model.load_state_dict(best_state)\n    return model, best_loss, last_train_loss, last_lr\n\n# ---------------------------------------------------------------------------\n# Tabular (XGB / CAT) training helpers – flatten sequences\n# ---------------------------------------------------------------------------\n\ndef flatten_sequences(X: List[np.ndarray]) -> np.ndarray:\n    Xf = np.stack([x.reshape(-1) for x in X]).astype(np.float32)\n    return Xf\n\n\ndef train_xgb_axis_model(X_tr: np.ndarray, y_tr: List[np.ndarray], X_va: np.ndarray, y_va: List[np.ndarray], cfg: Config, horizon: int):\n    if xgb is None:\n        raise ImportError(\"xgboost is not available in this environment\")\n    from sklearn.multioutput import MultiOutputRegressor\n\n    model = MultiOutputRegressor(\n        xgb.XGBRegressor(\n            max_depth=8,\n            n_estimators=1200,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            learning_rate=0.03,\n            tree_method=\"gpu_hist\",\n            predictor=\"gpu_predictor\",\n            reg_lambda=1.0,\n            reg_alpha=0.0,\n            random_state=cfg.SEED,\n        )\n    )\n    Ytr = np.stack([y for y in y_tr])\n    Yva = np.stack([y for y in y_va])\n    model.fit(X_tr, Ytr)\n    # simple validation RMSE\n    pred = model.predict(X_va)\n    rmse = float(np.sqrt(((pred - Yva) ** 2).mean()))\n    return model, rmse\n\n\ndef train_cat_axis_model(X_tr: np.ndarray, y_tr: List[np.ndarray], X_va: np.ndarray, y_va: List[np.ndarray], cfg: Config, horizon: int):\n    if CatBoostRegressor is None:\n        raise ImportError(\"catboost is not available in this environment\")\n    Ytr = np.stack([y for y in y_tr])\n    Yva = np.stack([y for y in y_va])\n    model = CatBoostRegressor(\n        loss_function=\"MultiRMSE\",\n        iterations=2500,\n        learning_rate=0.03,\n        depth=8,\n        subsample=0.8,\n        rsm=0.8,\n        random_state=cfg.SEED,\n        task_type=\"GPU\",\n        verbose=False,\n    )\n    model.fit(X_tr, Ytr, eval_set=(X_va, Yva), verbose=False)\n    pred = model.predict(X_va)\n    rmse = float(np.sqrt(((pred - Yva) ** 2).mean()))\n    return model, rmse\n\n\n# ---------------------------------------------------------------------------\n# Model save/load helpers\n# ---------------------------------------------------------------------------\n\ndef fold_dir(cfg: Config, fold: int) -> Path:\n    d = cfg.MODELS_DIR / f\"fold{fold}\"\n    d.mkdir(parents=True, exist_ok=True)\n    return d\n\n\ndef save_fold_artifacts(\n    fold: int,\n    scaler: StandardScaler,\n    model_dx,\n    model_dy,\n    cfg: Config,\n):\n    sdir = fold_dir(cfg, fold)\n    joblib.dump(scaler, sdir / \"scaler.pkl\")\n    if cfg.MODEL_NAME in {\"GRU_RES\", \"DYN\"}:\n        torch.save(model_dx.state_dict(), sdir / \"model_dx.pt\")\n        torch.save(model_dy.state_dict(), sdir / \"model_dy.pt\")\n    else:\n        joblib.dump(model_dx, sdir / \"model_dx.pkl\")\n        joblib.dump(model_dy, sdir / \"model_dy.pkl\")\n\n\ndef write_meta(cfg: Config, feature_cols: List[str]):\n    meta = {\n        \"model_name\": cfg.MODEL_NAME,\n        \"n_folds\": cfg.N_FOLDS,\n        \"feature_cols\": feature_cols,\n        \"window_size\": cfg.WINDOW_SIZE,\n        \"max_future_horizon\": cfg.MAX_FUTURE_HORIZON,\n        \"feature_groups\": cfg.FEATURE_GROUPS,\n        \"version\": 2,\n    }\n    with open(cfg.MODELS_DIR / \"meta.json\", \"w\") as f:\n        json.dump(meta, f)\n    log.info(\"[META] wrote meta.json to %s\", cfg.MODELS_DIR)\n\n\ndef load_cv(cfg: Config):\n    meta_path = cfg.MODELS_DIR / \"meta.json\"\n    assert meta_path.exists(), f\"meta.json not found: {meta_path}\"\n    with open(meta_path, \"r\") as f:\n        meta = json.load(f)\n    feature_cols = meta[\"feature_cols\"]\n    horizon = int(meta[\"max_future_horizon\"])\n    n_folds = int(meta[\"n_folds\"])\n    model_name = meta.get(\"model_name\", cfg.MODEL_NAME)\n\n    models_x, models_y, scalers = [], [], []\n    for fold in range(1, n_folds + 1):\n        sdir = fold_dir(cfg, fold)\n        scaler = joblib.load(sdir / \"scaler.pkl\")\n        scalers.append(scaler)\n        if model_name in {\"GRU_RES\", \"DYN\"}:\n            if model_name == \"GRU_RES\":\n                mx = GRUResidualModel(len(feature_cols), cfg.HIDDEN_DIM, horizon).to(cfg.DEVICE)\n                my = GRUResidualModel(len(feature_cols), cfg.HIDDEN_DIM, horizon).to(cfg.DEVICE)\n            else:\n                block_specs = [\n                    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1},\n                    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 1},\n                    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4},\n                    {\"type\": \"se\", \"r\": 4},\n                ]\n                mx = FlexibleSeqModel(len(feature_cols), horizon, block_specs, pooling=\"attn\").to(cfg.DEVICE)\n                my = FlexibleSeqModel(len(feature_cols), horizon, block_specs, pooling=\"attn\").to(cfg.DEVICE)\n            mx.load_state_dict(torch.load(sdir / \"model_dx.pt\", map_location=cfg.DEVICE))\n            my.load_state_dict(torch.load(sdir / \"model_dy.pt\", map_location=cfg.DEVICE))\n            mx.eval(); my.eval()\n        else:\n            mx = joblib.load(sdir / \"model_dx.pkl\")\n            my = joblib.load(sdir / \"model_dy.pkl\")\n        models_x.append(mx)\n        models_y.append(my)\n\n    log.info(\"[LOAD] loaded %d-fold models from %s\", len(models_x), cfg.MODELS_DIR)\n    return models_x, models_y, scalers, meta\n\n\n# ---------------------------------------------------------------------------\n# Main\n# ---------------------------------------------------------------------------\n\ndef main():\n    cfg = Config()\n    set_seed(cfg.SEED)\n\n    log.info(\"=\" * 80)\n    log.info(\"RUN MODE: TRAIN=%s | SUB=%s | MODEL=%s\", cfg.TRAIN, cfg.SUB, cfg.MODEL_NAME)\n    log.info(\"=\" * 80)\n\n    if cfg.TRAIN:\n        # 1) load\n        log.info(\"[1/3] Loading train data…\")\n        train_input_files = [cfg.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n        train_output_files = [cfg.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n        train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()], ignore_index=True)\n        train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()], ignore_index=True)\n\n        # 2) sequences\n        log.info(\"[2/3] Feature engineering & sequences…\")\n        seqs, tdx, tdy, tfids, seq_meta, feat_cols, dir_map = prepare_sequences(\n            train_input,\n            output_df=train_output,\n            is_training=True,\n            window_size=cfg.WINDOW_SIZE,\n            feature_groups=cfg.FEATURE_GROUPS,\n        )\n        write_meta(cfg, feat_cols)\n\n        # 3) 5-fold CV (single seed)\n        log.info(\"[3/3] 5-fold CV training (%s)…\", cfg.MODEL_NAME)\n        groups = np.array([d[\"game_id\"] for d in seq_meta])\n        gkf = GroupKFold(n_splits=cfg.N_FOLDS)\n        fold_metrics: List[Dict[str, float]] = []\n\n        for fold, (tr, va) in enumerate(gkf.split(seqs, groups=groups), 1):\n            log.info(\"-\" * 60)\n            log.info(\"Fold %d/%d\", fold, cfg.N_FOLDS)\n            X_tr_raw = [seqs[i] for i in tr]\n            X_va_raw = [seqs[i] for i in va]\n\n            scaler = StandardScaler()\n            scaler.fit(np.vstack([s for s in X_tr_raw]))\n            X_tr_sc = np.stack([scaler.transform(s) for s in X_tr_raw]).astype(np.float32)\n            X_va_sc = np.stack([scaler.transform(s) for s in X_va_raw]).astype(np.float32)\n\n            H = cfg.MAX_FUTURE_HORIZON\n            if cfg.MODEL_NAME in {\"GRU_RES\", \"DYN\"}:\n                # ΔX\n                mx, val_huber_x, tr_huber_x, lr_x = train_torch_axis_model(\n                    X_tr_sc, [tdx[i] for i in tr], X_va_sc, [tdx[i] for i in va],\n                    X_tr_sc.shape[-1], H, cfg, cfg.MODEL_NAME, axis_name=\"dx\"\n                )\n                # ΔY\n                my, val_huber_y, tr_huber_y, lr_y = train_torch_axis_model(\n                    X_tr_sc, [tdy[i] for i in tr], X_va_sc, [tdy[i] for i in va],\n                    X_tr_sc.shape[-1], H, cfg, cfg.MODEL_NAME, axis_name=\"dy\"\n                )\n\n                # ---- 準備驗證集目標與 mask\n                dx_va_t, m_va = prepare_targets([tdx[i] for i in va], H)\n                dy_va_t, _    = prepare_targets([tdy[i] for i in va], H)\n\n                # ---- 模型在驗證集上的預測\n                with torch.no_grad():\n                    Xv = torch.tensor(X_va_sc, device=cfg.DEVICE)\n                    pred_dx_va = mx(Xv).cpu()\n                    pred_dy_va = my(Xv).cpu()\n\n                # ---- 軸向 RMSE（檢查用）\n                rmse_dx = masked_rmse(pred_dx_va, dx_va_t, m_va)\n                rmse_dy = masked_rmse(pred_dy_va, dy_va_t, m_va)\n\n                # ---- 官方 Kaggle RMSE（重點）\n                rmse_kaggle = kaggle_combo_rmse(pred_dx_va, pred_dy_va, dx_va_t, dy_va_t, m_va)\n\n                # ---- 單行總結輸出\n                train_loss_avg = 0.5 * (tr_huber_x + tr_huber_y)  # 兩軸 Huber 平均，作為 Train Loss\n                log.info(\"Train Loss=%.4f | Val RMSE (dx=%.4f, dy=%.4f) | Kaggle RMSE=%.4f | LR=%.2e\",\n                         train_loss_avg, rmse_dx, rmse_dy, rmse_kaggle, lr_y)\n                fold_metrics.append({\n                    \"fold\": float(fold),\n                    \"rmse_dx\": float(rmse_dx),\n                    \"rmse_dy\": float(rmse_dy),\n                    \"rmse_kaggle\": float(rmse_kaggle),\n                })\n\n\n            elif cfg.MODEL_NAME == \"XGB\":\n                X_tr_flat = flatten_sequences(X_tr_sc)\n                X_va_flat = flatten_sequences(X_va_sc)\n\n                mx, _ = train_xgb_axis_model(X_tr_flat, [tdx[i] for i in tr],\n                                             X_va_flat, [tdx[i] for i in va], cfg, H)\n                my, _ = train_xgb_axis_model(X_tr_flat, [tdy[i] for i in tr],\n                                             X_va_flat, [tdy[i] for i in va], cfg, H)\n\n                # 準備目標與 mask\n                dx_va_t, m_va = prepare_targets([tdx[i] for i in va], H)\n                dy_va_t, _    = prepare_targets([tdy[i] for i in va], H)\n\n                # 驗證集預測（np -> torch）\n                pred_dx = torch.tensor(mx.predict(X_va_flat), dtype=torch.float32)\n                pred_dy = torch.tensor(my.predict(X_va_flat), dtype=torch.float32)\n\n                # 軸向 RMSE（參考）\n                rmse_dx = masked_rmse(pred_dx, dx_va_t, m_va)\n                rmse_dy = masked_rmse(pred_dy, dy_va_t, m_va)\n\n                # Kaggle RMSE（重點）\n                rmse_kaggle = kaggle_combo_rmse(pred_dx, pred_dy, dx_va_t, dy_va_t, m_va)\n                log.info(\"[VAL] fold %d (XGB): RMSE(dx=%.4f, dy=%.4f) | Kaggle RMSE=%.4f\",\n                         fold, rmse_dx, rmse_dy, rmse_kaggle)\n                fold_metrics.append({\n                    \"fold\": float(fold),\n                    \"rmse_dx\": float(rmse_dx),\n                    \"rmse_dy\": float(rmse_dy),\n                    \"rmse_kaggle\": float(rmse_kaggle),\n                })\n\n            elif cfg.MODEL_NAME == \"CAT\":\n                X_tr_flat = flatten_sequences(X_tr_sc)\n                X_va_flat = flatten_sequences(X_va_sc)\n\n                mx, _ = train_cat_axis_model(X_tr_flat, [tdx[i] for i in tr],\n                                             X_va_flat, [tdx[i] for i in va], cfg, H)\n                my, _ = train_cat_axis_model(X_tr_flat, [tdy[i] for i in tr],\n                                             X_va_flat, [tdy[i] for i in va], cfg, H)\n\n                dx_va_t, m_va = prepare_targets([tdx[i] for i in va], H)\n                dy_va_t, _    = prepare_targets([tdy[i] for i in va], H)\n\n                pred_dx = torch.tensor(mx.predict(X_va_flat), dtype=torch.float32)\n                pred_dy = torch.tensor(my.predict(X_va_flat), dtype=torch.float32)\n\n                rmse_dx = masked_rmse(pred_dx, dx_va_t, m_va)\n                rmse_dy = masked_rmse(pred_dy, dy_va_t, m_va)\n                rmse_kaggle = kaggle_combo_rmse(pred_dx, pred_dy, dx_va_t, dy_va_t, m_va)\n                log.info(\"[VAL] fold %d (CAT): RMSE(dx=%.4f, dy=%.4f) | Kaggle RMSE=%.4f\",\n                         fold, rmse_dx, rmse_dy, rmse_kaggle)\n                fold_metrics.append({\n                    \"fold\": float(fold),\n                    \"rmse_dx\": float(rmse_dx),\n                    \"rmse_dy\": float(rmse_dy),\n                    \"rmse_kaggle\": float(rmse_kaggle),\n                })\n\n\n            else:\n                raise ValueError(f\"Unknown MODEL_NAME={cfg.MODEL_NAME}\")\n\n            # 保存当前折的 scaler 与两个轴向模型\n            save_fold_artifacts(fold=fold, scaler=scaler, model_dx=mx, model_dy=my, cfg=cfg)\n        \n        \n        if fold_metrics:\n            mdf = pd.DataFrame(fold_metrics).sort_values(\"fold\")\n            # 均值與樣本標準差（ddof=1），以及方差\n            summary = {\n                \"model_name\": cfg.MODEL_NAME,\n                \"n_folds\": int(len(mdf)),\n                \"rmse_dx\": {\n                    \"mean\": float(mdf[\"rmse_dx\"].mean()),\n                    \"std\": float(mdf[\"rmse_dx\"].std(ddof=1)),\n                    \"var\": float(mdf[\"rmse_dx\"].var(ddof=1)),\n                },\n                \"rmse_dy\": {\n                    \"mean\": float(mdf[\"rmse_dy\"].mean()),\n                    \"std\": float(mdf[\"rmse_dy\"].std(ddof=1)),\n                    \"var\": float(mdf[\"rmse_dy\"].var(ddof=1)),\n                },\n                \"kaggle_rmse\": {\n                    \"mean\": float(mdf[\"rmse_kaggle\"].mean()),\n                    \"std\": float(mdf[\"rmse_kaggle\"].std(ddof=1)),\n                    \"var\": float(mdf[\"rmse_kaggle\"].var(ddof=1)),\n                },\n            }\n            # 打印總結\n            log.info(\"[CV SUMMARY] Folds=%d | Kaggle RMSE: mean=%.4f, std=%.4f, var=%.6f | \"\n                     \"dx mean=%.4f std=%.4f | dy mean=%.4f std=%.4f\",\n                     summary[\"n_folds\"],\n                     summary[\"kaggle_rmse\"][\"mean\"], summary[\"kaggle_rmse\"][\"std\"], summary[\"kaggle_rmse\"][\"var\"],\n                     summary[\"rmse_dx\"][\"mean\"], summary[\"rmse_dx\"][\"std\"],\n                     summary[\"rmse_dy\"][\"mean\"], summary[\"rmse_dy\"][\"std\"])\n            # 存檔（方便追溯）\n            mdf.to_csv(cfg.MODELS_DIR / \"fold_metrics.csv\", index=False)\n            with open(cfg.MODELS_DIR / \"cv_metrics.json\", \"w\") as f:\n                json.dump(summary, f, indent=2)\n        else:\n            log.warning(\"[CV SUMMARY] No fold metrics collected — please check data splits or masks.\")\n        \n        log.info(\"=\" * 80)\n        log.info(\"COMPLETE (TRAIN). Models saved under: %s\", cfg.MODELS_DIR)\n        return\n\n    # ---------------------------  SUBMIT / INFERENCE  ---------------------------\n    if cfg.SUB:\n        log.info(\"[1/3] Loading test data…\")\n        test_input = pd.read_csv(cfg.DATA_DIR / \"test_input.csv\")\n        test_template = pd.read_csv(cfg.DATA_DIR / \"test.csv\")\n\n        log.info(\"[2/3] Loading CV models & meta…\")\n        models_x, models_y, scalers, meta = load_cv(cfg)\n        saved_feature_cols = meta[\"feature_cols\"]\n        saved_window = int(meta[\"window_size\"])\n        model_name = meta.get(\"model_name\", cfg.MODEL_NAME)\n\n        log.info(\"[3/3] Building test sequences & predicting…\")\n        test_seqs, test_meta, feat_cols_t, _ = prepare_sequences(\n            test_input,\n            test_template=test_template,\n            is_training=False,\n            window_size=saved_window,\n            feature_groups=meta.get(\"feature_groups\", cfg.FEATURE_GROUPS),\n        )\n        assert feat_cols_t == saved_feature_cols, \\\n            f\"Feature mismatch! train:{len(saved_feature_cols)} vs test:{len(feat_cols_t)}\"\n\n        idx_x = feat_cols_t.index(\"x\")\n        idx_y = feat_cols_t.index(\"y\")\n        X_test_raw = list(test_seqs)\n        x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n        y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n\n        H = int(meta[\"max_future_horizon\"])\n        all_preds_dx, all_preds_dy = [], []\n\n        for mx, my, sc in zip(models_x, models_y, scalers):\n            X_sc = np.stack([sc.transform(s) for s in X_test_raw]).astype(np.float32)\n\n            if model_name in {\"GRU_RES\", \"DYN\"}:\n                X_t = torch.tensor(X_sc).to(cfg.DEVICE)\n                with torch.no_grad():\n                    pred_dx = mx(X_t).cpu().numpy()\n                    pred_dy = my(X_t).cpu().numpy()\n            elif model_name in {\"XGB\", \"CAT\"}:\n                X_flat = flatten_sequences(X_sc)\n                pred_dx = mx.predict(X_flat)\n                pred_dy = my.predict(X_flat)\n            else:\n                raise ValueError(f\"Unknown model_name {model_name}\")\n\n            all_preds_dx.append(pred_dx)\n            all_preds_dy.append(pred_dy)\n\n        ens_dx = np.mean(all_preds_dx, axis=0)\n        ens_dy = np.mean(all_preds_dy, axis=0)\n\n        # 组装提交（把统一坐标反变换回原始方向）\n        rows = []\n        tt_idx = test_template.set_index([\"game_id\", \"play_id\", \"nfl_id\"]).sort_index()\n\n        for i, meta_row in enumerate(test_meta):\n            gid = meta_row[\"game_id\"]\n            pid = meta_row[\"play_id\"]\n            nid = meta_row[\"nfl_id\"]\n            play_is_right = (meta_row[\"play_direction\"] == \"right\")\n\n            try:\n                fids = tt_idx.loc[(gid, pid, nid), \"frame_id\"]\n                if isinstance(fids, pd.Series):\n                    fids = fids.sort_values().tolist()\n                else:\n                    fids = [int(fids)]\n            except KeyError:\n                continue\n\n            for t, fid in enumerate(fids):\n                tt = min(t, H - 1)\n                x_uni = float(np.clip(x_last_uni[i] + ens_dx[i, tt], 0, FIELD_LENGTH))\n                y_uni = float(np.clip(y_last_uni[i] + ens_dy[i, tt], 0, FIELD_WIDTH))\n                x_out, y_out = invert_to_original_direction(x_uni, y_uni, play_is_right)\n                rows.append({\"id\": f\"{gid}_{pid}_{nid}_{int(fid)}\", \"x\": x_out, \"y\": y_out})\n\n        submission = pd.DataFrame(rows)\n        submission.to_csv(\"submission.csv\", index=False)\n        log.info(\"=\" * 80)\n        log.info(\"COMPLETE (SUB). Saved submission.csv  |  Rows: %d\", len(submission))\n        return\n\n    raise ValueError(\"Please set mode in Config: TRAIN=True/SUB=False or TRAIN=False/SUB=True\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T14:58:25.676494Z","iopub.execute_input":"2025-10-22T14:58:25.676811Z","iopub.status.idle":"2025-10-22T14:58:39.833533Z","shell.execute_reply.started":"2025-10-22T14:58:25.676789Z","shell.execute_reply":"2025-10-22T14:58:39.832507Z"}},"outputs":[],"execution_count":null}]}