{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:37:04.348154Z","iopub.execute_input":"2025-09-25T16:37:04.348502Z","iopub.status.idle":"2025-09-25T16:37:06.133913Z","shell.execute_reply.started":"2025-09-25T16:37:04.348467Z","shell.execute_reply":"2025-09-25T16:37:06.132528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Dependencies: torch, pandas, numpy, sklearn\n\nimport os, glob, math, random, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# Config / Hyperparams\n\nDATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction\"\nTRAIN_INPUT_GLOB = os.path.join(DATA_DIR, \"train\", \"input_2023_w*.csv\")\nTRAIN_OUTPUT_GLOB = os.path.join(DATA_DIR, \"train\", \"output_2023_w*.csv\")\nTEST_INPUT_PATH = os.path.join(DATA_DIR, \"test_input.csv\")\nTEST_CSV_PATH = os.path.join(DATA_DIR, \"test.csv\")\nSAMPLE_SUB_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nK_IN = 10            # number of pre-pass frames used (last K_IN frames)\nHIDDEN = 128         # LSTM hidden size\nNLAYERS = 2\nBIDIR = True\nBATCH_SIZE = 512\nLR = 1e-3\nEPOCHS = 10          # increase for better results\nPATIENCE = 3\nMAX_OUT_FRAMES = 60  # maximum frames to predict (unlikely to exceed; we'll find real max from data)\nDROPOUT = 0.1\n\n\n# Helpers: velocity & parsing\n\ndef add_velocity(df):\n    # dir is degrees (0 right). Convert to vx, vy (yards/sec)\n    rad = np.deg2rad(df[\"dir\"].values.astype(float))\n    vx = df[\"s\"].values.astype(float) * np.cos(rad)\n    vy = df[\"s\"].values.astype(float) * np.sin(rad)\n    df[\"vx\"] = vx\n    df[\"vy\"] = vy\n    return df\n\n# -----------------------\n# Load and aggregate train files\n# -----------------------\nprint(\"Loading training files...\")\ninput_paths = sorted(glob.glob(TRAIN_INPUT_GLOB))\noutput_paths = sorted(glob.glob(TRAIN_OUTPUT_GLOB))\n\ntrain_inputs = [pd.read_csv(p) for p in input_paths]\ntrain_outputs = [pd.read_csv(p) for p in output_paths]\ndf_in = pd.concat(train_inputs, ignore_index=True)\ndf_out = pd.concat(train_outputs, ignore_index=True)\nprint(\"Total input rows:\", len(df_in), \"Total output rows:\", len(df_out))\n\n# Precompute max output length\nmax_out = df_in[\"num_frames_output\"].max() if \"num_frames_output\" in df_in.columns else MAX_OUT_FRAMES\nMAX_OUT_FRAMES = int(max(max_out, MAX_OUT_FRAMES))\nprint(\"MAX_OUT_FRAMES set to\", MAX_OUT_FRAMES)\n\n# Add velocities\ndf_in = add_velocity(df_in)\n# output file doesn't contain s/dir typically; we only need x,y for output.\n\n\n# Build label encoders and scalers\n\n# player_role, player_position, player_side encode\nle_role = LabelEncoder()\ndf_in[\"player_role_f\"] = le_role.fit_transform(df_in[\"player_role\"].astype(str))\n\nle_pos = LabelEncoder()\ndf_in[\"player_pos_f\"] = le_pos.fit_transform(df_in[\"player_position\"].astype(str))\n\nside_map = {\"Offense\":1, \"Defense\":0}\ndf_in[\"player_side_f\"] = df_in[\"player_side\"].map(side_map).fillna(0).astype(int)\n\n# We'll scale numeric features per column with StandardScaler fit on inputs\nnumeric_cols = [\"x\",\"y\",\"s\",\"a\",\"vx\",\"vy\"]\nscaler = StandardScaler()\nscaler.fit(df_in[numeric_cols].values)\n\n# -----------------------\n# Build list of training examples\n# For each (game_id, play_id, nfl_id) we gather the last K_IN pre-pass frames (pad if needed) and the full post-pass outputs.\n# -----------------------\nprint(\"Creating training examples...\")\n\n# index outputs grouped for quick lookup\nout_group = df_out.groupby([\"game_id\",\"play_id\",\"nfl_id\"])\nin_group = df_in.groupby([\"game_id\",\"play_id\",\"nfl_id\"])\n\nexamples = []  # list of dicts: {key, input_df, out_df, num_out}\nfor name, in_grp in in_group:\n    gid, pid, nid = name\n    # corresponding out\n    try:\n        out_grp = out_group.get_group((gid,pid,nid)).sort_values(\"frame_id\")\n    except KeyError:\n        continue\n    in_grp_sorted = in_grp.sort_values(\"frame_id\")\n    # take last K_IN rows\n    last_frames = in_grp_sorted.tail(K_IN)\n    examples.append({\n        \"game_id\": gid, \"play_id\": pid, \"nfl_id\": nid,\n        \"input\": last_frames, \"output\": out_grp, \"num_out\": len(out_grp)\n    })\n\nprint(\"Total examples:\", len(examples))\n\n# -----------------------\n# Dataset + collate\n# -----------------------\nclass TrajectoryDataset(Dataset):\n    def __init__(self, examples, scaler, K_in=K_IN, max_out=MAX_OUT_FRAMES, numeric_cols=numeric_cols):\n        self.examples = examples\n        self.scaler = scaler\n        self.K_in = K_in\n        self.max_out = max_out\n        self.numeric_cols = numeric_cols\n    def __len__(self):\n        return len(self.examples)\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        inp = ex[\"input\"].copy()\n        out = ex[\"output\"].copy()\n        # build input feature matrix (K_in x features)\n        # features: x,y,vx,vy,a, role, pos, side, rel_ball_x,y\n        # get ball landing location from input (same for all frames in play)\n        ball_x = inp[\"ball_land_x\"].iloc[-1] if \"ball_land_x\" in inp.columns else np.nan\n        ball_y = inp[\"ball_land_y\"].iloc[-1] if \"ball_land_y\" in inp.columns else np.nan\n\n        # numeric scaled features\n        num = inp[self.numeric_cols].values.astype(float)\n        num = self.scaler.transform(num)\n        # relative to ball\n        rel = np.column_stack([ (inp[\"x\"].values - ball_x), (inp[\"y\"].values - ball_y) ])\n        # categorical features\n        role = inp[\"player_role_f\"].values.reshape(-1,1)\n        pos = inp[\"player_pos_f\"].values.reshape(-1,1)\n        side = inp[\"player_side_f\"].values.reshape(-1,1)\n        # concat features\n        feats = np.concatenate([num, rel, role, pos, side], axis=1)  # shape (n_frames, feat_dim)\n        # pad to K_in (pre-pad with first row)\n        if feats.shape[0] < self.K_in:\n            pad_n = self.K_in - feats.shape[0]\n            pad_rows = np.repeat(feats[[0], :], pad_n, axis=0)\n            feats = np.vstack([pad_rows, feats])\n        else:\n            feats = feats[-self.K_in:, :]\n        # target: output x,y sequence (num_out x 2) relative to last input pos\n        last_x = inp[\"x\"].values[-1]\n        last_y = inp[\"y\"].values[-1]\n        out_xy = out[[\"x\",\"y\"]].values.astype(float)\n        # compute displacement from last input\n        disp = out_xy - np.array([[last_x, last_y]])\n        # pad/truncate to max_out frames\n        T = disp.shape[0]\n        disp_padded = np.zeros((self.max_out, 2), dtype=float)\n        mask = np.zeros((self.max_out,), dtype=float)\n        take = min(T, self.max_out)\n        disp_padded[:take,:] = disp[:take,:]\n        mask[:take] = 1.0\n        sample = {\n            \"feats\": feats.astype(np.float32),\n            \"target_disp\": disp_padded.astype(np.float32),\n            \"mask\": mask.astype(np.float32),\n            \"meta\": (ex[\"game_id\"], ex[\"play_id\"], ex[\"nfl_id\"], T)\n        }\n        return sample\n\ndef collate_fn(batch):\n    feats = np.stack([b[\"feats\"] for b in batch], axis=0)        # (B, K_in, feat_dim)\n    targets = np.stack([b[\"target_disp\"] for b in batch], axis=0) # (B, max_out, 2)\n    masks = np.stack([b[\"mask\"] for b in batch], axis=0)          # (B, max_out)\n    metas = [b[\"meta\"] for b in batch]\n    return {\n        \"feats\": torch.from_numpy(feats),\n        \"targets\": torch.from_numpy(targets),\n        \"masks\": torch.from_numpy(masks),\n        \"metas\": metas\n    }\n\n# -----------------------\n# Split examples into train/val (by game_id to avoid leakage)\n# -----------------------\nprint(\"Splitting train/val by game...\")\ngame_ids = list({(e[\"game_id\"]) for e in examples})\nrandom.shuffle(game_ids)\nval_frac = 0.12\nn_val_games = max(1, int(len(game_ids) * val_frac))\nval_games = set(game_ids[:n_val_games])\ntrain_examples = [e for e in examples if e[\"game_id\"] not in val_games]\nval_examples = [e for e in examples if e[\"game_id\"] in val_games]\nprint(\"Train examples:\", len(train_examples), \"Val examples:\", len(val_examples))\n\ntrain_ds = TrajectoryDataset(train_examples, scaler)\nval_ds = TrajectoryDataset(val_examples, scaler)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n\n# -----------------------\n# Model: encoder LSTM -> linear outputs for entire output sequence\n# -----------------------\nclass TrajModel(nn.Module):\n    def __init__(self, feat_dim, hidden=HIDDEN, n_layers=NLAYERS, bidir=BIDIR, dropout=DROPOUT, max_out=MAX_OUT_FRAMES):\n        super().__init__()\n        self.hidden = hidden\n        self.n_layers = n_layers\n        self.bidir = bidir\n        self.max_out = max_out\n        self.lstm = nn.LSTM(input_size=feat_dim, hidden_size=hidden, num_layers=n_layers,\n                            batch_first=True, dropout=dropout, bidirectional=bidir)\n        mult = 2 if bidir else 1\n        # Map final hidden (concat of directions and layers) to a bottleneck then to outputs\n        self.fc1 = nn.Linear(hidden * mult, hidden)\n        self.act = nn.ReLU()\n        # output raw displacements for each frame: max_out * 2\n        self.fc_out = nn.Linear(hidden, max_out * 2)\n    def forward(self, x):\n        # x: (B, K_in, feat_dim)\n        out, (hn, cn) = self.lstm(x)  # hn: (num_layers * num_directions, B, hidden)\n        # take last layer's hidden for each direction\n        # reshape hn to (num_layers, num_directions, B, hidden)\n        nl = self.n_layers\n        nd = 2 if self.bidir else 1\n        hn = hn.view(nl, nd, x.size(0), self.hidden)\n        # take last layer\n        last = hn[-1]  # (nd, B, hidden)\n        if nd == 2:\n            # concat forward and backward\n            last = torch.cat([last[0], last[1]], dim=1)  # (B, hidden*2)\n        else:\n            last = last[0]  # (B, hidden)\n        h = self.act(self.fc1(last))\n        out = self.fc_out(h)  # (B, max_out*2)\n        out = out.view(x.size(0), self.max_out, 2)\n        return out\n\n# -----------------------\n# Training utilities\n# -----------------------\nfeat_dim = train_ds[0][\"feats\"].shape[1]\nmodel = TrajModel(feat_dim, hidden=HIDDEN, n_layers=NLAYERS, bidir=BIDIR, max_out=MAX_OUT_FRAMES).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\ncriterion = nn.MSELoss(reduction='none')  # we'll mask\n\ndef masked_rmse(pred, target, mask):\n    # pred, target: (B, T, 2), mask: (B, T)\n    mse = (pred - target).pow(2).mean(dim=2)  # (B, T)\n    masked = mse * mask\n    summed = masked.sum()\n    denom = mask.sum().clamp_min(1.0)\n    return torch.sqrt(summed / denom)\n\n# -----------------------\n# Train loop\n# -----------------------\nbest_val = 1e9\npatience = PATIENCE\nbest_epoch = -1\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    t0 = time.time()\n    train_loss = 0.0\n    n_batches = 0\n    for batch in train_loader:\n        feats = batch[\"feats\"].to(DEVICE)   # (B, K_in, feat_dim)\n        targets = batch[\"targets\"].to(DEVICE)  # (B, max_out, 2)\n        masks = batch[\"masks\"].to(DEVICE)    # (B, max_out)\n        preds = model(feats)                 # (B, max_out, 2)\n        loss_all = criterion(preds, targets).mean(dim=2)  # (B, T)\n        loss_masked = (loss_all * masks).sum() / masks.sum().clamp_min(1.0)\n        optimizer.zero_grad()\n        loss_masked.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        train_loss += loss_masked.item()\n        n_batches += 1\n    train_loss /= max(1, n_batches)\n\n    # validation\n    model.eval()\n    val_loss = 0.0\n    n_batches = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            feats = batch[\"feats\"].to(DEVICE)\n            targets = batch[\"targets\"].to(DEVICE)\n            masks = batch[\"masks\"].to(DEVICE)\n            preds = model(feats)\n            loss_all = criterion(preds, targets).mean(dim=2)\n            loss_masked = (loss_all * masks).sum() / masks.sum().clamp_min(1.0)\n            val_loss += loss_masked.item()\n            n_batches += 1\n    val_loss /= max(1, n_batches)\n    scheduler.step(val_loss)\n    t1 = time.time()\n    print(f\"Epoch {epoch} train_loss={train_loss:.5f} val_loss={val_loss:.5f} time={(t1-t0):.1f}s\")\n    # track best\n    if val_loss < best_val:\n        best_val = val_loss\n        best_epoch = epoch\n        torch.save({\"model_state\": model.state_dict(), \"scaler\": scaler, \"le_role\": le_role, \"le_pos\": le_pos},\n                   \"best_model.pth\")\n        print(\"Saved best_model.pth\")\n        patience = PATIENCE  # reset\n    else:\n        patience -= 1\n        if patience <= 0:\n            print(\"Early stopping.\")\n            break\n\n# -----------------------\n# Inference on test set\n# -----------------------\nprint(\"Running inference on test set...\")\n# load test input and test.csv (list of ids to predict)\ntest_input = pd.read_csv(TEST_INPUT_PATH)\ntest_df = pd.read_csv(TEST_CSV_PATH)\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\n# add velocity and encodings to test_input\ntest_input = add_velocity(test_input)\n# map encodings\ntest_input[\"player_role_f\"] = le_role.transform(test_input[\"player_role\"].astype(str))\ntest_input[\"player_pos_f\"] = le_pos.transform(test_input[\"player_position\"].astype(str))\ntest_input[\"player_side_f\"] = test_input[\"player_side\"].map(side_map).fillna(0).astype(int)\n\n# Build last K_in state per (game,play,nfl)\nlast_state = test_input.groupby([\"game_id\",\"play_id\",\"nfl_id\"]).apply(lambda g: g.sort_values(\"frame_id\").tail(K_IN)).reset_index(drop=True)\n\n# Prepare a dict for quick lookups of model predictions\n# We'll create batches from last_state, run model, then map outputs to required ids\nmodel = model.to(DEVICE)\nmodel.eval()\n\n# Create features same as dataset\ndef build_feats_from_rows(df_rows):\n    # df_rows: K_in rows (or less) sorted in ascending frame order\n    df = df_rows.copy()\n    ball_x = df[\"ball_land_x\"].iloc[-1] if \"ball_land_x\" in df.columns else 0.0\n    ball_y = df[\"ball_land_y\"].iloc[-1] if \"ball_land_y\" in df.columns else 0.0\n    num = df[numeric_cols].values.astype(float)\n    num = scaler.transform(num)\n    rel = np.column_stack([ (df[\"x\"].values - ball_x), (df[\"y\"].values - ball_y) ])\n    role = df[\"player_role_f\"].values.reshape(-1,1)\n    pos = df[\"player_pos_f\"].values.reshape(-1,1)\n    side = df[\"player_side_f\"].values.reshape(-1,1)\n    feats = np.concatenate([num, rel, role, pos, side], axis=1)\n    if feats.shape[0] < K_IN:\n        pad_n = K_IN - feats.shape[0]\n        pad_rows = np.repeat(feats[[0], :], pad_n, axis=0)\n        feats = np.vstack([pad_rows, feats])\n    else:\n        feats = feats[-K_IN:,:]\n    return feats.astype(np.float32)\n\n# build list of group keys and their feature arrays\ngrouped = last_state.groupby([\"game_id\",\"play_id\",\"nfl_id\"])\nkeys = []\nfeat_list = []\nmeta_info = []\nfor key, grp in grouped:\n    keys.append(key)\n    grp_sorted = grp.sort_values(\"frame_id\")\n    feat_list.append(build_feats_from_rows(grp_sorted))\n    # store last_x,last_y and last_frame and num_out if present in df_rows\n    last_row = grp_sorted.iloc[-1]\n    last_x = last_row[\"x\"]; last_y = last_row[\"y\"]\n    last_frame = int(last_row[\"frame_id\"])\n    num_out = int(last_row.get(\"num_frames_output\", MAX_OUT_FRAMES))\n    meta_info.append((last_x, last_y, last_frame, num_out))\n\n# batch prediction\nB = 2048\npred_dict = {}  # (game,play,nfl) -> predicted absolute positions array (T_out, 2)\nwith torch.no_grad():\n    for i in range(0, len(feat_list), B):\n        batch_feats = np.stack(feat_list[i:i+B], axis=0)\n        batch_feats_t = torch.from_numpy(batch_feats).to(DEVICE)\n        batch_preds = model(batch_feats_t).cpu().numpy()  # (b, max_out, 2) predicted displacements\n        for j, key in enumerate(keys[i:i+B]):\n            last_x, last_y, last_frame, num_out = meta_info[i+j]\n            disp = batch_preds[j]\n            take = min(int(num_out), disp.shape[0])\n            abs_pos = disp[:take,:] + np.array([[last_x, last_y]])\n            pred_dict[key] = (last_frame, abs_pos)\n\n# Build submission rows by reading test.csv rows and mapping\nrows = []\nfor row in test_df.itertuples(index=False):\n    gid, pid, nid, fid = row.game_id, row.play_id, row.nfl_id, row.frame_id\n    key = (gid, pid, nid)\n    if key not in pred_dict:\n        # fallback to last known pos\n        # try to find last known in test_input\n        grp = test_input[(test_input.game_id==gid)&(test_input.play_id==pid)&(test_input.nfl_id==nid)]\n        if len(grp)==0:\n            x_pred, y_pred = 60.0, 26.65\n        else:\n            last = grp.sort_values(\"frame_id\").iloc[-1]\n            # simple extrapolation fallback\n            dt = fid - int(last[\"frame_id\"])\n            x_pred = last[\"x\"] + last[\"vx\"] * dt\n            y_pred = last[\"y\"] + last[\"vy\"] * dt\n            x_pred = float(np.clip(x_pred, 0, 120))\n            y_pred = float(np.clip(y_pred, 0, 53.3))\n    else:\n        last_frame, abs_pos = pred_dict[key]\n        idx = int(fid - last_frame - 1)  # frame indices in output start at 1 after last input frame; adjust as needed\n        if idx < 0:\n            # requested frame is before predicted outputs -> fallback to last known\n            grp = test_input[(test_input.game_id==gid)&(test_input.play_id==pid)&(test_input.nfl_id==nid)]\n            last = grp.sort_values(\"frame_id\").iloc[-1]\n            x_pred = last[\"x\"]; y_pred = last[\"y\"]\n        elif idx >= abs_pos.shape[0]:\n            # beyond predicted length: use last predicted\n            p = abs_pos[-1]\n            x_pred, y_pred = float(p[0]), float(p[1])\n        else:\n            p = abs_pos[idx]\n            x_pred, y_pred = float(p[0]), float(p[1])\n    rows.append((f\"{gid}_{pid}_{nid}_{fid}\", x_pred, y_pred))\n\nsub_df = pd.DataFrame(rows, columns=[\"id\",\"x\",\"y\"])\n# ensure same order as sample_sub\nsubmission = sample_sub.drop(columns=[\"x\",\"y\"]).merge(sub_df, on=\"id\", how=\"left\")\n# final clipping\nsubmission[\"x\"] = submission[\"x\"].clip(0,120)\nsubmission[\"y\"] = submission[\"y\"].clip(0,53.3)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv with shape\", submission.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:51:10.796457Z","iopub.execute_input":"2025-09-25T16:51:10.796809Z"}},"outputs":[],"execution_count":null}]}