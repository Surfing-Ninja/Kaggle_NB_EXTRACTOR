{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom multiprocessing import Pool as MultiprocessingPool, cpu_count\nfrom tqdm.auto import tqdm\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.336593Z","iopub.execute_input":"2025-10-05T09:40:34.336876Z","iopub.status.idle":"2025-10-05T09:40:34.341621Z","shell.execute_reply.started":"2025-10-05T09:40:34.336855Z","shell.execute_reply":"2025-10-05T09:40:34.340906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nbasedir = '/kaggle/input/nfl-big-data-bowl-2026-prediction'\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.342652Z","iopub.execute_input":"2025-10-05T09:40:34.342877Z","iopub.status.idle":"2025-10-05T09:40:34.35704Z","shell.execute_reply.started":"2025-10-05T09:40:34.342862Z","shell.execute_reply":"2025-10-05T09:40:34.356206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_weekly_data(week_num):\n    input_df = pd.read_csv(f'{basedir}/train/input_2023_w{week_num:02d}.csv')\n    output_df = pd.read_csv(f'{basedir}/train/output_2023_w{week_num:02d}.csv')\n    return input_df, output_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.357893Z","iopub.execute_input":"2025-10-05T09:40:34.358137Z","iopub.status.idle":"2025-10-05T09:40:34.37112Z","shell.execute_reply.started":"2025-10-05T09:40:34.358115Z","shell.execute_reply":"2025-10-05T09:40:34.370602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_all_train_data():\n    print(\"Loading training data...\")\n    with MultiprocessingPool(min(cpu_count(), 18)) as pool:\n        results = list(tqdm(pool.imap(load_weekly_data, range(1, 19)), total=18))\n    \n    input_dfs = [r[0] for r in results]\n    output_dfs = [r[1] for r in results]\n    \n    input_data = pd.concat(input_dfs, ignore_index=True)\n    output_data = pd.concat(output_dfs, ignore_index=True)\n    \n    print(f\"Input data shape: {input_data.shape}\")\n    print(f\"Output data shape: {output_data.shape}\")\n    \n    return input_data, output_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.372456Z","iopub.execute_input":"2025-10-05T09:40:34.372679Z","iopub.status.idle":"2025-10-05T09:40:34.385752Z","shell.execute_reply.started":"2025-10-05T09:40:34.372664Z","shell.execute_reply":"2025-10-05T09:40:34.385161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def engineer_advanced_features(df):\n    \"\"\"Advanced feature engineering with sequence and interaction features\"\"\"\n    df = df.copy()\n    \n    df['velocity_x'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['velocity_y'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    df['dist_to_ball'] = np.sqrt(\n        (df['x'] - df['ball_land_x'])**2 + \n        (df['y'] - df['ball_land_y'])**2\n    )\n    \n    df['angle_to_ball'] = np.arctan2(\n        df['ball_land_y'] - df['y'],\n        df['ball_land_x'] - df['x']\n    )\n    \n    df['velocity_toward_ball'] = (\n        df['velocity_x'] * np.cos(df['angle_to_ball']) + \n        df['velocity_y'] * np.sin(df['angle_to_ball'])\n    )\n    \n    df['time_to_ball'] = df['num_frames_output'] / 10.0\n    \n    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n    \n    df['role_targeted_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    df['role_defensive_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['role_passer'] = (df['player_role'] == 'Passer').astype(int)\n    df['side_offense'] = (df['player_side'] == 'Offense').astype(int)\n    \n    height_parts = df['player_height'].str.split('-', expand=True)\n    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n    \n    df['acceleration_x'] = df['a'] * np.cos(np.radians(df['dir']))\n    df['acceleration_y'] = df['a'] * np.sin(np.radians(df['dir']))\n    \n    df['distance_to_target_x'] = df['ball_land_x'] - df['x']\n    df['distance_to_target_y'] = df['ball_land_y'] - df['y']\n    \n    df['speed_squared'] = df['s'] ** 2\n    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n    \n    df['velocity_alignment'] = np.cos(df['angle_to_ball'] - np.radians(df['dir']))\n    \n    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['time_to_ball']\n    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['time_to_ball']\n    \n    df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n    df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n    df['error_from_ball'] = np.sqrt(df['error_from_ball_x']**2 + df['error_from_ball_y']**2)\n    \n    df['momentum_x'] = df['player_weight'] * df['velocity_x']\n    df['momentum_y'] = df['player_weight'] * df['velocity_y']\n    \n    df['kinetic_energy'] = 0.5 * df['player_weight'] * df['speed_squared']\n    \n    df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n    df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n    \n    df['time_squared'] = df['time_to_ball'] ** 2\n    df['dist_squared'] = df['dist_to_ball'] ** 2\n    \n    df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['time_to_ball'] + 0.1)\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.386584Z","iopub.execute_input":"2025-10-05T09:40:34.386788Z","iopub.status.idle":"2025-10-05T09:40:34.405206Z","shell.execute_reply.started":"2025-10-05T09:40:34.386774Z","shell.execute_reply":"2025-10-05T09:40:34.404604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sequence_features(df):\n    \"\"\"Add temporal lag and rolling features\"\"\"\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            if col in df.columns:\n                df[f'{col}_rolling_mean_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n                df[f'{col}_rolling_std_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n    \n    for col in ['velocity_x', 'velocity_y']:\n        if col in df.columns:\n            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff()\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.405821Z","iopub.execute_input":"2025-10-05T09:40:34.406025Z","iopub.status.idle":"2025-10-05T09:40:34.421745Z","shell.execute_reply.started":"2025-10-05T09:40:34.406009Z","shell.execute_reply":"2025-10-05T09:40:34.421211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_training_dataset(input_df, output_df):\n    output_df = output_df.copy()\n    output_df['id'] = (output_df['game_id'].astype(str) + '_' + \n                       output_df['play_id'].astype(str) + '_' + \n                       output_df['nfl_id'].astype(str) + '_' + \n                       output_df['frame_id'].astype(str))\n    \n    output_df = output_df.rename(columns={'x': 'target_x', 'y': 'target_y'})\n    \n    input_agg = input_df.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    \n    if 'frame_id' in input_agg.columns:\n        input_agg = input_agg.drop('frame_id', axis=1)\n    \n    merged = output_df.merge(\n        input_agg,\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left',\n        suffixes=('', '_input')\n    )\n    \n    return merged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.422403Z","iopub.execute_input":"2025-10-05T09:40:34.422648Z","iopub.status.idle":"2025-10-05T09:40:34.439037Z","shell.execute_reply.started":"2025-10-05T09:40:34.422632Z","shell.execute_reply":"2025-10-05T09:40:34.438513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def physics_baseline_prediction(x, y, velocity_x, velocity_y, frame_id):\n    time_delta = frame_id / 10.0\n    pred_x = x + velocity_x * time_delta\n    pred_y = y + velocity_y * time_delta\n    pred_x = np.clip(pred_x, 0, 120)\n    pred_y = np.clip(pred_y, 0, 53.3)\n    return pred_x, pred_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.439676Z","iopub.execute_input":"2025-10-05T09:40:34.440096Z","iopub.status.idle":"2025-10-05T09:40:34.451783Z","shell.execute_reply.started":"2025-10-05T09:40:34.440078Z","shell.execute_reply":"2025-10-05T09:40:34.451112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_deep_model(input_dim, output_dim=1):\n    \"\"\"Create a deep neural network for regression\"\"\"\n    model = models.Sequential([\n        layers.Input(shape=(input_dim,)),\n        \n        # First block\n        layers.Dense(512, kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(0.3),\n        \n        # Second block\n        layers.Dense(256, kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(0.3),\n        \n        # Third block\n        layers.Dense(128, kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(0.2),\n        \n        # Fourth block\n        layers.Dense(64, kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(0.2),\n        \n        # Output layer\n        layers.Dense(output_dim, activation='linear')\n    ])\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.452363Z","iopub.execute_input":"2025-10-05T09:40:34.45257Z","iopub.status.idle":"2025-10-05T09:40:34.469517Z","shell.execute_reply.started":"2025-10-05T09:40:34.452555Z","shell.execute_reply":"2025-10-05T09:40:34.468778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef main():\n    print(f\"CPU cores: {cpu_count()}\")\n    print(f\"TensorFlow version: {tf.__version__}\")\n    print(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")\n    \n    input_data, output_data = load_all_train_data()\n    \n    print(\"\\n=== Advanced Feature Engineering ===\")\n    print(\"Step 1: Engineering advanced physics features...\")\n    input_features = engineer_advanced_features(input_data)\n    \n    print(\"Step 2: Adding sequence and rolling features...\")\n    input_features = add_sequence_features(input_features)\n    \n    print(f\"Feature engineered data shape: {input_features.shape}\")\n    print(f\"Total features: {input_features.shape[1]}\")\n    \n    print(\"\\nStep 3: Creating training dataset...\")\n    train_df = create_training_dataset(input_features, output_data)\n    print(f\"Training dataset shape: {train_df.shape}\")\n    \n    feature_cols = [\n        'x', 'y', 's', 'a', 'o', 'dir',\n        'velocity_x', 'velocity_y', 'dist_to_ball', 'angle_to_ball',\n        'velocity_toward_ball', 'time_to_ball', 'orientation_diff',\n        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer',\n        'side_offense', 'height_inches', 'player_weight', 'bmi',\n        'ball_land_x', 'ball_land_y', 'num_frames_output', 'frame_id',\n        'acceleration_x', 'acceleration_y', 'distance_to_target_x', 'distance_to_target_y',\n        'speed_squared', 'accel_magnitude', 'velocity_alignment',\n        'expected_x_at_ball', 'expected_y_at_ball',\n        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'angle_diff', 'time_squared', 'dist_squared', 'weighted_dist_by_time'\n    ]\n    \n    for lag in [1, 2, 3, 4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n            feature_cols.append(f'{col}_lag{lag}')\n    \n    for window in [3, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n            feature_cols.append(f'{col}_rolling_mean_{window}')\n            feature_cols.append(f'{col}_rolling_std_{window}')\n    \n    feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n    \n    available_features = [col for col in feature_cols if col in train_df.columns]\n    print(f\"Available features: {len(available_features)}\")\n    \n    train_df = train_df.dropna(subset=available_features + ['target_x', 'target_y'])\n    print(f\"Training data after removing NaNs: {train_df.shape}\")\n    \n    print(\"\\n=== Physics Baseline ===\")\n    baseline_x, baseline_y = physics_baseline_prediction(\n        train_df['x'].values,\n        train_df['y'].values,\n        train_df['velocity_x'].values,\n        train_df['velocity_y'].values,\n        train_df['frame_id'].values\n    )\n    \n    baseline_rmse = np.sqrt(\n        0.5 * (mean_squared_error(train_df['target_x'], baseline_x) +\n               mean_squared_error(train_df['target_y'], baseline_y))\n    )\n    print(f\"Physics Baseline RMSE: {baseline_rmse:.4f}\")\n    \n    # Prepare data\n    X = train_df[available_features].values\n    y_x = train_df['target_x'].values\n    y_y = train_df['target_y'].values\n    \n    # Initialize scalers\n    scaler_X = StandardScaler()\n    scaler_y_x = StandardScaler()\n    scaler_y_y = StandardScaler()\n    \n    # Initialize 5-fold cross-validation\n    n_folds = 5\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    models_x = []\n    models_y = []\n    scalers_X = []\n    scalers_y_x = []\n    scalers_y_y = []\n    val_rmse_scores = []\n    \n    print(f\"\\n=== Training Deep Neural Networks with 5-Fold CV ===\")\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"Fold {fold}/{n_folds}\")\n        print(f\"{'='*60}\")\n        \n        # Split data\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_x_train, y_x_val = y_x[train_idx], y_x[val_idx]\n        y_y_train, y_y_val = y_y[train_idx], y_y[val_idx]\n        \n        print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n        \n        # Scale features\n        scaler_X_fold = StandardScaler()\n        X_train_scaled = scaler_X_fold.fit_transform(X_train)\n        X_val_scaled = scaler_X_fold.transform(X_val)\n        \n        # Scale targets\n        scaler_y_x_fold = StandardScaler()\n        scaler_y_y_fold = StandardScaler()\n        \n        y_x_train_scaled = scaler_y_x_fold.fit_transform(y_x_train.reshape(-1, 1)).ravel()\n        y_y_train_scaled = scaler_y_y_fold.fit_transform(y_y_train.reshape(-1, 1)).ravel()\n        \n        # Store scalers\n        scalers_X.append(scaler_X_fold)\n        scalers_y_x.append(scaler_y_x_fold)\n        scalers_y_y.append(scaler_y_y_fold)\n        \n        # Create callbacks\n        early_stop = callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=30,\n            restore_best_weights=True,\n            verbose=1\n        )\n        \n        reduce_lr = callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=10,\n            min_lr=1e-7,\n            verbose=1\n        )\n        \n        # Train X-coordinate model\n        print(f\"\\nTraining X coordinate model for fold {fold}...\")\n        model_x = create_deep_model(X_train_scaled.shape[1])\n        \n        model_x.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n            loss='mse',\n            metrics=['mae']\n        )\n        \n        history_x = model_x.fit(\n            X_train_scaled, y_x_train_scaled,\n            validation_data=(X_val_scaled, scaler_y_x_fold.transform(y_x_val.reshape(-1, 1)).ravel()),\n            epochs=500,\n            batch_size=1024,\n            callbacks=[early_stop, reduce_lr],\n            verbose=1\n        )\n        \n        models_x.append(model_x)\n        \n        # Train Y-coordinate model\n        print(f\"\\nTraining Y coordinate model for fold {fold}...\")\n        model_y = create_deep_model(X_train_scaled.shape[1])\n        \n        model_y.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n            loss='mse',\n            metrics=['mae']\n        )\n        \n        history_y = model_y.fit(\n            X_train_scaled, y_y_train_scaled,\n            validation_data=(X_val_scaled, scaler_y_y_fold.transform(y_y_val.reshape(-1, 1)).ravel()),\n            epochs=500,\n            batch_size=1024,\n            callbacks=[early_stop, reduce_lr],\n            verbose=1\n        )\n        \n        models_y.append(model_y)\n        \n        # Validation predictions\n        pred_x_scaled = model_x.predict(X_val_scaled, verbose=0)\n        pred_y_scaled = model_y.predict(X_val_scaled, verbose=0)\n        \n        # Inverse transform\n        pred_x = scaler_y_x_fold.inverse_transform(pred_x_scaled).ravel()\n        pred_y = scaler_y_y_fold.inverse_transform(pred_y_scaled).ravel()\n        \n        pred_x = np.clip(pred_x, 0, 120)\n        pred_y = np.clip(pred_y, 0, 53.3)\n        \n        # Compute RMSE for this fold\n        fold_rmse = np.sqrt(\n            0.5 * (mean_squared_error(y_x_val, pred_x) +\n                   mean_squared_error(y_y_val, pred_y))\n        )\n        val_rmse_scores.append(fold_rmse)\n        print(f\"\\nFold {fold} RMSE: {fold_rmse:.4f}\")\n    \n    # Average RMSE across folds\n    nn_rmse = np.mean(val_rmse_scores)\n    print(f\"\\n{'='*60}\")\n    print(f\"DEEP NEURAL NETWORK PERFORMANCE (5-FOLD CV)\")\n    print(f\"{'='*60}\")\n    print(f\"Physics Baseline RMSE:     {baseline_rmse:.4f}\")\n    print(f\"Deep Neural Network:       {nn_rmse:.4f}\")\n    print(f\"Improvement:               {((baseline_rmse - nn_rmse) / baseline_rmse * 100):.2f}%\")\n    print(f\"Standard Deviation:        {np.std(val_rmse_scores):.4f}\")\n    print(f\"Target RMSE:               0.9000\")\n    target_met = 'YES - TARGET ACHIEVED!' if nn_rmse < 0.9 else 'NO - Continuing optimization...'\n    print(f\"Target Met:                {target_met}\")\n    print(f\"{'='*60}\")\n    \n    # Save models and scalers\n    print(\"\\nSaving models...\")\n    for i, (mx, my) in enumerate(zip(models_x, models_y)):\n        mx.save(f'model_x_fold{i+1}.keras')\n        my.save(f'model_y_fold{i+1}.keras')\n    \n    with open('scalers_5fold.pkl', 'wb') as f:\n        pickle.dump({\n            'scalers_X': scalers_X,\n            'scalers_y_x': scalers_y_x,\n            'scalers_y_y': scalers_y_y,\n            'features': available_features,\n            'rmse': nn_rmse\n        }, f)\n    print(\"Models and scalers saved successfully\")\n    \n    print(\"\\n=== Generating Submission ===\")\n    test_input = pd.read_csv(f'{basedir}/test_input.csv')\n    test_data = pd.read_csv(f'{basedir}/test.csv')\n    \n    print(\"Engineering features for test data...\")\n    test_features = engineer_advanced_features(test_input)\n    test_features = add_sequence_features(test_features)\n    \n    test_agg = test_features.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n    \n    if 'frame_id' in test_agg.columns:\n        test_agg = test_agg.drop('frame_id', axis=1)\n    \n    test_merged = test_data.merge(\n        test_agg,\n        on=['game_id', 'play_id', 'nfl_id'],\n        how='left'\n    )\n    \n    test_merged['id'] = (test_merged['game_id'].astype(str) + '_' + \n                         test_merged['play_id'].astype(str) + '_' + \n                         test_merged['nfl_id'].astype(str) + '_' + \n                         test_merged['frame_id'].astype(str))\n    \n    for col in available_features:\n        if col not in test_merged.columns:\n            test_merged[col] = 0\n    \n    X_test = test_merged[available_features].fillna(0).values\n    \n    # Ensemble predictions across all folds\n    pred_x_list = []\n    pred_y_list = []\n    \n    for i in range(n_folds):\n        X_test_scaled = scalers_X[i].transform(X_test)\n        \n        pred_x_scaled = models_x[i].predict(X_test_scaled, verbose=0)\n        pred_y_scaled = models_y[i].predict(X_test_scaled, verbose=0)\n        \n        pred_x = scalers_y_x[i].inverse_transform(pred_x_scaled).ravel()\n        pred_y = scalers_y_y[i].inverse_transform(pred_y_scaled).ravel()\n        \n        pred_x_list.append(pred_x)\n        pred_y_list.append(pred_y)\n    \n    # Average predictions\n    pred_x_test = np.mean(pred_x_list, axis=0)\n    pred_y_test = np.mean(pred_y_list, axis=0)\n    \n    pred_x_test = np.clip(pred_x_test, 0, 120)\n    pred_y_test = np.clip(pred_y_test, 0, 53.3)\n    \n    submission = pd.DataFrame({\n        'id': test_merged['id'],\n        'x': pred_x_test,\n        'y': pred_y_test\n    })\n    \n    submission.to_csv('submission.csv', index=False)\n    print(f\"\\n[SUCCESS] Submission saved: submission.csv\")\n    print(f\"Shape: {submission.shape}\")\n    \n    print(\"\\n=== Submission Validation ===\")\n    print(f\"No NaN values: {submission.isnull().sum().sum() == 0}\")\n    print(f\"X range: [{submission['x'].min():.2f}, {submission['x'].max():.2f}]\")\n    print(f\"Y range: [{submission['y'].min():.2f}, {submission['y'].max():.2f}]\")\n    print(f\"Unique IDs: {submission['id'].nunique()}\")\n    \n    return nn_rmse\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.471332Z","iopub.execute_input":"2025-10-05T09:40:34.471553Z","iopub.status.idle":"2025-10-05T09:40:34.495673Z","shell.execute_reply.started":"2025-10-05T09:40:34.471538Z","shell.execute_reply":"2025-10-05T09:40:34.494936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    final_rmse = main()\n    print(f\"\\n[FINAL] Validation RMSE: {final_rmse:.4f}\")\n    achievement = 'ACHIEVED!' if final_rmse < 0.9 else 'Not yet - need further optimization'\n    print(f\"[FINAL] Target RMSE < 0.9: {achievement}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:40:34.496453Z","iopub.execute_input":"2025-10-05T09:40:34.496671Z","iopub.status.idle":"2025-10-05T10:24:22.369935Z","shell.execute_reply.started":"2025-10-05T09:40:34.496653Z","shell.execute_reply":"2025-10-05T10:24:22.368958Z"}},"outputs":[],"execution_count":null}]}