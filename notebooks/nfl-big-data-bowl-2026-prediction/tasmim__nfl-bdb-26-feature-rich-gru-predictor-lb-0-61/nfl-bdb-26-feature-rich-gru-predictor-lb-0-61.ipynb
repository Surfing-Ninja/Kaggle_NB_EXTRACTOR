{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nCOMPLETE STEP 2: Better Features Pipeline\nFull pipeline with advanced feature engineering\n\nExpected: 0.62 → 0.59-0.60 RMSE\nTime: ~4-5 hours (training)\n\nWHAT THIS DOES:\n1. Load and prepare data\n2. Add ADVANCED FEATURES (30-40 new features)\n   - Distance rate features\n   - Target alignment features\n   - Multi-window rolling features\n   - Extended lag features\n   - Velocity change features\n   - Field position features\n   - Role-specific features\n   - Time-based features\n3. Train NN models with enhanced features\n4. Create submission\n\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nimport warnings\nimport os\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom torch.utils.data import TensorDataset, DataLoader\n\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIG\n# ============================================================================\n\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    OUTPUT_DIR = Path(\"./outputs\")\n    OUTPUT_DIR.mkdir(exist_ok=True)\n    \n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 512\n    EPOCHS = 200\n    PATIENCE = 20\n    LEARNING_RATE = 5e-4\n    \n    WINDOW_SIZE = 18\n    HIDDEN_DIM = 192\n    MAX_FUTURE_HORIZON = 94\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(Config.SEED)\n\n# ============================================================================\n# ENHANCED FEATURE ENGINEERING\n# ============================================================================\n\ndef height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n\ndef add_advanced_features(df):\n    \"\"\"\n    STEP 2: Add 30-40 advanced features\n    These are proven to improve performance\n    \"\"\"\n    print(\"Adding advanced features...\")\n    df = df.copy()\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # ==========================================\n    # GROUP 1: Distance Rate Features (3)\n    # ==========================================\n    if 'distance_to_ball' in df.columns:\n        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n        df['time_to_intercept'] = (df['distance_to_ball'] / \n                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n    \n    # ==========================================\n    # GROUP 2: Target Alignment Features (3)\n    # ==========================================\n    if 'ball_direction_x' in df.columns:\n        df['velocity_alignment'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_perpendicular'] = (\n            df['velocity_x'] * (-df['ball_direction_y']) +\n            df['velocity_y'] * df['ball_direction_x']\n        )\n        if 'acceleration_x' in df.columns:\n            df['accel_alignment'] = (\n                df['acceleration_x'] * df['ball_direction_x'] +\n                df['acceleration_y'] * df['ball_direction_y']\n            )\n    \n    # ==========================================\n    # GROUP 3: Multi-Window Rolling (24)\n    # ==========================================\n    for window in [3, 5, 10]:\n        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).mean()\n                )\n                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).std()\n                ).fillna(0)\n    \n    # ==========================================\n    # GROUP 4: Extended Lag Features (8)\n    # ==========================================\n    for lag in [4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n    \n    # ==========================================\n    # GROUP 5: Velocity Change Features (4)\n    # ==========================================\n    if 'velocity_x' in df.columns:\n        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n        # df['direction_change'] = df['direction_change'].apply(\n        #     lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        # )\n        dir_diff = df.groupby(gcols)['dir'].diff().fillna(0)\n        df['direction_change'] = (((dir_diff + 180) % 360) - 180)\n    \n    # ==========================================\n    # GROUP 6: Field Position Features (4)\n    # ==========================================\n    df['dist_from_left'] = df['y']\n    df['dist_from_right'] = 53.3 - df['y']\n    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n    \n    # ==========================================\n    # GROUP 7: Role-Specific Features (3)\n    # ==========================================\n    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n    \n    # ==========================================\n    # GROUP 8: Time Features (2)\n    # ==========================================\n    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n        lambda x: x / (x.max() + 1)\n    )\n    \n    print(f\"Total features after enhancement: {len(df.columns)}\")\n    \n    return df\n\ndef prepare_sequences_with_advanced_features(input_df, output_df=None, test_template=None, \n                                            is_training=True, window_size=8):\n    \"\"\"\n    Prepare sequences with ALL advanced features\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES\")\n    print(f\"{'='*80}\")\n    print(f\"Window size: {window_size}\")\n    \n    input_df = input_df.copy()\n    \n    # ==========================================\n    # BASIC FEATURES\n    # ==========================================\n    print(\"Step 1/3: Adding basic features...\")\n    \n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n    \n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    delta_t = 0.1\n    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n    \n    # Roles\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    \n    # Physics\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n    \n    # Ball features\n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n    \n    # Sort for temporal\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # Original lag features (1-3)\n    for lag in [1, 2, 3]:\n        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n    \n    # EMA features\n    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    \n    # ==========================================\n    # ADVANCED FEATURES (NEW!)\n    # ==========================================\n    print(\"Step 2/3: Adding advanced features...\")\n    input_df = add_advanced_features(input_df)\n    \n    # ==========================================\n    # FEATURE LIST (ENHANCED)\n    # ==========================================\n    print(\"Step 3/3: Creating sequences...\")\n    \n    feature_cols = [\n        # Core (9)\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n        \n        # Player (2)\n        'player_height_feet', 'player_weight',\n        \n        # Motion (6)\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        \n        # Roles (5)\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        \n        # Ball (5)\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n        \n        # Original temporal (15)\n        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n        \n        # NEW: Distance rate (3)\n        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n        \n        # NEW: Target alignment (3)\n        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n        \n        # NEW: Multi-window rolling (24)\n        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n        \n        # NEW: Extended lags (8)\n        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n        \n        # NEW: Velocity changes (4)\n        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n        \n        # NEW: Field position (4)\n        'dist_from_sideline', 'dist_from_endzone',\n        \n        # NEW: Role-specific (3)\n        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n        \n        # NEW: Time (2)\n        'frames_elapsed', 'normalized_time',\n    ]\n    \n    # Filter to existing\n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"Using {len(feature_cols)} features (was ~50, now ~90)\")\n    \n    # ==========================================\n    # CREATE SEQUENCES\n    # ==========================================\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    \n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n    \n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n    \n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        \n        input_window = group_df.tail(window_size)\n        \n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        \n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        seq = input_window[feature_cols].values\n        \n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n        \n        sequences.append(seq)\n        \n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            \n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n            \n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            \n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n        \n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id']\n        })\n    \n    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n    \n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n    return sequences, sequence_ids\n\n# ============================================================================\n# MODEL & LOSS (Same as before)\n# ============================================================================\n\nclass TemporalHuber(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.03):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n    \n    def forward(self, pred, target, mask):\n        err = pred - target\n        abs_err = torch.abs(err)\n        huber = torch.where(abs_err <= self.delta, 0.5 * err * err, \n                           self.delta * (abs_err - 0.5 * self.delta))\n        \n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            weight = torch.exp(-self.time_decay * t).view(1, L)\n            huber, mask = huber * weight, mask * weight\n        \n        return (huber * mask).sum() / (mask.sum() + 1e-8)\n\nclass SeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, 192, num_layers=3, batch_first=True, dropout=0.2)\n        self.conv1d = nn.Sequential(\n            nn.Conv1d(192, 128, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv1d(128, 128, kernel_size=5, padding=2),\n            nn.GELU(),\n        )\n        self.pool_ln = nn.LayerNorm(192)\n        self.pool_attn = nn.MultiheadAttention(192, num_heads=16, batch_first=True)\n        self.pool_query = nn.Parameter(torch.randn(1, 1, 192))\n        self.head = nn.Sequential(\n            nn.Linear(192 + 128, 256),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, horizon * 2)\n        )\n    \n    def forward(self, x):\n        h, _ = self.gru(x)\n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)\n        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n        out = self.head(ctx.squeeze(1))\n        return torch.cumsum(out, dim=1)\n\n# ============================================================================\n# TRAINING\n# ============================================================================\n\ndef prepare_targets(batch_axis, max_h):\n    tensors, masks = [], []\n    for arr in batch_axis:\n        L = len(arr)\n        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors.append(torch.tensor(padded))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors), torch.stack(masks)\n\ndef train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config):\n    device = config.DEVICE\n    model = SeqModel(input_dim, horizon).to(device)\n    \n    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=False)\n    \n    # Batches\n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        by, bm = prepare_targets([y_train[j] for j in range(i, end)], horizon)\n        train_batches.append((bx, by, bm))\n    \n    val_batches = []\n    for i in range(0, len(X_val), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_val))\n        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        by, bm = prepare_targets([y_val[j] for j in range(i, end)], horizon)\n        val_batches.append((bx, by, bm))\n    \n    best_loss, best_state, bad = float('inf'), None, 0\n    \n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for bx, by, bm in train_batches:\n            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n            pred = model(bx)\n            loss = criterion(pred, by, bm)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n        \n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by, bm in val_batches:\n                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n                pred = model(bx)\n                val_losses.append(criterion(pred, by, bm).item())\n        \n        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n        scheduler.step(val_loss)\n        \n        if epoch % 10 == 0:\n            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n    \n    if best_state:\n        model.load_state_dict(best_state)\n    \n    return model, best_loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T17:21:35.173419Z","iopub.execute_input":"2025-10-12T17:21:35.173948Z","iopub.status.idle":"2025-10-12T18:08:04.841026Z","shell.execute_reply.started":"2025-10-12T17:21:35.173925Z","shell.execute_reply":"2025-10-12T18:08:04.840206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MAIN PIPELINE\n# ============================================================================\n\nconfig = Config()\n\nprint(\"=\"*80)\nprint(\"STEP 2: BETTER FEATURES PIPELINE\")\nprint(\"=\"*80)\nprint(\"\\nAdding 30-40 advanced features to improve from 0.62 → 0.59-0.60\")\n\n# Load\nprint(\"\\n[1/4] Loading data...\")\ntrain_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\ntrain_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\ntrain_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\ntrain_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\ntest_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\ntest_template = pd.read_csv(config.DATA_DIR / \"test.csv\")\n\n# Prepare with advanced features\nprint(\"\\n[2/4] Preparing with ADVANCED features...\")\nsequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = prepare_sequences_with_advanced_features(\n    train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n)\n\nsequences = np.array(sequences, dtype=object)\ntargets_dx = np.array(targets_dx, dtype=object)\ntargets_dy = np.array(targets_dy, dtype=object)\n\n# Train\nprint(\"\\n[3/4] Training with enhanced features...\")\ngroups = np.array([d['game_id'] for d in sequence_ids])\ngkf = GroupKFold(n_splits=config.N_FOLDS)\n\nmodels_x, models_y, scalers = [], [], []\n\nfor fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n    print(f\"\\n{'='*60}\")\n    print(f\"Fold {fold}/{config.N_FOLDS}\")\n    print(f\"{'='*60}\")\n    \n    X_tr, X_va = sequences[tr], sequences[va]\n    \n    scaler = StandardScaler()\n    scaler.fit(np.vstack([s for s in X_tr]))\n    \n    X_tr_sc = np.stack([scaler.transform(s) for s in X_tr])\n    X_va_sc = np.stack([scaler.transform(s) for s in X_va])\n    \n    # Train X\n    print(\"Training X-axis model...\")\n    mx, loss_x = train_model(\n        X_tr_sc, targets_dx[tr], X_va_sc, targets_dx[va],\n        X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n    )\n    \n    # Train Y\n    print(\"Training Y-axis model...\")\n    my, loss_y = train_model(\n        X_tr_sc, targets_dy[tr], X_va_sc, targets_dy[va],\n        X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n    )\n    \n    models_x.append(mx)\n    models_y.append(my)\n    scalers.append(scaler)\n    \n    print(f\"\\nFold {fold} - X loss: {loss_x:.5f}, Y loss: {loss_y:.5f}\")\n\n# Test predictions\nprint(\"\\n[4/4] Creating test predictions...\")\ntest_sequences, test_ids = prepare_sequences_with_advanced_features(\n    test_input, test_template=test_template, is_training=False, window_size=config.WINDOW_SIZE\n)\n\nX_test = np.array(test_sequences, dtype=object)\nx_last = np.array([s[-1, 0] for s in X_test])\ny_last = np.array([s[-1, 1] for s in X_test])\n\n# Ensemble predictions across folds\nall_dx, all_dy = [], []\nfor mx, my, sc in zip(models_x, models_y, scalers):\n    X_sc = np.stack([sc.transform(s) for s in X_test])\n    X_t = torch.tensor(X_sc.astype(np.float32)).to(config.DEVICE)\n    \n    mx.eval()\n    my.eval()\n    \n    with torch.no_grad():\n        all_dx.append(mx(X_t).cpu().numpy())\n        all_dy.append(my(X_t).cpu().numpy())\n\nens_dx = np.mean(all_dx, axis=0)\nens_dy = np.mean(all_dy, axis=0)\n\n# Create submission\nrows = []\nH = ens_dx.shape[1]\n\nfor i, sid in enumerate(test_ids):\n    fids = test_template[\n        (test_template['game_id'] == sid['game_id']) &\n        (test_template['play_id'] == sid['play_id']) &\n        (test_template['nfl_id'] == sid['nfl_id'])\n    ]['frame_id'].sort_values().tolist()\n    \n    for t, fid in enumerate(fids):\n        tt = min(t, H - 1)\n        px = np.clip(x_last[i] + ens_dx[i, tt], 0, 120)\n        py = np.clip(y_last[i] + ens_dy[i, tt], 0, 53.3)\n        \n        rows.append({\n            'id': f\"{sid['game_id']}_{sid['play_id']}_{sid['nfl_id']}_{fid}\",\n            'x': px,\n            'y': py\n        })\n\nsubmission = pd.DataFrame(rows)\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2 COMPLETE!\")\nprint(\"=\"*80)\nprint(f\"✓ Saved submission.csv\")\nprint(f\"  Rows: {len(submission)}\")\nprint(f\"  Features used: ~90 (was ~50)\")\nprint(f\"\\nExpected improvement:\")\nprint(f\"  Before: 0.62 RMSE (baseline features)\")\nprint(f\"  After:  0.59-0.60 RMSE (with advanced features)\")\nprint(f\"\\nNew feature groups added:\")\nprint(f\"  1. Distance rate features (3)\")\nprint(f\"  2. Target alignment features (3)\")\nprint(f\"  3. Multi-window rolling features (24)\")\nprint(f\"  4. Extended lag features (8)\")\nprint(f\"  5. Velocity change features (4)\")\nprint(f\"  6. Field position features (4)\")\nprint(f\"  7. Role-specific features (3)\")\nprint(f\"  8. Time-based features (2)\")\nprint(f\"\\nTotal new features: ~40\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T17:21:35.173419Z","iopub.execute_input":"2025-10-12T17:21:35.173948Z","iopub.status.idle":"2025-10-12T18:08:04.841026Z","shell.execute_reply.started":"2025-10-12T17:21:35.173925Z","shell.execute_reply":"2025-10-12T18:08:04.840206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}