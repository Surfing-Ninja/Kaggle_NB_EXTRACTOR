{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom sklearn.model_selection import GroupKFold\nfrom pathlib import Path\nimport warnings\nimport gc\nimport time\n\nwarnings.filterwarnings('ignore')\n\n# ============ GPU Setup ============\ndef setup_gpus():\n    if torch.cuda.is_available():\n        gpu_count = torch.cuda.device_count()\n        print(f\"GPUs available: {gpu_count}\")\n        for i in range(gpu_count):\n            gpu_name = torch.cuda.get_device_name(i)\n            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n            print(f\"GPU {i}: {gpu_name} - {gpu_memory:.2f} GB\")\n        \n        device = torch.device('cuda')\n        torch.cuda.empty_cache()\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = False\n        \n        return device, gpu_count\n    else:\n        print(\"No GPU available, using CPU\")\n        return torch.device('cpu'), 0\n\ndevice, gpu_count = setup_gpus()\n\n# ============ Robust Dataset ============\nclass RobustNFLDataset(Dataset):\n    def __init__(self, data, is_training=True, stats_dict=None):\n        self.is_training = is_training\n        self.stats_dict = stats_dict if stats_dict is not None else {}\n        \n        print(\"Pre-computing features...\")\n        data = data.copy()\n        \n        # Ensure critical columns exist with proper defaults\n        if 'frame_id' not in data.columns:\n            data['frame_id'] = 1\n        if 'num_frames_output' not in data.columns:\n            data['num_frames_output'] = 10\n        if 'x_last' not in data.columns:\n            data['x_last'] = 60.0\n        if 'y_last' not in data.columns:\n            data['y_last'] = 26.65\n        if 'ball_land_x' not in data.columns:\n            data['ball_land_x'] = 60.0\n        if 'ball_land_y' not in data.columns:\n            data['ball_land_y'] = 26.65\n        if 'absolute_yardline_number' not in data.columns:\n            data['absolute_yardline_number'] = 50\n            \n        # Height and weight handling\n        def safe_height_to_inches(ht):\n            if isinstance(ht, str) and '-' in ht:\n                try:\n                    f, ins = ht.split('-')\n                    return int(f) * 12 + int(ins)\n                except:\n                    return 72\n            return 72\n        \n        if 'player_height' in data.columns:\n            data['player_height'] = data['player_height'].apply(safe_height_to_inches)\n        else:\n            data['player_height'] = 72\n            \n        if 'player_weight' not in data.columns:\n            data['player_weight'] = 220\n        else:\n            data['player_weight'] = data['player_weight'].fillna(220)\n        \n        # Time features\n        data['frame_offset'] = data['frame_id'].astype(float)\n        data['time_offset'] = data['frame_offset'] / 10.0\n        data['T'] = np.maximum(data['num_frames_output'].astype(float), 1.0)\n        data['t_rel'] = data['frame_offset'] / data['T']\n        data['t_rel_squared'] = data['t_rel'] ** 2\n        data['t_rel_cubed'] = data['t_rel'] ** 3\n        \n        # Ball distance and angle\n        dx_ball = data['ball_land_x'] - data['x_last']\n        dy_ball = data['ball_land_y'] - data['y_last']\n        data['dist_to_ball'] = np.sqrt(dx_ball**2 + dy_ball**2 + 1e-6)\n        angle_to_ball = np.arctan2(dy_ball, dx_ball)\n        data['sin_angle_ball'] = np.sin(angle_to_ball)\n        data['cos_angle_ball'] = np.cos(angle_to_ball)\n        data['log_dist_ball'] = np.log1p(data['dist_to_ball'])\n        \n        # Velocity features\n        if 's' in data.columns and 'dir' in data.columns:\n            dir_rad = np.deg2rad(data['dir'].fillna(0))\n            data['speed_x'] = data['s'].fillna(0) * np.sin(dir_rad)\n            data['speed_y'] = data['s'].fillna(0) * np.cos(dir_rad)\n            \n            # Parallel and perpendicular velocity\n            den = data['dist_to_ball'] + 1e-6\n            ux = dx_ball / den\n            uy = dy_ball / den\n            data['v_parallel'] = data['speed_x'] * ux + data['speed_y'] * uy\n            data['v_perpendicular'] = data['speed_x'] * uy - data['speed_y'] * ux\n        else:\n            data['s'] = 0\n            data['dir'] = 0\n            data['speed_x'] = 0\n            data['speed_y'] = 0\n            data['v_parallel'] = 0\n            data['v_perpendicular'] = 0\n        \n        # Acceleration features\n        if 'a' not in data.columns:\n            data['a'] = 0\n        \n        # Orientation features\n        if 'o' in data.columns:\n            o_rad = np.deg2rad(data['o'].fillna(0))\n            data['sin_orientation'] = np.sin(o_rad)\n            data['cos_orientation'] = np.cos(o_rad)\n            data['orientation_ball_alignment'] = np.cos(o_rad - angle_to_ball)\n        else:\n            data['o'] = 0\n            data['sin_orientation'] = 0\n            data['cos_orientation'] = 1\n            data['orientation_ball_alignment'] = 0\n        \n        # Player role features - SAFE HANDLING\n        if 'player_role' in data.columns:\n            data['is_target'] = (data['player_role'] == 'Targeted Receiver').astype(float)\n            data['is_passer'] = (data['player_role'] == 'Passer').astype(float)\n            data['is_coverage'] = (data['player_role'] == 'Defensive Coverage').astype(float)\n        else:\n            data['is_target'] = 0\n            data['is_passer'] = 0\n            data['is_coverage'] = 0\n        \n        # Target receiver position - SAFE HANDLING\n        data['target_x'] = data['ball_land_x']  # Default to ball position\n        data['target_y'] = data['ball_land_y']\n        \n        if 'player_role' in data.columns and 'is_target' in data.columns:\n            target_data = data[data['is_target'] == 1]\n            if len(target_data) > 0:\n                # Get target positions per play\n                target_positions = target_data.groupby(['game_id', 'play_id'])[['x_last', 'y_last']].first()\n                target_positions = target_positions.rename(columns={'x_last': 'target_x_temp', 'y_last': 'target_y_temp'})\n                target_positions = target_positions.reset_index()\n                \n                # Merge safely\n                if len(target_positions) > 0:\n                    data = data.merge(target_positions, on=['game_id', 'play_id'], how='left')\n                    data['target_x'] = data['target_x_temp'].fillna(data['target_x'])\n                    data['target_y'] = data['target_y_temp'].fillna(data['target_y'])\n                    data = data.drop(columns=['target_x_temp', 'target_y_temp'])\n        \n        # Target distance\n        dx_target = data['target_x'] - data['x_last']\n        dy_target = data['target_y'] - data['y_last']\n        data['dist_to_target'] = np.sqrt(dx_target**2 + dy_target**2 + 1e-6)\n        data['log_dist_target'] = np.log1p(data['dist_to_target'])\n        \n        # Side encoding\n        if 'player_side' in data.columns:\n            data['is_offense'] = (data['player_side'] == 'Offense').astype(float)\n            data['is_defense'] = (data['player_side'] == 'Defense').astype(float)\n        else:\n            data['is_offense'] = 0\n            data['is_defense'] = 0\n        \n        # Direction encoding\n        if 'play_direction' in data.columns:\n            data['play_left'] = (data['play_direction'] == 'left').astype(float)\n        else:\n            data['play_left'] = 0\n        \n        # Normalized positions\n        data['x_norm'] = data['x_last'] / 120.0\n        data['y_norm'] = data['y_last'] / 53.3\n        data['ball_x_norm'] = data['ball_land_x'] / 120.0\n        data['ball_y_norm'] = data['ball_land_y'] / 53.3\n        \n        # Build feature matrix\n        feature_cols = [\n            'x_last', 'y_last', 'x_norm', 'y_norm',\n            's', 'a', 'o', 'dir',\n            'speed_x', 'speed_y', 'v_parallel', 'v_perpendicular',\n            'frame_offset', 'time_offset', 't_rel', 't_rel_squared', 't_rel_cubed',\n            'dist_to_ball', 'sin_angle_ball', 'cos_angle_ball', 'log_dist_ball',\n            'dist_to_target', 'log_dist_target',\n            'sin_orientation', 'cos_orientation', 'orientation_ball_alignment',\n            'ball_x_norm', 'ball_y_norm', 'ball_land_x', 'ball_land_y',\n            'is_target', 'is_passer', 'is_coverage',\n            'is_offense', 'is_defense', 'play_left',\n            'player_height', 'player_weight',\n            'absolute_yardline_number', 'num_frames_output', 'T'\n        ]\n        \n        # Collect features safely\n        features_list = []\n        for col in feature_cols:\n            if col in data.columns:\n                features_list.append(data[col].fillna(0).values)\n            else:\n                features_list.append(np.zeros(len(data)))\n        \n        self.features = np.stack(features_list, axis=1).astype(np.float32)\n        \n        # Normalize features\n        self.normalize_features()\n        \n        # Store metadata\n        self.metadata = data[['game_id', 'play_id', 'nfl_id', 'frame_id', \n                              'x_last', 'y_last']].fillna(0).values\n        \n        # Training targets\n        if self.is_training:\n            if 'displacement_x' in data.columns and 'displacement_y' in data.columns:\n                self.targets = data[['displacement_x', 'displacement_y']].fillna(0).values.astype(np.float32)\n            elif 'x' in data.columns and 'y' in data.columns:\n                self.targets = np.stack([\n                    data['x'].fillna(60) - data['x_last'].fillna(60),\n                    data['y'].fillna(26.65) - data['y_last'].fillna(26.65)\n                ], axis=1).astype(np.float32)\n                self.targets = np.clip(self.targets, -10, 10)\n            else:\n                self.targets = np.zeros((len(data), 2), dtype=np.float32)\n            \n            # Sample weights\n            w_time = data['t_rel'].values\n            w_time = 1.0 + 0.6 * (w_time - w_time.min()) / (w_time.max() - w_time.min() + 1e-9)\n            w_role = np.where(data.get('is_target', pd.Series(np.zeros(len(data)))).values == 1, 2.0, 1.0)\n            self.weights = (w_time * w_role).astype(np.float32)\n        \n        print(f\"Dataset ready: {len(self)} samples, {self.features.shape[1]} features\")\n    \n    def normalize_features(self):\n        \"\"\"Normalize features safely\"\"\"\n        skip_indices = set([30, 31, 32, 33, 34, 35])  # Binary features\n        \n        if self.is_training and 'mean' not in self.stats_dict:\n            self.stats_dict['mean'] = np.zeros(self.features.shape[1])\n            self.stats_dict['std'] = np.ones(self.features.shape[1])\n            \n            for i in range(self.features.shape[1]):\n                if i not in skip_indices:\n                    self.stats_dict['mean'][i] = np.mean(self.features[:, i])\n                    self.stats_dict['std'][i] = np.std(self.features[:, i]) + 1e-6\n                    self.features[:, i] = (self.features[:, i] - self.stats_dict['mean'][i]) / self.stats_dict['std'][i]\n        elif 'mean' in self.stats_dict:\n            for i in range(self.features.shape[1]):\n                if i not in skip_indices:\n                    self.features[:, i] = (self.features[:, i] - self.stats_dict['mean'][i]) / self.stats_dict['std'][i]\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        sample = {\n            'features': torch.from_numpy(self.features[idx]),\n            'game_id': self.metadata[idx, 0],\n            'play_id': self.metadata[idx, 1],\n            'nfl_id': self.metadata[idx, 2],\n            'frame_id': self.metadata[idx, 3],\n            'x_last': self.metadata[idx, 4],\n            'y_last': self.metadata[idx, 5]\n        }\n        \n        if self.is_training:\n            sample['target'] = torch.from_numpy(self.targets[idx])\n            sample['weight'] = self.weights[idx]\n        \n        return sample\n\n# ============ Robust Models ============\nclass AttentionBlock(nn.Module):\n    def __init__(self, dim, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 4, dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        x = x.unsqueeze(1) if x.dim() == 2 else x\n        normed = self.norm(x)\n        x = x + self.attn(normed, normed, normed)[0]\n        x = x + self.mlp(self.norm(x))\n        return x.squeeze(1) if x.size(1) == 1 else x\n\nclass RobustTrajectoryModel(nn.Module):\n    def __init__(self, input_dim=41, hidden_dim=512, num_layers=6, dropout=0.2):\n        super().__init__()\n        \n        # Input projection\n        self.input_proj = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout)\n        )\n        \n        # Main blocks\n        self.blocks = nn.ModuleList([\n            AttentionBlock(hidden_dim, num_heads=8, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n        \n        # Output heads\n        self.output_head = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(dropout/2),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Linear(128, 2)\n        )\n        \n        self.apply(self._init_weights)\n    \n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.ones_(m.weight)\n            nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        x = self.input_proj(x)\n        \n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            if i == len(self.blocks) // 2:  # Skip connection at middle\n                identity = x\n            elif i == len(self.blocks) - 1:  # Add skip at end\n                x = x + identity * 0.3\n        \n        return self.output_head(x)\n\n# ============ Training Pipeline ============\nclass RobustPipeline:\n    def __init__(self, data_dir=\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\"):\n        self.data_dir = Path(data_dir)\n        self.device = device\n        self.gpu_count = gpu_count\n        self.models = []\n        self.stats_dict = {}\n        self.scaler = GradScaler() if device.type == 'cuda' else None\n    \n    def load_and_prepare_data(self):\n        \"\"\"Load and prepare data safely\"\"\"\n        print(\"Loading data...\")\n        \n        train_inputs = []\n        train_outputs = []\n        \n        for week in range(1, 19):\n            try:\n                inp_path = self.data_dir / f\"train/input_2023_w{week:02d}.csv\"\n                out_path = self.data_dir / f\"train/output_2023_w{week:02d}.csv\"\n                \n                if inp_path.exists() and out_path.exists():\n                    inp = pd.read_csv(inp_path)\n                    out = pd.read_csv(out_path)\n                    train_inputs.append(inp)\n                    train_outputs.append(out)\n                    print(f\"Loaded week {week}\")\n            except Exception as e:\n                print(f\"Could not load week {week}: {e}\")\n                continue\n        \n        if not train_inputs:\n            raise ValueError(\"No training data loaded!\")\n        \n        train_input = pd.concat(train_inputs, ignore_index=True)\n        train_output = pd.concat(train_outputs, ignore_index=True)\n        \n        del train_inputs, train_outputs\n        gc.collect()\n        \n        print(\"Preparing features...\")\n        train_data = self._prepare_features(train_input, train_output, is_training=True)\n        \n        # Test data\n        test_input = pd.read_csv(self.data_dir / \"test_input.csv\")\n        test_template = pd.read_csv(self.data_dir / \"test.csv\")\n        test_data = self._prepare_features(test_input, test_template, is_training=False)\n        \n        return train_data, test_data, test_template\n    \n    def _prepare_features(self, input_df, output_df, is_training):\n        \"\"\"Prepare features safely\"\"\"\n        \n        # Get last frame before throw\n        last = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n        last = last.groupby(['game_id', 'play_id', 'nfl_id'], as_index=False).last()\n        last = last.rename(columns={'x': 'x_last', 'y': 'y_last'})\n        \n        # Merge with output\n        data = output_df.copy()\n        \n        # Drop frame_id from last to avoid conflicts\n        if 'frame_id' in last.columns:\n            last = last.drop(columns=['frame_id'])\n        \n        data = data.merge(last, on=['game_id', 'play_id', 'nfl_id'], how='left')\n        \n        # Calculate displacements for training\n        if is_training and 'x' in data.columns and 'y' in data.columns:\n            data['displacement_x'] = (data['x'] - data['x_last'].fillna(data['x'])).clip(-10, 10)\n            data['displacement_y'] = (data['y'] - data['y_last'].fillna(data['y'])).clip(-10, 10)\n        \n        # Fill NaN values\n        numeric_cols = data.select_dtypes(include=[np.number]).columns\n        data[numeric_cols] = data[numeric_cols].fillna(0)\n        \n        return data\n    \n    def train_ensemble(self, train_data, n_models=8, n_folds=5, epochs=25):\n        \"\"\"Train ensemble of models\"\"\"\n        print(f\"\\nTraining ensemble: {n_models} models, {n_folds} folds\")\n        \n        groups = train_data['game_id'].values\n        gkf = GroupKFold(n_splits=n_folds)\n        \n        all_oof_predictions = []\n        \n        for model_idx in range(n_models):\n            print(f\"\\n{'='*50}\")\n            print(f\"Model {model_idx + 1}/{n_models}\")\n            print(f\"{'='*50}\")\n            \n            # Set seed for reproducibility\n            seed = model_idx * 42\n            torch.manual_seed(seed)\n            np.random.seed(seed)\n            \n            model_oof = np.zeros((len(train_data), 2))\n            fold_models = []\n            \n            for fold, (train_idx, val_idx) in enumerate(gkf.split(train_data, groups=groups)):\n                print(f\"\\nFold {fold + 1}/{n_folds}\")\n                \n                # Split data\n                train_fold = train_data.iloc[train_idx]\n                val_fold = train_data.iloc[val_idx]\n                \n                # Create datasets\n                train_dataset = RobustNFLDataset(train_fold, is_training=True)\n                self.stats_dict = train_dataset.stats_dict\n                \n                val_dataset = RobustNFLDataset(val_fold, is_training=True, \n                                              stats_dict=self.stats_dict)\n                \n                # Dataloaders\n                train_loader = DataLoader(\n                    train_dataset,\n                    batch_size=512 * max(1, self.gpu_count),\n                    shuffle=True,\n                    num_workers=4 if self.device.type == 'cuda' else 0,\n                    pin_memory=(self.device.type == 'cuda')\n                )\n                \n                val_loader = DataLoader(\n                    val_dataset,\n                    batch_size=1024,\n                    shuffle=False,\n                    num_workers=4 if self.device.type == 'cuda' else 0,\n                    pin_memory=(self.device.type == 'cuda')\n                )\n                \n                # Initialize model\n                model = RobustTrajectoryModel(\n                    input_dim=train_dataset.features.shape[1],\n                    hidden_dim=384 + (model_idx % 3) * 128,  # Vary architecture\n                    num_layers=5 + (model_idx % 3),\n                    dropout=0.2 + (model_idx % 4) * 0.05\n                ).to(self.device)\n                \n                if self.gpu_count > 1:\n                    model = nn.DataParallel(model)\n                \n                # Optimizer\n                optimizer = torch.optim.AdamW(\n                    model.parameters(),\n                    lr=2e-3,\n                    weight_decay=1e-4\n                )\n                \n                # Scheduler\n                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                    optimizer, T_max=epochs\n                )\n                \n                criterion = nn.SmoothL1Loss(reduction='none')\n                \n                # Training\n                best_val_loss = float('inf')\n                best_model_state = None\n                patience = 0\n                \n                for epoch in range(epochs):\n                    # Train\n                    model.train()\n                    train_loss = 0\n                    \n                    for batch in train_loader:\n                        features = batch['features'].to(self.device)\n                        targets = batch['target'].to(self.device)\n                        weights = batch['weight'].to(self.device)\n                        \n                        if self.device.type == 'cuda' and self.scaler:\n                            with autocast():\n                                predictions = model(features)\n                                loss = criterion(predictions, targets)\n                                weighted_loss = (loss.mean(dim=1) * weights).mean()\n                            \n                            optimizer.zero_grad()\n                            self.scaler.scale(weighted_loss).backward()\n                            self.scaler.unscale_(optimizer)\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                            self.scaler.step(optimizer)\n                            self.scaler.update()\n                        else:\n                            predictions = model(features)\n                            loss = criterion(predictions, targets)\n                            weighted_loss = (loss.mean(dim=1) * weights).mean()\n                            \n                            optimizer.zero_grad()\n                            weighted_loss.backward()\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                            optimizer.step()\n                        \n                        scheduler.step()\n                        train_loss += weighted_loss.item()\n                    \n                    # Validate every 3 epochs\n                    if epoch % 3 == 0:\n                        model.eval()\n                        val_loss = 0\n                        \n                        with torch.no_grad():\n                            for batch in val_loader:\n                                features = batch['features'].to(self.device)\n                                targets = batch['target'].to(self.device)\n                                \n                                predictions = model(features)\n                                loss = criterion(predictions, targets).mean()\n                                val_loss += loss.item()\n                        \n                        val_loss /= len(val_loader)\n                        \n                        if val_loss < best_val_loss:\n                            best_val_loss = val_loss\n                            best_model_state = model.state_dict().copy()\n                            patience = 0\n                        else:\n                            patience += 1\n                            if patience >= 3:\n                                break\n                        \n                        print(f\"  Epoch {epoch+1}: Val Loss={val_loss:.4f}\")\n                \n                # Load best model\n                if best_model_state is not None:\n                    model.load_state_dict(best_model_state)\n                \n                fold_models.append(model)\n                \n                # OOF predictions\n                model.eval()\n                with torch.no_grad():\n                    val_preds = []\n                    for batch in val_loader:\n                        features = batch['features'].to(self.device)\n                        predictions = model(features)\n                        val_preds.append(predictions.cpu().numpy())\n                    \n                    model_oof[val_idx] = np.vstack(val_preds)\n            \n            all_oof_predictions.append(model_oof)\n            self.models.extend(fold_models)\n        \n        # Calculate OOF score\n        final_oof = np.mean(all_oof_predictions, axis=0)\n        x_pred = train_data['x_last'].values + final_oof[:, 0]\n        y_pred = train_data['y_last'].values + final_oof[:, 1]\n        \n        if 'x' in train_data.columns and 'y' in train_data.columns:\n            rmse = np.sqrt(\n                ((x_pred - train_data['x'].values)**2 + \n                 (y_pred - train_data['y'].values)**2).mean() / 2\n            )\n            print(f\"\\n{'='*50}\")\n            print(f\"Final OOF RMSE: {rmse:.6f}\")\n            print(f\"{'='*50}\")\n    \n    def predict(self, test_data, test_template):\n        \"\"\"Generate predictions\"\"\"\n        print(\"\\nGenerating predictions...\")\n        \n        test_dataset = RobustNFLDataset(test_data, is_training=False, \n                                       stats_dict=self.stats_dict)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=1024,\n            shuffle=False,\n            num_workers=4 if self.device.type == 'cuda' else 0,\n            pin_memory=(self.device.type == 'cuda')\n        )\n        \n        all_predictions = []\n        \n        for i, model in enumerate(self.models):\n            print(f\"Predicting with model {i+1}/{len(self.models)}\")\n            model.eval()\n            model_preds = []\n            \n            with torch.no_grad():\n                for batch in test_loader:\n                    features = batch['features'].to(self.device)\n                    \n                    if self.device.type == 'cuda':\n                        with autocast():\n                            predictions = model(features)\n                    else:\n                        predictions = model(features)\n                    \n                    model_preds.append(predictions.cpu().numpy())\n            \n            all_predictions.append(np.vstack(model_preds))\n        \n        # Average predictions\n        final_predictions = np.mean(all_predictions, axis=0)\n        \n        # Apply physics constraints and create submission\n        results = []\n        for i in range(len(test_dataset)):\n            dx = final_predictions[i, 0]\n            dy = final_predictions[i, 1]\n            \n            x_last = test_dataset.metadata[i, 4]\n            y_last = test_dataset.metadata[i, 5]\n            frame_id = test_dataset.metadata[i, 3]\n            \n            # Frame-based decay\n            frame_decay = 0.98 ** (frame_id / 5.0)\n            dx *= frame_decay\n            dy *= frame_decay\n            \n            # Speed limit\n            max_speed = 9\n            disp_mag = np.sqrt(dx**2 + dy**2)\n            if disp_mag > max_speed:\n                scale = max_speed / disp_mag\n                dx *= scale\n                dy *= scale\n            \n            # Calculate position\n            x_pred = np.clip(x_last + dx, 0, 120)\n            y_pred = np.clip(y_last + dy, 0, 53.3)\n            \n            results.append({\n                'game_id': int(test_dataset.metadata[i, 0]),\n                'play_id': int(test_dataset.metadata[i, 1]),\n                'nfl_id': int(test_dataset.metadata[i, 2]),\n                'frame_id': int(test_dataset.metadata[i, 3]),\n                'x': x_pred,\n                'y': y_pred\n            })\n        \n        # Create submission\n        pred_df = pd.DataFrame(results)\n        submission = test_template.merge(\n            pred_df,\n            on=['game_id', 'play_id', 'nfl_id', 'frame_id'],\n            how='left'\n        )\n        \n        submission['x'] = submission['x'].fillna(60.0)\n        submission['y'] = submission['y'].fillna(26.65)\n        \n        submission['id'] = (\n            submission['game_id'].astype(str) + '_' +\n            submission['play_id'].astype(str) + '_' +\n            submission['nfl_id'].astype(str) + '_' +\n            submission['frame_id'].astype(str)\n        )\n        \n        return submission[['id', 'x', 'y']]\n\n# ============ Main Execution ============\nif __name__ == \"__main__\":\n    start_time = time.time()\n    \n    print(\"=\"*60)\n    print(\"NFL Big Data Bowl 2026 - Robust Pipeline\")\n    print(\"=\"*60)\n    \n    # Initialize\n    pipeline = RobustPipeline()\n    \n    # Load data\n    train_data, test_data, test_template = pipeline.load_and_prepare_data()\n    print(f\"\\nData loaded:\")\n    print(f\"Training samples: {len(train_data):,}\")\n    print(f\"Test samples: {len(test_data):,}\")\n    \n    # Train ensemble\n    pipeline.train_ensemble(\n        train_data,\n        n_models=10,  # 10 diverse models\n        n_folds=5,    # 5-fold CV\n        epochs=30     # 30 epochs with early stopping\n    )\n    \n    # Generate predictions\n    submission = pipeline.predict(test_data, test_template)\n    \n    # Save\n    submission.to_csv('submission.csv', index=False)\n    \n    elapsed = (time.time() - start_time) / 3600\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Pipeline completed in {elapsed:.2f} hours\")\n    print(f\"Submission shape: {submission.shape}\")\n    print(\"\\nFirst 10 predictions:\")\n    print(submission.head(10))\n    \n    # Clear GPU\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    print(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}