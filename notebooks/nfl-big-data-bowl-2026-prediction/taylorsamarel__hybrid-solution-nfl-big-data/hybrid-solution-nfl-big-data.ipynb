{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# NFL BIG DATA BOWL 2026 - ENHANCED WITH TEMPORAL FEATURES + PLAYER INTERACTIONS\n# Complete ensemble with temporal/time series oriented features\n# ================================================================================\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport gc\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nfrom scipy.spatial.distance import cdist\n\n# Machine Learning\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import RidgeCV\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\nwarnings.filterwarnings('ignore')\n\n# ================================================================================\n# CONFIGURATION\n# ================================================================================\n\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    SEEDS = [42, 123, 2024, 69]  # Multiple seeds for ensemble\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    MAX_SPEED = 12.0\n    N_FOLDS = 5\n    NN_BATCH_SIZE = 2048\n    NN_EPOCHS = 30\n    NN_LEARNING_RATE = 0.001\n\n# ================================================================================\n# DATA LOADING\n# ================================================================================\n\ndef load_data():\n    \"\"\"Load all training and test data\"\"\"\n    print(\"Loading data...\")\n    \n    # Training data\n    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    # Filter existing files\n    train_input_files = [f for f in train_input_files if f.exists()]\n    train_output_files = [f for f in train_output_files if f.exists()]\n    \n    print(f\"Found {len(train_input_files)} weeks of data\")\n    \n    # Load and concatenate\n    train_input = pd.concat([pd.read_csv(f) for f in tqdm(train_input_files, desc=\"Input\")], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in tqdm(train_output_files, desc=\"Output\")], ignore_index=True)\n    \n    # Test data\n    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n    \n    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n    \n    return train_input, train_output, test_input, test_template\n\n# ================================================================================\n# NEW: PLAYER-TO-PLAYER INTERACTION FEATURES\n# ================================================================================\n\ndef compute_player_interactions(input_df):\n    \"\"\"Compute player-to-player interaction features\"\"\"\n    print(\"  Computing player interaction features...\")\n    \n    interaction_features = []\n    \n    # Group by game and play to get last frame before throw\n    for (game_id, play_id), play_group in input_df.groupby(['game_id', 'play_id']):\n        # Get last frame before throw\n        last_frame = play_group.sort_values('frame_id').groupby('nfl_id').last().reset_index()\n        \n        # Create position matrix for all players\n        positions = last_frame[['x', 'y']].values\n        player_ids = last_frame['nfl_id'].values\n        player_sides = last_frame['player_side'].values\n        player_roles = last_frame['player_role'].values\n        \n        # Calculate pairwise distances\n        if len(positions) > 1:\n            distances = cdist(positions, positions)\n            \n            for i, player_id in enumerate(player_ids):\n                player_features = {\n                    'game_id': game_id,\n                    'play_id': play_id,\n                    'nfl_id': player_id\n                }\n                \n                # Mask for same side and opposite side\n                same_side_mask = (player_sides == player_sides[i]) & (np.arange(len(positions)) != i)\n                opp_side_mask = (player_sides != player_sides[i])\n                \n                # Nearest teammate\n                if np.any(same_side_mask):\n                    teammate_distances = distances[i][same_side_mask]\n                    player_features['nearest_teammate_dist'] = np.min(teammate_distances)\n                    player_features['avg_teammate_dist'] = np.mean(teammate_distances)\n                    player_features['teammates_within_5'] = np.sum(teammate_distances < 5)\n                    player_features['teammates_within_10'] = np.sum(teammate_distances < 10)\n                else:\n                    player_features['nearest_teammate_dist'] = 0\n                    player_features['avg_teammate_dist'] = 0\n                    player_features['teammates_within_5'] = 0\n                    player_features['teammates_within_10'] = 0\n                \n                # Nearest opponent\n                if np.any(opp_side_mask):\n                    opponent_distances = distances[i][opp_side_mask]\n                    player_features['nearest_opponent_dist'] = np.min(opponent_distances)\n                    player_features['avg_opponent_dist'] = np.mean(opponent_distances)\n                    player_features['opponents_within_5'] = np.sum(opponent_distances < 5)\n                    player_features['opponents_within_10'] = np.sum(opponent_distances < 10)\n                    \n                    # Pressure index (inverse of distance to nearest opponent)\n                    player_features['pressure_index'] = 1 / (player_features['nearest_opponent_dist'] + 1)\n                else:\n                    player_features['nearest_opponent_dist'] = 100\n                    player_features['avg_opponent_dist'] = 100\n                    player_features['opponents_within_5'] = 0\n                    player_features['opponents_within_10'] = 0\n                    player_features['pressure_index'] = 0\n                \n                # Local density (players within 10 yards)\n                player_features['local_density'] = np.sum(distances[i] < 10) - 1  # Exclude self\n                \n                # Formation spread from player's perspective\n                if np.any(same_side_mask):\n                    teammate_positions = positions[same_side_mask]\n                    if len(teammate_positions) > 0:\n                        player_features['team_spread_x'] = np.std(teammate_positions[:, 0])\n                        player_features['team_spread_y'] = np.std(teammate_positions[:, 1])\n                    else:\n                        player_features['team_spread_x'] = 0\n                        player_features['team_spread_y'] = 0\n                \n                # Special interactions for specific roles\n                if player_roles[i] == 'Targeted Receiver':\n                    # Find nearest coverage player\n                    coverage_mask = (player_roles == 'Defensive Coverage') & opp_side_mask\n                    if np.any(coverage_mask):\n                        coverage_distances = distances[i][coverage_mask]\n                        player_features['nearest_coverage_dist'] = np.min(coverage_distances)\n                        player_features['coverage_players_nearby'] = np.sum(coverage_distances < 10)\n                \n                # Vectorized features for direction to nearest players\n                if np.any(opp_side_mask):\n                    nearest_opp_idx = np.argmin(distances[i][opp_side_mask])\n                    opp_positions = positions[opp_side_mask]\n                    dx = opp_positions[nearest_opp_idx, 0] - positions[i, 0]\n                    dy = opp_positions[nearest_opp_idx, 1] - positions[i, 1]\n                    player_features['nearest_opp_dx'] = dx\n                    player_features['nearest_opp_dy'] = dy\n                    player_features['nearest_opp_angle'] = np.arctan2(dy, dx)\n                \n                interaction_features.append(player_features)\n    \n    return pd.DataFrame(interaction_features)\n\n# ================================================================================\n# FEATURE ENGINEERING - ENHANCED VERSION WITH TEMPORAL FEATURES\n# ================================================================================\n\ndef height_to_inches(height_str):\n    \"\"\"Convert height from 'ft-in' format to inches\"\"\"\n    if not isinstance(height_str, str) or '-' not in height_str:\n        return 70\n    try:\n        feet, inches = map(int, height_str.split('-'))\n        return feet * 12 + inches\n    except:\n        return 70\n\ndef prepare_features(input_df, output_df, is_training=True):\n    \"\"\"Complete feature preparation pipeline with temporal features and player interactions\"\"\"\n    \n    # Compute player interaction features\n    interaction_features = compute_player_interactions(input_df)\n    \n    # Get last frame before throw\n    last_frame = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']) \\\n                         .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False).last()\n    last_frame = last_frame.rename(columns={'x': 'x_last', 'y': 'y_last'})\n    \n    # Merge interaction features\n    if len(interaction_features) > 0:\n        last_frame = last_frame.merge(interaction_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    # ============================================================\n    # NEW: TEMPORAL FEATURES FROM INPUT SEQUENCE\n    # ============================================================\n    # Get temporal statistics from the input frames (before throw)\n    temporal_stats = input_df.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n        'x': ['mean', 'std', 'min', 'max'],\n        'y': ['mean', 'std', 'min', 'max'],\n        's': ['mean', 'std', 'max', 'min'],\n        'a': ['mean', 'std', 'max', 'min'],\n        'dir': lambda x: np.std(np.diff(x)) if len(x) > 1 else 0,  # Direction change rate\n        'o': lambda x: np.std(np.diff(x)) if len(x) > 1 else 0,    # Orientation change rate\n    }).reset_index()\n    temporal_stats.columns = ['_'.join(col).strip() if col[1] else col[0] \n                              for col in temporal_stats.columns.values]\n    temporal_stats = temporal_stats.rename(columns={\n        'dir_<lambda>': 'dir_change_rate',\n        'o_<lambda>': 'orientation_change_rate'\n    })\n    \n    # Get movement patterns from last N frames\n    last_n_frames = 5\n    recent_frames = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']) \\\n                            .groupby(['game_id', 'play_id', 'nfl_id']).tail(last_n_frames)\n    \n    # Calculate trajectory features from recent frames\n    trajectory_features = recent_frames.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n        'x': lambda x: (x.iloc[-1] - x.iloc[0]) if len(x) > 1 else 0,  # Recent displacement X\n        'y': lambda x: (x.iloc[-1] - x.iloc[0]) if len(x) > 1 else 0,  # Recent displacement Y\n        's': lambda x: x.diff().mean() if len(x) > 1 else 0,           # Acceleration trend\n    }).reset_index()\n    trajectory_features.columns = ['game_id', 'play_id', 'nfl_id', \n                                  'recent_displacement_x', 'recent_displacement_y', 'acceleration_trend']\n    \n    # Merge temporal features with last frame\n    last_frame = last_frame.merge(temporal_stats, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    last_frame = last_frame.merge(trajectory_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    # Convert height if available\n    if 'player_height' in last_frame.columns:\n        last_frame['height_inches'] = last_frame['player_height'].apply(height_to_inches)\n    \n    # Get target receiver position\n    targets = last_frame[last_frame['player_role'] == 'Targeted Receiver'][\n        ['game_id', 'play_id', 'x_last', 'y_last']\n    ].rename(columns={'x_last': 'target_x', 'y_last': 'target_y'})\n    targets = targets.drop_duplicates(['game_id', 'play_id'])\n    \n    last_frame = last_frame.merge(targets, on=['game_id', 'play_id'], how='left')\n    \n    # Columns to merge - include new temporal columns and interaction features\n    merge_cols = ['game_id', 'play_id', 'nfl_id', 'x_last', 'y_last', \n                  's', 'a', 'o', 'dir', 'player_role', 'player_side',\n                  'ball_land_x', 'ball_land_y', 'target_x', 'target_y',\n                  'play_direction', 'absolute_yardline_number', 'player_weight',\n                  # Temporal columns\n                  'x_mean', 'x_std', 'x_min', 'x_max',\n                  'y_mean', 'y_std', 'y_min', 'y_max',\n                  's_mean', 's_std', 's_max', 's_min',\n                  'a_mean', 'a_std', 'a_max', 'a_min',\n                  'dir_change_rate', 'orientation_change_rate',\n                  'recent_displacement_x', 'recent_displacement_y', 'acceleration_trend',\n                  # Player interaction columns\n                  'nearest_teammate_dist', 'avg_teammate_dist', 'teammates_within_5', 'teammates_within_10',\n                  'nearest_opponent_dist', 'avg_opponent_dist', 'opponents_within_5', 'opponents_within_10',\n                  'pressure_index', 'local_density', 'team_spread_x', 'team_spread_y',\n                  'nearest_coverage_dist', 'coverage_players_nearby',\n                  'nearest_opp_dx', 'nearest_opp_dy', 'nearest_opp_angle']\n    \n    if 'height_inches' in last_frame.columns:\n        merge_cols.append('height_inches')\n    \n    merge_cols = [c for c in merge_cols if c in last_frame.columns]\n    \n    # Merge with output\n    merged = output_df.merge(last_frame[merge_cols], \n                             on=['game_id', 'play_id', 'nfl_id'], \n                             how='left')\n    \n    # Engineer features\n    df = merged.copy()\n    \n    # ============================================================\n    # TEMPORAL FEATURES\n    # ============================================================\n    # Basic time features\n    df['time_seconds'] = df['frame_id'] / 10.0\n    df['time_normalized'] = df['frame_id'] / df.groupby(['game_id', 'play_id', 'nfl_id'])['frame_id'].transform('max')\n    \n    # Polynomial time features\n    df['time_squared'] = df['time_seconds'] ** 2\n    df['time_cubed'] = df['time_seconds'] ** 3\n    df['sqrt_time'] = np.sqrt(df['time_seconds'])\n    df['log_time'] = np.log1p(df['time_seconds'])\n    \n    # Fourier features for cyclical patterns\n    df['time_sin'] = np.sin(2 * np.pi * df['time_normalized'])\n    df['time_cos'] = np.cos(2 * np.pi * df['time_normalized'])\n    df['time_sin_2'] = np.sin(4 * np.pi * df['time_normalized'])\n    df['time_cos_2'] = np.cos(4 * np.pi * df['time_normalized'])\n    \n    # Phase-based features\n    df['is_early_play'] = (df['time_normalized'] < 0.33).astype(int)\n    df['is_mid_play'] = ((df['time_normalized'] >= 0.33) & (df['time_normalized'] < 0.67)).astype(int)\n    df['is_late_play'] = (df['time_normalized'] >= 0.67).astype(int)\n    \n    # NEW: Interaction features with time\n    if 'pressure_index' in df.columns:\n        df['pressure_x_time'] = df['pressure_index'] * df['time_seconds']\n        df['pressure_x_late_play'] = df['pressure_index'] * df['is_late_play']\n    \n    if 'nearest_opponent_dist' in df.columns:\n        df['opponent_closing_time'] = df['nearest_opponent_dist'] / (df['s'] + 1)\n        df['space_urgency'] = df['time_seconds'] / (df['opponent_closing_time'] + 0.1)\n    \n    # Velocity components\n    if 'dir' in df.columns and 's' in df.columns:\n        dir_rad = np.deg2rad(df['dir'].fillna(0))\n        df['velocity_x'] = df['s'] * np.sin(dir_rad)\n        df['velocity_y'] = df['s'] * np.cos(dir_rad)\n        \n        # Momentum features\n        if 'player_weight' in df.columns:\n            df['momentum_magnitude'] = df['player_weight'] * df['s']\n        \n        # Expected positions based on physics\n        df['expected_x_constant_v'] = df['x_last'] + df['velocity_x'] * df['time_seconds']\n        df['expected_y_constant_v'] = df['y_last'] + df['velocity_y'] * df['time_seconds']\n        \n        if 'a' in df.columns:\n            df['expected_x_with_accel'] = df['x_last'] + df['velocity_x'] * df['time_seconds'] + 0.5 * df['a'] * np.sin(dir_rad) * df['time_squared']\n            df['expected_y_with_accel'] = df['y_last'] + df['velocity_y'] * df['time_seconds'] + 0.5 * df['a'] * np.cos(dir_rad) * df['time_squared']\n    \n    # Movement consistency features\n    if 's_mean' in df.columns:\n        df['speed_consistency'] = df['s'] / (df['s_mean'] + 0.1)\n        df['speed_deviation'] = np.abs(df['s'] - df['s_mean'])\n        \n    if 'a_mean' in df.columns:\n        df['acceleration_consistency'] = df['a'] / (df['a_mean'] + 0.1)\n        df['acceleration_deviation'] = np.abs(df['a'] - df['a_mean'])\n    \n    # Temporal interaction features\n    df['time_x_speed'] = df['time_seconds'] * df['s']\n    df['time_x_acceleration'] = df['time_seconds'] * df['a']\n    df['time_squared_x_speed'] = df['time_squared'] * df['s']\n    \n    # Ball distance and angle\n    if all(col in df.columns for col in ['ball_land_x', 'ball_land_y', 'x_last', 'y_last']):\n        ball_dx = df['ball_land_x'] - df['x_last']\n        ball_dy = df['ball_land_y'] - df['y_last']\n        df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        \n        # Ball direction unit vectors\n        df['ball_direction_x'] = ball_dx / (df['distance_to_ball'] + 1e-6)\n        df['ball_direction_y'] = ball_dy / (df['distance_to_ball'] + 1e-6)\n        \n        # Time until ball arrival\n        estimated_ball_speed = 20.0  # yards/second\n        df['estimated_time_to_ball'] = df['distance_to_ball'] / estimated_ball_speed\n        df['time_ratio_to_ball'] = df['time_seconds'] / (df['estimated_time_to_ball'] + 0.1)\n        \n        # Closing speed\n        if 'velocity_x' in df.columns:\n            ball_unit_x = ball_dx / (df['distance_to_ball'] + 1e-6)\n            ball_unit_y = ball_dy / (df['distance_to_ball'] + 1e-6)\n            df['closing_speed'] = df['velocity_x'] * ball_unit_x + df['velocity_y'] * ball_unit_y\n            \n            # Projected time to reach ball\n            df['projected_time_to_ball'] = df['distance_to_ball'] / (np.abs(df['closing_speed']) + 0.1)\n            df['time_urgency'] = df['time_seconds'] / (df['projected_time_to_ball'] + 0.1)\n        \n        # Temporal ball distance features\n        df['distance_to_ball_x_time'] = df['distance_to_ball'] * df['time_seconds']\n        df['distance_to_ball_x_time_squared'] = df['distance_to_ball'] * df['time_squared']\n    \n    # Target distance\n    if 'target_x' in df.columns:\n        target_dx = df['target_x'] - df['x_last']\n        target_dy = df['target_y'] - df['y_last']\n        df['distance_to_target'] = np.sqrt(target_dx**2 + target_dy**2)\n        df['is_target'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n        df['angle_to_target'] = np.arctan2(target_dy, target_dx)\n        \n        # Temporal target features\n        df['distance_to_target_x_time'] = df['distance_to_target'] * df['time_seconds']\n        df['is_target_x_time_squared'] = df['is_target'] * df['time_squared']\n    \n    # Field position\n    df['x_normalized'] = df['x_last'] / Config.FIELD_X_MAX\n    df['y_normalized'] = df['y_last'] / Config.FIELD_Y_MAX\n    \n    # Distance from sidelines and endzone\n    df['distance_from_sideline'] = np.minimum(df['y_last'], Config.FIELD_Y_MAX - df['y_last'])\n    df['distance_from_endzone'] = np.minimum(df['x_last'], Config.FIELD_X_MAX - df['x_last'])\n    \n    # Role features\n    df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n    df['is_passer'] = (df['player_role'] == 'Passer').astype(int)\n    df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['is_redzone'] = (df['absolute_yardline_number'] <= 20).astype(int)\n    \n    # Interaction features\n    if 'is_target' in df.columns:\n        df['is_target_x_time'] = df['is_target'] * df['time_seconds']\n    if 'distance_to_ball' in df.columns:\n        df['distance_ball_x_speed'] = df['distance_to_ball'] * df['s']\n    \n    # Phase-specific role interactions\n    df['is_offense_x_early_play'] = df['is_offense'] * df['is_early_play']\n    df['is_offense_x_late_play'] = df['is_offense'] * df['is_late_play']\n    if 'is_target' in df.columns:\n        df['is_target_x_late_play'] = df['is_target'] * df['is_late_play']\n    \n    # Training targets\n    if is_training:\n        df['displacement_x'] = df['x'] - df['x_last']\n        df['displacement_y'] = df['y'] - df['y_last']\n        \n        # Remove invalid samples\n        valid_mask = (\n            df['displacement_x'].notna() & \n            df['displacement_y'].notna() &\n            (np.sqrt(df['displacement_x']**2 + df['displacement_y']**2) <= Config.MAX_SPEED * df['time_seconds'] * 1.5)\n        )\n        df = df[valid_mask].reset_index(drop=True)\n    \n    # Fill NaN values\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = df[numeric_cols].fillna(0)\n    \n    return df\n\n# ================================================================================\n# NEURAL NETWORK\n# ================================================================================\n\nclass SimpleNN(nn.Module):\n    \"\"\"Simple neural network for regression\"\"\"\n    \n    def __init__(self, input_dim):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)  # Single output\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\ndef train_neural_network(X_train, y_train, X_val, y_val, seed=42):\n    \"\"\"Train a neural network model\"\"\"\n    \n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Create datasets\n    train_dataset = TensorDataset(\n        torch.FloatTensor(X_train), \n        torch.FloatTensor(y_train.reshape(-1, 1))\n    )\n    val_dataset = TensorDataset(\n        torch.FloatTensor(X_val), \n        torch.FloatTensor(y_val.reshape(-1, 1))\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=Config.NN_BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=Config.NN_BATCH_SIZE)\n    \n    # Create model\n    model = SimpleNN(X_train.shape[1]).to(device)\n    \n    # Training setup\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=Config.NN_LEARNING_RATE)\n    \n    best_val_loss = float('inf')\n    best_model_state = model.state_dict()\n    patience_counter = 0\n    \n    for epoch in range(Config.NN_EPOCHS):\n        # Training\n        model.train()\n        train_losses = []\n        \n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n            train_losses.append(loss.item())\n        \n        # Validation\n        model.eval()\n        val_losses = []\n        \n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                val_losses.append(loss.item())\n        \n        avg_val_loss = np.mean(val_losses)\n        \n        # Early stopping\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_model_state = model.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= 10:\n                break\n    \n    # Load best model\n    model.load_state_dict(best_model_state)\n    \n    return model\n\n# ================================================================================\n# RESIDUAL MODELING (COMMENTED OUT)\n# ================================================================================\n\n# def train_residual_model(train_data, base_predictions_x, base_predictions_y, features):\n#     \"\"\"Train model to predict residuals from base predictions\"\"\"\n#     print(\"Training residual models...\")\n#     \n#     # Calculate residuals\n#     residual_x = train_data['displacement_x'].values - base_predictions_x\n#     residual_y = train_data['displacement_y'].values - base_predictions_y\n#     \n#     # Train lightweight model on residuals for X\n#     residual_model_x = LGBMRegressor(\n#         n_estimators=200,\n#         learning_rate=0.01,\n#         max_depth=4,\n#         num_leaves=31,\n#         min_child_samples=100,\n#         subsample=0.8,\n#         random_state=42,\n#         verbosity=-1\n#     )\n#     residual_model_x.fit(train_data[features].values, residual_x)\n#     \n#     # Train lightweight model on residuals for Y\n#     residual_model_y = LGBMRegressor(\n#         n_estimators=200,\n#         learning_rate=0.01,\n#         max_depth=4,\n#         num_leaves=31,\n#         min_child_samples=100,\n#         subsample=0.8,\n#         random_state=42,\n#         verbosity=-1\n#     )\n#     residual_model_y.fit(train_data[features].values, residual_y)\n#     \n#     return residual_model_x, residual_model_y\n\n# def apply_residual_correction(base_pred_x, base_pred_y, residual_models, X_test, max_correction=2.0):\n#     \"\"\"Apply residual corrections with guardrails\"\"\"\n#     residual_model_x, residual_model_y = residual_models\n#     \n#     # Predict residuals\n#     residual_x = residual_model_x.predict(X_test)\n#     residual_y = residual_model_y.predict(X_test)\n#     \n#     # Apply guardrails to limit residual magnitude\n#     residual_x = np.clip(residual_x, -max_correction, max_correction)\n#     residual_y = np.clip(residual_y, -max_correction, max_correction)\n#     \n#     # Apply residuals\n#     corrected_x = base_pred_x + residual_x\n#     corrected_y = base_pred_y + residual_y\n#     \n#     return corrected_x, corrected_y\n\n# ================================================================================\n# ENSEMBLE TRAINING\n# ================================================================================\n\ndef train_ensemble(train_data, features, seed=42):\n    \"\"\"Train complete ensemble\"\"\"\n    \n    print(f\"\\nTraining ensemble with seed {seed}...\")\n    \n    # Prepare data\n    X = train_data[features].values\n    y_dx = train_data['displacement_x'].values\n    y_dy = train_data['displacement_y'].values\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Cross-validation\n    groups = train_data['game_id'].values\n    gkf = GroupKFold(n_splits=Config.N_FOLDS)\n    \n    # Store models\n    models_dx = {'xgb': [], 'lgb': [], 'cat': [], 'nn': []}\n    models_dy = {'xgb': [], 'lgb': [], 'cat': [], 'nn': []}\n    \n    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, groups=groups)):\n        print(f\"  Fold {fold + 1}/{Config.N_FOLDS}\")\n        \n        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n        y_train_dx, y_val_dx = y_dx[train_idx], y_dx[val_idx]\n        y_train_dy, y_val_dy = y_dy[train_idx], y_dy[val_idx]\n        \n        # XGBoost\n        xgb_dx = XGBRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            max_depth=8,\n            subsample=0.8,\n            random_state=seed + fold,\n            tree_method='hist',\n            verbosity=0\n        )\n        xgb_dx.fit(X_train, y_train_dx)\n        models_dx['xgb'].append(xgb_dx)\n        \n        xgb_dy = XGBRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            max_depth=8,\n            subsample=0.8,\n            random_state=seed + fold + 100,\n            tree_method='hist',\n            verbosity=0\n        )\n        xgb_dy.fit(X_train, y_train_dy)\n        models_dy['xgb'].append(xgb_dy)\n        \n        # LightGBM\n        lgb_dx = LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            max_depth=8,\n            num_leaves=100,\n            subsample=0.8,\n            random_state=seed + fold,\n            verbosity=-1\n        )\n        lgb_dx.fit(X_train, y_train_dx)\n        models_dx['lgb'].append(lgb_dx)\n        \n        lgb_dy = LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            max_depth=8,\n            num_leaves=100,\n            subsample=0.8,\n            random_state=seed + fold + 100,\n            verbosity=-1\n        )\n        lgb_dy.fit(X_train, y_train_dy)\n        models_dy['lgb'].append(lgb_dy)\n        \n        # CatBoost\n        cat_dx = CatBoostRegressor(\n            iterations=1000,\n            learning_rate=0.05,\n            depth=8,\n            random_seed=seed + fold,\n            verbose=False\n        )\n        cat_dx.fit(X_train, y_train_dx)\n        models_dx['cat'].append(cat_dx)\n        \n        cat_dy = CatBoostRegressor(\n            iterations=1000,\n            learning_rate=0.05,\n            depth=8,\n            random_seed=seed + fold + 100,\n            verbose=False\n        )\n        cat_dy.fit(X_train, y_train_dy)\n        models_dy['cat'].append(cat_dy)\n        \n        # Neural Network\n        nn_dx = train_neural_network(X_train, y_train_dx, X_val, y_val_dx, seed + fold)\n        models_dx['nn'].append(nn_dx)\n        \n        nn_dy = train_neural_network(X_train, y_train_dy, X_val, y_val_dy, seed + fold + 100)\n        models_dy['nn'].append(nn_dy)\n    \n    return models_dx, models_dy, scaler\n\ndef predict_ensemble(models_dx, models_dy, scaler, X_test):\n    \"\"\"Generate predictions from ensemble\"\"\"\n    \n    X_scaled = scaler.transform(X_test)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Predictions for dx\n    all_preds_dx = []\n    \n    # Tree models\n    for model_type in ['xgb', 'lgb', 'cat']:\n        preds = []\n        for model in models_dx[model_type]:\n            preds.append(model.predict(X_scaled))\n        all_preds_dx.append(np.mean(preds, axis=0))\n    \n    # Neural network\n    nn_preds = []\n    X_tensor = torch.FloatTensor(X_scaled).to(device)\n    for model in models_dx['nn']:\n        model.eval()\n        with torch.no_grad():\n            pred = model(X_tensor).cpu().numpy().squeeze()\n        nn_preds.append(pred)\n    all_preds_dx.append(np.mean(nn_preds, axis=0))\n    \n    # Average all models\n    pred_dx = np.mean(all_preds_dx, axis=0)\n    \n    # Predictions for dy\n    all_preds_dy = []\n    \n    # Tree models\n    for model_type in ['xgb', 'lgb', 'cat']:\n        preds = []\n        for model in models_dy[model_type]:\n            preds.append(model.predict(X_scaled))\n        all_preds_dy.append(np.mean(preds, axis=0))\n    \n    # Neural network\n    nn_preds = []\n    for model in models_dy['nn']:\n        model.eval()\n        with torch.no_grad():\n            pred = model(X_tensor).cpu().numpy().squeeze()\n        nn_preds.append(pred)\n    all_preds_dy.append(np.mean(nn_preds, axis=0))\n    \n    # Average all models\n    pred_dy = np.mean(all_preds_dy, axis=0)\n    \n    return pred_dx, pred_dy\n\n# ================================================================================\n# POST-PROCESSING\n# ================================================================================\n\ndef apply_constraints(pred_x, pred_y, x_last, y_last, time_seconds):\n    \"\"\"Apply physics constraints\"\"\"\n    \n    dx = pred_x - x_last\n    dy = pred_y - y_last\n    displacement = np.sqrt(dx**2 + dy**2)\n    \n    max_displacement = Config.MAX_SPEED * time_seconds\n    \n    # Scale down impossible movements\n    mask = displacement > max_displacement\n    if np.any(mask):\n        scale = max_displacement[mask] / (displacement[mask] + 1e-6)\n        dx[mask] *= scale\n        dy[mask] *= scale\n        pred_x[mask] = x_last[mask] + dx[mask]\n        pred_y[mask] = y_last[mask] + dy[mask]\n    \n    # Clip to field boundaries\n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    return pred_x, pred_y\n\ndef smooth_trajectories(test_data, pred_x, pred_y):\n    \"\"\"Smooth trajectories\"\"\"\n    \n    test_data = test_data.copy()\n    test_data['pred_x'] = pred_x\n    test_data['pred_y'] = pred_y\n    \n    for (game_id, play_id, nfl_id), group in test_data.groupby(['game_id', 'play_id', 'nfl_id']):\n        if len(group) > 3:\n            idx = group.index\n            test_data.loc[idx, 'pred_x'] = gaussian_filter1d(group['pred_x'].values, sigma=0.5)\n            test_data.loc[idx, 'pred_y'] = gaussian_filter1d(group['pred_y'].values, sigma=0.5)\n    \n    return test_data['pred_x'].values, test_data['pred_y'].values\n\n# ================================================================================\n# MAIN PIPELINE\n# ================================================================================\n\ndef main():\n    \"\"\"Main execution pipeline\"\"\"\n    \n    print(\"=\"*80)\n    print(\" NFL BIG DATA BOWL 2026 - ENHANCED WITH TEMPORAL FEATURES + PLAYER INTERACTIONS\")\n    print(\"=\"*80)\n    \n    # Load data\n    train_input, train_output, test_input, test_template = load_data()\n    \n    # Prepare features\n    print(\"\\nPreparing enhanced temporal features with player interactions...\")\n    train_data = prepare_features(train_input, train_output, is_training=True)\n    test_data = prepare_features(test_input, test_template, is_training=False)\n    \n    print(f\"Train shape: {train_data.shape}\")\n    print(f\"Test shape: {test_data.shape}\")\n    \n    # Define features to use - including new temporal features and player interactions\n    feature_cols = [\n        # Original position and movement features\n        'x_last', 'y_last', 's', 'a', 'o', 'dir',\n        \n        # Basic temporal features\n        'time_seconds', 'time_normalized', 'time_squared',\n        \n        # Advanced temporal features\n        'time_cubed', 'sqrt_time', 'log_time',\n        'time_sin', 'time_cos', 'time_sin_2', 'time_cos_2',\n        'is_early_play', 'is_mid_play', 'is_late_play',\n        \n        # Historical statistics from input frames\n        'x_mean', 'x_std', 'x_min', 'x_max',\n        'y_mean', 'y_std', 'y_min', 'y_max',\n        's_mean', 's_std', 's_max', 's_min',\n        'a_mean', 'a_std', 'a_max', 'a_min',\n        'dir_change_rate', 'orientation_change_rate',\n        'recent_displacement_x', 'recent_displacement_y', 'acceleration_trend',\n        \n        # Movement consistency\n        'speed_consistency', 'speed_deviation',\n        'acceleration_consistency', 'acceleration_deviation',\n        \n        # Velocity and expected positions\n        'velocity_x', 'velocity_y',\n        'expected_x_constant_v', 'expected_y_constant_v',\n        'expected_x_with_accel', 'expected_y_with_accel',\n        \n        # Ball-related features\n        'distance_to_ball', 'angle_to_ball', 'closing_speed',\n        'ball_direction_x', 'ball_direction_y',\n        'estimated_time_to_ball', 'time_ratio_to_ball',\n        'projected_time_to_ball', 'time_urgency',\n        'distance_to_ball_x_time', 'distance_to_ball_x_time_squared',\n        \n        # Target features\n        'distance_to_target', 'is_target', 'angle_to_target',\n        'distance_to_target_x_time', 'is_target_x_time_squared',\n        \n        # Field position\n        'x_normalized', 'y_normalized',\n        'distance_from_sideline', 'distance_from_endzone',\n        \n        # Role features\n        'is_offense', 'is_passer', 'is_coverage',\n        'is_redzone',\n        \n        # Interaction features\n        'is_target_x_time', 'distance_ball_x_speed',\n        'time_x_speed', 'time_x_acceleration', 'time_squared_x_speed',\n        'is_offense_x_early_play', 'is_offense_x_late_play', 'is_target_x_late_play',\n        \n        # Other features\n        'absolute_yardline_number', 'player_weight',\n        \n        # NEW: Player interaction features\n        'nearest_teammate_dist', 'avg_teammate_dist', 'teammates_within_5', 'teammates_within_10',\n        'nearest_opponent_dist', 'avg_opponent_dist', 'opponents_within_5', 'opponents_within_10',\n        'pressure_index', 'local_density', 'team_spread_x', 'team_spread_y',\n        'nearest_coverage_dist', 'coverage_players_nearby',\n        'nearest_opp_dx', 'nearest_opp_dy', 'nearest_opp_angle',\n        'pressure_x_time', 'pressure_x_late_play',\n        'opponent_closing_time', 'space_urgency'\n    ]\n    \n    # Add momentum if available\n    if 'momentum_magnitude' in train_data.columns:\n        feature_cols.append('momentum_magnitude')\n    \n    # Filter available features\n    feature_cols = [f for f in feature_cols if f in train_data.columns]\n    print(f\"\\nUsing {len(feature_cols)} features including temporal enhancements and player interactions\")\n    print(f\"New features added:\")\n    print(\"  - Player-to-player distances and counts\")\n    print(\"  - Pressure indices and space urgency\")\n    print(\"  - Team formation spread and density\")\n    print(\"  - Temporal interaction combinations\")\n    \n    # Train ensembles with different seeds\n    all_predictions_dx = []\n    all_predictions_dy = []\n    \n    # For residual modeling (commented out)\n    # all_train_predictions_dx = []\n    # all_train_predictions_dy = []\n    \n    for seed in Config.SEEDS:\n        # Train\n        models_dx, models_dy, scaler = train_ensemble(train_data, feature_cols, seed)\n        \n        # Predict for test\n        X_test = test_data[feature_cols].values\n        pred_dx, pred_dy = predict_ensemble(models_dx, models_dy, scaler, X_test)\n        \n        all_predictions_dx.append(pred_dx)\n        all_predictions_dy.append(pred_dy)\n        \n        # For residual modeling (commented out)\n        # X_train = train_data[feature_cols].values\n        # train_pred_dx, train_pred_dy = predict_ensemble(models_dx, models_dy, scaler, X_train)\n        # all_train_predictions_dx.append(train_pred_dx)\n        # all_train_predictions_dy.append(train_pred_dy)\n        \n        # Clean up memory\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n        gc.collect()\n    \n    # Average predictions\n    print(\"\\nAveraging predictions...\")\n    final_pred_dx = np.mean(all_predictions_dx, axis=0)\n    final_pred_dy = np.mean(all_predictions_dy, axis=0)\n    \n    # Residual modeling (commented out)\n    # print(\"\\nTraining residual models...\")\n    # train_pred_dx_mean = np.mean(all_train_predictions_dx, axis=0)\n    # train_pred_dy_mean = np.mean(all_train_predictions_dy, axis=0)\n    # \n    # residual_model_x, residual_model_y = train_residual_model(\n    #     train_data, train_pred_dx_mean, train_pred_dy_mean, feature_cols\n    # )\n    # \n    # print(\"Applying residual corrections...\")\n    # final_pred_dx, final_pred_dy = apply_residual_correction(\n    #     final_pred_dx, final_pred_dy, \n    #     (residual_model_x, residual_model_y),\n    #     test_data[feature_cols].values,\n    #     max_correction=1.0  # Limit residual correction magnitude\n    # )\n    \n    # Calculate absolute positions\n    pred_x = test_data['x_last'].values + final_pred_dx\n    pred_y = test_data['y_last'].values + final_pred_dy\n    \n    # Apply constraints\n    print(\"Applying physics constraints...\")\n    pred_x, pred_y = apply_constraints(\n        pred_x, pred_y,\n        test_data['x_last'].values,\n        test_data['y_last'].values,\n        test_data['time_seconds'].values\n    )\n    \n    # Smooth trajectories\n    print(\"Smoothing trajectories...\")\n    pred_x, pred_y = smooth_trajectories(test_data, pred_x, pred_y)\n    \n    # Final clipping\n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    # Create submission\n    print(\"\\nCreating submission...\")\n    submission = pd.DataFrame({\n        'id': (test_data['game_id'].astype(str) + \"_\" +\n               test_data['play_id'].astype(str) + \"_\" +\n               test_data['nfl_id'].astype(str) + \"_\" +\n               test_data['frame_id'].astype(str)),\n        'x': pred_x,\n        'y': pred_y\n    })\n    \n    submission.to_csv(\"submission.csv\", index=False)\n    \n    print(f\"\\nâœ… Submission saved: {len(submission)} predictions\")\n    print(f\"X: mean={submission['x'].mean():.2f}, std={submission['x'].std():.2f}\")\n    print(f\"Y: mean={submission['y'].mean():.2f}, std={submission['y'].std():.2f}\")\n    \n    print(\"\\nFirst 5 predictions:\")\n    print(submission.head())\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" COMPLETE!\")\n    print(\"=\"*80)\n    \n    return submission\n\nif __name__ == \"__main__\":\n    submission = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}