{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nwarnings.filterwarnings('ignore')\n\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    SEEDS = [42, 123, 2024]  # 3 seeds for better ensemble\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    MAX_SPEED = 12.0\n    N_FOLDS = 5  # More folds for robustness\n\ndef load_data():\n    print(\"Loading data...\")\n    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    train_input_files = [f for f in train_input_files if f.exists()]\n    train_output_files = [f for f in train_output_files if f.exists()]\n    \n    train_input = pd.concat([pd.read_csv(f) for f in tqdm(train_input_files, desc=\"Loading train input\")], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in tqdm(train_output_files, desc=\"Loading train output\")], ignore_index=True)\n    \n    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n    \n    print(f\"Loaded {len(train_input):,} input rows, {len(train_output):,} output rows\")\n    return train_input, train_output, test_input, test_template\n\ndef prepare_features(input_df, output_df, is_training=True):\n    print(\"Preparing features...\")\n    \n    # Get last frame AND temporal statistics\n    last_frame = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']) \\\n                         .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False).last()\n    \n    # Temporal aggregations\n    temporal_stats = input_df.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n        's': ['mean', 'std', 'max', 'min'],\n        'a': ['mean', 'std', 'max'],\n        'x': ['mean', 'std'],\n        'y': ['mean', 'std'],\n        'dir': lambda x: np.std(np.diff(x)) if len(x) > 1 else 0\n    }).reset_index()\n    temporal_stats.columns = ['_'.join(col).strip() if col[1] else col[0] \n                              for col in temporal_stats.columns.values]\n    \n    last_frame = last_frame.merge(temporal_stats, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    # Merge with output\n    if is_training:\n        df = output_df.merge(last_frame, on=['game_id', 'play_id', 'nfl_id'], how='left', suffixes=('', '_last'))\n    else:\n        df = output_df.merge(last_frame, on=['game_id', 'play_id', 'nfl_id'], how='left', suffixes=('', '_last'))\n    \n    if 'x_last' not in df.columns:\n        df = df.rename(columns={'x': 'x_last', 'y': 'y_last'})\n    \n    # Time features\n    df['time_seconds'] = df['frame_id'] / 10.0\n    df['time_squared'] = df['time_seconds'] ** 2\n    df['time_cubed'] = df['time_seconds'] ** 3\n    df['sqrt_time'] = np.sqrt(df['time_seconds'])\n    \n    # Velocity components\n    if 'dir' in df.columns and 's' in df.columns:\n        dir_rad = np.deg2rad(df['dir'].fillna(0))\n        df['velocity_x'] = df['s'] * np.sin(dir_rad)\n        df['velocity_y'] = df['s'] * np.cos(dir_rad)\n        df['velocity_magnitude'] = df['s']\n    else:\n        df['velocity_x'] = 0\n        df['velocity_y'] = 0\n        df['velocity_magnitude'] = 0\n    \n    # Physics-based predictions (baseline)\n    df['expected_x'] = df['x_last'] + df['velocity_x'] * df['time_seconds']\n    df['expected_y'] = df['y_last'] + df['velocity_y'] * df['time_seconds']\n    \n    if 'a' in df.columns:\n        df['expected_x_accel'] = df['expected_x'] + 0.5 * df['a'] * np.sin(dir_rad) * df['time_squared']\n        df['expected_y_accel'] = df['expected_y'] + 0.5 * df['a'] * np.cos(dir_rad) * df['time_squared']\n        df['kinetic_energy'] = 0.5 * df['s'] ** 2  # Assuming unit mass\n    else:\n        df['expected_x_accel'] = df['expected_x']\n        df['expected_y_accel'] = df['expected_y']\n        df['kinetic_energy'] = 0\n    \n    # Ball features\n    if 'ball_land_x' in df.columns:\n        ball_dx = df['ball_land_x'] - df['x_last']\n        ball_dy = df['ball_land_y'] - df['y_last']\n        df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        df['ball_direction_x'] = ball_dx / (df['distance_to_ball'] + 1e-6)\n        df['ball_direction_y'] = ball_dy / (df['distance_to_ball'] + 1e-6)\n        \n        # Alignment with ball\n        if 'velocity_x' in df.columns:\n            df['velocity_toward_ball'] = (df['velocity_x'] * ball_dx + df['velocity_y'] * ball_dy) / (df['distance_to_ball'] + 1e-6)\n            df['angle_between_velocity_and_ball'] = np.arccos(np.clip(\n                (df['velocity_x'] * ball_dx + df['velocity_y'] * ball_dy) / \n                (np.sqrt(df['velocity_x']**2 + df['velocity_y']**2) * df['distance_to_ball'] + 1e-6),\n                -1, 1\n            ))\n    else:\n        df['distance_to_ball'] = 0\n        df['angle_to_ball'] = 0\n        df['ball_direction_x'] = 0\n        df['ball_direction_y'] = 0\n        df['velocity_toward_ball'] = 0\n        df['angle_between_velocity_and_ball'] = 0\n    \n    # Position features\n    df['x_normalized'] = df['x_last'] / Config.FIELD_X_MAX\n    df['y_normalized'] = df['y_last'] / Config.FIELD_Y_MAX\n    df['distance_from_sideline'] = np.minimum(df['y_last'], Config.FIELD_Y_MAX - df['y_last'])\n    df['near_sideline'] = (df['distance_from_sideline'] < 5).astype(int)\n    \n    # Role features\n    df['is_offense'] = (df['player_side'] == 'Offense').astype(int) if 'player_side' in df.columns else 0\n    df['is_target'] = (df['player_role'] == 'Targeted Receiver').astype(int) if 'player_role' in df.columns else 0\n    df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int) if 'player_role' in df.columns else 0\n    \n    # Interaction features\n    df['is_target_x_time'] = df['is_target'] * df['time_seconds']\n    df['distance_to_ball_x_time'] = df['distance_to_ball'] * df['time_seconds']\n    df['speed_x_time'] = df['velocity_magnitude'] * df['time_seconds']\n    \n    # Historical consistency\n    if 's_mean' in df.columns:\n        df['speed_vs_avg'] = df['s'] - df['s_mean']\n        df['speed_consistency'] = df['s'] / (df['s_mean'] + 0.1)\n    \n    # Training targets\n    if is_training and 'x' in df.columns:\n        df['displacement_x'] = df['x'] - df['x_last']\n        df['displacement_y'] = df['y'] - df['y_last']\n        \n        # Filter extreme outliers\n        max_displacement = Config.MAX_SPEED * df['time_seconds'] * 2.0\n        displacement = np.sqrt(df['displacement_x']**2 + df['displacement_y']**2)\n        valid_mask = (displacement <= max_displacement) & df['displacement_x'].notna() & df['displacement_y'].notna()\n        df = df[valid_mask].reset_index(drop=True)\n    \n    # Fill NaN\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = df[numeric_cols].fillna(0)\n    \n    return df\n\ndef train_model(train_data, features, target, seed=42):\n    print(f\"\\n  Training for {target}...\")\n    X = train_data[features].values\n    y = train_data[target].values\n    groups = train_data['game_id'].values\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    gkf = GroupKFold(n_splits=Config.N_FOLDS)\n    models = []\n    val_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_scaled, groups=groups)):\n        print(f\"    Fold {fold + 1}/{Config.N_FOLDS}...\", end=\" \")\n        fold_start = time.time()\n        \n        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        # XGBoost\n        model_xgb = XGBRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            max_depth=8,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed + fold,\n            tree_method='hist',\n            verbosity=0,\n            n_jobs=-1\n        )\n        model_xgb.fit(\n            X_train, y_train, \n            eval_set=[(X_val, y_val)], \n            early_stopping_rounds=50, \n            verbose=False\n        )\n        \n        # LightGBM for diversity\n        model_lgb = LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            max_depth=8,\n            subsample=0.8,\n            random_state=seed + fold + 100,\n            verbosity=-1,\n            n_jobs=-1\n        )\n        model_lgb.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            callbacks=[lgb.early_stopping(50, verbose=False)]\n        )\n        \n        # Ensemble both\n        pred_xgb = model_xgb.predict(X_val)\n        pred_lgb = model_lgb.predict(X_val)\n        pred_ensemble = 0.6 * pred_xgb + 0.4 * pred_lgb\n        \n        val_score = np.sqrt(np.mean((pred_ensemble - y_val) ** 2))\n        val_scores.append(val_score)\n        \n        models.append({'xgb': model_xgb, 'lgb': model_lgb})\n        \n        fold_time = time.time() - fold_start\n        print(f\"RMSE: {val_score:.4f} ({fold_time:.1f}s)\")\n    \n    mean_score = np.mean(val_scores)\n    std_score = np.std(val_scores)\n    print(f\"    CV Score: {mean_score:.4f} ± {std_score:.4f}\")\n    \n    return models, scaler\n\ndef predict_model(models, scaler, X_test):\n    X_scaled = scaler.transform(X_test)\n    predictions = []\n    \n    for model_dict in models:\n        pred_xgb = model_dict['xgb'].predict(X_scaled)\n        pred_lgb = model_dict['lgb'].predict(X_scaled)\n        pred = 0.6 * pred_xgb + 0.4 * pred_lgb\n        predictions.append(pred)\n    \n    return np.mean(predictions, axis=0)\n\ndef main():\n    print(\"=\"*80)\n    print(\" NFL BIG DATA BOWL 2026 - ROBUST TRAINING VERSION\")\n    print(\"=\"*80)\n    \n    start_time = time.time()\n    \n    train_input, train_output, test_input, test_template = load_data()\n    \n    # Prepare data\n    train_data = prepare_features(train_input, train_output, is_training=True)\n    test_data = prepare_features(test_input, test_template, is_training=False)\n    \n    print(f\"\\nTrain: {train_data.shape}, Test: {test_data.shape}\")\n    print(f\"Test template rows: {len(test_template)}\")\n    \n    # Select features\n    exclude = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y', \n               'displacement_x', 'displacement_y', 'x_last', 'y_last']\n    features = [col for col in train_data.select_dtypes(include=[np.number]).columns \n                if col not in exclude]\n    \n    print(f\"Using {len(features)} features\")\n    print(f\"Training {Config.N_FOLDS} folds × {len(Config.SEEDS)} seeds × 2 targets × 2 models = {Config.N_FOLDS * len(Config.SEEDS) * 2 * 2} total models\")\n    \n    # Train ensemble\n    all_preds_x = []\n    all_preds_y = []\n    \n    for i, seed in enumerate(Config.SEEDS):\n        print(f\"\\n{'='*80}\")\n        print(f\"SEED {i+1}/{len(Config.SEEDS)}: {seed}\")\n        print('='*80)\n        \n        seed_start = time.time()\n        \n        models_x, scaler_x = train_model(train_data, features, 'displacement_x', seed)\n        models_y, scaler_y = train_model(train_data, features, 'displacement_y', seed + 1000)\n        \n        X_test = test_data[features].values\n        pred_dx = predict_model(models_x, scaler_x, X_test)\n        pred_dy = predict_model(models_y, scaler_y, X_test)\n        \n        all_preds_x.append(pred_dx)\n        all_preds_y.append(pred_dy)\n        \n        seed_time = time.time() - seed_start\n        print(f\"\\nSeed {seed} completed in {seed_time:.1f}s\")\n    \n    # Average predictions\n    print(\"\\nAveraging ensemble predictions...\")\n    final_dx = np.mean(all_preds_x, axis=0)\n    final_dy = np.mean(all_preds_y, axis=0)\n    \n    # Calculate final positions\n    pred_x = test_data['x_last'].values + final_dx\n    pred_y = test_data['y_last'].values + final_dy\n    \n    # Apply physics constraints\n    print(\"Applying physics constraints...\")\n    dx = pred_x - test_data['x_last'].values\n    dy = pred_y - test_data['y_last'].values\n    displacement = np.sqrt(dx**2 + dy**2)\n    max_displacement = Config.MAX_SPEED * test_data['time_seconds'].values\n    \n    violations = np.sum(displacement > max_displacement)\n    if violations > 0:\n        print(f\"  Constraining {violations} predictions exceeding max speed\")\n        mask = displacement > max_displacement\n        scale = max_displacement[mask] / (displacement[mask] + 1e-6)\n        pred_x[mask] = test_data['x_last'].values[mask] + dx[mask] * scale\n        pred_y[mask] = test_data['y_last'].values[mask] + dy[mask] * scale\n    \n    # Clip to field\n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    # Create submission\n    test_data['pred_x'] = pred_x\n    test_data['pred_y'] = pred_y\n    \n    submission = pd.DataFrame({\n        'id': (test_data['game_id'].astype(str) + \"_\" +\n               test_data['play_id'].astype(str) + \"_\" +\n               test_data['nfl_id'].astype(str) + \"_\" +\n               test_data['frame_id'].astype(str)),\n        'x': test_data['pred_x'].astype(float),\n        'y': test_data['pred_y'].astype(float)\n    })\n    \n    # Verify completeness\n    expected_ids = set(test_template['game_id'].astype(str) + '_' + \n                      test_template['play_id'].astype(str) + '_' + \n                      test_template['nfl_id'].astype(str) + '_' + \n                      test_template['frame_id'].astype(str))\n    actual_ids = set(submission['id'])\n    \n    missing = expected_ids - actual_ids\n    if missing:\n        print(f\"\\nWARNING: {len(missing)} missing IDs - adding fallback predictions\")\n        missing_rows = test_template[\n            (test_template['game_id'].astype(str) + '_' + \n             test_template['play_id'].astype(str) + '_' + \n             test_template['nfl_id'].astype(str) + '_' + \n             test_template['frame_id'].astype(str)).isin(missing)\n        ].copy()\n        \n        missing_with_last = missing_rows.merge(\n            test_input.groupby(['game_id', 'play_id', 'nfl_id']).last()[['x', 'y']],\n            on=['game_id', 'play_id', 'nfl_id'],\n            how='left'\n        )\n        \n        missing_submission = pd.DataFrame({\n            'id': (missing_rows['game_id'].astype(str) + \"_\" +\n                   missing_rows['play_id'].astype(str) + \"_\" +\n                   missing_rows['nfl_id'].astype(str) + \"_\" +\n                   missing_rows['frame_id'].astype(str)),\n            'x': missing_with_last['x'].fillna(60.0).astype(float),\n            'y': missing_with_last['y'].fillna(26.65).astype(float)\n        })\n        \n        submission = pd.concat([submission, missing_submission], ignore_index=True)\n    \n    # Final validation\n    submission = submission.drop_duplicates('id', keep='first')\n    submission = submission[['id', 'x', 'y']]\n    submission['x'] = submission['x'].fillna(60.0).astype(float)\n    submission['y'] = submission['y'].fillna(26.65).astype(float)\n    submission = submission.sort_values('id').reset_index(drop=True)\n    \n    # Save\n    submission.to_csv(\"submission.csv\", index=False)\n    \n    total_time = time.time() - start_time\n    \n    print(f\"\\n{'='*80}\")\n    print(\"FINAL RESULTS\")\n    print('='*80)\n    print(f\"✅ Submission: {len(submission)} rows\")\n    print(f\"Expected: {len(test_template)} rows\")\n    print(f\"Match: {len(submission) == len(test_template)}\")\n    print(f\"NaN values: {submission.isna().sum().sum()}\")\n    print(f\"Total runtime: {total_time/60:.1f} minutes\")\n    print(f\"\\nPrediction statistics:\")\n    print(f\"  X: mean={submission['x'].mean():.2f}, std={submission['x'].std():.2f}\")\n    print(f\"  Y: mean={submission['y'].mean():.2f}, std={submission['y'].std():.2f}\")\n    print(\"\\nFirst 5 rows:\")\n    print(submission.head())\n    \n    return submission\n\nif __name__ == \"__main__\":\n    import lightgbm as lgb\n    submission = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}