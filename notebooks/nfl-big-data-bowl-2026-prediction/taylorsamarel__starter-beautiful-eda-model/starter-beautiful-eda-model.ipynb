{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# NFL BIG DATA BOWL 2026 - ADVANCED MODEL ZOO WITH INTELLIGENT ENSEMBLING\n# ================================================================================\n\nimport os\nimport gc\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom scipy import stats, signal\nfrom scipy.ndimage import gaussian_filter1d\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ML Libraries\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import (RandomForestRegressor, ExtraTreesRegressor, \n                            GradientBoostingRegressor, VotingRegressor, \n                            HistGradientBoostingRegressor, BaggingRegressor)\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor, RANSACRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel\n\n# Gradient Boosting\ntry:\n    from lightgbm import LGBMRegressor\n    HAS_LGBM = True\nexcept:\n    HAS_LGBM = False\n\ntry:\n    from xgboost import XGBRegressor\n    HAS_XGB = True\nexcept:\n    HAS_XGB = False\n\ntry:\n    from catboost import CatBoostRegressor\n    HAS_CAT = True\nexcept:\n    HAS_CAT = False\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\nprint(\"=\"*90)\nprint(\" \"*15 + \"NFL BIG DATA BOWL 2026 - ADVANCED MODEL ZOO\")\nprint(\" \"*20 + \"Intelligent Ensemble with Physics Constraints\")\nprint(\"=\"*90)\n\n# ================================================================================\n# CONFIGURATION\n# ================================================================================\n\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    SEED = 42\n    N_FOLDS = 5\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    MAX_SPEED = 12.0  # Max realistic speed in yards/second\n    MAX_ACCELERATION = 10.0  # Max realistic acceleration\n    USE_GPU = False  # Set to True if GPU available\n    ENABLE_NEURAL = True  # Neural networks can be slow\n    MIN_MODEL_PERFORMANCE = 0.3  # Models worse than this R¬≤ are excluded\n\n# ================================================================================\n# DATA LOADING\n# ================================================================================\n\ndef load_data():\n    \"\"\"Load all training and test data with error handling\"\"\"\n    try:\n        print(\"\\nüìä Loading Data...\")\n        \n        # Training data paths\n        input_files = sorted([Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" \n                             for w in range(1, 19)])\n        output_files = sorted([Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" \n                              for w in range(1, 19)])\n        \n        # Filter existing files only\n        input_files = [f for f in input_files if f.exists()]\n        output_files = [f for f in output_files if f.exists()]\n        \n        if not input_files or not output_files:\n            raise FileNotFoundError(\"Training files not found\")\n        \n        # Load with progress bar\n        df_in = pd.concat([pd.read_csv(f) for f in tqdm(input_files, desc=\"Input files\")], \n                         ignore_index=True)\n        df_out = pd.concat([pd.read_csv(f) for f in tqdm(output_files, desc=\"Output files\")], \n                          ignore_index=True)\n        \n        # Test data\n        test_in = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n        test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n        \n        print(f\"‚úì Data loaded: Train Input {df_in.shape}, Train Output {df_out.shape}\")\n        print(f\"‚úì Test Input {test_in.shape}, Test Template {test_template.shape}\")\n        \n        return df_in, df_out, test_in, test_template\n    \n    except Exception as e:\n        print(f\"‚ùå Error loading data: {e}\")\n        raise\n\n# ================================================================================\n# FEATURE ENGINEERING\n# ================================================================================\n\ndef height_to_inches(height_str):\n    \"\"\"Convert height from feet-inches format to total inches\"\"\"\n    if not isinstance(height_str, str) or '-' not in height_str:\n        return np.nan\n    try:\n        feet, inches = map(int, height_str.split('-'))\n        return feet * 12 + inches\n    except:\n        return np.nan\n\ndef create_features(df_in, df_out, test_in=None, test_template=None, is_train=True):\n    \"\"\"Comprehensive feature engineering with physics-based features\"\"\"\n    \n    try:\n        # Get data\n        df = df_in.copy() if is_train else test_in.copy()\n        \n        # Get last observation for each player\n        last_obs = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']).groupby(\n            ['game_id', 'play_id', 'nfl_id'], as_index=False\n        ).last()\n        \n        last_obs = last_obs.rename(columns={'x': 'x_last', 'y': 'y_last'})\n        \n        # Convert height to inches\n        if 'player_height' in last_obs.columns:\n            last_obs['height_inches'] = last_obs['player_height'].apply(height_to_inches)\n        \n        # Get target receiver info\n        targets = last_obs[last_obs['player_role'] == \"Targeted Receiver\"][\n            ['game_id', 'play_id', 'nfl_id', 'x_last', 'y_last']\n        ].copy()\n        \n        if len(targets) > 0:\n            targets = targets.rename(columns={\n                'nfl_id': 'target_nfl_id',\n                'x_last': 'target_x', \n                'y_last': 'target_y'\n            })\n            last_obs = last_obs.merge(\n                targets[['game_id', 'play_id', 'target_x', 'target_y', 'target_nfl_id']], \n                on=['game_id', 'play_id'], \n                how='left'\n            )\n        \n        # Columns to keep\n        keep_cols = ['game_id', 'play_id', 'nfl_id', 'x_last', 'y_last', \n                    's', 'a', 'o', 'dir', 'player_role', 'player_side', \n                    'player_position', 'ball_land_x', 'ball_land_y',\n                    'play_direction', 'absolute_yardline_number', \n                    'player_weight', 'height_inches', 'num_frames_output']\n        \n        if 'target_x' in last_obs.columns:\n            keep_cols.extend(['target_x', 'target_y', 'target_nfl_id'])\n        \n        keep_cols = [col for col in keep_cols if col in last_obs.columns]\n        \n        # Merge with output/template\n        if is_train:\n            result = df_out.merge(last_obs[keep_cols], on=['game_id', 'play_id', 'nfl_id'], \n                                 how='left')\n        else:\n            result = test_template.merge(last_obs[keep_cols], on=['game_id', 'play_id', 'nfl_id'], \n                                        how='left')\n        \n        # ============ Core Features ============\n        \n        # Time features\n        result['frame_offset'] = result['frame_id'].astype(float)\n        result['time_offset'] = result['frame_offset'] / 10.0\n        result['time_squared'] = result['time_offset'] ** 2\n        result['time_sqrt'] = np.sqrt(result['time_offset'])\n        result['time_log'] = np.log1p(result['time_offset'])\n        \n        # Normalize by total frames\n        if 'num_frames_output' in result.columns:\n            result['time_normalized'] = result['frame_offset'] / result['num_frames_output'].clip(lower=1)\n            result['time_remaining'] = 1 - result['time_normalized']\n        \n        # Distance to ball\n        dx_ball = result['ball_land_x'] - result['x_last']\n        dy_ball = result['ball_land_y'] - result['y_last']\n        result['dist_to_ball'] = np.sqrt(dx_ball**2 + dy_ball**2)\n        result['angle_to_ball'] = np.arctan2(dy_ball, dx_ball)\n        result['sin_angle_ball'] = np.sin(result['angle_to_ball'])\n        result['cos_angle_ball'] = np.cos(result['angle_to_ball'])\n        \n        # Target receiver features\n        if 'target_x' in result.columns:\n            dx_target = result['target_x'] - result['x_last']\n            dy_target = result['target_y'] - result['y_last']\n            result['dist_to_target'] = np.sqrt(dx_target**2 + dy_target**2)\n            result['angle_to_target'] = np.arctan2(dy_target, dx_target)\n            result['is_target'] = (result['nfl_id'] == result['target_nfl_id']).astype(int)\n            \n            # Relative positioning\n            result['target_ball_alignment'] = np.abs(result['angle_to_ball'] - result['angle_to_target'])\n            result['between_target_ball'] = (result['dist_to_target'] + result['dist_to_ball']) / 2\n        else:\n            result['dist_to_target'] = 0\n            result['is_target'] = 0\n        \n        # Velocity components\n        dir_rad = np.deg2rad(result['dir'])\n        ori_rad = np.deg2rad(result['o'])\n        result['vx'] = result['s'] * np.sin(dir_rad)\n        result['vy'] = result['s'] * np.cos(dir_rad)\n        result['v_magnitude'] = np.sqrt(result['vx']**2 + result['vy']**2)\n        \n        # Acceleration components\n        result['ax'] = result['a'] * np.sin(dir_rad)\n        result['ay'] = result['a'] * np.cos(dir_rad)\n        \n        # Direction alignment\n        result['dir_o_diff'] = np.abs(result['dir'] - result['o'])\n        result['dir_o_diff'] = np.minimum(result['dir_o_diff'], 360 - result['dir_o_diff'])\n        result['is_aligned'] = (result['dir_o_diff'] < 45).astype(int)\n        \n        # Field position features\n        result['dist_to_sideline'] = np.minimum(result['y_last'], Config.FIELD_Y_MAX - result['y_last'])\n        result['dist_to_endzone'] = np.minimum(result['x_last'], Config.FIELD_X_MAX - result['x_last'])\n        result['field_center_dist'] = np.abs(result['y_last'] - Config.FIELD_Y_MAX/2)\n        result['normalized_x'] = result['x_last'] / Config.FIELD_X_MAX\n        result['normalized_y'] = result['y_last'] / Config.FIELD_Y_MAX\n        \n        # Field zones (discretized positions)\n        result['x_zone'] = pd.cut(result['x_last'], bins=10, labels=False)\n        result['y_zone'] = pd.cut(result['y_last'], bins=5, labels=False)\n        \n        # Physics features\n        if 'player_weight' in result.columns:\n            result['momentum'] = result['s'] * result['player_weight'].fillna(200)\n            result['kinetic_energy'] = 0.5 * result['player_weight'].fillna(200) * result['s']**2\n        else:\n            result['momentum'] = result['s'] * 200\n            result['kinetic_energy'] = 0.5 * 200 * result['s']**2\n        \n        # BMI if height available\n        if 'height_inches' in result.columns and 'player_weight' in result.columns:\n            result['bmi'] = result['player_weight'] / (result['height_inches']**2) * 703\n        \n        # Movement potential (how far can player move given speed/acceleration)\n        result['max_displacement'] = (result['s'] * result['time_offset'] + \n                                     0.5 * result['a'] * result['time_offset']**2)\n        \n        # Closing speed to ball\n        if 'dist_to_ball' in result.columns:\n            ball_unit_x = dx_ball / (result['dist_to_ball'] + 1e-6)\n            ball_unit_y = dy_ball / (result['dist_to_ball'] + 1e-6)\n            result['closing_speed'] = result['vx'] * ball_unit_x + result['vy'] * ball_unit_y\n            result['tangential_speed'] = np.abs(result['vx'] * ball_unit_y - result['vy'] * ball_unit_x)\n        \n        # Interaction features\n        result['speed_acceleration_product'] = result['s'] * result['a']\n        result['speed_squared'] = result['s'] ** 2\n        result['acceleration_squared'] = result['a'] ** 2\n        \n        # Log transforms for skewed features\n        result['log_speed'] = np.log1p(result['s'])\n        result['log_dist_ball'] = np.log1p(result['dist_to_ball'])\n        result['log_dist_target'] = np.log1p(result['dist_to_target'])\n        \n        # Role-based features\n        result['is_offense'] = (result['player_side'] == 'Offense').astype(int)\n        result['is_defense'] = (result['player_side'] == 'Defense').astype(int)\n        result['is_passer'] = (result['player_role'] == 'Passer').astype(int)\n        \n        # Target variables for training\n        if is_train:\n            result['dx'] = result['x'] - result['x_last']\n            result['dy'] = result['y'] - result['y_last']\n            result['displacement'] = np.sqrt(result['dx']**2 + result['dy']**2)\n            \n            # Quality checks - remove unrealistic movements\n            max_possible_dist = Config.MAX_SPEED * result['time_offset'] * 2\n            result['is_valid'] = result['displacement'] <= max_possible_dist\n        \n        return result\n    \n    except Exception as e:\n        print(f\"‚ùå Error in feature engineering: {e}\")\n        raise\n\n# ================================================================================\n# MODEL ZOO\n# ================================================================================\n\nclass ModelZoo:\n    \"\"\"Comprehensive collection of diverse models\"\"\"\n    \n    def __init__(self, seed=42, enable_neural=True):\n        self.seed = seed\n        self.enable_neural = enable_neural\n        self.models = {}\n        self.build_zoo()\n    \n    def build_zoo(self):\n        \"\"\"Build diverse collection of models\"\"\"\n        \n        # Gradient Boosting Models\n        if HAS_LGBM:\n            self.models['lgbm_deep'] = LGBMRegressor(\n                n_estimators=1500, learning_rate=0.03, max_depth=12, \n                num_leaves=150, subsample=0.8, colsample_bytree=0.8,\n                reg_alpha=0.1, reg_lambda=0.1, random_state=self.seed, \n                verbosity=-1, n_jobs=-1\n            )\n            self.models['lgbm_shallow'] = LGBMRegressor(\n                n_estimators=2000, learning_rate=0.05, max_depth=5,\n                num_leaves=31, subsample=0.9, colsample_bytree=0.9,\n                random_state=self.seed+1, verbosity=-1, n_jobs=-1\n            )\n        \n        if HAS_XGB:\n            self.models['xgb_deep'] = XGBRegressor(\n                n_estimators=1500, learning_rate=0.03, max_depth=10,\n                subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1,\n                random_state=self.seed, tree_method='hist', n_jobs=-1\n            )\n            self.models['xgb_shallow'] = XGBRegressor(\n                n_estimators=2000, learning_rate=0.05, max_depth=4,\n                subsample=0.9, colsample_bytree=0.9,\n                random_state=self.seed+2, tree_method='hist', n_jobs=-1\n            )\n        \n        if HAS_CAT:\n            self.models['catboost'] = CatBoostRegressor(\n                iterations=1500, learning_rate=0.03, depth=8,\n                l2_leaf_reg=3, random_seed=self.seed, verbose=False\n            )\n        \n        # Tree-based Models\n        self.models['rf_deep'] = RandomForestRegressor(\n            n_estimators=300, max_depth=20, min_samples_split=5,\n            min_samples_leaf=2, random_state=self.seed, n_jobs=-1\n        )\n        \n        self.models['et_deep'] = ExtraTreesRegressor(\n            n_estimators=300, max_depth=20, min_samples_split=5,\n            min_samples_leaf=2, random_state=self.seed+3, n_jobs=-1\n        )\n        \n        self.models['gbm'] = GradientBoostingRegressor(\n            n_estimators=500, learning_rate=0.05, max_depth=6,\n            subsample=0.8, random_state=self.seed+4\n        )\n        \n        self.models['hist_gbm'] = HistGradientBoostingRegressor(\n            max_iter=500, learning_rate=0.05, max_depth=8,\n            random_state=self.seed+5\n        )\n        \n        # Linear Models\n        self.models['ridge'] = Ridge(alpha=1.0, random_state=self.seed)\n        self.models['lasso'] = Lasso(alpha=0.01, random_state=self.seed+6)\n        self.models['elastic'] = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=self.seed+7)\n        self.models['huber'] = HuberRegressor(epsilon=1.35, alpha=0.01)\n        \n        # Neighbors\n        self.models['knn'] = KNeighborsRegressor(n_neighbors=20, weights='distance', n_jobs=-1)\n        \n        # Neural Networks\n        if self.enable_neural:\n            self.models['mlp'] = MLPRegressor(\n                hidden_layer_sizes=(128, 64, 32),\n                activation='relu', solver='adam', \n                learning_rate_init=0.001, max_iter=500,\n                random_state=self.seed, early_stopping=True\n            )\n    \n    def get_models(self):\n        \"\"\"Return dictionary of models\"\"\"\n        return self.models\n\n# ================================================================================\n# INTELLIGENT ENSEMBLE\n# ================================================================================\n\nclass IntelligentEnsemble:\n    \"\"\"Advanced ensemble with model selection and weighting\"\"\"\n    \n    def __init__(self, models, n_folds=5, min_performance=0.3):\n        self.models = models\n        self.n_folds = n_folds\n        self.min_performance = min_performance\n        self.model_weights = {}\n        self.selected_models = {}\n        self.oof_predictions = {}\n        self.scalers = {}\n        self.meta_model = None\n    \n    def evaluate_model(self, model, X_train, y_train, X_val, y_val, sample_weight=None):\n        \"\"\"Evaluate single model and return performance metrics\"\"\"\n        try:\n            if sample_weight is not None and hasattr(model, 'fit'):\n                # Check if model supports sample_weight\n                import inspect\n                sig = inspect.signature(model.fit)\n                if 'sample_weight' in sig.parameters:\n                    model.fit(X_train, y_train, sample_weight=sample_weight)\n                else:\n                    model.fit(X_train, y_train)\n            else:\n                model.fit(X_train, y_train)\n            \n            pred_val = model.predict(X_val)\n            \n            # Check for constant predictions\n            if np.std(pred_val) < 0.01:\n                return None, None, False\n            \n            # Calculate metrics\n            mse = mean_squared_error(y_val, pred_val)\n            mae = mean_absolute_error(y_val, pred_val)\n            r2 = 1 - mse / np.var(y_val)\n            \n            return pred_val, {'mse': mse, 'mae': mae, 'r2': r2}, True\n            \n        except Exception as e:\n            print(f\"Model evaluation failed: {e}\")\n            return None, None, False\n    \n    def fit(self, X, y, groups=None, sample_weight=None):\n        \"\"\"Fit ensemble with cross-validation and model selection\"\"\"\n        \n        print(\"\\nüîß Training Intelligent Ensemble...\")\n        \n        # Standardize features\n        self.scalers['standard'] = StandardScaler()\n        self.scalers['robust'] = RobustScaler()\n        \n        X_scaled = self.scalers['standard'].fit_transform(X)\n        X_robust = self.scalers['robust'].fit_transform(X)\n        \n        # Setup cross-validation\n        if groups is not None:\n            cv = GroupKFold(n_splits=self.n_folds)\n            cv_iter = cv.split(X, y, groups)\n        else:\n            cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n            cv_iter = cv.split(X, y)\n        \n        # Initialize OOF predictions\n        n_samples = len(X)\n        model_scores = {name: [] for name in self.models.keys()}\n        oof_preds = {name: np.zeros(n_samples) for name in self.models.keys()}\n        \n        # Cross-validation\n        for fold, (train_idx, val_idx) in enumerate(cv_iter):\n            print(f\"\\n  Fold {fold + 1}/{self.n_folds}\")\n            \n            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n            X_train_robust, X_val_robust = X_robust[train_idx], X_robust[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            if sample_weight is not None:\n                w_train = sample_weight[train_idx]\n            else:\n                w_train = None\n            \n            # Train each model\n            for name, model in tqdm(self.models.items(), desc=\"  Training models\"):\n                # Use robust scaling for linear models\n                if name in ['ridge', 'lasso', 'elastic', 'huber']:\n                    X_tr, X_vl = X_train_robust, X_val_robust\n                else:\n                    X_tr, X_vl = X_train, X_val\n                \n                # Clone model\n                model_clone = model.__class__(**model.get_params())\n                \n                # Evaluate\n                pred_val, metrics, success = self.evaluate_model(\n                    model_clone, X_tr, y_train, X_vl, y_val, w_train\n                )\n                \n                if success and metrics['r2'] > self.min_performance:\n                    oof_preds[name][val_idx] = pred_val\n                    model_scores[name].append(metrics['r2'])\n                else:\n                    model_scores[name].append(0)\n        \n        # Select best models\n        print(\"\\nüìä Model Performance:\")\n        selected_models = []\n        \n        for name in self.models.keys():\n            if model_scores[name]:\n                mean_score = np.mean(model_scores[name])\n                if mean_score > self.min_performance:\n                    selected_models.append(name)\n                    self.model_weights[name] = mean_score\n                    print(f\"  ‚úì {name:15} R¬≤: {mean_score:.4f}\")\n                else:\n                    print(f\"  ‚úó {name:15} R¬≤: {mean_score:.4f} (excluded)\")\n        \n        # Normalize weights\n        if self.model_weights:\n            total_weight = sum(self.model_weights.values())\n            self.model_weights = {k: v/total_weight for k, v in self.model_weights.items()}\n        \n        # Train meta-model (stacking)\n        if len(selected_models) > 1:\n            meta_features = np.column_stack([oof_preds[name] for name in selected_models])\n            self.meta_model = Ridge(alpha=0.1)\n            self.meta_model.fit(meta_features, y)\n            print(f\"\\n‚úì Meta-model trained on {len(selected_models)} base models\")\n        \n        # Retrain selected models on full data\n        print(\"\\nüîÑ Retraining selected models on full data...\")\n        self.selected_models = {}\n        \n        for name in selected_models:\n            if name in ['ridge', 'lasso', 'elastic', 'huber']:\n                X_train_final = X_robust\n            else:\n                X_train_final = X_scaled\n            \n            model = self.models[name]\n            if sample_weight is not None and hasattr(model, 'fit'):\n                import inspect\n                sig = inspect.signature(model.fit)\n                if 'sample_weight' in sig.parameters:\n                    model.fit(X_train_final, y, sample_weight=sample_weight)\n                else:\n                    model.fit(X_train_final, y)\n            else:\n                model.fit(X_train_final, y)\n            \n            self.selected_models[name] = model\n        \n        self.oof_predictions = oof_preds\n        return self\n    \n    def predict(self, X):\n        \"\"\"Generate ensemble predictions\"\"\"\n        \n        X_scaled = self.scalers['standard'].transform(X)\n        X_robust = self.scalers['robust'].transform(X)\n        \n        predictions = []\n        weights = []\n        \n        for name, model in self.selected_models.items():\n            if name in ['ridge', 'lasso', 'elastic', 'huber']:\n                X_input = X_robust\n            else:\n                X_input = X_scaled\n            \n            pred = model.predict(X_input)\n            predictions.append(pred)\n            weights.append(self.model_weights.get(name, 1.0))\n        \n        if not predictions:\n            # Fallback to simple average if no models selected\n            return np.zeros(len(X))\n        \n        # Weighted average\n        predictions = np.array(predictions)\n        weights = np.array(weights).reshape(-1, 1)\n        weighted_pred = np.sum(predictions * weights, axis=0) / np.sum(weights)\n        \n        # If we have a meta-model, use it for final prediction\n        if self.meta_model is not None and len(predictions) > 1:\n            meta_features = predictions.T\n            weighted_pred = self.meta_model.predict(meta_features)\n        \n        return weighted_pred\n\n# ================================================================================\n# PHYSICS CONSTRAINTS AND SMOOTHING\n# ================================================================================\n\ndef apply_physics_constraints(predictions, last_positions, time_offset, max_speed=12.0):\n    \"\"\"Apply realistic physics constraints to predictions\"\"\"\n    \n    # Maximum possible displacement\n    max_displacement = max_speed * time_offset\n    \n    # Calculate predicted displacement\n    dx_pred = predictions[:, 0] if predictions.ndim > 1 else predictions\n    dy_pred = predictions[:, 1] if predictions.ndim > 1 else np.zeros_like(predictions)\n    \n    displacement = np.sqrt(dx_pred**2 + dy_pred**2)\n    \n    # Apply constraints where needed\n    mask = displacement > max_displacement\n    if np.any(mask):\n        scale = max_displacement[mask] / (displacement[mask] + 1e-6)\n        dx_pred[mask] *= scale\n        dy_pred[mask] *= scale\n    \n    return dx_pred, dy_pred\n\ndef smooth_trajectory(positions, window_size=3, sigma=1.0):\n    \"\"\"Smooth trajectory using Gaussian filter\"\"\"\n    \n    if len(positions) < window_size:\n        return positions\n    \n    try:\n        # Apply Gaussian smoothing\n        smoothed = gaussian_filter1d(positions, sigma=sigma, axis=0)\n        return smoothed\n    except:\n        return positions\n\ndef detect_and_fix_outliers(predictions, method='iqr', threshold=3):\n    \"\"\"Detect and fix outlier predictions\"\"\"\n    \n    if method == 'iqr':\n        Q1 = np.percentile(predictions, 25, axis=0)\n        Q3 = np.percentile(predictions, 75, axis=0)\n        IQR = Q3 - Q1\n        lower = Q1 - threshold * IQR\n        upper = Q3 + threshold * IQR\n    elif method == 'zscore':\n        mean = np.mean(predictions, axis=0)\n        std = np.std(predictions, axis=0)\n        lower = mean - threshold * std\n        upper = mean + threshold * std\n    else:\n        return predictions\n    \n    # Clip outliers\n    predictions = np.clip(predictions, lower, upper)\n    return predictions\n\n# ================================================================================\n# MAIN PIPELINE\n# ================================================================================\n\ndef main_pipeline():\n    \"\"\"Main training and prediction pipeline\"\"\"\n    \n    try:\n        # Load data\n        df_in, df_out, test_in, test_template = load_data()\n        \n        # Feature engineering\n        print(\"\\n‚öôÔ∏è Engineering features...\")\n        train = create_features(df_in, df_out, is_train=True)\n        test = create_features(test_in, test_template, test_in, test_template, is_train=False)\n        \n        # Remove invalid training samples if exists\n        if 'is_valid' in train.columns:\n            print(f\"  Removing {(~train['is_valid']).sum()} invalid samples\")\n            train = train[train['is_valid']].reset_index(drop=True)\n        \n        # Define features\n        feature_cols = [\n            'x_last', 'y_last', 's', 'a', 'o', 'dir',\n            'frame_offset', 'time_offset', 'time_squared', 'time_sqrt', 'time_log',\n            'dist_to_ball', 'angle_to_ball', 'sin_angle_ball', 'cos_angle_ball',\n            'dist_to_target', 'angle_to_target', 'is_target',\n            'vx', 'vy', 'v_magnitude', 'ax', 'ay',\n            'dir_o_diff', 'is_aligned',\n            'dist_to_sideline', 'dist_to_endzone', 'field_center_dist',\n            'normalized_x', 'normalized_y', 'x_zone', 'y_zone',\n            'momentum', 'kinetic_energy', 'max_displacement',\n            'closing_speed', 'tangential_speed',\n            'speed_acceleration_product', 'speed_squared', 'acceleration_squared',\n            'log_speed', 'log_dist_ball', 'log_dist_target',\n            'is_offense', 'is_defense', 'is_passer',\n            'absolute_yardline_number', 'player_weight'\n        ]\n        \n        # Add time_normalized if exists\n        if 'time_normalized' in train.columns:\n            feature_cols.extend(['time_normalized', 'time_remaining'])\n        \n        # Add physical attributes if exist\n        if 'height_inches' in train.columns:\n            feature_cols.append('height_inches')\n        if 'bmi' in train.columns:\n            feature_cols.append('bmi')\n        if 'target_ball_alignment' in train.columns:\n            feature_cols.extend(['target_ball_alignment', 'between_target_ball'])\n        \n        # Filter to existing columns\n        feature_cols = [f for f in feature_cols if f in train.columns and f in test.columns]\n        print(f\"  Using {len(feature_cols)} features\")\n        \n        # Handle missing values\n        for col in feature_cols:\n            if col in train.columns:\n                median_val = train[col].median()\n                train[col] = train[col].fillna(median_val)\n                test[col] = test[col].fillna(median_val)\n        \n        # Prepare data\n        X_train = train[feature_cols].values.astype(np.float32)\n        y_dx = train['dx'].values.astype(np.float32)\n        y_dy = train['dy'].values.astype(np.float32)\n        \n        X_test = test[feature_cols].values.astype(np.float32)\n        \n        # Sample weights (emphasize target receivers and later frames)\n        sample_weight = np.ones(len(train))\n        if 'is_target' in train.columns:\n            sample_weight[train['is_target'] == 1] *= 2.0\n        if 'time_normalized' in train.columns:\n            sample_weight *= (1 + 0.5 * train['time_normalized'].values)\n        \n        # Groups for GroupKFold\n        groups = train['game_id'].values\n        \n        # Initialize model zoo\n        print(\"\\nü¶Å Initializing Model Zoo...\")\n        zoo = ModelZoo(seed=Config.SEED, enable_neural=Config.ENABLE_NEURAL)\n        models = zoo.get_models()\n        print(f\"  Created {len(models)} diverse models\")\n        \n        # Train ensemble for X displacement\n        print(\"\\nüìà Training ensemble for X-displacement...\")\n        ensemble_dx = IntelligentEnsemble(\n            models=models,\n            n_folds=Config.N_FOLDS,\n            min_performance=Config.MIN_MODEL_PERFORMANCE\n        )\n        ensemble_dx.fit(X_train, y_dx, groups=groups, sample_weight=sample_weight)\n        \n        # Train ensemble for Y displacement\n        print(\"\\nüìà Training ensemble for Y-displacement...\")\n        ensemble_dy = IntelligentEnsemble(\n            models=models,\n            n_folds=Config.N_FOLDS,\n            min_performance=Config.MIN_MODEL_PERFORMANCE\n        )\n        ensemble_dy.fit(X_train, y_dy, groups=groups, sample_weight=sample_weight)\n        \n        # Generate predictions\n        print(\"\\nüéØ Generating predictions...\")\n        pred_dx = ensemble_dx.predict(X_test)\n        pred_dy = ensemble_dy.predict(X_test)\n        \n        # Apply physics constraints\n        print(\"  Applying physics constraints...\")\n        pred_dx, pred_dy = apply_physics_constraints(\n            np.column_stack([pred_dx, pred_dy]),\n            test[['x_last', 'y_last']].values,\n            test['time_offset'].values,\n            max_speed=Config.MAX_SPEED\n        )\n        \n        # Detect and fix outliers\n        print(\"  Detecting outliers...\")\n        pred_dx = detect_and_fix_outliers(pred_dx.reshape(-1, 1), method='iqr').flatten()\n        pred_dy = detect_and_fix_outliers(pred_dy.reshape(-1, 1), method='iqr').flatten()\n        \n        # Calculate final positions\n        pred_x = test['x_last'].values + pred_dx\n        pred_y = test['y_last'].values + pred_dy\n        \n        # Apply field boundaries\n        pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n        pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n        \n        # Smooth trajectories for each player\n        print(\"  Smoothing trajectories...\")\n        unique_players = test.groupby(['game_id', 'play_id', 'nfl_id']).groups\n        \n        for player_group in unique_players.values():\n            if len(player_group) > 2:\n                player_idx = list(player_group)\n                positions = np.column_stack([pred_x[player_idx], pred_y[player_idx]])\n                smoothed = smooth_trajectory(positions, window_size=3, sigma=0.5)\n                pred_x[player_idx] = smoothed[:, 0]\n                pred_y[player_idx] = smoothed[:, 1]\n        \n        # Create submission\n        print(\"\\nüìù Creating submission...\")\n        test['id'] = (test['game_id'].astype(str) + \"_\" +\n                     test['play_id'].astype(str) + \"_\" +\n                     test['nfl_id'].astype(str) + \"_\" +\n                     test['frame_id'].astype(str))\n        \n        submission = pd.DataFrame({\n            'id': test['id'],\n            'x': pred_x,\n            'y': pred_y\n        })\n        \n        # Final boundary check\n        submission['x'] = submission['x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n        submission['y'] = submission['y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n        \n        # Save submission\n        submission.to_csv(\"submission.csv\", index=False)\n        print(f\"‚úÖ Submission saved: {len(submission)} predictions\")\n        \n        # Display statistics\n        print(\"\\nüìä Prediction Statistics:\")\n        print(f\"  X: Mean={submission['x'].mean():.2f}, Std={submission['x'].std():.2f}\")\n        print(f\"  Y: Mean={submission['y'].mean():.2f}, Std={submission['y'].std():.2f}\")\n        print(\"\\nSample predictions:\")\n        print(submission.head(10))\n        \n        # Cleanup\n        del df_in, df_out, train, test, X_train, X_test\n        gc.collect()\n        \n        print(\"\\n\" + \"=\"*90)\n        print(\" \"*25 + \"PIPELINE COMPLETE! üèà\")\n        print(\"=\"*90)\n        \n        return submission\n    \n    except Exception as e:\n        print(f\"\\n‚ùå Pipeline failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n        # Create fallback submission\n        print(\"\\n‚ö†Ô∏è Creating fallback submission...\")\n        try:\n            test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n            submission = pd.DataFrame({\n                'id': (test_template['game_id'].astype(str) + \"_\" +\n                      test_template['play_id'].astype(str) + \"_\" +\n                      test_template['nfl_id'].astype(str) + \"_\" +\n                      test_template['frame_id'].astype(str)),\n                'x': 60.0,  # Middle of field\n                'y': 26.65  # Middle of field\n            })\n            submission.to_csv(\"submission.csv\", index=False)\n            print(\"‚úì Fallback submission created\")\n            return submission\n        except:\n            print(\"‚ùå Could not create fallback submission\")\n            return None\n\n# ================================================================================\n# EXECUTION\n# ================================================================================\n\nif __name__ == \"__main__\":\n    submission = main_pipeline()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}