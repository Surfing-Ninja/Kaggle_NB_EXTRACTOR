{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# NFL BIG DATA BOWL 2026 - ENHANCED WITH GRAPH NEURAL FEATURES\n# Incorporates lightweight graph convolutions and attention mechanisms\n# ================================================================================\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport gc\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nfrom scipy.spatial.distance import cdist\nfrom scipy.spatial import Voronoi\nfrom scipy.stats import entropy\nfrom sklearn.cluster import DBSCAN\n\n# Machine Learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\nwarnings.filterwarnings('ignore')\n\n# ================================================================================\n# CONFIGURATION\n# ================================================================================\n\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    SEEDS = [42, 123]  # Reduced to 2 seeds as requested\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    MAX_SPEED = 12.0\n    N_FOLDS = 3\n    NN_BATCH_SIZE = 4096\n    NN_EPOCHS = 20\n    NN_LEARNING_RATE = 0.002\n    GRAPH_DISTANCE_THRESHOLD = 15.0\n    GRAPH_K_NEAREST = 5\n\n# ================================================================================\n# DATA LOADING\n# ================================================================================\n\ndef load_data():\n    \"\"\"Load all training and test data\"\"\"\n    print(\"Loading data...\")\n    \n    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    train_input_files = [f for f in train_input_files if f.exists()]\n    train_output_files = [f for f in train_output_files if f.exists()]\n    \n    print(f\"Found {len(train_input_files)} weeks of data\")\n    \n    train_input = pd.concat([pd.read_csv(f) for f in tqdm(train_input_files, desc=\"Input\")], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in tqdm(train_output_files, desc=\"Output\")], ignore_index=True)\n    \n    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n    \n    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n    \n    return train_input, train_output, test_input, test_template\n\n# ================================================================================\n# GRAPH NEURAL NETWORK COMPONENTS\n# ================================================================================\n\nclass SimpleGraphConvLayer(nn.Module):\n    \"\"\"Lightweight graph convolution layer\"\"\"\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = nn.Linear(in_features * 2, out_features)\n        self.activation = nn.ReLU()\n        \n    def forward(self, node_features, edge_index):\n        # Simple message passing\n        row, col = edge_index\n        messages = torch.cat([node_features[row], node_features[col]], dim=1)\n        aggregated = torch.zeros_like(node_features[:, :self.linear.out_features])\n        \n        for i in range(len(node_features)):\n            mask = (row == i)\n            if mask.any():\n                aggregated[i] = self.linear(messages[mask]).mean(dim=0)\n        \n        return self.activation(aggregated)\n\nclass LightweightGraphNet(nn.Module):\n    \"\"\"Simple GNN for extracting graph features\"\"\"\n    def __init__(self, input_dim=6, hidden_dim=32, output_dim=16):\n        super().__init__()\n        self.node_encoder = nn.Linear(input_dim, hidden_dim)\n        self.conv1 = SimpleGraphConvLayer(hidden_dim, hidden_dim)\n        self.conv2 = SimpleGraphConvLayer(hidden_dim, output_dim)\n        self.global_pool = nn.Linear(output_dim, output_dim)\n        \n    def forward(self, node_features, edge_index):\n        # Encode nodes\n        x = F.relu(self.node_encoder(node_features))\n        \n        # Graph convolutions\n        x = self.conv1(x, edge_index)\n        x = self.conv2(x, edge_index)\n        \n        # Global pooling\n        graph_embedding = x.mean(dim=0)\n        \n        return graph_embedding, x\n\ndef build_play_graph(frame_data):\n    \"\"\"Build graph structure for a single frame\"\"\"\n    positions = frame_data[['x', 'y']].values\n    n_players = len(positions)\n    \n    if n_players < 2:\n        return None, None\n    \n    # Calculate distances\n    distances = cdist(positions, positions)\n    \n    # Build edges based on KNN and distance threshold\n    edges = []\n    for i in range(n_players):\n        # Get k nearest neighbors\n        nearest_indices = np.argsort(distances[i])[1:Config.GRAPH_K_NEAREST+1]\n        for j in nearest_indices:\n            if distances[i, j] < Config.GRAPH_DISTANCE_THRESHOLD:\n                edges.append([i, j])\n                edges.append([j, i])  # Bidirectional\n    \n    if len(edges) == 0:\n        return None, None\n    \n    edge_index = torch.tensor(edges, dtype=torch.long).t()\n    \n    # Node features\n    node_features = torch.tensor(\n        frame_data[['x', 'y', 's', 'a', 'o', 'dir']].values,\n        dtype=torch.float32\n    )\n    \n    return node_features, edge_index\n\ndef extract_graph_features(input_df):\n    \"\"\"Extract graph-based features for all plays\"\"\"\n    print(\"  Computing graph neural features...\")\n    \n    # Initialize simple GNN\n    gnn_model = LightweightGraphNet()\n    gnn_model.eval()\n    \n    graph_features = []\n    \n    for (game_id, play_id), play_group in tqdm(input_df.groupby(['game_id', 'play_id']), \n                                                desc=\"Graph features\", leave=False):\n        # Get last frame\n        last_frame = play_group.sort_values('frame_id').groupby('nfl_id').last().reset_index()\n        \n        # Build graph\n        node_features, edge_index = build_play_graph(last_frame)\n        \n        if node_features is None:\n            continue\n        \n        # Extract features using GNN\n        with torch.no_grad():\n            graph_embedding, node_embeddings = gnn_model(node_features, edge_index)\n        \n        # Store features for each player\n        for i, (_, player) in enumerate(last_frame.iterrows()):\n            player_features = {\n                'game_id': game_id,\n                'play_id': play_id,\n                'nfl_id': player['nfl_id']\n            }\n            \n            # Add node embedding features\n            node_embed = node_embeddings[i].numpy()\n            for j, val in enumerate(node_embed[:8]):  # Use first 8 features\n                player_features[f'graph_node_feat_{j}'] = val\n            \n            # Add graph-level features (same for all players in the play)\n            for j, val in enumerate(graph_embedding.numpy()[:8]):\n                player_features[f'graph_global_feat_{j}'] = val\n            \n            graph_features.append(player_features)\n    \n    return pd.DataFrame(graph_features)\n\n# ================================================================================\n# ATTENTION-BASED PLAYER PAIR FEATURES\n# ================================================================================\n\ndef compute_attention_features(input_df):\n    \"\"\"Compute attention scores between key player pairs\"\"\"\n    print(\"  Computing attention-based interaction features...\")\n    features = []\n    \n    for (game_id, play_id), play_group in input_df.groupby(['game_id', 'play_id']):\n        last_frame = play_group.sort_values('frame_id').groupby('nfl_id').last().reset_index()\n        \n        # Get key players\n        target_receiver = last_frame[last_frame['player_role'] == 'Targeted Receiver']\n        defenders = last_frame[last_frame['player_side'] == 'Defense']\n        other_receivers = last_frame[last_frame['player_role'] == 'Other Route Runner']\n        \n        if len(target_receiver) == 0:\n            continue\n        \n        target = target_receiver.iloc[0]\n        target_pos = np.array([target['x'], target['y']])\n        target_vel = np.array([target['s'] * np.sin(np.deg2rad(target['dir'])),\n                               target['s'] * np.cos(np.deg2rad(target['dir']))])\n        \n        # Compute attention weights for defenders to target\n        if len(defenders) > 0:\n            def_positions = defenders[['x', 'y']].values\n            def_velocities = np.array([\n                defenders['s'].values * np.sin(np.deg2rad(defenders['dir'].values)),\n                defenders['s'].values * np.cos(np.deg2rad(defenders['dir'].values))\n            ]).T\n            \n            # Distance-based attention\n            distances = np.sqrt(np.sum((def_positions - target_pos)**2, axis=1))\n            distance_attention = F.softmax(-torch.tensor(distances), dim=0).numpy()\n            \n            # Velocity alignment attention\n            vel_alignments = []\n            for def_vel in def_velocities:\n                alignment = np.dot(def_vel, target_vel) / (np.linalg.norm(def_vel) * np.linalg.norm(target_vel) + 1e-6)\n                vel_alignments.append(alignment)\n            velocity_attention = F.softmax(torch.tensor(vel_alignments), dim=0).numpy()\n            \n            # Combined attention\n            combined_attention = (distance_attention + velocity_attention) / 2\n            \n            # Store features for defenders\n            for i, (_, defender) in enumerate(defenders.iterrows()):\n                features.append({\n                    'game_id': game_id,\n                    'play_id': play_id,\n                    'nfl_id': defender['nfl_id'],\n                    'attention_to_target_distance': distance_attention[i],\n                    'attention_to_target_velocity': velocity_attention[i],\n                    'attention_to_target_combined': combined_attention[i],\n                    'is_primary_defender': int(combined_attention[i] == combined_attention.max()),\n                    'attention_rank': len(defenders) - np.argsort(combined_attention).tolist().index(i)\n                })\n        \n        # Compute attention between receivers (route combinations)\n        if len(other_receivers) > 0:\n            all_receivers = pd.concat([target_receiver, other_receivers])\n            rec_positions = all_receivers[['x', 'y']].values\n            \n            # Pairwise attention matrix\n            rec_distances = cdist(rec_positions, rec_positions)\n            np.fill_diagonal(rec_distances, np.inf)\n            \n            for i, (_, receiver) in enumerate(all_receivers.iterrows()):\n                min_receiver_dist = np.min(rec_distances[i])\n                features.append({\n                    'game_id': game_id,\n                    'play_id': play_id,\n                    'nfl_id': receiver['nfl_id'],\n                    'min_receiver_spacing': min_receiver_dist,\n                    'receiver_isolation': int(min_receiver_dist > 10),\n                    'route_proximity_score': 1 / (min_receiver_dist + 1)\n                })\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# ENHANCED PLAYER INTERACTION FEATURES\n# ================================================================================\n\ndef compute_player_interactions(input_df):\n    \"\"\"Compute player-to-player interaction features with graph context\"\"\"\n    print(\"  Computing player interaction features...\")\n    \n    interaction_features = []\n    \n    for (game_id, play_id), play_group in input_df.groupby(['game_id', 'play_id']):\n        last_frame = play_group.sort_values('frame_id').groupby('nfl_id').last().reset_index()\n        \n        positions = last_frame[['x', 'y']].values\n        player_ids = last_frame['nfl_id'].values\n        player_sides = last_frame['player_side'].values\n        player_roles = last_frame['player_role'].values\n        speeds = last_frame['s'].values\n        directions = last_frame['dir'].values\n        orientations = last_frame['o'].values\n        \n        if len(positions) > 1:\n            distances = cdist(positions, positions)\n            \n            # Compute graph centrality (simplified)\n            adjacency = (distances < Config.GRAPH_DISTANCE_THRESHOLD).astype(float)\n            np.fill_diagonal(adjacency, 0)\n            degree_centrality = adjacency.sum(axis=1) / (len(positions) - 1)\n            \n            for i, player_id in enumerate(player_ids):\n                player_features = {\n                    'game_id': game_id,\n                    'play_id': play_id,\n                    'nfl_id': player_id,\n                    'graph_degree_centrality': degree_centrality[i]\n                }\n                \n                same_side_mask = (player_sides == player_sides[i]) & (np.arange(len(positions)) != i)\n                opp_side_mask = (player_sides != player_sides[i])\n                \n                if np.any(same_side_mask):\n                    teammate_distances = distances[i][same_side_mask]\n                    player_features['nearest_teammate_dist'] = np.min(teammate_distances)\n                    player_features['avg_teammate_dist'] = np.mean(teammate_distances)\n                    player_features['teammates_within_5'] = np.sum(teammate_distances < 5)\n                    player_features['teammates_within_10'] = np.sum(teammate_distances < 10)\n                    player_features['teammates_within_3'] = np.sum(teammate_distances < 3)\n                    \n                    # Graph-based team cohesion\n                    teammate_adjacency = adjacency[i][same_side_mask]\n                    player_features['team_graph_connectivity'] = np.mean(teammate_adjacency)\n                else:\n                    player_features['nearest_teammate_dist'] = 0\n                    player_features['avg_teammate_dist'] = 0\n                    player_features['teammates_within_5'] = 0\n                    player_features['teammates_within_10'] = 0\n                    player_features['teammates_within_3'] = 0\n                    player_features['team_graph_connectivity'] = 0\n                \n                if np.any(opp_side_mask):\n                    opponent_distances = distances[i][opp_side_mask]\n                    player_features['nearest_opponent_dist'] = np.min(opponent_distances)\n                    player_features['avg_opponent_dist'] = np.mean(opponent_distances)\n                    player_features['opponents_within_5'] = np.sum(opponent_distances < 5)\n                    player_features['opponents_within_10'] = np.sum(opponent_distances < 10)\n                    player_features['opponents_within_3'] = np.sum(opponent_distances < 3)\n                    player_features['pressure_index'] = 1 / (player_features['nearest_opponent_dist'] + 1)\n                    \n                    opp_speeds = speeds[opp_side_mask]\n                    speed_weighted_distances = opponent_distances / (opp_speeds + 1)\n                    player_features['speed_weighted_pressure'] = 1 / (np.min(speed_weighted_distances) + 1)\n                    \n                    # Graph-based opponent pressure\n                    opponent_adjacency = adjacency[i][opp_side_mask]\n                    player_features['opponent_graph_pressure'] = np.sum(opponent_adjacency)\n                else:\n                    player_features['nearest_opponent_dist'] = 100\n                    player_features['avg_opponent_dist'] = 100\n                    player_features['opponents_within_5'] = 0\n                    player_features['opponents_within_10'] = 0\n                    player_features['opponents_within_3'] = 0\n                    player_features['pressure_index'] = 0\n                    player_features['speed_weighted_pressure'] = 0\n                    player_features['opponent_graph_pressure'] = 0\n                \n                player_features['local_density'] = np.sum(distances[i] < 10) - 1\n                \n                if np.any(same_side_mask):\n                    teammate_positions = positions[same_side_mask]\n                    if len(teammate_positions) > 0:\n                        player_features['team_spread_x'] = np.std(teammate_positions[:, 0])\n                        player_features['team_spread_y'] = np.std(teammate_positions[:, 1])\n                        player_features['team_centroid_x'] = np.mean(teammate_positions[:, 0])\n                        player_features['team_centroid_y'] = np.mean(teammate_positions[:, 1])\n                        player_features['team_compactness'] = np.mean(np.sqrt(np.sum((teammate_positions - teammate_positions.mean(axis=0))**2, axis=1)))\n                    else:\n                        player_features['team_spread_x'] = 0\n                        player_features['team_spread_y'] = 0\n                        player_features['team_centroid_x'] = positions[i, 0]\n                        player_features['team_centroid_y'] = positions[i, 1]\n                        player_features['team_compactness'] = 0\n                \n                if player_roles[i] == 'Targeted Receiver':\n                    coverage_mask = (player_roles == 'Defensive Coverage') & opp_side_mask\n                    if np.any(coverage_mask):\n                        coverage_distances = distances[i][coverage_mask]\n                        player_features['nearest_coverage_dist'] = np.min(coverage_distances)\n                        player_features['coverage_players_nearby'] = np.sum(coverage_distances < 10)\n                        player_features['coverage_density'] = np.sum(coverage_distances < 5)\n                        \n                        nearest_cov_idx = np.argmin(coverage_distances)\n                        cov_positions = positions[coverage_mask]\n                        cov_pos = cov_positions[nearest_cov_idx]\n                        \n                        receiver_to_sideline = min(positions[i, 1], Config.FIELD_Y_MAX - positions[i, 1])\n                        defender_to_sideline = min(cov_pos[1], Config.FIELD_Y_MAX - cov_pos[1])\n                        player_features['inside_leverage'] = int(defender_to_sideline < receiver_to_sideline)\n                        player_features['outside_leverage'] = int(defender_to_sideline > receiver_to_sideline)\n                \n                if np.any(opp_side_mask):\n                    nearest_opp_idx = np.argmin(distances[i][opp_side_mask])\n                    opp_positions = positions[opp_side_mask]\n                    dx = opp_positions[nearest_opp_idx, 0] - positions[i, 0]\n                    dy = opp_positions[nearest_opp_idx, 1] - positions[i, 1]\n                    player_features['nearest_opp_dx'] = dx\n                    player_features['nearest_opp_dy'] = dy\n                    player_features['nearest_opp_angle'] = np.arctan2(dy, dx)\n                \n                interaction_features.append(player_features)\n    \n    return pd.DataFrame(interaction_features)\n\n# ================================================================================\n# TRAJECTORY FEATURES\n# ================================================================================\n\ndef add_trajectory_features(input_df):\n    \"\"\"Enhanced trajectory analysis\"\"\"\n    print(\"  Computing trajectory features...\")\n    features = []\n    \n    for (game_id, play_id, nfl_id), group in input_df.groupby(['game_id', 'play_id', 'nfl_id']):\n        sorted_group = group.sort_values('frame_id')\n        \n        feature_dict = {\n            'game_id': game_id,\n            'play_id': play_id,\n            'nfl_id': nfl_id\n        }\n        \n        if len(sorted_group) >= 3:\n            directions = np.deg2rad(sorted_group['dir'].values)\n            direction_diffs = np.diff(directions)\n            feature_dict['path_curvature'] = np.std(direction_diffs)\n            feature_dict['max_direction_change'] = np.max(np.abs(direction_diffs))\n            \n            positions = sorted_group[['x', 'y']].values\n            path_length = np.sum(np.sqrt(np.sum(np.diff(positions, axis=0)**2, axis=1)))\n            straight_distance = np.sqrt((positions[-1][0] - positions[0][0])**2 + \n                                       (positions[-1][1] - positions[0][1])**2)\n            feature_dict['path_sinuosity'] = path_length / (straight_distance + 0.1)\n            feature_dict['total_path_length'] = path_length\n        else:\n            feature_dict['path_curvature'] = 0\n            feature_dict['path_sinuosity'] = 1\n            feature_dict['max_direction_change'] = 0\n            feature_dict['total_path_length'] = 0\n        \n        if len(sorted_group) >= 3:\n            feature_dict['jerk'] = np.std(np.diff(sorted_group['a'].values))\n            feature_dict['max_acceleration'] = np.max(np.abs(sorted_group['a'].values))\n        else:\n            feature_dict['jerk'] = 0\n            feature_dict['max_acceleration'] = 0\n            \n        features.append(feature_dict)\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# ROLLING STATISTICS\n# ================================================================================\n\ndef compute_rolling_stats(input_df):\n    \"\"\"Compute rolling window statistics\"\"\"\n    print(\"  Computing rolling statistics...\")\n    features = []\n    \n    window_sizes = [3, 5]\n    \n    for (game_id, play_id, nfl_id), group in input_df.groupby(['game_id', 'play_id', 'nfl_id']):\n        sorted_group = group.sort_values('frame_id')\n        \n        feature_dict = {\n            'game_id': game_id,\n            'play_id': play_id,\n            'nfl_id': nfl_id\n        }\n        \n        for window in window_sizes:\n            if len(sorted_group) >= window:\n                recent = sorted_group.tail(window)\n                \n                feature_dict[f'rolling_speed_mean_{window}'] = recent['s'].mean()\n                feature_dict[f'rolling_speed_std_{window}'] = recent['s'].std()\n                feature_dict[f'rolling_accel_mean_{window}'] = recent['a'].mean()\n                feature_dict[f'rolling_dir_variance_{window}'] = recent['dir'].var()\n                \n                if len(recent) >= 2:\n                    speed_changes = np.diff(recent['s'].values)\n                    feature_dict[f'speed_acceleration_{window}'] = np.mean(speed_changes)\n                else:\n                    feature_dict[f'speed_acceleration_{window}'] = 0\n            else:\n                feature_dict[f'rolling_speed_mean_{window}'] = 0\n                feature_dict[f'rolling_speed_std_{window}'] = 0\n                feature_dict[f'rolling_accel_mean_{window}'] = 0\n                feature_dict[f'rolling_dir_variance_{window}'] = 0\n                feature_dict[f'speed_acceleration_{window}'] = 0\n        \n        features.append(feature_dict)\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# SEPARATION DYNAMICS\n# ================================================================================\n\ndef compute_separation_dynamics(input_df):\n    \"\"\"Compute how separation is changing over time\"\"\"\n    print(\"  Computing separation dynamics...\")\n    features = []\n    \n    for (game_id, play_id), play_group in input_df.groupby(['game_id', 'play_id']):\n        target_data = play_group[play_group['player_role'] == 'Targeted Receiver']\n        if len(target_data) == 0:\n            continue\n        \n        target_id = target_data['nfl_id'].iloc[0]\n        target_frames = target_data.sort_values('frame_id')\n        \n        defenders = play_group[play_group['player_side'] == 'Defense']\n        \n        for defender_id in defenders['nfl_id'].unique():\n            defender_frames = play_group[play_group['nfl_id'] == defender_id].sort_values('frame_id')\n            \n            common_frames = np.intersect1d(target_frames['frame_id'].values, defender_frames['frame_id'].values)\n            \n            if len(common_frames) >= 3:\n                target_pos = target_frames[target_frames['frame_id'].isin(common_frames)][['x', 'y']].values\n                defender_pos = defender_frames[defender_frames['frame_id'].isin(common_frames)][['x', 'y']].values\n                \n                separations = np.sqrt(np.sum((target_pos - defender_pos)**2, axis=1))\n                \n                if len(separations) >= 2:\n                    separation_rate = np.mean(np.diff(separations)) * 10\n                    separation_accel = np.std(np.diff(separations)) * 10\n                    max_separation = np.max(separations)\n                    min_separation = np.min(separations)\n                else:\n                    separation_rate = 0\n                    separation_accel = 0\n                    max_separation = separations[0]\n                    min_separation = separations[0]\n                \n                features.append({\n                    'game_id': game_id,\n                    'play_id': play_id,\n                    'nfl_id': defender_id,\n                    'separation_rate': separation_rate,\n                    'separation_acceleration': separation_accel,\n                    'max_separation_achieved': max_separation,\n                    'min_separation': min_separation,\n                    'separation_range': max_separation - min_separation\n                })\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# ROUTE CLASSIFICATION FEATURES\n# ================================================================================\n\ndef classify_route_pattern(input_df):\n    \"\"\"Infer likely route type from pre-snap movement\"\"\"\n    print(\"  Computing route classification features...\")\n    features = []\n    \n    for (game_id, play_id, nfl_id), group in input_df.groupby(['game_id', 'play_id', 'nfl_id']):\n        if group['player_role'].iloc[0] not in ['Targeted Receiver', 'Other Route Runner']:\n            continue\n            \n        sorted_group = group.sort_values('frame_id')\n        if len(sorted_group) < 3:\n            continue\n        \n        x_vals = sorted_group['x'].values\n        y_vals = sorted_group['y'].values\n        \n        vertical_displacement = x_vals[-1] - x_vals[0]\n        horizontal_displacement = abs(y_vals[-1] - y_vals[0])\n        \n        route_depth = abs(vertical_displacement)\n        route_angle = np.arctan2(horizontal_displacement, abs(vertical_displacement) + 0.1)\n        \n        if len(sorted_group) >= 5:\n            directions = np.deg2rad(sorted_group['dir'].values)\n            direction_changes = np.abs(np.diff(directions))\n            has_break = np.any(direction_changes > np.pi/4)\n            break_point_frame = np.argmax(direction_changes) if has_break else 0\n            num_breaks = np.sum(direction_changes > np.pi/4)\n        else:\n            has_break = False\n            break_point_frame = 0\n            num_breaks = 0\n        \n        features.append({\n            'game_id': game_id, 'play_id': play_id, 'nfl_id': nfl_id,\n            'route_depth': route_depth,\n            'route_angle': route_angle,\n            'has_route_break': int(has_break),\n            'break_point_normalized': break_point_frame / (len(sorted_group) - 1) if len(sorted_group) > 1 else 0,\n            'is_vertical_route': int(abs(route_angle) < np.pi/6),\n            'is_crossing_route': int(abs(route_angle) > np.pi/3),\n            'num_route_breaks': num_breaks,\n            'route_complexity': horizontal_displacement + route_depth\n        })\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# PURSUIT ANGLE FEATURES\n# ================================================================================\n\ndef compute_pursuit_features(input_df):\n    \"\"\"Calculate pursuit angles and relative velocities\"\"\"\n    print(\"  Computing pursuit angle features...\")\n    features = []\n    \n    for (game_id, play_id), play_group in input_df.groupby(['game_id', 'play_id']):\n        last_frame = play_group.sort_values('frame_id').groupby('nfl_id').last().reset_index()\n        \n        target = last_frame[last_frame['player_role'] == 'Targeted Receiver']\n        defenders = last_frame[last_frame['player_side'] == 'Defense']\n        \n        if len(target) == 0 or len(defenders) == 0:\n            continue\n            \n        target = target.iloc[0]\n        target_vel = np.array([\n            target['s'] * np.sin(np.deg2rad(target['dir'])),\n            target['s'] * np.cos(np.deg2rad(target['dir']))\n        ])\n        target_pos = np.array([target['x'], target['y']])\n        \n        for _, defender in defenders.iterrows():\n            defender_pos = np.array([defender['x'], defender['y']])\n            defender_vel = np.array([\n                defender['s'] * np.sin(np.deg2rad(defender['dir'])),\n                defender['s'] * np.cos(np.deg2rad(defender['dir']))\n            ])\n            \n            to_target = target_pos - defender_pos\n            to_target_norm = to_target / (np.linalg.norm(to_target) + 0.1)\n            \n            defender_vel_norm = defender_vel / (np.linalg.norm(defender_vel) + 0.1)\n            pursuit_angle = np.arccos(np.clip(np.dot(defender_vel_norm, to_target_norm), -1, 1))\n            \n            relative_vel = target_vel - defender_vel\n            closing_speed = -np.dot(relative_vel, to_target_norm)\n            \n            distance = np.linalg.norm(to_target)\n            time_to_intercept = distance / (closing_speed + 0.1) if closing_speed > 0 else 999\n            \n            features.append({\n                'game_id': game_id,\n                'play_id': play_id,\n                'nfl_id': defender['nfl_id'],\n                'pursuit_angle_to_target': pursuit_angle,\n                'closing_speed_to_target': closing_speed,\n                'time_to_intercept': min(time_to_intercept, 10),\n                'pursuit_efficiency': np.cos(pursuit_angle) * defender['s'],\n                'lateral_separation': abs(np.cross(to_target_norm, defender_vel_norm))\n            })\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# FORMATION AND COVERAGE FEATURES\n# ================================================================================\n\ndef add_formation_features(input_df):\n    \"\"\"Analyze formation structure\"\"\"\n    print(\"  Computing formation features...\")\n    features = []\n    \n    for (game_id, play_id), play_group in input_df.groupby(['game_id', 'play_id']):\n        last_frame = play_group.sort_values('frame_id').groupby('nfl_id').last().reset_index()\n        \n        offense = last_frame[last_frame['player_side'] == 'Offense']\n        defense = last_frame[last_frame['player_side'] == 'Defense']\n        \n        if len(offense) < 2 or len(defense) < 2:\n            continue\n        \n        off_positions = offense[['x', 'y']].values\n        off_x_spread = np.max(off_positions[:, 0]) - np.min(off_positions[:, 0])\n        off_y_spread = np.max(off_positions[:, 1]) - np.min(off_positions[:, 1])\n        off_x_center = np.mean(off_positions[:, 0])\n        off_y_center = np.mean(off_positions[:, 1])\n        \n        def_positions = defense[['x', 'y']].values\n        def_x_spread = np.max(def_positions[:, 0]) - np.min(def_positions[:, 0])\n        def_y_spread = np.max(def_positions[:, 1]) - np.min(def_positions[:, 1])\n        \n        line_of_scrimmage = offense['x'].mean()\n        defenders_in_box = np.sum((defense['x'] < line_of_scrimmage + 5) & \n                                   (defense['y'] > 15) & (defense['y'] < 38))\n        defenders_deep = np.sum(defense['x'] > line_of_scrimmage + 10)\n        \n        receivers = offense[offense['player_role'].isin(['Targeted Receiver', 'Other Route Runner'])]\n        if len(receivers) >= 3:\n            rec_y = receivers['y'].values\n            left_receivers = np.sum(rec_y < 26.65)\n            right_receivers = np.sum(rec_y > 26.65)\n            is_trips = max(left_receivers, right_receivers) >= 3\n            is_bunch = np.any(cdist(receivers[['x', 'y']].values, receivers[['x', 'y']].values) < 3) if len(receivers) > 1 else False\n        else:\n            is_trips = False\n            is_bunch = False\n        \n        for _, player in last_frame.iterrows():\n            player_features = {\n                'game_id': game_id,\n                'play_id': play_id,\n                'nfl_id': player['nfl_id'],\n                'offensive_width': off_y_spread,\n                'offensive_depth': off_x_spread,\n                'defensive_width': def_y_spread,\n                'defenders_in_box': defenders_in_box,\n                'defenders_deep': defenders_deep,\n                'formation_width_ratio': off_y_spread / (def_y_spread + 1),\n                'dist_from_off_center_x': player['x'] - off_x_center,\n                'dist_from_off_center_y': player['y'] - off_y_center,\n                'is_trips_formation': int(is_trips),\n                'is_bunch_formation': int(is_bunch)\n            }\n            features.append(player_features)\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# SPACE CONTROL FEATURES\n# ================================================================================\n\ndef compute_space_control(input_df):\n    \"\"\"Calculate space control/influence for each player\"\"\"\n    print(\"  Computing space control features...\")\n    features = []\n    \n    for (game_id, play_id), play_group in input_df.groupby(['game_id', 'play_id']):\n        last_frame = play_group.sort_values('frame_id').groupby('nfl_id').last().reset_index()\n        \n        positions = last_frame[['x', 'y']].values\n        speeds = last_frame['s'].values\n        \n        if len(positions) < 4:\n            continue\n        \n        try:\n            vor = Voronoi(positions)\n            voronoi_available = True\n        except:\n            voronoi_available = False\n        \n        for i, (player_id, player_side, speed) in enumerate(zip(last_frame['nfl_id'], \n                                                                  last_frame['player_side'],\n                                                                  last_frame['s'])):\n            distances = np.sqrt(np.sum((positions - positions[i])**2, axis=1))\n            \n            immediate_control = np.pi * 5**2\n            speed_factor = 1 + (speed / 10)\n            weighted_control = immediate_control * speed_factor\n            \n            same_side_mask = last_frame['player_side'] == player_side\n            opp_mask = ~same_side_mask\n            \n            if np.any(opp_mask):\n                nearest_opp_dist = np.min(distances[opp_mask])\n                space_cushion = max(0, nearest_opp_dist - 3)\n            else:\n                space_cushion = 10\n            \n            player_features = {\n                'game_id': game_id,\n                'play_id': play_id,\n                'nfl_id': player_id,\n                'control_area': min(weighted_control, 200),\n                'space_cushion': space_cushion,\n                'control_x_speed': weighted_control * speed\n            }\n            \n            if voronoi_available:\n                try:\n                    region_index = vor.point_region[i]\n                    region = vor.regions[region_index]\n                    if -1 not in region and len(region) > 0:\n                        polygon = [vor.vertices[j] for j in region]\n                        polygon = np.array(polygon)\n                        x = polygon[:, 0]\n                        y = polygon[:, 1]\n                        area = 0.5 * abs(sum(x[j]*y[j+1] - x[j+1]*y[j] for j in range(len(polygon)-1)))\n                        player_features['voronoi_cell_area'] = min(area, 500)\n                    else:\n                        player_features['voronoi_cell_area'] = weighted_control\n                except:\n                    player_features['voronoi_cell_area'] = weighted_control\n            else:\n                player_features['voronoi_cell_area'] = weighted_control\n            \n            features.append(player_features)\n    \n    return pd.DataFrame(features)\n\n# ================================================================================\n# MAIN FEATURE PREPARATION\n# ================================================================================\n\ndef height_to_inches(height_str):\n    \"\"\"Convert height from 'ft-in' format to inches\"\"\"\n    if not isinstance(height_str, str) or '-' not in height_str:\n        return 70\n    try:\n        feet, inches = map(int, height_str.split('-'))\n        return feet * 12 + inches\n    except:\n        return 70\n\ndef prepare_features(input_df, output_df, is_training=True):\n    \"\"\"Complete feature preparation pipeline with graph features\"\"\"\n    \n    # Compute all feature sets\n    interaction_features = compute_player_interactions(input_df)\n    trajectory_features = add_trajectory_features(input_df)\n    rolling_features = compute_rolling_stats(input_df)\n    separation_features = compute_separation_dynamics(input_df)\n    route_features = classify_route_pattern(input_df)\n    pursuit_features = compute_pursuit_features(input_df)\n    formation_features = add_formation_features(input_df)\n    space_features = compute_space_control(input_df)\n    \n    # NEW: Graph and attention features\n    graph_features = extract_graph_features(input_df)\n    attention_features = compute_attention_features(input_df)\n    \n    # Get last frame\n    last_frame = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']) \\\n                         .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False).last()\n    last_frame = last_frame.rename(columns={'x': 'x_last', 'y': 'y_last'})\n    \n    # Merge all features\n    for feat_df in [interaction_features, trajectory_features, rolling_features, \n                     separation_features, route_features, pursuit_features, \n                     formation_features, space_features, graph_features, attention_features]:\n        if len(feat_df) > 0:\n            last_frame = last_frame.merge(feat_df, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    # Temporal statistics\n    temporal_stats = input_df.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n        'x': ['mean', 'std', 'min', 'max'],\n        'y': ['mean', 'std', 'min', 'max'],\n        's': ['mean', 'std', 'max', 'min'],\n        'a': ['mean', 'std', 'max', 'min'],\n        'dir': lambda x: np.std(np.diff(x)) if len(x) > 1 else 0,\n        'o': lambda x: np.std(np.diff(x)) if len(x) > 1 else 0,\n    }).reset_index()\n    temporal_stats.columns = ['_'.join(col).strip() if col[1] else col[0] \n                              for col in temporal_stats.columns.values]\n    temporal_stats = temporal_stats.rename(columns={\n        'dir_<lambda>': 'dir_change_rate',\n        'o_<lambda>': 'orientation_change_rate'\n    })\n    \n    # Recent frames\n    last_n_frames = 5\n    recent_frames = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']) \\\n                            .groupby(['game_id', 'play_id', 'nfl_id']).tail(last_n_frames)\n    \n    trajectory_stats = recent_frames.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n        'x': lambda x: (x.iloc[-1] - x.iloc[0]) if len(x) > 1 else 0,\n        'y': lambda x: (x.iloc[-1] - x.iloc[0]) if len(x) > 1 else 0,\n        's': lambda x: x.diff().mean() if len(x) > 1 else 0,\n    }).reset_index()\n    trajectory_stats.columns = ['game_id', 'play_id', 'nfl_id', \n                                  'recent_displacement_x', 'recent_displacement_y', 'acceleration_trend']\n    \n    last_frame = last_frame.merge(temporal_stats, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    last_frame = last_frame.merge(trajectory_stats, on=['game_id', 'play_id', 'nfl_id'], how='left')\n    \n    if 'player_height' in last_frame.columns:\n        last_frame['height_inches'] = last_frame['player_height'].apply(height_to_inches)\n    \n    # Target receiver position\n    targets = last_frame[last_frame['player_role'] == 'Targeted Receiver'][\n        ['game_id', 'play_id', 'x_last', 'y_last']\n    ].rename(columns={'x_last': 'target_x', 'y_last': 'target_y'})\n    targets = targets.drop_duplicates(['game_id', 'play_id'])\n    \n    last_frame = last_frame.merge(targets, on=['game_id', 'play_id'], how='left')\n    \n    # Merge with output\n    merge_cols = [col for col in last_frame.columns if col not in ['frame_id']]\n    merged = output_df.merge(last_frame[merge_cols], \n                             on=['game_id', 'play_id', 'nfl_id'], \n                             how='left')\n    \n    df = merged.copy()\n    \n    # Core temporal features\n    df['time_seconds'] = df['frame_id'] / 10.0\n    df['time_normalized'] = df['frame_id'] / df.groupby(['game_id', 'play_id', 'nfl_id'])['frame_id'].transform('max')\n    df['time_squared'] = df['time_seconds'] ** 2\n    df['time_cubed'] = df['time_seconds'] ** 3\n    df['sqrt_time'] = np.sqrt(df['time_seconds'])\n    df['log_time'] = np.log1p(df['time_seconds'])\n    df['time_sin'] = np.sin(2 * np.pi * df['time_normalized'])\n    df['time_cos'] = np.cos(2 * np.pi * df['time_normalized'])\n    df['time_sin_2'] = np.sin(4 * np.pi * df['time_normalized'])\n    df['time_cos_2'] = np.cos(4 * np.pi * df['time_normalized'])\n    \n    df['is_early_play'] = (df['time_normalized'] < 0.33).astype(int)\n    df['is_mid_play'] = ((df['time_normalized'] >= 0.33) & (df['time_normalized'] < 0.67)).astype(int)\n    df['is_late_play'] = (df['time_normalized'] >= 0.67).astype(int)\n    \n    if 'pressure_index' in df.columns:\n        df['pressure_x_time'] = df['pressure_index'] * df['time_seconds']\n        df['pressure_x_late_play'] = df['pressure_index'] * df['is_late_play']\n    \n    if 'nearest_opponent_dist' in df.columns:\n        df['opponent_closing_time'] = df['nearest_opponent_dist'] / (df['s'] + 1)\n        df['space_urgency'] = df['time_seconds'] / (df['opponent_closing_time'] + 0.1)\n    \n    # Velocity & Physics\n    if 'dir' in df.columns and 's' in df.columns:\n        dir_rad = np.deg2rad(df['dir'].fillna(0))\n        o_rad = np.deg2rad(df['o'].fillna(0))\n        \n        df['velocity_x'] = df['s'] * np.sin(dir_rad)\n        df['velocity_y'] = df['s'] * np.cos(dir_rad)\n        \n        df['angle_diff_orientation_movement'] = np.abs(df['o'] - df['dir'])\n        df['angle_diff_orientation_movement'] = np.where(\n            df['angle_diff_orientation_movement'] > 180,\n            360 - df['angle_diff_orientation_movement'],\n            df['angle_diff_orientation_movement']\n        )\n        \n        if 'player_weight' in df.columns:\n            df['momentum_magnitude'] = df['player_weight'] * df['s']\n        \n        df['expected_x_constant_v'] = df['x_last'] + df['velocity_x'] * df['time_seconds']\n        df['expected_y_constant_v'] = df['y_last'] + df['velocity_y'] * df['time_seconds']\n        \n        if 'a' in df.columns:\n            df['expected_x_with_accel'] = df['x_last'] + df['velocity_x'] * df['time_seconds'] + 0.5 * df['a'] * np.sin(dir_rad) * df['time_squared']\n            df['expected_y_with_accel'] = df['y_last'] + df['velocity_y'] * df['time_seconds'] + 0.5 * df['a'] * np.cos(dir_rad) * df['time_squared']\n    \n    if 's_mean' in df.columns:\n        df['speed_consistency'] = df['s'] / (df['s_mean'] + 0.1)\n        df['speed_deviation'] = np.abs(df['s'] - df['s_mean'])\n        \n    if 'a_mean' in df.columns:\n        df['acceleration_consistency'] = df['a'] / (df['a_mean'] + 0.1)\n        df['acceleration_deviation'] = np.abs(df['a'] - df['a_mean'])\n    \n    df['time_x_speed'] = df['time_seconds'] * df['s']\n    df['time_x_acceleration'] = df['time_seconds'] * df['a']\n    df['time_squared_x_speed'] = df['time_squared'] * df['s']\n    \n    # Ball features\n    if all(col in df.columns for col in ['ball_land_x', 'ball_land_y', 'x_last', 'y_last']):\n        ball_dx = df['ball_land_x'] - df['x_last']\n        ball_dy = df['ball_land_y'] - df['y_last']\n        df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        df['ball_direction_x'] = ball_dx / (df['distance_to_ball'] + 1e-6)\n        df['ball_direction_y'] = ball_dy / (df['distance_to_ball'] + 1e-6)\n        \n        if 'o' in df.columns:\n            ball_angle = np.rad2deg(df['angle_to_ball'])\n            angle_to_ball_diff = np.abs(df['o'] - ball_angle)\n            angle_to_ball_diff = np.where(angle_to_ball_diff > 180, 360 - angle_to_ball_diff, angle_to_ball_diff)\n            df['facing_ball'] = (angle_to_ball_diff < 45).astype(int)\n        \n        estimated_ball_speed = 20.0\n        df['estimated_time_to_ball'] = df['distance_to_ball'] / estimated_ball_speed\n        df['time_ratio_to_ball'] = df['time_seconds'] / (df['estimated_time_to_ball'] + 0.1)\n        df['ball_hang_time'] = df['distance_to_ball'] / estimated_ball_speed\n        df['ball_peak_time'] = df['ball_hang_time'] / 2\n        df['time_after_peak'] = np.maximum(0, df['time_seconds'] - df['ball_peak_time'])\n        df['ball_descent_phase'] = (df['time_seconds'] > df['ball_peak_time']).astype(int)\n        \n        progress = np.clip(df['time_seconds'] / df['ball_hang_time'], 0, 1)\n        df['ball_progress'] = progress\n        df['expected_ball_x'] = df['x_last'] + ball_dx * progress\n        df['expected_ball_y'] = df['y_last'] + ball_dy * progress\n        df['distance_to_ball_trajectory'] = np.sqrt(\n            (df['x_last'] - df['expected_ball_x'])**2 + \n            (df['y_last'] - df['expected_ball_y'])**2\n        )\n        \n        if 'velocity_x' in df.columns:\n            ball_unit_x = ball_dx / (df['distance_to_ball'] + 1e-6)\n            ball_unit_y = ball_dy / (df['distance_to_ball'] + 1e-6)\n            df['closing_speed'] = df['velocity_x'] * ball_unit_x + df['velocity_y'] * ball_unit_y\n            df['projected_time_to_ball'] = df['distance_to_ball'] / (np.abs(df['closing_speed']) + 0.1)\n            df['time_urgency'] = df['time_seconds'] / (df['projected_time_to_ball'] + 0.1)\n        \n        df['distance_to_ball_x_time'] = df['distance_to_ball'] * df['time_seconds']\n        df['distance_to_ball_x_time_squared'] = df['distance_to_ball'] * df['time_squared']\n    \n    # Target features\n    if 'target_x' in df.columns:\n        target_dx = df['target_x'] - df['x_last']\n        target_dy = df['target_y'] - df['y_last']\n        df['distance_to_target'] = np.sqrt(target_dx**2 + target_dy**2)\n        df['is_target'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n        df['angle_to_target'] = np.arctan2(target_dy, target_dx)\n        df['distance_to_target_x_time'] = df['distance_to_target'] * df['time_seconds']\n        df['is_target_x_time_squared'] = df['is_target'] * df['time_squared']\n    \n    # Field position & zones\n    df['x_normalized'] = df['x_last'] / Config.FIELD_X_MAX\n    df['y_normalized'] = df['y_last'] / Config.FIELD_Y_MAX\n    df['distance_from_sideline'] = np.minimum(df['y_last'], Config.FIELD_Y_MAX - df['y_last'])\n    df['distance_from_endzone'] = np.minimum(df['x_last'], Config.FIELD_X_MAX - df['x_last'])\n    \n    df['in_hash_marks'] = ((df['y_last'] > 18.5) & (df['y_last'] < 34.8)).astype(int)\n    df['field_zone_left'] = (df['y_last'] < 17.77).astype(int)\n    df['field_zone_middle'] = ((df['y_last'] >= 17.77) & (df['y_last'] <= 35.53)).astype(int)\n    df['field_zone_right'] = (df['y_last'] > 35.53).astype(int)\n    df['depth_zone_short'] = (df['absolute_yardline_number'] < 10).astype(int)\n    df['depth_zone_medium'] = ((df['absolute_yardline_number'] >= 10) & \n                               (df['absolute_yardline_number'] < 20)).astype(int)\n    df['depth_zone_deep'] = (df['absolute_yardline_number'] >= 20).astype(int)\n    \n    # Role features\n    df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n    df['is_passer'] = (df['player_role'] == 'Passer').astype(int)\n    df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n    df['is_redzone'] = (df['absolute_yardline_number'] <= 20).astype(int)\n    \n    df['speed_percentile_player'] = df.groupby('nfl_id')['s'].rank(pct=True)\n    df['speed_vs_role_avg'] = df['s'] - df.groupby('player_role')['s'].transform('mean')\n    df['acceleration_ratio'] = df['a'] / (df['s'] + 1)\n    if 'player_weight' in df.columns:\n        df['speed_momentum'] = df['s'] * df['player_weight']\n    \n    # Interaction features\n    if 'is_target' in df.columns:\n        df['is_target_x_time'] = df['is_target'] * df['time_seconds']\n    if 'distance_to_ball' in df.columns:\n        df['distance_ball_x_speed'] = df['distance_to_ball'] * df['s']\n    \n    df['is_offense_x_early_play'] = df['is_offense'] * df['is_early_play']\n    df['is_offense_x_late_play'] = df['is_offense'] * df['is_late_play']\n    if 'is_target' in df.columns:\n        df['is_target_x_late_play'] = df['is_target'] * df['is_late_play']\n    \n    # Training targets\n    if is_training:\n        df['displacement_x'] = df['x'] - df['x_last']\n        df['displacement_y'] = df['y'] - df['y_last']\n        \n        valid_mask = (\n            df['displacement_x'].notna() & \n            df['displacement_y'].notna() &\n            (np.sqrt(df['displacement_x']**2 + df['displacement_y']**2) <= Config.MAX_SPEED * df['time_seconds'] * 1.5)\n        )\n        df = df[valid_mask].reset_index(drop=True)\n    \n    # Fill NaN\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = df[numeric_cols].fillna(0)\n    \n    return df\n\n# ================================================================================\n# NEURAL NETWORK\n# ================================================================================\n\nclass EnhancedNN(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            \n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            \n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            \n            nn.Linear(128, 64),\n            nn.ReLU(),\n            \n            nn.Linear(64, 1)\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\ndef train_neural_network(X_train, y_train, X_val, y_val, seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.reshape(-1, 1)))\n    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val.reshape(-1, 1)))\n    \n    train_loader = DataLoader(train_dataset, batch_size=Config.NN_BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=Config.NN_BATCH_SIZE)\n    \n    model = EnhancedNN(X_train.shape[1]).to(device)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=Config.NN_LEARNING_RATE)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n    \n    best_val_loss = float('inf')\n    best_model_state = model.state_dict()\n    patience_counter = 0\n    \n    for epoch in range(Config.NN_EPOCHS):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n        \n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                val_losses.append(loss.item())\n        \n        avg_val_loss = np.mean(val_losses)\n        scheduler.step(avg_val_loss)\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_model_state = model.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= 10:\n                break\n    \n    model.load_state_dict(best_model_state)\n    return model\n\n# ================================================================================\n# ENSEMBLE TRAINING\n# ================================================================================\n\ndef train_ensemble(train_data, features, seed=42):\n    print(f\"\\nTraining ensemble with seed {seed}...\")\n    \n    X = train_data[features].values\n    y_dx = train_data['displacement_x'].values\n    y_dy = train_data['displacement_y'].values\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    groups = train_data['game_id'].values\n    gkf = GroupKFold(n_splits=Config.N_FOLDS)\n    \n    models_dx = {'xgb': [], 'lgb': [], 'cat': [], 'nn': []}\n    models_dy = {'xgb': [], 'lgb': [], 'cat': [], 'nn': []}\n    \n    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, groups=groups)):\n        print(f\"  Fold {fold + 1}/{Config.N_FOLDS}\")\n        \n        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n        y_train_dx, y_val_dx = y_dx[train_idx], y_dx[val_idx]\n        y_train_dy, y_val_dy = y_dy[train_idx], y_dy[val_idx]\n        \n        # XGBoost\n        xgb_dx = XGBRegressor(n_estimators=1200, learning_rate=0.05, max_depth=9, \n                             subsample=0.8, colsample_bytree=0.8, random_state=seed + fold, \n                             tree_method='hist', verbosity=0)\n        xgb_dx.fit(X_train, y_train_dx)\n        models_dx['xgb'].append(xgb_dx)\n        \n        xgb_dy = XGBRegressor(n_estimators=1200, learning_rate=0.05, max_depth=9, \n                             subsample=0.8, colsample_bytree=0.8, random_state=seed + fold + 100, \n                             tree_method='hist', verbosity=0)\n        xgb_dy.fit(X_train, y_train_dy)\n        models_dy['xgb'].append(xgb_dy)\n        \n        # LightGBM\n        lgb_dx = LGBMRegressor(n_estimators=1200, learning_rate=0.05, max_depth=9, \n                              num_leaves=120, subsample=0.8, colsample_bytree=0.8,\n                              random_state=seed + fold, verbosity=-1)\n        lgb_dx.fit(X_train, y_train_dx)\n        models_dx['lgb'].append(lgb_dx)\n        \n        lgb_dy = LGBMRegressor(n_estimators=1200, learning_rate=0.05, max_depth=9, \n                              num_leaves=120, subsample=0.8, colsample_bytree=0.8,\n                              random_state=seed + fold + 100, verbosity=-1)\n        lgb_dy.fit(X_train, y_train_dy)\n        models_dy['lgb'].append(lgb_dy)\n        \n        # CatBoost\n        cat_dx = CatBoostRegressor(iterations=1200, learning_rate=0.05, depth=9, \n                                   l2_leaf_reg=3, random_seed=seed + fold, verbose=False)\n        cat_dx.fit(X_train, y_train_dx)\n        models_dx['cat'].append(cat_dx)\n        \n        cat_dy = CatBoostRegressor(iterations=1200, learning_rate=0.05, depth=9, \n                                   l2_leaf_reg=3, random_seed=seed + fold + 100, verbose=False)\n        cat_dy.fit(X_train, y_train_dy)\n        models_dy['cat'].append(cat_dy)\n        \n        # Neural Network\n        nn_dx = train_neural_network(X_train, y_train_dx, X_val, y_val_dx, seed + fold)\n        models_dx['nn'].append(nn_dx)\n        \n        nn_dy = train_neural_network(X_train, y_train_dy, X_val, y_val_dy, seed + fold + 100)\n        models_dy['nn'].append(nn_dy)\n    \n    return models_dx, models_dy, scaler\n\ndef predict_ensemble(models_dx, models_dy, scaler, X_test):\n    X_scaled = scaler.transform(X_test)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    all_preds_dx = []\n    for model_type in ['xgb', 'lgb', 'cat']:\n        preds = [model.predict(X_scaled) for model in models_dx[model_type]]\n        all_preds_dx.append(np.mean(preds, axis=0))\n    \n    X_tensor = torch.FloatTensor(X_scaled).to(device)\n    nn_preds = []\n    for model in models_dx['nn']:\n        model.eval()\n        with torch.no_grad():\n            pred = model(X_tensor).cpu().numpy().squeeze()\n        nn_preds.append(pred)\n    all_preds_dx.append(np.mean(nn_preds, axis=0))\n    \n    pred_dx = np.mean(all_preds_dx, axis=0)\n    \n    all_preds_dy = []\n    for model_type in ['xgb', 'lgb', 'cat']:\n        preds = [model.predict(X_scaled) for model in models_dy[model_type]]\n        all_preds_dy.append(np.mean(preds, axis=0))\n    \n    nn_preds = []\n    for model in models_dy['nn']:\n        model.eval()\n        with torch.no_grad():\n            pred = model(X_tensor).cpu().numpy().squeeze()\n        nn_preds.append(pred)\n    all_preds_dy.append(np.mean(nn_preds, axis=0))\n    \n    pred_dy = np.mean(all_preds_dy, axis=0)\n    \n    return pred_dx, pred_dy\n\n# ================================================================================\n# POST-PROCESSING\n# ================================================================================\n\ndef apply_constraints(pred_x, pred_y, x_last, y_last, time_seconds):\n    dx = pred_x - x_last\n    dy = pred_y - y_last\n    displacement = np.sqrt(dx**2 + dy**2)\n    \n    max_displacement = Config.MAX_SPEED * time_seconds\n    \n    mask = displacement > max_displacement\n    if np.any(mask):\n        scale = max_displacement[mask] / (displacement[mask] + 1e-6)\n        dx[mask] *= scale\n        dy[mask] *= scale\n        pred_x[mask] = x_last[mask] + dx[mask]\n        pred_y[mask] = y_last[mask] + dy[mask]\n    \n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    return pred_x, pred_y\n\ndef smooth_trajectories(test_data, pred_x, pred_y):\n    test_data = test_data.copy()\n    test_data['pred_x'] = pred_x\n    test_data['pred_y'] = pred_y\n    \n    for (game_id, play_id, nfl_id), group in test_data.groupby(['game_id', 'play_id', 'nfl_id']):\n        if len(group) > 3:\n            idx = group.index\n            test_data.loc[idx, 'pred_x'] = gaussian_filter1d(group['pred_x'].values, sigma=0.5)\n            test_data.loc[idx, 'pred_y'] = gaussian_filter1d(group['pred_y'].values, sigma=0.5)\n    \n    return test_data['pred_x'].values, test_data['pred_y'].values\n\n# ================================================================================\n# MAIN PIPELINE\n# ================================================================================\n\ndef main():\n    print(\"=\"*80)\n    print(\" NFL BIG DATA BOWL 2026 - ENHANCED WITH GRAPH NEURAL FEATURES\")\n    print(\"=\"*80)\n    \n    train_input, train_output, test_input, test_template = load_data()\n    \n    print(\"\\nPreparing features with graph neural enhancements...\")\n    train_data = prepare_features(train_input, train_output, is_training=True)\n    test_data = prepare_features(test_input, test_template, is_training=False)\n    \n    print(f\"Train shape: {train_data.shape}\")\n    print(f\"Test shape: {test_data.shape}\")\n    \n    # Auto-detect all numeric features\n    exclude_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y', \n                    'displacement_x', 'displacement_y', 'x_last', 'y_last']\n    feature_cols = [col for col in train_data.select_dtypes(include=[np.number]).columns \n                    if col not in exclude_cols]\n    \n    print(f\"\\nUsing {len(feature_cols)} total features\")\n    print(\"New features: graph neural embeddings, attention mechanisms, graph centrality\")\n    \n    all_predictions_dx = []\n    all_predictions_dy = []\n    \n    for seed in Config.SEEDS:\n        models_dx, models_dy, scaler = train_ensemble(train_data, feature_cols, seed)\n        \n        X_test = test_data[feature_cols].values\n        pred_dx, pred_dy = predict_ensemble(models_dx, models_dy, scaler, X_test)\n        \n        all_predictions_dx.append(pred_dx)\n        all_predictions_dy.append(pred_dy)\n        \n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n        gc.collect()\n    \n    print(\"\\nAveraging predictions...\")\n    final_pred_dx = np.mean(all_predictions_dx, axis=0)\n    final_pred_dy = np.mean(all_predictions_dy, axis=0)\n    \n    pred_x = test_data['x_last'].values + final_pred_dx\n    pred_y = test_data['y_last'].values + final_pred_dy\n    \n    print(\"Applying physics constraints...\")\n    pred_x, pred_y = apply_constraints(\n        pred_x, pred_y,\n        test_data['x_last'].values,\n        test_data['y_last'].values,\n        test_data['time_seconds'].values\n    )\n    \n    print(\"Smoothing trajectories...\")\n    pred_x, pred_y = smooth_trajectories(test_data, pred_x, pred_y)\n    \n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    print(\"\\nCreating submission...\")\n    submission = pd.DataFrame({\n        'id': (test_data['game_id'].astype(str) + \"_\" +\n               test_data['play_id'].astype(str) + \"_\" +\n               test_data['nfl_id'].astype(str) + \"_\" +\n               test_data['frame_id'].astype(str)),\n        'x': pred_x,\n        'y': pred_y\n    })\n    \n    submission.to_csv(\"submission.csv\", index=False)\n    \n    print(f\"\\n Submission saved: {len(submission)} predictions\")\n    print(f\"X: mean={submission['x'].mean():.2f}, std={submission['x'].std():.2f}\")\n    print(f\"Y: mean={submission['y'].mean():.2f}, std={submission['y'].std():.2f}\")\n    \n    print(\"\\nFirst 5 predictions:\")\n    print(submission.head())\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" COMPLETE WITH GRAPH NEURAL ENHANCEMENTS!\")\n    print(\"=\"*80)\n    \n    return submission\n\nif __name__ == \"__main__\":\n    submission = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}