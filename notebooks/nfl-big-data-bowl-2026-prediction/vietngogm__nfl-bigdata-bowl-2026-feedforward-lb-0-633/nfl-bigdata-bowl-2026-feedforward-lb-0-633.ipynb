{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# Import packages\n# ============================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm \n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display options\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 120)\nsns.set_style(\"whitegrid\")\n\nprint(\"‚úÖ Libraries installed and imported successfully!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n#  Load and Explore Data\n# ============================================================\n\n# Paths to your uploaded files\ntest_input_path = \"/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv\"\ntest_path = \"/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv\"\n\n\n# Load datasets\ntest_input = pd.read_csv(test_input_path)\ntest = pd.read_csv(test_path)\n\n# Show basic info\nprint(\"‚úÖ Files loaded successfully!\\n\")\n\nprint(\"test_input.csv shape:\", test_input.shape)\nprint(\"test.csv shape:\", test.shape)\n\nprint(\"\\n--- test_input columns ---\")\nprint(test_input.columns.tolist())\n\nprint(\"\\n--- test columns ---\")\nprint(test.columns.tolist())\n\n# Display first few rows\ndisplay(test_input.head(3))\ndisplay(test.head(3))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# 1Ô∏è‚É£ Encode categorical columns\n# ---------------------------\ntest_input = test_input.copy()\n\n# Encode categorical features\nfor col in ['player_side', 'player_role', 'player_position']:\n    if col in test_input.columns:\n        le = LabelEncoder()\n        test_input[col + '_enc'] = le.fit_transform(test_input[col].astype(str))\n\n# ---------------------------\n# 2Ô∏è‚É£ Add geometric & distance-based features\n# ---------------------------\ndef add_geometric_features(df):\n    df = df.copy()\n    \n    # Distance from player to ball landing location\n    df['distance_to_ball'] = np.sqrt(\n        (df['x'] - df['ball_land_x'])**2 + (df['y'] - df['ball_land_y'])**2\n    )\n    \n    # Relative positions\n    df['relative_x'] = df['x'] - df['ball_land_x']\n    df['relative_y'] = df['y'] - df['ball_land_y']\n    \n    # Direction towards ball (angle)\n    df['angle_to_ball'] = np.degrees(np.arctan2(df['relative_y'], df['relative_x']))\n    \n    # Speed and acceleration (already provided)\n    df['speed'] = df['s']\n    df['accel'] = df['a']\n    \n    # Normalized frame progress\n    df['frame_id_norm'] = df.groupby(['game_id', 'play_id'])['frame_id'].transform(\n        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-6)\n    )\n    \n    # Target flag (if this player is the one to predict)\n    df['is_targeted'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n    \n    return df\n\n# Apply to dataset\nfeatures_df = add_geometric_features(test_input)\n\nprint(\"‚úÖ Feature engineering complete\")\nprint(\"Feature sample:\")\ndisplay(features_df.head(5))\n\n# ---------------------------\n# 3Ô∏è‚É£ Select model input columns\n# ---------------------------\nfeature_cols = [\n    'x', 'y', 's', 'a', 'distance_to_ball', 'angle_to_ball',\n    'relative_x', 'relative_y', 'frame_id_norm', 'is_targeted',\n    'player_side_enc', 'player_role_enc', 'player_position_enc'\n]\n\nprint(f\"Total engineered features: {len(feature_cols)}\")\nprint(feature_cols)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Step 5: Two Model Versions\n#  - Baseline: GradientBoostingRegressor (sklearn)\n#  - Neural: PyTorch feedforward + time-conditioning\n# ============================================================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# -----------------------------\n# 1) Discover training files\n# -----------------------------\ninput_files = []\noutput_files = []\nfor dirname, _, filenames in os.walk('/kaggle/input/nfl-big-data-bowl-2026-prediction/train/'):\n    for fn in filenames:\n        if fn.lower().startswith(\"input_\") and fn.lower().endswith(\".csv\"):\n            input_files.append(os.path.join(dirname, fn))\n        if fn.lower().startswith(\"output_\") and fn.lower().endswith(\".csv\"):\n            output_files.append(os.path.join(dirname, fn))\n\n#/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w17.csv\n#/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w05.csv\n\n# If the canonical 'train' folder exists (competition legacy names), include those\nif not input_files or not output_files:\n    # fallback to common names\n    #for dirname, _, filenames in os.walk('/kaggle/input'):\n    for dirname, _, filenames in os.walk('/kaggle/input/nfl-big-data-bowl-2026-prediction/train/'): \n        for fn in filenames:\n            if \"input_2023\" in fn.lower() or \"input\" in fn.lower():\n                if fn.lower().endswith('.csv'):\n                    input_files.append(os.path.join(dirname, fn))\n            if \"output_2023\" in fn.lower() or \"output\" in fn.lower():\n                if fn.lower().endswith('.csv'):\n                    output_files.append(os.path.join(dirname, fn))\n\nprint(\"Found input files:\", input_files[:5])\nprint(\"Found output files:\", output_files[:5])\n\nif not input_files or not output_files:\n    print(\"\\n‚ö†Ô∏è Could not find training input/output CSVs under /kaggle/input.\")\n    print(\"If you do have training files, place them in the Kaggle input directory or adapt paths.\")\n    raise SystemExit()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# 2) Load & concat training files\n# -----------------------------\ntrain_input = pd.concat([pd.read_csv(p) for p in input_files], ignore_index=True)\ntrain_output = pd.concat([pd.read_csv(p) for p in output_files], ignore_index=True)\n\nprint(\"train_input shape:\", train_input.shape)\nprint(\"train_output shape:\", train_output.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# 3) Build training examples:\n#    - For each (game, play, nfl_id) take last input row (pre-pass snapshot)\n#    - Join to all output frames (these are the targets)\n#    - Each output frame becomes one training row; features come from last input snapshot\n# -----------------------------\n\n# get last input snapshot per player per play\nlast_input = (\n    train_input\n    .sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False)\n    .last()\n)\n\n# join with all output rows (targets)\nmerged = pd.merge(\n    train_output,\n    last_input.drop(columns=['x', 'y']),  # drop input x,y because output x,y are targets\n    on=['game_id', 'play_id', 'nfl_id'],\n    how='left',\n    suffixes=('_out', '_in')\n)\n\n# drop rows with missing critical values\nmerged = merged.dropna(subset=['frame_id_out', 'ball_land_x', 'ball_land_y'])\n\n# keep only relevant columns\nprint(\"Merged training rows:\", merged.shape)\ndisplay(merged.head())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# 4) Feature engineering\n# -----------------------------\ndef add_features(df):\n    df = df.copy()\n\n    # Fix missing column names ‚Äî detect what exists\n    if 'x_in' not in df.columns:\n        # if the merge dropped input x/y, try using backup naming\n        input_x_col = 'x_snap' if 'x_snap' in df.columns else None\n        input_y_col = 'y_snap' if 'y_snap' in df.columns else None\n    else:\n        input_x_col, input_y_col = 'x_in', 'y_in'\n\n    # fallback if not found (skip feature)\n    if input_x_col is None or input_y_col is None:\n        print(\"‚ö†Ô∏è Warning: No input x/y found; creating dummy 0 columns\")\n        df['x_in'] = 0.0\n        df['y_in'] = 0.0\n        input_x_col, input_y_col = 'x_in', 'y_in'\n    else:\n        df['x_in'] = df[input_x_col]\n        df['y_in'] = df[input_y_col]\n\n    # basic geometry relative to ball landing\n    df['distance_to_ball'] = np.sqrt((df['x_in'] - df['ball_land_x'])**2 + (df['y_in'] - df['ball_land_y'])**2)\n    df['relative_x'] = df['x_in'] - df['ball_land_x']\n    df['relative_y'] = df['y_in'] - df['ball_land_y']\n    df['angle_to_ball'] = np.degrees(np.arctan2(df['relative_y'], df['relative_x']))\n\n    # normalize frame id\n    if 'frame_id_out' in df.columns:\n        fid = 'frame_id_out'\n    elif 'frame_id' in df.columns:\n        fid = 'frame_id'\n    else:\n        fid = None\n\n    if fid:\n        df['frame_id_norm'] = (\n            (df[fid] - df.groupby(['game_id', 'play_id'])[fid].transform('min')) /\n            (df.groupby(['game_id', 'play_id'])[fid].transform('max') -\n             df.groupby(['game_id', 'play_id'])[fid].transform('min') + 1e-6)\n        )\n\n    # speed & accel from snapshot\n    if 's' in df.columns: df['speed'] = df['s']\n    if 'a' in df.columns: df['accel'] = df['a']\n\n    # targeted receiver flag (if available)\n    df['is_targeted'] = (df.get('player_role', '') == 'Targeted Receiver').astype(int)\n    df['player_side_enc'] = (df.get('player_side', '') == 'Offense').astype(int)\n\n    # keep snapshot positions for later use\n    df['x_snap'] = df['x_in']\n    df['y_snap'] = df['y_in']\n\n    return df\n\n\n# Apply feature function\nmerged = add_features(merged)\n\n# target columns\nmerged['target_x'] = merged.get('x_out', merged.get('x'))\nmerged['target_y'] = merged.get('y_out', merged.get('y'))\n\n# Create relative frame index per player\nfid_col = 'frame_id_out' if 'frame_id_out' in merged.columns else 'frame_id'\nmerged['out_frame_index'] = (\n    merged.groupby(['game_id','play_id','nfl_id'])[fid_col]\n    .rank(method='first')\n    .astype(int)\n)\n\n# Debug sample\nprint(\"‚úÖ Example training sample:\")\ndisplay(merged[[\n    'game_id','play_id','nfl_id',fid_col,'out_frame_index',\n    'x_snap','y_snap','target_x','target_y',\n    'distance_to_ball','angle_to_ball','is_targeted'\n]].head(5))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# 5) Prepare X,y\n# Each training row corresponds to one output frame (frame_id in output)\n# We'll use the snapshot features + the integer index of the output frame (normalized) as input.\n# -----------------------------\nfeature_cols = [\n    'x_snap','y_snap','speed','accel','distance_to_ball','angle_to_ball',\n    'relative_x','relative_y','frame_id_norm','is_targeted','player_side_enc'\n]\n# add normalized out_frame_index\nmerged['out_frame_norm'] = merged['out_frame_index'] / (merged.groupby(['game_id','play_id','nfl_id'])['out_frame_index'].transform('max') + 1e-6)\nfeature_cols.append('out_frame_norm')\n\nX = merged[feature_cols].fillna(0).values\ny = merged[['target_x','target_y']].values\n\nprint(\"X shape:\", X.shape, \"y shape:\", y.shape)\n# -----------------------------\n# 6) Train/val split\n# -----------------------------\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\nprint(\"Train samples:\", X_train.shape[0], \"Val samples:\", X_val.shape[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# 7) Baseline model: GradientBoosting with MultiOutputRegressor\n# -----------------------------\n# print(\"\\n=== Baseline: GradientBoostingRegressor ===\")\n# base_est = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)\n# baseline = MultiOutputRegressor(base_est)\n# baseline.fit(X_train, y_train)\n# y_pred_val = baseline.predict(X_val)\n# rmse_baseline = np.sqrt(mean_squared_error(y_val, y_pred_val))\n# print(f\"Baseline RMSE (x,y combined): {rmse_baseline:.4f}\")\n\n# # also compute RMSE per coordinate\n# rmse_x = np.sqrt(mean_squared_error(y_val[:,0], y_pred_val[:,0]))\n# rmse_y = np.sqrt(mean_squared_error(y_val[:,1], y_pred_val[:,1]))\n# print(f\"RMSE x: {rmse_x:.4f}, RMSE y: {rmse_y:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# 8) Neural model: simple feedforward that conditions on out_frame_norm\n# -----------------------------\nprint(\"\\n=== Neural Model (PyTorch): Feedforward Conditioning on time index ===\")\n\nclass FeedforwardTimeModel(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2)  # predict x,y\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# convert data to torch\nX_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\nX_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)\n\nmodel = FeedforwardTimeModel(X_train.shape[1]).to(device)\nopt = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\n\n# training loop\nn_epochs = 40\nbatch_size = 1024\nbest_val_rmse = 1e9\nbest_state = None\n\nfor epoch in range(n_epochs):\n    model.train()\n    perm = np.random.permutation(X_train_t.shape[0])\n    losses = []\n    for i in range(0, len(perm), batch_size):\n        idx = perm[i:i+batch_size]\n        xb = X_train_t[idx]\n        yb = y_train_t[idx]\n        pred = model(xb)\n        loss = criterion(pred, yb)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        losses.append(loss.item())\n    # validation\n    model.eval()\n    with torch.no_grad():\n        pred_val = model(X_val_t).cpu().numpy()\n    val_rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n    if val_rmse < best_val_rmse:\n        best_val_rmse = val_rmse\n        best_state = model.state_dict()\n    if epoch % 5 == 0 or epoch == n_epochs-1:\n        print(f\"Epoch {epoch+1}/{n_epochs} | train_loss={np.mean(losses):.4f} | val_rmse={val_rmse:.4f}\")\n\n# load best weights\nif best_state is not None:\n    model.load_state_dict(best_state)\nprint(f\"Best neural val RMSE: {best_val_rmse:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ---------PLOT DISTRIBUTION------------------\n# # 1Ô∏è‚É£ Basic summary statistics\n# # ---------------------------\n# print(\"=== Basic Data Info ===\")\n# print(test_input.info())\n# print(\"\\n=== Sample Rows ===\")\n# display(test_input.head(5))\n\n# print(\"\\n=== Unique Games & Plays ===\")\n# print(\"Games:\", test_input['game_id'].nunique())\n# print(\"Plays:\", test_input['play_id'].nunique())\n# print(\"Players:\", test_input['nfl_id'].nunique())\n\n# # ---------------------------\n# # 2Ô∏è‚É£ Distribution plots\n# # ---------------------------\n# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# sns.histplot(test_input['x'], bins=50, ax=axes[0], color='skyblue', kde=True)\n# axes[0].set_title(\"Distribution of X Positions (Field Length)\")\n# axes[0].set_xlabel(\"X (yards)\")\n\n# sns.histplot(test_input['y'], bins=30, ax=axes[1], color='lightgreen', kde=True)\n# axes[1].set_title(\"Distribution of Y Positions (Field Width)\")\n# axes[1].set_xlabel(\"Y (yards)\")\n\n# sns.countplot(x='player_side', data=test_input, ax=axes[2], palette='Set2')\n# axes[2].set_title(\"Offense vs Defense Player Counts\")\n\n# plt.tight_layout()\n# plt.show()\n\n# # ---------------------------\n# # 3Ô∏è‚É£ Function to visualize play on field\n# # ---------------------------\n# def plot_play(df, game_id, play_id):\n#     play_df = df[(df['game_id'] == game_id) & (df['play_id'] == play_id)]\n#     if play_df.empty:\n#         print(f\"No data found for game {game_id}, play {play_id}\")\n#         return\n\n#     plt.figure(figsize=(10, 5))\n#     plt.title(f\"Player Positions - Game {game_id}, Play {play_id}\", fontsize=14)\n\n#     # Field boundaries\n#     plt.xlim(0, 120)\n#     plt.ylim(0, 53.3)\n#     plt.gca().set_facecolor(\"mediumseagreen\")\n\n#     # Plot players\n#     off = play_df[play_df['player_side'] == 'Offense']\n#     defn = play_df[play_df['player_side'] == 'Defense']\n\n#     plt.scatter(off['x'], off['y'], color='blue', label='Offense', s=50, alpha=0.7)\n#     plt.scatter(defn['x'], defn['y'], color='red', label='Defense', s=50, alpha=0.7)\n\n#     # Mark ball landing\n#     if 'ball_land_x' in play_df.columns and 'ball_land_y' in play_df.columns:\n#         plt.scatter(play_df['ball_land_x'].iloc[0], play_df['ball_land_y'].iloc[0], \n#                     color='gold', s=120, marker='*', edgecolor='black', label='Ball Landing')\n\n#     plt.xlabel(\"X (yards along field length)\")\n#     plt.ylabel(\"Y (yards across field width)\")\n#     plt.legend()\n#     plt.show()\n\n# # ---------------------------\n# # 4Ô∏è‚É£ Visualize one or two sample plays\n# # ---------------------------\n# sample_game = test_input['game_id'].iloc[0]\n# sample_play = test_input['play_id'].iloc[0]\n# plot_play(test_input, sample_game, sample_play)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from multiprocessing import Pool as MultiprocessingPool, cpu_count","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# import polars as pl\n# import numpy as np\n# import torch\n# from sklearn.ensemble import GradientBoostingRegressor\n# from sklearn.multioutput import MultiOutputRegressor\n\n# import kaggle_evaluation.nfl_inference_server\n\n# # -----------------------------------------------------\n# # Load your pre-trained models here (if pre-saved)\n# # Or reinitialize from current notebook context\n# # -----------------------------------------------------\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# # Assume we have baseline and neural model objects defined in memory\n# # If you saved them as pickle/pt, load them here\n# # Example:\n# # baseline = joblib.load(\"/kaggle/input/my-models/baseline.pkl\")\n# # model.load_state_dict(torch.load(\"/kaggle/input/my-models/neural.pt\"))\n\n# # -----------------------------------------------------\n# # Define the feature function (reused from your notebook)\n# # -----------------------------------------------------\n# def add_features(df: pd.DataFrame) -> pd.DataFrame:\n#     df = df.copy()\n#     if 'x' in df.columns and 'ball_land_x' in df.columns:\n#         df['distance_to_ball'] = np.sqrt((df['x'] - df['ball_land_x'])**2 + (df['y'] - df['ball_land_y'])**2)\n#         df['relative_x'] = df['x'] - df['ball_land_x']\n#         df['relative_y'] = df['y'] - df['ball_land_y']\n#         df['angle_to_ball'] = np.degrees(np.arctan2(df['relative_y'], df['relative_x']))\n#     else:\n#         df['distance_to_ball'] = 0.0\n#         df['relative_x'] = 0.0\n#         df['relative_y'] = 0.0\n#         df['angle_to_ball'] = 0.0\n\n#     if 's' in df.columns: df['speed'] = df['s']\n#     if 'a' in df.columns: df['accel'] = df['a']\n\n#     df['is_targeted'] = (df.get('player_role', '') == 'Targeted Receiver').astype(int)\n#     df['player_side_enc'] = (df.get('player_side', '') == 'Offense').astype(int)\n\n#     df['x_snap'] = df['x']\n#     df['y_snap'] = df['y']\n#     return df\n\n\n# # -----------------------------------------------------\n# # The main predict() function required by Kaggle\n# # -----------------------------------------------------\n# def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n#     \"\"\"Generate predictions for x, y positions for each player-frame in the test batch.\"\"\"\n#     # Convert Polars to Pandas for processing\n#     test = test.to_pandas()\n#     test_input = test_input.to_pandas()\n\n#     # Merge with last known player state (test_input is pre-throw snapshot)\n#     merged = pd.merge(\n#         test,\n#         test_input.groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False).last(),\n#         on=[\"game_id\", \"play_id\", \"nfl_id\"],\n#         how=\"left\",\n#         suffixes=(\"\", \"_snap\")\n#     )\n\n#      # Add features\n#     merged = add_features(merged)\n\n#     # Compute normalized frame id\n#     merged[\"frame_id_norm\"] = (\n#         merged.groupby([\"game_id\", \"play_id\"])[\"frame_id\"]\n#         .transform(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-6))\n#     )\n\n#     # Normalized output frame index per player\n#     merged[\"out_frame_index\"] = merged.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"frame_id\"].rank(method=\"first\")\n#     merged[\"out_frame_norm\"] = merged[\"out_frame_index\"] / (\n#         merged.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"out_frame_index\"].transform(\"max\") + 1e-6\n#     )\n\n#     # Features for inference (must match training exactly!)\n#     feature_cols = [\n#         \"x_snap\", \"y_snap\", \"speed\", \"accel\",\n#         \"distance_to_ball\", \"angle_to_ball\",\n#         \"relative_x\", \"relative_y\",\n#         \"frame_id_norm\", \"is_targeted\",\n#         \"player_side_enc\", \"out_frame_norm\"\n#     ]\n\n#     X = merged[feature_cols].fillna(0).values\n\n\n#     # Predictions from baseline\n#     baseline_preds = baseline.predict(X)\n\n#     # Predictions from neural model\n#     model.eval()\n#     with torch.no_grad():\n#         Xt = torch.tensor(X, dtype=torch.float32).to(device)\n#         neural_preds = model(Xt).cpu().numpy()\n\n#     # Blend\n#     x_pred = 0.5 * baseline_preds[:, 0] + 0.5 * neural_preds[:, 0]\n#     y_pred = 0.5 * baseline_preds[:, 1] + 0.5 * neural_preds[:, 1]\n\n#     predictions = pl.DataFrame({\n#         \"x\": x_pred.tolist(),\n#         \"y\": y_pred.tolist()\n#     })\n\n#     assert len(predictions) == len(test)\n#     return predictions\n\n\n# # -----------------------------------------------------\n# # Run server for evaluation or local gateway testing\n# # -----------------------------------------------------\n# inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n\n# if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n#     inference_server.serve()\n# else:\n#     inference_server.run_local_gateway((\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\",))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# üèà NFL Big Data Bowl 2026 ‚Äî Final Submission\n# ============================================================\n\nimport os\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nimport torch\nimport kaggle_evaluation.nfl_inference_server\n\n\n# ------------------------------------------------------------\n# CONFIG\n# ------------------------------------------------------------\nclass Config:\n    MODEL_DIR = \"/kaggle/input/my-trained-models\"  # adjust if you have saved models\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BLEND_WEIGHT = 0.5  # baseline / neural blend\n\n\n# ------------------------------------------------------------\n# MAIN PREDICTOR CLASS\n# ------------------------------------------------------------\nclass MainPredictor:\n    \"\"\"Unified model wrapper for inference.\"\"\"\n    def __init__(self):\n        print(\"üß† Initializing Main Predictor...\")\n        self.device = Config.DEVICE\n        self.baseline = baseline       # GradientBoostingRegressor\n        self.model = model             # PyTorch MLP\n        self.model.eval()\n        print(f\"‚úÖ Models loaded (device={self.device})\")\n\n    def _prepare_features(self, test: pd.DataFrame, test_input: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Merge and compute engineered features.\"\"\"\n        # Merge last known snapshot\n        last_snap = (\n            test_input.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n            .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n            .last()\n        )\n        df = pd.merge(\n            test,\n            last_snap,\n            on=[\"game_id\", \"play_id\", \"nfl_id\"],\n            how=\"left\",\n            suffixes=(\"\", \"_snap\")\n        )\n        df = add_features(df)\n\n        # Normalized frame ids\n        df[\"frame_id_norm\"] = (\n            df.groupby([\"game_id\", \"play_id\"])[\"frame_id\"]\n            .transform(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-6))\n        )\n        df[\"out_frame_index\"] = df.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"frame_id\"].rank(method=\"first\")\n        df[\"out_frame_norm\"] = df[\"out_frame_index\"] / (\n            df.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"out_frame_index\"].transform(\"max\") + 1e-6\n        )\n\n        # Ensure consistent features\n        feature_cols = [\n            \"x_snap\", \"y_snap\", \"speed\", \"accel\",\n            \"distance_to_ball\", \"angle_to_ball\",\n            \"relative_x\", \"relative_y\",\n            \"frame_id_norm\", \"is_targeted\",\n            \"player_side_enc\", \"out_frame_norm\"\n        ]\n        for col in feature_cols:\n            if col not in df.columns:\n                df[col] = 0.0\n        return df, feature_cols\n\n    def predict(self, test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n        \"\"\"Main prediction logic for Kaggle inference API.\"\"\"\n        test = test.to_pandas()\n        test_input = test_input.to_pandas()\n\n        # Feature preparation\n        merged, feature_cols = self._prepare_features(test, test_input)\n        X = merged[feature_cols].fillna(0).astype(float).values\n\n        # Baseline predictions\n        base_preds = self.baseline.predict(X)\n\n        # Neural predictions\n        with torch.no_grad():\n            Xt = torch.tensor(X, dtype=torch.float32).to(self.device)\n            neural_preds = self.model(Xt).cpu().numpy()\n\n        # Blend both predictions\n        w = Config.BLEND_WEIGHT\n        x_pred = w * base_preds[:, 0] + (1 - w) * neural_preds[:, 0]\n        y_pred = w * base_preds[:, 1] + (1 - w) * neural_preds[:, 1]\n\n        preds = pl.DataFrame({\"x\": x_pred.tolist(), \"y\": y_pred.tolist()})\n        assert len(preds) == len(test)\n        return preds\n\n\n# ------------------------------------------------------------\n# SERVER SETUP\n# ------------------------------------------------------------\nlb_predictor = MainPredictor()\n\ndef predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Competition prediction function using blended pipeline.\"\"\"\n    return lb_predictor.predict(test, test_input)\n\n\nprint(\"üöÄ Setting up NFL Big Data Bowl 2026 Inference Server...\")\nprint(f\"üìÅ Model directory: {Config.MODEL_DIR}\")\nprint(f\"üéØ Blended Predictor: Baseline + Neural\")\nprint(f\"üîß Features: 12 engineered + normalized time features\")\nprint(f\"üèà Model blend weight: {Config.BLEND_WEIGHT:.2f}\")\nprint(f\"üíª Device: {Config.DEVICE}\")\n\ninference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    print(\"üèà Starting competition inference server with blended pipeline...\")\n    inference_server.serve()\nelse:\n    print(\"üî¨ Running local gateway for testing...\")\n    inference_server.run_local_gateway((\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\",))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lb_predictor = MainPredictor()\n\n# def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n#     \"\"\"Competition prediction function using LB 0.604 pipeline\"\"\"\n#     return lb_predictor.predict(test, test_input)\n\n# # SERVER SETUP\n# print(\"üöÄ Setting up NFL Big Data Bowl 2026 Inference Server...\")\n# print(f\"üìÅ Model directory: {Config.MODEL_DIR}\")\n# print(f\"üéØ Target: LB 0.604 Performance\")\n# print(f\"üîß Features: 114 complete features with player interactions\")\n# print(f\"üèà Model: 5-fold ensemble with full feature engineering\")\n\n# inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n\n# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n#     print(\"üèà Starting competition inference server with LB 0.604 pipeline...\")\n#     inference_server.serve()\n# else:\n#     print(\"üî¨ Running local test gateway with LB 0.604 pipeline...\")\n#     inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ============================================================\n# # üèÅ STEP 11 ‚Äî Generate Final Submission File\n# # ============================================================\n\n# import pandas as pd\n# import numpy as np\n# import torch\n# import os\n\n# # ------------------------------------------------------------\n# # 1Ô∏è‚É£ Locate and load test files\n# # ------------------------------------------------------------\n# test_candidates = [\n#     \"/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv\"\n#     # \"/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv\",\n#     # \"/mnt/data/test.csv\"\n# ]\n# test_path = next((p for p in test_candidates if os.path.exists(p)), None)\n# if test_path is None:\n#     raise FileNotFoundError(\"‚ùå test.csv not found.\")\n# test_df = pd.read_csv(test_path)\n# print(f\"‚úÖ Loaded test.csv ‚Äî {len(test_df):,} rows\")\n\n# test_input_candidates = [\n#     \"/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv\"\n#     # \"/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv\",\n#     # \"/mnt/data/test_input.csv\"\n# ]\n# test_input_path = next((p for p in test_input_candidates if os.path.exists(p)), None)\n# if test_input_path is None:\n#     raise FileNotFoundError(\"‚ùå test_input.csv not found.\")\n# test_input = pd.read_csv(test_input_path)\n# print(f\"‚úÖ Loaded test_input.csv ‚Äî {len(test_input):,} rows\")\n\n# # ------------------------------------------------------------\n# # 2Ô∏è‚É£ Merge last snapshot with test frames\n# # ------------------------------------------------------------\n# snap_last = (\n#     test_input\n#     .sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n#     .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n#     .last()\n# )\n# merged_test = pd.merge(\n#     test_df,\n#     snap_last,\n#     on=[\"game_id\", \"play_id\", \"nfl_id\"],\n#     how=\"left\",\n#     suffixes=(\"\", \"_snap\")\n# )\n\n# # ------------------------------------------------------------\n# # 3Ô∏è‚É£ Add features (reuse same function)\n# # ------------------------------------------------------------\n# merged_test = add_features(merged_test)\n\n# merged_test[\"frame_id_norm\"] = (\n#     merged_test.groupby([\"game_id\", \"play_id\"])[\"frame_id\"]\n#     .transform(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-6))\n# )\n# merged_test[\"out_frame_index\"] = merged_test.groupby(\n#     [\"game_id\", \"play_id\", \"nfl_id\"]\n# )[\"frame_id\"].rank(method=\"first\").astype(int)\n# merged_test[\"out_frame_norm\"] = merged_test[\"out_frame_index\"] / (\n#     merged_test.groupby([\"game_id\", \"play_id\", \"nfl_id\"])[\"out_frame_index\"].transform(\"max\") + 1e-6\n# )\n\n# # ------------------------------------------------------------\n# # 4Ô∏è‚É£ Prepare model input (must match training feature_cols)\n# # ------------------------------------------------------------\n# feature_cols = [\n#     \"x_snap\", \"y_snap\", \"speed\", \"accel\",\n#     \"distance_to_ball\", \"angle_to_ball\",\n#     \"relative_x\", \"relative_y\",\n#     \"frame_id_norm\", \"is_targeted\",\n#     \"player_side_enc\", \"out_frame_norm\"\n# ]\n# X_test = merged_test[feature_cols].fillna(0).values\n\n# # ------------------------------------------------------------\n# # 5Ô∏è‚É£ Generate predictions from both models\n# # ------------------------------------------------------------\n# print(\"‚öôÔ∏è Generating predictions...\")\n\n# baseline_preds = baseline.predict(X_test)\n# model.eval()\n# with torch.no_grad():\n#     Xt = torch.tensor(X_test, dtype=torch.float32).to(device)\n#     neural_preds = model(Xt).cpu().numpy()\n\n# # Blend (50/50)\n# merged_test[\"x\"] = 0.5 * baseline_preds[:, 0] + 0.5 * neural_preds[:, 0]\n# merged_test[\"y\"] = 0.5 * baseline_preds[:, 1] + 0.5 * neural_preds[:, 1]\n\n# # ------------------------------------------------------------\n# # 6Ô∏è‚É£ Format submission file\n# # ------------------------------------------------------------\n# submission = merged_test[[\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\"]].copy()\n# submission[\"x\"] = submission[\"x\"].astype(float)\n# submission[\"y\"] = submission[\"y\"].astype(float)\n# submission = submission.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"]).reset_index(drop=True)\n\n# submission_path = \"submission.csv\"\n# submission.to_csv(submission_path, index=False)\n\n# # ------------------------------------------------------------\n# # 7Ô∏è‚É£ Final checks and preview\n# # ------------------------------------------------------------\n# print(f\"‚úÖ Submission file created: {submission_path}\")\n# print(f\"üì¶ Total predictions: {len(submission):,}\")\n# print(\"\\n--- Preview ---\")\n# display(submission.head(10))\n\n# # optional: verify column structure\n# assert list(submission.columns) == [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\"], \"‚ùå Column mismatch!\"\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import mean_squared_error\n# import numpy as np\n# import pandas as pd\n\n# # ============================================================\n# # üßÆ Evaluate Local Submission Score (RMSE)\n# # ============================================================\n\n# def compute_local_rmse(pred_df: pd.DataFrame, true_df: pd.DataFrame):\n#     \"\"\"\n#     Compute RMSE between predicted and true (x, y).\n#     \"\"\"\n#     # Merge on keys\n#     merged = pd.merge(\n#         pred_df,\n#         true_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y']],\n#         on=['game_id', 'play_id', 'nfl_id', 'frame_id'],\n#         how='inner',\n#         suffixes=('_pred', '_true')\n#     )\n    \n#     if len(merged) == 0:\n#         print(\"‚ö†Ô∏è No overlapping rows between predictions and ground truth.\")\n#         return np.nan\n    \n#     rmse_x = mean_squared_error(merged['x_true'], merged['x_pred'], squared=False)\n#     rmse_y = mean_squared_error(merged['y_true'], merged['y_pred'], squared=False)\n    \n#     rmse_total = np.sqrt((rmse_x**2 + rmse_y**2) / 2)\n    \n#     print(f\"RMSE_x: {rmse_x:.4f}\")\n#     print(f\"RMSE_y: {rmse_y:.4f}\")\n#     print(f\"‚úÖ Combined RMSE: {rmse_total:.4f}\")\n    \n#     return rmse_total\n\n\n# # ============================================================\n# # üß© Example: Evaluate Using Evaluation Dataset\n# # ============================================================\n\n# # Locate your local \"evaluation_solutions\" data (contains true outputs)\n# eval_candidates = [\n#     \"/kaggle/input/nfl-big-data-bowl-2026-prediction/evaluation_output.csv\",\n#     \"/kaggle/input/nfl-big-data-bowl-2026/evaluation_output.csv\",\n#     \"/mnt/data/output_2023_w01.csv\",  # if you've uploaded\n# ]\n\n# found_eval = None\n# for p in eval_candidates:\n#     if os.path.exists(p):\n#         found_eval = p\n#         break\n\n# if found_eval:\n#     print(f\"‚úÖ Using ground truth from: {found_eval}\")\n#     eval_true = pd.read_csv(found_eval)\n    \n#     # Read your submission file\n#     submission = pd.read_csv(\"submission.csv\")\n    \n#     # Compute score\n#     print(\"\\nüî¢ Computing local submission RMSE...\")\n#     score = compute_local_rmse(submission, eval_true)\n    \n#     print(f\"\\nüèÅ Estimated Local RMSE Score: {score:.5f}\")\n# else:\n#     print(\"‚ö†Ô∏è Could not find evaluation ground truth file to compute local RMSE.\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T08:40:12.659Z"}},"outputs":[],"execution_count":null}]}