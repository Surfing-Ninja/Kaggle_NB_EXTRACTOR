{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"},{"sourceId":13192775,"sourceType":"datasetVersion","datasetId":8354894}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom tqdm import tqdm\nimport pickle\nimport json\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:44:58.112678Z","iopub.execute_input":"2025-09-27T23:44:58.113421Z","iopub.status.idle":"2025-09-27T23:45:05.212569Z","shell.execute_reply.started":"2025-09-27T23:44:58.113386Z","shell.execute_reply":"2025-09-27T23:45:05.211987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\ndef set_seed(seed=42):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    np.random.seed(seed)\n    import random\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:45:26.325592Z","iopub.execute_input":"2025-09-27T23:45:26.326153Z","iopub.status.idle":"2025-09-27T23:45:26.330441Z","shell.execute_reply.started":"2025-09-27T23:45:26.32613Z","shell.execute_reply":"2025-09-27T23:45:26.329916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction/\"\nMODELS_DIR = \"/kaggle/input/nfl2026-preprocessed/tree-models\"  \nOUTPUT_DIR = \"/kaggle/working/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:45:26.661046Z","iopub.execute_input":"2025-09-27T23:45:26.661308Z","iopub.status.idle":"2025-09-27T23:45:26.665091Z","shell.execute_reply.started":"2025-09-27T23:45:26.661288Z","shell.execute_reply":"2025-09-27T23:45:26.664407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_temporal_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Add engineered temporal features that change across frames\"\"\"\n    # Check if required columns exist\n    if 'dir' not in df.columns or 's' not in df.columns:\n        print(\"Warning: 'dir' or 's' columns missing for velocity calculation\")\n        return df\n    \n    # Velocity components (following data convention)\n    df = df.with_columns([\n        (pl.col(\"s\") * (pl.col(\"dir\") * np.pi / 180).sin()).alias(\"speed_x\"),\n        (pl.col(\"s\") * (pl.col(\"dir\") * np.pi / 180).cos()).alias(\"speed_y\")\n    ])\n    \n    # Momentum features using player weight converted to kg (lbs / 2.20462)\n    if 'player_weight' in df.columns:\n        df = df.with_columns([\n            (pl.col(\"speed_x\") * (pl.col(\"player_weight\").fill_null(200) / 2.20462)).alias(\"momentum_x\"),\n            (pl.col(\"speed_y\") * (pl.col(\"player_weight\").fill_null(200) / 2.20462)).alias(\"momentum_y\")\n        ])\n    else:\n        # Fallback using average NFL player weight in kg (~90.7 kg = 200 lbs)\n        df = df.with_columns([\n            (pl.col(\"speed_x\") * 90.7).alias(\"momentum_x\"),\n            (pl.col(\"speed_y\") * 90.7).alias(\"momentum_y\")\n        ])\n    \n    return df\n\ndef get_target_receiver_info(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Get target receiver position info for each play\"\"\"\n    if 'player_role' not in df.columns:\n        print(\"Warning: 'player_role' column missing\")\n        return pl.DataFrame({\n            \"game_id\": [], \"play_id\": [], \"target_x_last\": [], \n            \"target_y_last\": [], \"target_speed_last\": [], \"target_position\": []\n        })\n    \n    target_receivers = df.filter(pl.col(\"player_role\") == \"Targeted Receiver\")\n    if target_receivers.height == 0:\n        print(\"Warning: No targeted receivers found\")\n        return pl.DataFrame({\n            \"game_id\": [], \"play_id\": [], \"target_x_last\": [], \n            \"target_y_last\": [], \"target_speed_last\": [], \"target_position\": []\n        })\n    \n    target_info = target_receivers.group_by([\"game_id\", \"play_id\"]).agg([\n        pl.col(\"x\").last().alias(\"target_x_last\"),\n        pl.col(\"y\").last().alias(\"target_y_last\"),\n        pl.col(\"s\").last().alias(\"target_speed_last\"),\n        pl.col(\"player_position\").first().alias(\"target_position\")\n    ])\n    \n    return target_info\n\ndef calculate_temporal_aggregates(df_input: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Calculate statistical aggregates of temporal features across all pre-throw frames\"\"\"\n    \n    print(f\"Available columns: {df_input.columns}\")\n    \n    # Add temporal engineered features\n    df_temporal = add_temporal_features(df_input.clone())\n    \n    # Define temporal features to aggregate\n    potential_features = [\n        \"x\", \"y\", \"s\", \"a\", \"o\", \"dir\",\n        \"speed_x\", \"speed_y\", \"momentum_x\", \"momentum_y\"\n    ]\n    \n    # Filter to only existing columns\n    temporal_features_to_agg = [feat for feat in potential_features if feat in df_temporal.columns]\n    print(f\"Features to aggregate: {temporal_features_to_agg}\")\n    \n    # Build aggregation expressions for Polars\n    print(\"Building aggregation expressions...\")\n    agg_exprs = []\n    \n    # Add frame count\n    agg_exprs.append(pl.col(\"frame_id\").count().alias(\"num_input_frames\"))\n    \n    for feature in temporal_features_to_agg:\n        # Create aggregation expressions for each statistical function\n        agg_exprs.extend([\n            pl.col(feature).mean().alias(f\"{feature}_mean\"),\n            pl.col(feature).std().alias(f\"{feature}_std\"),\n            pl.col(feature).min().alias(f\"{feature}_min\"),\n            pl.col(feature).max().alias(f\"{feature}_max\"),\n            pl.col(feature).quantile(0.25).alias(f\"{feature}_q25\"),\n            pl.col(feature).quantile(0.75).alias(f\"{feature}_q75\"),\n            # Safe skew and kurtosis with fallback to 0 for edge cases\n            pl.col(feature).skew().fill_null(0).alias(f\"{feature}_skew\"),\n            pl.col(feature).kurtosis().fill_null(0).alias(f\"{feature}_kurt\"),\n            pl.col(feature).last().alias(f\"{feature}_last\"),\n            (pl.col(feature).max() - pl.col(feature).min()).alias(f\"{feature}_range\")\n        ])\n    \n    # Perform aggregation\n    print(f\"Performing aggregation on {df_temporal.height:,} rows...\")\n    aggregated = df_temporal.group_by([\"game_id\", \"play_id\", \"nfl_id\"]).agg(agg_exprs)\n    \n    print(f\"Aggregation complete! Result shape: {aggregated.shape}\")\n    \n    return aggregated\n\ndef encode_categorical_features_polars(df: pl.DataFrame, categorical_cols: list, encoders, fit=False):\n    \"\"\"Encode categorical features using pre-trained label encoders\"\"\"\n    df_encoded = df.clone()\n    \n    for col in categorical_cols:\n        if col in df.columns and col in encoders:\n            # Convert to pandas for sklearn LabelEncoder (temporary)\n            col_series = df_encoded.select(pl.col(col).fill_null(\"unknown\").cast(pl.Utf8)).to_pandas()[col]\n            le = encoders[col]\n            \n            # Handle unseen categories\n            encoded_values = []\n            for val in col_series:\n                if val in le.classes_:\n                    encoded_values.append(le.transform([val])[0])\n                else:\n                    encoded_values.append(-1)  # Unseen category\n            \n            df_encoded = df_encoded.with_columns(\n                pl.Series(f\"{col}_encoded\", encoded_values)\n            )\n        else:\n            # Column not found or no encoder available\n            df_encoded = df_encoded.with_columns(\n                pl.lit(-1).alias(f\"{col}_encoded\")\n            )\n    \n    return df_encoded\n\ndef process_test_data(test_input_path, test_template_path, encoders):\n    \"\"\"Process test data using the same pipeline as training\"\"\"\n    print(\"Loading test data...\")\n    \n    # Load test input data\n    df_test_in = pl.read_csv(test_input_path)\n    test_template = pd.read_csv(test_template_path)\n    \n    print(f\"Test input shape: {df_test_in.shape}\")\n    print(f\"Test template shape: {test_template.shape}\")\n    \n    # Note: Player height is not used as a feature (removed as not useful for prediction)\n    \n    # Extract constant features from test input\n    print(\"Extracting constant features from test input...\")\n    constant_from_input = df_test_in.group_by([\"game_id\", \"play_id\", \"nfl_id\"]).first().select([\n        \"game_id\", \"play_id\", \"nfl_id\", \"num_frames_output\", \"ball_land_x\", \"ball_land_y\", \n        \"absolute_yardline_number\", \"player_weight\"\n    ])\n    \n    # Convert test template to polars for merging\n    df_test_template = pl.from_pandas(test_template)\n    \n    # Add frame offset features to test template\n    print(\"Adding frame offset features to test template...\")\n    df_test_template = df_test_template.join(\n        constant_from_input.select([\"game_id\", \"play_id\", \"nfl_id\", \"num_frames_output\", \"player_weight\"]), \n        on=[\"game_id\", \"play_id\", \"nfl_id\"], how=\"left\"\n    )\n    \n    # Create T and t_rel using num_frames_output\n    df_test_template = df_test_template.with_columns([\n        pl.col(\"frame_id\").cast(pl.Float64).alias(\"frame_offset\"),\n        pl.col(\"num_frames_output\").cast(pl.Float64).clip(1.0, None).alias(\"T\")\n    ])\n    \n    # Add time-based features\n    df_test_template = df_test_template.with_columns([\n        (pl.col(\"frame_offset\") / 10.0).alias(\"time_offset\"),  # Convert frames to seconds (10 fps)\n        (pl.col(\"frame_offset\") / pl.col(\"T\")).alias(\"t_rel\")   # Relative time position\n    ])\n    \n    print(\"Processing temporal aggregates...\")\n    # Calculate temporal feature aggregates from test input data\n    temporal_agg = calculate_temporal_aggregates(df_test_in)\n    \n    print(\"Processing constant features...\")\n    # Prepare constant features\n    constant_features = constant_from_input.select([\n        \"game_id\", \"play_id\", \"nfl_id\", \"ball_land_x\", \"ball_land_y\", \"absolute_yardline_number\"\n    ])\n    \n    print(\"Processing target receiver info...\")\n    # Get target receiver info\n    df_temporal_for_target = add_temporal_features(df_test_in.clone())\n    target_receiver_info = get_target_receiver_info(df_temporal_for_target)\n    \n    print(\"Processing categorical features...\")\n    # Get categorical features (one per player per play)\n    categorical_df = df_test_in.group_by([\"game_id\", \"play_id\", \"nfl_id\"]).first()\n    \n    # Filter to only existing categorical features\n    existing_cat_features = [\"player_position\", \"player_role\", \"player_side\", \"play_direction\"]\n    existing_cat_features = [col for col in existing_cat_features if col in categorical_df.columns]\n    \n    if existing_cat_features:\n        categorical_df = categorical_df.select([\"game_id\", \"play_id\", \"nfl_id\"] + existing_cat_features)\n        \n        # Encode categorical features using pre-trained encoders\n        categorical_encoded = encode_categorical_features_polars(\n            categorical_df, existing_cat_features, encoders, fit=False\n        )\n    else:\n        categorical_encoded = df_test_in.group_by([\"game_id\", \"play_id\", \"nfl_id\"]).first().select([\"game_id\", \"play_id\", \"nfl_id\"])\n    \n    print(\"Merging all features...\")\n    # Merge all feature types with test template\n    test_processed = df_test_template.clone()\n    \n    print(f\"Before merge - test shape: {test_processed.shape}\")\n    test_processed = test_processed.join(temporal_agg, on=[\"game_id\", \"play_id\", \"nfl_id\"], how=\"left\")\n    print(f\"After temporal merge - test shape: {test_processed.shape}\")\n    \n    test_processed = test_processed.join(constant_features, on=[\"game_id\", \"play_id\", \"nfl_id\"], how=\"left\") \n    print(f\"After constant merge - test shape: {test_processed.shape}\")\n    \n    test_processed = test_processed.join(categorical_encoded, on=[\"game_id\", \"play_id\", \"nfl_id\"], how=\"left\")\n    print(f\"After categorical merge - test shape: {test_processed.shape}\")\n    \n    # Merge target receiver info\n    test_processed = test_processed.join(target_receiver_info, on=[\"game_id\", \"play_id\"], how=\"left\")\n    print(f\"After target receiver merge - test shape: {test_processed.shape}\")\n    \n    # Add ball landing relative features using last pre-throw position\n    if all(col in test_processed.columns for col in ['ball_land_x', 'ball_land_y', 'x_last', 'y_last']):\n        test_processed = test_processed.with_columns([\n            # Distance from last position to ball landing location\n            ((pl.col(\"ball_land_x\") - pl.col(\"x_last\")).pow(2) + \n             (pl.col(\"ball_land_y\") - pl.col(\"y_last\")).pow(2)).sqrt().alias(\"distance_to_ball_landing\"),\n            \n            # Angle from last position to ball landing location\n            pl.arctan2(pl.col(\"ball_land_y\") - pl.col(\"y_last\"), \n                      pl.col(\"ball_land_x\") - pl.col(\"x_last\")).alias(\"angle_to_ball_landing\")\n        ])\n        print(\"Added ball landing relative features\")\n    \n    # Add target receiver relative features using last pre-throw position\n    if all(col in test_processed.columns for col in ['target_x_last', 'target_y_last', 'x_last', 'y_last']):\n        test_processed = test_processed.with_columns([\n            # Distance from last position to target receiver's last position\n            ((pl.col(\"target_x_last\") - pl.col(\"x_last\")).pow(2) + \n             (pl.col(\"target_y_last\") - pl.col(\"y_last\")).pow(2)).sqrt().alias(\"distance_to_target\"),\n            \n            # Angle from last position to target receiver's last position\n            pl.arctan2(pl.col(\"target_y_last\") - pl.col(\"y_last\"), \n                      pl.col(\"target_x_last\") - pl.col(\"x_last\")).alias(\"angle_to_target\")\n        ])\n        print(\"Added target receiver relative features\")\n    \n    return test_processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:45:28.11734Z","iopub.execute_input":"2025-09-27T23:45:28.117887Z","iopub.status.idle":"2025-09-27T23:45:28.26167Z","shell.execute_reply.started":"2025-09-27T23:45:28.117864Z","shell.execute_reply":"2025-09-27T23:45:28.260919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_trained_models(models_dir):\n    \"\"\"Load all trained models and metadata\"\"\"\n    print(f\"Loading trained models from: {models_dir}\")\n    \n    # Use the models_dir directly (no subdirectory needed)\n    latest_model_dir = models_dir\n    print(f\"Using model directory: {latest_model_dir}\")\n    \n    # Load metadata\n    metadata_path = os.path.join(latest_model_dir, \"training_metadata.json\")\n    if not os.path.exists(metadata_path):\n        raise ValueError(f\"training_metadata.json not found in {latest_model_dir}\")\n    \n    with open(metadata_path, 'r') as f:\n        metadata = json.load(f)\n    \n    print(f\"Loaded metadata for {metadata['n_folds']} folds\")\n    print(f\"Features used: {metadata['n_features']}\")\n    \n    # Load models for each algorithm\n    models = {}\n    algorithms = ['lightgbm', 'xgboost', 'catboost']\n    \n    for algo in algorithms:\n        algo_dir = os.path.join(latest_model_dir, algo)\n        if os.path.exists(algo_dir):\n            models[algo] = {'dx': [], 'dy': []}\n            \n            # Load models for each fold\n            for fold in range(metadata['n_folds']):\n                fold_dir = os.path.join(algo_dir, f\"fold_{fold}\")\n                if os.path.exists(fold_dir):\n                    # Load dx model\n                    with open(os.path.join(fold_dir, \"model_dx.pkl\"), 'rb') as f:\n                        models[algo]['dx'].append(pickle.load(f))\n                    \n                    # Load dy model\n                    with open(os.path.join(fold_dir, \"model_dy.pkl\"), 'rb') as f:\n                        models[algo]['dy'].append(pickle.load(f))\n                    \n                    print(f\"Loaded {algo} fold {fold} models\")\n            \n            print(f\"Loaded {len(models[algo]['dx'])} {algo} models\")\n    \n    return models, metadata\ndef load_encoders():\n    \"\"\"Load categorical encoders\"\"\"\n    encoder_path = \"/kaggle/input/nfl2026-preprocessed/processed-data-trees/categorical_encoders.pkl\"\n    \n    if os.path.exists(encoder_path):\n        with open(encoder_path, 'rb') as f:\n            encoders = pickle.load(f)\n        print(f\"Loaded encoders for: {list(encoders.keys())}\")\n        return encoders\n    else:\n        print(\"Warning: No encoders found, using empty dict\")\n        return {}\n\ndef make_predictions(test_processed, models, feature_lists):\n    \"\"\"Make predictions using all trained models\"\"\"\n    print(\"Making predictions...\")\n    \n    # Convert to pandas for model prediction\n    test_df = test_processed.to_pandas()\n    \n    # Get feature list\n    ALL_FEATURES = feature_lists['all_features']\n    \n    # Check which features are available\n    available_features = [feat for feat in ALL_FEATURES if feat in test_df.columns]\n    missing_features = [feat for feat in ALL_FEATURES if feat not in test_df.columns]\n    \n    if missing_features:\n        print(f\"Warning: Missing features: {missing_features}\")\n        print(f\"Using {len(available_features)} out of {len(ALL_FEATURES)} features\")\n    \n    X_test = test_df[available_features]\n    \n    # Initialize prediction arrays\n    predictions = {}\n    \n    # Make predictions for each algorithm\n    for algo_name, algo_models in models.items():\n        print(f\"Predicting with {algo_name}...\")\n        \n        pred_dx_folds = []\n        pred_dy_folds = []\n        \n        # Predict with each fold\n        for fold in range(len(algo_models['dx'])):\n            model_dx = algo_models['dx'][fold]\n            model_dy = algo_models['dy'][fold]\n            \n            # Make predictions\n            pred_dx = model_dx.predict(X_test)\n            pred_dy = model_dy.predict(X_test)\n            \n            pred_dx_folds.append(pred_dx)\n            pred_dy_folds.append(pred_dy)\n        \n        # Average predictions across folds\n        predictions[algo_name] = {\n            'dx': np.mean(pred_dx_folds, axis=0),\n            'dy': np.mean(pred_dy_folds, axis=0)\n        }\n        \n        print(f\"Completed {algo_name} predictions (averaged across {len(pred_dx_folds)} folds)\")\n    \n    return predictions, X_test\n\ndef create_ensemble_predictions(predictions):\n    \"\"\"Create ensemble predictions by averaging across algorithms\"\"\"\n    print(\"Creating ensemble predictions...\")\n    \n    # algorithms = ['lightgbm']        # Test LightGBM only\n    # algorithms = ['xgboost']         # Test XGBoost only  \n    # algorithms = ['catboost']        # Test CatBoost only\n    # algorithms = ['lightgbm', 'xgboost']     # Test LightGBM + XGBoost\n    # algorithms = ['lightgbm', 'catboost']    # Test LightGBM + CatBoost\n    # algorithms = ['xgboost', 'catboost']     # Test XGBoost + CatBoost\n    algorithms = ['lightgbm', 'xgboost', 'catboost']     # Test XGBoost + CatBoost\n    print(f\"Ensembling {len(algorithms)} algorithms: {algorithms}\")\n    \n    # Average predictions across all algorithms\n    ensemble_dx = np.mean([predictions[algo]['dx'] for algo in algorithms], axis=0)\n    ensemble_dy = np.mean([predictions[algo]['dy'] for algo in algorithms], axis=0)\n    \n    return ensemble_dx, ensemble_dy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:49:01.606495Z","iopub.execute_input":"2025-09-27T23:49:01.607105Z","iopub.status.idle":"2025-09-27T23:49:01.619318Z","shell.execute_reply.started":"2025-09-27T23:49:01.607079Z","shell.execute_reply":"2025-09-27T23:49:01.61852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_submission(test_processed, predictions):\n    \"\"\"Create submission file\"\"\"\n    print(\"Creating submission file...\")\n    \n    # Convert test data to pandas for easier manipulation\n    test_df = test_processed.to_pandas()\n    \n    # Use ensemble predictions\n    ensemble_dx, ensemble_dy = create_ensemble_predictions(predictions)\n    \n    # Convert displacements to absolute positions\n    test_df['pred_x'] = test_df['x_last'] + ensemble_dx\n    test_df['pred_y'] = test_df['y_last'] + ensemble_dy\n    \n    # Create submission ID\n    test_df['id'] = (test_df['game_id'].astype(str) + \"_\" +\n                    test_df['play_id'].astype(str) + \"_\" +\n                    test_df['nfl_id'].astype(str) + \"_\" +\n                    test_df['frame_id'].astype(str))\n    \n    # Create submission\n    submission = test_df[['id', 'pred_x', 'pred_y']].rename(columns={'pred_x': 'x', 'pred_y': 'y'})\n    \n    # Save submission\n    submission_path = os.path.join(OUTPUT_DIR, \"submission.csv\")\n    submission.to_csv(submission_path, index=False)\n    print(f\"Saved submission.csv ({len(submission)} rows)\")\n    \n    return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:49:06.912452Z","iopub.execute_input":"2025-09-27T23:49:06.912762Z","iopub.status.idle":"2025-09-27T23:49:06.920632Z","shell.execute_reply.started":"2025-09-27T23:49:06.91274Z","shell.execute_reply":"2025-09-27T23:49:06.919914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"Main inference function\"\"\"\n    print(\"NFL Big Data Bowl 2026 - Inference Pipeline\")\n    print(\"=\" * 60)\n    \n    # Paths\n    test_input_path = os.path.join(DATA_DIR, \"test_input.csv\")\n    test_template_path = os.path.join(DATA_DIR, \"test.csv\")\n    \n    # Load encoders\n    print(\"Loading categorical encoders...\")\n    encoders = load_encoders()\n    \n    # Load trained models\n    print(\"Loading trained models...\")\n    models, metadata = load_trained_models(MODELS_DIR)\n    \n    # Load feature lists\n    print(\"Loading feature lists...\")\n    with open(\"/kaggle/input/nfl2026-preprocessed/processed-data-trees/feature_lists.pkl\", 'rb') as f:\n        feature_lists = pickle.load(f)\n    \n    print(f\"Feature summary:\")\n    print(f\"  Total features: {len(feature_lists['all_features'])}\")\n    print(f\"  Temporal features: {len(feature_lists['temporal_agg_features'])}\")\n    print(f\"  Constant features: {len(feature_lists['constant_features_final'])}\")\n    print(f\"  Categorical features: {len(feature_lists['categorical_features_final'])}\")\n    \n    # Process test data\n    print(\"Processing test data...\")\n    test_processed = process_test_data(test_input_path, test_template_path, encoders)\n    print(f\"Processed test data shape: {test_processed.shape}\")\n    \n    # Make predictions\n    print(\"Making predictions...\")\n    predictions, X_test = make_predictions(test_processed, models, feature_lists)\n    \n    # Create submission\n    print(\"Creating submission file...\")\n    submission = create_submission(test_processed, predictions)\n    \n    # Print summary\n    print(f\"\\n{'='*60}\")\n    print(\"INFERENCE COMPLETE!\")\n    print(f\"{'='*60}\")\n    print(f\"Models used: {list(models.keys())}\")\n    print(f\"Total folds per model: {metadata['n_folds']}\")\n    print(f\"Features used: {len(X_test.columns)}\")\n    print(f\"Test samples processed: {len(X_test):,}\")\n    print(f\"Submission files saved to: {OUTPUT_DIR}\")\n    \n    if submission is not None:\n        print(f\"Final submission shape: {submission.shape}\")\n        print(f\"Sample predictions:\")\n        print(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:49:40.577588Z","iopub.execute_input":"2025-09-27T23:49:40.578187Z","iopub.status.idle":"2025-09-27T23:49:40.608744Z","shell.execute_reply.started":"2025-09-27T23:49:40.578164Z","shell.execute_reply":"2025-09-27T23:49:40.607967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T23:49:40.844835Z","iopub.execute_input":"2025-09-27T23:49:40.845527Z","iopub.status.idle":"2025-09-27T23:49:47.731657Z","shell.execute_reply.started":"2025-09-27T23:49:40.845484Z","shell.execute_reply":"2025-09-27T23:49:47.730889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}