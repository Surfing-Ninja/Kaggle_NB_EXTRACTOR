{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"},{"sourceId":13423773,"sourceType":"datasetVersion","datasetId":8520041},{"sourceId":13430064,"sourceType":"datasetVersion","datasetId":8524005},{"sourceId":13431662,"sourceType":"datasetVersion","datasetId":8525147}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nfrom torch import nn\n\n# ===========================\n# CONFIG\n# ===========================\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    MODEL_DIR = Path(\"/kaggle/input/temporal2\") \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    WINDOW_SIZE = 10\n    MAX_FUTURE_HORIZON = 94\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n\nconfig = Config()\n\ntest_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\ntest_template = pd.read_csv(config.DATA_DIR / \"test.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:12:00.601939Z","iopub.execute_input":"2025-10-19T09:12:00.6025Z","iopub.status.idle":"2025-10-19T09:12:00.751713Z","shell.execute_reply.started":"2025-10-19T09:12:00.602475Z","shell.execute_reply":"2025-10-19T09:12:00.751128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n\ndef add_advanced_features(df):\n    \"\"\"Enhanced feature engineering from Notebook 1\"\"\"\n    print(\"Adding advanced features...\")\n    df = df.copy()\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n\n    # Distance Rate Features\n    if 'distance_to_ball' in df.columns:\n        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n        df['time_to_intercept'] = (df['distance_to_ball'] /\n                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n\n    # Target Alignment Features\n    if 'ball_direction_x' in df.columns:\n        df['velocity_alignment'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_perpendicular'] = (\n            df['velocity_x'] * (-df['ball_direction_y']) +\n            df['velocity_y'] * df['ball_direction_x']\n        )\n        if 'acceleration_x' in df.columns:\n            df['accel_alignment'] = (\n                df['acceleration_x'] * df['ball_direction_x'] +\n                df['acceleration_y'] * df['ball_direction_y']\n            )\n\n    # Multi-Window Rolling\n    for window in [3, 5, 10]:\n        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).mean()\n                )\n                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).std()\n                ).fillna(0)\n\n    # Extended Lag Features\n    for lag in [4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n\n    # Velocity Change Features\n    if 'velocity_x' in df.columns:\n        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n        df['direction_change'] = df['direction_change'].apply(\n            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        )\n\n    # Field Position Features\n    df['dist_from_left'] = df['y']\n    df['dist_from_right'] = 53.3 - df['y']\n    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n\n    # Role-Specific Features\n    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n\n    # Time Features\n    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n        lambda x: x / (x.max() + 1)\n    )\n\n    return df\n\ndef prepare_combined_features(input_df, output_df=None, test_template=None, is_training=True, window_size=10):\n    \"\"\"COMBINED: Advanced features + enhanced preprocessing\"\"\"\n    print(f\"Preparing COMBINED sequences (window_size={window_size})...\")\n\n    input_df = input_df.copy()\n\n\n    # BASIC FEATURES\n\n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n\n    # Enhanced motion features (From Notebook 2)\n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    o_rad = np.deg2rad(input_df['o'].fillna(0))\n\n    input_df['velocity_x'] = input_df['s'] * np.sin(dir_rad)\n    input_df['velocity_y'] = input_df['s'] * np.cos(dir_rad)\n    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n    input_df['orientation_x'] = np.sin(o_rad)\n    input_df['orientation_y'] = np.cos(o_rad)\n\n    # Enhanced roles\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    input_df['is_rusher'] = (input_df['player_role'] == 'Pass Rusher').astype(int)\n\n    # Field position (enhanced)\n    input_df['field_x_norm'] = (input_df['x'] - Config.FIELD_X_MIN) / (Config.FIELD_X_MAX - Config.FIELD_X_MIN)\n    input_df['field_y_norm'] = (input_df['y'] - Config.FIELD_Y_MIN) / (Config.FIELD_Y_MAX - Config.FIELD_Y_MIN)\n\n    # Physics features\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n\n    # Ball features\n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n\n    # Sort for temporal features\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n\n    # Enhanced temporal features\n    for lag in [1, 2, 3, 5]:\n        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n        input_df[f's_lag{lag}'] = input_df.groupby(gcols)['s'].shift(lag)\n\n    # Multiple EMA smoothing (\n    for alpha in [0.1, 0.3, 0.5]:\n        input_df[f'velocity_x_ema_{alpha}'] = input_df.groupby(gcols)['velocity_x'].transform(\n            lambda x: x.ewm(alpha=alpha, adjust=False).mean()\n        )\n        input_df[f'velocity_y_ema_{alpha}'] = input_df.groupby(gcols)['velocity_y'].transform(\n            lambda x: x.ewm(alpha=alpha, adjust=False).mean()\n        )\n\n\n    # ADVANCED FEATURES\n\n    input_df = add_advanced_features(input_df)\n\n\n    # COMBINED FEATURE LIST\n\n    feature_cols = [\n        # Core tracking (8)\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id',\n        'ball_land_x', 'ball_land_y',\n\n        # Player attributes (2)\n        'player_height_feet', 'player_weight',\n\n        # Enhanced motion (7)\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'orientation_x', 'orientation_y',\n        'kinetic_energy',\n\n        # Roles (6)\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer', 'is_rusher',\n\n        # Field position (6)\n        'field_x_norm', 'field_y_norm',\n        'dist_from_sideline', 'dist_from_endzone',\n        'distance_to_sideline', 'distance_to_endzone',\n\n        # Ball interaction (5)\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n\n        # Enhanced temporal (20)\n        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1', 's_lag1',\n        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2', 's_lag2',\n        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3', 's_lag3',\n        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5', 's_lag5',\n\n        # Multiple EMAs (6)\n        'velocity_x_ema_0.1', 'velocity_y_ema_0.1',\n        'velocity_x_ema_0.3', 'velocity_y_ema_0.3',\n        'velocity_x_ema_0.5', 'velocity_y_ema_0.5',\n\n        # Advanced features (20+)\n        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n        'frames_elapsed', 'normalized_time',\n\n        # Rolling features (selective)\n        'velocity_x_roll5', 'velocity_y_roll5', 's_roll5', 'a_roll5',\n        'velocity_x_std5', 'velocity_y_std5', 's_std5', 'a_std5',\n    ]\n\n    # Filter to existing columns\n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"Using {len(feature_cols)} COMBINED features\")\n\n\n    # CREATE SEQUENCES\n\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n\n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n\n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups)):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n\n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n\n        input_window = group_df.tail(window_size)\n\n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n\n        # Enhanced imputation\n        input_window = input_window.ffill().bfill()\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n\n        seq = input_window[feature_cols].values\n\n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n\n        sequences.append(seq)\n\n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n\n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n\n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n\n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id']\n        })\n\n    print(f\"Created {len(sequences)} sequences\")\n\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n    return sequences, sequence_ids, feature_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:12:00.752814Z","iopub.execute_input":"2025-10-19T09:12:00.753051Z","iopub.status.idle":"2025-10-19T09:12:00.781563Z","shell.execute_reply.started":"2025-10-19T09:12:00.753026Z","shell.execute_reply":"2025-10-19T09:12:00.78093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride=1, dilation=1, padding=0, dropout=0.2):\n        super().__init__()\n        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                               stride=stride, padding=padding, dilation=dilation)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                               stride=stride, padding=padding, dilation=dilation)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.conv2(out)\n        out = self.relu2(out)\n        out = self.dropout2(out)\n        res = x if self.downsample is None else self.downsample(x)\n        return nn.functional.relu(out + res)\nclass EnhancedSeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        self.horizon = horizon\n\n        self.gru = nn.GRU(input_dim, 192, num_layers=3, batch_first=True, dropout=0.2)\n\n        self.conv1d = nn.Sequential(\n            TemporalBlock(192, 128, kernel_size=3, padding=1),\n            nn.GELU(),\n            TemporalBlock(128, 128, kernel_size=5, padding=2),\n            nn.GELU(),\n        )\n\n        self.pool_ln = nn.LayerNorm(192)\n        self.pool_attn = nn.MultiheadAttention(192, num_heads=8, batch_first=True, dropout=0.1)\n        self.pool_query = nn.Parameter(torch.randn(1, 1, 192))\n\n        self.head = nn.Sequential(\n            nn.Linear(192 + 128, 256),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, horizon * 2)\n        )\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n            elif isinstance(module, nn.GRU):\n                for name, param in module.named_parameters():\n                    if 'weight' in name:\n                        nn.init.orthogonal_(param)\n                    elif 'bias' in name:\n                        nn.init.constant_(param, 0)\n\n    def forward(self, x):\n        h, _ = self.gru(x)\n\n        h_conv = self.conv1d(h.transpose(1, 2)).transpose(1, 2)\n        h_conv_pool = h_conv.mean(dim=1)\n\n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)\n        h_norm = self.pool_ln(h)\n        ctx, _ = self.pool_attn(q, h_norm, h_norm)\n        ctx = ctx.squeeze(1)\n\n        combined = torch.cat([ctx, h_conv_pool], dim=1)\n\n        out = self.head(combined)\n        out = out.view(B, 2, self.horizon)\n\n        out = torch.cumsum(out, dim=2)\n\n        return out[:, 0, :], out[:, 1, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:12:00.782332Z","iopub.execute_input":"2025-10-19T09:12:00.782558Z","iopub.status.idle":"2025-10-19T09:12:00.805898Z","shell.execute_reply.started":"2025-10-19T09:12:00.782533Z","shell.execute_reply":"2025-10-19T09:12:00.805182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_sequences, test_ids, feature_cols = prepare_combined_features(\n    test_input, test_template=test_template, is_training=False, window_size=config.WINDOW_SIZE\n)\n\nINPUT_DIM = len(feature_cols)\nprint(f\"✅ Detected INPUT_DIM = {INPUT_DIM}\")\n\nX_test = np.array(test_sequences, dtype=object)\nx_last = np.array([s[-1, 0] for s in X_test])\ny_last = np.array([s[-1, 1] for s in X_test])\n\n\nmodels, scalers = [], []\nfor fold in range(1, 6):\n    model_path = config.MODEL_DIR / f\"best_model_fold{fold}.pt\"\n\n    \n    checkpoint = torch.load(model_path, map_location=config.DEVICE, weights_only=False)\n    \n    \n    model = EnhancedSeqModel(input_dim=INPUT_DIM, horizon=config.MAX_FUTURE_HORIZON)\n    model.load_state_dict(checkpoint[\"model_state\"])\n    model.to(config.DEVICE)\n    model.eval()\n    models.append(model)\n\n    # スケーラー復元\n    scaler = StandardScaler()\n    scaler.mean_ = np.array(checkpoint[\"scaler_mean\"])\n    scaler.scale_ = np.array(checkpoint[\"scaler_scale\"])\n    scalers.append(scaler)\n\n    print(f\"✅ Loaded: {model_path.name}, with scaler\")\n\n\nall_dx, all_dy = [], []\n\nfor model, scaler in zip(models, scalers):\n    \n    X_scaled = np.stack([scaler.transform(s) for s in X_test])\n    X_tensor = torch.tensor(X_scaled.astype(np.float32)).to(config.DEVICE)\n    \n    with torch.no_grad():\n        dx, dy = model(X_tensor)\n        all_dx.append(dx.cpu().numpy())\n        all_dy.append(dy.cpu().numpy())\n\nens_dx = np.mean(all_dx, axis=0)\nens_dy = np.mean(all_dy, axis=0)\n\n\nrows = []\nH = ens_dx.shape[1]\n\nfor i, sid in enumerate(test_ids):\n    fids = test_template[\n        (test_template['game_id'] == sid['game_id']) &\n        (test_template['play_id'] == sid['play_id']) &\n        (test_template['nfl_id'] == sid['nfl_id'])\n    ]['frame_id'].sort_values().tolist()\n\n    for t, fid in enumerate(fids):\n        tt = min(t, H - 1)\n        px = np.clip(x_last[i] + ens_dx[i, tt], config.FIELD_X_MIN, config.FIELD_X_MAX)\n        py = np.clip(y_last[i] + ens_dy[i, tt], config.FIELD_Y_MIN, config.FIELD_Y_MAX)\n        rows.append({\n            'id': f\"{sid['game_id']}_{sid['play_id']}_{sid['nfl_id']}_{fid}\",\n            'x': float(px),\n            'y': float(py)\n        })\n\nsubmission = pd.DataFrame(rows)\nsubmission['id'] = submission['id'].astype(str)\nsubmission['x'] = submission['x'].astype(float)\nsubmission['y'] = submission['y'].astype(float)\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ submission.csv saved:\", submission.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:12:00.807332Z","iopub.execute_input":"2025-10-19T09:12:00.807679Z","iopub.status.idle":"2025-10-19T09:12:25.064239Z","shell.execute_reply.started":"2025-10-19T09:12:00.80766Z","shell.execute_reply":"2025-10-19T09:12:25.063499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:12:25.064978Z","iopub.execute_input":"2025-10-19T09:12:25.065224Z","iopub.status.idle":"2025-10-19T09:12:25.08196Z","shell.execute_reply.started":"2025-10-19T09:12:25.065205Z","shell.execute_reply":"2025-10-19T09:12:25.081174Z"}},"outputs":[],"execution_count":null}]}