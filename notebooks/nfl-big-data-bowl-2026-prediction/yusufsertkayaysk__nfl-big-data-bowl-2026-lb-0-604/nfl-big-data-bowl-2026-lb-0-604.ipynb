{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nimport joblib\nimport os\nimport gc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom torch.utils.data import TensorDataset, DataLoader\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONFIG & DATA LOADING","metadata":{}},{"cell_type":"code","source":"class Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 512\n    EPOCHS = 120\n    PATIENCE = 20\n    LEARNING_RATE = 5e-4\n    \n    WINDOW_SIZE = 12\n    HIDDEN_DIM = 192\n    MAX_FUTURE_HORIZON = 94\n    USE_PLAYERS_INTERACTIONS = True  \n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(Config.SEED)\n# \ndef load_data():\n    \"\"\"Load all training and test data\"\"\"\n    print(\"Loading data...\")\n    \n    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    train_input_files = [f for f in train_input_files if f.exists()]\n    train_output_files = [f for f in train_output_files if f.exists()]\n    \n    print(f\"Found {len(train_input_files)} weeks of data\")\n    \n    train_input = pd.concat(\n        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_input_files, start=1)],\n        ignore_index=True\n    )\n    train_output = pd.concat(\n        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_output_files, start=1)],\n        ignore_index=True\n    )\n    \n    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n    \n    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n    \n    return train_input, train_output, test_input, test_template","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FEATURE ENGINEERING **Rich Features**\n","metadata":{}},{"cell_type":"code","source":"def height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n\ndef add_advanced_features(df):\n    \"\"\"Enhanced feature engineering from Notebook 1\"\"\"\n    print(\"Adding advanced features...\")\n    df = df.copy()\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # Distance Rate Features\n    if 'distance_to_ball' in df.columns:\n        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n        df['time_to_intercept'] = (df['distance_to_ball'] / \n                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n    \n    # Target Alignment Features\n    if 'ball_direction_x' in df.columns:\n        df['velocity_alignment'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_perpendicular'] = (\n            df['velocity_x'] * (-df['ball_direction_y']) +\n            df['velocity_y'] * df['ball_direction_x']\n        )\n        if 'acceleration_x' in df.columns:\n            df['accel_alignment'] = (\n                df['acceleration_x'] * df['ball_direction_x'] +\n                df['acceleration_y'] * df['ball_direction_y']\n            )\n    \n    # Multi-Window Rolling\n    for window in [3, 5, 10]:\n        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).mean()\n                )\n                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).std()\n                ).fillna(0)\n    \n    # Extended Lag Features\n    for lag in [4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n    \n    # Velocity Change Features\n    if 'velocity_x' in df.columns:\n        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n        df['direction_change'] = df['direction_change'].apply(\n            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        )\n    \n    # Field Position Features\n    df['dist_from_left'] = df['y']\n    df['dist_from_right'] = 53.3 - df['y']\n    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n    \n    # Role-Specific Features\n    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n    \n    # Time Features\n    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n        lambda x: x / (x.max() + 1)\n    )\n    \n    return df\n\ndef compute_player_interactions(df):\n    \"\"\"Compute player interaction features - Notebook 1's key advantage\"\"\"\n    print(\"Computing player interaction features...\")\n    \n    agg_rows = []\n    for (g, p, f), grp in tqdm(df.groupby(['game_id', 'play_id', 'frame_id'], sort=False), \n                               desc=\"Player interactions\"):\n        n = len(grp)\n        nfl_ids = grp['nfl_id'].to_numpy()\n        compute_mask = grp['player_to_predict'].to_numpy().astype(bool) if 'player_to_predict' in grp.columns else np.ones(n, dtype=bool)\n        \n        if n < 2:\n            for nid in nfl_ids[compute_mask]:\n                agg_rows.append({\n                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n                    'distance_to_player_mean_offense': np.nan,\n                    'distance_to_player_min_offense': np.nan,\n                    'distance_to_player_max_offense': np.nan,\n                    'relative_velocity_magnitude_mean_offense': np.nan,\n                    'relative_velocity_magnitude_min_offense': np.nan,\n                    'relative_velocity_magnitude_max_offense': np.nan,\n                    'angle_to_player_mean_offense': np.nan,\n                    'angle_to_player_min_offense': np.nan,\n                    'angle_to_player_max_offense': np.nan,\n                    'distance_to_player_mean_defense': np.nan,\n                    'distance_to_player_min_defense': np.nan,\n                    'distance_to_player_max_defense': np.nan,\n                    'relative_velocity_magnitude_mean_defense': np.nan,\n                    'relative_velocity_magnitude_min_defense': np.nan,\n                    'relative_velocity_magnitude_max_defense': np.nan,\n                    'angle_to_player_mean_defense': np.nan,\n                    'angle_to_player_min_defense': np.nan,\n                    'angle_to_player_max_defense': np.nan,\n                    'nearest_opponent_dist': np.nan,\n                    'nearest_opponent_angle': np.nan,\n                    'nearest_opponent_rel_speed': np.nan,\n                })\n            continue\n        \n        x = grp['x'].to_numpy(dtype=np.float32)\n        y = grp['y'].to_numpy(dtype=np.float32)\n        vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n        vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n        is_off = grp['is_offense'].to_numpy().astype(bool)\n        \n        # Pairwise geometry\n        dx = x[None, :] - x[:, None]\n        dy = y[None, :] - y[:, None]\n        dist = np.sqrt(dx * dx + dy * dy)\n        angle_mat = np.arctan2(-dy, -dx)\n        dvx = vx[:, None] - vx[None, :]\n        dvy = vy[:, None] - vy[None, :]\n        rel_speed = np.sqrt(dvx * dvx + dvy * dvy)\n        \n        # Masks\n        opp_mask = (is_off[:, None] != is_off[None, :])\n        np.fill_diagonal(opp_mask, False)\n        \n        mask_off = np.broadcast_to(is_off[None, :], (n, n)).copy()\n        mask_def = np.broadcast_to(~is_off[None, :], (n, n)).copy()\n        np.fill_diagonal(mask_off, False)\n        np.fill_diagonal(mask_def, False)\n        \n        # Nearest opponent\n        dist_opp = np.where(opp_mask, dist, np.nan)\n        nearest_dist = np.nanmin(dist_opp, axis=1)\n        nearest_idx = np.nanargmin(dist_opp, axis=1)\n        all_nan = ~np.isfinite(nearest_dist)\n        nearest_idx_safe = nearest_idx.copy()\n        nearest_idx_safe[all_nan] = 0\n        nearest_angle = np.take_along_axis(angle_mat, nearest_idx_safe[:, None], axis=1).squeeze(1)\n        nearest_rel = np.take_along_axis(rel_speed, nearest_idx_safe[:, None], axis=1).squeeze(1)\n        nearest_angle[all_nan] = np.nan\n        nearest_rel[all_nan] = np.nan\n        \n        # Group-wise aggregations\n        d_off = np.where(mask_off, dist, np.nan)\n        d_def = np.where(mask_def, dist, np.nan)\n        d_mean_o = np.nanmean(d_off, axis=1); d_min_o = np.nanmin(d_off, axis=1); d_max_o = np.nanmax(d_off, axis=1)\n        d_mean_d = np.nanmean(d_def, axis=1); d_min_d = np.nanmin(d_def, axis=1); d_max_d = np.nanmax(d_def, axis=1)\n        \n        v_off = np.where(mask_off, rel_speed, np.nan)\n        v_def = np.where(mask_def, rel_speed, np.nan)\n        v_mean_o = np.nanmean(v_off, axis=1); v_min_o = np.nanmin(v_off, axis=1); v_max_o = np.nanmax(v_off, axis=1)\n        v_mean_d = np.nanmean(v_def, axis=1); v_min_d = np.nanmin(v_def, axis=1); v_max_d = np.nanmax(v_def, axis=1)\n        \n        sinA = np.sin(angle_mat); cosA = np.cos(angle_mat)\n        cnt_off = mask_off.sum(axis=1).astype(np.float32)\n        cnt_def = mask_def.sum(axis=1).astype(np.float32)\n        denom_off = np.where(cnt_off > 0, cnt_off, np.nan)\n        denom_def = np.where(cnt_def > 0, cnt_def, np.nan)\n        \n        sin_sum_off = (sinA * mask_off).sum(axis=1)\n        cos_sum_off = (cosA * mask_off).sum(axis=1)\n        sin_sum_def = (sinA * mask_def).sum(axis=1)\n        cos_sum_def = (cosA * mask_def).sum(axis=1)\n        \n        a_mean_o = np.arctan2(sin_sum_off / denom_off, cos_sum_off / denom_off)\n        a_mean_d = np.arctan2(sin_sum_def / denom_def, cos_sum_def / denom_def)\n        \n        a_off = np.where(mask_off, angle_mat, np.nan)\n        a_def = np.where(mask_def, angle_mat, np.nan)\n        a_min_o = np.nanmin(a_off, axis=1); a_max_o = np.nanmax(a_off, axis=1)\n        a_min_d = np.nanmin(a_def, axis=1); a_max_d = np.nanmax(a_def, axis=1)\n        \n        for idx, nid in enumerate(nfl_ids):\n            if not compute_mask[idx]:\n                continue\n            agg_rows.append({\n                'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': int(nid),\n                'distance_to_player_mean_offense': d_mean_o[idx],\n                'distance_to_player_min_offense': d_min_o[idx],\n                'distance_to_player_max_offense': d_max_o[idx],\n                'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n                'relative_velocity_magnitude_min_offense': v_min_o[idx],\n                'relative_velocity_magnitude_max_offense': v_max_o[idx],\n                'angle_to_player_mean_offense': a_mean_o[idx],\n                'angle_to_player_min_offense': a_min_o[idx],\n                'angle_to_player_max_offense': a_max_o[idx],\n                'distance_to_player_mean_defense': d_mean_d[idx],\n                'distance_to_player_min_defense': d_min_d[idx],\n                'distance_to_player_max_defense': d_max_d[idx],\n                'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n                'relative_velocity_magnitude_min_defense': v_min_d[idx],\n                'relative_velocity_magnitude_max_defense': v_max_d[idx],\n                'angle_to_player_mean_defense': a_mean_d[idx],\n                'angle_to_player_min_defense': a_min_d[idx],\n                'angle_to_player_max_defense': a_max_d[idx],\n                'nearest_opponent_dist': float(nearest_dist[idx]) if np.isfinite(nearest_dist[idx]) else np.nan,\n                'nearest_opponent_angle': float(nearest_angle[idx]) if np.isfinite(nearest_angle[idx]) else np.nan,\n                'nearest_opponent_rel_speed': float(nearest_rel[idx]) if np.isfinite(nearest_rel[idx]) else np.nan,\n            })\n    \n    return pd.DataFrame(agg_rows)\n\ndef prepare_sequences(input_df, output_df=None, test_template=None, is_training=True, window_size=12):\n    \"\"\"Prepare sequences with ALL 114 features from Notebook 1\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"PREPARING SEQUENCES WITH 114 FEATURES\")\n    print(f\"{'='*80}\")\n    print(f\"Window size: {window_size}\")\n    \n    input_df = input_df.copy()\n    \n    # Basic features\n    print(\"Step 1/4: Adding basic features...\")\n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n    \n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    delta_t = 0.1\n    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n    input_df['o_sin'] = np.sin(np.deg2rad(input_df['o'].fillna(0)))\n    input_df['o_cos'] = np.cos(np.deg2rad(input_df['o'].fillna(0)))\n    input_df['dir_sin'] = np.sin(np.deg2rad(input_df['dir'].fillna(0)))\n    input_df['dir_cos'] = np.cos(np.deg2rad(input_df['dir'].fillna(0)))\n    \n    # Roles\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    \n    # Physics\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n    \n    # Ball features\n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n    \n    # Sort for temporal\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # Lag features\n    for lag in [1, 2, 3]:\n        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n    \n    # EMA features\n    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    \n    # Advanced features\n    print(\"Step 2/4: Adding advanced features...\")\n    input_df = add_advanced_features(input_df)\n    \n    # Player interactions\n    print(\"Step 3/4: Adding player interaction features...\")\n    if Config.USE_PLAYERS_INTERACTIONS:\n        interaction_agg = compute_player_interactions(input_df)\n        input_df = input_df.merge(\n            interaction_agg,\n            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n            how='left'\n        )\n    \n    # Feature list (114 features)\n    print(\"Step 4/4: Creating sequences...\")\n    feature_cols = [\n        'x', 'y', 's', 'a', 'ball_land_x', 'ball_land_y',\n        'o_sin', 'o_cos', 'dir_sin', 'dir_cos',\n        'player_height_feet', 'player_weight',\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n        'dist_from_sideline', 'dist_from_endzone',\n        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n        'frames_elapsed', 'normalized_time',\n        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed',\n    ]\n    \n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"Using {len(feature_cols)} features\")\n    \n    # Create sequences\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    \n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n    \n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n    \n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        \n        input_window = group_df.tail(window_size)\n        \n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            first = input_window.iloc[0:1].copy()\n            pad_df = pd.concat([first] * pad_len, ignore_index=True)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        \n        input_window = input_window.ffill().bfill().fillna(0.0)\n        \n        seq = input_window[feature_cols].values\n        seq = np.nan_to_num(seq, nan=0.0)\n        \n        sequences.append(seq)\n        \n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            \n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n            \n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            \n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n        \n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id']\n        })\n    \n    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n    \n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n    return sequences, sequence_ids, feature_cols","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ENHANCED MODEL - **GRU + Conv1D + Attention**\n","metadata":{}},{"cell_type":"code","source":"class HybridSeqModel(nn.Module):\n    \"\"\"Hybrid model combining best of both notebooks\"\"\"\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        self.horizon = horizon\n        \n        # Enhanced GRU\n        self.gru = nn.GRU(input_dim, 192, num_layers=3, \n                         batch_first=True, dropout=0.2, bidirectional=False)\n        \n        # Conv1D for local patterns\n        self.conv1d = nn.Sequential(\n            nn.Conv1d(192, 128, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Conv1d(128, 128, kernel_size=5, padding=2),\n            nn.GELU(),\n        )\n        \n        # Enhanced attention pooling\n        self.pool_ln = nn.LayerNorm(192)\n        self.pool_attn = nn.MultiheadAttention(192, num_heads=8, \n                                               batch_first=True, dropout=0.1)\n        self.pool_query = nn.Parameter(torch.randn(1, 1, 192))\n        \n        # Separate prediction heads for X and Y\n        self.head_shared = nn.Sequential(\n            nn.Linear(192 + 128, 256),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Dropout(0.2),\n        )\n        \n        self.head_x = nn.Linear(128, horizon)\n        self.head_y = nn.Linear(128, horizon)\n        \n        self.initialize_weights()\n    \n    def initialize_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n            elif isinstance(module, nn.GRU):\n                for name, param in module.named_parameters():\n                    if 'weight' in name:\n                        nn.init.orthogonal_(param)\n                    elif 'bias' in name:\n                        nn.init.constant_(param, 0)\n    \n    def forward(self, x):\n        # GRU encoding\n        h, _ = self.gru(x)\n        \n        # Conv1D for local temporal patterns\n        h_conv = self.conv1d(h.transpose(1, 2)).transpose(1, 2)\n        h_conv_pool = h_conv.mean(dim=1)\n        \n        # Attention pooling\n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)\n        h_norm = self.pool_ln(h)\n        ctx, _ = self.pool_attn(q, h_norm, h_norm)\n        ctx = ctx.squeeze(1)\n        \n        # Combine representations\n        combined = torch.cat([ctx, h_conv_pool], dim=1)\n        \n        # Shared processing\n        shared = self.head_shared(combined)\n        \n        # Separate X and Y predictions with cumsum\n        out_x = torch.cumsum(self.head_x(shared), dim=1)\n        out_y = torch.cumsum(self.head_y(shared), dim=1)\n        \n        return out_x, out_y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ENHANCED LOSS - Velocity-Consistent Loss","metadata":{}},{"cell_type":"code","source":"\nclass EnhancedTemporalLoss(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.05, velocity_weight=0.1):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n        self.velocity_weight = velocity_weight\n        self.huber = nn.SmoothL1Loss(reduction='none')\n    \n    def forward(self, pred_dx, pred_dy, target_dx, target_dy, mask):\n        L = pred_dx.size(1)\n        t = torch.arange(L, device=pred_dx.device).float()\n        time_weights = torch.exp(-self.time_decay * t).view(1, L)\n        \n        # Position loss\n        loss_dx = self.huber(pred_dx, target_dx) * time_weights\n        loss_dy = self.huber(pred_dy, target_dy) * time_weights\n        \n        masked_loss_dx = (loss_dx * mask).sum() / (mask.sum() + 1e-8)\n        masked_loss_dy = (loss_dy * mask).sum() / (mask.sum() + 1e-8)\n        position_loss = (masked_loss_dx + masked_loss_dy) / 2\n        \n        # Velocity consistency loss\n        if self.velocity_weight > 0:\n            pred_velocity_x = torch.diff(pred_dx, dim=1, prepend=torch.zeros_like(pred_dx[:, :1]))\n            pred_velocity_y = torch.diff(pred_dy, dim=1, prepend=torch.zeros_like(pred_dy[:, :1]))\n            target_velocity_x = torch.diff(target_dx, dim=1, prepend=torch.zeros_like(target_dx[:, :1]))\n            target_velocity_y = torch.diff(target_dy, dim=1, prepend=torch.zeros_like(target_dy[:, :1]))\n            \n            velocity_loss = (\n                self.huber(pred_velocity_x[:, :-1], target_velocity_x[:, :-1]).mean() +\n                self.huber(pred_velocity_y[:, :-1], target_velocity_y[:, :-1]).mean()\n            ) * self.velocity_weight\n            \n            return position_loss + velocity_loss\n        \n        return position_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def prepare_targets(batch_dx, batch_dy, max_h):\n    tensors_dx, tensors_dy, masks = [], [], []\n    for dx_arr, dy_arr in zip(batch_dx, batch_dy):\n        L = len(dx_arr)\n        padded_dx = np.pad(dx_arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        padded_dy = np.pad(dy_arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors_dx.append(torch.tensor(padded_dx))\n        tensors_dy.append(torch.tensor(padded_dy))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors_dx), torch.stack(tensors_dy), torch.stack(masks)\n\ndef train_hybrid_model(X_train, y_dx_train, y_dy_train, X_val, y_dx_val, y_dy_val, \n                       input_dim, horizon, config, fold_num):\n    device = config.DEVICE\n    model = HybridSeqModel(input_dim, horizon).to(device)\n    \n    criterion = EnhancedTemporalLoss(delta=0.5, time_decay=0.05, velocity_weight=0.08)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)\n    \n    steps_per_epoch = len(X_train) // config.BATCH_SIZE + 1\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=config.LEARNING_RATE,\n        epochs=config.EPOCHS, steps_per_epoch=steps_per_epoch\n    )\n    \n    # Prepare batches\n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        by_dx, by_dy, bm = prepare_targets(\n            [y_dx_train[j] for j in range(i, end)],\n            [y_dy_train[j] for j in range(i, end)], horizon\n        )\n        train_batches.append((bx, by_dx, by_dy, bm))\n    \n    val_batches = []\n    for i in range(0, len(X_val), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_val))\n        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        by_dx, by_dy, bm = prepare_targets(\n            [y_dx_val[j] for j in range(i, end)],\n            [y_dy_val[j] for j in range(i, end)], horizon\n        )\n        val_batches.append((bx, by_dx, by_dy, bm))\n    \n    best_loss, best_state, bad = float('inf'), None, 0\n    \n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        \n        for bx, by_dx, by_dy, bm in train_batches:\n            bx = bx.to(device)\n            by_dx, by_dy, bm = by_dx.to(device), by_dy.to(device), bm.to(device)\n            \n            pred_dx, pred_dy = model(bx)\n            loss = criterion(pred_dx, pred_dy, by_dx, by_dy, bm)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            train_losses.append(loss.item())\n        \n        # Validation\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for bx, by_dx, by_dy, bm in val_batches:\n                bx = bx.to(device)\n                by_dx, by_dy, bm = by_dx.to(device), by_dy.to(device), bm.to(device)\n                pred_dx, pred_dy = model(bx)\n                loss = criterion(pred_dx, pred_dy, by_dx, by_dy, bm)\n                val_losses.append(loss.item())\n        \n        train_loss = np.mean(train_losses)\n        val_loss = np.mean(val_losses)\n        \n        if epoch % 10 == 0:\n            lr = scheduler.get_last_lr()[0]\n            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}, lr={lr:.2e}\")\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n    \n    if best_state:\n        model.load_state_dict(best_state)\n    \n    return model, best_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# INFERENCE","metadata":{}},{"cell_type":"code","source":"def create_ensemble_predictions(models, scalers, X_test_unscaled, test_seq_ids, \n                               test_template, config):\n    \"\"\"Generate ensemble predictions from multiple folds\"\"\"\n    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n    N = len(X_test_unscaled)\n    \n    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n    \n    per_fold_dx, per_fold_dy = [], []\n    \n    for model, scaler in zip(models, scalers):\n        scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n        X = np.stack(scaled.astype(np.float32))\n        \n        device = next(model.parameters()).device\n        ds = TensorDataset(torch.from_numpy(X))\n        dl = DataLoader(ds, batch_size=config.BATCH_SIZE, shuffle=False)\n        \n        dx_list, dy_list = [], []\n        model.eval()\n        with torch.no_grad():\n            for (batch,) in dl:\n                batch = batch.to(device)\n                dx, dy = model(batch)\n                dx_list.append(dx.cpu().numpy())\n                dy_list.append(dy.cpu().numpy())\n        \n        dx_cum = np.vstack(dx_list)\n        dy_cum = np.vstack(dy_list)\n        per_fold_dx.append(dx_cum)\n        per_fold_dy.append(dy_cum)\n    \n    # Ensemble by mean\n    ens_dx = np.mean(np.stack(per_fold_dx, axis=0), axis=0)\n    ens_dy = np.mean(np.stack(per_fold_dy, axis=0), axis=0)\n    \n    # Create submission\n    test_meta = pd.DataFrame(test_seq_ids)\n    out_rows = []\n    H = ens_dx.shape[1]\n    \n    for i, seq_info in test_meta.iterrows():\n        game_id = int(seq_info['game_id'])\n        play_id = int(seq_info['play_id'])\n        nfl_id = int(seq_info['nfl_id'])\n        \n        frame_ids = test_template[\n            (test_template['game_id'] == game_id) &\n            (test_template['play_id'] == play_id) &\n            (test_template['nfl_id'] == nfl_id)\n        ]['frame_id'].sort_values().tolist()\n        \n        for t, frame_id in enumerate(frame_ids):\n            tt = t if t < H else H - 1\n            px = np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n            py = np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            out_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n                'x': px,\n                'y': py\n            })\n    \n    return pd.DataFrame(out_rows)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MAIN TRAINING PIPELINE","metadata":{}},{"cell_type":"code","source":"def main():\n    config = Config()\n    \n    # Header\n    print(\"=\" * 80)\n    print(\"üèà  HYBRID NFL PREDICTION MODEL - FULL TRAINING PIPELINE\".center(80))\n    print(\"=\" * 100)\n    print(\"\\nüß†  Key Components:\")\n    print(\"   ‚Ä¢ 114 engineered features \")\n    print(\"   ‚Ä¢ 21 player interaction features\")\n    print(\"   ‚Ä¢ Enhanced GRU + Conv1D + Attention hybrid \")\n    print(\"   ‚Ä¢ Velocity-consistent loss for smoother trajectories\")\n    print(\"   ‚Ä¢ OneCycleLR scheduler for stable convergence\")\n    print(\"   ‚Ä¢ 5-Fold Ensemble with GroupKFold\")\n    print(f\"   ‚Ä¢ Running on device: {config.DEVICE}\")\n    print(\"=\" * 80)\n    \n    # Step 1: Load data\n    print(\"\\n[1/5] üìÇ Loading data...\")\n    train_input, train_output, test_input, test_template = load_data()\n    \n    # Step 2: Prepare sequences\n    print(\"\\n[2/5] üß© Preparing sequences with 114 features...\")\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols = prepare_sequences(\n        train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n    )\n    \n    sequences = np.array(sequences, dtype=object)\n    targets_dx = np.array(targets_dx, dtype=object)\n    targets_dy = np.array(targets_dy, dtype=object)\n    \n    print(\"\\nüìä Dataset Summary:\")\n    print(f\"   - Total sequences     : {len(sequences)}\")\n    print(f\"   - Feature dimension   : {sequences[0].shape[-1]}\")\n    print(f\"   - Window size         : {config.WINDOW_SIZE}\")\n    print(f\"   - Max future horizon  : {config.MAX_FUTURE_HORIZON}\")\n    \n    # Step 3: Cross-validation\n    print(\"\\n[3/5] üéØ Training with 5-Fold Cross-Validation...\")\n    groups = np.array([d['game_id'] for d in sequence_ids])\n    gkf = GroupKFold(n_splits=config.N_FOLDS)\n    \n    models, scalers, fold_scores = [], [], []\n    \n    for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n        print(f\"\\n{'-' * 100}\")\n        print(f\"üîÅ  Fold {fold}/{config.N_FOLDS}\")\n        print(f\"{'-' * 100}\")\n        print(f\"Train size: {len(tr):>6} | Validation size: {len(va):>6}\")\n        \n        X_tr, X_va = sequences[tr], sequences[va]\n        \n        # Scale features\n        scaler = StandardScaler()\n        scaler.fit(np.vstack(X_tr))\n        X_tr_scaled = np.stack([scaler.transform(s) for s in X_tr])\n        X_va_scaled = np.stack([scaler.transform(s) for s in X_va])\n        \n        # Train model\n        model, val_loss = train_hybrid_model(\n            X_tr_scaled, targets_dx[tr], targets_dy[tr], \n            X_va_scaled, targets_dx[va], targets_dy[va],\n            X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config, fold\n        )\n        \n        models.append(model)\n        scalers.append(scaler)\n        fold_scores.append(val_loss)\n        \n        print(f\"‚úÖ  Fold {fold} completed | Validation Loss: {val_loss:.4f}\")\n        \n        # Save checkpoint\n        fold_dir = Path(f\"fold_{fold}\")\n        fold_dir.mkdir(exist_ok=True)\n        \n        torch.save({\n            \"state_dict\": model.state_dict(),\n            \"config\": {\n                \"input_dim\": X_tr[0].shape[-1],\n                \"horizon\": config.MAX_FUTURE_HORIZON\n            }\n        }, fold_dir / \"model.pt\")\n        \n        joblib.dump(scaler, fold_dir / \"scaler.joblib\")\n        print(f\"üíæ  Saved checkpoint ‚Üí {fold_dir}/\")\n        \n        # Memory cleanup\n        del X_tr_scaled, X_va_scaled\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    # CV summary\n    print(f\"\\n{'=' * 80}\")\n    print(\"üìà  CROSS-VALIDATION SUMMARY\")\n    print(f\"{'=' * 80}\")\n    for i, score in enumerate(fold_scores, 1):\n        print(f\"   ‚Ä¢ Fold {i}: {score:.4f}\")\n    print(f\"\\n   ‚Üí Mean CV Loss: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\")\n    print(f\"{'=' * 80}\")\n    \n    # Step 4: Test prediction\n    print(\"\\n[4/5]  Generating test predictions...\")\n    test_sequences, test_ids, _ = prepare_sequences(\n        test_input, test_template=test_template, is_training=False, \n        window_size=config.WINDOW_SIZE\n    )\n    \n    submission = create_ensemble_predictions(\n        models, scalers, test_sequences, test_ids, test_template, config\n    )\n    \n    # Step 5: Save submission\n    print(\"\\n[5/5]  Saving submission file...\")\n    submission.to_csv(\"submission.csv\", index=False)\n    \n    # Completion summary\n    print(f\"\\n{'=' * 80}\")\n    print(\"üèÅ  TRAINING COMPLETED SUCCESSFULLY\")\n    print(f\"{'=' * 80}\")\n    print(f\" Submission saved as: submission.csv\")\n    print(f\" Total predictions : {len(submission)}\")\n \n    \n    print(\"Key Improvements:\".center(60))\n    print(\"   ‚Ä¢ 114 engineered features (vs 82 in previous version)\")\n    print(\"   ‚Ä¢ Player interaction dynamics for spatial awareness\")\n    print(\"   ‚Ä¢ GRU + Conv1D + Attention hybrid for trajectory modeling\")\n    print(\"   ‚Ä¢ Velocity-consistent loss for smoother motion predictions\")\n    print(\"   ‚Ä¢ OneCycleLR scheduler for faster, more stable convergence\")\n    print(\"   ‚Ä¢ 5-fold ensemble for robustness and generalization\")\n    print(\"=\" * 80 + \"\\n\")\n    \n    return submission\n\n# RUN TRAINING\nif __name__ == \"__main__\":\n    submission = main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}