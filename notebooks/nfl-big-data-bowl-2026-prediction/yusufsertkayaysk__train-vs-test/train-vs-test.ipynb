{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> **Special Thanks üôè**  \n> Huge thanks to the **NVIDIA Developer Blog** team for sharing *The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data*.  \n>  \n> It‚Äôs an incredibly insightful resource that inspired several improvements in this notebook.  \n>  \n> üìñ [Read the full article here ‚Üí](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport glob\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import ks_2samp, wasserstein_distance\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.auto import tqdm\n\n# Try to use cuDF for GPU acceleration if available\ntry:\n    import cudf\n    import cupy as cp\n    GPU_AVAILABLE = True\n    print(\"‚úÖ NVIDIA cuDF detected - Using GPU acceleration\")\nexcept ImportError:\n    GPU_AVAILABLE = False\n    print(\"‚ö†Ô∏è  NVIDIA cuDF not available - Using pandas CPU\")\n\nwarnings.filterwarnings('ignore')\n# ================================================================================\n# ENHANCED EDA CLASS FOR SMART ANALYSIS\n# ================================================================================\n\nclass SmartEDA:\n    \"\"\"\n    Advanced EDA that goes beyond basic statistics to detect:\n    1. Train-test distribution shifts\n    2. Temporal patterns and concept drift\n    3. Multivariate distribution differences\n    \"\"\"\n    \n    def __init__(self, gpu_acceleration=False):\n        self.gpu_acceleration = gpu_acceleration and GPU_AVAILABLE\n        self.results = {}\n        \n    def load_data_gpu(self, df):\n        \"\"\"Load data to GPU if available\"\"\"\n        if self.gpu_acceleration:\n            return cudf.from_pandas(df)\n        return df\n    \n    def to_numpy(self, data):\n        \"\"\"Convert GPU data to numpy for scipy compatibility\"\"\"\n        if hasattr(data, 'to_numpy'):\n            return data.to_numpy()\n        elif hasattr(data, 'values'):\n            return data.values\n        return data\n    \n    def compute_distribution_distance(self, train_data, test_data, feature):\n        \"\"\"Compute multiple distribution distance metrics with GPU compatibility\"\"\"\n        # Extract data and convert to numpy for scipy compatibility\n        train_vals = self.to_numpy(train_data[feature].dropna())\n        test_vals = self.to_numpy(test_data[feature].dropna())\n        \n        if len(train_vals) == 0 or len(test_vals) == 0:\n            return {}\n        \n        # KS Test for distribution similarity\n        try:\n            ks_stat, ks_pvalue = ks_2samp(train_vals, test_vals)\n        except Exception as e:\n            print(f\"Warning: KS test failed for {feature}: {e}\")\n            ks_stat, ks_pvalue = 0, 1\n        \n        # Wasserstein distance (Earth Mover's Distance)\n        try:\n            wasserstein_dist = wasserstein_distance(train_vals, test_vals)\n        except:\n            wasserstein_dist = 0\n        \n        # Mean difference\n        mean_diff = np.mean(test_vals) - np.mean(train_vals)\n        mean_diff_pct = (mean_diff / np.mean(train_vals)) * 100 if np.mean(train_vals) != 0 else 0\n        \n        # Variance ratio\n        var_ratio = np.var(test_vals) / np.var(train_vals) if np.var(train_vals) != 0 else 1\n        \n        return {\n            'ks_statistic': ks_stat,\n            'ks_pvalue': ks_pvalue,\n            'wasserstein_distance': wasserstein_dist,\n            'mean_difference': mean_diff,\n            'mean_difference_pct': mean_diff_pct,\n            'variance_ratio': var_ratio,\n            'distribution_shift': ks_pvalue < 0.05 or abs(mean_diff_pct) > 10\n        }\n    \n    def analyze_temporal_patterns(self, df, time_column, target_columns):\n        \"\"\"Analyze temporal patterns and concept drift\"\"\"\n        print(\"üîç Analyzing temporal patterns...\")\n        \n        temporal_insights = {}\n        \n        # Convert to GPU if available\n        df_gpu = self.load_data_gpu(df)\n        \n        # Weekly patterns (if time_column represents weeks)\n        if 'week' in df_gpu.columns:\n            if self.gpu_acceleration:\n                weekly_stats = df_gpu.groupby('week')[target_columns].agg(['mean', 'std', 'count']).compute()\n            else:\n                weekly_stats = df_gpu.groupby('week')[target_columns].agg(['mean', 'std', 'count'])\n            \n            temporal_insights['weekly_patterns'] = weekly_stats\n            \n            # Detect concept drift across weeks\n            weeks = sorted(df_gpu['week'].unique())\n            if len(weeks) > 1:\n                first_week_data = df_gpu[df_gpu['week'] == weeks[0]][target_columns]\n                last_week_data = df_gpu[df_gpu['week'] == weeks[-1]][target_columns]\n                \n                # Convert to numpy for statistical tests\n                first_week_data = self.to_numpy(first_week_data) if self.gpu_acceleration else first_week_data\n                last_week_data = self.to_numpy(last_week_data) if self.gpu_acceleration else last_week_data\n                \n                drift_scores = {}\n                for col in target_columns:\n                    if len(first_week_data[col].dropna()) > 0 and len(last_week_data[col].dropna()) > 0:\n                        first_vals = self.to_numpy(first_week_data[col].dropna())\n                        last_vals = self.to_numpy(last_week_data[col].dropna())\n                        \n                        ks_stat, ks_pvalue = ks_2samp(first_vals, last_vals)\n                        drift_scores[col] = {\n                            'ks_statistic': ks_stat,\n                            'ks_pvalue': ks_pvalue,\n                            'concept_drift': ks_pvalue < 0.05\n                        }\n                \n                temporal_insights['concept_drift'] = drift_scores\n        \n        return temporal_insights\n    \n    def create_distribution_shift_dashboard(self, train_df, test_df, features):\n        \"\"\"Create comprehensive distribution shift analysis with GPU optimization\"\"\"\n        print(\"üìä Creating distribution shift dashboard...\")\n        \n        distribution_analysis = {}\n        \n        # Convert to GPU for faster computation (except for statistical tests)\n        train_gpu = self.load_data_gpu(train_df)\n        test_gpu = self.load_data_gpu(test_df)\n        \n        # For statistical tests, we'll use numpy but benefit from GPU for data processing\n        # Analyze each feature\n        for feature in tqdm(features, desc=\"Analyzing feature distributions\"):\n            if feature in train_gpu.columns and feature in test_gpu.columns:\n                distribution_analysis[feature] = self.compute_distribution_distance(\n                    train_gpu, test_gpu, feature\n                )\n        \n        return distribution_analysis\n    \n    def visualize_distribution_shifts(self, train_df, test_df, distribution_analysis, top_n=15):\n        \"\"\"Visualize the most significant distribution shifts\"\"\"\n        \n        # Get features with significant shifts\n        shifted_features = []\n        for feature, metrics in distribution_analysis.items():\n            if metrics.get('distribution_shift', False):\n                shifted_features.append((feature, metrics['ks_statistic']))\n        \n        # Sort by KS statistic (most significant first)\n        shifted_features.sort(key=lambda x: x[1], reverse=True)\n        top_shifted = shifted_features[:top_n]\n        \n        if not top_shifted:\n            print(\"‚úÖ No significant distribution shifts detected!\")\n            return\n        \n        print(f\"üö® Found {len(shifted_features)} features with distribution shifts\")\n        print(\"Top shifted features:\")\n        for feature, ks_stat in top_shifted[:10]:\n            metrics = distribution_analysis[feature]\n            print(f\"  ‚Ä¢ {feature}: KS={ks_stat:.3f}, MeanDiff={metrics['mean_difference_pct']:.1f}%\")\n        \n        # Create visualization - convert GPU data to pandas for plotting\n        if self.gpu_acceleration:\n            train_plot = train_df.to_pandas() if hasattr(train_df, 'to_pandas') else train_df\n            test_plot = test_df.to_pandas() if hasattr(test_df, 'to_pandas') else test_df\n        else:\n            train_plot = train_df\n            test_plot = test_df\n        \n        n_features = min(len(top_shifted), 9)  # Show up to 3x3 grid\n        nrows = int(np.ceil(n_features / 3))\n        ncols = min(3, n_features)\n        \n        fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n        if n_features == 1:\n            axes = [axes]\n        else:\n            axes = axes.flatten()\n        \n        for idx, (feature, ks_stat) in enumerate(top_shifted[:n_features]):\n            ax = axes[idx]\n            \n            # Plot distributions\n            train_vals = train_plot[feature].dropna()\n            test_vals = test_plot[feature].dropna()\n            \n            # Normalize for comparison\n            if len(train_vals) > 0 and len(test_vals) > 0:\n                sns.kdeplot(train_vals, ax=ax, label='Train', fill=True, alpha=0.5)\n                sns.kdeplot(test_vals, ax=ax, label='Test', fill=True, alpha=0.5)\n                \n                metrics = distribution_analysis[feature]\n                ax.set_title(f'{feature}\\nKS: {ks_stat:.3f}, ŒîŒº: {metrics[\"mean_difference_pct\"]:.1f}%', \n                           fontsize=10, fontweight='bold')\n                ax.legend()\n            \n            if idx >= n_features - 1:\n                break\n        \n        # Hide empty subplots\n        for idx in range(n_features, len(axes)):\n            axes[idx].set_visible(False)\n        \n        plt.suptitle('Top Feature Distribution Shifts: Train vs Test', fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig('distribution_shifts.png', dpi=120, bbox_inches='tight')\n        plt.show()\n        \n        return top_shifted\n\n# ================================================================================\n# ENHANCED DATA LOADING WITH TEMPORAL ANALYSIS\n# ================================================================================\n\ndef load_data_with_temporal_analysis(data_dir):\n    \"\"\"Enhanced data loading that preserves temporal information\"\"\"\n    \n    print(\"üïí Loading data with temporal analysis...\")\n    \n    # Load training data with week information\n    input_files = sorted(glob.glob(os.path.join(data_dir, \"train/input_2023_w*.csv\")))\n    output_files = sorted(glob.glob(os.path.join(data_dir, \"train/output_2023_w*.csv\")))\n    \n    train_weeks = []\n    \n    # Load each week separately to preserve temporal structure\n    for week_idx, (in_file, out_file) in enumerate(zip(input_files, output_files)):\n        week_num = week_idx + 1\n        df_in_week = pd.read_csv(in_file)\n        df_out_week = pd.read_csv(out_file)\n        \n        # Add week information\n        df_in_week['week'] = week_num\n        df_out_week['week'] = week_num\n        \n        train_weeks.append((df_in_week, df_out_week))\n    \n    # Combine with week information\n    df_in = pd.concat([week[0] for week in train_weeks], ignore_index=True)\n    df_out = pd.concat([week[1] for week in train_weeks], ignore_index=True)\n    \n    # Load test data\n    test_in = pd.read_csv(os.path.join(data_dir, \"test_input.csv\"))\n    test_template = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n    \n    # Try to infer test week if possible\n    if 'game_id' in test_in.columns:\n        # Simple heuristic: if game_id patterns differ from training, mark as different temporal period\n        train_game_prefixes = df_in['game_id'].astype(str).str[:3].unique()\n        test_game_prefixes = test_in['game_id'].astype(str).str[:3].unique()\n        \n        if len(set(test_game_prefixes) - set(train_game_prefixes)) > 0:\n            test_in['temporal_period'] = 'future'\n            test_template['temporal_period'] = 'future'\n        else:\n            test_in['temporal_period'] = 'same_period'\n            test_template['temporal_period'] = 'same_period'\n    \n    print(f\"‚úÖ Loaded {len(train_weeks)} weeks of training data\")\n    print(f\"‚úÖ Training periods: Weeks {df_in['week'].min()} to {df_in['week'].max()}\")\n    \n    return df_in, df_out, test_in, test_template\n\n# ================================================================================\n# GPU-ACCELERATED CORRELATION ANALYSIS\n# ================================================================================\n\ndef accelerated_correlation_analysis(train_df, test_df, features):\n    \"\"\"Perform fast correlation analysis using GPU acceleration\"\"\"\n    print(\"üöÄ Performing GPU-accelerated correlation analysis...\")\n    \n    # Use GPU if available\n    if GPU_AVAILABLE:\n        train_gpu = cudf.from_pandas(train_df[features].dropna())\n        test_gpu = cudf.from_pandas(test_df[features].dropna())\n        \n        # Compute correlations on GPU (much faster for large datasets)\n        train_corr = train_gpu.corr().to_pandas()\n        test_corr = test_gpu.corr().to_pandas()\n        \n        # Compute correlation differences\n        corr_diff = test_corr - train_corr\n    else:\n        # Fallback to pandas\n        train_corr = train_df[features].dropna().corr()\n        test_corr = test_df[features].dropna().corr()\n        corr_diff = test_corr - train_corr\n    \n    return train_corr, test_corr, corr_diff\n\n# ================================================================================\n# MAIN ENHANCED EDA EXECUTION\n# ================================================================================\n\ndef perform_enhanced_eda():\n    \"\"\"Perform comprehensive EDA with distribution shift and temporal analysis\"\"\"\n    \n    print(\"üöÄ STARTING ENHANCED EXPLORATORY DATA ANALYSIS\")\n    print(\"=\"*90)\n    \n    # Initialize smart EDA\n    eda = SmartEDA(gpu_acceleration=True)\n    \n    # Load data\n    DATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction/\"\n    df_in, df_out, test_in, test_template = load_data_with_temporal_analysis(DATA_DIR)\n    \n    # Explain importance\n    explain_distribution_shift_importance()\n    explain_temporal_analysis_importance()\n    \n    # ============================================================================\n    # DISTRIBUTION SHIFT ANALYSIS\n    # ============================================================================\n    \n    print(\"\\n\" + \"=\"*90)\n    print(\"üîç ANALYZING TRAIN-TEST DISTRIBUTION SHIFTS\")\n    print(\"=\"*90)\n    \n    # Prepare comparable datasets\n    # For training, use the last observed positions (similar to test setup)\n    train_last_obs = df_in.sort_values(['game_id','play_id','nfl_id','frame_id']).groupby(\n        ['game_id','play_id','nfl_id'], as_index=False\n    ).last()\n    \n    # Select key features for distribution comparison\n    key_features = ['x', 'y', 's', 'a', 'o', 'dir', 'ball_land_x', 'ball_land_y', \n                   'absolute_yardline_number', 'player_weight']\n    \n    # Only use features present in both datasets\n    common_features = [f for f in key_features if f in train_last_obs.columns and f in test_in.columns]\n    \n    print(f\"Analyzing {len(common_features)} common features for distribution shifts...\")\n    \n    # Perform distribution shift analysis\n    distribution_analysis = eda.create_distribution_shift_dashboard(\n        train_last_obs, test_in, common_features\n    )\n    \n    # Visualize results\n    shifted_features = eda.visualize_distribution_shifts(\n        train_last_obs, test_in, distribution_analysis\n    )\n    \n    # ============================================================================\n    # TEMPORAL ANALYSIS\n    # ============================================================================\n    \n    print(\"\\n\" + \"=\"*90)\n    print(\"‚è∞ ANALYZING TEMPORAL PATTERNS AND CONCEPT DRIFT\")\n    print(\"=\"*90)\n    \n    # Analyze how player behavior changes across weeks\n    temporal_features = ['s', 'a', 'x', 'y']  # Speed, acceleration, positions\n    \n    # Create weekly aggregates\n    weekly_behavior = df_in.groupby('week')[temporal_features].agg(['mean', 'std']).reset_index()\n    \n    # Plot temporal evolution\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    for idx, feature in enumerate(temporal_features):\n        ax = axes[idx]\n        \n        # Plot mean with confidence intervals\n        weeks = weekly_behavior['week']\n        means = weekly_behavior[(feature, 'mean')]\n        stds = weekly_behavior[(feature, 'std')]\n        \n        ax.plot(weeks, means, marker='o', linewidth=2, label='Weekly Mean')\n        ax.fill_between(weeks, means - stds, means + stds, alpha=0.2, label='¬±1 Std Dev')\n        \n        # Add trend line\n        z = np.polyfit(weeks, means, 1)\n        p = np.poly1d(z)\n        ax.plot(weeks, p(weeks), \"r--\", alpha=0.8, label=f'Trend (slope: {z[0]:.3f})')\n        \n        ax.set_xlabel('Week')\n        ax.set_ylabel(feature)\n        ax.set_title(f'Temporal Evolution: {feature}', fontweight='bold')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    plt.suptitle('Player Behavior Temporal Analysis Across Weeks', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('temporal_analysis.png', dpi=120, bbox_inches='tight')\n    plt.show()\n    \n    # ============================================================================\n    # GPU-ACCELERATED CORRELATION ANALYSIS\n    # ============================================================================\n    \n    print(\"\\n\" + \"=\"*90)\n    print(\"üöÄ PERFORMING GPU-ACCELERATED CORRELATION ANALYSIS\")\n    print(\"=\"*90)\n    \n    # Use GPU for fast correlation computation\n    correlation_features = [f for f in common_features if f in ['x', 'y', 's', 'a', 'o', 'dir']]\n    \n    if len(correlation_features) >= 3:  # Need at least 3 features for meaningful correlation\n        train_corr, test_corr, corr_diff = accelerated_correlation_analysis(\n            train_last_obs, test_in, correlation_features\n        )\n        \n        # Plot correlation matrices\n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        \n        # Train correlations\n        sns.heatmap(train_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n                   ax=axes[0], square=True, cbar_kws={'shrink': 0.8})\n        axes[0].set_title('Training Data Correlations', fontweight='bold')\n        \n        # Test correlations\n        sns.heatmap(test_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n                   ax=axes[1], square=True, cbar_kws={'shrink': 0.8})\n        axes[1].set_title('Test Data Correlations', fontweight='bold')\n        \n        # Correlation differences\n        sns.heatmap(corr_diff, annot=True, fmt='.2f', cmap='RdYlBu_r', center=0, \n                   ax=axes[2], square=True, cbar_kws={'shrink': 0.8})\n        axes[2].set_title('Correlation Differences (Test - Train)', fontweight='bold')\n        \n        plt.suptitle('GPU-Accelerated Correlation Analysis: Train vs Test', fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig('correlation_analysis.png', dpi=120, bbox_inches='tight')\n        plt.show()\n        \n        # Identify significant correlation changes\n        significant_changes = np.abs(corr_diff) > 0.1\n        if significant_changes.any().any():\n            print(\"üö® Significant correlation changes detected:\")\n            for i, row in significant_changes.iterrows():\n                for j, val in row.items():\n                    if val and i != j:\n                        print(f\"  ‚Ä¢ {i} vs {j}: Œî = {corr_diff.loc[i, j]:.3f}\")\n    \n    # ============================================================================\n    # MULTIVARIATE DISTRIBUTION ANALYSIS\n    # ============================================================================\n    \n    print(\"\\n\" + \"=\"*90)\n    print(\"üìä ANALYZING MULTIVARIATE DISTRIBUTION DIFFERENCES\")\n    print(\"=\"*90)\n    \n    # Compare joint distributions of key feature pairs\n    feature_pairs = [('s', 'a'), ('x', 'y'), ('ball_land_x', 'ball_land_y')]\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    for idx, (feat1, feat2) in enumerate(feature_pairs):\n        if feat1 in common_features and feat2 in common_features:\n            # Train distribution\n            ax1 = axes[0, idx]\n            sample_train = train_last_obs.sample(min(5000, len(train_last_obs)))\n            h1 = ax1.hist2d(sample_train[feat1], sample_train[feat2], bins=30, cmap='Blues', alpha=0.8)\n            ax1.set_title(f'Train: {feat1} vs {feat2}', fontweight='bold')\n            ax1.set_xlabel(feat1)\n            ax1.set_ylabel(feat2)\n            plt.colorbar(h1[3], ax=ax1)\n            \n            # Test distribution  \n            ax2 = axes[1, idx]\n            sample_test = test_in.sample(min(5000, len(test_in)))\n            h2 = ax2.hist2d(sample_test[feat1], sample_test[feat2], bins=30, cmap='Reds', alpha=0.8)\n            ax2.set_title(f'Test: {feat1} vs {feat2}', fontweight='bold')\n            ax2.set_xlabel(feat1)\n            ax2.set_ylabel(feat2)\n            plt.colorbar(h2[3], ax=ax2)\n    \n    plt.suptitle('Multivariate Distribution Comparison: Train vs Test', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('multivariate_comparison.png', dpi=120, bbox_inches='tight')\n    plt.show()\n    \n\n    # GPU performance report\n    if GPU_AVAILABLE:\n        print(f\"\\n‚ö° **GPU ACCELERATION**: Enabled - ~10-100x speedup for large datasets\")\n        print(\"   ‚Ä¢ Faster correlation computation\")\n        print(\"   ‚Ä¢ Efficient multivariate analysis\")\n        print(\"   ‚Ä¢ Scalable to millions of rows\")\n    \n    return {\n        'distribution_analysis': distribution_analysis,\n        'temporal_analysis': weekly_behavior,\n        'shifted_features': shifted_features,\n        'temporal_trends': week_correlations\n    }\n\n# ================================================================================\n# EXECUTE ENHANCED EDA\n# ================================================================================\n\nif __name__ == \"__main__\":\n    # Perform the enhanced EDA\n    eda_results = perform_enhanced_eda()\n    \n    print(\"\\n\" + \"=\"*90)\n    print(\"‚úÖ ENHANCED EDA COMPLETE!\")\n    print(\"=\"*90)\n    \n    # Summary statistics\n    if eda_results.get('shifted_features'):\n        print(f\"üö® Distribution shifts detected: {len(eda_results['shifted_features'])} features\")\n    else:\n        print(\"‚úÖ No significant distribution shifts detected\")\n    \n    significant_trends = sum(1 for v in eda_results.get('temporal_trends', {}).values() if abs(v) > 0.01)\n    if significant_trends > 0:\n        print(f\"üìà Temporal trends detected: {significant_trends} features\")\n    else:\n        print(\"‚úÖ No significant temporal trends detected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T16:07:27.579512Z","iopub.execute_input":"2025-10-06T16:07:27.579807Z","iopub.status.idle":"2025-10-06T16:07:50.499645Z","shell.execute_reply.started":"2025-10-06T16:07:27.579787Z","shell.execute_reply":"2025-10-06T16:07:50.49696Z"}},"outputs":[],"execution_count":null}]}