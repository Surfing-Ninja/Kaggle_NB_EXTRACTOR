{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30407,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Introduction\n\nIn my [previous notebook](https://www.kaggle.com/code/faeghehghofrani/simple-cleaning-no-eda-ridge-classifier), I obtained a suitable model with relatively high accuracy by using a simple method to clean tweet data and evaluate 6 classification models on 2 data splitting methods.\n\nIn this notebook, using EDA techniques, data engineering and more basic data cleaning, I achieved higher accuracy than the previous notebook.\n\nTo write this document, I have used the following notebooks:\n* https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n* https://www.kaggle.com/code/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n* https://www.kaggle.com/code/rohitgarud/all-almost-data-preprocessing-techniques-for-nlp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport warnings\nimport string\nimport re\nfrom scipy.sparse import hstack, coo_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report\nfrom sklearn.linear_model import RidgeClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nwarnings.filterwarnings('ignore')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:40:07.505746Z","iopub.execute_input":"2023-03-21T12:40:07.50623Z","iopub.status.idle":"2023-03-21T12:40:09.044529Z","shell.execute_reply.started":"2023-03-21T12:40:07.506188Z","shell.execute_reply":"2023-03-21T12:40:09.043167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:40:12.304317Z","iopub.execute_input":"2023-03-21T12:40:12.304754Z","iopub.status.idle":"2023-03-21T12:40:12.384174Z","shell.execute_reply.started":"2023-03-21T12:40:12.304715Z","shell.execute_reply":"2023-03-21T12:40:12.382997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T17:19:54.766669Z","iopub.execute_input":"2023-03-20T17:19:54.767017Z","iopub.status.idle":"2023-03-20T17:19:54.794713Z","shell.execute_reply.started":"2023-03-20T17:19:54.76699Z","shell.execute_reply":"2023-03-20T17:19:54.79334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA","metadata":{}},{"cell_type":"code","source":"def Info_dataFrame(df):\n    name =[x for x in globals() if globals()[x] is df][0]\n    print('informaion of {}:'.format(name))\n    print(\"--\"*20)\n    print(df.info())\n    print(\"==\"*20)\n    print('informaion about count of Null in {}:'.format(name))\n    print(\"--\"*20)\n    print(df.isnull().sum())\n    print(\"==\"*20)\n    print('informaion about count of unique Value in {}:'.format(name))\n    print(\"--\"*20)\n    print(df.nunique())","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:40:23.482972Z","iopub.execute_input":"2023-03-21T12:40:23.483416Z","iopub.status.idle":"2023-03-21T12:40:23.492446Z","shell.execute_reply.started":"2023-03-21T12:40:23.483377Z","shell.execute_reply":"2023-03-21T12:40:23.490981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Info_dataFrame(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:40:27.500793Z","iopub.execute_input":"2023-03-21T12:40:27.501205Z","iopub.status.idle":"2023-03-21T12:40:27.547446Z","shell.execute_reply.started":"2023-03-21T12:40:27.501171Z","shell.execute_reply":"2023-03-21T12:40:27.545945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Info_dataFrame(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:40:33.867656Z","iopub.execute_input":"2023-03-21T12:40:33.868132Z","iopub.status.idle":"2023-03-21T12:40:33.891235Z","shell.execute_reply.started":"2023-03-21T12:40:33.868089Z","shell.execute_reply":"2023-03-21T12:40:33.889812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Keyword Analysis\n\nBased on the above information, the number of null cells and the number of unique values of Keyword in the train and test data are as follows:\n\n----------------\n##### train data:\n----------------\nAll data = 7613\n\nNull data = 61\n\nNumber of unique value = 221\n\n\n----------------\n##### test data:\n----------------\nAll data = 3263\n\nNull data = 26\n\nNumber of unique value = 221","metadata":{"execution":{"iopub.status.busy":"2023-03-18T11:01:27.771005Z","iopub.execute_input":"2023-03-18T11:01:27.771573Z","iopub.status.idle":"2023-03-18T11:01:27.780195Z","shell.execute_reply.started":"2023-03-18T11:01:27.771532Z","shell.execute_reply":"2023-03-18T11:01:27.778734Z"}}},{"cell_type":"markdown","source":"#### 2. location Analysis\nBased on the above information, the number of null cells and the number of unique values of location in the train and test data are as follows:\n\n----------------\n##### train data:\n----------------\nAll data = 7613\n\nNull data = 2533\n\nNumber of unique value = 3341\n\n\n----------------\n##### test data:\n----------------\nAll data = 3263\n\nNull data = 1105\n\nNumber of unique value = 1602","metadata":{}},{"cell_type":"markdown","source":"#### 3. text Analysis\n\nIn the analysis of tweets, we extract meta features because these features can help the model to identify fake or true tweets.","metadata":{}},{"cell_type":"code","source":"def text_analyze(df):\n    # word_count\n    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n\n    # stop_word_count\n    df['stop_word_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n\n    # url_count\n    df['url_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n    # punctuation_count\n    df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n    # hashtag_count\n    df['hashtag_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n    # mention_count\n    df['mention_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n\n    # emoji-count\n    def emoji_count(text):\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        list_emoji = emoji_pattern.findall(text)\n        return len(list_emoji)\n\n    df['emoji-count'] = df['text'].apply(emoji_count)\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:40:42.369009Z","iopub.execute_input":"2023-03-21T12:40:42.369423Z","iopub.status.idle":"2023-03-21T12:40:42.381451Z","shell.execute_reply.started":"2023-03-21T12:40:42.369389Z","shell.execute_reply":"2023-03-21T12:40:42.380309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_analyze(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:40:45.859396Z","iopub.execute_input":"2023-03-21T12:40:45.86015Z","iopub.status.idle":"2023-03-21T12:41:01.239511Z","shell.execute_reply.started":"2023-03-21T12:40:45.860095Z","shell.execute_reply":"2023-03-21T12:41:01.23793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_analyze(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:41:10.368316Z","iopub.execute_input":"2023-03-21T12:41:10.368733Z","iopub.status.idle":"2023-03-21T12:41:17.041834Z","shell.execute_reply.started":"2023-03-21T12:41:10.368697Z","shell.execute_reply":"2023-03-21T12:41:17.040389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning text\n\nTo clean tweet data, various data processing techniques have been used. The cleaning steps are as follows:\n\n- Removing HTML\n- Expand Contractions\n- Removing URLs\n- Removing Email IDs\n- Remove emojis\n- Removing Tweeter Mentions char\n- Abbreviation/Acronym Disambiguation\n- Removing Unicode Characters\n- Removing Punctuations\n- Handling Digits or Words with Digits\n- Removing Stopwords\n- lower case","metadata":{}},{"cell_type":"code","source":"pip install contractions","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:41:32.500853Z","iopub.execute_input":"2023-03-21T12:41:32.501286Z","iopub.status.idle":"2023-03-21T12:41:47.535948Z","shell.execute_reply.started":"2023-03-21T12:41:32.50125Z","shell.execute_reply":"2023-03-21T12:41:47.534499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import contractions\nimport string\n#from spellchecker import SpellChecker\n\ndef clean_tweet(text):\n\n    # Remove_HTMLs\n    new_text = re.sub(r'<.*?>',\"\", text)\n\n    # Expand_Contractions\n    expanded_words = []   \n    for word in new_text.split():\n        # using contractions.fix to expand the shortened words\n        expanded_words.append(contractions.fix(word))  \n   \n    new_text = ' '.join(expanded_words)\n\n    # Remove_URLs\n    new_text = re.sub(r'https?://(www\\.)?(\\w+)(\\.\\w+)(/\\w*)?',\"\", new_text)\n\n    # Remove_Email_IDs\n    new_text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+',\"\", new_text)\n\n    # Remove_emojis\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    new_text = emoji_pattern.sub(r'', new_text)\n\n    # Remove_Tweeter_Mentions_Chars\n    new_text = re.sub(r'@\\w+',\"\", new_text)\n\n    # Abbreviation/Acronym_Disambiguation\n    new_text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", new_text)\n    new_text = re.sub(r\"mÌ¼sica\", \"music\", new_text)\n    new_text = re.sub(r\"okwx\", \"Oklahoma City Weather\", new_text)\n    new_text = re.sub(r\"arwx\", \"Arkansas Weather\", new_text)    \n    new_text = re.sub(r\"gawx\", \"Georgia Weather\", new_text)  \n    new_text = re.sub(r\"scwx\", \"South Carolina Weather\", new_text)  \n    new_text = re.sub(r\"cawx\", \"California Weather\", new_text)\n    new_text = re.sub(r\"tnwx\", \"Tennessee Weather\", new_text)\n    new_text = re.sub(r\"azwx\", \"Arizona Weather\", new_text)  \n    new_text = re.sub(r\"alwx\", \"Alabama Weather\", new_text)\n    new_text = re.sub(r\"wordpressdotcom\", \"wordpress\", new_text)    \n    new_text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", new_text)\n    new_text = re.sub(r\"Suruc\", \"Sanliurfa\", new_text)\n\n    new_text = re.sub(r\"&gt;\", \">\", new_text)\n    new_text = re.sub(r\"&lt;\", \"<\", new_text)\n    new_text = re.sub(r\"&amp;\", \"&\", new_text)\n\n    # Remove_Unicode_Characters\n    new_text = new_text.encode(\"ascii\", \"ignore\").decode()\n\n    # Remove_Punctuations\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        new_text = re.sub(re.escape(p), \" \",new_text)\n\n    # Remove_Digits\n    new_text = re.sub(r'\\w*\\d+\\w*', \"\",new_text)\n\n    # Remove_Stopwords\n    new_text = \" \".join([word for word in str(new_text).split() if word not in stopwords.words('english')])\n\n    # Lower_case\n    new_text = new_text.lower()\n    \n    return new_text","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:41:58.802591Z","iopub.execute_input":"2023-03-21T12:41:58.803086Z","iopub.status.idle":"2023-03-21T12:41:58.837176Z","shell.execute_reply.started":"2023-03-21T12:41:58.803037Z","shell.execute_reply":"2023-03-21T12:41:58.835978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['clean_text'] = train_df['text'].apply(clean_tweet)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:42:10.700566Z","iopub.execute_input":"2023-03-21T12:42:10.701056Z","iopub.status.idle":"2023-03-21T12:42:27.016269Z","shell.execute_reply.started":"2023-03-21T12:42:10.701013Z","shell.execute_reply":"2023-03-21T12:42:27.014893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['clean_text'] = test_df['text'].apply(clean_tweet)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:45:49.980172Z","iopub.execute_input":"2023-03-21T12:45:49.981505Z","iopub.status.idle":"2023-03-21T12:45:57.050342Z","shell.execute_reply.started":"2023-03-21T12:45:49.981451Z","shell.execute_reply":"2023-03-21T12:45:57.048722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model\n\nAccording to the result of the [previous notebook](https://www.kaggle.com/code/faeghehghofrani/simple-cleaning-no-eda-ridge-classifier), in this document we also use the tfidf method for tweets, and then by adding other meta data to the obtained matrix and finding the best classification model, we train and test the model.","metadata":{}},{"cell_type":"code","source":"# Convert the text column to a matrix of TF-IDF features\n\ntfidf = TfidfVectorizer()\ntext_matrix = tfidf.fit_transform(train_df['clean_text'])\n\n# Concatenate the TF-IDF matrix with the other columns\n\narr = np.array(train_df[['word_count', 'stop_word_count', 'url_count', 'punctuation_count', 'hashtag_count', 'mention_count', 'emoji-count']].values)\nM = coo_matrix(arr)\ntext_matrix_full = hstack((text_matrix, M))\n\ntext_matrix_full","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:46:54.341279Z","iopub.execute_input":"2023-03-21T12:46:54.341778Z","iopub.status.idle":"2023-03-21T12:46:54.494944Z","shell.execute_reply.started":"2023-03-21T12:46:54.341732Z","shell.execute_reply":"2023-03-21T12:46:54.493507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_train(df, model):\n    tfidf = TfidfVectorizer()\n    text_matrix = tfidf.fit_transform(df['clean_text'])\n    arr = np.array(df[['word_count', 'stop_word_count', 'url_count', 'punctuation_count', 'hashtag_count', 'mention_count', 'emoji-count']].values)\n    M = coo_matrix(arr)\n    text_matrix_full = hstack((text_matrix, M))\n    X = text_matrix.toarray()\n    y = df['target'].values\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size= 0.3, random_state= 42, stratify=y)\n\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    accuracy = round(accuracy_score(y_test,y_pred),3)\n    \n    print(f'Accuracy: {np.round(accuracy*100,2)}%')\n    print('='*50)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:47:14.823235Z","iopub.execute_input":"2023-03-21T12:47:14.823734Z","iopub.status.idle":"2023-03-21T12:47:14.833579Z","shell.execute_reply.started":"2023-03-21T12:47:14.823658Z","shell.execute_reply":"2023-03-21T12:47:14.832145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(), GaussianNB(), RidgeClassifier()]\nprint(\"Models Accuracy with TFIDF split:\")\nfor mod in models:\n    print(f'Model :{mod}')\n    split_train(train_df, mod)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:47:20.741497Z","iopub.execute_input":"2023-03-21T12:47:20.741952Z","iopub.status.idle":"2023-03-21T12:51:36.734901Z","shell.execute_reply.started":"2023-03-21T12:47:20.741911Z","shell.execute_reply":"2023-03-21T12:51:36.733534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf = TfidfVectorizer()\ntrain_Vector = tf.fit_transform(train_df['clean_text'])\narr = np.array(train_df[['word_count', 'stop_word_count', 'url_count', 'punctuation_count', 'hashtag_count', 'mention_count', 'emoji-count']].values)\nM = coo_matrix(arr)\ntrain_Vector_full = hstack((train_Vector, M))\n\nmodel = RidgeClassifier()\nmodel.fit(train_Vector_full, train_df[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:53:31.224279Z","iopub.execute_input":"2023-03-21T12:53:31.224744Z","iopub.status.idle":"2023-03-21T12:53:31.48322Z","shell.execute_reply.started":"2023-03-21T12:53:31.224693Z","shell.execute_reply":"2023-03-21T12:53:31.48145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_vector = tf.transform(test_df['clean_text'])\narr_test = np.array(test_df[['word_count', 'stop_word_count', 'url_count', 'punctuation_count', 'hashtag_count', 'mention_count', 'emoji-count']].values)\nM_test = coo_matrix(arr_test)\ntest_vector_full = hstack((test_vector, M_test))\ntest_vector_full","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:53:46.882454Z","iopub.execute_input":"2023-03-21T12:53:46.88304Z","iopub.status.idle":"2023-03-21T12:53:46.947178Z","shell.execute_reply.started":"2023-03-21T12:53:46.882971Z","shell.execute_reply":"2023-03-21T12:53:46.94582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(test_vector_full)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:53:54.161496Z","iopub.execute_input":"2023-03-21T12:53:54.161946Z","iopub.status.idle":"2023-03-21T12:53:54.173378Z","shell.execute_reply.started":"2023-03-21T12:53:54.161906Z","shell.execute_reply":"2023-03-21T12:53:54.172271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:54:00.041471Z","iopub.execute_input":"2023-03-21T12:54:00.042887Z","iopub.status.idle":"2023-03-21T12:54:00.0669Z","shell.execute_reply.started":"2023-03-21T12:54:00.042831Z","shell.execute_reply":"2023-03-21T12:54:00.065527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\"id\":test_df[\"id\"],\"target\":model.predict(test_vector_full)})","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:56:13.841472Z","iopub.execute_input":"2023-03-21T12:56:13.84193Z","iopub.status.idle":"2023-03-21T12:56:13.851727Z","shell.execute_reply.started":"2023-03-21T12:56:13.84189Z","shell.execute_reply":"2023-03-21T12:56:13.85041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:56:20.401467Z","iopub.execute_input":"2023-03-21T12:56:20.401936Z","iopub.status.idle":"2023-03-21T12:56:20.416052Z","shell.execute_reply.started":"2023-03-21T12:56:20.401897Z","shell.execute_reply":"2023-03-21T12:56:20.414685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:56:28.340246Z","iopub.execute_input":"2023-03-21T12:56:28.340655Z","iopub.status.idle":"2023-03-21T12:56:28.355643Z","shell.execute_reply.started":"2023-03-21T12:56:28.340619Z","shell.execute_reply":"2023-03-21T12:56:28.354746Z"},"trusted":true},"execution_count":null,"outputs":[]}]}