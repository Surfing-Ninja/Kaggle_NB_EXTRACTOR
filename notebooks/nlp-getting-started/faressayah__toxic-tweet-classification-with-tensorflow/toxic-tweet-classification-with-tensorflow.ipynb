{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":1149416,"sourceType":"datasetVersion","datasetId":551536},{"sourceId":1258099,"sourceType":"datasetVersion","datasetId":483720},{"sourceId":1451513,"sourceType":"datasetVersion","datasetId":798386},{"sourceId":1640141,"sourceType":"datasetVersion","datasetId":519753}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìö Natural Language Processing with Disaster Tweets üí¨\n\nüéØ Welcome to the toxic tweet classification notebook! This tutorial is hands-on and focused on how to fine-tune pre-trained models for specific use cases.\n\nwe will start by cleaning the dataset and then build a model to classify tweet. \n\nThe base model we will use is [Twitter-roBERTa-base for Sentiment Analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment?text=I+like+you.+I+love+you) ","metadata":{}},{"cell_type":"code","source":"import re\nimport string\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import TFAutoModel\n\n\nprint(tf.__version__)\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:58:58.34872Z","iopub.execute_input":"2024-12-06T14:58:58.349382Z","iopub.status.idle":"2024-12-06T14:59:14.27495Z","shell.execute_reply.started":"2024-12-06T14:58:58.349349Z","shell.execute_reply":"2024-12-06T14:59:14.27416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìã Loading the Data","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"../input/nlp-getting-started\"\n\ntrain = pd.read_csv(f\"{DATA_PATH}/train.csv\")\ntest = pd.read_csv(f\"{DATA_PATH}/test.csv\")\nsample_sub = pd.read_csv(f\"{DATA_PATH}/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:59:14.276899Z","iopub.execute_input":"2024-12-06T14:59:14.277852Z","iopub.status.idle":"2024-12-06T14:59:14.349973Z","shell.execute_reply.started":"2024-12-06T14:59:14.277822Z","shell.execute_reply":"2024-12-06T14:59:14.349165Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:59:14.351183Z","iopub.execute_input":"2024-12-06T14:59:14.351755Z","iopub.status.idle":"2024-12-06T14:59:14.365415Z","shell.execute_reply.started":"2024-12-06T14:59:14.351715Z","shell.execute_reply":"2024-12-06T14:59:14.36467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:59:14.366436Z","iopub.execute_input":"2024-12-06T14:59:14.366676Z","iopub.status.idle":"2024-12-06T14:59:14.379887Z","shell.execute_reply.started":"2024-12-06T14:59:14.366652Z","shell.execute_reply":"2024-12-06T14:59:14.37913Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üî® Preprocessing","metadata":{}},{"cell_type":"code","source":"#Use regex to clean the data\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text \n\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\n# train['text'] = train['text'].apply(lambda x : remove_url(x))\n# train['text'] = train['text'].apply(lambda x : remove_punct(x))\n# train['text'] = train['text'].apply(lambda x : remove_emoji(x))\n# train['text'] = train['text'].apply(lambda x : decontraction(x))\n# train['text'] = train['text'].apply(lambda x : seperate_alphanumeric(x))\n# train['text'] = train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n\n# test['text'] = test['text'].apply(lambda x : remove_url(x))\n# test['text'] = test['text'].apply(lambda x : remove_punct(x))\n# test['text'] = test['text'].apply(lambda x : remove_emoji(x))\n# test['text'] = test['text'].apply(lambda x : decontraction(x))\n# test['text'] = test['text'].apply(lambda x : seperate_alphanumeric(x))\n# test['text'] = test['text'].apply(lambda x : unique_char(cont_rep_char,x))","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:59:14.382062Z","iopub.execute_input":"2024-12-06T14:59:14.382456Z","iopub.status.idle":"2024-12-06T14:59:14.392916Z","shell.execute_reply.started":"2024-12-06T14:59:14.382418Z","shell.execute_reply":"2024-12-06T14:59:14.392161Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ‚úÇÔ∏è Tokenization","metadata":{}},{"cell_type":"code","source":"SEQ_LEN = 256\nBATCH_SIZE = 16\nNUM_OF_SAMPLES = len(train)\nMODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\nSPLIT_SIZE = 0.7\nEPOCHS = 100\nLEARNING_RATE = 1e-5\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nvocab_size = len(tokenizer.vocab)\nprint(f\"[INFO]: Tokenizer for the model {MODEL_NAME} has {vocab_size} unique words\")\n\nmax_sequence_length = tokenizer.model_max_length\nprint(f\"[INFO]: Maximum sequence length the model {MODEL_NAME} is able to handle: {max_sequence_length} \")\n\ntrain_tokens = tokenizer(\n    train['text'].tolist(), \n    max_length=SEQ_LEN, \n    truncation=True, \n    padding=\"max_length\", \n    add_special_tokens=True, \n    return_tensors='np'\n)\n\ny_train = train['target'].values\nlabels = np.zeros((NUM_OF_SAMPLES, y_train.max() + 1))\nlabels[np.arange(NUM_OF_SAMPLES), y_train] = 1\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    (\n        train_tokens['input_ids'], \n        train_tokens['attention_mask'], \n        labels\n    )\n)\n\ndef map_func(input_ids, masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': masks\n    }, labels\n\ndataset = dataset.map(map_func)\ndataset = dataset.shuffle(10000).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n\nsize = int((train_tokens['input_ids'].shape[0] // BATCH_SIZE) * SPLIT_SIZE)\n\ntrain_ds = dataset.take(size)\nval_ds = dataset.skip(size)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:59:14.393883Z","iopub.execute_input":"2024-12-06T14:59:14.394143Z","iopub.status.idle":"2024-12-06T14:59:17.532148Z","shell.execute_reply.started":"2024-12-06T14:59:14.394119Z","shell.execute_reply":"2024-12-06T14:59:17.531509Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ü§ñ Model Building","metadata":{}},{"cell_type":"code","source":"model = TFAutoModel.from_pretrained(MODEL_NAME)\n\n# Two inputs\ninput_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')\n\n# Transformer\n# embeddings = model.bert(input_ids, attention_mask=mask)[1]\nembeddings = model(input_ids, attention_mask=mask)[0]\nembeddings = embeddings[:, 0, :]\n# Classifier head\nx = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n# x = tf.keras.layers.Dropout(0.1)(x)\ny = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n\nbert_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n\n# freeze bert layers\n# bert_model.layers[2].trainable = False\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.BinaryAccuracy()\n\nbert_model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n\nhistory = bert_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-06T15:02:19.834163Z","iopub.execute_input":"2024-12-06T15:02:19.835129Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìä Model Evaluation","metadata":{}},{"cell_type":"code","source":"def plot_learning_evolution(r):\n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(2, 2, 1)\n    plt.plot(r.history['loss'], label='Loss')\n    plt.plot(r.history['val_loss'], label='val_Loss')\n    plt.title('Loss evolution during trainig')\n    plt.legend()\n\n    plt.subplot(2, 2, 2)\n    plt.plot(r.history['binary_accuracy'], label='binary_accuracy')\n    plt.plot(r.history['val_binary_accuracy'], label='val_binary_accuracy')\n    plt.title('Accuracy score evolution during trainig')\n    plt.legend();","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.256541Z","iopub.status.idle":"2024-12-06T14:27:55.256945Z","shell.execute_reply.started":"2024-12-06T14:27:55.256776Z","shell.execute_reply":"2024-12-06T14:27:55.256795Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_learning_evolution(history)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.258137Z","iopub.status.idle":"2024-12-06T14:27:55.258469Z","shell.execute_reply.started":"2024-12-06T14:27:55.258312Z","shell.execute_reply":"2024-12-06T14:27:55.258329Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_model.evaluate(val_ds)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.259671Z","iopub.status.idle":"2024-12-06T14:27:55.259963Z","shell.execute_reply.started":"2024-12-06T14:27:55.25982Z","shell.execute_reply":"2024-12-06T14:27:55.259835Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prep_data(text):\n    tokens = tokenizer(\n        text, max_length=256, truncation=True, \n        padding='max_length', \n        add_special_tokens=True, \n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tokens['input_ids'], \n        'attention_mask': tokens['attention_mask']\n    }\n\ntest['target'] = None\n\nfor i, row in test.iterrows():\n    tokens = prep_data(row['text'])\n#     probs = bert_model.predict(tokens)\n    probs = bert_model.predict_on_batch(tokens)\n    pred = np.argmax(probs)\n    test.at[i, 'target'] = pred\n    \ntest['target'] = test['target'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.261641Z","iopub.status.idle":"2024-12-06T14:27:55.262111Z","shell.execute_reply.started":"2024-12-06T14:27:55.261861Z","shell.execute_reply":"2024-12-06T14:27:55.261883Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.263802Z","iopub.status.idle":"2024-12-06T14:27:55.264256Z","shell.execute_reply.started":"2024-12-06T14:27:55.264022Z","shell.execute_reply":"2024-12-06T14:27:55.264057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.265721Z","iopub.status.idle":"2024-12-06T14:27:55.266169Z","shell.execute_reply.started":"2024-12-06T14:27:55.265932Z","shell.execute_reply":"2024-12-06T14:27:55.265956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Making submission","metadata":{"trusted":true}},{"cell_type":"code","source":"sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(), 'target':test['target']})\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.267354Z","iopub.status.idle":"2024-12-06T14:27:55.267804Z","shell.execute_reply.started":"2024-12-06T14:27:55.267567Z","shell.execute_reply":"2024-12-06T14:27:55.26759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-06T14:27:55.269543Z","iopub.status.idle":"2024-12-06T14:27:55.269985Z","shell.execute_reply.started":"2024-12-06T14:27:55.269755Z","shell.execute_reply":"2024-12-06T14:27:55.269778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}