{"cells":[{"metadata":{},"cell_type":"markdown","source":">In this notebook, the concepts of text mining and natural language processing are explained. The reader is taken by the hand and it is explained step by step how it works."},{"metadata":{},"cell_type":"markdown","source":"![https://3nions.com/wp-content/uploads/2020/01/comp_1.gif](https://3nions.com/wp-content/uploads/2020/01/comp_1.gif)\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency theyâ€™re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, itâ€™s not always clear whether a personâ€™s words are actually announcing a disaster. \n<p style=\"font-size : 12px\"><em>extract from the competition description</em></p>\n<p style=\"font-size : 12px\"><em>gif from: https://3nions.com/wp-content/uploads/2020/01/comp_1.gif</em></p>\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='top'></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"><p style=\"font-size : 25px\"><font color=\"grey\">Overview:</font></p></h3>\n\n1. [<font color=\"grey\">Data Preparation<font/>](#1)      \n2. [<font color=\"grey\">Wordcloud<font/>](#2)\n3. [<font color=\"grey\">Deep Learning<font/>](#3)<br>\n    - 3.1 [<font color=\"grey\">Tokenizer<font/>](#3.1)\n    - 3.2 [<font color=\"grey\">Auto Encoder<font/>](#3.2)\n    - 3.3 [<font color=\"grey\">Deep Learning Architecture<font/>](#3.3)\n    - 3.4 [<font color=\"grey\">Cross Validation<font/>](#3.4)\n    - 3.5 [<font color=\"grey\">Final Model<font/>](#3.5)  \n4. [<font color=\"grey\">Submission<font/>](#4)"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"options(warn=-1)\nset.seed(111)\n\nlibrary(data.table)\nlibrary(tm)\nlibrary(ggplot2)\nlibrary(ggwordcloud)\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(plotly)\nlibrary(htmlwidgets)\nlibrary(IRdisplay)\nlibrary(hunspell)\nlibrary(gridExtra)\nlibrary(DT)\n\ninstall.packages(\"qdap\")\nlibrary(qdap)\n\ndir.create(file.path(\"charts/\"), showWarnings = FALSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"grey\"><b>1. Data Preparation </b></font><br><a id=\"1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The tm-package is a text mining package. With keras it is the most important one for the following explanations.\n\nFor text mining, first step is to read the text files. It depends on the formats whether it is .pdf, .csv, .docx or another kind of format. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tm::getSources","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Reading the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN <- fread(\"../input/nlp-getting-started/train.csv\", data.table = F)\nTEST <- fread(\"../input/nlp-getting-started/test.csv\", data.table = F)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"unlist(strsplit(as.character(TRAIN$text[1]), split = \" \")) %>%\nhunspell_check()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data dimensions:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dim(TRAIN); dim(TEST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s take a look at the data:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dtW <- DT::datatable(TRAIN)\n\npath <-\"charts/dtW.html\"\nsaveWidget(dtW, file.path(normalizePath(dirname(path)),basename(path)))\nIRdisplay::display_html('<iframe src=\"charts/dtW.html\" align=\"center\" width=\"100%\" height=\"500\" frameBorder=\"0\"></iframe>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After reading the data, first step is to create a text corpus. A corpus gives the text data a controllable structure. This makes it possible to select specific text components. In this case, the tweeds. (*If you read a -pdf for example, you might get the pages instead*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpTRAIN <- Corpus(VectorSource(TRAIN$text))\ncorpTEST <- Corpus(VectorSource(TEST$text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpTRAIN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nWe have one corpus containing 7613 documents. The corpus allows us to perform all possible cleanups.\n\n* tolower is used to compare words from the start of a sentence with other words.\n* removement of emojis ðŸ¤¬\n* In the English language, cotractions are often used. These are converted back to the original form by appropriate vectors.\n* removement of punctations and numbers to obtain a clean text.\n* dropping stopwords. Stopwords are words with no specific meaning like \"a\", \"the\", or \"by\".\n* stemming is an alorithm (in this case Porter) to find the word stem. For example, the stem of the words \"paint\", \"painted\", \"painting\" is the same.\n* dropping URLs.\n* After all those cleanups the text might have many whitespaces besides. So, a stripping of whitespaces reduces multiple whitespaces to one between each word.\n\nWe define a function to do all the mentioned steps:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"emojis <- c(\"\\U0001F600-\\U0001F64F\", \"\\U0001F300-\\U0001F5FF\", \"\\U0001F680-\\U0001F6FF\", \"\\U0001F1E0-\\U0001F1FF\", \"\\U00002702-\\U000027B0\", \"\\U000024C2-\\U0001F251\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Contractions are hidden. Click on \"code\" to get insight.\n\nContractions are from [here](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"contractions_1 = c( \n\"ain't\",\n\"aren't\",\n\"can't\",\n\"can't've\",\n\"'cause\",\n\"could've\",\n\"couldn't\",\n\"couldn't've\",\n\"didn't\",\n\"doesn't\",\n\"don't\",\n\"hadn't\",\n\"hadn't've\",\n\"hasn't\",\n\"haven't\",\n\"he'd\",\n\"he'd've\",\n\"he'll\",\n\"he'll've\",\n\"he's\",\n\"how'd\",\n\"how'd'y\",\n\"how'll\",\n\"how's\",\n\"I'd\",\n\"I'd've\",\n\"I'll\",\n\"I'll've\",\n\"I'm\",\n\"I've\",\n\"isn't\",\n\"it'd\",\n\"it'd've\",\n\"it'll\",\n\"it'll've\",\n\"it's\",\n\"let's\",\n\"ma'am\",\n\"mayn't\",\n\"might've\",\n\"mightn't\",\n\"mightn't've\",\n\"must've\",\n\"mustn't\",\n\"mustn't've\",\n\"needn't\",\n\"needn't've\",\n\"o'clock\",\n\"oughtn't\",\n\"oughtn't've\",\n\"shan't\",\n\"sha'n't\",\n\"shan't've\",\n\"she'd\",\n\"she'd've\",\n\"she'll\",\n\"she'll've\",\n\"she's\",\n\"should've\",\n\"shouldn't\",\n\"shouldn't've\",\n\"so've\",\n\"so's\",\n\"that'd\",\n\"that'd've\",\n\"that's\",\n\"there'd\",\n\"there'd've\",\n\"there's\",\n\"they'd\",\n\"they'd've\",\n\"they'll\",\n\"they'll've\",\n\"they're\",\n\"they've\",\n\"to've\",\n\"wasn't\",\n\"we'd\",\n\"we'd've\",\n\"we'll\",\n\"we'll've\",\n\"we're\",\n\"we've\",\n\"weren't\",\n\"what'll\",\n\"what'll've\",\n\"what're\",\n\"what's\",\n\"what've\",\n\"when's\",\n\"when've\",\n\"where'd\",\n\"where's\",\n\"where've\",\n\"who'll\",\n\"who'll've\",\n\"who's\",\n\"who've\",\n\"why's\",\n\"why've\",\n\"will've\",\n\"won't\",\n\"won't've\",\n\"would've\",\n\"wouldn't\",\n\"wouldn't've\",\n\"y'all\",\n\"y'all'd\",\n\"y'all'd've\",\n\"y'all're\",\n\"y'all've\",\n\"you'd\",\n\"you'd've\",\n\"you'll\",\n\"you'll've\",\n\"you're\",\n\"you've\"\n)\n\ncontractions_2 = c( \n\"am not\",\n\"are not\",\n\"cannot\",\n\"cannot have\",\n\"because\",\n\"could have\",\n\"could not\",\n\"could not have\",\n\"did not\",\n\"does not\",\n\"do not\",\n\"had not\",\n\"had not have\",\n\"has not\",\n\"have not\",\n\"he would\",\n\"he would have\",\n\"he will\",\n\"he will have\",\n\"he is\",\n\"how did\",\n\"how do you\",\n\"how will\",\n\"how is\",\n\"I would\",\n\"I would have\",\n\"I will\",\n\"I will have\",\n\"I am\",\n\"I have\",\n\"is not\",\n\"it would\",\n\"it would have\",\n\"it will\",\n\"it will have\",\n\"it is\",\n\"let us\",\n\"madam\",\n\"may not\",\n\"might have\",\n\"might not\",\n\"might not have\",\n\"must have\",\n\"must not\",\n\"must not have\",\n\"need not\",\n\"need not have\",\n\"of the clock\",\n\"ought not\",\n\"ought not have\",\n\"shall not\",\n\"shall not\",\n\"shall not have\",\n\"she would\",\n\"she would have\",\n\"she will\",\n\"she will have\",\n\"she is\",\n\"should have\",\n\"should not\",\n\"should not have\",\n\"so have\",\n\"so is\",\n\"that would\",\n\"that would have\",\n\"that is\",\n\"there would\",\n\"there would have\",\n\"there is\",\n\"they would\",\n\"they would have\",\n\"they will\",\n\"they will have\",\n\"they are\",\n\"they have\",\n\"to have\",\n\"was not\",\n\"we would\",\n\"we would have\",\n\"we will\",\n\"we will have\",\n\"we are\",\n\"we have\",\n\"were not\",\n\"what will\",\n\"what will have\",\n\"what are\",\n\"what is\",\n\"what have\",\n\"when is\",\n\"when have\",\n\"where did\",\n\"where is\",\n\"where have\",\n\"who will\",\n\"who will have\",\n\"who is\",\n\"who have\",\n\"why is\",\n\"why have\",\n\"will have\",\n\"will not\",\n\"will not have\",\n\"would have\",\n\"would not\",\n\"would not have\",\n\"you all\",\n\"you all would\",\n\"you all would have\",\n\"you all are\",\n\"you all have\",\n\"you would\",\n\"you would have\",\n\"you will\",\n\"you will have\",\n\"you are\",\n\"you have\"\n)\n\ncontra <- data.frame(contractions_1, contractions_2)\ncolnames(contra) <- c(\"contraction\", \"expanded\")\n\n#data(contractions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"cleanUp <- function(CORPUS){\n    CORPUS <- tm_map(CORPUS, content_transformer(tolower))\n    CORPUS <- tm_map(CORPUS, content_transformer(function(x){mgsub(x, pattern = emojis, replacement = \"\")}))\n    CORPUS <- tm_map(CORPUS, content_transformer(function(x){replace_contraction(x, contraction = contra)}))\n    CORPUS <- tm_map(CORPUS, content_transformer(removePunctuation), ucp = F)\n    CORPUS <- tm_map(CORPUS, content_transformer(removeNumbers))\n    CORPUS <- tm_map(CORPUS, removeWords, stopwords(\"english\"))\n    CORPUS <- tm_map(CORPUS, stemDocument, language = \"english\")\n    CORPUS <- tm_map(CORPUS, content_transformer(function(x){gsub(\"\\\\S*http+\\\\S*\", \"\", x)}))\n    CORPUS <- tm_map(CORPUS, stripWhitespace)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Example1 <- as.character(corpTRAIN[[16]])\nExample2 <- as.character(corpTRAIN[[39]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Application of the cleanUp function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpTRAIN <- cleanUp(corpTRAIN)\ncorpTEST <- cleanUp(corpTEST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s check the results (printing some tweets):"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"a = paste(\"Before: \", Example1, \" After: \", as.character(corpTRAIN[[16]]), sep = \"\")\nb = paste(\"Before: \", Example2, \" After: \", as.character(corpTRAIN[[39]]), sep = \"\")\ncat(a, b, sep = \"\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For explanation purpose, it makes sense to create a dataset containing the number of each word in each document. This kind of dataset is called a term document matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tdmTR <- TermDocumentMatrix(corpTRAIN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The term document matrix looks like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"inspect(tdmTR[1:10,]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we want to create a word cloud. What a word cloud is, we will find out in a moment. To analyze only words that occur more frequently, we will create a data set from which all words that occur less than 50 times will be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"freq <- findFreqTerms(tdmTR, lowfreq = 50, highfreq = Inf)\nfreq <- as.matrix(tdmTR[freq,])\nfreq <- as.data.frame(freq)\nhead(freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s calculate the sum of each row:"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq <- as.data.frame(rowSums(freq))\ncolnames(freq) <- \"num\"\nfreq$word <- rownames(freq)\nhead(freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"grey\"><b>2. Wordcloud </b></font><br><a id=\"2\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"This is a wordcloud. The thicker and greener the word, the more often it appears. With this method it is relatively easy to find out what a text is about without having to read the text separately. "},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 18, repr.plot.height = 8) \nset.seed(5555)\nfreq$angle <- 45 * sample(-2:2, nrow(freq), replace = TRUE, prob = c(1, 0, 4, 0, 1))\n\nggplot(freq, aes(label = word, \n                    size = num, \n                    color = num, \n                    angle = angle)) +\n  geom_text_wordcloud(shape = \"circle\", \n                      rm_outside = TRUE,\n                      area_corr = F,\n                      rstep = .01,\n                      max_grid_size = 256,\n                      grid_size = 7,\n                      grid_margin = .4\n  ) +\n  scale_size_area(max_size = 12.5) +\n  theme_minimal() +\n  scale_color_gradient(low = \"darkgrey\", high = \"#53d1b1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"grey\"><b>3. Deep Learning </b></font><br><a id=\"3\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"Now we prepare the data for the application with tensorflow with keras. First the training data set is divided into a test and a learning data set by converting the list into a dataframe. We do not use the term document matrix but the corpus, because we want to work with word embeddings. More about word embeddings later."},{"metadata":{},"cell_type":"markdown","source":"Stop words, removement of numbers or punctation, and stemming reduce information, therefore they are not used in the following cleanUp function for text classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanUp <- function(CORPUS){\n    CORPUS <- tm_map(CORPUS, content_transformer(tolower))\n    CORPUS <- tm_map(CORPUS, content_transformer(function(x){mgsub(x, pattern = emojis, replacement = \"\")}))\n    CORPUS <- tm_map(CORPUS, content_transformer(function(x){replace_contraction(x, contraction = contra)}))\n    CORPUS <- tm_map(CORPUS, content_transformer(function(x){gsub(\"\\\\S*http+\\\\S*\", \"\", x)}))\n    CORPUS <- tm_map(CORPUS, stripWhitespace)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpTRAIN <- Corpus(VectorSource(TRAIN$text)) %>% cleanUp()\ncorpTEST <- Corpus(VectorSource(TEST$text)) %>% cleanUp()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text <- unlist(corpTRAIN)\ntextTest <- unlist(corpTEST)\n\ntext <- as.data.frame(text)\ntextTest <- as.data.frame(textTest)\n\ntext <- text[1:(nrow(text)-1), ]\ntext <- as.data.frame(text)\ntail(text)\n\ntextTest <- textTest[1:(nrow(textTest)-1), ]\ntextTest <- as.data.frame(textTest)\ntail(textTest)\n\nval <- data.frame(TRAIN$target, text$text)\ncolnames(val) <- c(\"target\", \"text\")\nhead(val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train test partition:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ttPart <- .7\nset.seed(123)\nu <- runif(n = nrow(val), min = 0, max = 1)\n\ntr_val <- val[u <= ttPart,]\nte_val <- val[u > ttPart,]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"head(tr_val); head(te_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dimensions:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dim(tr_val); dim(te_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"grey\"><b>3.1 Tokenizer </b></font><br><a id=\"3.1\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"A tokenizer is a procedure in which a unique value is assigned to each word. A maximum number of words is defined so as not to have too many categories. It is very important that the tokenizer is used for both training and test data sets. If a separate tokenizer were created for each record, different words would be assigned to different categories.\n\nFirst step is to define the vocabulary size."},{"metadata":{"trusted":true},"cell_type":"code","source":"voc_size <- tr_val$text %>%\n                as.character() %>%\n                    paste(., collapse = \" \") %>%\n                        strsplit(., split = \" \") %>%\n                            unlist() %>%\n                                factor() %>%\n                                    unique() %>%\n                                        length()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cat(\"Vocabulary size is:\", voc_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#GloVe <- fread(\"../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\", quote = \"\")\n#GloVe <- GloVe$V1\n#voc_size <- length(GloVe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer <- text_tokenizer(num_words = voc_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer %>%  fit_text_tokenizer(tr_val$text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer$word_index %>% head(., n=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_seqs_val_tr <- texts_to_sequences(tokenizer, tr_val$text)\ntext_seqs_val_te <- texts_to_sequences(tokenizer, te_val$text)\nhead(text_seqs_val_tr); head(text_seqs_val_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each tweed has a different length. That's why we expand all tweeds with a pad to a unique size. The padding fills the remaining columns with zeros (post-padding). Normally, the maximum size is 140, but due to the clean-ups we reduce the number of words per tweet to 120. The iput shape represents the number of features in the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputShape = 120","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train <- text_seqs_val_tr %>% pad_sequences(maxlen = inputShape, padding = \"post\")\nx_test <- text_seqs_val_te %>% pad_sequences(maxlen = inputShape, padding = \"post\")\ndim(x_train); dim(x_test)\nhead(x_train); head(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train <- tr_val$target\nlength(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now let's see if the train- test split has a comparable data structure at all. But how should this work for natural language? The answer to this is called autoencoder."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"grey\"><b>3.2 Auto Encoder </b></font><br><a id=\"3.2\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"An autoencoder consists of an encoder that first reduces the dimensions. Then the representation is restored by the so-called decoder. This method is used for dimension reduction (like here) or for the creation of artificial representations.\n\n![https://blog.keras.io/img/ae/autoencoder_schema.jpg](https://blog.keras.io/img/ae/autoencoder_schema.jpg)"},{"metadata":{},"cell_type":"markdown","source":"If we manage to visualize the bottleneck, we can simply represent the compressed representations in a visualizable dimensionality and color them according to the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\neps = 45\nlatent_size = 3\nclip = 3\nleRa = .0025\ndec = .00001","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoder and Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"ENCODER_in <- layer_input(shape = c(inputShape))\nENCODER_out = ENCODER_in %>%\n  layer_embedding(input_dim = voc_size, \n                  output_dim = 30) %>%\n  layer_dropout(.2) %>%\n  layer_global_max_pooling_1d() %>%\n  layer_batch_normalization() %>%\n  layer_dense(units=120) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units=60) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units=latent_size) %>% \n  layer_activation_leaky_relu()\nENCODER = keras_model(ENCODER_in, ENCODER_out)\nsummary(ENCODER)\n\nDECODER_in = layer_input(shape = latent_size)\nDECODER_out = DECODER_in %>% \n  layer_batch_normalization() %>%\n  layer_dense(units=60) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units=120) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = inputShape, activation = \"relu\") # Because output must be >= 0\nDECODER = keras_model(DECODER_in, DECODER_out)\nsummary(DECODER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AEN_in = layer_input(shape = inputShape)\nAEN_out = AEN_in %>% \n  ENCODER() %>% \n  DECODER()\n   \nAEN = keras_model(AEN_in, AEN_out)\nsummary(AEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse <- function(y_pred, y_true){\n  y_pred = k_cast(y_pred, dtype=\"float32\")\n  y_true = k_cast(y_true, dtype=\"float32\")\n  rmse = k_sqrt(k_mean(k_square(y_pred - y_true), axis=-1)) \n  return(rmse)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AEN %>% keras::compile(\n  loss = rmse,\n  optimizer = optimizer_adamax(beta_2 = .95, lr = leRa, decay = dec),\n  metrics = c('accuracy')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop <- callback_early_stopping(monitor = 'accuracy', patience = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(123)\n\nhist <- AEN %>%\n  fit(\n    x_train,\n    x_train,\n    batch_size = batch_size,\n    epochs = eps,\n    callbacks = c(stop)\n  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = ENCODER %>% \n    predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded = DECODER %>% \n    predict(encoded) %>% \n        as.data.frame()\n\nhead(decoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encodedTest = ENCODER %>% \n    predict(x_test) %>% \n        as.data.frame()\nencodedTest$split <- \"validation\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded <- encoded %>% as.data.frame()\nencoded$split <- \"train\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded <- rbind(encoded, encodedTest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two-color split shows the training and validation datasets. Both are created from the training dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"col = c(\"#53d1b1\", \"grey\")\nPlot3D <- plot_ly(encoded, \n                  x=~V1, \n                  y=~V2, \n                  z=~V3, \n                  type=\"scatter3d\", \n                  mode=\"markers\", \n                  opacity = 1, \n                  color = ~split,\n                  colors = col,\n                  hoverinfo = 'text',\n                  text = ~paste('V1:', V1, '<br>V2:', V2 ,\"$\", \"<br>V3:\", V3),\n                  marker = list(size=6, \n                                symbol = \"circle\", \n                                line = list(color = 'grey', width = .01))\n                  ) %>% \n\nlayout(title = \"Compressed Representation\")\n\npath <-\"charts/Plot3D.html\"\nsaveWidget(Plot3D, file.path(normalizePath(dirname(path)),basename(path)))\ndisplay_html('<iframe src=\"charts/Plot3D.html\" align=\"center\" width=\"100%\" height=\"600\" frameBorder=\"0\"></iframe>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is noticeable that the train- test split is not comparable in all areas. This is typical for language, because it usually represents very complex structures of sequences. However, the final prediction has a similar problem. Therefore, we leave it as it is.\n\n*By the way, the kink is caused by the leakyRelu activation function.*"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"grey\"><b>3.3 Deep Learning Architecture </b></font><br><a id=\"3.3\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"For NLP purpose, the architecture of neural nets is very specific. In this case, it consists of an initial embedding followed by a classification model."},{"metadata":{},"cell_type":"markdown","source":"Early stopping:"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop1 <- callback_early_stopping(monitor = 'val_loss', patience = 7)\nstop2 <- callback_early_stopping(monitor = 'val_loss', patience = 4)\nstop3 <- callback_early_stopping(monitor = 'val_loss', patience = 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set batch size not too high:"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the input shape. This depends on the method. If we would use a hidden state, we would have to use batch_shape instead of shape. The shape is in this case the number of columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat(\"No of columns =\", inputShape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT1 <- layer_input(shape = c(inputShape))\nINPUT2 <- layer_input(shape = c(inputShape))\nINPUT3 <- layer_input(shape = c(inputShape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of one hot encoding, which often leads to high variance, we use word embeddings. The one-hot matrix is used as input to match the weightings of the hidden layers with the individual categories. Depending on how many hidden layers are used so many features are created by the embedding. An embedding is therefore a preliminary neural network for categorical feature encoding.\n\n*  input_dim is the number of categories to encode\n*  output_dim is the number of hidden layers (features) you want to obtain. The lower the output_dim, the more information is lost. However, this is accompanied by a reduction in complexity, which reduces the predictive variance.\n\nThe first model is a Long Short Term Memory (LSTM) to learn the words in a meaningful sequence. Since language is often very complex and not every sentence unfolds its meaning from left to right, a bidirectional LSTM is used. the following image shows how it works:\n\n![](https://i.stack.imgur.com/aTDpS.png)\n\nThe second model is a Multi Layer Perceptron (MLP) if form of a ResNet:\n\n![](https://pvsmt99345.i.lithium.com/t5/image/serverpage/image-id/42339i8BA3F2CCCEDE7458/image-size/large?v=1.0&px=999)\n\nThe third model is a 1D-CNN. A 1D-CNN rolls over the sentences. It is capable of learning successive structures. It is mainly used in time series analysis.\n\n![](https://cezannec.github.io/assets/cnn_text/conv_1D_time.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTPUT1 <- INPUT1 %>%\n        layer_embedding(input_dim = voc_size, \n                        output_dim = 6) %>%\n        layer_dropout(.3) %>%\n        bidirectional(layer_lstm(units = 16, \n                                 return_sequences = TRUE, \n                                 recurrent_dropout = .4,\n                                 activation = \"selu\",\n                                 kernel_initializer='lecun_normal')) %>%\n        bidirectional(layer_lstm(units = 10, \n                                 return_sequences = FALSE, \n                                 recurrent_dropout = .35,\n                                 activation = \"elu\")) %>%\n\n        layer_dense(units = 1, activation = \"sigmoid\")\n\nOUTPUT2.0 <- INPUT2 %>%\n        layer_embedding(input_dim = voc_size, \n                        output_dim = 10) %>%\n        layer_dropout(.4) %>%\n\n        layer_global_max_pooling_1d() \n\nOUTPUT2.1 <- OUTPUT2.0 %>%\n        layer_dense(units = 16) %>%\n        layer_activation_leaky_relu() %>%\n        layer_dropout(.4) %>%\n        layer_batch_normalization() \n\nOUTPUT2.2 <- OUTPUT2.1 %>%\n        layer_dense(units = 16) %>%\n        layer_activation_leaky_relu() %>%\n        layer_dropout(.4) %>%\n        layer_batch_normalization() \n\nOUTPUT2 <- layer_average(list(OUTPUT2.1, OUTPUT2.2)) %>%\n\n        layer_dense(units = 10) %>%\n        layer_activation_leaky_relu() %>% \n        layer_dropout(.3) %>%\n        layer_batch_normalization() %>%\n\n        layer_dense(units = 1, activation = \"sigmoid\")\n\nOUTPUT3 <- INPUT3 %>%\n        layer_embedding(input_dim = voc_size, \n                        output_dim = 10) %>%\n        layer_dropout(.35) %>%\n\n        layer_conv_1d(filters = 8, \n                      kernel_size = 4, \n                      strides = 1, \n                      padding = \"same\") %>%\n        layer_activation_leaky_relu() %>% \n        layer_batch_normalization() %>%\n        layer_dropout(.35) %>%\n\n        layer_conv_1d(filters = 16, \n                      kernel_size = 3, \n                      strides = 1, \n                      padding = \"same\") %>%\n        layer_activation_leaky_relu() %>% \n        layer_batch_normalization() %>%\n        layer_dropout(.35) %>%\n\n        layer_conv_1d(filters = 24, \n                      kernel_size = 2, \n                      strides = 1, \n                      padding = \"same\") %>%\n        layer_batch_normalization() %>%\n        layer_activation_leaky_relu() %>% \n        layer_dropout(.35) %>%\n\n        layer_flatten() %>%\n\n        layer_dense(units = 1, activation = \"sigmoid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To obtain a trainable model one has to connect the input with the output of the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 <- keras_model(list(INPUT1), OUTPUT1)\nmodel2 <- keras_model(list(INPUT2), OUTPUT2)\nmodel3 <- keras_model(list(INPUT3), OUTPUT3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s take a look at the architecture:"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(model1); summary(model2); summary(model3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient tuning\n\nA detailed explanation of the gradient decent tuning you will find <a href=\"https://www.kaggle.com/frankmollard/gradient-decent-tutorial\"><span style=\"cursor:help\"><font color=\"#53d1b1\">here<font/></span></a>."},{"metadata":{"trusted":true},"cell_type":"code","source":"mom = .9\nclip = 3\nleRa = .0025\ndec = .00001","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compiling the three models, all with adamax optimizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 %>% keras::compile(\n  loss = tf$losses$BinaryCrossentropy(label_smoothing = 0),\n  optimizer = optimizer_adamax(beta_2 = .95, lr = leRa, decay = dec),\n  metrics = c('accuracy')\n)\n\nmodel2 %>% keras::compile(\n  loss = tf$losses$BinaryCrossentropy(label_smoothing = 0),\n  #optimizer_sgd(momentum = mom, nesterov = TRUE, clipvalue = clip, lr = leRa, decay = dec),\n  optimizer = optimizer_adamax(beta_2 = .95, lr = leRa, decay = dec),\n  metrics = c('accuracy')\n)\n\nmodel3 %>% keras::compile(\n  loss = tf$losses$BinaryCrossentropy(label_smoothing = 0),\n  optimizer = optimizer_adam(beta_2 = .95, lr = leRa + .0015, decay = dec),\n  metrics = c('accuracy')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting the number of epochs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"eps = 45","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"grey\"><b>3.4 Cross Validation </b></font><br><a id=\"3.4\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"Model training with early stopping:"},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(123)\n\nhist1 <- model1 %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = eps,\n    validation_split = 0.3,\n    callbacks = c(stop1)\n  )\n\nhist2 <- model2 %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = eps,\n    validation_split = 0.3,\n    callbacks = c(stop2)\n  )\n\nhist3 <- model3 %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = eps,\n    validation_split = 0.3,\n    callbacks = c(stop3)\n  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s check the CV-performance."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"options(repr.plot.width = 16, repr.plot.height = 12)\n\ncols <- c(\"#53d1b1\", \"darkgrey\")\n\n#hist1\nmet <- as.data.frame(unlist(hist1$metrics)) %>% rename(value = `unlist(hist1$metrics)`)\nmet$metrics <- gsub(\"\\\\d+\", \"\", rownames(met))\nmet$epochs <- rep(1:(nrow(met)/4), times = 4)\n\nmet <- met %>% filter(metrics %in% c(\"accuracy\", \"val_accuracy\"))\n\np11 <- ggplot(met, aes(x=epochs, y=value, color=metrics)) +\n  geom_point(size=5, \n             shape=16) + \n  theme_minimal(base_size = 15) + \n  geom_smooth(method=loess, \n              aes(fill=metrics), \n              level = 0.5,\n              formula = y ~ x\n             ) +\n  scale_color_manual(values = cols) + \n  scale_fill_manual(values = cols) + \n  ylab(\"Accuracy\") + \n  ggtitle(\"LSTM\")\n\nmet <- as.data.frame(unlist(hist1$metrics)) %>% rename(value = `unlist(hist1$metrics)`)\nmet$metrics <- gsub(\"\\\\d+\", \"\", rownames(met))\nmet$epochs <- rep(1:(nrow(met)/4), times = 4)\n\nmet <- met %>% filter(metrics %in% c(\"loss\", \"val_loss\"))\n\np12 <- ggplot(met, aes(x=epochs, y=value, color=metrics)) +\n  geom_point(size=5, \n             shape=16) + \n  theme_minimal(base_size = 15) + \n  geom_smooth(method=loess, \n              aes(fill=metrics), \n              level = 0.5,\n              formula = y ~ x\n             ) +\n  scale_color_manual(values = cols) + \n  scale_fill_manual(values = cols) + \n  ylab(\"Loss\") + \n  ggtitle(\"LSTM\")\n\n#hist2\n\noptions(repr.plot.width = 16, repr.plot.height = 12)\n\nmet <- as.data.frame(unlist(hist2$metrics)) %>% rename(value = `unlist(hist2$metrics)`)\nmet$metrics <- gsub(\"\\\\d+\", \"\", rownames(met))\nmet$epochs <- rep(1:(nrow(met)/4), times = 4)\n\nmet <- met %>% filter(metrics %in% c(\"accuracy\", \"val_accuracy\"))\n\np21 <- ggplot(met, aes(x=epochs, y=value, color=metrics)) +\n  geom_point(size=5, \n             shape=16) + \n  theme_minimal(base_size = 15) + \n  geom_smooth(method=loess, \n              aes(fill=metrics), \n              level = 0.5,\n              formula = y ~ x\n             ) +\n  scale_color_manual(values = cols) + \n  scale_fill_manual(values = cols) + \n  ylab(\"Accuracy\") + \n  ggtitle(\"MLP\")\n\nmet <- as.data.frame(unlist(hist2$metrics)) %>% rename(value = `unlist(hist2$metrics)`)\nmet$metrics <- gsub(\"\\\\d+\", \"\", rownames(met))\nmet$epochs <- rep(1:(nrow(met)/4), times = 4)\n\nmet <- met %>% filter(metrics %in% c(\"loss\", \"val_loss\"))\n\np22 <- ggplot(met, aes(x=epochs, y=value, color=metrics)) +\n  geom_point(size=5, \n             shape=16) + \n  theme_minimal(base_size = 15) + \n  geom_smooth(method=loess, \n              aes(fill=metrics), \n              level = 0.5,\n              formula = y ~ x\n             ) +\n  scale_color_manual(values = cols) + \n  scale_fill_manual(values = cols) + \n  ylab(\"Loss\") + \n  ggtitle(\"MLP\")\n\n#hist3\n\noptions(repr.plot.width = 16, repr.plot.height = 12)\n\nmet <- as.data.frame(unlist(hist3$metrics)) %>% rename(value = `unlist(hist3$metrics)`)\nmet$metrics <- gsub(\"\\\\d+\", \"\", rownames(met))\nmet$epochs <- rep(1:(nrow(met)/4), times = 4)\n\nmet <- met %>% filter(metrics %in% c(\"accuracy\", \"val_accuracy\"))\n\np31 <- ggplot(met, aes(x=epochs, y=value, color=metrics)) +\n  geom_point(size=5, \n             shape=16) + \n  theme_minimal(base_size = 15) + \n  geom_smooth(method=loess, \n              aes(fill=metrics), \n              level = 0.5,\n              formula = y ~ x\n             ) +\n  scale_color_manual(values = cols) + \n  scale_fill_manual(values = cols) + \n  ylab(\"Accuracy\") + \n  ggtitle(\"1D-CNN\")\n\nmet <- as.data.frame(unlist(hist3$metrics)) %>% rename(value = `unlist(hist3$metrics)`)\nmet$metrics <- gsub(\"\\\\d+\", \"\", rownames(met))\nmet$epochs <- rep(1:(nrow(met)/4), times = 4)\n\nmet <- met %>% filter(metrics %in% c(\"loss\", \"val_loss\"))\n\np32 <- ggplot(met, aes(x=epochs, y=value, color=metrics)) +\n  geom_point(size=5, \n             shape=16) + \n  theme_minimal(base_size = 15) + \n  geom_smooth(method=loess, \n              aes(fill=metrics), \n              level = 0.5,\n              formula = y ~ x\n             ) +\n  scale_color_manual(values = cols) + \n  scale_fill_manual(values = cols) + \n  ylab(\"Loss\") + \n  ggtitle(\"1D-CNN\")\n\n\ngrid.arrange(p11, p12, p21, p22, p31, p32, nrow = 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s check the results:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"results1 <- model1 %>% evaluate(x_test, te_val$target)\nresults2 <- model2 %>% evaluate(x_test, te_val$target)\nresults3 <- model3 %>% evaluate(x_test, te_val$target)\nresults1; results2; results3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s find the best weight to blend the models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds1 <- model1  %>% predict(x_test)\npreds2 <- model2  %>% predict(x_test)\npreds3 <- model3  %>% predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"H=c(0:10*.1)\nA = rep(NA, times = length(H))\nAA = NULL\n#h=.2\n#b=.1\nfor(h in H) {\n  B=c(0:(10-h*10)*.1)\n  k = 1\n      \n  for(b in B){\n        \n    BagPred <-\n          preds1 * h + \n          preds2 * b + \n          preds3 * (1 - b - h)\n        \n    A[k] <- sum(ifelse(round(BagPred) == te_val$target, 1, 0)) / nrow(te_val)\n      \n    k=k+1\n  }\n\n  A = as.data.frame(A)\n\n  if(length(H)-nrow(A) != 0){\n      NAs = as.data.frame(rep(NA, times = (length(H)-nrow(A))))\n      colnames(NAs) = colnames(A)\n      A = rbind(A, NAs)\n  }\n  AA = rbind(AA, t(A))\n\n  A = rep(NA, times = length(H))\n\n}\n\ncolnames(AA) <- seq(from = 0, to = 1, by = .1)\nrownames(AA) <- seq(from = 0, to = 1, by = .1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Maximum <- max(AA, na.rm = T)\n\nM = NULL\nz = which(AA == Maximum)\nz = z[1]\nrow = ifelse(z%%11 == 0, z %/%11, z %/%11 + 1)\ncol = ifelse(z%%11 == 0, 11, z%%11)\n\ncat(\"Maximum accuracy at\", H[col], \"= LSTM\", H[row], \"= MLP, and\", (1-H[row]-H[col]), \"= 1D-CNN\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"AA <- melt(AA, na.rm = TRUE)\ncolnames(AA) = c(\"LSTM\", \"MLP\", \"accuracy\")\n\np <- ggplot(data = AA, aes(x=MLP, y=LSTM, fill=accuracy)) + \n            geom_tile(color = \"white\") +\n            scale_fill_gradient2(low = \"#FFFFFF\", high = \"#4c6b6f\", mid = \"#53d1b1\", \n            midpoint = .76, limit = c(.70, .80), space = \"Lab\", \n            name=\"accuracy\\nmodel mix\") +\n            theme_minimal()+ \n            theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                             size = 12, hjust = 1),\n                  axis.title = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.background = element_blank()) +\n            coord_fixed()\n\np <- ggplotly(p)\n\npath <-\"charts/Heat.html\"\nsaveWidget(p, file.path(normalizePath(dirname(path)),basename(path)))\nIRdisplay::display_html('<iframe src=\"charts/Heat.html\" align=\"center\" width=\"100%\" height=\"500\" frameBorder=\"0\"></iframe>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"grey\"><b>3.5 Final Model </b></font><br><a id=\"3.4\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"If the early stoping criterion has not worked, then you get a zero. Therefore, a condition is necessary:"},{"metadata":{"trusted":true},"cell_type":"code","source":"eps1 = ifelse(stop1$stopped_epoch > 0, stop1$stopped_epoch, eps)\neps2 = ifelse(stop2$stopped_epoch > 0, stop2$stopped_epoch, eps)\neps3 = ifelse(stop3$stopped_epoch > 0, stop3$stopped_epoch, eps)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"a = paste(\"The cross-validation for model1 stopped at \", eps1, \" epochs. \", sep = \"\")\nb = paste(\"The cross-validation for model2 stopped at \", eps2, \" epochs.\", sep = \"\")\nc = paste(\"The cross-validation for model3 stopped at \", eps3, \" epochs.\", sep = \"\")\n\ncat(a, b, c, sep = \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tokenizer must always refer to either a pre-trained vector such as GloVe or Word2Vec or the training data! \n\nThe eventual training data (val) is bigger than the validation data. So, we redefine the vocabulary size:"},{"metadata":{"trusted":true},"cell_type":"code","source":"voc_size <- val$text %>%\n                as.character() %>%\n                    paste(., collapse = \" \") %>%\n                        strsplit(., split = \" \") %>%\n                            unlist() %>%\n                                factor() %>%\n                                    unique() %>%\n                                        length()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cat(\"Vocabulary size is:\", voc_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer <- text_tokenizer(num_words = voc_size)\ntokenizer %>%  fit_text_tokenizer(val$text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s check the tokenizer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer$word_index %>% head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we convert the text into a sequence again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_tr_seqs <- texts_to_sequences(tokenizer, val$text)\ntext_te_seqs <- texts_to_sequences(tokenizer, textTest$textTest)\n\nhead(text_tr_seqs); head(text_te_seqs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Padding:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train <- text_tr_seqs %>% pad_sequences(maxlen = inputShape, padding = \"post\") \nx_test <- text_te_seqs %>% pad_sequences(maxlen = inputShape, padding = \"post\")\ndim(x_train); dim(x_test)\nhead(x_train); head(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train <- val$target\nlength(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Redefine the architecture due to changing vocabulary size:"},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTPUT1 <- INPUT1 %>%\n        layer_embedding(input_dim = voc_size, \n                        output_dim = 6) %>%\n        layer_dropout(.3) %>%\n        bidirectional(layer_lstm(units = 16, \n                                 return_sequences = TRUE, \n                                 recurrent_dropout = .4,\n                                 activation = \"selu\",\n                                 kernel_initializer='lecun_normal')) %>%\n        bidirectional(layer_lstm(units = 10, \n                                 return_sequences = FALSE, \n                                 recurrent_dropout = .35,\n                                 activation = \"elu\")) %>%\n\n        layer_dense(units = 1, activation = \"sigmoid\")\n\nOUTPUT2.0 <- INPUT2 %>%\n        layer_embedding(input_dim = voc_size, \n                        output_dim = 10) %>%\n        layer_dropout(.4) %>%\n\n        layer_global_max_pooling_1d() \n\nOUTPUT2.1 <- OUTPUT2.0 %>%\n        layer_dense(units = 16) %>%\n        layer_activation_leaky_relu() %>%\n        layer_dropout(.4) %>%\n        layer_batch_normalization() \n\nOUTPUT2.2 <- OUTPUT2.1 %>%\n        layer_dense(units = 16) %>%\n        layer_activation_leaky_relu() %>%\n        layer_dropout(.4) %>%\n        layer_batch_normalization() \n\nOUTPUT2 <- layer_average(list(OUTPUT2.1, OUTPUT2.2)) %>%\n\n        layer_dense(units = 10) %>%\n        layer_activation_leaky_relu() %>% \n        layer_dropout(.3) %>%\n        layer_batch_normalization() %>%\n\n        layer_dense(units = 1, activation = \"sigmoid\")\n\nOUTPUT3 <- INPUT3 %>%\n        layer_embedding(input_dim = voc_size, \n                        output_dim = 10) %>%\n        layer_dropout(.35) %>%\n\n        layer_conv_1d(filters = 10, \n                      kernel_size = 4, \n                      strides = 1, \n                      padding = \"same\") %>%\n        layer_activation_leaky_relu() %>% \n        layer_batch_normalization() %>%\n        layer_dropout(.35) %>%\n\n        layer_conv_1d(filters = 15, \n                      kernel_size = 3, \n                      strides = 1, \n                      padding = \"same\") %>%\n        layer_activation_leaky_relu() %>% \n        layer_batch_normalization() %>%\n        layer_dropout(.35) %>%\n\n        layer_conv_1d(filters = 20, \n                      kernel_size = 2, \n                      strides = 1, \n                      padding = \"same\") %>%\n        layer_batch_normalization() %>%\n        layer_activation_leaky_relu() %>% \n        layer_dropout(.35) %>%\n\n        layer_flatten() %>%\n\n        layer_dense(units = 1, activation = \"sigmoid\")\n\n\nmodel_final1 <- keras_model(list(INPUT1), OUTPUT1)\nmodel_final2 <- keras_model(list(INPUT2), OUTPUT2)\nmodel_final3 <- keras_model(list(INPUT3), OUTPUT3)\n\n\nmodel_final1 %>% compile(\n  loss = tf$losses$BinaryCrossentropy(label_smoothing = 0),\n  optimizer = optimizer_adamax(beta_2 = .95, lr = leRa, decay = dec),\n  metrics = c('accuracy')\n)\n\nmodel_final2 %>% compile(\n  loss = tf$losses$BinaryCrossentropy(label_smoothing = 0),\n  #optimizer_sgd(momentum = mom, nesterov = TRUE, clipvalue = clip, lr = leRa, decay = dec),\n  optimizer = optimizer_adamax(beta_2 = .95, lr = leRa, decay = dec),\n  metrics = c('accuracy')\n)\n\nmodel_final3 %>% compile(\n  loss = tf$losses$BinaryCrossentropy(label_smoothing = 0),\n  optimizer = optimizer_adam(beta_2 = .95, lr = leRa + .0015, decay = dec),\n  metrics = c('accuracy')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train models with appropriate epochs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist1 <- model_final1 %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = eps1\n  )\n\nhist2 <- model_final2 %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = eps2\n  )\n\nhist3 <- model_final3 %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = batch_size,\n    epochs = eps3\n  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds1 <- model_final1  %>% predict(x_test)\npreds2 <- model_final2  %>% predict(x_test)\npreds3 <- model_final3  %>% predict(x_test)\n\nblend <- H[row] * preds1 + H[col] * preds2 + (1-H[row]-H[col]) * preds3\nhead(blend)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"grey\"><b>4. Submission </b></font><br><a id=\"4\"></a>\n<a href=\"#top\" class=\"btn-xs btn-default\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\" data-toggle=\"tooltip\" title=\"Go back to the table of contents\">Go back to the TOP</a>"},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s have a look at the encoded layers again to get an idea of the position of predicted values."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"encodedTest = ENCODER %>% \n    predict(x_test) %>% \n        as.data.frame()\n\nlibrary(dichromat)\n\ncolFun <- colorRampPalette(c(\"white\", \"#53d1b1\"))\ncolGrad <- colFun(100)\n\npredictions <- blend %>% as.data.frame()\nencodedTest$prediction <- predictions$V1\n\nPlot3D <- plot_ly(encodedTest, \n                  x=~V1, \n                  y=~V2, \n                  z=~V3, \n                  type=\"scatter3d\", \n                  mode=\"markers\", \n                  opacity = 1, \n                  color = ~prediction,\n                  colors = colGrad,\n                  hoverinfo = 'text',\n                  text = ~paste('Prediction:', prediction),\n                  marker = list(size=6, \n                                symbol = \"circle\", \n                                line = list(color = 'darkgrey', width = .01),\n                                showscale = FALSE\n                                )\n                  ) %>% \n\nlayout(title = \"Compressed Representation Colored by Prediction\")\n\npath <-\"charts/Plot3D2.html\"\nsaveWidget(Plot3D, file.path(normalizePath(dirname(path)),basename(path)))\ndisplay_html('<iframe src=\"charts/Plot3D2.html\" align=\"center\" width=\"100%\" height=\"600\" frameBorder=\"0\"></iframe>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading in the sample submission file:"},{"metadata":{"trusted":true},"cell_type":"code","source":"SamSub <- fread(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submit:"},{"metadata":{"trusted":true},"cell_type":"code","source":"SamSub$target <- round(blend)\nhead(SamSub, n = 20)\n\nfwrite(SamSub, \"SampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <center>If you like the kernel, give it an upvote:</center>\n<center><a href=\"#top\" class=\"btn btn-default btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:#53d1b1\" data-toggle=\"popover\">Go back to the TOP</a></center>"}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}