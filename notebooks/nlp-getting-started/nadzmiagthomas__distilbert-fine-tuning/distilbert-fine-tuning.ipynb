{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning with DistilBERT\nIn this notebook I will be using the pre-trained model DistilBERT to make predictions on [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview). Most stuff will be done by using HuggingFace libraries, and I will be fine-tuning the model by following [this HuggingFace's tutorial](https://huggingface.co/course/chapter3/1?fw=pt).\n\nBefore we begin make sure the notebook is using GPU otherwise it will take longer to train the model: [How to Enable GPU in Kaggle](https://www.kaggle.com/code/dansbecker/running-kaggle-kernels-with-a-gpu/notebook)","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Load the pre-trained model and its respective tokenizer \nTo use different model, simple change the checkpoint to any pre-trained text classification model available in HuggingFace. It should be noted that some model can't be directly fine-tuned using transformers API. [A list of models can be found here](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads)","metadata":{}},{"cell_type":"code","source":"from transformers import  AutoModelForSequenceClassification, AutoTokenizer\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # Define which pre-trained model we will be using\nclassifier = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) # Get the classifier\ntokenizer = AutoTokenizer.from_pretrained(checkpoint) # Get the tokenizer","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-08T08:19:10.689847Z","iopub.execute_input":"2023-02-08T08:19:10.69037Z","iopub.status.idle":"2023-02-08T08:19:56.310964Z","shell.execute_reply.started":"2023-02-08T08:19:10.690263Z","shell.execute_reply":"2023-02-08T08:19:56.30987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Load the data and preprocess it ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n# Load the training data\ntrain_path = '/kaggle/input/nlp-getting-started/train.csv'\ndf = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:56.313163Z","iopub.execute_input":"2023-02-08T08:19:56.313542Z","iopub.status.idle":"2023-02-08T08:19:56.354888Z","shell.execute_reply.started":"2023-02-08T08:19:56.313503Z","shell.execute_reply":"2023-02-08T08:19:56.353992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:56.356122Z","iopub.execute_input":"2023-02-08T08:19:56.356484Z","iopub.status.idle":"2023-02-08T08:19:57.294984Z","shell.execute_reply.started":"2023-02-08T08:19:56.356447Z","shell.execute_reply":"2023-02-08T08:19:57.294005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the data overall\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:57.29768Z","iopub.execute_input":"2023-02-08T08:19:57.298087Z","iopub.status.idle":"2023-02-08T08:19:57.321577Z","shell.execute_reply.started":"2023-02-08T08:19:57.298045Z","shell.execute_reply":"2023-02-08T08:19:57.320541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To make this simple we will drop id, keyword, location and only keep text and target\ndf = df.loc[:,[\"text\", \"target\"]]","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:57.323117Z","iopub.execute_input":"2023-02-08T08:19:57.323481Z","iopub.status.idle":"2023-02-08T08:19:57.329177Z","shell.execute_reply.started":"2023-02-08T08:19:57.323444Z","shell.execute_reply":"2023-02-08T08:19:57.32793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and evaluation (stratified)\nfrom sklearn.model_selection import train_test_split\ndf_train, df_eval = train_test_split(df, train_size=0.8,stratify=df.target, random_state=42) # Stratified splitting ","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:57.330928Z","iopub.execute_input":"2023-02-08T08:19:57.331277Z","iopub.status.idle":"2023-02-08T08:19:57.718545Z","shell.execute_reply.started":"2023-02-08T08:19:57.331242Z","shell.execute_reply":"2023-02-08T08:19:57.717586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Turn pandas dataframe into dataset\nWe will be using Trainer API from HuggingFace for fine-tuning, and it requires data in the form of Dataset. Therefore, we will convert our Pandas DataFrame into DataSet stored in DatasetDict","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nraw_datasets = DatasetDict({\n    \"train\": Dataset.from_pandas(df_train),\n    \"eval\": Dataset.from_pandas(df_eval)\n})","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:57.719792Z","iopub.execute_input":"2023-02-08T08:19:57.720146Z","iopub.status.idle":"2023-02-08T08:19:58.067895Z","shell.execute_reply.started":"2023-02-08T08:19:57.720113Z","shell.execute_reply":"2023-02-08T08:19:58.066976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the datasets\nprint(\"Dataset Dict:\\n\", raw_datasets)\nprint(\"\\n\\nTrain's features:\\n\", raw_datasets[\"train\"].features)\nprint(\"\\n\\nFirst row of Train:\\n\", raw_datasets[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:58.069273Z","iopub.execute_input":"2023-02-08T08:19:58.069953Z","iopub.status.idle":"2023-02-08T08:19:58.078638Z","shell.execute_reply.started":"2023-02-08T08:19:58.069914Z","shell.execute_reply":"2023-02-08T08:19:58.077431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Tokenizing\nNeural network require the input to be in the form of numbers for training to take place. Therefore, we will convert our text into vector of numbers (tokens) by using tokenizer.","metadata":{}},{"cell_type":"code","source":"# Tokenize the text, and truncate the text if it exceed the tokenizer maximum length. Batched=True to tokenize multiple texts at the same time.\ntokenized_datasets = raw_datasets.map(lambda dataset: tokenizer(dataset['text'], truncation=True), batched=True)\nprint(tokenized_datasets)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:58.080118Z","iopub.execute_input":"2023-02-08T08:19:58.0806Z","iopub.status.idle":"2023-02-08T08:19:58.795481Z","shell.execute_reply.started":"2023-02-08T08:19:58.080559Z","shell.execute_reply":"2023-02-08T08:19:58.794491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the first row\nprint(tokenized_datasets[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:58.798792Z","iopub.execute_input":"2023-02-08T08:19:58.799767Z","iopub.status.idle":"2023-02-08T08:19:58.805266Z","shell.execute_reply.started":"2023-02-08T08:19:58.799726Z","shell.execute_reply":"2023-02-08T08:19:58.804229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to remove text and \\_\\_index_level_0__ as they are not needed for our model fine-tuning. Also we will rename \"target\" to \"labels\", as Trainer API require the target to be named \"labels\"","metadata":{}},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"__index_level_0__\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"target\", \"labels\")\nprint(tokenized_datasets)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:19:58.807035Z","iopub.execute_input":"2023-02-08T08:19:58.80772Z","iopub.status.idle":"2023-02-08T08:19:58.82078Z","shell.execute_reply.started":"2023-02-08T08:19:58.807683Z","shell.execute_reply":"2023-02-08T08:19:58.819663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Training\nWe will be using Trainer API from HuggingFace for training. ","metadata":{}},{"cell_type":"code","source":"!pip -q install evaluate","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-08T08:19:58.822254Z","iopub.execute_input":"2023-02-08T08:19:58.822736Z","iopub.status.idle":"2023-02-08T08:20:10.971235Z","shell.execute_reply.started":"2023-02-08T08:19:58.822692Z","shell.execute_reply":"2023-02-08T08:20:10.97006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\nimport numpy as np\nimport evaluate\n\n# Padding for batch of data that will be fed into model for training\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Training args \ntraining_args = TrainingArguments(\"test-trainer\", num_train_epochs=1, evaluation_strategy=\"epoch\", \n                                  weight_decay=5e-4, save_strategy=\"no\", report_to=\"none\")\n\n# Metric for validation error\ndef compute_metrics(eval_preds):\n    metric = evaluate.load(\"glue\", \"mrpc\") # F1 and Accuracy\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Define trainer\ntrainer = Trainer(\n    classifier,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"eval\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:20:10.975216Z","iopub.execute_input":"2023-02-08T08:20:10.975539Z","iopub.status.idle":"2023-02-08T08:20:21.168594Z","shell.execute_reply.started":"2023-02-08T08:20:10.975505Z","shell.execute_reply":"2023-02-08T08:20:21.167149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start the fine-tuning \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:20:21.169973Z","iopub.execute_input":"2023-02-08T08:20:21.17117Z","iopub.status.idle":"2023-02-08T08:20:58.31619Z","shell.execute_reply.started":"2023-02-08T08:20:21.171126Z","shell.execute_reply":"2023-02-08T08:20:58.315155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The trainer report our Training Loss, Validation Loss, Accuracy and also F1","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Quick evaluation using classification metrics\nWe'll be using [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to do quick evaluation ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Make prediction on evaluation dataset\ny_pred = trainer.predict(tokenized_datasets[\"eval\"]).predictions\ny_pred = np.argmax(y_pred, axis=-1)\n\n# Get the true labels\ny_true = tokenized_datasets[\"eval\"][\"labels\"]\ny_true = np.array(y_true)\n\n# Print the classification report\nprint(classification_report(y_true, y_pred, digits=3))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T08:20:58.318317Z","iopub.execute_input":"2023-02-08T08:20:58.319099Z","iopub.status.idle":"2023-02-08T08:21:02.249065Z","shell.execute_reply.started":"2023-02-08T08:20:58.319049Z","shell.execute_reply":"2023-02-08T08:21:02.247979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Get the test data\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nids = df_test.id # Save ids\ndf_test = df_test.loc[:,[\"text\"]] # Keep only text\n\n# Turn the DataFrame into appropriate format\ntest_dataset = Dataset.from_pandas(df_test)\ntest_dataset = test_dataset.map(lambda dataset: tokenizer(dataset['text'], truncation=True), batched=True)\ntest_dataset = test_dataset.remove_columns('text')\n\n# Get the prediction\npredictions = trainer.predict(test_dataset)\npreds = np.argmax(predictions.predictions, axis=-1)\n\n# Turn submission into DataFrame and save into CSV files\nsubmission = pd.DataFrame({\"id\":ids, \"target\":preds})\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-08T08:21:02.250845Z","iopub.execute_input":"2023-02-08T08:21:02.25157Z","iopub.status.idle":"2023-02-08T08:21:07.344758Z","shell.execute_reply.started":"2023-02-08T08:21:02.251526Z","shell.execute_reply":"2023-02-08T08:21:07.343714Z"},"trusted":true},"execution_count":null,"outputs":[]}]}