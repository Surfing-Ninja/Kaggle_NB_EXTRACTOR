{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Natural Language Processing With Disaster Tweets**","metadata":{}},{"cell_type":"markdown","source":"# **1. Data Overview**","metadata":{}},{"cell_type":"markdown","source":"### **1.1. Meta Data**\n- **id**: A unique identifier for each tweet.\n- **text**: The text content of the tweet.\n- **location**: The location from which the tweet was sent (this field may be blank).\n- **keyword**: A specific keyword found in the tweet (this field may be blank).\n- **target**: This attribute is present only in `train.csv`. It indicates whether a tweet is about a real disaster (1) or not (0).\n\n### **1.2. What am I predicting?**\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a **1**. If not, predict a **0**.","metadata":{}},{"cell_type":"markdown","source":"### **1.3. Importing Libararies**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport emoji\nimport nltk\nimport spacy\nimport scipy.sparse\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import FunctionTransformer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,classification_report\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, OrdinalEncoder, StandardScaler","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:32:52.142979Z","iopub.execute_input":"2024-06-20T15:32:52.143976Z","iopub.status.idle":"2024-06-20T15:33:32.492018Z","shell.execute_reply.started":"2024-06-20T15:32:52.143906Z","shell.execute_reply":"2024-06-20T15:33:32.490894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.4. Reading data and header view**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.493903Z","iopub.execute_input":"2024-06-20T15:33:32.494562Z","iopub.status.idle":"2024-06-20T15:33:32.557348Z","shell.execute_reply.started":"2024-06-20T15:33:32.494531Z","shell.execute_reply":"2024-06-20T15:33:32.556212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.5. Shape of Data**","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.55874Z","iopub.execute_input":"2024-06-20T15:33:32.559189Z","iopub.status.idle":"2024-06-20T15:33:32.565893Z","shell.execute_reply.started":"2024-06-20T15:33:32.559147Z","shell.execute_reply":"2024-06-20T15:33:32.564856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.6. Description of Data**","metadata":{}},{"cell_type":"code","source":"df.describe(include='all').round(2).style.format(precision=2).background_gradient(cmap=\"Reds\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.569167Z","iopub.execute_input":"2024-06-20T15:33:32.570058Z","iopub.status.idle":"2024-06-20T15:33:32.663252Z","shell.execute_reply.started":"2024-06-20T15:33:32.569994Z","shell.execute_reply":"2024-06-20T15:33:32.662118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.7. Info about data**","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.66455Z","iopub.execute_input":"2024-06-20T15:33:32.66489Z","iopub.status.idle":"2024-06-20T15:33:32.682537Z","shell.execute_reply.started":"2024-06-20T15:33:32.664862Z","shell.execute_reply":"2024-06-20T15:33:32.681372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.8. Null values in Data**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.684057Z","iopub.execute_input":"2024-06-20T15:33:32.684477Z","iopub.status.idle":"2024-06-20T15:33:32.698038Z","shell.execute_reply.started":"2024-06-20T15:33:32.684439Z","shell.execute_reply":"2024-06-20T15:33:32.696907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.9. Value Counts of Location**","metadata":{}},{"cell_type":"code","source":"df['location'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.699478Z","iopub.execute_input":"2024-06-20T15:33:32.699993Z","iopub.status.idle":"2024-06-20T15:33:32.712777Z","shell.execute_reply.started":"2024-06-20T15:33:32.699955Z","shell.execute_reply":"2024-06-20T15:33:32.711685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.10. Value Counts of Keyword**","metadata":{}},{"cell_type":"code","source":"df['keyword'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.714187Z","iopub.execute_input":"2024-06-20T15:33:32.714541Z","iopub.status.idle":"2024-06-20T15:33:32.727527Z","shell.execute_reply.started":"2024-06-20T15:33:32.714511Z","shell.execute_reply":"2024-06-20T15:33:32.7264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.11. Checking Duplicates**","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.728867Z","iopub.execute_input":"2024-06-20T15:33:32.729249Z","iopub.status.idle":"2024-06-20T15:33:32.745909Z","shell.execute_reply.started":"2024-06-20T15:33:32.729217Z","shell.execute_reply":"2024-06-20T15:33:32.744807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.12. Checking Data Types**","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.749405Z","iopub.execute_input":"2024-06-20T15:33:32.749749Z","iopub.status.idle":"2024-06-20T15:33:32.757427Z","shell.execute_reply.started":"2024-06-20T15:33:32.74972Z","shell.execute_reply":"2024-06-20T15:33:32.756423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Exploratory Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"### **2.1. Histogram of Numerical Columns**","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=len(df.select_dtypes(include='number').columns), figsize=(20, 5), tight_layout=True)\n\nfor i, col in enumerate(df.select_dtypes(include='number').columns):\n    ax = axes[i]\n    df[col].hist(ax=ax, color='#5DADE2', edgecolor='black', alpha=0.7)\n    ax.set_title(col, fontsize=14, fontweight='bold', color='#2E4053')\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n\nfig.suptitle('Histograms of Numerical Columns', fontsize=20, fontweight='bold', color='#1A5276', y=1.05)\nplt.subplots_adjust(top=0.85)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:32.758786Z","iopub.execute_input":"2024-06-20T15:33:32.75922Z","iopub.status.idle":"2024-06-20T15:33:33.434795Z","shell.execute_reply.started":"2024-06-20T15:33:32.759183Z","shell.execute_reply":"2024-06-20T15:33:33.433701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.2. Boxplot of Numerical Columns**","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.set_context('talk')\n\nplt.figure(figsize=(15, 5))\nax = sns.boxplot(data=df, orient='h', palette='Set2')\n\nplt.title('Box Plots of Numerical Columns', fontsize=20, fontweight='bold', color='#1A5276', pad=20)\nplt.xlabel('Values', fontsize=15, fontweight='bold', color='#2E4053')\nplt.ylabel('Columns', fontsize=15, fontweight='bold', color='#2E4053')\n\nax.tick_params(axis='x', colors='#2E4053', labelsize=12)\nax.tick_params(axis='y', colors='#2E4053', labelsize=12)\n\nfor patch in ax.artists:\n    patch.set_edgecolor('#1A5276')\n    patch.set_linewidth(2)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:33.436235Z","iopub.execute_input":"2024-06-20T15:33:33.436625Z","iopub.status.idle":"2024-06-20T15:33:33.710939Z","shell.execute_reply.started":"2024-06-20T15:33:33.436595Z","shell.execute_reply":"2024-06-20T15:33:33.709882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.3. Heatmap for MCAR(Missing Completely At Random)**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(df.isnull())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:33.712294Z","iopub.execute_input":"2024-06-20T15:33:33.712696Z","iopub.status.idle":"2024-06-20T15:33:34.159033Z","shell.execute_reply.started":"2024-06-20T15:33:33.71266Z","shell.execute_reply":"2024-06-20T15:33:34.157913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"### **3.1. Imputing Null Values**","metadata":{}},{"cell_type":"code","source":"df['keyword'] = df['keyword'].fillna(df['keyword'].mode()[0])\ndf['location'] = df['location'].fillna(value='Missing')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.160308Z","iopub.execute_input":"2024-06-20T15:33:34.160613Z","iopub.status.idle":"2024-06-20T15:33:34.16992Z","shell.execute_reply.started":"2024-06-20T15:33:34.160588Z","shell.execute_reply":"2024-06-20T15:33:34.16882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.171347Z","iopub.execute_input":"2024-06-20T15:33:34.171685Z","iopub.status.idle":"2024-06-20T15:33:34.186607Z","shell.execute_reply.started":"2024-06-20T15:33:34.171658Z","shell.execute_reply":"2024-06-20T15:33:34.185188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3.2. Dropping \"id\" irrelevant feature**","metadata":{}},{"cell_type":"code","source":"df.drop(columns=['id'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.187743Z","iopub.execute_input":"2024-06-20T15:33:34.188125Z","iopub.status.idle":"2024-06-20T15:33:34.197407Z","shell.execute_reply.started":"2024-06-20T15:33:34.188087Z","shell.execute_reply":"2024-06-20T15:33:34.196356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.199207Z","iopub.execute_input":"2024-06-20T15:33:34.199742Z","iopub.status.idle":"2024-06-20T15:33:34.216127Z","shell.execute_reply.started":"2024-06-20T15:33:34.199704Z","shell.execute_reply":"2024-06-20T15:33:34.214833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3.3. Encoding \"Keyword\" and \"Location\" columns**","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.217656Z","iopub.execute_input":"2024-06-20T15:33:34.218026Z","iopub.status.idle":"2024-06-20T15:33:34.226335Z","shell.execute_reply.started":"2024-06-20T15:33:34.217979Z","shell.execute_reply":"2024-06-20T15:33:34.225326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['keyword'] = le.fit_transform(df['keyword'])\ndf['location'] = le.fit_transform(df['location'])","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.227645Z","iopub.execute_input":"2024-06-20T15:33:34.227972Z","iopub.status.idle":"2024-06-20T15:33:34.249059Z","shell.execute_reply.started":"2024-06-20T15:33:34.227938Z","shell.execute_reply":"2024-06-20T15:33:34.24773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.250384Z","iopub.execute_input":"2024-06-20T15:33:34.250714Z","iopub.status.idle":"2024-06-20T15:33:34.265052Z","shell.execute_reply.started":"2024-06-20T15:33:34.250687Z","shell.execute_reply":"2024-06-20T15:33:34.263884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. NLP on text column**","metadata":{}},{"cell_type":"markdown","source":"### **4.1. Lowercasing**","metadata":{}},{"cell_type":"code","source":"df['text'] = df['text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.266365Z","iopub.execute_input":"2024-06-20T15:33:34.26671Z","iopub.status.idle":"2024-06-20T15:33:34.280396Z","shell.execute_reply.started":"2024-06-20T15:33:34.266673Z","shell.execute_reply":"2024-06-20T15:33:34.279059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.281645Z","iopub.execute_input":"2024-06-20T15:33:34.282081Z","iopub.status.idle":"2024-06-20T15:33:34.295366Z","shell.execute_reply.started":"2024-06-20T15:33:34.282044Z","shell.execute_reply":"2024-06-20T15:33:34.294314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.2. Removing html tags**","metadata":{}},{"cell_type":"code","source":"def remove_html_tags(text):\n    clean_text = re.sub('<.*?>', '', text)\n    return clean_text","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.296863Z","iopub.execute_input":"2024-06-20T15:33:34.297387Z","iopub.status.idle":"2024-06-20T15:33:34.304928Z","shell.execute_reply.started":"2024-06-20T15:33:34.297348Z","shell.execute_reply":"2024-06-20T15:33:34.30386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.306134Z","iopub.execute_input":"2024-06-20T15:33:34.306475Z","iopub.status.idle":"2024-06-20T15:33:34.325538Z","shell.execute_reply.started":"2024-06-20T15:33:34.306447Z","shell.execute_reply":"2024-06-20T15:33:34.32441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.327156Z","iopub.execute_input":"2024-06-20T15:33:34.32752Z","iopub.status.idle":"2024-06-20T15:33:34.342434Z","shell.execute_reply.started":"2024-06-20T15:33:34.327483Z","shell.execute_reply":"2024-06-20T15:33:34.341302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.3. Removing URL's**","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    clean_text = re.sub(url_pattern, '', text)\n    return clean_text","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.343818Z","iopub.execute_input":"2024-06-20T15:33:34.345203Z","iopub.status.idle":"2024-06-20T15:33:34.351656Z","shell.execute_reply.started":"2024-06-20T15:33:34.345133Z","shell.execute_reply":"2024-06-20T15:33:34.350637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_urls)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.352847Z","iopub.execute_input":"2024-06-20T15:33:34.35318Z","iopub.status.idle":"2024-06-20T15:33:34.390593Z","shell.execute_reply.started":"2024-06-20T15:33:34.353145Z","shell.execute_reply":"2024-06-20T15:33:34.389447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:33:34.396541Z","iopub.execute_input":"2024-06-20T15:33:34.396914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.4. Removing Punctuations**","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(text):\n    punctuation = string.punctuation\n    clean_text = text.translate(str.maketrans('', '', punctuation))\n    return clean_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_punctuation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.5. Chat Word Treatment**","metadata":{}},{"cell_type":"code","source":"chat_words_mapping = {\n    \"lol\": \"laughing out loud\",\n    \"brb\": \"be right back\",\n    \"btw\": \"by the way\",\n    \"afk\": \"away from keyboard\",\n    \"rofl\": \"rolling on the floor laughing\",\n    \"ttyl\": \"talk to you later\",\n    \"np\": \"no problem\",\n    \"thx\": \"thanks\",\n    \"omg\": \"oh my god\",\n    \"idk\": \"I don't know\",\n    \"np\": \"no problem\",\n    \"gg\": \"good game\",\n    \"g2g\": \"got to go\",\n    \"b4\": \"before\",\n    \"cu\": \"see you\",\n    \"yw\": \"you're welcome\",\n    \"wtf\": \"what the f*ck\",\n    \"imho\": \"in my humble opinion\",\n    \"jk\": \"just kidding\",\n    \"gf\": \"girlfriend\",\n    \"bf\": \"boyfriend\",\n    \"u\": \"you\",\n    \"r\": \"are\",\n    \"2\": \"to\",\n    \"4\": \"for\",\n    \"b\": \"be\",\n    \"c\": \"see\",\n    \"y\": \"why\",\n    \"tho\": \"though\",\n    \"smh\": \"shaking my head\",\n    \"lolz\": \"laughing out loud\",\n    \"h8\": \"hate\",\n    \"luv\": \"love\",\n    \"pls\": \"please\",\n    \"sry\": \"sorry\",\n    \"tbh\": \"to be honest\",\n    \"omw\": \"on my way\",\n    \"omw2syg\": \"on my way to see your girlfriend\",\n}\n\ndef expand_chat_words(text):\n    words = text.split()\n    expanded_words = [chat_words_mapping.get(word.lower(), word) for word in words]\n    return ' '.join(expanded_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(expand_chat_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.6. Removing Stop Words**","metadata":{}},{"cell_type":"code","source":"def remove_stop_words(text):\n\ttokens = nltk.word_tokenize(text)\n\tstop_words = set(stopwords.words('english'))\n\tfiltered_tokens = [token for token in tokens if token not in stop_words]\n\tpreprocessed_text = ' '.join(filtered_tokens)\n\treturn preprocessed_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.7. Replacing emojis with meanings**","metadata":{}},{"cell_type":"code","source":"def replace_emojis_with_meanings(text):\n    def replace(match):\n        emoji_char = match.group()\n        emoji_meaning = emoji.demojize(emoji_char)\n        return emoji_meaning\n\n    emoji_pattern = re.compile(\"[\"\n                            u\"\\U0001F600-\\U0001F64F\"\n                            u\"\\U0001F300-\\U0001F5FF\"\n                            u\"\\U0001F680-\\U0001F6FF\"\n                            u\"\\U0001F1E0-\\U0001F1FF\"\n                            u\"\\U00002500-\\U00002BEF\"\n                            u\"\\U00002702-\\U000027B0\"\n                            u\"\\U00002702-\\U000027B0\"\n                            u\"\\U000024C2-\\U0001F251\"\n                            u\"\\U0001f926-\\U0001f937\"\n                            u\"\\U00010000-\\U0010ffff\"\n                            u\"\\u2640-\\u2642\"\n                            u\"\\u2600-\\u2B55\"\n                            u\"\\u200d\"\n                            u\"\\u23cf\"\n                            u\"\\u23e9\"\n                            u\"\\u231a\"\n                            u\"\\ufe0f\"\n                            u\"\\u3030\"\n                            \"]+\", flags=re.UNICODE)\n    text_with_meanings = emoji_pattern.sub(replace, text)\n    return text_with_meanings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(replace_emojis_with_meanings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.8. Word Tokenization**","metadata":{}},{"cell_type":"code","source":"def word_tokenization(text):\n    return nltk.word_tokenize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['token_text'] = df['text'].apply(word_tokenization)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['text'].head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.9. POS Tagging**","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])\n\ndef batch_pos_tagging(texts):\n    docs = list(nlp.pipe(texts, batch_size=50))\n    return [[(token.text, token.pos_) for token in doc] for doc in docs]\n\nbatch_size = 50\nnum_batches = (len(df) + batch_size - 1) // batch_size\n\npos_tags = []\nfor i in tqdm(range(num_batches)):\n    start = i * batch_size\n    end = min((i + 1) * batch_size, len(df))\n    batch_texts = df['text'][start:end].tolist()\n    pos_tags.extend(batch_pos_tagging(batch_texts))\n\ndf['POS_Tags'] = pos_tags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['token_text'] = df['token_text'].apply(lambda x: ' '.join(x))\ndf['POS_Tags'] = df['POS_Tags'].apply(lambda x: ' '.join([i[1] for i in x]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Modeling**","metadata":{}},{"cell_type":"markdown","source":"### **5.1. Transformation**","metadata":{}},{"cell_type":"code","source":"class TextLengthExtractor(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return pd.DataFrame(X['text'].apply(len))\n\nclass NumHashtagsExtractor(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return pd.DataFrame(X['text'].apply(lambda x: len([word for word in x.split() if word.startswith('#')])))\n\nclass NumMentionsExtractor(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return pd.DataFrame(X['text'].apply(lambda x: len([word for word in x.split() if word.startswith('@')])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **5.2. Train Test Split**","metadata":{}},{"cell_type":"code","source":"X = df[['text']]\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **5.3. Creating Pipeline for model**","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('text', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x['text'], validate=False)),\n            ('tfidf', TfidfVectorizer(max_features=10000))\n        ])),\n        ('text_length', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x, validate=False)),\n            ('extract', TextLengthExtractor())\n        ])),\n        ('num_hashtags', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x, validate=False)),\n            ('extract', NumHashtagsExtractor())\n        ])),\n        ('num_mentions', Pipeline([\n            ('selector', FunctionTransformer(lambda x: x, validate=False)),\n            ('extract', NumMentionsExtractor())\n        ]))\n    ])),\n    ('clf', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    'features__text__tfidf__max_features': [5000, 10000],\n    'clf__n_estimators': [100, 200],\n    'clf__learning_rate': [0.1, 0.01]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **5.4. Applying Model**","metadata":{}},{"cell_type":"code","source":"grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=1)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **5.5. Getting Predictions on Test Data**","metadata":{}},{"cell_type":"code","source":"best_model = grid_search.best_estimator_\ny_pred_best = best_model.predict(X_test)\nprint(classification_report(y_test, y_pred_best))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6. Submission**","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X = test_df[['text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_submission = best_model.predict(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': test_df['id'], 'target': y_pred_submission})\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Thank You So Much**","metadata":{}}]}