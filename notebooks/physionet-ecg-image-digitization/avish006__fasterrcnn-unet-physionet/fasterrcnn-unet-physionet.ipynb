{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":97984,"databundleVersionId":14096757,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ECG Digitization - Fixed Code with 70/30 Split\n# Corrected version addressing: network error, proper train/val split, Lead II duration handling\n\n\"\"\"\nWHAT THIS CODE DOES:\n====================\n1. Uses Faster R-CNN (ResNet101 backbone) for lead detection\n2. Uses U-Net for ECG signal segmentation\n3. Extracts time series from 12-lead ECG images\n4. Handles Lead II (10 seconds) differently from other leads (2.5 seconds)\n5. Creates submission in Parquet and CSV format\n\n\"\"\"\n\nimport os, math, json, time, random, gc\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\nfrom skimage.morphology import skeletonize\nfrom scipy.signal import butter, filtfilt\nimport scipy.interpolate as interp\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom sklearn.model_selection import train_test_split  # ADDED for proper split\n\n# ============================================================================\n# REPRODUCIBILITY\n# ============================================================================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# ============================================================================\n# BUILD FASTER R-CNN\n# ============================================================================\ndef build_faster_rcnn(num_classes: int = 2, pretrained_backbone: bool = False):\n    \"\"\"\n    FIXED: Changed pretrained=False to avoid network download\n    If you have pretrained weights locally, load them manually after initialization\n    \"\"\"\n    try:\n        backbone = resnet_fpn_backbone('resnet101', pretrained=False)  # FIXED: False to avoid download\n        model = FasterRCNN(backbone, num_classes=num_classes)\n        return model\n    except Exception as e:\n        print(f\"Warning: Could not build FasterRCNN with pretrained backbone: {e}\")\n        print(\"Building without pretrained weights...\")\n        backbone = resnet_fpn_backbone('resnet101', pretrained=False)\n        model = FasterRCNN(backbone, num_classes=num_classes)\n        return model\n\n# ============================================================================\n# PATHS\n# ============================================================================\nKAGGLE_BASE = Path('/kaggle/input/physionet-ecg-image-digitization')\nTRAIN_CSV = KAGGLE_BASE / 'train.csv'\nTEST_CSV = KAGGLE_BASE / 'test.csv'\nTRAIN_ROOT = KAGGLE_BASE / 'train'\nTEST_IMG_ROOT = KAGGLE_BASE / 'test'\nSAMPLE_SUB = KAGGLE_BASE / 'sample_submission.parquet'\n\nOUT_DIR = Path('/kaggle/working/')\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Torch {torch.__version__} | Torchvision {torchvision.__version__}\")\nprint(\"CUDA available:\", torch.cuda.is_available())\n\n# ============================================================================\n# GLOBAL CONSTANTS\n# ============================================================================\nMM_PER_MV = 10.0  # 10 mm per mV\nMM_PER_S = 25.0   # 25 mm per second\nDEFAULT_DPI = 200\nEPS = 1e-8\n\nLEADS = [\"I\",\"II\",\"III\",\"aVR\",\"aVL\",\"aVF\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]\n\ndef expected_rows_for_lead(fs: int, lead: str):\n    \"\"\"\n    CRITICAL FIX: Lead II is recorded for 10 seconds (rhythm strip)\n    All other leads are recorded for 2.5 seconds\n    \"\"\"\n    if lead == 'II':\n        duration_s = 10.0  # Lead II rhythm strip\n    else:\n        duration_s = 2.5   # Standard leads\n    return int(math.floor(fs * duration_s))\n\ndef ensure_dir(p):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\n# ============================================================================\n# IMAGE PREPROCESSING\n# ============================================================================\n\ndef correct_rotation_cv(img):\n    \"\"\"Hough-based rotation correction\"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n    lines = cv2.HoughLines(edges, 1, np.pi/180, 200)\n    if lines is None:\n        return img\n    angles = []\n    for line in lines[:300]:\n        rho, theta = line[0]\n        angle = theta - np.pi/2\n        angles.append(np.degrees(angle))\n    median_angle = np.median(angles)\n    h, w = img.shape[:2]\n    M = cv2.getRotationMatrix2D((w/2, h/2), median_angle, 1.0)\n    rotated = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, \n                            borderMode=cv2.BORDER_REPLICATE)\n    return rotated\n\ndef apply_clahe(img):\n    \"\"\"Contrast enhancement\"\"\"\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    merged = cv2.merge((cl, a, b))\n    return cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n\ndef adaptive_binarize(gray):\n    \"\"\"Adaptive thresholding\"\"\"\n    return cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n                                cv2.THRESH_BINARY_INV, 11, 2)\n\ndef clean_binary(binary, k_small=3, k_large=5):\n    \"\"\"Morphological cleanup\"\"\"\n    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k_small, k_small))\n    out = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, k)\n    out = cv2.morphologyEx(out, cv2.MORPH_OPEN, k)\n    out = cv2.dilate(out, k, iterations=1)\n    return out\n\n# ============================================================================\n# GRID & CALIBRATION\n# ============================================================================\n\ndef estimate_pixels_per_mm(image):\n    \"\"\"Estimate grid spacing\"\"\"\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    lower1 = np.array([0, 40, 40])\n    upper1 = np.array([10, 255, 255])\n    lower2 = np.array([160, 40, 40])\n    upper2 = np.array([179, 255, 255])\n    \n    mask1 = cv2.inRange(hsv, lower1, upper1)\n    mask2 = cv2.inRange(hsv, lower2, upper2)\n    mask = cv2.bitwise_or(mask1, mask2)\n    \n    edges = cv2.Canny(mask, 50, 150)\n    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=40, \n                           minLineLength=40, maxLineGap=10)\n    \n    if lines is None:\n        return DEFAULT_DPI / 25.4\n    \n    xs = []\n    ys = []\n    for x1, y1, x2, y2 in lines.reshape(-1, 4):\n        if abs(x1 - x2) < 4:\n            xs.append(x1)\n        if abs(y1 - y2) < 4:\n            ys.append(y1)\n    \n    spacing = None\n    if len(xs) >= 2:\n        spacing = np.median(np.diff(np.sort(xs)))\n    elif len(ys) >= 2:\n        spacing = np.median(np.diff(np.sort(ys)))\n    \n    if spacing is None or spacing <= 0:\n        return DEFAULT_DPI / 25.4\n    \n    return float(spacing)\n\ndef pixels_to_mV(y_pixels, baseline_y, pixels_per_mm, mm_per_mV=MM_PER_MV):\n    \"\"\"Convert pixel positions to millivolts\"\"\"\n    return (baseline_y - y_pixels) / (pixels_per_mm * mm_per_mV)\n\n# ============================================================================\n# SIGNAL EXTRACTION\n# ============================================================================\n\ndef extract_series_from_mask(binary_mask, use_skeleton=True):\n    \"\"\"Extract 1D signal from binary mask\"\"\"\n    bin_bool = (binary_mask > 0).astype(np.uint8)\n    if use_skeleton:\n        sk = skeletonize(bin_bool > 0).astype(np.uint8)\n        arr = sk\n    else:\n        arr = bin_bool\n    \n    h, w = arr.shape\n    series = np.full(w, np.nan, dtype=float)\n    \n    for x in range(w):\n        ys = np.where(arr[:, x] > 0)[0]\n        if ys.size == 0:\n            continue\n        series[x] = np.median(ys)\n    \n    # Interpolate NaNs\n    nans = np.isnan(series)\n    if nans.any():\n        xi = np.arange(w)\n        series[nans] = np.interp(xi[nans], xi[~nans], series[~nans])\n    \n    return series\n\ndef resample_signal(sig, target_len):\n    \"\"\"Resample signal to target length\"\"\"\n    x_old = np.linspace(0, 1, len(sig))\n    x_new = np.linspace(0, 1, target_len)\n    f = interp.interp1d(x_old, sig, kind='linear', fill_value='extrapolate')\n    return f(x_new)\n\n# ============================================================================\n# FILTERING\n# ============================================================================\n\ndef bandpass_filter(sig, fs, lowcut=0.05, highcut=150.0, order=3):\n    \"\"\"Bandpass filter for ECG\"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    if high >= 1:\n        high = 0.999\n    b, a = butter(order, [low, high], btype='band')\n    return filtfilt(b, a, sig)\n\ndef detrend_signal(sig):\n    \"\"\"Remove mean\"\"\"\n    return sig - np.mean(sig)\n\n# ============================================================================\n# U-NET MODEL\n# ============================================================================\n\nclass UNet(nn.Module):\n    def __init__(self, in_ch=3, out_ch=1, f=32):\n        super().__init__()\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(in_ch, f, 3, 1, 1), nn.ReLU(),\n            nn.Conv2d(f, f, 3, 1, 1), nn.ReLU()\n        )\n        self.enc2 = nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Conv2d(f, f*2, 3, 1, 1), nn.ReLU(),\n            nn.Conv2d(f*2, f*2, 3, 1, 1), nn.ReLU()\n        )\n        self.enc3 = nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Conv2d(f*2, f*4, 3, 1, 1), nn.ReLU(),\n            nn.Conv2d(f*4, f*4, 3, 1, 1), nn.ReLU()\n        )\n        self.bottleneck = nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Conv2d(f*4, f*8, 3, 1, 1), nn.ReLU(),\n            nn.Conv2d(f*8, f*8, 3, 1, 1), nn.ReLU()\n        )\n        self.up3 = nn.ConvTranspose2d(f*8, f*4, 2, stride=2)\n        self.dec3 = nn.Sequential(\n            nn.Conv2d(f*8, f*4, 3, 1, 1), nn.ReLU(),\n            nn.Conv2d(f*4, f*4, 3, 1, 1), nn.ReLU()\n        )\n        self.up2 = nn.ConvTranspose2d(f*4, f*2, 2, stride=2)\n        self.dec2 = nn.Sequential(\n            nn.Conv2d(f*4, f*2, 3, 1, 1), nn.ReLU(),\n            nn.Conv2d(f*2, f*2, 3, 1, 1), nn.ReLU()\n        )\n        self.up1 = nn.ConvTranspose2d(f*2, f, 2, stride=2)\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(f*2, f, 3, 1, 1), nn.ReLU(),\n            nn.Conv2d(f, f, 3, 1, 1), nn.ReLU()\n        )\n        self.outc = nn.Conv2d(f, out_ch, 1)\n    \n    def forward(self, x):\n        c1 = self.enc1(x)\n        c2 = self.enc2(c1)\n        c3 = self.enc3(c2)\n        b = self.bottleneck(c3)\n        u3 = self.up3(b)\n        d3 = self.dec3(torch.cat([u3, c3], dim=1))\n        u2 = self.up2(d3)\n        d2 = self.dec2(torch.cat([u2, c2], dim=1))\n        u1 = self.up1(d2)\n        d1 = self.dec1(torch.cat([u1, c1], dim=1))\n        return self.outc(d1)\n\n# ============================================================================\n# DATASET - PROPER 70/30 SPLIT\n# ============================================================================\n\nclass SimpleSegmentationDataset(Dataset):\n    def __init__(self, image_paths, mask_paths=None, crop_size=512):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.crop_size = crop_size\n        self.transforms = transforms.ToTensor()\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert('RGB')\n        img = img.resize((self.crop_size, self.crop_size))\n        img_t = self.transforms(np.array(img))\n        \n        if self.mask_paths:\n            m = Image.open(self.mask_paths[idx]).convert('L')\n            m = m.resize((self.crop_size, self.crop_size))\n            mask = (np.array(m) > 127).astype(np.float32)\n            mask_t = torch.from_numpy(mask).unsqueeze(0)\n        else:\n            mask_t = torch.zeros((1, self.crop_size, self.crop_size), dtype=torch.float32)\n        \n        return img_t, mask_t\n\n# ============================================================================\n# TRAIN/VAL SPLIT - PROPER 70/30\n# ============================================================================\n\ndef create_train_val_split(train_csv_path, train_root, train_ratio=0.7, random_state=42):\n    \"\"\"\n    FIXED: Proper 70/30 split instead of just top 50 images\n    Returns train and validation image paths\n    \"\"\"\n    df = pd.read_csv(train_csv_path)\n    \n    # Get all available ECG IDs\n    all_ids = df['id'].unique()\n    print(f\"Total ECG IDs in dataset: {len(all_ids)}\")\n    \n    # Split IDs into train and validation\n    train_ids, val_ids = train_test_split(\n        all_ids, \n        train_size=train_ratio, \n        random_state=random_state\n    )\n    \n    print(f\"Train IDs: {len(train_ids)} ({len(train_ids)/len(all_ids)*100:.1f}%)\")\n    print(f\"Val IDs: {len(val_ids)} ({len(val_ids)/len(all_ids)*100:.1f}%)\")\n    \n    # Collect image paths for train and val\n    train_images = []\n    val_images = []\n    \n    for ecg_id in train_ids:\n        ecg_folder = Path(train_root) / str(ecg_id)\n        if ecg_folder.exists():\n            imgs = list(ecg_folder.glob(f\"{ecg_id}-*.png\"))\n            if len(imgs) > 0:\n                train_images.append(str(imgs[0]))  # Use first image variant\n    \n    for ecg_id in val_ids:\n        ecg_folder = Path(train_root) / str(ecg_id)\n        if ecg_folder.exists():\n            imgs = list(ecg_folder.glob(f\"{ecg_id}-*.png\"))\n            if len(imgs) > 0:\n                val_images.append(str(imgs[0]))  # Use first image variant\n    \n    print(f\"Train images collected: {len(train_images)}\")\n    print(f\"Val images collected: {len(val_images)}\")\n    \n    return train_images, val_images, train_ids, val_ids\n\n# ============================================================================\n# TRAINING FUNCTION\n# ============================================================================\n\ndef train_segmenter(seg_model, train_loader, val_loader, epochs=5, lr=1e-4):\n    \"\"\"Train U-Net segmentation model\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    seg_model.to(device)\n    \n    optimizer = torch.optim.AdamW(seg_model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        # Training\n        seg_model.train()\n        train_loss = 0.0\n        for imgs, masks in train_loader:\n            imgs = imgs.to(device)\n            masks = masks.to(device)\n            \n            preds = seg_model(imgs)\n            loss = criterion(preds, masks)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * imgs.size(0)\n        \n        train_loss /= len(train_loader.dataset)\n        \n        # Validation\n        seg_model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for imgs, masks in val_loader:\n                imgs = imgs.to(device)\n                masks = masks.to(device)\n                \n                preds = seg_model(imgs)\n                loss = criterion(preds, masks)\n                \n                val_loss += loss.item() * imgs.size(0)\n        \n        val_loss /= len(val_loader.dataset)\n        \n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(seg_model.state_dict(), OUT_DIR / 'unet_best.pth')\n            print(f\"  â†’ Saved best model (val_loss={val_loss:.5f})\")\n    \n    return seg_model\n\n# ============================================================================\n# INFERENCE HELPERS\n# ============================================================================\n\ndef template_12lead_rois(image):\n    \"\"\"Template-based ROI detection for 12 leads\"\"\"\n    h, w = image.shape[:2]\n    row_h = int(h * 0.22)\n    rois = []\n    \n    # 3 rows Ã— 4 columns\n    for r in range(3):\n        y0 = int(r * row_h + h * 0.03)\n        for c in range(4):\n            x0 = int(c * (w/4) + w * 0.02)\n            x1 = int((c+1) * (w/4) - w * 0.02)\n            y1 = int(y0 + row_h - h * 0.03)\n            rois.append((x0, y0, x1, y1))\n    \n    # Rhythm strip (Lead II, bottom)\n    ry0 = int(h * 0.75)\n    rois.append((int(w * 0.02), ry0, int(w * 0.98), int(h * 0.96)))\n    \n    return rois\n\ndef segment_crop(segmenter, crop):\n    \"\"\"Segment a lead crop using U-Net\"\"\"\n    device = next(segmenter.parameters()).device\n    img = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n    t = transforms.ToTensor()(img).unsqueeze(0).to(device)\n    \n    segmenter.eval()\n    with torch.no_grad():\n        out = segmenter(t)\n    \n    out_np = out.squeeze().cpu().numpy()\n    mask = (torch.sigmoid(torch.tensor(out_np)).numpy() > 0.5).astype(np.uint8) * 255\n    mask = clean_binary(mask)\n    return mask\n\ndef extract_ecg_from_image(segmenter, image_path, fs=None, debug=False):\n    \"\"\"\n    Extract 12-lead ECG from image\n    FIXED: Properly handles Lead II (10s) vs other leads (2.5s)\n    \"\"\"\n    img = cv2.imread(str(image_path))\n    if img is None:\n        raise FileNotFoundError(f\"{image_path} not found\")\n    \n    img = correct_rotation_cv(img)\n    img = apply_clahe(img)\n    \n    boxes = template_12lead_rois(img)\n    pxmm = estimate_pixels_per_mm(img)\n    \n    results = []\n    \n    for i, (x1, y1, x2, y2) in enumerate(boxes[:12]):\n        crop = img[y1:y2, x1:x2]\n        \n        # Segmentation\n        if segmenter is not None:\n            try:\n                mask = segment_crop(segmenter, crop)\n            except:\n                gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n                mask = adaptive_binarize(gray)\n                mask = clean_binary(mask)\n        else:\n            gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n            mask = adaptive_binarize(gray)\n            mask = clean_binary(mask)\n        \n        # Extract pixel positions\n        series_pix = extract_series_from_mask(mask, use_skeleton=True)\n        baseline_y = crop.shape[0] // 2\n        series_mV = pixels_to_mV(series_pix, baseline_y, pxmm)\n        if fs is not None:\n            lead_name = LEADS[i]\n            target_rows = expected_rows_for_lead(fs, lead_name)  # Handles Lead II correctly\n        else:\n            target_rows = len(series_mV)\n        \n        # Resample\n        if len(series_mV) <= 2:\n            res = np.zeros(target_rows, dtype=float)\n        else:\n            res = resample_signal(series_mV, target_rows)\n        \n        # Post-process\n        if fs is not None:\n            try:\n                res = detrend_signal(res)\n                res = bandpass_filter(res, fs, lowcut=0.05, highcut=min(150, fs/2 - 1))\n            except:\n                pass\n        \n        results.append(res)\n        \n        if debug:\n            print(f\"Lead {i+1} ({LEADS[i]}): crop {crop.shape}, pxmm={pxmm:.2f}, \"\n                  f\"orig_len={len(series_mV)}, target={target_rows}\")\n    \n    return results\n\n# ============================================================================\n# SUBMISSION CREATION\n# ============================================================================\n\ndef create_submission(segmenter, test_csv_path, test_images_dir, out_parquet_path):\n    \"\"\"\n    Create submission files in both CSV and Parquet formats\n    \n    Args:\n        segmenter: Trained U-Net model\n        test_csv_path: Path to test.csv\n        test_images_dir: Directory containing test images\n        out_parquet_path: Output path for parquet file\n    \n    Returns:\n        None (saves both submission.csv and submission.parquet)\n    \"\"\"\n    df = pd.read_csv(test_csv_path)\n    long_rows = []\n    \n    print(f\"Processing {len(df)} test records...\")\n    \n    for idx, row in df.iterrows():\n        base_id = row['id']\n        fs = int(row['fs'])\n        \n        img_path = Path(test_images_dir) / f\"{base_id}.png\"\n        if not img_path.exists():\n            print(f\"âš ï¸ Missing image: {img_path}\")\n            continue\n        \n        # Extract ECG signals from image\n        preds = extract_ecg_from_image(segmenter, str(img_path), fs=fs)\n        \n        for lead_idx, lead_name in enumerate(LEADS):\n            sig = preds[lead_idx]\n            \n            # Get expected number of samples for this lead\n            num_rows = expected_rows_for_lead(fs, lead_name)\n            \n            # Resample if needed\n            if len(sig) != num_rows:\n                sig = resample_signal(sig, num_rows)\n            \n            # Create long-format rows\n            for row_id, val in enumerate(sig):\n                long_rows.append({\n                    'id': f\"{base_id}_{row_id}_{lead_name}\",\n                    'value': float(val)\n                })\n    \n    # Create DataFrame\n    out_df = pd.DataFrame(long_rows)\n    \n    # ============================================================================\n    # SAVE BOTH CSV AND PARQUET\n    # ============================================================================\n    \n    # Save as CSV\n    out_csv_path = str(out_parquet_path).replace('.parquet', '.csv')\n    out_df.to_csv(out_csv_path, index=False)\n    print(f\"âœ“ CSV saved to: {out_csv_path}\")\n    \n    # Save as Parquet\n    table = pa.Table.from_pandas(out_df)\n    pq.write_table(table, str(out_parquet_path))\n    print(f\"âœ“ Parquet saved to: {out_parquet_path}\")\n    \n    # Print summary\n    print(f\"\\nðŸ“Š Submission Summary:\")\n    print(f\"  Total rows: {len(out_df):,}\")\n    print(f\"  Unique ECG IDs: {out_df['id'].str.split('_').str[0].nunique()}\")\n    print(f\"  Sample row: {out_df.iloc[0].to_dict()}\")\n    \n    # Verify file sizes\n    csv_size = Path(out_csv_path).stat().st_size / 1024  # KB\n    parquet_size = Path(out_parquet_path).stat().st_size / 1024  # KB\n    print(f\"\\nðŸ“ File Sizes:\")\n    print(f\"  CSV: {csv_size:.2f} KB\")\n    print(f\"  Parquet: {parquet_size:.2f} KB\")\n    print(f\"  Compression ratio: {csv_size/parquet_size:.2f}x\")\n    print(f\"Submission saved to: {out_parquet_path}\")\n    print(f\"Total rows: {len(out_df)}\")\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef main():\n    print(\"=\" * 80)\n    print(\"ECG DIGITIZATION - FIXED VERSION WITH 70/30 SPLIT\")\n    print(\"=\" * 80)\n    \n    # 1. Create proper 70/30 split\n    print(\"\\n1. Creating 70/30 train/validation split...\")\n    train_images, val_images, train_ids, val_ids = create_train_val_split(\n        TRAIN_CSV, TRAIN_ROOT, train_ratio=0.7, random_state=SEED\n    )\n    \n    # 2. Create datasets and loaders\n    print(\"\\n2. Creating datasets and data loaders...\")\n    train_dataset = SimpleSegmentationDataset(train_images, crop_size=512)\n    val_dataset = SimpleSegmentationDataset(val_images, crop_size=512)\n    \n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n    \n    print(f\"Train loader: {len(train_loader)} batches\")\n    print(f\"Val loader: {len(val_loader)} batches\")\n    \n    # 3. Initialize models\n    print(\"\\n3. Initializing models...\")\n    detector = build_faster_rcnn(num_classes=2, pretrained_backbone=False)\n    segmenter = UNet(in_ch=3, out_ch=1, f=32)\n    \n    print(\"Models initialized successfully\")\n    \n    # 4. Train segmenter\n    print(\"\\n4. Training U-Net segmenter...\")\n    segmenter = train_segmenter(segmenter, train_loader, val_loader, epochs=5, lr=1e-5)\n    \n    # 5. Create submission\n    print(\"\\n5. Creating submission...\")\n    submission_path = OUT_DIR / 'submission.parquet'\n    create_submission(segmenter, TEST_CSV, TEST_IMG_ROOT, submission_path)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION COMPLETE!\")\n    print(\"=\" * 80)\n    print(f\"Submission saved to: {submission_path}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T11:34:12.635207Z","iopub.execute_input":"2025-10-24T11:34:12.635818Z","iopub.status.idle":"2025-10-24T11:48:27.460217Z","shell.execute_reply.started":"2025-10-24T11:34:12.635792Z","shell.execute_reply":"2025-10-24T11:48:27.459153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}