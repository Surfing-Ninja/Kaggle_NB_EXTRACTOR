{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97984,"databundleVersionId":14096757,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ’¡ *If this notebook helps you understand the ECG reconstruction process, please consider giving it an upvote. It really helps others discover it too!*","metadata":{}},{"cell_type":"markdown","source":"**Main idea:**  \n1) From the training CSVs we build per-lead *shape templates* by detecting R-peaks, cutting beat-aligned windows, and taking a **median beat** per lead.  \n2) At test time, for each requested lead we **tile** that template beat to the target length using a few **BPM hypotheses** (beats-per-minute).  \n3) We apply a light **low-pass filter**, normalize to **shape space (z-score)**, and make a **micro-ensemble** of:  \n   - best BPM by autocorrelation,  \n   - a fixed prior BPM for that lead,  \n   - a plain per-lead mean (resampled) template.  \n4) For limb leads, we optionally apply **Einthoven's law** consistency by deriving III/aVR/aVL/aVF from I and II and **blending**.  \n5) Finally we scale to the required output range and write `submission.csv`.\n\n*The goal is to understand the structure of ECG signals and how careful signal processing can go surprisingly far â€” even without machine learning.*","metadata":{}},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom scipy.signal import butter, filtfilt, find_peaks\n\n\nTRAIN_DIR = '/kaggle/input/physionet-ecg-image-digitization/train/'\nTRAIN_CSV = '/kaggle/input/physionet-ecg-image-digitization/train.csv'\nTEST_DIR  = '/kaggle/input/physionet-ecg-image-digitization/test/'\nTEST_CSV  = '/kaggle/input/physionet-ecg-image-digitization/test.csv'\n\n\n# Config\nLEADS = ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n\n# Beat extraction window around R \nR_PRE_S   = 0.20\nR_POST_S  = 0.40\nBEAT_LEN  = 360  # resample each beat to this length (shape template)\n\n# Band-pass for R detection & morphology preservation\nBP_LO_HZ  = 5.0\nBP_HI_HZ  = 25.0\nBP_ORDER  = 2\n\n# BPM sweep (choose best by autocorr peak score)\nBPM_CANDIDATES = [55, 65, 75, 85, 95]\n\n# Output scale \nMIN_VAL, MAX_VAL = 0.0, 0.09\n\n# Einthoven blend weight\nEINTHOVEN_BLEND_W = 0.6 \n\n# Micro-ensemble weights\nimport numpy as np\nENSEMBLE_W = np.array([0.5, 0.3, 0.2], dtype=np.float32)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation\n- **Beat window** (`R_PRE_S`, `R_POST_S`): captures Pâ€“QRSâ€“T around each R-peak.  \n- **Band-pass 5â€“25 Hz**: emphasizes QRS energy and keeps morphology.  \n- **BPM candidates**: a small sweep of plausible resting rates; we pick the one that yields the strongest autocorrelation peak.  \n- **Einthoven blend**: ensures limb leads don't contradict physics (I, II â†’ III/aVR/aVL/aVF).  \n- **Ensemble**: stabilizes predictions by mixing hypotheses instead of trusting a single BPM guess.","metadata":{}},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"def zscore(x):\n    import numpy as np\n    x = np.asarray(x, np.float32)\n    s = np.std(x) + 1e-8\n    return (x - np.mean(x)) / s\n\ndef bandpass(x, fs, lo=BP_LO_HZ, hi=BP_HI_HZ, order=BP_ORDER):\n    import numpy as np\n    from scipy.signal import butter, filtfilt\n    x = np.asarray(x, np.float32)\n    if len(x) < 10:\n        return x\n    nyq = 0.5 * fs\n    lo_n = max(lo/nyq, 1e-3)\n    hi_n = min(hi/nyq, 0.99)\n    b, a = butter(order, [lo_n, hi_n], btype='band')\n    return filtfilt(b, a, x).astype(np.float32)\n\ndef soft_minmax_scale(x, lo=MIN_VAL, hi=MAX_VAL):\n    import numpy as np\n    x = np.asarray(x, np.float32)\n    mn, mx = float(np.min(x)), float(np.max(x))\n    if not np.isfinite(mn) or not np.isfinite(mx) or mx <= mn:\n        return np.full_like(x, (lo+hi)/2, np.float32)\n    y = (x - mn) / (mx - mn)\n    return (lo + y * (hi - lo)).astype(np.float32)\n\ndef autocorr_peak_score(y, fs, min_rr_s=0.35, max_rr_s=1.5):\n    \"\"\"Return normalized peak of autocorrelation within plausible RR range.\"\"\"\n    import numpy as np\n    y = zscore(y)\n    ac = np.correlate(y, y, mode='full')[len(y)-1:]\n    lo = int(max(min_rr_s * fs, 1))\n    hi = int(min(max_rr_s * fs, len(ac)-1))\n    if hi <= lo:\n        return 0.0\n    segment = ac[lo:hi]\n    if segment.size == 0:\n        return 0.0\n    peak = float(segment.max())\n    norm = float(ac[0]) + 1e-8\n    return float(np.clip(peak / norm, 0.0, 1.0))\n\ndef resample_to_length(x, n):\n    import numpy as np\n    return np.interp(\n        np.linspace(0, 1, n, dtype=np.float32),\n        np.linspace(0, 1, len(x), dtype=np.float32),\n        x\n    ).astype(np.float32)\n\ndef apply_lowpass(x, fs, cutoff=15.0, order=2):\n    import numpy as np\n    from scipy.signal import butter, filtfilt\n    if len(x) <= 10:\n        return x\n    nyq = 0.5 * fs\n    wn = min(cutoff/nyq, 0.99)\n    b, a = butter(order, wn, btype='low')\n    return filtfilt(b, a, x).astype(np.float32)\n\ndef scale_to_lead_range(x, lead_stat=None, lo=MIN_VAL, hi=MAX_VAL):\n    \"\"\"Final mapping: simple robust scaling to [lo, hi].\"\"\"\n    return soft_minmax_scale(x, lo, hi)\n\n# Einthoven relations \ndef derive_limb_leads_from_I_II(yI, yII):\n    III = yII - yI\n    aVR = -(yI + yII) / 2.0\n    aVL = yI - 0.5 * yII\n    aVF = yII - 0.5 * yI\n    return {'III': III, 'aVR': aVR, 'aVL': aVL, 'aVF': aVF}\n\ndef soft_blend(a, b, w):\n    return (1.0 - w) * a + w * b\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key utilities**  \n- `autocorr_peak_score`: quantifies how *periodic* a signal is at plausible RR intervals; used to pick the best BPM hypothesis.  \n- `bandpass` & `apply_lowpass`: gentle filters to emphasize QRS and then smooth the final shape.  \n- `resample_to_length`: ensures any template/lead matches the requested number of rows.","metadata":{}},{"cell_type":"markdown","source":"## Build per-lead stats & beat-aligned templates from the train set","metadata":{}},{"cell_type":"markdown","source":"We **detect R-peaks** on a filtered/z-scored signal, extract windows around each peak, resample to a fixed `BEAT_LEN`, and compute the **median beat** per lead.  \nWe also collect **lead-wise BPM samples** to form a **prior** (median BPM) per lead.","metadata":{}},{"cell_type":"code","source":"def build_per_lead_stats_and_beats(train_csv, train_dir, leads=LEADS):\n    import os\n    import numpy as np\n    import pandas as pd\n    from tqdm.auto import tqdm\n    from scipy.signal import find_peaks\n\n    meta = pd.read_csv(train_csv)\n    lead_vals = {ld: [] for ld in leads}\n    lead_beats = {ld: [] for ld in leads}\n    lead_bpm_samples = {ld: [] for ld in leads}\n\n    for row in tqdm(meta.itertuples(index=False), total=len(meta), desc=\"Scan train\"):\n        rid = str(row.id)\n        fs  = int(row.fs)\n        csvp = os.path.join(train_dir, rid, f\"{rid}.csv\")\n        if not os.path.exists(csvp):\n            continue\n        try:\n            df = pd.read_csv(csvp)\n        except:\n            continue\n\n        for ld in leads:\n            if ld not in df.columns:\n                continue\n            y = df[ld].dropna().to_numpy(np.float32)\n            if y.size < 200:\n                continue\n\n            # Aggregate stats pool (raw)\n            lead_vals[ld].append(y)\n\n            # R-peak detection on band-passed signal\n            y_bp = bandpass(zscore(y), fs)\n            prominence = max(0.4 * np.std(y_bp), 0.15)\n            distance = int(0.35 * fs)\n            pks, _ = find_peaks(y_bp, distance=distance, prominence=prominence)\n            if len(pks) < 2:\n                continue\n\n            # BPM estimate for this record & lead\n            rr = np.diff(pks) / float(fs)\n            rr = rr[(rr > 0.3) & (rr < 2.0)]\n            if rr.size >= 1:\n                bpm = float(np.clip(60.0 / np.median(rr), 40.0, 160.0))\n                lead_bpm_samples[ld].append(bpm)\n\n            # Extract beats around R and resample to BEAT_LEN\n            n_pre  = int(round(R_PRE_S * fs))\n            n_post = int(round(R_POST_S * fs))\n            for pk in pks:\n                a, b = pk - n_pre, pk + n_post\n                if a < 0 or b >= len(y):\n                    continue\n                seg = y[a:b+1].astype(np.float32)\n                seg_rs = np.interp(\n                    np.linspace(0, 1, BEAT_LEN, dtype=np.float32),\n                    np.linspace(0, 1, len(seg), dtype=np.float32),\n                    seg\n                ).astype(np.float32)\n                lead_beats[ld].append(seg_rs)\n\n\n    lead_stats = {}\n    for ld in leads:\n        if len(lead_vals[ld]) == 0:\n            lead_stats[ld] = {'mean': 0.0, 'std': 0.1, 'median': 0.0, 'min': -0.5, 'max': 0.5}\n            continue\n        vals = np.concatenate(lead_vals[ld]).astype(np.float32)\n        if vals.size == 0:\n            lead_stats[ld] = {'mean': 0.0, 'std': 0.1, 'median': 0.0, 'min': -0.5, 'max': 0.5}\n        else:\n            lead_stats[ld] = {\n                'mean': float(np.mean(vals)),\n                'std':  float(np.std(vals)) if vals.size > 1 else 0.1,\n                'median': float(np.median(vals)),\n                'min': float(np.min(vals)),\n                'max': float(np.max(vals))\n            }\n    return lead_stats, lead_beats, lead_bpm_samples\n\ndef build_lead_templates(lead_beats, leads=LEADS):\n    import numpy as np\n    lead_template = {}\n    for ld in leads:\n        if len(lead_beats[ld]) > 0:\n            arr = np.vstack(lead_beats[ld]).astype(np.float32)\n            tpl = np.median(arr, axis=0).astype(np.float32)\n        else:\n            t = np.linspace(0, 1, BEAT_LEN, dtype=np.float32)\n            tpl = np.sin(2 * np.pi * t).astype(np.float32)\n        lead_template[ld] = zscore(tpl)\n    return lead_template\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plain per-lead mean templates ","metadata":{}},{"cell_type":"markdown","source":"\nThis branch **ignores beats** and simply averages resampled, normalized signals per lead across the training set.  \nIt smooths idiosyncrasies and often yields a stable baseline when ensembled.\n","metadata":{}},{"cell_type":"code","source":"def make_plain_mean_template(train_csv, train_dir, leads=LEADS, template_len=500):\n    import os\n    import numpy as np\n    import pandas as pd\n    meta = pd.read_csv(train_csv)\n    lead_means = {}\n    for ld in leads:\n        resamp_signals = []\n        for row in meta.itertuples(index=False):\n            rid = str(row.id)\n            csvp = os.path.join(train_dir, rid, f\"{rid}.csv\")\n            if not os.path.exists(csvp):\n                continue\n            try:\n                df = pd.read_csv(csvp)\n            except:\n                continue\n            if ld not in df.columns:\n                continue\n            s = df[ld].dropna().to_numpy(np.float32)\n            if s.size < 50:\n                continue\n            s_norm = (s - np.mean(s)) / (np.std(s) + 1e-8)\n            s_rs = np.interp(\n                np.linspace(0, 1, template_len, dtype=np.float32),\n                np.linspace(0, 1, len(s_norm), dtype=np.float32),\n                s_norm\n            ).astype(np.float32)\n            resamp_signals.append(s_rs)\n        if len(resamp_signals) > 0:\n            lead_means[ld] = np.mean(np.vstack(resamp_signals), axis=0).astype(np.float32)\n        else:\n            t = np.linspace(0, 1, template_len, dtype=np.float32)\n            lead_means[ld] = np.sin(2*np.pi*t).astype(np.float32)\n    return lead_means\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BPM tiling & selection via autocorrelation","metadata":{}},{"cell_type":"code","source":"\ndef tile_template(template_beat, fs, n_out, bpm, amp=1.0):\n    import numpy as np\n    beat_samples = max(4, int(round((60.0 / max(bpm, 1e-6)) * fs)))\n    one = np.interp(\n        np.linspace(0, 1, beat_samples, dtype=np.float32),\n        np.linspace(0, 1, len(template_beat), dtype=np.float32),\n        template_beat\n    ).astype(np.float32)\n    reps = int(np.ceil(n_out / len(one)))\n    y = np.tile(one, reps)[:n_out]\n    y = zscore(y) * float(amp)\n    return y.astype(np.float32)\n\ndef choose_best_bpm(template_beat, fs, n_out, bpm_list=BPM_CANDIDATES):\n    best_bpm, best_score, best_y = None, -1.0, None\n    for bpm in bpm_list:\n        y = tile_template(template_beat, fs, n_out, bpm, amp=1.0)\n        sc = autocorr_peak_score(y, fs)\n        if sc > best_score:\n            best_bpm, best_score, best_y = bpm, sc, y\n    return best_bpm, best_y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We **stretch and tile** the median-beat template to match a candidate BPM and the target number of samples.  \nWe then pick the BPM whose synthesized series shows the **strongest periodicity** (highest autocorrelation peak in plausible RR lags).","metadata":{}},{"cell_type":"markdown","source":"## Step 1â€“2: Build assets (stats, beat templates, mean templates, BPM priors)","metadata":{}},{"cell_type":"code","source":"\nprint(\"[1/4] Scanning train to build stats & beat-aligned templates...\")\nlead_stats, lead_beats, lead_bpms = build_per_lead_stats_and_beats(TRAIN_CSV, TRAIN_DIR, LEADS)\nlead_template = build_lead_templates(lead_beats, LEADS)\n\nprint(\"[2/4] Building plain mean templates (ensemble branch)...\")\nmean_templates = make_plain_mean_template(TRAIN_CSV, TRAIN_DIR, LEADS, template_len=500)\n\nper_lead_bpm_prior = {}\nfor ld in LEADS:\n    if len(lead_bpms[ld]) > 0:\n        per_lead_bpm_prior[ld] = float(np.median(np.array(lead_bpms[ld], dtype=np.float32)))\n    else:\n        per_lead_bpm_prior[ld] = 75.0  # sensible fallback\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3â€“4: Predict test with micro-ensemble and Einthoven blending","metadata":{}},{"cell_type":"code","source":"\nprint(\"[3/4] Predicting test...\")\ntest = pd.read_csv(TEST_CSV)\n\n# Pre-group rows by record id to handle Einthoven consistency across a record\nrecords = {}\nfor r in test.itertuples(index=False):\n    records.setdefault(int(r.id), []).append(r)\n\npredictions = {}\n\nfor rid, items in tqdm(records.items(), desc=\"Records\"):\n    tmp_store = {}     \n    scales_store = {}  \n\n    # Synthesize each requested lead\n    for r in items:\n        lead = str(r.lead)\n        fs   = int(r.fs)\n        n    = int(r.number_of_rows)\n\n        tpl_beat = lead_template.get(lead, lead_template['II'])\n\n        # BPM sweep branch\n        best_bpm, y_best = choose_best_bpm(tpl_beat, fs, n, BPM_CANDIDATES)\n\n        # Fixed bpm branch (median prior per lead)\n        bpm_fixed = per_lead_bpm_prior.get(lead, 75.0)\n        y_fixed   = tile_template(tpl_beat, fs, n, bpm_fixed, amp=1.0)\n\n        # Plain mean template branch\n        y_mean = resample_to_length(mean_templates.get(lead, mean_templates['II']), n)\n\n        # Light low-pass on each branch\n        y_best  = apply_lowpass(y_best, fs, cutoff=15.0, order=2)\n        y_fixed = apply_lowpass(y_fixed, fs, cutoff=15.0, order=2)\n        y_mean  = apply_lowpass(y_mean, fs,  cutoff=15.0, order=2)\n\n        # Normalize branches to shape space\n        B = zscore(y_best)\n        F = zscore(y_fixed)\n        M = zscore(y_mean)\n\n        # Micro-ensemble in shape space\n        w = ENSEMBLE_W / (np.sum(ENSEMBLE_W) + 1e-8)\n        y_syn = (w[0]*B + w[1]*F + w[2]*M).astype(np.float32)\n\n        tmp_store[lead] = y_syn\n        scales_store[lead] = (fs, n)\n\n    #  Einthoven pass \n    if EINTHOVEN_BLEND_W > 0.0 and 'I' in tmp_store and 'II' in tmp_store:\n        for dlead in ['III', 'aVR', 'aVL', 'aVF']:\n            if dlead not in scales_store:\n                continue\n            _, n_d = scales_store[dlead]\n            yI_rs  = resample_to_length(tmp_store['I'],  n_d)\n            yII_rs = resample_to_length(tmp_store['II'], n_d)\n            derived_all = derive_limb_leads_from_I_II(yI_rs, yII_rs)\n            ydrv = zscore(derived_all[dlead])\n\n            if dlead in tmp_store and len(tmp_store[dlead]) == n_d:\n                tmp_store[dlead] = soft_blend(tmp_store[dlead], ydrv, EINTHOVEN_BLEND_W)\n            else:\n                tmp_store[dlead] = ydrv\n\n    # Final scaling per lead \n    for lead, y in tmp_store.items():\n        fs, n_rows = scales_store.get(lead, (500, len(y)))\n        y_scaled = scale_to_lead_range(y, None, MIN_VAL, MAX_VAL)\n        predictions[(rid, lead)] = y_scaled.astype(np.float32)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"\nprint(\"[4/4] Writing submission.csv ...\")\nrows = []\nfor r in test.itertuples(index=False):\n    rid   = int(r.id)\n    lead  = str(r.lead)\n    n     = int(r.number_of_rows)\n    y     = predictions[(rid, lead)]\n    if len(y) != n:\n        y = resample_to_length(y, n)\n    for i in range(n):\n        rows.append((f\"{rid}_{i}_{lead}\", float(y[i])))\n\nsub = pd.DataFrame(rows, columns=['id','value'])\nsub.to_csv('submission.csv', index=False)\nsub.head(7)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## Why this works (and when it might fail)\nECGs are quasi-periodic. A **median beat template** captures typical morphology while rejecting outliers.  \nBy **tiling** the template at plausible BPMs, we synthesize a full-length series that matches the requested\nsampling rate and duration. The **autocorrelation** criterion encourages rhythmic consistency.\nAn **ensemble** of hypotheses hedges against a wrong BPM pick. **Einthoven blending** reduces contradictions\nacross limb leads by enforcing physiological relations.\n\nEdge cases include bigeminy/trigeminy, strong motion artifacts, or sudden rate changes within the record.  \n\n**Reproducibility:** No random seeds are required since the pipeline is deterministic.  \n**Dependencies:** `numpy`, `pandas`, `scipy`, `tqdm`  (all standard on Kaggle).\n\n## Final Thoughts\n\nIf this notebook helped you learn something new,  \n**donâ€™t forget to leave an upvote ðŸš€** - it really helps others find it and keeps me motivated to share more tutorials like this!  \n\nGood luck with your own experiments, and happy Kaggle-ing!","metadata":{}}]}