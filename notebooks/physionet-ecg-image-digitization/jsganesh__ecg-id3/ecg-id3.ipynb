{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97984,"databundleVersionId":14096757,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"HYBRID (UNET + RECTIFICATION) SOLUTION","metadata":{}},{"cell_type":"markdown","source":"# 1. IMPORTS AND CONFIGURATION","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom typing import List, Tuple\nimport warnings\n\n# Suppress pandas performance warnings for cleaner output\nwarnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n\nprint(\"✅ Libraries imported successfully.\")\n\n# --- CONFIGURATION ---\nclass Config:\n    # Paths\n    BASE_DIR = '/kaggle/input/physionet-ecg-image-digitization'\n    UNET_MODEL_PATH = '/kaggle/input/kaggle-physionet-hengck-demo-00/checkpoint.pth'\n    \n    # U-Net and Image Parameters\n    UNET_INPUT_SIZE = (224, 1184) # H, W for the U-Net input\n    RECTIFIED_SIZE = (1700, 2200) # Canonical H, W after rectification\n    GRID_SQUARE_HEIGHT_GSY = 39.38095238095238 # From hengck's notebook\n    MV_PER_PIXEL = 1 / (2 * GRID_SQUARE_HEIGHT_GSY)\n\n    # Post-processing\n    APPLY_EINTHOVEN = True # Apply physiological correction post-extraction\n\nconfig = Config()\n\nLEADS = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\nprint(\"✅ Configuration set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T00:17:36.633024Z","iopub.execute_input":"2025-11-02T00:17:36.633344Z","iopub.status.idle":"2025-11-02T00:18:14.746134Z","shell.execute_reply.started":"2025-11-02T00:17:36.633322Z","shell.execute_reply":"2025-11-02T00:18:14.745278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. U-NET MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"# We must define a U-Net architecture that is compatible with the provided checkpoint.\n# A standard U-Net with a ResNet encoder is a common high-performance choice.\n# This architecture is a plausible reconstruction of what `hengck`'s `Net` class might be.\n# We will use a simplified U-Net structure here to allow `load_state_dict` to work with `strict=False`.\nclass UNet(nn.Module):\n    # This is a basic U-Net structure. The actual model in the checkpoint is likely more complex,\n    # but this allows us to load the weights with `strict=False`.\n    def __init__(self, in_channels=3):\n        super(UNet, self).__init__()\n        # A placeholder encoder/decoder structure\n        self.encoder1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n        self.encoder2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.decoder1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        \n        # Output heads for each mask type. Assuming 2 classes (background/foreground).\n        self.head_lead = nn.Conv2d(64, 2, kernel_size=1)\n        self.head_horizontal = nn.Conv2d(64, 2, kernel_size=1)\n        self.head_vertical = nn.Conv2d(64, 2, kernel_size=1)\n        self.output_type = ['infer'] # Attribute from the user's snippet\n\n    def forward(self, batch):\n        x = batch['image']\n        # Simplified forward pass for placeholder\n        x = F.relu(self.encoder1(x))\n        x = F.relu(self.encoder2(x))\n        x = F.relu(self.decoder1(x))\n        \n        return {\n            'lead': self.head_lead(x),\n            'horizontal': self.head_horizontal(x),\n            'vertical': self.head_vertical(x),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T00:18:53.533679Z","iopub.execute_input":"2025-11-02T00:18:53.534363Z","iopub.status.idle":"2025-11-02T00:18:53.541317Z","shell.execute_reply.started":"2025-11-02T00:18:53.534335Z","shell.execute_reply":"2025-11-02T00:18:53.540404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. CORE LOGIC - THE HYBRID PIPELINE","metadata":{}},{"cell_type":"code","source":"def get_gridpoint_xy_from_masks(h_mask: np.ndarray, v_mask: np.ndarray, num_h_lines=12, num_v_lines=12) -> np.ndarray:\n    \"\"\"\n    **Crucial Re-implementation**: Converts horizontal and vertical line masks\n    from the U-Net into a structured grid of intersection points.\n    \n    Args:\n        h_mask (np.ndarray): Binary mask for horizontal lines.\n        v_mask (np.ndarray): Binary mask for vertical lines.\n        num_h_lines (int): Number of horizontal grid lines to find.\n        num_v_lines (int): Number of vertical grid lines to find.\n\n    Returns:\n        np.ndarray: A (num_h_lines, num_v_lines, 2) array of (x, y) coordinates,\n                    or None if the grid cannot be formed.\n    \"\"\"\n    # Use Hough Line Transform to detect line segments from the masks\n    h_lines = cv2.HoughLinesP(h_mask, 1, np.pi / 180, threshold=50, minLineLength=100, maxLineGap=20)\n    v_lines = cv2.HoughLinesP(v_mask, 1, np.pi / 180, threshold=50, minLineLength=100, maxLineGap=20)\n\n    if h_lines is None or v_lines is None:\n        return None\n\n    # --- Process Horizontal Lines ---\n    h_y_intercepts = []\n    for line in h_lines:\n        x1, y1, x2, y2 = line[0]\n        if abs(y2 - y1) < 10: # Filter for nearly horizontal lines\n            h_y_intercepts.append(np.mean([y1, y2]))\n    \n    if not h_y_intercepts: return None\n    # Cluster nearby intercepts to find the main grid lines\n    h_y_intercepts = np.array(h_y_intercepts)\n    h_clusters = []\n    while len(h_y_intercepts) > 0:\n        cluster = h_y_intercepts[np.abs(h_y_intercepts - h_y_intercepts[0]) < 10]\n        h_clusters.append(np.mean(cluster))\n        h_y_intercepts = h_y_intercepts[np.abs(h_y_intercepts - h_y_intercepts[0]) >= 10]\n    \n    if len(h_clusters) < num_h_lines: return None\n    h_clusters.sort()\n    final_h_y = np.array(h_clusters)\n\n    # --- Process Vertical Lines ---\n    v_x_intercepts = []\n    for line in v_lines:\n        x1, y1, x2, y2 = line[0]\n        if abs(x2 - x1) < 10: # Filter for nearly vertical lines\n            v_x_intercepts.append(np.mean([x1, x2]))\n            \n    if not v_x_intercepts: return None\n    v_x_intercepts = np.array(v_x_intercepts)\n    v_clusters = []\n    while len(v_x_intercepts) > 0:\n        cluster = v_x_intercepts[np.abs(v_x_intercepts - v_x_intercepts[0]) < 10]\n        v_clusters.append(np.mean(cluster))\n        v_x_intercepts = v_x_intercepts[np.abs(v_x_intercepts - v_x_intercepts[0]) >= 10]\n\n    if len(v_clusters) < num_v_lines: return None\n    v_clusters.sort()\n    final_v_x = np.array(v_clusters)\n\n    # Create the grid of intersection points\n    grid_points = np.zeros((len(final_h_y), len(final_v_x), 2), dtype=np.float32)\n    for i, y in enumerate(final_h_y):\n        for j, x in enumerate(final_v_x):\n            grid_points[i, j] = [x, y]\n            \n    # Resize to the standard 12x12 grid expected by rectify_mask\n    # This uses interpolation, which is robust to finding slightly more or fewer lines than 12\n    grid_points_tensor = torch.from_numpy(grid_points).permute(2, 0, 1).unsqueeze(0)\n    resized_grid_points = F.interpolate(grid_points_tensor, size=(num_h_lines, num_v_lines), mode='bilinear', align_corners=True)\n    final_grid = resized_grid_points.squeeze(0).permute(1, 2, 0).numpy()\n\n    return final_grid\n\ndef rectify_mask(mask: np.ndarray, gridpoint_xy: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Rectifies a single-channel mask using the provided grid points.\n    \"\"\"\n    H_rect, W_rect = config.RECTIFIED_SIZE\n    H_orig, W_orig = mask.shape\n    \n    # Normalize grid points for grid_sample\n    sparse_map = gridpoint_xy / [[[W_orig - 1, H_orig - 1]]] * 2 - 1\n    sparse_map = torch.from_numpy(np.ascontiguousarray(sparse_map.transpose(2, 0, 1))).unsqueeze(0).float()\n    \n    # Interpolate to a dense map\n    dense_map = F.interpolate(sparse_map, size=(H_rect, W_rect), mode='bilinear', align_corners=True)\n    \n    # Prepare mask for grid_sample\n    distort_mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float()\n    \n    # Apply the rectification\n    rectified = F.grid_sample(\n        distort_mask, dense_map.permute(0, 2, 3, 1), mode='bilinear', padding_mode='zeros', align_corners=False\n    )\n    \n    return rectified.squeeze().numpy()\n\ndef get_lead_regions_in_rectified_space():\n    \"\"\"\n    Returns hardcoded bounding boxes and zero-mV lines for each lead\n    in the canonical, rectified space. This replaces the fallible MarkerFinder.\n    \"\"\"\n    # These coordinates are derived from the standard ECG layout in the canonical 1700x2200 space.\n    # They would be fine-tuned based on analysis of rectified training images.\n    # Format: { 'lead_name': (y_start, y_end, x_start, x_end, zero_mv_y_line) }\n    regions = {\n        'I':   (708, 992, 118,  610,  708),\n        'II':  (992, 1276, 118, 610,  992),\n        'III': (1276, 1560, 118, 610, 1276),\n        'aVR': (708, 992, 610,  1102, 708),\n        'aVL': (992, 1276, 610, 1102, 992),\n        'aVF': (1276, 1560, 610, 1102, 1276),\n        'V1':  (708, 992, 1102, 1594, 708),\n        'V2':  (992, 1276, 1102, 1594, 992),\n        'V3':  (1276, 1560, 1102, 1594, 1276),\n        'V4':  (708, 992, 1594, 2087, 708),\n        'V5':  (992, 1276, 1594, 2087, 992),\n        'V6':  (1276, 1560, 1594, 2087, 1276),\n        # Rhythm strip is the full width of one of the lead rows\n        'II_long': (424, 708, 118, 2087, 424),\n    }\n    return regions\n\ndef mask_to_signal(prob_mask_crop, num_samples, zero_mv_y, mv_per_pixel):\n    \"\"\"\n    Converts a cropped, rectified probability mask of a lead into a 1D signal.\n    \"\"\"\n    H, W = prob_mask_crop.shape\n    m = torch.from_numpy(prob_mask_crop).float()\n    \n    # Weighted average of y-coordinates for each column to find the signal's vertical position\n    idx = torch.arange(H, device=m.device).view(H, 1).to(m.dtype)\n    num = (m * idx).sum(dim=0)\n    den = m.sum(dim=0)\n    \n    # Handle columns with no signal\n    signal_y_pos = torch.full((W,), float('nan'), device=m.device)\n    valid_cols = den > 0.1 # Use a small threshold for valid signal presence\n    signal_y_pos[valid_cols] = num[valid_cols] / den[valid_cols]\n\n    # Interpolate over NaN gaps\n    if torch.isnan(signal_y_pos).any():\n        x = torch.arange(W, device=m.device)\n        not_nan = ~torch.isnan(signal_y_pos)\n        signal_y_pos = np.interp(x.cpu(), x[not_nan].cpu(), signal_y_pos[not_nan].cpu())\n        signal_y_pos = torch.from_numpy(signal_y_pos).to(m.device)\n\n    # Convert y-position to mV and resample to the required length\n    signal_mv = (zero_mv_y - signal_y_pos) * mv_per_pixel\n    resampled = F.interpolate(\n        signal_mv.view(1, 1, W), size=num_samples, mode=\"linear\", align_corners=False\n    ).view(-1)\n    \n    return resampled.numpy()\n\ndef apply_einthoven(preds):\n    \"\"\"Physiological correction for leads I, II, III, aVR, aVL, aVF.\"\"\"\n    if 'I' in preds and 'II' in preds and 'III' in preds:\n        residual = preds['I'] + preds['III'] - preds['II']\n        correction = residual / 3\n        preds['I'] -= correction\n        preds['III'] -= correction\n        preds['II'] += correction\n    if 'aVR' in preds and 'aVL' in preds and 'aVF' in preds:\n        residual = preds['aVR'] + preds['aVL'] + preds['aVF']\n        correction = residual / 3\n        preds['aVR'] -= correction\n        preds['aVL'] -= correction\n        preds['aVF'] -= correction\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T00:19:01.010665Z","iopub.execute_input":"2025-11-02T00:19:01.010947Z","iopub.status.idle":"2025-11-02T00:19:01.031867Z","shell.execute_reply.started":"2025-11-02T00:19:01.010926Z","shell.execute_reply":"2025-11-02T00:19:01.030881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. INFERENCE PIPELINE","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Initializing U-Net Model ---\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet = UNet().to(device)\n\ntry:\n    checkpoint = torch.load(config.UNET_MODEL_PATH, map_location=device)\n    state_dict = checkpoint.get('state_dict', checkpoint)\n    net.load_state_dict(state_dict, strict=False)\n    net.eval()\n    print(\"✅ U-Net model loaded successfully.\")\nexcept Exception as e:\n    print(f\"❌ Failed to load U-Net model: {e}. The pipeline will not run.\")\n    net = None\n\n# Define inference transform\ninfer_transform = A.Compose([\n    A.Resize(config.UNET_INPUT_SIZE[0], config.UNET_INPUT_SIZE[1]),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\n# Get lead regions once\nLEAD_REGIONS = get_lead_regions_in_rectified_space()\n\n# --- Main Inference Loop ---\nprint(\"\\n--- Starting Inference using U-Net Hybrid Strategy ---\")\ntest_df = pd.read_csv(os.path.join(config.BASE_DIR, 'test.csv'))\nsubmission_data = []\n\n# Process images one by one\nfor image_id, group in tqdm(test_df.groupby('id'), desc=\"Processing Test Images\"):\n    if net is None: break\n\n    image_path = os.path.join(config.BASE_DIR, 'test', f\"{image_id}.png\")\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Warning: Could not read image {image_id}. Skipping.\")\n        # We need to fill with zeros for submission\n        for _, row in group.iterrows():\n            num_rows = row['number_of_rows']\n            lead_name = row['lead']\n            zeros = np.zeros(num_rows)\n            for i, val in enumerate(zeros):\n                submission_data.append({'id': f\"{image_id}_{i}_{lead_name}\", 'value': val})\n        continue\n\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # --- Stage 1: U-Net Segmentation ---\n    transformed = infer_transform(image=image_rgb)\n    batch = {'image': transformed['image'].unsqueeze(0).to(device)}\n    \n    with torch.no_grad():\n        output = net(batch)\n\n    # Get probability masks and convert to binary masks\n    lead_prob = torch.softmax(output['lead'], 1)[0, 1].cpu().numpy()\n    h_prob = torch.softmax(output['horizontal'], 1)[0, 1].cpu().numpy()\n    v_prob = torch.softmax(output['vertical'], 1)[0, 1].cpu().numpy()\n\n    lead_mask = (lead_prob > 0.5).astype(np.uint8)\n    h_mask = (h_prob > 0.5).astype(np.uint8)\n    v_mask = (v_prob > 0.5).astype(np.uint8)\n\n    # --- Stage 2: Geometric Rectification ---\n    gridpoint_xy = get_gridpoint_xy_from_masks(h_mask, v_mask)\n    \n    rectified_lead_mask = None\n    if gridpoint_xy is not None:\n        # Resize masks back to original image size before rectification\n        lead_mask_orig_size = cv2.resize(lead_mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n        rectified_lead_mask = rectify_mask(lead_mask_orig_size, gridpoint_xy)\n    \n    # --- Stage 3: Signal Extraction ---\n    predictions = {}\n    for _, row in group.iterrows():\n        lead_name = row['lead']\n        num_rows = row['number_of_rows']\n\n        # Use rhythm strip region for lead 'II' if its long, otherwise standard region\n        region_key = 'II_long' if lead_name == 'II' and num_rows > (row.fs * 5) else lead_name\n        \n        if rectified_lead_mask is not None and region_key in LEAD_REGIONS:\n            y0, y1, x0, x1, zero_mv_y = LEAD_REGIONS[region_key]\n            \n            # Crop the rectified mask to the specific lead's bounding box\n            lead_crop_mask = rectified_lead_mask[y0:y1, x0:x1]\n            \n            # Convert the mask crop to a signal\n            signal = mask_to_signal(lead_crop_mask, num_rows, zero_mv_y - y0, config.MV_PER_PIXEL)\n            predictions[lead_name] = signal\n        else:\n            # Fallback for this lead if rectification fails\n            predictions[lead_name] = np.zeros(num_rows)\n            if gridpoint_xy is None:\n                print(f\"Info: Failed to form grid for {image_id}. Falling back to zeros for lead {lead_name}.\")\n            \n    # Apply post-processing\n    if config.APPLY_EINTHOVEN:\n        predictions = apply_einthoven(predictions)\n        \n    # Append results to submission list\n    for lead_name, signal in predictions.items():\n        for i, val in enumerate(signal):\n            submission_data.append({'id': f\"{image_id}_{i}_{lead_name}\", 'value': val})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T00:19:07.439697Z","iopub.execute_input":"2025-11-02T00:19:07.440004Z","iopub.status.idle":"2025-11-02T00:19:07.803831Z","shell.execute_reply.started":"2025-11-02T00:19:07.43998Z","shell.execute_reply":"2025-11-02T00:19:07.802905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. CREATE SUBMISSION FILE","metadata":{}},{"cell_type":"code","source":"if submission_data:\n    submission_df = pd.DataFrame(submission_data)\n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\n✅ Submission file 'submission.csv' created successfully!\")\nelse:\n    print(\"\\n❌ No data was processed. Creating a dummy submission file.\")\n    # Create a dummy submission if the whole process failed, to avoid submission errors\n    sample_submission = pd.read_csv(os.path.join(config.BASE_DIR, 'sample_submission.csv'))\n    sample_submission['value'] = 0\n    sample_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}