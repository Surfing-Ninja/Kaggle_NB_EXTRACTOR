{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97984,"databundleVersionId":14096757,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The kernel is similar in part to the kernel of the [excellent work](https://www.kaggle.com/code/antonoof/large-eda-and-statistical-model) by [expert Antonoof](https://www.kaggle.com/antonoof), where he conducts an analysis and supports it with a detailed EDA.\nWith this kernel we want to take a look at the system's behavior, basically this is the first glance.","metadata":{}},{"cell_type":"markdown","source":"#### Original.LB = 0.16","metadata":{}},{"cell_type":"code","source":"import os,numpy,pandas; from scipy.signal import butter,filtfilt\n\npath   = '/kaggle/input/physionet-ecg-image-digitization/'\n\ntrain  = pandas.read_csv     (path + 'train.csv')\ntest   = pandas.read_csv     (path + 'test.csv')\nsubm_1 = pandas.read_parquet (path + \"sample_submission.parquet\")\n\nTRAIN_DIR =                   path + 'train/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T10:47:56.070334Z","iopub.execute_input":"2025-10-25T10:47:56.070756Z","iopub.status.idle":"2025-10-25T10:47:57.50773Z","shell.execute_reply.started":"2025-10-25T10:47:56.070723Z","shell.execute_reply":"2025-10-25T10:47:57.506615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"leads = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n\nlead_templates, template_len = {}, 500\n\nfor lead in leads:\n    PURE_signals = []\n    for _, row in train.iterrows():\n        csv_path = os.path.join(TRAIN_DIR, str(row['id']), f\"{row['id']}.csv\")\n        if not os.path.exists(csv_path): continue\n        try:\n            #----------------------------------------------\n            df = pandas.read_csv(csv_path)\n            if lead not in df.columns: continue \n            #----------------------------------------------\n            s = df[lead].dropna().values.astype(numpy.float32)\n            if len(s) < 50: continue\n            #----------------------------------------------\n            s_norm = (s - s.mean()) / (s.std() + 1e-8)\n            s_resamp = numpy.interp(\n                numpy.linspace(0, 1, template_len),\n                numpy.linspace(0, 1, len(s_norm)), s_norm \n            )\n            PURE_signals.append(s_resamp)\n            #----------------------------------------------\n        except: continue\n        #----------------------------------------------\n    if PURE_signals:\n        lead_templates[lead] = numpy.mean(PURE_signals, axis=0)\n    else:\n        t = numpy.linspace(0, 1, template_len)\n        lead_templates[lead] = numpy.sin(2 * numpy.pi * t)\n    #----------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T10:48:01.477027Z","iopub.execute_input":"2025-10-25T10:48:01.477366Z","iopub.status.idle":"2025-10-25T10:49:30.417745Z","shell.execute_reply.started":"2025-10-25T10:48:01.47734Z","shell.execute_reply":"2025-10-25T10:49:30.416699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions, min_val, max_val = {}, 0.0, 0.07\n\nfor _, row in test.iterrows():\n    \n    base_id, lead, n_rows  = row['id'],row['lead'],row['number_of_rows']\n\n    fs = row.get('fs', 500)\n    \n    template = lead_templates.get(lead, lead_templates['II']).copy()\n    \n    if len(template) != n_rows:\n        signal = numpy.interp(\n            numpy.linspace(0, 1, n_rows),\n            numpy.linspace(0, 1, len(template)),\n            template\n        )\n    else:\n        signal = template\n    \n    if len(signal) > 10:\n        normal_cutoff = min(30*fs, 0.99)\n        b, a = butter(2, normal_cutoff, btype='low')\n        signal = filtfilt(b, a, signal)\n    \n    s_min, s_max = signal.min(), signal.max()\n    \n    if s_max - s_min < 1e-8:\n        signal = numpy.full(n_rows, (min_val + max_val) / 2)\n    else:\n        signal = (signal - s_min) / (s_max - s_min)\n        signal = min_val + signal * (max_val - min_val)\n    \n    predictions[(base_id, lead)] = signal.astype(numpy.float32)\n\ndata = []\nfor _, row in test.iterrows():\n    _id,lead,n_rows = row['id'],row['lead'],row['number_of_rows']\n    signal = predictions[(_id, lead)]\n    for i in range(n_rows):\n        data.append( { 'id':f\"{_id}_{i}_{lead}\", 'value':float(signal[i]) } )\n\nsubm_original1 = pandas.DataFrame(data)\nsubm_original1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T10:49:38.489524Z","iopub.execute_input":"2025-10-25T10:49:38.489872Z","iopub.status.idle":"2025-10-25T10:49:38.683765Z","shell.execute_reply.started":"2025-10-25T10:49:38.489848Z","shell.execute_reply":"2025-10-25T10:49:38.682881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Original.LB = 0.17","metadata":{}},{"cell_type":"code","source":"from scipy.signal import butter,filtfilt\n\nimport numpy as np, pandas as pd\n\nsubm_2 = pandas.read_parquet (path + \"sample_submission.parquet\")\n\nleads = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n\ntemplate_len, lead_templates = 500, {}\n\nfor lead in leads:\n    \n    list_signalov = []\n    \n    for _, row in train.iterrows():\n        \n        csv_path = os.path.join(TRAIN_DIR, str(row['id']), f\"{row['id']}.csv\")\n        \n        if not os.path.exists(csv_path): continue\n        \n        try:\n            df = pd.read_csv(csv_path)          \n            if lead not in df.columns: continue\n            \n            s = df[lead].dropna().values.astype(np.float32)\n            if len(s) < 50: continue\n            \n            norma = (s - s.mean()) / (s.std() + 1e-8)\n            \n            s_resamp =\\\n              np.interp(np.linspace(0,1,template_len), np.linspace(0,1,len(norma)), norma)\n            \n            list_signalov.append(s_resamp)\n        except:\n            continue\n    \n    if list_signalov:\n        lead_templates[lead] = np.mean(list_signalov, axis=0)\n    else:\n        lead_templates[lead] = np.sin(2 * np.pi * np.linspace(0,1,template_len))\n        \n\npredictions, min_val, max_val = {}, 0.0, 0.09\n\n\nfor _, row in test.iterrows():\n    \n    base_id, lead, n_rows = row['id'],row['lead'],row['number_of_rows']\n\n    fs = row.get('fs', 500)\n    \n    template = lead_templates.get(lead, lead_templates['II']).copy()\n    \n    if len(template) != n_rows:\n        _1_signal =\\\n            np.interp(np.linspace(0, 1, n_rows), np.linspace(0, 1, len(template)), template)\n    else:\n        _1_signal = template\n\n    \n    if len(_1_signal)>10 :\n        normal_cutoff = min      (30 * fs, 0.99)\n        b,a           = butter   (2, normal_cutoff, btype='low')\n        _1_signal     = filtfilt (b,a, _1_signal)\n\n    \n    _1s_min, _1s_Max = _1_signal.min(),_1_signal.max()\n\n    \n    if s_max - s_min < 1e-8:\n        _1_signal = np.full(n_rows, (min_val + max_val) / 2)\n    else:\n        _1_signal = (_1_signal - _1s_min) / (_1s_Max - _1s_min)\n        \n        _1_signal = min_val + _1_signal * (max_val - min_val)\n    \n    predictions[(base_id, lead)] = _1_signal.astype(np.float32)\n\n\ndata = []\n\n\nfor _, row in test.iterrows():\n    \n    base_id, lead, n_rows = row['id'],row['lead'],row['number_of_rows']\n    \n    _1_signal = predictions[(base_id, lead)]\n    \n    for i in range(n_rows):\n        data.append({'id': f\"{base_id}_{i}_{lead}\",'value': float(_1_signal[i])})\n\npd.set_option('display.float_format', '{:.15f}'.format)\n            \nsubm_original2 = pd.DataFrame(data)\nsubm_original2 . to_csv('submission.csv', index=False)\nsubm_original2 . head(11)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:50:51.390681Z","iopub.execute_input":"2025-10-25T11:50:51.391009Z","iopub.status.idle":"2025-10-25T11:52:11.886582Z","shell.execute_reply.started":"2025-10-25T11:50:51.390986Z","shell.execute_reply":"2025-10-25T11:52:11.885537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm_comparison = pd.merge(subm_original2, subm_original1, on='id')\n\nsubm_comparison['subm_2 - subm_1'] = subm_original2['value'] - subm_original1['value']\n\nsubm_comparison","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-10-25T10:58:09.278755Z","iopub.execute_input":"2025-10-25T10:58:09.279086Z","iopub.status.idle":"2025-10-25T10:58:09.34625Z","shell.execute_reply.started":"2025-10-25T10:58:09.279062Z","shell.execute_reply":"2025-10-25T10:58:09.344989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os,ast\nimport numpy as np\nimport pandas as pd\n\nfrom bokeh.plotting import figure, gridplot \nfrom bokeh.io import output_file, show, output_notebook\noutput_notebook()","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-10-25T10:58:15.568948Z","iopub.execute_input":"2025-10-25T10:58:15.56934Z","iopub.status.idle":"2025-10-25T10:58:16.220262Z","shell.execute_reply.started":"2025-10-25T10:58:15.569313Z","shell.execute_reply":"2025-10-25T10:58:16.219234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bokeh_show(\n        params,\n        df_cross,\n        colors, \n        show_figures1, \n        show_figures2, wps_fig2,\n        color_cross):\n    \n    def dossier(js,subms,cols):\n        def quant(i,js,subms,cols):\n            return {\"c\" : i, \"q\" : sum([1 for subm in cols[i] if subm == subms[js]])}\n        return {\n            'name' : subms[js], 'color' : colors[js],\n            'q_in' : [quant(i,js,subms,cols) for i in range(len(subms))]\n        }\n    alls = pd.read_csv(f'tida_desc.csv')\n    matrix = [ast.literal_eval(str(row.alls)) for row in alls.itertuples()]\n    subms = sorted(matrix[0])\n    cols = [[data[i] for data in matrix] for i in range(len(subms))]\n    df_subms = pd.DataFrame({f'col_{i}': [x[i] for x in matrix] for i in range(len(subms))})\n    dossiers = [dossier(js,subms,cols) for js in range(len(subms))]\n    subm_names = [one_dossier['name'] for one_dossier in dossiers]\n    figures1,qss,i = [],[],0\n    height = 101 if len(colors)==2\\\n        else 134 if len(colors)==3 else (154 if len(colors)==4 else 174)\n    for one_dossier in dossiers: \n        i_col = 'alls. ' + str(one_dossier['q_in'][i]['c'])\n        qs = [one['q'] for one in one_dossier['q_in']]\n        x_names = [name.replace(\"Group\",\"\").replace(\"subm_\",\"\") for name in subm_names]\n        width = 157  if len(colors) == 5\\\n            else (121 if len(colors) == 8\\\n            else (131 if len(colors) == 9\\\n            else (141 if len(colors) == 10\\\n            else (171 if len(colors) == 11 else 111))))\n        f = figure(x_range=x_names,width=width, height=height, title=i_col)\n        f.vbar(x=x_names, width=0.585, top=qs, color=colors)\n        figures1.append(f)\n        qss.append(qs)\n        i+=1\n    grid = gridplot([figures1])\n    output_file('tida_alls.html')\n    if show_figures1 == True: show(grid)\n    sub_wts = params['subwts']\n    main_wts = [subm['weight'] for subm in params['subm']]\n    mms,acc_mass = [],[]\n    for j in range(len(dossiers)):\n        one_dossier = dossiers[j]\n        qs = [one['q'] for one in one_dossier['q_in']]\n        mm = [qs[h] * (main_wts[j] + sub_wts[h]) for h in range(len(sub_wts))]\n        mass = sum(mm)\n        mms.append(mm)\n        acc_mass.append(round(mass))                        #subm_names[::-1]\n    y_names = [name + \" - \" + str(mass) for name,mass in zip(subm_names,acc_mass)]\n    f1 = figure(y_range=y_names, width=313, height=height, title='relations of general masses')\n    f1.hbar(y=y_names, height=0.585, right=acc_mass, left=0, color=colors)\n    output_file('tida_alls2.html')\n    alls = [f'alls.{i}' for i in range(len(dossiers))]\n    subm = [f'sub{i}'   for i in range(len(dossiers))] \n    mmsT  = np.asarray(mms).T\n    data = {'cols' : alls}\n    for i in range(len(dossiers)): data[f'sub{i}'] = mmsT[i,:]\n    f2 = figure(y_range=alls, height=height, width=274, title=\" ( relations of columns masses )\")\n    f2.hbar_stack(subm, y='cols', height=0.585, color=colors, source=data)\n    qssT  = np.asarray(qss).T\n    data = {'cols' : alls}\n    for i in range(len(dossiers)): data[f'sub{i}'] = qssT[i,:]\n    f3 = figure(y_range=alls, height=height, width=215, title=\"ratios in columns\")\n    f3.hbar_stack(subm, y='cols', height=0.585, color=colors, source=data)\n    grid = gridplot([[f3,f2,f1]])\n    show(grid)\n    if show_figures2 == True:\n        def read(params,i):\n            FiN = params[\"path\"] + params[\"subm\"][i][\"name\"] + \".csv\"\n            target_name_back = {'target':params[\"target\"],'pred':params[\"target\"]}\n            return pd.read_csv(FiN).rename(columns=target_name_back)\n        dfs = [read(params,i) for i in range(len(params[\"subm\"]))] + [df_cross]\n        f   = figure(width=800, height=274)\n        f.title.text = 'Click on legend entries to mute the corresponding lines'\n        b,e        = 21000,21021\n        line_x     = [dfs[i][b:e]['id']    for i in range(len(dfs))]\n        line_y     = [dfs[i][b:e]['value'] for i in range(len(dfs))]\n        color      = colors + [color_cross]\n        alpha      = [0.8 for i in range(len(dfs)-1)] + [0.95]\n        lws        = [1.0 for i in range(len(dfs)-1)] + [1.00]\n        legend = subm_names + ['cross']\n        for i in range(len(legend)):\n            f.line(line_x[i], line_y[i], line_width=lws[i], color=color[i], alpha=alpha[i],\n                   muted_color='white',legend_label=legend[i])\n        f.legend.location = \"top_left\"\n        f.legend.click_policy=\"mute\"\n        show(f)\n\n\ndef color_scheme(dk,color):\n    colors    = ['red','green','blue','silver','gold']\n    clr_Red   = [\"crimson\",\"orangered\",\"red\",'tomato',\"firebrick\",]\n    clr_Green = [\"green\",\"limegreen\",\"darkgreen\",\"forestgreen\",'lime']\n    clr_Blue  = [\"mediumblue\",\"steelblue\",\"blue\",\"royalblue\",'midnightblue']\n    clr_silver= ['gray','darkgray','gainsboro','silver','whitesmoke']\n    clr_alls  = ['crimson',\"darkgreen\",'mediumblue',\"darkmagenta\"]\n    clr_Brown = [\"maroon\",\"sienna\",\"sandybrown\",\"chocolate\",'brown',]\n    l = len(dk['subm'])\n    if color == 'alls'  : colors = clr_alls   [0:l]\n    if color == 'red'   : colors = clr_Red    [0:l]\n    if color == 'green' : colors = clr_Green  [0:l]\n    if color == 'blue'  : colors = clr_Blue   [0:l]\n    if color == 'brown' : colors = clr_Brown  [0:l]\n    if color == 'silver': colors = clr_silver [0:l]\n    return colors\n\n\ndef h_blend(params,color,cross='silver',\n            figures1=False,figures2=False,wf2=555,\n            details=False):\n\n    import copy\n\n    color_cross = cross\n\n    dk = copy.deepcopy(params)\n\n    show_details,show_figures1,show_figures2 = details,figures1,figures2\n\n    file_short_names = [subm['name'] for subm in params['subm']]\n    type_sort    = params['type_sort'][0]\n    dk['asc']    = params['type_sort'][1]\n    dk['desc']   = params['type_sort'][2]\n    dk['id']     = params['id_target'][0]\n    dk['target'] = params['id_target'][1]\n# ------------------------------------------------------------------------\n    def read(dk,i):\n        tnm = dk[\"subm\"][i][\"name\"]\n        FiN = dk[\"path\"] + tnm + \".csv\"\n        return pd.read_csv(FiN).rename(columns={\n            'target':tnm, 'pred':tnm, dk[\"target\"]:tnm})\n        \n    def merge(dfs_subm):\n        df_subms = pd.merge(dfs_subm[0],  dfs_subm[1], on=[dk['id']])\n        for i in range(2, len(dk[\"subm\"])): \n            df_subms = pd.merge(df_subms, dfs_subm[i], on=[dk['id']])\n        return df_subms\n        \n    def da(dk,sorting_direction,show_details):\n        \n        df_subms = merge([read(dk,i) for i in range(len(dk[\"subm\"]))])\n        cols = [col for col in df_subms.columns if col != dk['id']]\n        short_name_cols = [c for c in cols]\n        \n        def alls1(x, sd=sorting_direction,cs=cols):\n            reverse = True if sd=='desc' else False\n            tes = {c: x[c] for c in cs}.items()\n            subms_sorted = [t[0] for t in sorted(tes,key=lambda k:k[1],reverse=reverse)]\n            return subms_sorted\n\n        import random\n\n        def alls2(x, sd=sorting_direction,cs=cols):\n            reverse = True if sd=='desc' else False\n            tes = {c: x[c] for c in cs}.items()\n            subms_random = [t[0] for t in tes]\n            random.shuffle(subms_random)\n            return subms_random\n\n        alls = alls1 if type_sort == 'asc/desc' else alls2\n            \n        def summa(x,cs,wts,ic_alls): \n            return sum([x[cs[j]] * (wts[0][j] + wts[1][ic_alls[j]]) for j in range(len(cs))])\n            \n        wts = [[[e['weight'] for e in dk[\"subm\"]], [w for w in dk[\"subwts\" ]]]]\n          \n        def correct(x, cs=cols, wts=wts):\n            i = [x['alls'].index(c) for c in short_name_cols]\n            return summa(x,cs,wts[0],i)\n\n        if len(wts) == 1:\n            correct_sub_weights = [wt for wt in dk[\"subwts\"]]\n            weights = [subm['weight'] for subm in dk[\"subm\"]]\n            def correct(x, cs=cols, w=weights, cw=correct_sub_weights):\n                ic = [x['alls'].index(c) for c in short_name_cols]\n                cS = [x[cols[j]] * (w[j] + cw[ic[j]]) for j in range(len(cols))]\n                return sum(cS)\n                   \n        def amxm(x, cs=cols):\n            list_values = x[cs].to_list()\n            mxm = abs(max(list_values)-min(list_values))\n            return mxm\n\n        if len(wts) > 1:\n            df_subms['mx-m']   = df_subms.apply(lambda x: amxm   (x), axis=1)\n        df_subms['alls']       = df_subms.apply(lambda x: alls   (x), axis=1)\n        df_subms[dk[\"target\"]] = df_subms.apply(lambda x: correct(x), axis=1)\n        schema_rename = { old_nc:new_shnc for old_nc, new_shnc in zip(cols, short_name_cols) }\n        df_subms = df_subms.rename(columns=schema_rename)\n        df_subms = df_subms.rename(columns={dk[\"target\"]:\"ensemble\"})\n        df_subms.insert(loc=1, column=' _ ', value=['   '] * len(df_subms))\n        df_subms[' _ '] = df_subms[' _ '].astype(str)\n        pd.set_option('display.max_rows',100)\n        pd.set_option('display.float_format', '{:.15f}'.format)\n        vcols = [dk['id']]+[' _ '] + short_name_cols + [' _ ']+['alls']+[' _ ']+['ensemble']\n        if len(wts) > 1: vcols.append([' _ '] + ['mx-m'])\n        df_subms = df_subms[vcols]\n        if show_details and sorting_direction=='asc': display(df_subms.head(3))\n        df_subms = df_subms.rename(columns={\"ensemble\":dk[\"target\"]})\n        for snc in short_name_cols: df_subms[snc] = df_subms[snc].round(15)\n        df_subms.to_csv(f'tida_{sorting_direction}.csv', index=False)\n        return df_subms[[dk['id'],dk['target']]]\n   \n    def ensemble_da(dk,        show_details): \n        dfD    = da(dk,'desc', show_details)\n        dfA    = da(dk,'asc',  show_details)\n        dfA[dk['target']] = dk['desc']*dfD[dk['target']] + dfA[dk['target']]*dk['asc']\n        return dfA\n\n    da = ensemble_da(dk,show_details)\n    colors = color_scheme(dk, color)\n    bokeh_show(dk, da, colors, show_figures1, show_figures2, wf2, color_cross)\n    return  da\n\n\ndef matrix_vs(path,fs_names):\n    def load(path,fs_names):\n        dfs = [pd.read_csv(path + name_subm +'.csv') for name_subm in fs_names]\n        for i in range(len(dfs)):\n            dfs[i] = dfs[i].rename(columns={\"accident_risk\": f'{fs_names[i]}'})\n        dfsm = pd.merge(dfs[0], dfs[1], on=\"id\")\n        for i in range(2,len(dfs)):\n            dfsm = pd.merge(dfsm,dfs[i],on='id')\n        return dfsm   \n    def make_list_vs(fs_names):\n        list = []\n        for i in range(0,len(fs_names)-1):\n            for j in range(i+1,len(fs_names)):\n                list.append(fs_names[i] + \"_vs_\" + fs_names[j])\n        return list\n    def get_mvs(dfs, list_vs):\n        def get_abs_distance(x,t1,t2):\n            return abs(x[t1]-x[t2])\n        for vs in list_vs:\n            t = vs.split('_vs_')\n            dfs[vs] = dfs.apply(lambda x: get_abs_distance(x,t[0],t[1]), axis=1)\n        return dfs   \n    def distance_vs(name, st_names, list_vs, dfs):\n        distances = []\n        for st in st_names:\n            vs_between = name + \"_vs_\" + st\n            if vs_between not in list_vs:\n                distances.append(0)\n            else: distances.append(round(dfs[vs_between].sum()))\n        return distances\n    dfs = load(path,fs_names)\n    list_vs = make_list_vs(fs_names)\n    mvs = get_mvs(dfs, list_vs)\n    m1 = pd.DataFrame({'subm':fs_names})\n    m2 = pd.DataFrame({ name :distance_vs(name, fs_names, list_vs, mvs) for name in fs_names})\n    matrix = pd.concat([m1,m2],axis=1)\n    return matrix","metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-10-25T10:58:21.418805Z","iopub.execute_input":"2025-10-25T10:58:21.419277Z","iopub.status.idle":"2025-10-25T10:58:21.571935Z","shell.execute_reply.started":"2025-10-25T10:58:21.419249Z","shell.execute_reply":"2025-10-25T10:58:21.570284Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# subm_1[\"value\"] = subm_original2['value'] + subm_original1['value'] / 100_000\n# subm_2[\"value\"] = subm_original2['value']\n\n# subm_1.to_csv('subm_1.csv',index=False)\n# subm_2.to_csv('subm_2.csv',index=False)\n\n# params = {\n#       'path'     : '/kaggle/working/',                           # v6.lb=0.11\n#       'id_target': ['id',\"value\"],          \n#       'type_sort': ['asc/desc',0.30,0.70],\n#       'subwts'   : [ +0.002,-0.002 ],       \n#       'subm'     : [\n#          { 'name': f'subm_1','weight': 0.003 },\n#          { 'name': f'subm_2','weight': 0.997 },]\n# }\n# df_cross = h_blend(params, color='alls', figures1=True, details=True)\n\n#------------------------------------------------------------------------------\n\n# subm_1[\"value\"] = subm_original2['value'] + subm_original1['value'] / 10_000_000\n# subm_2[\"value\"] = subm_original2['value']\n\n# subm_1.to_csv('subm_1.csv',index=False)\n# subm_2.to_csv('subm_2.csv',index=False)\n\n# params = {\n#       'path'     : '/kaggle/working/',                            # v7.lb=0.11\n#       'id_target': ['id',\"value\"],          \n#       'type_sort': ['asc/desc',0.30,0.70],\n#       'subwts'   : [ +0.00002,-0.00002 ],       \n#       'subm'     : [\n#          { 'name': f'subm_1','weight': 0.00003 },\n#          { 'name': f'subm_2','weight': 0.99997 },]\n# }\n# df_cross = h_blend(params, color='alls', figures1=True, details=True)\n\n\n#------------------------------------------------------------------------------\n\n# subm_1[\"value\"] = subm_original2['value'] + subm_original1['value'] / 10_000_000_000\n# subm_2[\"value\"] = subm_original2['value']\n\n# subm_1.to_csv('subm_1.csv',index=False)\n# subm_2.to_csv('subm_2.csv',index=False)\n\n# params = {\n#       'path'     : '/kaggle/working/',                            # v8.lb=0.11\n#       'id_target': ['id',\"value\"],          \n#       'type_sort': ['asc/desc',0.30,0.70],\n#       'subwts'   : [ +0.00000001,-0.00000001 ],       \n#       'subm'     : [\n#          { 'name': f'subm_1','weight': 0.00000001 },\n#          { 'name': f'subm_2','weight': 0.99999999 },]\n# }\n# df_cross = h_blend(params, color='alls', figures1=True, details=True)\n\n#------------------------------------------------------------------------------\n\nsubm_1[\"value\"] = subm_original2['value'] + subm_original1['value'] / 100_000\nsubm_2[\"value\"] = subm_original2['value']\n\nsubm_1.to_csv('subm_1.csv',index=False)\nsubm_2.to_csv('subm_2.csv',index=False)\n\nparams = {\n      'path'     : '/kaggle/working/',                           # v12.lb=0.11\n      'id_target': ['id',\"value\"],          \n      'type_sort': ['asc/desc',0.500001,0.499999],\n      'subwts'   : [ +0.001,-0.001 ],       \n      'subm'     : [\n         { 'name': f'subm_1','weight': 0.001 },\n         { 'name': f'subm_2','weight': 0.999 },]\n}\ndf_cross = h_blend(params, color='alls', figures1=True, details=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:40:40.359853Z","iopub.execute_input":"2025-10-25T11:40:40.36018Z","iopub.status.idle":"2025-10-25T11:40:48.173103Z","shell.execute_reply.started":"2025-10-25T11:40:40.360158Z","shell.execute_reply":"2025-10-25T11:40:48.172151Z"},"jupyter":{"source_hidden":true},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(1,3): os.remove(f'/kaggle/working/subm_{i}.csv')\n\ndf_cross.to_csv('submission.csv', index=False)\ndf_cross.head(11)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:40:53.054173Z","iopub.execute_input":"2025-10-25T11:40:53.054588Z","iopub.status.idle":"2025-10-25T11:40:53.247095Z","shell.execute_reply.started":"2025-10-25T11:40:53.054564Z","shell.execute_reply":"2025-10-25T11:40:53.246152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nsubm_original2['value'] =\\\n    subm_original2['value'] * random.choice([1.00007,1.00008,1.00010]) * 0.50 +\\\n    subm_original2['value'] / random.choice([1.00007,1.00008,1.00010]) * 0.50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:57:06.70979Z","iopub.execute_input":"2025-10-25T11:57:06.710102Z","iopub.status.idle":"2025-10-25T11:57:06.71748Z","shell.execute_reply.started":"2025-10-25T11:57:06.710082Z","shell.execute_reply":"2025-10-25T11:57:06.716639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm_original2.to_csv('submission.csv', index=False)\nsubm_original2.head(11)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:57:11.37656Z","iopub.execute_input":"2025-10-25T11:57:11.376907Z","iopub.status.idle":"2025-10-25T11:57:11.571585Z","shell.execute_reply.started":"2025-10-25T11:57:11.376882Z","shell.execute_reply":"2025-10-25T11:57:11.570498Z"}},"outputs":[],"execution_count":null}]}