{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97984,"databundleVersionId":14096757,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"299fbeca","cell_type":"markdown","source":"\n# ü©∫ PhysioNet ‚Äì ECG **Image Digitization + Reconstruction** (Hybrid)\n\n> üí° **If this notebook helps you understand ECG reconstruction and image digitization, please consider giving it an upvote.**\n> It helps others discover it and keeps me motivated to share more.\n\n**What this does:**  \n- Extracts **12-lead time-series** directly from **paper ECG images** (scans/photos) using a fast OpenCV pipeline  \n- Builds **per-lead median templates** from training CSVs  \n- Reconstructs test signals via **BPM-tiling + micro-ensemble**  \n- **Hybrid blend:** combines the image-digitized traces with the template-based reconstruction  \n- Enforces **Einthoven‚Äôs law** for limb-lead consistency  \n- Writes a valid **`submission.csv`**\n\nüì¶ **Kaggle constraints:** No internet, ‚â§9h runtime.  \n","metadata":{}},{"id":"24698807","cell_type":"markdown","source":"\n## üìë Contents\n1. [Config](#config)  \n2. [Utilities (Signals)](#utils-signals)  \n3. [Utilities (Images)](#utils-images)  \n4. [Template Building from Training CSVs](#templates)  \n5. [Reconstruction (BPM tiling + Ensemble + Einthoven)](#recon)  \n6. [Image Digitization (OpenCV pipeline + model hook)](#digitize)  \n7. [Hybrid Blending](#hybrid)  \n8. [Predict & Create `submission.csv`](#submission)  \n9. [Visual Checks](#viz)  \n10. [Notes & Next Steps](#next)\n","metadata":{}},{"id":"3f214a65","cell_type":"markdown","source":"## ‚öôÔ∏è Step 1 ‚Äî Config","metadata":{}},{"id":"a623f1a3","cell_type":"code","source":"\nimport os, math, random, glob, cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom scipy.signal import butter, filtfilt, find_peaks\n\n# Kaggle paths\nINPUT_DIR = Path(\"/kaggle/input/physionet-ecg-image-digitization\")\nTRAIN_CSV = INPUT_DIR / \"train.csv\"\nTRAIN_DIR = INPUT_DIR / \"train\"\nTEST_CSV  = INPUT_DIR / \"test.csv\"\nTEST_IMG_DIR = INPUT_DIR / \"test_images\"\n\n# Output\nWORK_DIR = Path(\"/kaggle/working\")\nWORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# Leads & defaults\nLEADS = [\"I\",\"II\",\"III\",\"aVR\",\"aVL\",\"aVF\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]\n\n# Reconstruction params\nR_PRE_S, R_POST_S = 0.20, 0.40\nBEAT_LEN = 360\nBP_LO_HZ, BP_HI_HZ, BP_ORDER = 5.0, 25.0, 2\nBPM_CANDIDATES = [55, 65, 75, 85, 95]\nENSEMBLE_W = np.array([0.5, 0.3, 0.2], dtype=np.float32)\nEINTHOVEN_BLEND_W = 0.6\nMIN_VAL, MAX_VAL = 0.0, 0.09\n\n# Image-digitization params\nDEFAULT_FS = 500\nDEFAULT_DURATION_S = 10.0\nASSUME_MM_PER_S = 25.0\nASSUME_MM_PER_MV = 10.0\n\n# Reproducibility\nos.environ[\"PYTHONHASHSEED\"] = \"0\"\nrandom.seed(0); np.random.seed(0)\nnp.set_printoptions(suppress=True, floatmode=\"fixed\", precision=6)\n\nprint(\"‚úÖ Paths:\")\nprint(\" - train.csv:\", TRAIN_CSV.exists())\nprint(\" - test.csv:\", TEST_CSV.exists())\nprint(\" - test_images dir:\", TEST_IMG_DIR.exists())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:05.311788Z","iopub.execute_input":"2025-10-26T21:56:05.312108Z","iopub.status.idle":"2025-10-26T21:56:05.324547Z","shell.execute_reply.started":"2025-10-26T21:56:05.312088Z","shell.execute_reply":"2025-10-26T21:56:05.323764Z"}},"outputs":[],"execution_count":null},{"id":"5a48c256","cell_type":"markdown","source":"## üßÆ Step 2 ‚Äî Utilities (Signals)\n<details><summary>Show / hide helpers</summary>","metadata":{}},{"id":"74c0613f","cell_type":"code","source":"\ndef zscore(x):\n    x = np.asarray(x, np.float32)\n    s = np.std(x) + 1e-8\n    return (x - np.mean(x)) / s\n\ndef bandpass(x, fs, lo=5.0, hi=25.0, order=2):\n    if len(x) < 10: return x.astype(np.float32)\n    nyq = 0.5 * fs\n    lo = max(lo/nyq, 1e-3); hi = min(hi/nyq, 0.99)\n    b, a = butter(order, [lo, hi], btype='band')\n    return filtfilt(b, a, x).astype(np.float32)\n\ndef apply_lowpass(x, fs, cutoff=15.0, order=2):\n    if len(x) <= 10: return x.astype(np.float32)\n    nyq = 0.5 * fs; wn = min(cutoff/nyq, 0.99)\n    b, a = butter(order, wn, btype='low')\n    return filtfilt(b, a, x).astype(np.float32)\n\ndef resample_to_length(x, n):\n    if len(x) == n: return x.astype(np.float32)\n    return np.interp(\n        np.linspace(0, 1, n, dtype=np.float32),\n        np.linspace(0, 1, len(x), dtype=np.float32),\n        x.astype(np.float32)\n    ).astype(np.float32)\n\ndef soft_minmax_scale(x, lo=0.0, hi=0.09):\n    x = np.asarray(x, np.float32)\n    if x.size == 0:\n        return np.full(1, (lo + hi) / 2, np.float32)\n    mn, mx = float(np.nanmin(x)), float(np.nanmax(x))\n    if not np.isfinite(mn) or not np.isfinite(mx) or mx <= mn:\n        return np.full_like(x, (lo + hi) / 2, np.float32)\n    y = (x - mn) / (mx - mn)\n    return np.clip(lo + y * (hi - lo), lo, hi).astype(np.float32)\n\ndef derive_limb_leads_from_I_II(yI, yII):\n    \"\"\"Safely derive limb leads from I and II, resampling if needed.\"\"\"\n    yI = np.asarray(yI, np.float32)\n    yII = np.asarray(yII, np.float32)\n    # Resample to common length if mismatch\n    if len(yI) != len(yII):\n        n_common = min(len(yI), len(yII))\n        yI = resample_to_length(yI, n_common)\n        yII = resample_to_length(yII, n_common)\n    III = yII - yI\n    aVR = -(yI + yII) / 2.0\n    aVL = yI - 0.5 * yII\n    aVF = yII - 0.5 * yI\n    return {'III': III, 'aVR': aVR, 'aVL': aVL, 'aVF': aVF}\n\n\ndef soft_blend(a, b, w):\n    return (1.0 - float(w)) * a + float(w) * b\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:05.326334Z","iopub.execute_input":"2025-10-26T21:56:05.326744Z","iopub.status.idle":"2025-10-26T21:56:05.342548Z","shell.execute_reply.started":"2025-10-26T21:56:05.326723Z","shell.execute_reply":"2025-10-26T21:56:05.341678Z"}},"outputs":[],"execution_count":null},{"id":"173b8f72","cell_type":"markdown","source":"</details>","metadata":{}},{"id":"f406fadb","cell_type":"markdown","source":"## ü©ª Step 3 ‚Äî Utilities (Images)\n<details><summary>Show / hide helpers</summary>","metadata":{}},{"id":"4fca1cef","cell_type":"code","source":"\ndef deskew_image(img_bgr):\n    g = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n    g = cv2.GaussianBlur(g, (3,3), 0)\n    edges = cv2.Canny(g, 50, 150, apertureSize=3)\n    lines = cv2.HoughLines(edges, 1, np.pi/180, 200)\n    angle = 0.0\n    if lines is not None and len(lines) > 0:\n        thetas = [l[0][1] for l in lines if abs(l[0][1] - np.pi/2) > np.deg2rad(30)]\n        if len(thetas) > 0:\n            theta = float(np.median(thetas))\n            angle = (theta - np.pi/2) * 180/np.pi\n    (h, w) = img_bgr.shape[:2]\n    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n    return cv2.warpAffine(img_bgr, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n\ndef estimate_pixels_per_mm(gray):\n    col = gray.mean(axis=0)\n    col = (col - col.mean()) / (col.std() + 1e-6)\n    ac = np.correlate(col, col, mode=\"full\")\n    ac = ac[ac.size//2:]\n    peaks = np.argpartition(ac, -10)[-10:]\n    peaks = np.sort(peaks)\n    diffs = np.diff(peaks)\n    diffs = diffs[diffs > 2]\n    if len(diffs) == 0:\n        return 20.0  # fallback\n    return float(np.median(diffs))\n\ndef extract_waveform_mask(img_bgr):\n    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n    mask1 = cv2.inRange(hsv, (0,50,50), (10,255,255))\n    mask2 = cv2.inRange(hsv, (170,50,50), (180,255,255))\n    mask_red = cv2.bitwise_or(mask1, mask2)\n    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n    _, mask_dark = cv2.threshold(gray, 70, 255, cv2.THRESH_BINARY_INV)\n    mask = cv2.bitwise_or(mask_red, mask_dark)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((2,2),np.uint8))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((2,2),np.uint8))\n    return mask\n\ndef split_into_leads_naive(img_bgr):\n    H, W = img_bgr.shape[:2]\n    panels = {}\n    idx = 0\n    for r in range(3):\n        for c in range(4):\n            x0 = int(c*W/4); x1 = int((c+1)*W/4)\n            y0 = int(r*H/3); y1 = int((r+1)*H/3)\n            if idx < len(LEADS):\n                panels[LEADS[idx]] = img_bgr[y0:y1, x0:x1].copy()\n                idx += 1\n    return panels\n\ndef mask_to_timeseries(mask, fs=500, duration_s=10.0, pixels_per_mm=20.0):\n    ys, xs = np.where(mask > 0)\n    n_out = int(round(fs * duration_s))\n    if len(xs) < 10:\n        return np.zeros(n_out, np.float32)\n    yx = []\n    for x in range(mask.shape[1]):\n        ys_col = ys[xs == x]\n        yx.append(np.median(ys_col) if len(ys_col) else np.nan)\n    yx = pd.Series(yx).interpolate(limit_direction=\"both\").to_numpy(np.float32)\n    s_per_mm = 1.0 / 25.0\n    t_pix = np.arange(len(yx)) / float(pixels_per_mm) * s_per_mm\n    v_pix = (np.nanmean(yx) - yx) / float(pixels_per_mm) * 0.1\n    t_uniform = np.linspace(0.0, duration_s, n_out, dtype=np.float32)\n    if np.ptp(t_pix) < 1e-6:\n        return np.zeros(n_out, np.float32)\n    v_uniform = np.interp(t_uniform, np.linspace(0.0, duration_s, len(v_pix), dtype=np.float32), v_pix)\n    return v_uniform.astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:05.343374Z","iopub.execute_input":"2025-10-26T21:56:05.343665Z","iopub.status.idle":"2025-10-26T21:56:05.360361Z","shell.execute_reply.started":"2025-10-26T21:56:05.343637Z","shell.execute_reply":"2025-10-26T21:56:05.359493Z"}},"outputs":[],"execution_count":null},{"id":"6d5eea3f","cell_type":"markdown","source":"</details>","metadata":{}},{"id":"a701c8aa","cell_type":"markdown","source":"## ü´Ä Step 4 ‚Äî Templates from Training CSVs","metadata":{}},{"id":"b6ffd84d","cell_type":"code","source":"\ndef build_per_lead_stats_and_beats(train_csv, train_dir, leads=LEADS):\n    meta = pd.read_csv(train_csv)\n    lead_vals = {ld: [] for ld in leads}\n    lead_beats = {ld: [] for ld in leads}\n    lead_bpm_samples = {ld: [] for ld in leads}\n    for row in tqdm(meta.itertuples(index=False), total=len(meta), desc=\"Scan train\"):\n        rid = str(row.id); fs = int(row.fs)\n        csvp = Path(train_dir) / rid / f\"{rid}.csv\"\n        if not csvp.exists(): continue\n        try: df = pd.read_csv(csvp)\n        except: continue\n        for ld in leads:\n            if ld not in df.columns: continue\n            y = df[ld].dropna().to_numpy(np.float32)\n            if y.size < 200: continue\n            lead_vals[ld].append(y)\n            y_bp = bandpass(zscore(y), fs)\n            iqr = np.subtract(*np.percentile(y_bp, [75,25]))\n            scale = iqr if np.isfinite(iqr) and iqr>0 else np.std(y_bp)\n            prominence = max(0.35*scale, 0.12)\n            distance = int(max(0.32*fs,1))\n            pks,_ = find_peaks(y_bp, distance=distance, prominence=prominence)\n            if len(pks) < 2: continue\n            rr = np.diff(pks) / float(fs)\n            rr = rr[(rr>0.3)&(rr<2.0)]\n            if rr.size >= 1:\n                lead_bpm_samples[ld].append(float(np.clip(60.0/np.median(rr), 40.0, 160.0)))\n            n_pre, n_post = int(round(0.20*fs)), int(round(0.40*fs))\n            for pk in pks:\n                a, b = pk-n_pre, pk+n_post\n                if a < 0 or b >= len(y): continue\n                seg = y[a:b+1].astype(np.float32)\n                lead_beats[ld].append(resample_to_length(seg, 360))\n\n    lead_stats = {}\n    for ld in leads:\n        if len(lead_vals[ld]) == 0:\n            lead_stats[ld] = {'mean':0.0,'std':0.1}\n        else:\n            vals = np.concatenate(lead_vals[ld]).astype(np.float32)\n            lead_stats[ld] = {'mean': float(np.mean(vals)), 'std': float(np.std(vals))}\n    return lead_stats, lead_beats, lead_bpm_samples\n\ndef build_lead_templates(lead_beats, leads=LEADS):\n    lead_template = {}\n    for ld in leads:\n        if len(lead_beats[ld]) > 0:\n            arr = np.vstack(lead_beats[ld]).astype(np.float32)\n            tpl = np.median(arr, axis=0).astype(np.float32)\n        else:\n            t = np.linspace(0, 1, 360, dtype=np.float32)\n            tpl = np.sin(2*np.pi*t).astype(np.float32)\n        lead_template[ld] = zscore(tpl)\n    return lead_template\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:05.470566Z","iopub.execute_input":"2025-10-26T21:56:05.470854Z","iopub.status.idle":"2025-10-26T21:56:05.484087Z","shell.execute_reply.started":"2025-10-26T21:56:05.470835Z","shell.execute_reply":"2025-10-26T21:56:05.483154Z"}},"outputs":[],"execution_count":null},{"id":"893bb88f","cell_type":"code","source":"\nprint(\"‚öôÔ∏è [1/6] Scanning training to build templates...\")\nlead_stats, lead_beats, lead_bpms = build_per_lead_stats_and_beats(TRAIN_CSV, TRAIN_DIR, LEADS)\nlead_template = build_lead_templates(lead_beats, LEADS)\nmean_templates = {ld: (np.mean(np.vstack(v), axis=0) if len(v)>0 else lead_template.get('II')) \n                  for ld, v in lead_beats.items()}\nper_lead_bpm_prior = {ld: (float(np.median(v)) if len(v)>0 else 75.0) for ld, v in lead_bpms.items()}\nprint(\"Templates built for\", len(lead_template), \"leads.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:05.485494Z","iopub.execute_input":"2025-10-26T21:56:05.485749Z","iopub.status.idle":"2025-10-26T21:56:33.394633Z","shell.execute_reply.started":"2025-10-26T21:56:05.485732Z","shell.execute_reply":"2025-10-26T21:56:33.393761Z"}},"outputs":[],"execution_count":null},{"id":"0cbdd004","cell_type":"markdown","source":"## üí´ Step 5 ‚Äî Reconstruction (BPM tiling + Ensemble + Einthoven)\n<details><summary>Show / hide code</summary>","metadata":{}},{"id":"4026dd24","cell_type":"code","source":"\ndef tile_template(template_beat, fs, n_out, bpm, amp=1.0):\n    beat_samples = max(4, int(round((60.0 / max(bpm, 1e-6)) * fs)))\n    one = np.interp(\n        np.linspace(0, 1, beat_samples, dtype=np.float32),\n        np.linspace(0, 1, len(template_beat), dtype=np.float32),\n        template_beat\n    ).astype(np.float32)\n    reps = int(np.ceil(n_out / len(one)))\n    y = np.tile(one, reps)[:n_out]\n    y = zscore(y) * float(amp)\n    return y.astype(np.float32)\n\ndef _nextpow2(n):\n    return 1 << (int(math.ceil(math.log2(max(1, n)))))\n\ndef autocorr_peak_score(y, fs, min_rr_s=0.35, max_rr_s=1.5):\n    y = zscore(y).astype(np.float32); n = len(y)\n    if n < 16: return 0.0\n    m = _nextpow2(2*n - 1)\n    Y = np.fft.rfft(y, n=m)\n    ac_full = np.fft.irfft(Y * np.conj(Y), n=m)\n    ac = ac_full[:n]\n    lo = int(max(min_rr_s * fs, 1)); hi = int(min(max_rr_s * fs, n - 1))\n    if hi <= lo: return 0.0\n    seg = ac[lo:hi]\n    if seg.size == 0: return 0.0\n    peak = float(seg.max()); norm = float(ac[0]) + 1e-8\n    return float(np.clip(peak / norm, 0.0, 1.0))\n\ndef choose_best_bpm(template_beat, fs, n_out, bpm_list=[55,65,75,85,95]):\n    best_bpm, best_score, best_y = None, -1.0, None\n    for bpm in bpm_list:\n        y = tile_template(template_beat, fs, n_out, bpm, amp=1.0)\n        sc = autocorr_peak_score(y, fs)\n        if sc > best_score:\n            best_bpm, best_score, best_y = bpm, sc, y\n    return best_bpm, best_y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:33.395461Z","iopub.execute_input":"2025-10-26T21:56:33.395728Z","iopub.status.idle":"2025-10-26T21:56:33.407131Z","shell.execute_reply.started":"2025-10-26T21:56:33.395708Z","shell.execute_reply":"2025-10-26T21:56:33.406168Z"}},"outputs":[],"execution_count":null},{"id":"694cb92b","cell_type":"markdown","source":"</details>","metadata":{}},{"id":"1362c570","cell_type":"markdown","source":"## ü©ª Step 6 ‚Äî Image Digitization (OpenCV + optional model hook)\n<details><summary>Show / hide code</summary>","metadata":{}},{"id":"c85d1c03","cell_type":"code","source":"\nUSE_SEGMENTATION_MODEL = False  # Set True when you attach your weights and implement the hook.\n\ndef init_segmentor_if_available():\n    if not USE_SEGMENTATION_MODEL:\n        return None\n    # Example:\n    # from my_digitizer import load_model\n    # return load_model(\"/kaggle/input/my-weights/model.pth\")\n    return None\n\ndef segment_mask_with_model_or_classical(segmentor, img_bgr):\n    if segmentor is None:\n        return extract_waveform_mask(img_bgr)\n    # Example:\n    # return segmentor.predict_mask(img_bgr)\n    return extract_waveform_mask(img_bgr)\n\ndef digitize_image_to_leads(img_bgr, fs=500, duration_s=10.0, einthoven_blend_w=0.0, segmentor=None):\n    img = deskew_image(img_bgr)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    ppm = estimate_pixels_per_mm(gray)\n    panels = split_into_leads_naive(img)\n    traces = {}\n    for ld, pimg in panels.items():\n        mask = segment_mask_with_model_or_classical(segmentor, pimg)\n        y = mask_to_timeseries(mask, fs=fs, duration_s=duration_s, pixels_per_mm=ppm)\n        y = apply_lowpass(zscore(y), fs)\n        traces[ld] = y\n    if einthoven_blend_w > 0.0 and (\"I\" in traces) and (\"II\" in traces):\n        derived = derive_limb_leads_from_I_II(traces[\"I\"], traces[\"II\"])\n        for dlead, ydrv in derived.items():\n            ydrv = zscore(ydrv)\n            if dlead in traces and len(traces[dlead]) == len(ydrv):\n                traces[dlead] = soft_blend(traces[dlead], ydrv, einthoven_blend_w)\n            else:\n                traces[dlead] = ydrv\n    for ld in list(traces.keys()):\n        traces[ld] = soft_minmax_scale(traces[ld], 0.0, 0.09)\n    return traces\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:33.408214Z","iopub.execute_input":"2025-10-26T21:56:33.40864Z","iopub.status.idle":"2025-10-26T21:56:33.417578Z","shell.execute_reply.started":"2025-10-26T21:56:33.408615Z","shell.execute_reply":"2025-10-26T21:56:33.416756Z"}},"outputs":[],"execution_count":null},{"id":"1e0a5854","cell_type":"markdown","source":"</details>","metadata":{}},{"id":"91706273","cell_type":"markdown","source":"## üîÄ Step 7 ‚Äî Hybrid Blending","metadata":{}},{"id":"2a7bc248","cell_type":"code","source":"\ndef hybrid_blend(digitized, reconstructed, w_digitized=0.6):\n    out = {}\n    for ld in LEADS:\n        yd = digitized.get(ld, None)\n        yr = reconstructed.get(ld, None)\n        if yd is None and yr is None:\n            continue\n        if yd is None:\n            out[ld] = yr\n        elif yr is None:\n            out[ld] = yd\n        else:\n            n = max(len(yd), len(yr))\n            yd_rs = resample_to_length(yd, n)\n            yr_rs = resample_to_length(yr, n)\n            out[ld] = soft_minmax_scale(soft_blend(yr_rs, yd_rs, w_digitized), 0.0, 0.09)\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:33.420154Z","iopub.execute_input":"2025-10-26T21:56:33.420448Z","iopub.status.idle":"2025-10-26T21:56:33.42584Z","shell.execute_reply.started":"2025-10-26T21:56:33.420428Z","shell.execute_reply":"2025-10-26T21:56:33.425073Z"}},"outputs":[],"execution_count":null},{"id":"f566f972","cell_type":"markdown","source":"## üì¶ Step 8 ‚Äî Predict & Create `submission.csv","metadata":{}},{"id":"1b48380a","cell_type":"code","source":"\nprint(\"‚öôÔ∏è [2/6] Loading test metadata...\")\ntest_df = pd.read_csv(TEST_CSV)\nby_record = {}\nfor r in test_df.itertuples(index=False):\n    by_record.setdefault(int(r.id), []).append(r)\n\nsegmentor = init_segmentor_if_available()\npred = {}\n\nprint(\"‚öôÔ∏è [3/6] Digitizing images & reconstructing...\")\nfor rid, items in tqdm(by_record.items(), desc=\"Records\"):\n    # Locate test image\n    img_path = None\n    jpg = TEST_IMG_DIR / f\"{rid}.jpg\"\n    png = TEST_IMG_DIR / f\"{rid}.png\"\n    if jpg.exists(): img_path = jpg\n    elif png.exists(): img_path = png\n\n    # Digitization path\n    if img_path is not None:\n        img = cv2.imread(str(img_path))\n        digitized_traces = digitize_image_to_leads(img, fs=500, duration_s=10.0, einthoven_blend_w=0.6, segmentor=segmentor)\n    else:\n        digitized_traces = {}\n\n    # Reconstruction path\n    recon_traces = {}\n    for r in items:\n        lead = str(r.lead); fs = int(r.fs); n = int(r.number_of_rows)\n        tpl = lead_template.get(lead, lead_template.get(\"II\"))\n        best_bpm, y_best = choose_best_bpm(tpl, fs, n)\n        y_fixed = tile_template(tpl, fs, n, per_lead_bpm_prior.get(lead, 75.0))\n        y_mean  = resample_to_length(mean_templates.get(lead, mean_templates.get(\"II\")), n)\n        B,F,M = zscore(apply_lowpass(y_best,fs)), zscore(apply_lowpass(y_fixed,fs)), zscore(apply_lowpass(y_mean,fs))\n        w = np.array([0.5,0.3,0.2], dtype=np.float32); w = w/(np.sum(w)+1e-8)\n        y_syn = (w[0]*B + w[1]*F + w[2]*M).astype(np.float32)\n        recon_traces[lead] = soft_minmax_scale(y_syn, 0.0, 0.09)\n\n    if ('I' in recon_traces) and ('II' in recon_traces) and 0.6>0:\n        derived = derive_limb_leads_from_I_II(recon_traces['I'], recon_traces['II'])\n        for dlead, ydrv in derived.items():\n            if dlead in recon_traces:\n                recon_traces[dlead] = soft_blend(recon_traces[dlead], zscore(ydrv), 0.6)\n            else:\n                recon_traces[dlead] = zscore(ydrv)\n\n    # Hybrid\n    traces = hybrid_blend(digitized_traces, recon_traces, w_digitized=0.6)\n\n    # Store resampled to request\n    for r in items:\n        lead = str(r.lead); n = int(r.number_of_rows)\n        y = traces.get(lead, np.zeros(n, np.float32))\n        if len(y) != n:\n            y = resample_to_length(y, n)\n        pred[(rid, lead)] = soft_minmax_scale(y, 0.0, 0.09)\n\nprint(\"‚öôÔ∏è [4/6] Writing submission.csv ...\")\nrows = []\nfor r in test_df.itertuples(index=False):\n    rid, lead, n = int(r.id), str(r.lead), int(r.number_of_rows)\n    y = pred.get((rid, lead), np.zeros(n, np.float32))\n    if len(y) != n: y = resample_to_length(y, n)\n    for i in range(n):\n        rows.append((f\"{rid}_{i}_{lead}\", float(y[i])))\n\nsub = pd.DataFrame(rows, columns=[\"id\",\"value\"])\nsub_path = WORK_DIR / \"submission.csv\"\nsub.to_csv(sub_path, index=False)\nprint(\"‚úÖ Done:\", sub_path)\nsub.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:33.426707Z","iopub.execute_input":"2025-10-26T21:56:33.426962Z","iopub.status.idle":"2025-10-26T21:56:33.92806Z","shell.execute_reply.started":"2025-10-26T21:56:33.426943Z","shell.execute_reply":"2025-10-26T21:56:33.927196Z"}},"outputs":[],"execution_count":null},{"id":"c32f2ef9-611a-4a65-a970-e7916147bade","cell_type":"code","source":"# ‚úÖ Final check for Kaggle submission file\nimport os\nsub_path = \"/kaggle/working/submission.csv\"\n\nif os.path.exists(sub_path):\n    import pandas as pd\n    df = pd.read_csv(sub_path)\n    print(f\"‚úÖ submission.csv found ({len(df)} rows)\")\n    display(df.head(10))\nelse:\n    print(\"‚ùå submission.csv not found ‚Äî please check earlier errors.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:58:32.710784Z","iopub.execute_input":"2025-10-26T21:58:32.711122Z","iopub.status.idle":"2025-10-26T21:58:32.778952Z","shell.execute_reply.started":"2025-10-26T21:58:32.711103Z","shell.execute_reply":"2025-10-26T21:58:32.778281Z"}},"outputs":[],"execution_count":null},{"id":"b6526551","cell_type":"markdown","source":"## üëÄ Step 9 ‚Äî Visual Checks {#viz}","metadata":{}},{"id":"265ce113","cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nsome_id = None\nfor rid in by_record.keys():\n    jpg = TEST_IMG_DIR / f\"{rid}.jpg\"\n    png = TEST_IMG_DIR / f\"{rid}.png\"\n    if jpg.exists() or png.exists():\n        some_id = rid; break\n\nif some_id is not None:\n    leads_to_plot = [\"I\",\"II\",\"V1\",\"V5\"]\n    plt.figure(figsize=(12,4))\n    for ld in leads_to_plot:\n        key = (some_id, ld)\n        if key in pred:\n            plt.plot(pred[key], label=ld)\n    plt.title(f\"Hybrid predictions (record {some_id})\")\n    plt.legend()\n    plt.show()\nelse:\n    print(\"No example test image found for quick plotting preview.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T21:56:33.929126Z","iopub.execute_input":"2025-10-26T21:56:33.929464Z","iopub.status.idle":"2025-10-26T21:56:33.937824Z","shell.execute_reply.started":"2025-10-26T21:56:33.929434Z","shell.execute_reply":"2025-10-26T21:56:33.937068Z"}},"outputs":[],"execution_count":null},{"id":"9ac872ac","cell_type":"markdown","source":"\n## üìù Step 10 ‚Äî Notes & Next Steps {#next}\n\n**Why this hybrid works:**  \n- ECGs are quasi-periodic; a **median beat** is a strong prior.  \n- The **image digitizer** recovers subject-specific morphology from the paper scan.  \n- The **hybrid** balances physics-based priors with actual observed morphology.\n\n**Limitations:**  \n- Naive 3√ó4 panel split; replace with YOLO/FRCNN detector for robust layouts.  \n- Grid spacing estimated heuristically; add calibration pulse detection for precise mV/time scaling.  \n- Optional U-Net/nnU-Net segmentor not included here (hook provided).\n\n**Upgrades:**  \n- Train a segmentation model on ECG-Image-Kit / ECG-Image-Database and enable `USE_SEGMENTATION_MODEL`.  \n- Tempo-tracking BPM (multi-segment autocorrelation) for rate changes.  \n- Rhythm-strip parsing and vendor-specific panel templates.\n\n---\n\n‚ù§Ô∏è If you found this helpful, an **upvote** would be amazing.  \nHappy Kaggle-ing! üöÄ\n","metadata":{}}]}