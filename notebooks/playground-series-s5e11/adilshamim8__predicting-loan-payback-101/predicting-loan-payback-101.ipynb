{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Loan Payback 101\n> ### Playground Series S5E11\n\n\n## Table of Contents \n1. [Introduction & Setup](#introduction)\n2. [Data Loading & Overview](#data-loading)\n3. [Exploratory Data Analysis (EDA)](#eda)\n4. [Feature Engineering](#feature-engineering)\n5. [Complete Feature Engineering](#feature-engineering)\n6. [Model Training](#model-training)\n   - 6.1 LightGBM\n   - 6.2 XGBoost\n   - 6.3 CatBoost\n7. [Model Evaluation](#evaluation)\n8. [Ensemble Methods](#ensemble)\n9. [Submission Generation](#submission)\n10. [Conclusion](#conclusion)\n","metadata":{}},{"cell_type":"markdown","source":"### Competition Goal\n\nPredict the **probability** that a borrower will pay back their loan based on:\n- Financial metrics (income, debt, credit score)\n- Loan characteristics (amount, interest rate, purpose)\n- Personal information (gender, marital status, education, employment)\n\n**Evaluation:** Area Under the ROC Curve (AUC-ROC)","metadata":{}},{"cell_type":"markdown","source":"<a id='introduction'></a>\n# 1️ Introduction & Setup","metadata":{}},{"cell_type":"code","source":"# Core libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\nfrom scipy.stats import rankdata\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n\n# Models\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\n# Settings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Seed for reproducibility\nSEED = 42\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:08.436618Z","iopub.execute_input":"2025-11-07T12:26:08.436862Z","iopub.status.idle":"2025-11-07T12:26:15.327013Z","shell.execute_reply.started":"2025-11-07T12:26:08.43683Z","shell.execute_reply":"2025-11-07T12:26:15.326387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nclass Config:\n    \"\"\"Configuration class for hyperparameters and settings\"\"\"\n    N_SPLITS = 5\n    SEED = 42\n    TARGET = 'loan_paid_back'\n    VERBOSE = True\n    \n    # Model weights for ensemble (will be optimized later)\n    WEIGHTS = {\n        'lgb': 0.33,\n        'xgb': 0.33,\n        'cat': 0.34\n    }\n\nconfig = Config()\nprint(\"Configuration loaded successfully!\")\nprint(f\"   - Number of folds: {config.N_SPLITS}\")\nprint(f\"   - Random seed: {config.SEED}\")\nprint(f\"   - Target variable: {config.TARGET}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:15.327637Z","iopub.execute_input":"2025-11-07T12:26:15.328046Z","iopub.status.idle":"2025-11-07T12:26:15.333749Z","shell.execute_reply.started":"2025-11-07T12:26:15.328029Z","shell.execute_reply":"2025-11-07T12:26:15.333014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='data-loading'></a>\n# 2️ Data Loading & Overview","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n\n\nprint(f\" Train shape: {train.shape}\")\nprint(f\" Test shape: {test.shape}\")\nprint(f\" Sample submission shape: {sample_submission.shape}\")\nprint(f\"\\n Total rows: {train.shape[0]:,}\")\nprint(f\" Total features: {train.shape[1] - 2} (excluding id and target)\")\nprint(f\" Test samples to predict: {test.shape[0]:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:15.335037Z","iopub.execute_input":"2025-11-07T12:26:15.335316Z","iopub.status.idle":"2025-11-07T12:26:16.907581Z","shell.execute_reply.started":"2025-11-07T12:26:15.335289Z","shell.execute_reply":"2025-11-07T12:26:16.906732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First look at the data\nprint(\"TRAIN DATA PREVIEW\")\ndisplay(train.head(10))\n\nprint(\"TEST DATA PREVIEW\")\ndisplay(test.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:16.908441Z","iopub.execute_input":"2025-11-07T12:26:16.90924Z","iopub.status.idle":"2025-11-07T12:26:16.945932Z","shell.execute_reply.started":"2025-11-07T12:26:16.909209Z","shell.execute_reply":"2025-11-07T12:26:16.945221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data types and memory usage\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA INFO - TRAIN\")\nprint(\"=\"*80)\ntrain.info()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA INFO - TEST\")\nprint(\"=\"*80)\ntest.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:16.9467Z","iopub.execute_input":"2025-11-07T12:26:16.94703Z","iopub.status.idle":"2025-11-07T12:26:17.198269Z","shell.execute_reply.started":"2025-11-07T12:26:16.947Z","shell.execute_reply":"2025-11-07T12:26:17.197646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\n\nmissing_train = train.isnull().sum()\nmissing_test = test.isnull().sum()\n\nmissing_df = pd.DataFrame({\n    'Feature': train.columns,\n    'Train Missing': missing_train.values,\n    'Train Missing %': (missing_train.values / len(train) * 100).round(2),\n    'Test Missing': [missing_test.get(col, 0) for col in train.columns],\n    'Test Missing %': [(missing_test.get(col, 0) / len(test) * 100) for col in train.columns]\n})\n\nmissing_summary = missing_df[(missing_df['Train Missing'] > 0) | (missing_df['Test Missing'] > 0)]\n\nif len(missing_summary) > 0:\n    display(missing_summary.style.background_gradient(cmap='Reds'))\nelse:\n    print(\" No missing values found in train or test data!\")\n    print(\" Data quality is excellent - ready for modeling!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:17.198936Z","iopub.execute_input":"2025-11-07T12:26:17.1992Z","iopub.status.idle":"2025-11-07T12:26:17.440636Z","shell.execute_reply.started":"2025-11-07T12:26:17.199183Z","shell.execute_reply":"2025-11-07T12:26:17.439749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for duplicates\ntrain_duplicates = train.duplicated().sum()\ntest_duplicates = test.duplicated().sum()\n\nprint(f\"Train duplicates: {train_duplicates}\")\nprint(f\"Test duplicates: {test_duplicates}\")\n\nif train_duplicates == 0 and test_duplicates == 0:\n    print(\" No duplicates found!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:17.441595Z","iopub.execute_input":"2025-11-07T12:26:17.441932Z","iopub.status.idle":"2025-11-07T12:26:17.876259Z","shell.execute_reply.started":"2025-11-07T12:26:17.4419Z","shell.execute_reply":"2025-11-07T12:26:17.875397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target distribution\nprint(\"TARGET DISTRIBUTION ANALYSIS\")\nprint(\"=\"*80)\n\ntarget_counts = train[config.TARGET].value_counts()\ntarget_pct = train[config.TARGET].value_counts(normalize=True) * 100\n\ntarget_summary = pd.DataFrame({\n    'Value': target_counts.index,\n    'Count': target_counts.values,\n    'Percentage': target_pct.values\n})\n\ndisplay(target_summary.style.background_gradient(cmap='Blues'))\n\nprint(f\"\\nTarget Statistics:\")\nprint(f\"   - Mean: {train[config.TARGET].mean():.4f}\")\nprint(f\"   - Median: {train[config.TARGET].median():.4f}\")\nprint(f\"   - Std: {train[config.TARGET].std():.4f}\")\n\n# Check for class imbalance\nimbalance_ratio = target_counts.min() / target_counts.max()\nprint(f\"\\n  Class Balance Ratio: {imbalance_ratio:.3f}\")\nif imbalance_ratio < 0.5:\n    print(\"  Dataset is imbalanced - consider using stratified sampling\")\nelse:\n    print(\" Dataset is relatively balanced\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:17.877083Z","iopub.execute_input":"2025-11-07T12:26:17.87739Z","iopub.status.idle":"2025-11-07T12:26:17.966083Z","shell.execute_reply.started":"2025-11-07T12:26:17.877366Z","shell.execute_reply":"2025-11-07T12:26:17.965527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize target distribution\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Count plot\ncolors = ['#FF6B6B', '#4ECDC4']\ntrain[config.TARGET].value_counts().plot(kind='bar', ax=axes[0], color=colors, edgecolor='black', linewidth=1.5)\naxes[0].set_title('Target Distribution (Count)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Loan Paid Back', fontsize=12)\naxes[0].set_ylabel('Count', fontsize=12)\naxes[0].tick_params(rotation=0)\naxes[0].grid(alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor container in axes[0].containers:\n    axes[0].bar_label(container, fmt='%d', fontsize=10, fontweight='bold')\n\n# Pie chart\nexplode = (0.05, 0.05)\naxes[1].pie(train[config.TARGET].value_counts(), \n            labels=['Not Paid Back (0)', 'Paid Back (1)'], \n            autopct='%1.2f%%', \n            colors=colors, \n            explode=explode, \n            shadow=True, \n            startangle=90,\n            textprops={'fontsize': 11, 'fontweight': 'bold'})\naxes[1].set_title('Target Distribution (Percentage)', fontsize=14, fontweight='bold')\n\n# Distribution plot\naxes[2].hist(train[config.TARGET], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\naxes[2].axvline(train[config.TARGET].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {train[config.TARGET].mean():.3f}\")\naxes[2].set_title('Target Value Distribution', fontsize=14, fontweight='bold')\naxes[2].set_xlabel('Loan Paid Back', fontsize=12)\naxes[2].set_ylabel('Frequency', fontsize=12)\naxes[2].legend(fontsize=10)\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:17.96848Z","iopub.execute_input":"2025-11-07T12:26:17.968679Z","iopub.status.idle":"2025-11-07T12:26:18.651088Z","shell.execute_reply.started":"2025-11-07T12:26:17.968664Z","shell.execute_reply":"2025-11-07T12:26:18.650338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='eda'></a>\n# 3️ Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Feature Type Identification","metadata":{}},{"cell_type":"code","source":"# Separate numerical and categorical columns\nnumerical_cols = train.select_dtypes(include=[np.number]).columns.tolist()\nnumerical_cols.remove('id')\nif config.TARGET in numerical_cols:\n    numerical_cols.remove(config.TARGET)\n\ncategorical_cols = train.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"FEATURE TYPE SUMMARY\")\nprint(\"=\"*80)\nprint(f\"\\n Numerical features ({len(numerical_cols)}):\")\nfor i, col in enumerate(numerical_cols, 1):\n    print(f\"   {i}. {col}\")\n\nprint(f\"\\n Categorical features ({len(categorical_cols)}):\")\nfor i, col in enumerate(categorical_cols, 1):\n    print(f\"   {i}. {col}\")\n\nprint(f\"\\n Total predictive features: {len(numerical_cols) + len(categorical_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:18.65201Z","iopub.execute_input":"2025-11-07T12:26:18.652311Z","iopub.status.idle":"2025-11-07T12:26:18.704597Z","shell.execute_reply.started":"2025-11-07T12:26:18.652288Z","shell.execute_reply":"2025-11-07T12:26:18.704007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Numerical Features Analysis","metadata":{}},{"cell_type":"code","source":"# Statistical summary of numerical features\nprint(\"NUMERICAL FEATURES - STATISTICAL SUMMARY\")\nprint(\"=\"*80)\n\nnumerical_stats = train[numerical_cols].describe().T\nnumerical_stats['missing'] = train[numerical_cols].isnull().sum().values\nnumerical_stats['skewness'] = train[numerical_cols].skew().values\nnumerical_stats['kurtosis'] = train[numerical_cols].kurtosis().values\n\ndisplay(numerical_stats.style.background_gradient(cmap='coolwarm', subset=['mean', 'std', 'skewness', 'kurtosis']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:18.705222Z","iopub.execute_input":"2025-11-07T12:26:18.705435Z","iopub.status.idle":"2025-11-07T12:26:18.920424Z","shell.execute_reply.started":"2025-11-07T12:26:18.705419Z","shell.execute_reply":"2025-11-07T12:26:18.919666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of numerical features\nfig, axes = plt.subplots(3, 2, figsize=(16, 14))\naxes = axes.flatten()\n\nfor idx, col in enumerate(numerical_cols):\n    # Plot distribution with KDE\n    axes[idx].hist(train[col], bins=50, alpha=0.6, color='steelblue', edgecolor='black', density=True, label='Histogram')\n    \n    # Add KDE\n    train[col].plot(kind='kde', ax=axes[idx], color='red', linewidth=2, label='KDE')\n    \n    axes[idx].set_title(f'{col} Distribution', fontsize=13, fontweight='bold', pad=10)\n    axes[idx].set_xlabel(col, fontsize=11)\n    axes[idx].set_ylabel('Density', fontsize=11)\n    axes[idx].grid(alpha=0.3, linestyle='--')\n    \n    # Add statistics box\n    mean_val = train[col].mean()\n    median_val = train[col].median()\n    std_val = train[col].std()\n    \n    axes[idx].axvline(mean_val, color='green', linestyle='--', linewidth=2, alpha=0.7, label=f'Mean: {mean_val:.2f}')\n    axes[idx].axvline(median_val, color='orange', linestyle='--', linewidth=2, alpha=0.7, label=f'Median: {median_val:.2f}')\n    \n    axes[idx].legend(fontsize=9, loc='upper right')\n\nplt.suptitle('Numerical Features Distribution Analysis', fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:26:18.921332Z","iopub.execute_input":"2025-11-07T12:26:18.92164Z","iopub.status.idle":"2025-11-07T12:27:09.554431Z","shell.execute_reply.started":"2025-11-07T12:26:18.92162Z","shell.execute_reply":"2025-11-07T12:27:09.553706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation analysis\nprint(\"CORRELATION ANALYSIS\")\nprint(\"=\"*80)\n\nplt.figure(figsize=(14, 11))\ncorrelation_matrix = train[numerical_cols + [config.TARGET]].corr()\n\n# Create mask for upper triangle\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n\n# Create heatmap\nsns.heatmap(correlation_matrix, \n            mask=mask, \n            annot=True, \n            fmt='.3f', \n            cmap='coolwarm', \n            center=0, \n            square=True, \n            linewidths=1,\n            cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"},\n            annot_kws={\"size\": 9})\n\nplt.title('Correlation Matrix - Numerical Features & Target', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\n# Feature importance based on correlation with target\ntarget_corr = correlation_matrix[config.TARGET].drop(config.TARGET).sort_values(ascending=False)\n\nprint(\"\\n Correlation with Target (sorted by absolute value):\")\ntarget_corr_abs = target_corr.abs().sort_values(ascending=False)\nfor feature, corr_val in target_corr_abs.items():\n    actual_corr = target_corr[feature]\n    print(f\"   {feature:30s}: {actual_corr:7.4f} (abs: {corr_val:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:09.555585Z","iopub.execute_input":"2025-11-07T12:27:09.555835Z","iopub.status.idle":"2025-11-07T12:27:09.999115Z","shell.execute_reply.started":"2025-11-07T12:27:09.555816Z","shell.execute_reply":"2025-11-07T12:27:09.998334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize correlation with target\nplt.figure(figsize=(12, 6))\ntarget_corr_df = target_corr.sort_values()\ncolors_list = ['#FF6B6B' if x < 0 else '#4ECDC4' for x in target_corr_df.values]\n\nplt.barh(range(len(target_corr_df)), target_corr_df.values, color=colors_list, edgecolor='black', linewidth=1.2)\nplt.yticks(range(len(target_corr_df)), target_corr_df.index, fontsize=11)\nplt.xlabel('Correlation Coefficient', fontsize=12, fontweight='bold')\nplt.title('Feature Correlation with Target Variable', fontsize=14, fontweight='bold', pad=15)\nplt.axvline(x=0, color='black', linestyle='-', linewidth=1)\nplt.grid(alpha=0.3, axis='x')\n\n# Add value labels\nfor i, v in enumerate(target_corr_df.values):\n    plt.text(v, i, f' {v:.4f}', va='center', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:09.999944Z","iopub.execute_input":"2025-11-07T12:27:10.00036Z","iopub.status.idle":"2025-11-07T12:27:10.233182Z","shell.execute_reply.started":"2025-11-07T12:27:10.000334Z","shell.execute_reply":"2025-11-07T12:27:10.232487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Box plots by target - showing distribution differences\nfig, axes = plt.subplots(3, 2, figsize=(16, 14))\naxes = axes.flatten()\n\nfor idx, col in enumerate(numerical_cols):\n    # Create box plot\n    data_to_plot = [train[train[config.TARGET] == 0][col].dropna(), \n                    train[train[config.TARGET] == 1][col].dropna()]\n    \n    bp = axes[idx].boxplot(data_to_plot, \n                           labels=['Not Paid (0)', 'Paid (1)'],\n                           patch_artist=True,\n                           showmeans=True,\n                           meanline=True)\n    \n    # Color the boxes\n    colors = ['#FF6B6B', '#4ECDC4']\n    for patch, color in zip(bp['boxes'], colors):\n        patch.set_facecolor(color)\n        patch.set_alpha(0.6)\n    \n    axes[idx].set_title(f'{col} by Target', fontsize=13, fontweight='bold')\n    axes[idx].set_ylabel(col, fontsize=11)\n    axes[idx].grid(alpha=0.3, axis='y')\n    \n    # Add mean values as text\n    mean_0 = train[train[config.TARGET] == 0][col].mean()\n    mean_1 = train[train[config.TARGET] == 1][col].mean()\n    axes[idx].text(1, mean_0, f'μ={mean_0:.1f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n    axes[idx].text(2, mean_1, f'μ={mean_1:.1f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nplt.suptitle('Numerical Features by Target - Box Plot Analysis', fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:10.233829Z","iopub.execute_input":"2025-11-07T12:27:10.234086Z","iopub.status.idle":"2025-11-07T12:27:11.98089Z","shell.execute_reply.started":"2025-11-07T12:27:10.234065Z","shell.execute_reply":"2025-11-07T12:27:11.980127Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Categorical Features Analysis","metadata":{}},{"cell_type":"code","source":"# Categorical features summary\nprint(\"CATEGORICAL FEATURES - DETAILED ANALYSIS\")\nprint(\"=\"*80)\n\nfor col in categorical_cols:\n    print(f\"Feature: {col.upper()}\")\n    print(f\"{'='*80}\")\n    print(f\"Unique values: {train[col].nunique()}\")\n    print(f\"Most common: {train[col].mode()[0]}\")\n    print(f\"\\nValue Counts:\")\n    \n    value_counts_df = pd.DataFrame({\n        'Value': train[col].value_counts().index,\n        'Count': train[col].value_counts().values,\n        'Percentage': (train[col].value_counts(normalize=True) * 100).values\n    })\n    display(value_counts_df.head(10).style.background_gradient(cmap='Blues', subset=['Count', 'Percentage']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:11.981766Z","iopub.execute_input":"2025-11-07T12:27:11.982161Z","iopub.status.idle":"2025-11-07T12:27:13.079498Z","shell.execute_reply.started":"2025-11-07T12:27:11.982133Z","shell.execute_reply":"2025-11-07T12:27:13.078954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize categorical features distribution\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\naxes = axes.flatten()\n\nfor idx, col in enumerate(categorical_cols):\n    value_counts = train[col].value_counts()\n    \n    # Create bar plot\n    bars = axes[idx].bar(range(len(value_counts)), value_counts.values, \n                         color='steelblue', edgecolor='black', linewidth=1.2, alpha=0.8)\n    \n    axes[idx].set_title(f'{col} Distribution', fontsize=13, fontweight='bold', pad=10)\n    axes[idx].set_xlabel(col, fontsize=11)\n    axes[idx].set_ylabel('Count', fontsize=11)\n    axes[idx].set_xticks(range(len(value_counts)))\n    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\n    axes[idx].grid(alpha=0.3, axis='y', linestyle='--')\n    \n    # Add value labels on bars\n    for bar in bars:\n        height = bar.get_height()\n        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n                      f'{int(height):,}',\n                      ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nplt.suptitle('Categorical Features Distribution', fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:13.080299Z","iopub.execute_input":"2025-11-07T12:27:13.080568Z","iopub.status.idle":"2025-11-07T12:27:14.674516Z","shell.execute_reply.started":"2025-11-07T12:27:13.080546Z","shell.execute_reply":"2025-11-07T12:27:14.673699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target rate by categorical features\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\naxes = axes.flatten()\n\noverall_mean = train[config.TARGET].mean()\n\nfor idx, col in enumerate(categorical_cols):\n    target_rate = train.groupby(col)[config.TARGET].agg(['mean', 'count']).sort_values('mean', ascending=False)\n    \n    # Create bar plot\n    bars = axes[idx].bar(range(len(target_rate)), target_rate['mean'].values, \n                         color='coral', edgecolor='black', linewidth=1.2, alpha=0.8)\n    \n    axes[idx].set_title(f'Payback Rate by {col}', fontsize=13, fontweight='bold', pad=10)\n    axes[idx].set_xlabel(col, fontsize=11)\n    axes[idx].set_ylabel('Payback Rate', fontsize=11)\n    axes[idx].set_xticks(range(len(target_rate)))\n    axes[idx].set_xticklabels(target_rate.index, rotation=45, ha='right')\n    axes[idx].axhline(y=overall_mean, color='red', linestyle='--', \n                      linewidth=2.5, alpha=0.7, label=f'Overall: {overall_mean:.3f}')\n    axes[idx].legend(fontsize=10, loc='best')\n    axes[idx].grid(alpha=0.3, axis='y', linestyle='--')\n    axes[idx].set_ylim([0, max(target_rate['mean'].max() * 1.1, overall_mean * 1.2)])\n    \n    # Add value labels with sample counts\n    for i, (bar, count) in enumerate(zip(bars, target_rate['count'].values)):\n        height = bar.get_height()\n        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n                      f'{height:.3f}\\n(n={count:,})',\n                      ha='center', va='bottom', fontsize=8, fontweight='bold')\n\nplt.suptitle('Target Rate Analysis by Categorical Features', fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:14.675321Z","iopub.execute_input":"2025-11-07T12:27:14.675547Z","iopub.status.idle":"2025-11-07T12:27:16.405455Z","shell.execute_reply.started":"2025-11-07T12:27:14.675531Z","shell.execute_reply":"2025-11-07T12:27:16.404632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Advanced Statistical Analysis","metadata":{}},{"cell_type":"code","source":"# Skewness and Kurtosis detailed analysis\nprint(\"SKEWNESS AND KURTOSIS ANALYSIS\")\nprint(\"=\"*80)\n\nskew_kurt_df = pd.DataFrame({\n    'Feature': numerical_cols,\n    'Skewness': [skew(train[col]) for col in numerical_cols],\n    'Kurtosis': [kurtosis(train[col]) for col in numerical_cols],\n    'Min': [train[col].min() for col in numerical_cols],\n    'Max': [train[col].max() for col in numerical_cols],\n    'Range': [train[col].max() - train[col].min() for col in numerical_cols]\n})\n\n# Add interpretation\nskew_kurt_df['Skew_Interpretation'] = skew_kurt_df['Skewness'].apply(\n    lambda x: 'Right-skewed' if x > 1 else ('Left-skewed' if x < -1 else 'Symmetric')\n)\n\nskew_kurt_df = skew_kurt_df.sort_values('Skewness', key=abs, ascending=False)\n\ndisplay(skew_kurt_df.style.background_gradient(cmap='coolwarm', subset=['Skewness', 'Kurtosis']))\n\nprint(\"\\n Interpretation Guide:\")\nprint(\"    Skewness:\")\nprint(\"      - Close to 0: Symmetric distribution (Normal-like)\")\nprint(\"      - > 1: Highly right-skewed (long tail on right)\")\nprint(\"      - < -1: Highly left-skewed (long tail on left)\")\nprint(\"\\n   Kurtosis:\")\nprint(\"      - Close to 0: Normal-like tails\")\nprint(\"      - > 0: Heavy tails (more outliers)\")\nprint(\"      - < 0: Light tails (fewer outliers)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:16.406224Z","iopub.execute_input":"2025-11-07T12:27:16.406415Z","iopub.status.idle":"2025-11-07T12:27:16.499441Z","shell.execute_reply.started":"2025-11-07T12:27:16.406399Z","shell.execute_reply":"2025-11-07T12:27:16.498812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Outlier detection using IQR method\nprint(\"OUTLIER DETECTION (IQR METHOD)\")\nprint(\"=\"*80)\n\noutlier_summary = []\n\nfor col in numerical_cols:\n    Q1 = train[col].quantile(0.25)\n    Q3 = train[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers = train[(train[col] < lower_bound) | (train[col] > upper_bound)][col]\n    outlier_count = len(outliers)\n    outlier_pct = (outlier_count / len(train)) * 100\n    \n    outlier_summary.append({\n        'Feature': col,\n        'Q1': Q1,\n        'Q3': Q3,\n        'IQR': IQR,\n        'Lower_Bound': lower_bound,\n        'Upper_Bound': upper_bound,\n        'Outlier_Count': outlier_count,\n        'Outlier_Percentage': outlier_pct\n    })\n\noutlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier_Percentage', ascending=False)\ndisplay(outlier_df.style.background_gradient(cmap='Reds', subset=['Outlier_Count', 'Outlier_Percentage']))\n\nprint(f\"\\n Outlier Summary:\")\nprint(f\"   - Total features with outliers: {(outlier_df['Outlier_Count'] > 0).sum()}\")\nprint(f\"   - Average outlier percentage: {outlier_df['Outlier_Percentage'].mean():.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:16.500096Z","iopub.execute_input":"2025-11-07T12:27:16.500293Z","iopub.status.idle":"2025-11-07T12:27:16.643359Z","shell.execute_reply.started":"2025-11-07T12:27:16.50028Z","shell.execute_reply":"2025-11-07T12:27:16.642621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='feature-engineering'></a>\n# 4️ Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Advanced Feature Engineering Function","metadata":{}},{"cell_type":"code","source":"def advanced_feature_engineering(df, is_train=True):\n    \"\"\"\n    Comprehensive feature engineering for loan prediction\n    \n    This function creates multiple types of features:\n    - Financial ratios and metrics\n    - Risk scores and composite metrics\n    - Interaction features\n    - Binned/categorical versions of numerical features\n    - Statistical aggregations\n    - Domain-specific features\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe (train or test)\n    is_train : bool\n        Whether this is training data\n    \n    Returns:\n    --------\n    df : pd.DataFrame\n        Dataframe with engineered features\n    \"\"\"\n    \n    df = df.copy()\n    \n    print(\"FEATURE ENGINEERING PIPELINE\")\n    print(\"=\"*80)\n    print(f\"Starting features: {df.shape[1]}\")\n    \n    # ========================================\n    # 1. FINANCIAL RATIO FEATURES\n    # ========================================\n    print(\"\\n[1/11]  Creating financial ratio features...\")\n    \n    # Core financial ratios\n    df['loan_to_income_ratio'] = df['loan_amount'] / (df['annual_income'] + 1)\n    df['monthly_income'] = df['annual_income'] / 12\n    df['monthly_payment_estimate'] = (df['loan_amount'] * df['interest_rate']) / 1200\n    df['payment_to_income_ratio'] = df['monthly_payment_estimate'] / (df['monthly_income'] + 1)\n    \n    # Debt calculations\n    df['current_debt_amount'] = df['debt_to_income_ratio'] * df['annual_income']\n    df['total_debt_with_loan'] = df['current_debt_amount'] + df['loan_amount']\n    df['new_debt_to_income'] = df['total_debt_with_loan'] / (df['annual_income'] + 1)\n    df['debt_increase_ratio'] = df['new_debt_to_income'] / (df['debt_to_income_ratio'] + 0.01)\n    df['debt_increase_amount'] = df['loan_amount']\n    \n    # Disposable income\n    df['disposable_income'] = df['annual_income'] - df['current_debt_amount']\n    df['disposable_income_ratio'] = df['disposable_income'] / (df['annual_income'] + 1)\n    df['loan_to_disposable_income'] = df['loan_amount'] / (df['disposable_income'] + 1)\n    df['monthly_disposable_income'] = df['disposable_income'] / 12\n    \n    # Payment burden\n    df['payment_to_disposable_ratio'] = df['monthly_payment_estimate'] / (df['monthly_disposable_income'] + 1)\n    df['annual_payment_burden'] = df['monthly_payment_estimate'] * 12\n    df['payment_burden_ratio'] = df['annual_payment_burden'] / (df['annual_income'] + 1)\n    \n    print(f\"✓ Created 16 financial ratio features\")\n    \n    # ========================================\n    # 2. CREDIT SCORE FEATURES\n    # ========================================\n    print(\"\\n[2/11]  Creating credit score features...\")\n    \n    # Normalize and transform credit score\n    df['credit_score_normalized'] = df['credit_score'] / 850\n    df['credit_risk_score'] = 1 - df['credit_score_normalized']\n    df['credit_score_squared'] = df['credit_score'] ** 2\n    df['credit_score_log'] = np.log1p(df['credit_score'])\n    \n    # Credit categories\n    df['credit_category'] = pd.cut(df['credit_score'], \n                                     bins=[0, 580, 670, 740, 800, 850],\n                                     labels=['poor', 'fair', 'good', 'very_good', 'excellent'])\n    \n    # Credit score bins\n    df['credit_bin'] = pd.cut(df['credit_score'], bins=10, labels=False)\n    \n    # Interactions with other features\n    df['credit_income_interaction'] = df['credit_score'] * df['annual_income']\n    df['credit_times_dti'] = df['credit_score'] * df['debt_to_income_ratio']\n    df['credit_loan_interaction'] = df['credit_score'] * df['loan_amount']\n    \n    print(f\"✓ Created 9 credit score features\")\n    \n    # ========================================\n    # 3. INTEREST RATE FEATURES\n    # ========================================\n    print(\"\\n[3/11]  Creating interest rate features...\")\n    \n    # Interest rate flags and categories\n    df['high_interest_flag'] = (df['interest_rate'] > df['interest_rate'].median()).astype(int)\n    df['very_high_interest'] = (df['interest_rate'] > df['interest_rate'].quantile(0.75)).astype(int)\n    df['low_interest_flag'] = (df['interest_rate'] < df['interest_rate'].quantile(0.25)).astype(int)\n    \n    # Interest cost calculations\n    df['total_interest_cost'] = df['loan_amount'] * df['interest_rate'] / 100\n    df['interest_burden'] = df['total_interest_cost'] / (df['annual_income'] + 1)\n    df['monthly_interest_cost'] = df['total_interest_cost'] / 12\n    \n    # Interest rate vs credit score (should be inversely related)\n    df['interest_credit_mismatch'] = df['interest_rate'] * (1 - df['credit_score_normalized'])\n    df['interest_credit_ratio'] = df['interest_rate'] / (df['credit_score'] / 100)\n    \n    # Interest rate transformations\n    df['interest_rate_squared'] = df['interest_rate'] ** 2\n    df['interest_rate_log'] = np.log1p(df['interest_rate'])\n    \n    print(f\"✓ Created 10 interest rate features\")\n    \n    # ========================================\n    # 4. COMPOSITE RISK SCORES\n    # ========================================\n    print(\"\\n[4/11]   Creating composite risk scores...\")\n    \n    # Multi-factor risk scores (weighted combinations)\n    df['risk_score_v1'] = (\n        df['debt_to_income_ratio'] * 0.25 +\n        df['loan_to_income_ratio'] * 0.25 +\n        df['credit_risk_score'] * 0.30 +\n        (df['interest_rate'] / 100) * 0.20\n    )\n    \n    df['risk_score_v2'] = (\n        df['payment_to_income_ratio'] * 0.40 +\n        df['new_debt_to_income'] * 0.35 +\n        df['interest_burden'] * 0.25\n    )\n    \n    df['risk_score_v3'] = (\n        df['debt_to_income_ratio'] * 0.30 +\n        df['payment_burden_ratio'] * 0.30 +\n        df['credit_risk_score'] * 0.40\n    )\n    \n    # Affordability score (higher is better)\n    df['affordability_score'] = (\n        df['credit_score_normalized'] * 0.40 +\n        (1 - df['debt_to_income_ratio']) * 0.30 +\n        df['disposable_income_ratio'] * 0.30\n    )\n    \n    # Financial health score\n    df['financial_health_score'] = (\n        df['affordability_score'] * 0.60 -\n        df['risk_score_v1'] * 0.40\n    )\n    \n    print(f\"✓ Created 5 composite risk scores\")\n    \n    # Continue in next cell...\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:16.644196Z","iopub.execute_input":"2025-11-07T12:27:16.644425Z","iopub.status.idle":"2025-11-07T12:27:16.658106Z","shell.execute_reply.started":"2025-11-07T12:27:16.644398Z","shell.execute_reply":"2025-11-07T12:27:16.657387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='feature-engineering'></a>\n# 5 Complete Feature Engineering","metadata":{}},{"cell_type":"code","source":"def complete_feature_engineering(df):\n    \"\"\"\n    Comprehensive feature engineering pipeline for loan prediction\n    \"\"\"\n    df = df.copy()\n    \n    print(\" FEATURE ENGINEERING PIPELINE\")\n    print(\"=\"*80)\n    print(f\"Starting features: {df.shape[1]}\")\n    \n    # 1. FINANCIAL RATIOS\n    print(\"\\n[1/11]  Financial ratio features...\")\n    df['loan_to_income_ratio'] = df['loan_amount'] / (df['annual_income'] + 1)\n    df['monthly_income'] = df['annual_income'] / 12\n    df['monthly_payment_estimate'] = (df['loan_amount'] * df['interest_rate']) / 1200\n    df['payment_to_income_ratio'] = df['monthly_payment_estimate'] / (df['monthly_income'] + 1)\n    df['current_debt_amount'] = df['debt_to_income_ratio'] * df['annual_income']\n    df['total_debt_with_loan'] = df['current_debt_amount'] + df['loan_amount']\n    df['new_debt_to_income'] = df['total_debt_with_loan'] / (df['annual_income'] + 1)\n    df['debt_increase_ratio'] = df['new_debt_to_income'] / (df['debt_to_income_ratio'] + 0.01)\n    df['disposable_income'] = df['annual_income'] - df['current_debt_amount']\n    df['disposable_income_ratio'] = df['disposable_income'] / (df['annual_income'] + 1)\n    df['loan_to_disposable_income'] = df['loan_amount'] / (df['disposable_income'] + 1)\n    df['monthly_disposable_income'] = df['disposable_income'] / 12\n    df['payment_to_disposable_ratio'] = df['monthly_payment_estimate'] / (df['monthly_disposable_income'] + 1)\n    df['annual_payment_burden'] = df['monthly_payment_estimate'] * 12\n    df['payment_burden_ratio'] = df['annual_payment_burden'] / (df['annual_income'] + 1)\n    print(f\"✓ Created 15 features\")\n    \n    # 2. CREDIT SCORE FEATURES\n    print(\"[2/11]  Credit score features...\")\n    df['credit_score_normalized'] = df['credit_score'] / 850\n    df['credit_risk_score'] = 1 - df['credit_score_normalized']\n    df['credit_score_squared'] = df['credit_score'] ** 2\n    df['credit_score_log'] = np.log1p(df['credit_score'])\n    df['credit_category'] = pd.cut(df['credit_score'], bins=[0, 580, 670, 740, 800, 850],\n                                     labels=['poor', 'fair', 'good', 'very_good', 'excellent'])\n    df['credit_income_interaction'] = df['credit_score'] * df['annual_income']\n    df['credit_times_dti'] = df['credit_score'] * df['debt_to_income_ratio']\n    df['credit_loan_interaction'] = df['credit_score'] * df['loan_amount']\n    print(f\"✓ Created 8 features\")\n    \n    # 3. INTEREST RATE FEATURES\n    print(\"[3/11]  Interest rate features...\")\n    df['high_interest_flag'] = (df['interest_rate'] > df['interest_rate'].median()).astype(int)\n    df['very_high_interest'] = (df['interest_rate'] > df['interest_rate'].quantile(0.75)).astype(int)\n    df['low_interest_flag'] = (df['interest_rate'] < df['interest_rate'].quantile(0.25)).astype(int)\n    df['total_interest_cost'] = df['loan_amount'] * df['interest_rate'] / 100\n    df['interest_burden'] = df['total_interest_cost'] / (df['annual_income'] + 1)\n    df['interest_credit_mismatch'] = df['interest_rate'] * (1 - df['credit_score_normalized'])\n    df['interest_credit_ratio'] = df['interest_rate'] / (df['credit_score'] / 100)\n    df['interest_rate_squared'] = df['interest_rate'] ** 2\n    print(f\"✓ Created 8 features\")\n    \n    # 4. RISK SCORES\n    print(\"[4/11]   Composite risk scores...\")\n    df['risk_score_v1'] = (df['debt_to_income_ratio'] * 0.25 + df['loan_to_income_ratio'] * 0.25 +\n                           df['credit_risk_score'] * 0.30 + (df['interest_rate'] / 100) * 0.20)\n    df['risk_score_v2'] = (df['payment_to_income_ratio'] * 0.40 + df['new_debt_to_income'] * 0.35 +\n                           df['interest_burden'] * 0.25)\n    df['affordability_score'] = (df['credit_score_normalized'] * 0.40 + \n                                 (1 - df['debt_to_income_ratio']) * 0.30 +\n                                 df['disposable_income_ratio'] * 0.30)\n    df['financial_health_score'] = df['affordability_score'] * 0.60 - df['risk_score_v1'] * 0.40\n    print(f\"   ✓ Created 4 features\")\n    \n    # 5. LOAN AMOUNT FEATURES\n    print(\"[5/11]  Loan amount features...\")\n    df['loan_size'] = pd.cut(df['loan_amount'], bins=[0, 10000, 20000, 30000, np.inf],\n                              labels=['small', 'medium', 'large', 'very_large'])\n    df['loan_amount_squared'] = df['loan_amount'] ** 2\n    df['loan_amount_log'] = np.log1p(df['loan_amount'])\n    df['annual_income_log'] = np.log1p(df['annual_income'])\n    df['loan_amount_sqrt'] = np.sqrt(df['loan_amount'])\n    print(f\"✓ Created 5 features\")\n    \n    # 6. BINNING FEATURES\n    print(\"[6/11]  Binned features...\")\n    df['income_decile'] = pd.qcut(df['annual_income'], q=10, labels=False, duplicates='drop')\n    df['credit_decile'] = pd.qcut(df['credit_score'], q=10, labels=False, duplicates='drop')\n    df['loan_decile'] = pd.qcut(df['loan_amount'], q=10, labels=False, duplicates='drop')\n    df['dti_decile'] = pd.qcut(df['debt_to_income_ratio'], q=10, labels=False, duplicates='drop')\n    df['interest_decile'] = pd.qcut(df['interest_rate'], q=10, labels=False, duplicates='drop')\n    print(f\"✓ Created 5 features\")\n    \n    # 7. INTERACTION FEATURES\n    print(\"[7/11]  Interaction features...\")\n    df['income_x_credit'] = df['annual_income'] * df['credit_score']\n    df['dti_x_interest'] = df['debt_to_income_ratio'] * df['interest_rate']\n    df['loan_x_interest'] = df['loan_amount'] * df['interest_rate']\n    df['income_x_dti'] = df['annual_income'] * df['debt_to_income_ratio']\n    df['income_credit_loan'] = df['annual_income'] * df['credit_score'] * df['loan_amount']\n    df['dti_interest_credit'] = df['debt_to_income_ratio'] * df['interest_rate'] * df['credit_score']\n    print(f\"✓ Created 6 features\")\n    \n    # 8. GRADE FEATURES\n    print(\"[8/11]  Grade/subgrade features...\")\n    df['grade'] = df['grade_subgrade'].str[0]\n    df['subgrade_num'] = df['grade_subgrade'].str[1:].astype(int)\n    grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n    df['grade_numeric'] = df['grade'].map(grade_map)\n    df['full_grade_score'] = df['grade_numeric'] * 10 + df['subgrade_num']\n    df['grade_credit_ratio'] = df['full_grade_score'] / (df['credit_score'] / 100)\n    print(f\"✓ Created 5 features\")\n    \n    # 9. STATISTICAL AGGREGATIONS\n    print(\"[9/11]  Statistical aggregations...\")\n    df['mean_financial_metrics'] = df[['debt_to_income_ratio', 'loan_to_income_ratio', \n                                        'payment_to_income_ratio']].mean(axis=1)\n    df['max_financial_burden'] = df[['debt_to_income_ratio', 'loan_to_income_ratio', \n                                      'payment_to_income_ratio']].max(axis=1)\n    df['min_financial_burden'] = df[['debt_to_income_ratio', 'loan_to_income_ratio', \n                                      'payment_to_income_ratio']].min(axis=1)\n    df['std_financial_metrics'] = df[['debt_to_income_ratio', 'loan_to_income_ratio', \n                                       'payment_to_income_ratio']].std(axis=1)\n    print(f\"✓ Created 4 features\")\n    \n    # 10. CATEGORICAL COMBINATIONS\n    print(\"[10/11]  Categorical combinations...\")\n    df['gender_marital'] = df['gender'] + '_' + df['marital_status']\n    df['education_employment'] = df['education_level'] + '_' + df['employment_status']\n    df['gender_education'] = df['gender'] + '_' + df['education_level']\n    df['marital_employment'] = df['marital_status'] + '_' + df['employment_status']\n    df['purpose_grade'] = df['loan_purpose'] + '_' + df['grade']\n    df['employment_purpose'] = df['employment_status'] + '_' + df['loan_purpose']\n    print(f\"✓ Created 6 features\")\n    \n    # 11. ANOMALY FLAGS\n    print(\"[11/11]  Anomaly detection flags...\")\n    df['extreme_dti'] = (df['debt_to_income_ratio'] > df['debt_to_income_ratio'].quantile(0.90)).astype(int)\n    df['low_income'] = (df['annual_income'] < df['annual_income'].quantile(0.25)).astype(int)\n    df['large_loan'] = (df['loan_amount'] > df['loan_amount'].quantile(0.75)).astype(int)\n    df['risky_combo_1'] = ((df['debt_to_income_ratio'] > 0.4) & (df['credit_score'] < 650)).astype(int)\n    df['risky_combo_2'] = ((df['loan_to_income_ratio'] > 0.5) & (df['interest_rate'] > 15)).astype(int)\n    df['safe_combo'] = ((df['credit_score'] > 750) & (df['debt_to_income_ratio'] < 0.3)).astype(int)\n    df['high_risk_all'] = (df['extreme_dti'] & df['risky_combo_1']).astype(int)\n    print(f\"✓ Created 7 features\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\" Feature Engineering Complete!\")\n    print(f\"   Final features: {df.shape[1]}\")\n    print(f\"   New features: {df.shape[1] - 13}\")\n    print(\"=\"*80)\n    \n    return df\n\n# Apply feature engineering\ntrain_fe = complete_feature_engineering(train)\ntest_fe = complete_feature_engineering(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:16.658716Z","iopub.execute_input":"2025-11-07T12:27:16.658907Z","iopub.status.idle":"2025-11-07T12:27:19.315722Z","shell.execute_reply.started":"2025-11-07T12:27:16.658894Z","shell.execute_reply":"2025-11-07T12:27:19.314933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode categorical features\nprint(\"ENCODING CATEGORICAL FEATURES\")\nprint(\"=\"*80)\n\ncategorical_features = train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n\nle_dict = {}\nfor col in categorical_features:\n    le = LabelEncoder()\n    train_fe[col] = le.fit_transform(train_fe[col].astype(str))\n    test_fe[col] = le.transform(test_fe[col].astype(str))\n    le_dict[col] = le\n    print(f\"✓ {col}: {len(le.classes_)} classes\")\n\nprint(f\"\\n Encoded {len(categorical_features)} features\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:19.316743Z","iopub.execute_input":"2025-11-07T12:27:19.31711Z","iopub.status.idle":"2025-11-07T12:27:22.311203Z","shell.execute_reply.started":"2025-11-07T12:27:19.317078Z","shell.execute_reply":"2025-11-07T12:27:22.31039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare datasets\nfeature_cols = [col for col in train_fe.columns if col not in ['id', config.TARGET]]\n\nX = train_fe[feature_cols]\ny = train_fe[config.TARGET]\nX_test = test_fe[feature_cols]\ntest_ids = test_fe['id']\n\nprint(\"FINAL DATA READY\")\nprint(\"=\"*80)\nprint(f\"X: {X.shape}\")\nprint(f\"y: {y.shape}\")\nprint(f\"X_test: {X_test.shape}\")\nprint(f\"Features: {len(feature_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:22.312039Z","iopub.execute_input":"2025-11-07T12:27:22.312308Z","iopub.status.idle":"2025-11-07T12:27:22.766084Z","shell.execute_reply.started":"2025-11-07T12:27:22.312284Z","shell.execute_reply":"2025-11-07T12:27:22.765227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='model-training'></a>\n# 6 Model Training","metadata":{}},{"cell_type":"markdown","source":"## 6.1 LightGBM","metadata":{}},{"cell_type":"code","source":"def train_lightgbm(X, y, X_test, n_splits=5):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros(len(X))\n    test_preds = np.zeros(len(X_test))\n    feature_importance = pd.DataFrame()\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'max_depth': -1,\n        'min_child_samples': 20,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'random_state': SEED,\n        'verbose': -1,\n        'n_jobs': -1\n    }\n    \n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f\"\\n{'='*80}\")\n        print(f\"Fold {fold + 1}/{n_splits}\")\n        print(f\"{'='*80}\")\n        \n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n        \n        model = lgb.train(\n            params, train_data, num_boost_round=2000,\n            valid_sets=[train_data, val_data],\n            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n        )\n        \n        oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n        test_preds += model.predict(X_test, num_iteration=model.best_iteration) / n_splits\n        \n        score = roc_auc_score(y_val, oof_preds[val_idx])\n        fold_scores.append(score)\n        print(f\"Fold {fold + 1} AUC: {score:.6f}\")\n        \n        fold_importance = pd.DataFrame({\n            'feature': X.columns,\n            'importance': model.feature_importance(importance_type='gain'),\n            'fold': fold + 1\n        })\n        feature_importance = pd.concat([feature_importance, fold_importance])\n    \n    overall_score = roc_auc_score(y, oof_preds)\n    print(f\"\\n{'='*80}\")\n    print(f\"LightGBM OOF AUC: {overall_score:.6f}\")\n    print(f\"Mean: {np.mean(fold_scores):.6f} (+/- {np.std(fold_scores):.6f})\")\n    print(f\"{'='*80}\")\n    \n    return oof_preds, test_preds, feature_importance, overall_score\n\nprint(\"\\n Training LightGBM...\")\nlgb_oof, lgb_test, lgb_importance, lgb_score = train_lightgbm(X, y, X_test, n_splits=config.N_SPLITS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T12:27:22.766953Z","iopub.execute_input":"2025-11-07T12:27:22.767163Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.2 XGBoost","metadata":{}},{"cell_type":"code","source":"def train_xgboost(X, y, X_test, n_splits=5):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros(len(X))\n    test_preds = np.zeros(len(X_test))\n    \n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'max_depth': 6,\n        'learning_rate': 0.05,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'min_child_weight': 1,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'random_state': SEED,\n        'tree_method': 'hist',\n        'n_jobs': -1\n    }\n    \n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f\"\\nFold {fold + 1}/{n_splits}\")\n        \n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        model = xgb.XGBClassifier(**params, n_estimators=2000)\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=100,\n            verbose=200\n        )\n        \n        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] / n_splits\n        \n        score = roc_auc_score(y_val, oof_preds[val_idx])\n        fold_scores.append(score)\n        print(f\"Fold {fold + 1} AUC: {score:.6f}\")\n    \n    overall_score = roc_auc_score(y, oof_preds)\n    print(f\"\\nXGBoost OOF AUC: {overall_score:.6f}\")\n    print(f\"Mean: {np.mean(fold_scores):.6f} (+/- {np.std(fold_scores):.6f})\")\n    \n    return oof_preds, test_preds, overall_score\n\nprint(\"\\n Training XGBoost...\")\nxgb_oof, xgb_test, xgb_score = train_xgboost(X, y, X_test, n_splits=config.N_SPLITS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.3 CatBoost","metadata":{}},{"cell_type":"code","source":"def train_catboost(X, y, X_test, n_splits=5):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros(len(X))\n    test_preds = np.zeros(len(X_test))\n    \n    params = {\n        'iterations': 2000,\n        'learning_rate': 0.05,\n        'depth': 6,\n        'l2_leaf_reg': 3,\n        'random_seed': SEED,\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'early_stopping_rounds': 100,\n        'verbose': 200,\n        'task_type': 'CPU'\n    }\n    \n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f\"\\nFold {fold + 1}/{n_splits}\")\n        \n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        model = CatBoostClassifier(**params)\n        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n        \n        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] / n_splits\n        \n        score = roc_auc_score(y_val, oof_preds[val_idx])\n        fold_scores.append(score)\n        print(f\"Fold {fold + 1} AUC: {score:.6f}\")\n    \n    overall_score = roc_auc_score(y, oof_preds)\n    print(f\"\\nCatBoost OOF AUC: {overall_score:.6f}\")\n    print(f\"Mean: {np.mean(fold_scores):.6f} (+/- {np.std(fold_scores):.6f})\")\n    \n    return oof_preds, test_preds, overall_score\n\nprint(\"\\n Training CatBoost...\")\ncat_oof, cat_test, cat_score = train_catboost(X, y, X_test, n_splits=config.N_SPLITS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='evaluation'></a>\n# 7 Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Compare models\ncomparison = pd.DataFrame({\n    'Model': ['LightGBM', 'XGBoost', 'CatBoost'],\n    'OOF AUC': [lgb_score, xgb_score, cat_score]\n}).sort_values('OOF AUC', ascending=False)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*80)\ndisplay(comparison.style.background_gradient(cmap='Greens'))\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Bar chart\naxes[0].bar(comparison['Model'], comparison['OOF AUC'], \n            color=['#FFD700', '#C0C0C0', '#CD7F32'], edgecolor='black', linewidth=2)\naxes[0].set_title('Model Performance', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('OOF AUC', fontsize=12)\naxes[0].grid(alpha=0.3, axis='y')\nfor i, (model, score) in enumerate(zip(comparison['Model'], comparison['OOF AUC'])):\n    axes[0].text(i, score, f'{score:.6f}', ha='center', va='bottom', fontweight='bold')\n\n# ROC curves\nfor name, oof in [('LightGBM', lgb_oof), ('XGBoost', xgb_oof), ('CatBoost', cat_oof)]:\n    fpr, tpr, _ = roc_curve(y, oof)\n    axes[1].plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={roc_auc_score(y, oof):.4f})')\n\naxes[1].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\naxes[1].set_xlabel('FPR', fontsize=12)\naxes[1].set_ylabel('TPR', fontsize=12)\naxes[1].set_title('ROC Curves', fontsize=14, fontweight='bold')\naxes[1].legend(loc='lower right')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature importance from LightGBM\nimportance_df = lgb_importance.groupby('feature')['importance'].mean().sort_values(ascending=False).reset_index()\n\nplt.figure(figsize=(12, 10))\ntop_n = 30\nsns.barplot(data=importance_df.head(top_n), y='feature', x='importance', palette='viridis')\nplt.title(f'Top {top_n} Important Features', fontsize=16, fontweight='bold')\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTop 15 Features:\")\ndisplay(importance_df.head(15))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='ensemble'></a>\n# 8 Ensemble Methods","metadata":{}},{"cell_type":"code","source":"print(\"CREATING ENSEMBLE\")\nprint(\"=\"*80)\n\n# Simple average\nsimple_oof = (lgb_oof + xgb_oof + cat_oof) / 3\nsimple_test = (lgb_test + xgb_test + cat_test) / 3\nsimple_score = roc_auc_score(y, simple_oof)\n\n# Weighted average\ntotal = lgb_score + xgb_score + cat_score\nw_lgb = lgb_score / total\nw_xgb = xgb_score / total\nw_cat = cat_score / total\n\nweighted_oof = lgb_oof * w_lgb + xgb_oof * w_xgb + cat_oof * w_cat\nweighted_test = lgb_test * w_lgb + xgb_test * w_xgb + cat_test * w_cat\nweighted_score = roc_auc_score(y, weighted_oof)\n\n# Rank average\nrank_oof = (rankdata(lgb_oof) + rankdata(xgb_oof) + rankdata(cat_oof)) / (3 * len(y))\nrank_test = (rankdata(lgb_test) + rankdata(xgb_test) + rankdata(cat_test)) / (3 * len(lgb_test))\nrank_score = roc_auc_score(y, rank_oof)\n\nensemble_results = pd.DataFrame({\n    'Ensemble': ['Simple Average', 'Weighted Average', 'Rank Average'],\n    'OOF AUC': [simple_score, weighted_score, rank_score]\n}).sort_values('OOF AUC', ascending=False)\n\nprint(\"\\nEnsemble Results:\")\ndisplay(ensemble_results.style.background_gradient(cmap='Greens'))\n\nprint(f\"\\nWeights: LGB={w_lgb:.3f}, XGB={w_xgb:.3f}, CAT={w_cat:.3f}\")\n\n# Choose best\nbest_idx = ensemble_results['OOF AUC'].idxmax()\nbest_name = ensemble_results.loc[best_idx, 'Ensemble']\nbest_score = ensemble_results.loc[best_idx, 'OOF AUC']\n\nif best_name == 'Simple Average':\n    final_preds = simple_test\nelif best_name == 'Weighted Average':\n    final_preds = weighted_test\nelse:\n    final_preds = rank_test\n\nprint(f\"\\n Best: {best_name} (AUC: {best_score:.6f})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='submission'></a>\n# 9 Submission Generation","metadata":{}},{"cell_type":"code","source":"# # Create submission\n# submission = pd.DataFrame({\n#     'id': test_ids,\n#     config.TARGET: final_preds\n# })\n\n# submission.to_csv('submission.csv', index=False)\n\n# print(\"SUBMISSION CREATED\")\n# print(\"=\"*80)\n# print(f\"File: submission.csv\")\n# print(f\"Shape: {submission.shape}\")\n# print(f\"\\nPreview:\")\n# display(submission.head(10))\n\n# print(f\"\\nStatistics:\")\n# print(submission[config.TARGET].describe())\n\n# # Visualize\n# fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# axes[0].hist(submission[config.TARGET], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n# axes[0].axvline(submission[config.TARGET].mean(), color='red', linestyle='--', linewidth=2,\n#                 label=f'Mean: {submission[config.TARGET].mean():.4f}')\n# axes[0].set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n# axes[0].set_xlabel('Probability', fontsize=12)\n# axes[0].set_ylabel('Frequency', fontsize=12)\n# axes[0].legend()\n# axes[0].grid(alpha=0.3)\n\n# axes[1].boxplot(submission[config.TARGET], vert=True, patch_artist=True,\n#                 boxprops=dict(facecolor='lightblue', alpha=0.7))\n# axes[1].set_title('Box Plot', fontsize=14, fontweight='bold')\n# axes[1].set_ylabel('Probability', fontsize=12)\n# axes[1].grid(alpha=0.3, axis='y')\n\n# plt.tight_layout()\n# plt.show()\n\n# print(\"\\n READY FOR SUBMISSION!\")","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='conclusion'></a>\n# 10 Conclusion & Final Summary","metadata":{}},{"cell_type":"code","source":"# print(\" FINAL SUMMARY\")\n\n# print(\"\\n MODEL SCORES:\")\n# print(f\"LightGBM:          {lgb_score:.6f}\")\n# print(f\"XGBoost:           {xgb_score:.6f}\")\n# print(f\"CatBoost:          {cat_score:.6f}\")\n# print(f\"Simple Ensemble:   {simple_score:.6f}\")\n# print(f\"Weighted Ensemble: {weighted_score:.6f}\")\n# print(f\"Rank Ensemble:     {rank_score:.6f}\")\n\n# print(f\"\\n BEST MODEL: {best_name}\")\n# print(f\" BEST AUC: {best_score:.6f}\")\n\n# print(\"\\n KEY INSIGHTS:\")\n# print(f\"\\n1. Feature Engineering:\")\n# print(f\"• Created {len(feature_cols)} features from 13 original\")\n# print(f\"• Financial ratios were most important\")\n# print(f\"• Risk scores captured complex patterns\")\n\n# print(f\"\\n2. Model Performance:\")\n# best_single = max(lgb_score, xgb_score, cat_score)\n# improvement = ((best_score - best_single) / best_single) * 100\n# print(f\"• Best single: {best_single:.6f}\")\n# print(f\"• Ensemble gain: +{improvement:.4f}%\")\n# print(f\"• 5-fold CV for robustness\")\n\n# print(f\"\\n3. Top 5 Features:\")\n# for i, feat in enumerate(importance_df.head(5)['feature'], 1):\n#     print(f\"   {i}. {feat}\")\n\n# print(\"\\n4. Improvements for Future:\")\n# print(\"• Hyperparameter optimization\")\n# print(\"• Add original dataset\")\n# print(\"• Stacking ensemble\")\n# print(\"• Neural network models\")\n# print(\"• Feature selection\")\n\n# # print(\"\\n\" + \"=\"*80)\n# # print(\" COMPLETE - READY FOR KAGGLE SUBMISSION!\")\n# # print(\"=\"*80)\n# # print(f\"\\n File: submission.csv\")\n# # print(f\" Rows: {len(submission):,}\")\n# # print(f\" Expected LB: ~{best_score:.4f}\")\n# # print(f\"\\n Good luck, {test.shape[0]:,} predictions ready!\")","metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Connect with Me  \n\nFeel free to follow me on these platforms:  \n\n[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/AdilShamim8)  \n[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/adilshamim8)  \n[![Twitter](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/adil_shamim8)  ","metadata":{}},{"cell_type":"markdown","source":"Reference: @yeoyunsianggeremie https://www.kaggle.com/code/yeoyunsianggeremie/ps5e11-agentic-ai-solution-single-xgb/notebook","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport json\nimport logging\nfrom pathlib import Path\nimport shutil\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import QuantileTransformer, PowerTransformer, KBinsDiscretizer\n\nimport xgboost as xgb\nimport optuna  # tuning used only in FULL mode\n\n# -------------------------\n# Setup logging early\n# -------------------------\nBASE_DIR = Path(\"/kaggle/input/playground-series-s5e11\")\nOUTPUT_DIR = Path(\".\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nLOG_FILE = OUTPUT_DIR / \"code_8_1_v4.txt\"\nSUBMISSION_PATH = OUTPUT_DIR / \"submission_4.csv\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n    handlers=[\n        logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\"),\n        logging.StreamHandler(sys.stdout),\n    ],\n)\nprint(\"Log file initialized at %s\", LOG_FILE)\n\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\nprint(\"HF_TOKEN present: %s\", \"yes\" if HF_TOKEN else \"no\")\n\n# -------------------------\n# Device selection for XGBoost (purpose: choose CUDA if available)\n# -------------------------\ndef detect_cuda_available() -> bool:\n    exe = shutil.which(\"nvidia-smi\")\n    if exe is None:\n        return False\n    out = os.popen(f\"{exe} -L\").read().strip()\n    return len(out) > 0\n\nCUDA_AVAILABLE = detect_cuda_available()\nXGB_DEVICE = \"cuda:0\" if CUDA_AVAILABLE else \"cpu\"\nif CUDA_AVAILABLE:\n    print(\"CUDA detected. Using device='%s' with tree_method='hist'.\", XGB_DEVICE)\nelse:\n    print(\"CUDA not detected. Using CPU (device='cpu') with tree_method='hist'.\")\n\n# -------------------------\n# Competition schema\n# -------------------------\nTRAIN_PATH = BASE_DIR / \"train.csv\"\nTEST_PATH = BASE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = BASE_DIR / \"sample_submission.csv\"\n\nTARGET_COL = \"loan_paid_back\"   # binary classification; metric: ROC AUC\nID_COL = \"id\"\nFOLD_COL = \"fold\"\nMETA_COLS = {TARGET_COL, ID_COL, FOLD_COL}\n\n# Optional numeric columns for special transforms (if present)\nC_INCOME = \"annual_income\"\nC_DTI = \"debt_to_income_ratio\"\n\n# -------------------------\n# Load data (purpose: read CSVs; inputs: train/test paths)\n# -------------------------\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\nprint(\"Loaded data. Train shape: %s | Test shape: %s\", train.shape, test.shape)\nassert TARGET_COL in train.columns, f\"Missing target column '{TARGET_COL}' in train.csv\"\nassert ID_COL in train.columns and ID_COL in test.columns, \"Missing id column in train/test.\"\n\n# -------------------------\n# Typing helpers and encoders (purpose: reusable feature builders; inputs: DataFrames/Series)\n# -------------------------\ndef get_cat_num_cols(df: pd.DataFrame, target_col: str, id_col: str, exclude: set):\n    cols = [c for c in df.columns if c not in exclude]\n    cat_cols = [c for c in cols if df[c].dtype == \"object\" or str(df[c].dtype).startswith(\"category\")]\n    num_cols = [c for c in cols if c not in cat_cols]\n    return cat_cols, num_cols\n\ndef pick_top_cats(cat_cols, df, k=6, exclude: set = None):\n    exclude = exclude or set()\n    cands = []\n    for c in cat_cols:\n        if c in exclude:\n            continue\n        n_unique = df[c].nunique(dropna=True)\n        if 2 <= n_unique <= 200:\n            cands.append((c, n_unique))\n    cands.sort(key=lambda t: (-t[1], t[0]))\n    sel = [c for c, _ in cands[:k]]\n    if len(sel) < min(k, len(cat_cols)):\n        rest = [c for c in cat_cols if c not in sel and c not in exclude]\n        sel += rest[: (k - len(sel))]\n    return sel[:k]\n\ndef pick_top_nums(num_cols, df, k=5, exclude: set = None):\n    exclude = exclude or set()\n    stats = []\n    for c in num_cols:\n        if c in exclude:\n            continue\n        series = df[c]\n        if series.dtype.kind not in \"biufc\":\n            continue\n        nunq = series.nunique(dropna=True)\n        if nunq <= 2:\n            continue\n        var = series.var(skipna=True)\n        stats.append((c, 0.0 if pd.isna(var) else float(var)))\n    stats.sort(key=lambda t: -t[1])\n    return [c for c, _ in stats[:k]]\n\ndef add_missing_indicators(df: pd.DataFrame, exclude_cols):\n    for c in df.columns:\n        if c in exclude_cols:\n            continue\n        ind_name = f\"{c}__isna\"\n        if ind_name not in df.columns:\n            df[ind_name] = df[c].isna().astype(np.int8)\n    return df\n\ndef frequency_encode(train_pool: pd.DataFrame, series: pd.Series):\n    counts = train_pool[series.name].value_counts(dropna=False)\n    return counts.to_dict()\n\ndef compute_te_map(x: pd.Series, y: pd.Series, m: float = 10.0):\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    gr = df.groupby(\"x\")[\"y\"].agg([\"mean\", \"count\"])\n    global_mean = float(y.mean())\n    smooth = (gr[\"mean\"] * gr[\"count\"] + global_mean * m) / (gr[\"count\"] + m)\n    return smooth.to_dict(), global_mean\n\ndef oof_target_encode(train_pool_df, y, col, folds, m=10.0):\n    oof = pd.Series(index=train_pool_df.index, dtype=\"float32\")\n    for f, (tr_idx, va_idx) in folds.items():\n        tr_df = train_pool_df.loc[tr_idx]\n        tr_y = y.loc[tr_idx]\n        mp, gmean = compute_te_map(tr_df[col], tr_y, m)\n        oof.loc[va_idx] = train_pool_df.loc[va_idx, col].map(mp).fillna(gmean).astype(\"float32\")\n    full_map, full_gmean = compute_te_map(train_pool_df[col], y, m)\n    return oof, full_map, full_gmean\n\ndef compute_woe_map(x: pd.Series, y: pd.Series, eps: float = 0.5):\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    pos = df.groupby(\"x\")[\"y\"].sum(min_count=1)\n    cnt = df.groupby(\"x\")[\"y\"].count()\n    neg = cnt - pos\n    total_pos = float(pos.sum())\n    total_neg = float(neg.sum())\n    dist_pos = (pos + eps) / (total_pos + eps * len(pos))\n    dist_neg = (neg + eps) / (total_neg + eps * len(neg))\n    woe = np.log((dist_pos) / (dist_neg))\n    mapping = woe.to_dict()\n    iv = ((dist_pos - dist_neg) * woe).sum()\n    return mapping, float(iv)\n\ndef oof_woe_encode(train_pool_df, y, col, folds, eps=0.5):\n    oof = pd.Series(index=train_pool_df.index, dtype=\"float32\")\n    for f, (tr_idx, va_idx) in folds.items():\n        tr_df = train_pool_df.loc[tr_idx]\n        tr_y = y.loc[tr_idx]\n        mp, _iv = compute_woe_map(tr_df[col], tr_y, eps)\n        # Clip WOE values to stabilize\n        mp = {k: float(np.clip(v, -3.0, 3.0)) for k, v in mp.items()}\n        oof.loc[va_idx] = train_pool_df.loc[va_idx, col].map(mp).fillna(0.0).astype(\"float32\")\n    full_map, iv_full = compute_woe_map(train_pool_df[col], y, eps)\n    full_map = {k: float(np.clip(v, -3.0, 3.0)) for k, v in full_map.items()}\n    return oof, full_map, iv_full\n\ndef fit_kbins(train_pool_series, n_bins=10):\n    med = float(np.nanmedian(train_pool_series.values))\n    tr_vals = train_pool_series.fillna(med).values.reshape(-1, 1)\n    enc = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"quantile\")\n    enc.fit(tr_vals)\n    return enc, med\n\ndef transform_kbins(enc, med, series):\n    vals = series.fillna(med).values.reshape(-1, 1)\n    b = enc.transform(vals).astype(\"float32\").reshape(-1)\n    b = np.where(np.isfinite(b), b, -1.0)\n    return pd.Series(b, index=series.index, dtype=\"float32\")\n\ndef fit_rank_gaussian(train_pool_series, random_state=2025):\n    med = float(np.nanmedian(train_pool_series.values))\n    tr_vals = train_pool_series.fillna(med).values.reshape(-1, 1)\n    qt = QuantileTransformer(n_quantiles=min(1000, len(tr_vals)), output_distribution=\"normal\", random_state=random_state)\n    qt.fit(tr_vals)\n    return qt, med\n\ndef transform_rank_gaussian(qt, med, series):\n    vals = series.fillna(med).values.reshape(-1, 1)\n    out = qt.transform(vals).astype(\"float32\").reshape(-1)\n    return pd.Series(out, index=series.index, dtype=\"float32\")\n\ndef fit_yeojohnson(train_pool_series):\n    med = float(np.nanmedian(train_pool_series.values))\n    tr_vals = train_pool_series.fillna(med).values.reshape(-1, 1)\n    pt = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n    pt.fit(tr_vals)\n    return pt, med\n\ndef transform_yeojohnson(pt, med, series):\n    vals = series.fillna(med).values.reshape(-1, 1)\n    out = pt.transform(vals).astype(\"float32\").reshape(-1)\n    return pd.Series(out, index=series.index, dtype=\"float32\")\n\ndef group_mean_deviation(train_pool_df, val_df, test_df, cat_cols, num_cols):\n    # Fit group means on train_pool and map to val/test; guard meta columns.\n    for c in cat_cols:\n        if c in META_COLS or c not in train_pool_df.columns:\n            continue\n        for n in num_cols:\n            if n in META_COLS or n not in train_pool_df.columns:\n                continue\n            gname = f\"{n}__gm_{c}\"\n            devname = f\"{n}__dev_{c}\"\n            grp = train_pool_df.groupby(c, observed=True)[n].mean()\n            global_mean = float(train_pool_df[n].mean())\n            train_pool_df[gname] = train_pool_df[c].map(grp).fillna(global_mean).astype(\"float32\")\n            val_df[gname] = val_df[c].map(grp).fillna(global_mean).astype(\"float32\")\n            test_df[gname] = test_df[c].map(grp).fillna(global_mean).astype(\"float32\")\n            train_pool_df[devname] = (train_pool_df[n] - train_pool_df[gname]).astype(\"float32\")\n            val_df[devname] = (val_df[n] - val_df[gname]).astype(\"float32\")\n            test_df[devname] = (test_df[n] - test_df[gname]).astype(\"float32\")\n    return train_pool_df, val_df, test_df\n\ndef group_percentile_feature(train_pool_df, val_df, test_df, group_col, value_col, feature_name, q=100):\n    if group_col not in train_pool_df.columns or value_col not in train_pool_df.columns:\n        print(\"Percentile feature skipped (missing): %s within %s\", value_col, group_col)\n        return train_pool_df, val_df, test_df\n    edges_dict = {}\n    for g, sub in train_pool_df[[group_col, value_col]].dropna().groupby(group_col, observed=True):\n        vals = sub[value_col].values\n        if len(vals) < 2:\n            continue\n        qs = np.linspace(0.0, 1.0, q + 1)\n        try_edges = np.quantile(vals, qs)\n        edges = try_edges.copy()\n        for i in range(1, len(edges)):\n            if edges[i] <= edges[i - 1]:\n                edges[i] = np.nextafter(edges[i - 1], float(\"inf\"))\n        edges_dict[g] = edges\n\n    def apply_edges(df_in: pd.DataFrame):\n        out = pd.Series(index=df_in.index, dtype=\"float32\")\n        out.iloc[:] = np.nan\n        for g, idx in df_in.groupby(group_col, observed=True).groups.items():\n            e = edges_dict.get(g, None)\n            if e is None:\n                out.loc[idx] = 0.5\n                continue\n            v = df_in.loc[idx, value_col].fillna(e[0]).values\n            bins = np.digitize(v, e[1:-1], right=True)\n            denom = max(1, len(e) - 2)\n            out.loc[idx] = bins.astype(\"float32\") / float(denom)\n        out.fillna(0.5, inplace=True)\n        return out\n\n    train_pool_df[feature_name] = apply_edges(train_pool_df[[group_col, value_col]].copy())\n    val_df[feature_name] = apply_edges(val_df[[group_col, value_col]].copy())\n    test_df[feature_name] = apply_edges(test_df[[group_col, value_col]].copy())\n    return train_pool_df, val_df, test_df\n\n# -------------------------\n# Global feature selections (purpose: choose candidate categorical and numeric columns)\n# -------------------------\nexclude_for_typing = {TARGET_COL, ID_COL, FOLD_COL}\nall_cat, all_num = get_cat_num_cols(train, TARGET_COL, ID_COL, exclude=exclude_for_typing)\nsel_cat = pick_top_cats(all_cat, train, k=6, exclude=META_COLS)\nsel_num_for_deviation = pick_top_nums(all_num, train, k=5, exclude=META_COLS)\ntransform_targets = [c for c in [C_INCOME, C_DTI] if c in train.columns]\nall_features_for_te = [c for c in (all_cat + all_num) if c not in META_COLS]\nprint(\"Selected categoricals (≤6): %s\", sel_cat)\nprint(\"Selected numeric for group-mean deviations (≤5): %s\", sel_num_for_deviation)\nprint(\"Numeric transform targets: %s\", transform_targets)\nprint(\"All features for target encoding (%d): %s\", len(all_features_for_te), all_features_for_te)\n\n# -------------------------\n# Preprocess for arbitrary held-out fold (purpose: per-fold encoders; inputs: train_df, test_df, held_out_fold)\n# -------------------------\ndef preprocess_for_outer_fold(train_df, test_df, held_out_fold, sel_cat, sel_num_for_deviation, transform_targets, all_features_for_te):\n    \"\"\"Fit encoders/transforms on train_pool=all folds except held_out_fold; apply to its validation and test.\"\"\"\n    if held_out_fold == -1:\n        tr_pool = train_df.copy()\n        va = train_df.iloc[0:0].copy()  # empty\n    else:\n        tr_pool = train_df.loc[train_df[FOLD_COL] != held_out_fold].copy()\n        va = train_df.loc[train_df[FOLD_COL] == held_out_fold].copy()\n    te = test_df.copy()\n    y_pool = tr_pool[TARGET_COL].astype(int)\n\n    # Inner folds based on existing assignment in tr_pool\n    inner_fold_ids = sorted(int(f) for f in tr_pool[FOLD_COL].unique().tolist())\n    inner_folds = {}\n    for f in inner_fold_ids:\n        inner_tr_idx = tr_pool.index[tr_pool[FOLD_COL] != f]\n        inner_va_idx = tr_pool.index[tr_pool[FOLD_COL] == f]\n        inner_folds[int(f)] = (inner_tr_idx, inner_va_idx)\n\n    # Frequency encoding\n    for c in sel_cat:\n        if c not in tr_pool.columns:\n            continue\n        mapping = frequency_encode(tr_pool, tr_pool[c])\n        tr_pool[f\"{c}__freq\"] = tr_pool[c].map(mapping).fillna(0).astype(\"float32\")\n        if len(va) > 0:\n            va[f\"{c}__freq\"] = va[c].map(mapping).fillna(0).astype(\"float32\")\n        te[f\"{c}__freq\"] = te[c].map(mapping).fillna(0).astype(\"float32\")\n\n    # OOF TE on ALL features (categorical + numerical)\n    for c in all_features_for_te:\n        if c not in tr_pool.columns:\n            continue\n        oof_te, te_map, te_g = oof_target_encode(tr_pool, y_pool, c, inner_folds, m=10.0)\n        tr_pool[f\"{c}__te_m10\"] = oof_te.astype(\"float32\")\n        if len(va) > 0:\n            va[f\"{c}__te_m10\"] = va[c].map(te_map).fillna(te_g).astype(\"float32\")\n        te[f\"{c}__te_m10\"] = te[c].map(te_map).fillna(te_g).astype(\"float32\")\n\n    # OOF WOE (clip WOE) - only for categorical features\n    for c in sel_cat:\n        if c not in tr_pool.columns:\n            continue\n        oof_woe, woe_map, _iv = oof_woe_encode(tr_pool, y_pool, c, inner_folds, eps=0.5)\n        tr_pool[f\"{c}__woe\"] = oof_woe.astype(\"float32\")\n        if len(va) > 0:\n            va[f\"{c}__woe\"] = va[c].map(woe_map).fillna(0.0).astype(\"float32\")\n        te[f\"{c}__woe\"] = te[c].map(woe_map).fillna(0.0).astype(\"float32\")\n\n    # Numeric transforms on income & DTI\n    for col in transform_targets:\n        if col not in tr_pool.columns:\n            continue\n        enc, med = fit_kbins(tr_pool[col], n_bins=10)\n        tr_pool[f\"{col}__qbin10\"] = transform_kbins(enc, med, tr_pool[col])\n        if len(va) > 0:\n            va[f\"{col}__qbin10\"] = transform_kbins(enc, med, va[col])\n        te[f\"{col}__qbin10\"] = transform_kbins(enc, med, te[col])\n\n        qt, med_q = fit_rank_gaussian(tr_pool[col])\n        tr_pool[f\"{col}__rgauss\"] = transform_rank_gaussian(qt, med_q, tr_pool[col])\n        if len(va) > 0:\n            va[f\"{col}__rgauss\"] = transform_rank_gaussian(qt, med_q, va[col])\n        te[f\"{col}__rgauss\"] = transform_rank_gaussian(qt, med_q, te[col])\n\n        pt, med_p = fit_yeojohnson(tr_pool[col])\n        tr_pool[f\"{col}__yeoj\"] = transform_yeojohnson(pt, med_p, tr_pool[col])\n        if len(va) > 0:\n            va[f\"{col}__yeoj\"] = transform_yeojohnson(pt, med_p, va[col])\n        te[f\"{col}__yeoj\"] = transform_yeojohnson(pt, med_p, te[col])\n\n    # Group mean deviations\n    tr_pool, va, te = group_mean_deviation(tr_pool, va, te, sel_cat, sel_num_for_deviation)\n\n    # Percentile features\n    if \"credit_score\" in tr_pool.columns and \"grade_subgrade\" in tr_pool.columns:\n        tr_pool, va, te = group_percentile_feature(tr_pool, va, te, \"grade_subgrade\", \"credit_score\", \"credit_score__pctl_in_grade\")\n    if \"credit_score\" in tr_pool.columns and \"education_level\" in tr_pool.columns:\n        tr_pool, va, te = group_percentile_feature(tr_pool, va, te, \"education_level\", \"credit_score\", \"credit_score__pctl_in_edu\")\n\n    # Missingness indicators\n    tr_pool = add_missing_indicators(tr_pool, exclude_cols=META_COLS)\n    if len(va) > 0:\n        va = add_missing_indicators(va, exclude_cols=META_COLS)\n    te = add_missing_indicators(te, exclude_cols={ID_COL})\n\n    # Feature list: original numeric (excluding raw categoricals/meta) + engineered blocks\n    excl = {TARGET_COL, ID_COL, FOLD_COL}\n    raw_cat, raw_num = get_cat_num_cols(train_df, TARGET_COL, ID_COL, exclude=excl)\n    raw_num_cols = [c for c in raw_num if c not in META_COLS]\n\n    eng_cols = [c for c in tr_pool.columns if (\n        c not in train_df.columns or\n        c.endswith(\"__freq\") or c.endswith(\"__te_m10\") or c.endswith(\"__woe\") or\n        \"__gm_\" in c or \"__dev_\" in c or\n        c.endswith(\"__qbin10\") or c.endswith(\"__rgauss\") or c.endswith(\"__yeoj\") or\n        c.endswith(\"__isna\") or\n        c.endswith(\"__pctl_in_grade\") or c.endswith(\"__pctl_in_edu\")\n    )]\n    feature_cols = sorted(set(raw_num_cols + eng_cols))\n    feature_cols = [c for c in feature_cols if (c not in META_COLS and not c.endswith(\"__iv\"))]\n\n    X_tr = tr_pool[feature_cols].copy()\n    y_tr = tr_pool[TARGET_COL].astype(int).copy()\n    if len(va) > 0:\n        X_va = va[feature_cols].copy()\n        y_va = va[TARGET_COL].astype(int).copy()\n    else:\n        X_va = va  # empty\n        y_va = va  # empty\n    X_te = te[feature_cols].copy()\n    return X_tr, y_tr, X_va, y_va, X_te, feature_cols\n\n# -------------------------\n# XGBoost params and trainers\n# -------------------------\ndef build_xgb_params(base_lr=0.05, n_estimators=1500, early_stopping_rounds=100):\n    params = dict(\n        booster=\"gbtree\",\n        objective=\"binary:logistic\",\n        eval_metric=\"auc\",\n        tree_method=\"hist\",\n        device=XGB_DEVICE,   # 'cuda:0' or 'cpu'\n        learning_rate=base_lr,\n        max_depth=6,\n        min_child_weight=8,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        colsample_bylevel=0.8,\n        gamma=0.0,\n        reg_lambda=1.0,\n        reg_alpha=0.0,\n        max_bin=256,\n        grow_policy=\"depthwise\",\n        random_state=2025,\n        n_estimators=n_estimators,\n        n_jobs=0,\n        early_stopping_rounds=early_stopping_rounds,\n        verbosity=1,\n    )\n    return params\n\ndef train_xgb_single(X_tr, y_tr, X_va, y_va, params, label=\"baseline\"):\n    t0 = time.time()\n    clf = xgb.XGBClassifier(**params)\n    clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n    best_it = getattr(clf, \"best_iteration\", None)\n    y_pred_va = clf.predict_proba(X_va, iteration_range=(0, best_it + 1) if best_it is not None else None)[:, 1]\n    val_auc = roc_auc_score(y_va, y_pred_va)\n    elapsed = time.time() - t0\n    print(\"XGB %s: val AUC=%.6f | best_iteration=%s | time=%.1fs\", label, val_auc, str(best_it), elapsed)\n    return clf, val_auc, best_it, elapsed\n\ndef optuna_tune_xgb(X_tr, y_tr, X_va, y_va, base_params, time_budget_sec=300):\n    print(\"Optuna tuning start (budget=%ds).\", time_budget_sec)\n    study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_ps_s5e11_v4\")\n\n    def objective(trial: optuna.trial.Trial):\n        p = base_params.copy()\n        p.update({\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.12),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 9),\n            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 2.0, 12.0),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.5, 5.0, log=True),\n            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 2.0),\n            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n            \"max_bin\": trial.suggest_categorical(\"max_bin\", [128, 256, 512]),\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 600, 1500),\n        })\n        # Keep device and tree_method fixed\n        p[\"tree_method\"] = base_params[\"tree_method\"]\n        p[\"device\"] = base_params[\"device\"]\n        p[\"random_state\"] = 2025\n        p[\"early_stopping_rounds\"] = base_params[\"early_stopping_rounds\"]\n\n        model = xgb.XGBClassifier(**p)\n        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n        best_it = getattr(model, \"best_iteration\", None)\n        y_pred = model.predict_proba(X_va, iteration_range=(0, best_it + 1) if best_it is not None else None)[:, 1]\n        auc = roc_auc_score(y_va, y_pred)\n        return auc\n\n    study.optimize(objective, n_trials=200, timeout=time_budget_sec, gc_after_trial=True)\n    best_params = study.best_params\n    best_value = study.best_value\n    print(\"Optuna best AUC=%.6f with params=%s\", best_value, json.dumps(best_params))\n\n    tuned_params = base_params.copy()\n    tuned_params.update(best_params)\n    # Retrain on the same fold-0 split to verify\n    model = xgb.XGBClassifier(**tuned_params)\n    t0 = time.time()\n    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n    best_it = getattr(model, \"best_iteration\", None)\n    y_pred = model.predict_proba(X_va, iteration_range=(0, best_it + 1) if best_it is not None else None)[:, 1]\n    auc = roc_auc_score(y_va, y_pred)\n    elapsed = time.time() - t0\n    print(\"Tuned retrain: val AUC=%.6f | best_iteration=%s | retrain_time=%.1fs\", auc, str(best_it), elapsed)\n    return model, auc, best_it, tuned_params\n\n# -------------------------\n# CV trainer and final refit\n# -------------------------\ndef assign_outer_folds(df: pd.DataFrame, n_splits=5, seed=2025):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    folds = np.full(len(df), -1, dtype=int)\n    for i, (_, va_idx) in enumerate(skf.split(df.drop(columns=[TARGET_COL]), df[TARGET_COL].values)):\n        folds[va_idx] = i\n    out = df.copy()\n    out[FOLD_COL] = folds\n    return out\n\ndef train_xgb_cv_and_predict(train_df, test_df, params, n_splits=5, debug=False):\n    print(\"Starting %d-fold CV training with per-fold encoders.\", n_splits)\n    train_df = assign_outer_folds(train_df, n_splits=n_splits, seed=2025)\n    oof = np.zeros(len(train_df), dtype=np.float32)\n    test_preds = []\n    fold_aucs = []\n    best_its = []\n\n    for f in range(n_splits):\n        print(\"Fold %d/%d: preprocessing (fit on train folds only).\", f+1, n_splits)\n        X_tr, y_tr, X_va, y_va, X_te, feats = preprocess_for_outer_fold(\n            train_df, test_df, held_out_fold=f,\n            sel_cat=sel_cat, sel_num_for_deviation=sel_num_for_deviation, transform_targets=transform_targets,\n            all_features_for_te=all_features_for_te\n        )\n        params_use = params.copy()\n        if debug:\n            # Downsample training to 1000 rows in DEBUG to save time\n            if len(X_tr) > 1000:\n                X_tr, _, y_tr, _ = train_test_split(X_tr, y_tr, test_size=(1.0 - 1000/len(X_tr)), stratify=y_tr, random_state=2025)\n            params_use[\"n_estimators\"] = min(200, params_use.get(\"n_estimators\", 1500))\n            params_use[\"early_stopping_rounds\"] = min(20, params_use.get(\"early_stopping_rounds\", 100))\n\n        print(\"Fold %d: training XGBoost.\", f+1)\n        clf = xgb.XGBClassifier(**params_use)\n        clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n        best_it = getattr(clf, \"best_iteration\", None)\n        y_va_pred = clf.predict_proba(X_va, iteration_range=(0, best_it + 1) if best_it is not None else None)[:, 1]\n        fold_auc = roc_auc_score(y_va, y_va_pred)\n        fold_aucs.append(fold_auc)\n        oof[train_df.index[train_df[FOLD_COL] == f]] = y_va_pred\n        y_te_pred = clf.predict_proba(X_te, iteration_range=(0, best_it + 1) if best_it is not None else None)[:, 1]\n        test_preds.append(y_te_pred)\n        best_its.append(best_it if best_it is not None else params_use.get(\"n_estimators\", 1000))\n        print(\"Fold %d: AUC=%.6f | best_iteration=%s\", f+1, fold_auc, str(best_it))\n\n    oof_auc = roc_auc_score(train_df[TARGET_COL].values, oof)\n    y_test_cv = np.mean(np.vstack(test_preds), axis=0)\n    print(\"CV complete. OOF AUC=%.6f | per-fold AUCs=%s | median best_it=%d\",\n                 oof_auc, [round(a, 6) for a in fold_aucs], int(np.median(best_its)))\n    return y_test_cv, oof_auc, int(np.median(best_its)), feats\n\ndef refit_full_and_predict(train_df, test_df, params, debug=False):\n    print(\"Refit on all training data with inner OOF encoders; no held-out validation.\")\n    # Assign inner folds (for encoders) deterministically\n    train_df_full = assign_outer_folds(train_df, n_splits=5, seed=2025)\n    X_tr_full, y_tr_full, X_va_dummy, y_va_dummy, X_te_full, feats_full = preprocess_for_outer_fold(\n        train_df_full, test_df, held_out_fold=-1,\n        sel_cat=sel_cat, sel_num_for_deviation=sel_num_for_deviation, transform_targets=transform_targets,\n        all_features_for_te=all_features_for_te\n    )\n    params_use = params.copy()\n    if debug:\n        if len(X_tr_full) > 1000:\n            X_tr_full, _, y_tr_full, _ = train_test_split(X_tr_full, y_tr_full, test_size=(1.0 - 1000/len(X_tr_full)), stratify=y_tr_full, random_state=2025)\n        params_use[\"n_estimators\"] = min(200, params_use.get(\"n_estimators\", 1500))\n        params_use[\"early_stopping_rounds\"] = min(20, params_use.get(\"early_stopping_rounds\", 100))\n\n    # For full refit, use training data as eval_set just to track rounds; acceptable since encoders are fixed.\n    clf_full = xgb.XGBClassifier(**params_use)\n    clf_full.fit(X_tr_full, y_tr_full, eval_set=[(X_tr_full, y_tr_full)], verbose=False)\n    best_it_full = getattr(clf_full, \"best_iteration\", None)\n    y_test_full = clf_full.predict_proba(X_te_full, iteration_range=(0, best_it_full + 1) if best_it_full is not None else None)[:, 1]\n    print(\"Full refit complete. best_iteration=%s | n_features=%d\", str(best_it_full), X_tr_full.shape[1])\n    return y_test_full, best_it_full, feats_full\n\n# -------------------------\n# Main pipeline runs twice: DEBUG then FULL\n# -------------------------\ndef run_pipeline(DEBUG: bool):\n    mode = \"DEBUG\" if DEBUG else \"FULL\"\n    print(\"===== Running in %s mode =====\", mode)\n\n    # Create a single 5-fold assignment for baseline/tuning on fold 0\n    train_folds = assign_outer_folds(train.copy(), n_splits=5, seed=2025)\n    # Preprocess for fold 0 for baseline/tuning\n    X_tr0, y_tr0, X_va0, y_va0, X_te0, feats0 = preprocess_for_outer_fold(\n        train_folds, test.copy(), held_out_fold=0,\n        sel_cat=sel_cat, sel_num_for_deviation=sel_num_for_deviation, transform_targets=transform_targets,\n        all_features_for_te=all_features_for_te\n    )\n\n    # Baseline params (reduced trees in DEBUG)\n    if DEBUG:\n        base_params = build_xgb_params(base_lr=0.05, n_estimators=150, early_stopping_rounds=20)\n        # Downsample training to 1000 rows for the initial fold-0 baseline\n        if len(X_tr0) > 1000:\n            X_tr0, _, y_tr0, _ = train_test_split(X_tr0, y_tr0, test_size=(1.0 - 1000/len(X_tr0)), stratify=y_tr0, random_state=2025)\n    else:\n        base_params = build_xgb_params(base_lr=0.05, n_estimators=1500, early_stopping_rounds=100)\n\n    print(\"Baseline training on fold 0 (purpose: establish reference AUC).\")\n    model_base, auc_base, best_it_base, t_base = train_xgb_single(X_tr0, y_tr0, X_va0, y_va0, base_params, label=\"baseline-fold0\")\n\n    # Tuning only in FULL mode\n    if not DEBUG:\n        model_tuned, auc_tuned, best_it_tuned, tuned_params = optuna_tune_xgb(\n            X_tr0, y_tr0, X_va0, y_va0, base_params, time_budget_sec=300\n        )\n        if auc_tuned >= auc_base:\n            final_params = tuned_params\n            print(\"Selected tuned params (AUC=%.6f >= baseline %.6f).\", auc_tuned, auc_base)\n        else:\n            final_params = base_params\n            print(\"Selected baseline params (AUC=%.6f > tuned %.6f).\", auc_base, auc_tuned)\n    else:\n        final_params = base_params\n        print(\"DEBUG mode: tuning skipped; using baseline params.\")\n\n    # CV training + predictions\n    y_test_cv, oof_auc, median_best_it, feats_cv = train_xgb_cv_and_predict(\n        train.copy(), test.copy(), final_params, n_splits=5, debug=DEBUG\n    )\n\n    if DEBUG:\n        print(\"DEBUG mode: submission generation skipped per requirements.\")\n        return\n\n    # Full refit + predictions\n    y_test_full, best_it_full, feats_full = refit_full_and_predict(\n        train.copy(), test.copy(), final_params, debug=False\n    )\n\n    # Blend CV ensemble with full-refit model (simple mean)\n    y_test_final = 0.5 * y_test_cv + 0.5 * y_test_full\n    y_test_final = np.clip(y_test_final, 1e-9, 1 - 1e-9)\n\n    # Write submission\n    submission = pd.DataFrame({ID_COL: test[ID_COL].values, TARGET_COL: y_test_final})\n    submission.to_csv(SUBMISSION_PATH, index=False)\n    print(\"Submission written to %s\", SUBMISSION_PATH)\n\n    # Log prediction distribution\n    desc = pd.Series(y_test_final).describe(percentiles=[0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99])\n    print(\"Prediction distribution summary:\\n%s\", desc.to_string())\n    print(\"Run complete: OOF AUC=%.6f | median_best_it(CV)=%d | device=%s\", oof_auc, median_best_it, XGB_DEVICE)\n\n# -------------------------\n# Execute: DEBUG then FULL\n# -------------------------\nrun_pipeline(DEBUG=True)   # no submission\nrun_pipeline(DEBUG=False)  # produce submission_4.csv","metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null}]}