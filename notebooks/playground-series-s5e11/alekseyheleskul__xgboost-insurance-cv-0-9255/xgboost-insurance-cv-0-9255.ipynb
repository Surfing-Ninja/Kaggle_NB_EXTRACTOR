{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"credit to @masayakawamata for his cool XGB baseline, which I partially used and adapted here.","metadata":{}},{"cell_type":"code","source":"# for more information : https://www.kaggle.com/competitions/playground-series-s5e10/discussion/612990\n\n!pip install xgboost -U -q # should be xgboost 3.1.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:14.062307Z","iopub.execute_input":"2025-11-02T22:58:14.062864Z","iopub.status.idle":"2025-11-02T22:58:23.72919Z","shell.execute_reply.started":"2025-11-02T22:58:14.062843Z","shell.execute_reply":"2025-11-02T22:58:23.728445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"While most of the Kaggle playground competitions can be described as a funny picture below, this notebook serves as a baseline for predicting loan payback.\n\nSo, let's start with the first character pulling the rope â€” the grandfather. In our case, this is XGBoost, which gives us a CV of 0.9252.","metadata":{}},{"cell_type":"markdown","source":"![kaggle comp](https://i.ibb.co/bjFdHNTC/photo-2025-11-01-12-33-30-PM.jpg)","metadata":{}},{"cell_type":"markdown","source":"Updated version: Added TE encoding on low gain columns","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTARGET = 'loan_paid_back'\nN_SPLITS = 11\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:23.730571Z","iopub.execute_input":"2025-11-02T22:58:23.73095Z","iopub.status.idle":"2025-11-02T22:58:26.970115Z","shell.execute_reply.started":"2025-11-02T22:58:23.730922Z","shell.execute_reply":"2025-11-02T22:58:26.969487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig_df = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\nprint(f\"Train Shape: {train_df.shape}, Test Shape: {test_df.shape}, Orig Shape: {orig_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:26.970742Z","iopub.execute_input":"2025-11-02T22:58:26.971127Z","iopub.status.idle":"2025-11-02T22:58:28.943507Z","shell.execute_reply.started":"2025-11-02T22:58:26.971104Z","shell.execute_reply":"2025-11-02T22:58:28.942633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['is_train'] = 1\ntest_df['is_train'] = 0\n\ncombined_df = pd.concat([train_df, test_df.assign(loan_paid_back=0)], ignore_index=True)\n\nprint(f\"Train Shape: {train_df.shape}, Test Shape: {test_df.shape}, Orig Shape: {orig_df.shape}\")\nprint(f\"Combined Shape: {combined_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:28.945455Z","iopub.execute_input":"2025-11-02T22:58:28.94567Z","iopub.status.idle":"2025-11-02T22:58:29.061524Z","shell.execute_reply.started":"2025-11-02T22:58:28.945654Z","shell.execute_reply":"2025-11-02T22:58:29.060723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nBASE_FEATURES = [col for col in train_df.columns if col not in ['id', TARGET, 'is_train']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:29.062545Z","iopub.execute_input":"2025-11-02T22:58:29.062813Z","iopub.status.idle":"2025-11-02T22:58:29.066786Z","shell.execute_reply.started":"2025-11-02T22:58:29.062782Z","shell.execute_reply":"2025-11-02T22:58:29.066073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df[TARGET].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:29.06745Z","iopub.execute_input":"2025-11-02T22:58:29.067756Z","iopub.status.idle":"2025-11-02T22:58:29.090981Z","shell.execute_reply.started":"2025-11-02T22:58:29.067728Z","shell.execute_reply":"2025-11-02T22:58:29.090237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(orig_df[TARGET].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:29.091779Z","iopub.execute_input":"2025-11-02T22:58:29.09205Z","iopub.status.idle":"2025-11-02T22:58:29.105097Z","shell.execute_reply.started":"2025-11-02T22:58:29.092034Z","shell.execute_reply":"2025-11-02T22:58:29.10434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This isn't perfectly uniform, but it's far from a serious imbalance. In fact, it's quite normal for AUC. Since this metric evaluates both rank and prediction accuracy, it's robust to even moderate imbalances.","metadata":{}},{"cell_type":"code","source":"print(combined_df[TARGET].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:29.105798Z","iopub.execute_input":"2025-11-02T22:58:29.106053Z","iopub.status.idle":"2025-11-02T22:58:29.125275Z","shell.execute_reply.started":"2025-11-02T22:58:29.106031Z","shell.execute_reply":"2025-11-02T22:58:29.124678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we are fine!","metadata":{}},{"cell_type":"code","source":"ORIG_FEATURES = []\n\n\nfor col in BASE_FEATURES:\n\n    agg_df = orig_df.groupby(col)[TARGET].agg(['mean', 'size']).reset_index()\n    \n    new_mean_col_name = f\"orig_mean_{col}\"\n    new_count_col_name = f\"orig_count_{col}\"\n    agg_df.rename(columns={'mean': new_mean_col_name, 'size': new_count_col_name}, inplace=True)\n    \n    combined_df = combined_df.merge(agg_df, on=col, how='left')\n    \n    ORIG_FEATURES.extend([new_mean_col_name, new_count_col_name])\n\ncombined_df.annual_income.astype(int)\nprint(f\"{len(ORIG_FEATURES)} features created from original data.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:29.125968Z","iopub.execute_input":"2025-11-02T22:58:29.126273Z","iopub.status.idle":"2025-11-02T22:58:31.838728Z","shell.execute_reply.started":"2025-11-02T22:58:29.12625Z","shell.execute_reply":"2025-11-02T22:58:31.837919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bins = [-1, 2000, 3000, 100000]\nlabels = ['rare_emp_status', 'medium_freq_emp_status', 'high_freq_emp_status']\nnew_freq_feature = 'employment_status_freq_cat'\n\ncombined_df[new_freq_feature] = pd.cut(\n    combined_df['orig_count_employment_status'],\n    bins=bins,\n    labels=labels\n).astype('category')\n\nprint(f\"new feature '{new_freq_feature}' created.\")\nprint(combined_df[new_freq_feature].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:31.840997Z","iopub.execute_input":"2025-11-02T22:58:31.841217Z","iopub.status.idle":"2025-11-02T22:58:31.869847Z","shell.execute_reply.started":"2025-11-02T22:58:31.8412Z","shell.execute_reply":"2025-11-02T22:58:31.869248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ALL_FEATURES = BASE_FEATURES + ORIG_FEATURES + [new_freq_feature]\nCATS.append(new_freq_feature)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:31.87054Z","iopub.execute_input":"2025-11-02T22:58:31.871087Z","iopub.status.idle":"2025-11-02T22:58:31.874642Z","shell.execute_reply.started":"2025-11-02T22:58:31.871061Z","shell.execute_reply":"2025-11-02T22:58:31.874045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_df[\"annual_income\"] = (combined_df[\"annual_income\"]//100)*100\ncombined_df[\"loan_amount\"] = (combined_df[\"loan_amount\"]//100)*100\n\ncombined_df['credit_score_cat'] = pd.qcut(combined_df['credit_score'], q=15, labels=False, duplicates='drop')\nALL_FEATURES.append('credit_score_cat')\n\ncombined_df['dti_cat'] = pd.qcut(combined_df['debt_to_income_ratio'], q=15, labels=False, duplicates='drop')\nALL_FEATURES.append('dti_cat')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:31.875412Z","iopub.execute_input":"2025-11-02T22:58:31.87562Z","iopub.status.idle":"2025-11-02T22:58:32.027759Z","shell.execute_reply.started":"2025-11-02T22:58:31.875604Z","shell.execute_reply":"2025-11-02T22:58:32.027117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CATS = []\nNUMS = []\nfor c in ALL_FEATURES:\n    t = \"CAT\"\n    if combined_df[c].dtype=='object':\n        CATS.append(c)\n    else:\n        NUMS.append(c)\n        t = \"NUM\"\n    n = combined_df[c].nunique()\n    na = combined_df[c].isna().sum()\n    print(f\"[{t}] {c} has {n} unique and {na} NA\")\nprint(\"CATS:\", CATS )\nprint(\"NUMS:\", NUMS )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:32.028526Z","iopub.execute_input":"2025-11-02T22:58:32.028809Z","iopub.status.idle":"2025-11-02T22:58:32.877496Z","shell.execute_reply.started":"2025-11-02T22:58:32.028784Z","shell.execute_reply":"2025-11-02T22:58:32.876822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in CATS:\n    combined_df[col] = combined_df[col].astype('category')\n    \ntrain_final = combined_df[combined_df['is_train'] == 1].drop(columns=['is_train'])\ntest_final = combined_df[combined_df['is_train'] == 0].drop(columns=['is_train', TARGET])\n\nX = train_final[ALL_FEATURES]\ny = train_final[TARGET]\nX_test = test_final[ALL_FEATURES]\n\nprint(f\"Features {len(ALL_FEATURES)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:32.878178Z","iopub.execute_input":"2025-11-02T22:58:32.878431Z","iopub.status.idle":"2025-11-02T22:58:33.437836Z","shell.execute_reply.started":"2025-11-02T22:58:32.878407Z","shell.execute_reply":"2025-11-02T22:58:33.437161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\nskf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:33.438589Z","iopub.execute_input":"2025-11-02T22:58:33.438777Z","iopub.status.idle":"2025-11-02T22:58:33.442582Z","shell.execute_reply.started":"2025-11-02T22:58:33.438763Z","shell.execute_reply":"2025-11-02T22:58:33.441767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:33.443455Z","iopub.execute_input":"2025-11-02T22:58:33.443693Z","iopub.status.idle":"2025-11-02T22:58:33.455096Z","shell.execute_reply.started":"2025-11-02T22:58:33.443663Z","shell.execute_reply":"2025-11-02T22:58:33.454389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'n_estimators': 10_000,\n    'learning_rate': 0.01,\n    'early_stopping_rounds': 1000,\n    'random_state': SEED,\n    'n_jobs': -1,\n    'device': 'cuda', # 'cpu'\n    'enable_categorical': True\n    # 'scale_pos_weight': 1\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:33.455771Z","iopub.execute_input":"2025-11-02T22:58:33.456242Z","iopub.status.idle":"2025-11-02T22:58:33.46838Z","shell.execute_reply.started":"2025-11-02T22:58:33.456219Z","shell.execute_reply":"2025-11-02T22:58:33.467752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# taken from: https://www.kaggle.com/code/masayakawamata/s5e11-te-xgb-interaction-features\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Target Encoder with support for multiple aggregation functions,\n    cross-validation leakage prevention, and smoothing.\n    \"\"\"\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n        self.cols_to_encode = cols_to_encode\n        self.aggs = aggs\n        self.cv = cv\n        self.smooth = smooth\n        self.drop_original = drop_original\n        self.mappings_ = {}\n        self.global_stats_ = {}\n\n    def fit(self, X, y):\n        temp_df = X.copy()\n        temp_df['target'] = y\n\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n\n\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n        \n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n\n        for col in self.cols_to_encode:\n            if pd.api.types.is_categorical_dtype(X_transformed[col]):\n                X_transformed[col] = X_transformed[col].astype(str)\n\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n                new_col_name = f'TE_{col}_{agg_func}'\n                map_series = self.mappings_[col][agg_func]\n                X_transformed[new_col_name] = X_transformed[col].astype(str).map(map_series)\n                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n        \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        encoded_features = pd.DataFrame(index=X.index)\n        \n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n\n        for train_idx, val_idx in kf.split(X, y):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx].copy()\n\n            for col in self.cols_to_encode:\n                if pd.api.types.is_categorical_dtype(X_train[col]):\n                    X_train[col] = X_train[col].astype(str)\n                    X_val[col] = X_val[col].astype(str)\n\n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n\n            for col in self.cols_to_encode:\n                for agg_func in self.aggs:\n                    new_col_name = f'TE_{col}_{agg_func}'\n                    fold_global_stat = y_train.agg(agg_func)\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n\n                    if agg_func == 'mean':\n                        counts = temp_df_train.groupby(col)['target'].count()\n\n                        m = self.smooth\n                        if self.smooth == 'auto':\n                            variance_between = mapping.var()\n                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n                            m = avg_variance_within / variance_between if variance_between > 0 else 0\n\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n                        encoded_values = X_val[col].map(smoothed_mapping)\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n\n                    encoded_values_filled = encoded_values.fillna(fold_global_stat).astype(float)\n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values_filled\n\n        X_transformed = X.copy()\n        for col in encoded_features.columns:\n            X_transformed[col] = encoded_features[col]\n            \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:33.4691Z","iopub.execute_input":"2025-11-02T22:58:33.469372Z","iopub.status.idle":"2025-11-02T22:58:33.482841Z","shell.execute_reply.started":"2025-11-02T22:58:33.469358Z","shell.execute_reply":"2025-11-02T22:58:33.48232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test))\nfeature_importances = pd.DataFrame(index=ALL_FEATURES)\n\nCOLS_TO_ENCODE = [\n    'grade_subgrade', \n    'gender', \n    'marital_status', \n    'education_level', \n    'loan_purpose'\n]\n\nCATS_FOR_XGB = [col for col in CATS if col not in COLS_TO_ENCODE]\n\ngc.collect()\n\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f'--- Fold {fold}/{N_SPLITS} ---')\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n\n    X_test_fold = X_test.copy()\n\n\n    TE = TargetEncoder(cols_to_encode=COLS_TO_ENCODE, cv=5  , smooth='auto', aggs=['mean'], drop_original=True)\n    \n\n    X_train = TE.fit_transform(X_train, y_train)\n    \n\n    X_val = TE.transform(X_val)\n    X_test_fold = TE.transform(X_test_fold)\n\n\n    for col in CATS_FOR_XGB:\n        X_train[col] = X_train[col].astype('category')\n        X_val[col] = X_val[col].astype('category')\n        X_test_fold[col] = X_test_fold[col].astype('category')\n\n\n\n    model = XGBClassifier(**params)\n    \n    model.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              verbose=1000)\n\n\n    val_preds = model.predict_proba(X_val)[:, 1]\n    oof_preds[val_idx] = val_preds\n    \n    fold_score = roc_auc_score(y_val, val_preds)\n    print(f'Fold {fold} AUC: {fold_score:.4f}')\n    \n\n    test_preds += model.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n\n\noverall_auc = roc_auc_score(y, oof_preds)\nprint(f'Overall OOF AUC: {overall_auc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T22:58:33.484966Z","iopub.execute_input":"2025-11-02T22:58:33.485164Z","iopub.status.idle":"2025-11-02T23:07:50.19102Z","shell.execute_reply.started":"2025-11-02T22:58:33.48515Z","shell.execute_reply":"2025-11-02T23:07:50.190222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importances = model.feature_importances_\n\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns, \n    'importance': feature_importances\n})\n\nimportance_df = importance_df.sort_values('importance', ascending=False)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12, 20))\nsns.barplot(x='importance', \n            y='feature', \n            data=importance_df.head(20)) \nplt.title(f'Feature Importance ({N_SPLITS} FOLDS model)')\nplt.xlabel('Importance Score')\nplt.ylabel('Features')\nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:07:50.191841Z","iopub.execute_input":"2025-11-02T23:07:50.192109Z","iopub.status.idle":"2025-11-02T23:07:50.670134Z","shell.execute_reply.started":"2025-11-02T23:07:50.192083Z","shell.execute_reply":"2025-11-02T23:07:50.669339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # refit \n# final_params = params.copy()\n# final_params['n_estimators'] = model.best_iteration \n# del final_params['early_stopping_rounds']\n\n# final_model = XGBClassifier(**final_params)\n# final_model.fit(X, y, verbose=False)\n\n# plt.rcParams[\"figure.dpi\"] = 160      \n# fig, ax = plt.subplots(figsize=(15, 20))\n\n# xgb.plot_importance(\n#     final_model,\n#     max_num_features=20,\n#     importance_type=\"gain\",\n#     ax=ax,\n#     show_values=False,                \n#     grid=False\n# )\n\n# ax.set_title(\"XGB Feature Importances\", fontsize=18)\n# ax.tick_params(axis=\"both\", labelsize=12)\n# fig.tight_layout()\n\n# plt.show()\n\n# # del final_model\n# # gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:07:50.671283Z","iopub.execute_input":"2025-11-02T23:07:50.671613Z","iopub.status.idle":"2025-11-02T23:07:50.675428Z","shell.execute_reply.started":"2025-11-02T23:07:50.67159Z","shell.execute_reply":"2025-11-02T23:07:50.674675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame({'id': train_df.id, TARGET: oof_preds}).to_csv(f'oof_xgb+te_{N_SPLITS}FOLDS_cv_{overall_auc:.5}.csv', index=False)\npd.DataFrame({'id': test_df.id, TARGET: test_preds}).to_csv(f'oof_xgb+te_{N_SPLITS}FOLDS_cv_{overall_auc:.5}_submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:07:50.676216Z","iopub.execute_input":"2025-11-02T23:07:50.67655Z","iopub.status.idle":"2025-11-02T23:07:52.181935Z","shell.execute_reply.started":"2025-11-02T23:07:50.676525Z","shell.execute_reply":"2025-11-02T23:07:52.181318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}