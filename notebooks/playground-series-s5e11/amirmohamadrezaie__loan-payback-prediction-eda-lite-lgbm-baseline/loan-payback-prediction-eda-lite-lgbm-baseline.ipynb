{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nfrom collections import defaultdict\nfrom scipy.stats import ks_2samp\nfrom scipy.spatial.distance import jensenshannon\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport lightgbm as lgb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:40:55.709843Z","iopub.execute_input":"2025-11-01T08:40:55.710152Z","iopub.status.idle":"2025-11-01T08:40:55.715662Z","shell.execute_reply.started":"2025-11-01T08:40:55.710134Z","shell.execute_reply":"2025-11-01T08:40:55.714853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:40:55.716539Z","iopub.execute_input":"2025-11-01T08:40:55.71681Z","iopub.status.idle":"2025-11-01T08:40:56.857646Z","shell.execute_reply.started":"2025-11-01T08:40:55.716792Z","shell.execute_reply":"2025-11-01T08:40:56.856711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:40:56.858604Z","iopub.execute_input":"2025-11-01T08:40:56.858841Z","iopub.status.idle":"2025-11-01T08:40:57.053208Z","shell.execute_reply.started":"2025-11-01T08:40:56.858822Z","shell.execute_reply":"2025-11-01T08:40:57.052292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDR: shapes, target balance, types, missingness, cardinality ","metadata":{}},{"cell_type":"code","source":"\nif \"df_train\" not in globals():\n    df_train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\n    df_test  = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\n    df_sub   = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n\nTARGET = \"loan_paid_back\"\nID_COL = \"id\"\n\nfeat_cols = [c for c in df_train.columns if c not in [TARGET, ID_COL]]\nnum_cols = df_train[feat_cols].select_dtypes(include=[\"number\"]).columns.tolist()\ncat_cols = df_train[feat_cols].select_dtypes(include=[\"object\"]).columns.tolist()\n\nprint(f\"Train shape: {df_train.shape} | Test shape: {df_test.shape}\")\nprint(f\"Numeric cols ({len(num_cols)}): {num_cols}\")\nprint(f\"Categorical cols ({len(cat_cols)}): {cat_cols}\")\n\nvc = df_train[TARGET].value_counts().sort_index()\nprint(\"\\nTarget counts (0/1):\")\nprint(vc.to_frame(\"count\"))\nprint(\"Positive rate:\", df_train[TARGET].mean())\n\nprint(\"\\nDuplicate IDs -> train:\", df_train[ID_COL].duplicated().sum(), \n      \"| test:\", df_test[ID_COL].duplicated().sum())\n\nmiss_tr = df_train.isna().sum()\nmiss_te = df_test.isna().sum()\nprint(\"\\nMissing (train):\")\nprint(miss_tr[miss_tr>0].sort_values(ascending=False))\nprint(\"\\nMissing (test):\")\nprint(miss_te[miss_te>0].sort_values(ascending=False))\n\ncard = df_train[cat_cols].nunique().sort_values(ascending=False)\nprint(\"\\nCategorical cardinality (top-10):\")\nprint(card.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:40:57.054155Z","iopub.execute_input":"2025-11-01T08:40:57.054405Z","iopub.status.idle":"2025-11-01T08:40:57.651926Z","shell.execute_reply.started":"2025-11-01T08:40:57.054366Z","shell.execute_reply":"2025-11-01T08:40:57.651178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# diagnostics and category alignment","metadata":{}},{"cell_type":"code","source":"\nfor c in df_train.select_dtypes(include=[\"object\"]).columns:\n    cats = pd.Index(df_train[c].astype(\"string\").unique()).union(df_test[c].astype(\"string\").unique())\n    df_train[c] = pd.Categorical(df_train[c].astype(\"string\"), categories=cats)\n    df_test[c]  = pd.Categorical(df_test[c].astype(\"string\"),  categories=cats)\n\n\ndrift_num, drift_cat = [], []\n\ntry:\n    have_scipy = True\nexcept Exception:\n    have_scipy = False\n\nif have_scipy:\n    for c in num_cols:\n        ks = ks_2samp(df_train[c].values, df_test[c].values).statistic\n        drift_num.append((c, ks))\n    for c in cat_cols:\n        p = df_train[c].value_counts(normalize=True)\n        q = df_test[c].value_counts(normalize=True)\n        idx = p.index.union(q.index)\n        p = p.reindex(idx, fill_value=0); q = q.reindex(idx, fill_value=0)\n        js = float(jensenshannon(p.values, q.values))\n        drift_cat.append((c, js))\nelse:\n    for c in num_cols:\n        proxy = abs(df_train[c].median() - df_test[c].median()) / (df_train[c].mad() + 1e-9)\n        drift_num.append((c, proxy))\n    for c in cat_cols:\n        tr_top = df_train[c].value_counts(normalize=True).iloc[:1].sum()\n        te_top = df_test[c].value_counts(normalize=True).iloc[:1].sum()\n        drift_cat.append((c, abs(tr_top - te_top)))\n\n\nnum_top = sorted(drift_num, key=lambda x: x[1], reverse=True)[:5]\ncat_top = sorted(drift_cat, key=lambda x: x[1], reverse=True)[:5]\n\nprint(\"Top-5 numeric drift (KS or proxy):\")\nfor c,v in num_top: print(f\"{c:22s} -> {v:.4f}\")\nprint(\"\\nTop-5 categorical drift (JS or proxy):\")\nfor c,v in cat_top: print(f\"{c:22s} -> {v:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:40:57.654126Z","iopub.execute_input":"2025-11-01T08:40:57.654349Z","iopub.status.idle":"2025-11-01T08:40:58.994827Z","shell.execute_reply.started":"2025-11-01T08:40:57.654332Z","shell.execute_reply":"2025-11-01T08:40:58.994093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  signal checks to guide modeling","metadata":{}},{"cell_type":"code","source":"\ny = df_train[TARGET].astype(int).values\n\nX_num = df_train[num_cols].copy()\nX_cat = pd.DataFrame(index=df_train.index)\nfor c in cat_cols:\n    X_cat[c] = df_train[c].astype(\"category\").cat.codes  # -1 for NaN/unseen\nX_mi = pd.concat([X_num, X_cat], axis=1)\ndiscrete_mask = np.array([False]*len(num_cols) + [True]*len(cat_cols))\nmi = mutual_info_classif(X_mi, y, discrete_features=discrete_mask, random_state=2025)\nmi_s = pd.Series(mi, index=X_mi.columns).sort_values(ascending=False)\n\ncorr_pears = df_train[num_cols + [TARGET]].corr(method=\"pearson\")[TARGET].drop(TARGET)\ncorr_abs = corr_pears.abs().sort_values(ascending=False)\n\nspread_rows = []\nfor c in cat_cols:\n    tab = df_train.groupby(c)[TARGET].agg([\"count\",\"mean\"])\n    tab = tab[tab[\"count\"] >= 100]  # avoid tiny levels\n    if len(tab) >= 2:\n        spread = tab[\"mean\"].max() - tab[\"mean\"].min()\n        spread_rows.append((c, spread, len(tab)))\nspread_df = pd.DataFrame(spread_rows, columns=[\"feature\",\"target_rate_spread\",\"n_levels\"]).sort_values(\"target_rate_spread\", ascending=False)\n\n\nprint(\"Top-10 features by Mutual Information:\")\ndisplay(mi_s.head(10).to_frame(\"mutual_info\"))\n\nprint(\"Top-10 numeric by |Pearson corr| with target:\")\ndisplay(corr_abs.head(10).to_frame(\"|pearson|\"))\n\nprint(\"Top-10 categorical by target-rate spread (support >=100):\")\ndisplay(spread_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:40:58.995724Z","iopub.execute_input":"2025-11-01T08:40:58.99602Z","iopub.status.idle":"2025-11-01T08:41:23.006366Z","shell.execute_reply.started":"2025-11-01T08:40:58.99598Z","shell.execute_reply":"2025-11-01T08:41:23.005625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Correlation heatmap between numeric features and the target","metadata":{}},{"cell_type":"code","source":"\nTARGET = \"loan_paid_back\"\nID_COL = \"id\"\n\nnum_cols = df_train.drop(columns=[TARGET, ID_COL]).select_dtypes(include=[\"number\"]).columns.tolist()\n\ncorr = df_train[num_cols + [TARGET]].corr(method=\"pearson\")\n\nplt.figure(figsize=(6,5))\nsns.heatmap(corr, annot=True, fmt=\".2f\", square=True, cbar=True)\nplt.title(\"Correlation Matrix (Numeric Features + Target)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:41:23.007174Z","iopub.execute_input":"2025-11-01T08:41:23.007453Z","iopub.status.idle":"2025-11-01T08:41:23.423608Z","shell.execute_reply.started":"2025-11-01T08:41:23.007431Z","shell.execute_reply":"2025-11-01T08:41:23.422703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Skewness plot for ALL numeric features","metadata":{}},{"cell_type":"code","source":"\nTARGET = \"loan_paid_back\"\nID_COL = \"id\"\n\n\nnum_cols = df_train.drop(columns=[TARGET, ID_COL]).select_dtypes(include=[\"number\"]).columns.tolist()\nprint(\"Numeric columns:\", num_cols)\n\nlog_scale_features = set()\n\nraw_skew = df_train[num_cols].skew()\n\nn = len(num_cols)\ncols = 3\nrows = math.ceil(n / cols)\nfig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*3.6))\naxes = axes.flatten()\n\nfor i, c in enumerate(num_cols):\n    ax = axes[i]\n    x = df_train[c].astype(float).values\n\n    use_log = c in log_scale_features\n    x_plot = np.log1p(x) if use_log else x\n\n    ax.hist(x_plot, bins=50, density=True)\n\n    mean_val = np.mean(x_plot)\n    med_val  = np.median(x_plot)\n    ax.axvline(mean_val, linestyle=\"--\", linewidth=1.1, label=\"mean\")\n    ax.axvline(med_val,  linestyle=\"-.\", linewidth=1.1, label=\"median\")\n\n    note = \" (log1p)\" if use_log else \"\"\n    ax.set_title(f\"{c}{note}\\nskew={raw_skew[c]:.2f}\")\n    ax.set_xlabel(c + note)\n    ax.set_ylabel(\"density\")\n    ax.legend()\n\nfor j in range(i+1, len(axes)):\n    axes[j].axis(\"off\")\n\nplt.suptitle(\"Skewness — distributions of ALL numeric features\", y=1.02)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:41:23.424531Z","iopub.execute_input":"2025-11-01T08:41:23.424795Z","iopub.status.idle":"2025-11-01T08:41:25.147663Z","shell.execute_reply.started":"2025-11-01T08:41:23.42477Z","shell.execute_reply":"2025-11-01T08:41:25.146772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class imbalance check","metadata":{}},{"cell_type":"code","source":"\nTARGET = \"loan_paid_back\"\nvc = df_train[TARGET].value_counts().sort_index()        \nprops = (vc / vc.sum()).sort_index()                      \n\nfig, ax = plt.subplots(figsize=(4,3))\nprops.plot(kind=\"bar\", ax=ax)\n\nfor i, v in enumerate(props.values):\n    ax.text(i, v + 0.01, f\"{v*100:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_ylim(0, 1.1 * props.max())\nax.set_xticklabels(props.index.astype(str))\nax.set_ylabel(\"Proportion\")\nax.set_title(\"Target Distribution (Class Imbalance)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:41:25.148669Z","iopub.execute_input":"2025-11-01T08:41:25.149085Z","iopub.status.idle":"2025-11-01T08:41:25.308417Z","shell.execute_reply.started":"2025-11-01T08:41:25.149059Z","shell.execute_reply":"2025-11-01T08:41:25.307547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Box-plots for ALL numeric features + outlier (IQR)","metadata":{}},{"cell_type":"code","source":"\nTARGET = \"loan_paid_back\"\nID_COL = \"id\"\n\nnum_cols = df_train.drop(columns=[TARGET, ID_COL]).select_dtypes(include=[\"number\"]).columns.tolist()\nprint(\"Numeric columns:\", num_cols)\n\nlog_scale_features = {\"annual_income\", \"loan_amount\"}  \n\niqr_rows = []\nfor c in num_cols:\n    x = df_train[c].astype(float)\n    q1, q3 = x.quantile([0.25, 0.75])\n    iqr = q3 - q1\n    lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n    out_frac = ((x < lo) | (x > hi)).mean()\n    iqr_rows.append((c, float(q1), float(q3), float(lo), float(hi), float(out_frac)))\niqr_df = pd.DataFrame(iqr_rows, columns=[\"feature\",\"Q1\",\"Q3\",\"lower_fence\",\"upper_fence\",\"outlier_fraction\"]).sort_values(\"outlier_fraction\", ascending=False)\nprint(\"\\nApprox. outlier fractions by feature (IQR rule):\")\ndisplay(iqr_df)\n\nn = len(num_cols)\ncols = 3\nrows = math.ceil(n / cols)\nfig, axes = plt.subplots(rows, cols, figsize=(cols*4.2, rows*4.0))\naxes = axes.flatten()\n\nfor i, c in enumerate(num_cols):\n    ax = axes[i]\n    x = df_train[c].astype(float).values\n    use_log = c in log_scale_features\n    x_plot = np.log1p(x) if use_log else x\n\n    sns.boxplot(y=x_plot, ax=ax, fliersize=2, width=0.4)\n    ax.set_title(f\"{c}{' (log1p)' if use_log else ''}\")\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"value\" + (\" (log1p)\" if use_log else \"\"))\n\n    out_pct = iqr_df.loc[iqr_df[\"feature\"]==c, \"outlier_fraction\"].values[0] * 100\n    ax.text(0.05, 0.93, f\"outliers ≈ {out_pct:.1f}%\", transform=ax.transAxes, ha=\"left\", va=\"top\")\n\nfor j in range(i+1, len(axes)):\n    axes[j].axis(\"off\")\n\nplt.suptitle(\"Box-plots for numeric features (IQR-based outlier visualization)\", y=1.02)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:41:25.309433Z","iopub.execute_input":"2025-11-01T08:41:25.309833Z","iopub.status.idle":"2025-11-01T08:41:26.323374Z","shell.execute_reply.started":"2025-11-01T08:41:25.309804Z","shell.execute_reply":"2025-11-01T08:41:26.322535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Minimal preprocessing","metadata":{}},{"cell_type":"code","source":"\nTARGET = \"loan_paid_back\"\nID_COL = \"id\"\n\nfeat_cols = [c for c in df_train.columns if c not in [TARGET, ID_COL]]\nnum_cols  = df_train[feat_cols].select_dtypes(include=[\"number\"]).columns.tolist()\ncat_cols  = df_train[feat_cols].select_dtypes(include=[\"object\"]).columns.tolist()\n\ndef compute_iqr_fences(df, cols):\n    fences = {}\n    for c in cols:\n        x = df[c].astype(float)\n        q1, q3 = x.quantile([0.25, 0.75])\n        iqr = q3 - q1\n        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n        if c == \"debt_to_income_ratio\":\n            lo = max(lo, 0.0)\n        fences[c] = (float(lo), float(hi))\n    return fences\n\niqr_fences = compute_iqr_fences(df_train, num_cols)\n\ndef apply_clip(df, fences):\n    df2 = df.copy()\n    for c,(lo,hi) in fences.items():\n        df2[c] = df2[c].astype(float).clip(lower=lo, upper=hi)\n    return df2\n\ntr = apply_clip(df_train, iqr_fences)\nte = apply_clip(df_test,  iqr_fences)\n\ndef add_features(df):\n    out = df.copy()\n    if \"grade_subgrade\" in out.columns:\n        s = out[\"grade_subgrade\"].astype(str)\n        out[\"grade_letter\"] = s.str[0]\n        out[\"subgrade_num\"] = s.str[1:]\n    if \"annual_income\" in out.columns:\n        out[\"log_annual_income\"] = np.log1p(out[\"annual_income\"])\n    if \"loan_amount\" in out.columns:\n        out[\"log_loan_amount\"] = np.log1p(out[\"loan_amount\"])\n    if set([\"annual_income\",\"loan_amount\"]).issubset(out.columns):\n        out[\"income_to_loan\"] = out[\"annual_income\"] / (1.0 + out[\"loan_amount\"])\n    if set([\"interest_rate\",\"loan_amount\",\"annual_income\"]).issubset(out.columns):\n        out[\"interest_burden\"] = (out[\"interest_rate\"] * out[\"loan_amount\"]) / (1.0 + out[\"annual_income\"])\n    return out\n\ntr = add_features(tr)\nte = add_features(te)\n\nall_feats = [c for c in tr.columns if c not in [TARGET, ID_COL]]\nnum_final = tr[all_feats].select_dtypes(include=[\"number\"]).columns.tolist()\ncat_final = tr[all_feats].select_dtypes(include=[\"object\"]).columns.tolist()\n\nfor c in cat_final:\n    cats = pd.Index(tr[c].astype(\"string\").unique()).union(te[c].astype(\"string\").unique())\n    tr[c] = pd.Categorical(tr[c].astype(\"string\"), categories=cats)\n    te[c] = pd.Categorical(te[c].astype(\"string\"), categories=cats)\n\nprint(f\"Final numeric features: {len(num_final)}\")\nprint(f\"Final categorical features: {len(cat_final)}\")\n\nX_train = tr[all_feats].copy()\ny_train = tr[TARGET].astype(int).values\nX_test  = te[all_feats].copy()\ncategorical_feature = cat_final\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:41:26.324409Z","iopub.execute_input":"2025-11-01T08:41:26.324645Z","iopub.status.idle":"2025-11-01T08:41:27.252985Z","shell.execute_reply.started":"2025-11-01T08:41:26.324625Z","shell.execute_reply":"2025-11-01T08:41:27.252278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nSEED  = 2025\nFOLDS = 5\n\noof = np.zeros(len(X_train))\npred_test = np.zeros(len(X_test))\nfold_aucs = []\n\nkf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_train, y_train), 1):\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n\n    model = lgb.LGBMClassifier(\n        n_estimators=4000,\n        learning_rate=0.03,\n        num_leaves=64,\n        subsample=0.9,\n        colsample_bytree=0.8,\n        min_child_samples=50,\n        reg_lambda=5.0,\n        reg_alpha=0.0,\n        objective=\"binary\",\n        random_state=SEED + fold,\n        n_jobs=-1,\n\n    )\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"auc\",\n        categorical_feature=categorical_feature,\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)]\n    )\n\n    val_pred = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:,1]\n    oof[va_idx] = val_pred\n    fold_auc = roc_auc_score(y_va, val_pred)\n    fold_aucs.append(fold_auc)\n\n    pred_test += model.predict_proba(X_test, num_iteration=model.best_iteration_)[:,1] / FOLDS\n\n    print(f\"Fold {fold}: AUC = {fold_auc:.5f} | best_iter = {model.best_iteration_}\")\n\nprint(\"\\nCV AUCs:\", [round(a,5) for a in fold_aucs], \n      \"| Mean:\", round(np.mean(fold_aucs),5), \n      \"±\", round(np.std(fold_aucs),5))\nprint(\"OOF AUC:\", round(roc_auc_score(y_train, oof),5))\n\nsub = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"loan_paid_back\": np.clip(pred_test, 0, 1)\n})\nsub.to_csv(\"submission_lgb_preproc.csv\", index=False)\nprint(\"Saved -> submission_lgb_preproc.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:41:27.253845Z","iopub.execute_input":"2025-11-01T08:41:27.254132Z","iopub.status.idle":"2025-11-01T08:47:15.379007Z","shell.execute_reply.started":"2025-11-01T08:41:27.254102Z","shell.execute_reply":"2025-11-01T08:47:15.377967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\npred_series = pd.Series(np.clip(pred_test, 0, 1), index=df_test[\"id\"])  # id -> prob\n\nsub = df_sub.copy()\nassert \"id\" in sub.columns and \"loan_paid_back\" in sub.columns, \"Unexpected df_sub format!\"\nassert len(sub) == len(df_test), \"df_sub and df_test should have same length!\"\n\nsub[\"loan_paid_back\"] = pred_series.loc[sub[\"id\"]].values\n\nassert sub[\"loan_paid_back\"].between(0,1).all(), \"Predictions must be in [0,1]!\"\nassert not sub[\"loan_paid_back\"].isna().any(), \"Found NaNs in predictions after alignment!\"\n\nsub.to_csv(\"submission.csv\", index=False)\nprint(\"Saved: submission.csv\")\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:47:15.380014Z","iopub.execute_input":"2025-11-01T08:47:15.380689Z","iopub.status.idle":"2025-11-01T08:47:15.974172Z","shell.execute_reply.started":"2025-11-01T08:47:15.380659Z","shell.execute_reply":"2025-11-01T08:47:15.973284Z"}},"outputs":[],"execution_count":null}]}