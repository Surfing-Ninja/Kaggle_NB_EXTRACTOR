{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================\n# CELL 1: Title and Introduction\n# ==========================================\n\"\"\"\n# üè¶ Predicting Loan Payback - Playground Series S5E11\n## A Comprehensive Analysis and Modeling Approach\n\n**Competition Goal:** Predict the probability that a borrower will pay back their loan\n\n**Evaluation Metric:** ROC AUC Score\n\n---\n\n### üìã Notebook Structure:\n1. **Data Loading & Overview** üìä\n2. **Exploratory Data Analysis** üîç\n3. **Feature Engineering** ‚öôÔ∏è\n4. **Model Training & Validation** ü§ñ\n5. **Ensemble & Predictions** üéØ\n6. **Submission** üì§\n\n---\n**Author:** Your Name | **Date:** November 2025\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:42.603146Z","iopub.execute_input":"2025-11-02T14:16:42.603388Z","iopub.status.idle":"2025-11-02T14:16:42.613144Z","shell.execute_reply.started":"2025-11-02T14:16:42.603369Z","shell.execute_reply":"2025-11-02T14:16:42.612476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 2: Import Libraries\n# ==========================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"‚úÖ All libraries imported successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:42.614218Z","iopub.execute_input":"2025-11-02T14:16:42.614712Z","iopub.status.idle":"2025-11-02T14:16:48.519932Z","shell.execute_reply.started":"2025-11-02T14:16:42.614684Z","shell.execute_reply":"2025-11-02T14:16:48.519312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 3: Load Data\n# ==========================================\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n\nprint(f\"üìä Train shape: {train.shape}\")\nprint(f\"üìä Test shape: {test.shape}\")\nprint(f\"üìä Submission shape: {submission.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:48.520539Z","iopub.execute_input":"2025-11-02T14:16:48.520971Z","iopub.status.idle":"2025-11-02T14:16:50.14384Z","shell.execute_reply.started":"2025-11-02T14:16:48.520952Z","shell.execute_reply":"2025-11-02T14:16:50.143142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 4: Data Overview\n# ==========================================\nprint(\"=\" * 80)\nprint(\"TRAINING DATA OVERVIEW\")\nprint(\"=\" * 80)\nprint(train.head(10))\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DATA TYPES & NULL VALUES\")\nprint(\"=\" * 80)\nprint(train.info())\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STATISTICAL SUMMARY\")\nprint(\"=\" * 80)\nprint(train.describe())\n\n# Check for missing values\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MISSING VALUES\")\nprint(\"=\" * 80)\nmissing = train.isnull().sum()\nif missing.sum() > 0:\n    print(missing[missing > 0])\nelse:\n    print(\"‚úÖ No missing values found!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:50.145233Z","iopub.execute_input":"2025-11-02T14:16:50.145487Z","iopub.status.idle":"2025-11-02T14:16:50.750795Z","shell.execute_reply.started":"2025-11-02T14:16:50.145471Z","shell.execute_reply":"2025-11-02T14:16:50.750052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 5: Target Variable Analysis\n# ==========================================\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Target distribution\ntarget_counts = train['loan_paid_back'].value_counts()\naxes[0].bar(target_counts.index, target_counts.values, color=['#FF6B6B', '#4ECDC4'])\naxes[0].set_xlabel('Loan Paid Back', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\naxes[0].set_title('üéØ Target Variable Distribution', fontsize=14, fontweight='bold')\naxes[0].set_xticks([0, 1])\naxes[0].set_xticklabels(['Not Paid (0)', 'Paid (1)'])\n\nfor i, v in enumerate(target_counts.values):\n    axes[0].text(i, v + 100, str(v), ha='center', fontweight='bold')\n\n# Target percentage\ntarget_pct = train['loan_paid_back'].value_counts(normalize=True) * 100\ncolors = ['#FF6B6B', '#4ECDC4']\nexplode = (0.05, 0.05)\naxes[1].pie(target_pct.values, labels=['Not Paid (0)', 'Paid (1)'], autopct='%1.2f%%',\n            colors=colors, explode=explode, startangle=90, textprops={'fontweight': 'bold'})\naxes[1].set_title('ü•ß Target Variable Percentage', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìä Class Distribution:\")\nprint(f\"   ‚Ä¢ Not Paid (0): {target_counts[0]:,} ({target_pct[0]:.2f}%)\")\nprint(f\"   ‚Ä¢ Paid (1): {target_counts[1]:,} ({target_pct[1]:.2f}%)\")\nprint(f\"\\n‚öñÔ∏è Class Balance Ratio: {target_counts[1]/target_counts[0]:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:50.751492Z","iopub.execute_input":"2025-11-02T14:16:50.751743Z","iopub.status.idle":"2025-11-02T14:16:51.128678Z","shell.execute_reply.started":"2025-11-02T14:16:50.751722Z","shell.execute_reply":"2025-11-02T14:16:51.127944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 6: Numerical Features Analysis\n# ==========================================\n# Identify numerical and categorical features\nnumerical_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\nnumerical_features.remove('id')\nnumerical_features.remove('loan_paid_back')\n\ncategorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"üî¢ Numerical Features ({len(numerical_features)}): {numerical_features}\")\nprint(f\"üè∑Ô∏è Categorical Features ({len(categorical_features)}): {categorical_features}\")\n\n# Correlation heatmap\nprint(\"\\nüìä Computing correlation matrix...\")\ncorrelation_matrix = train[numerical_features + ['loan_paid_back']].corr()\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('üî• Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\n# Top correlations with target\ntarget_corr = correlation_matrix['loan_paid_back'].drop('loan_paid_back').sort_values(ascending=False)\nprint(\"\\nüéØ Top 10 Features Correlated with Target:\")\nprint(target_corr.head(10))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:51.129487Z","iopub.execute_input":"2025-11-02T14:16:51.129768Z","iopub.status.idle":"2025-11-02T14:16:51.685246Z","shell.execute_reply.started":"2025-11-02T14:16:51.12974Z","shell.execute_reply":"2025-11-02T14:16:51.684549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 7: Distribution of Top Numerical Features\n# ==========================================\n# Select top features for visualization\ntop_features = target_corr.abs().sort_values(ascending=False).head(6).index.tolist()\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.ravel()\n\nfor idx, feature in enumerate(top_features):\n    for target_val in [0, 1]:\n        data = train[train['loan_paid_back'] == target_val][feature]\n        axes[idx].hist(data, bins=50, alpha=0.6, \n                      label=f'Paid={target_val}', \n                      color=['#FF6B6B', '#4ECDC4'][target_val])\n    \n    axes[idx].set_xlabel(feature, fontweight='bold')\n    axes[idx].set_ylabel('Frequency', fontweight='bold')\n    axes[idx].set_title(f'Distribution: {feature}', fontweight='bold')\n    axes[idx].legend()\n    axes[idx].grid(alpha=0.3)\n\nplt.suptitle('üìä Top 6 Features Distribution by Target', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:51.686087Z","iopub.execute_input":"2025-11-02T14:16:51.686337Z","iopub.status.idle":"2025-11-02T14:16:53.473038Z","shell.execute_reply.started":"2025-11-02T14:16:51.686312Z","shell.execute_reply":"2025-11-02T14:16:53.472264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 8: Categorical Features Analysis\n# ==========================================\nif len(categorical_features) > 0:\n    n_cat = len(categorical_features)\n    n_cols = 3\n    n_rows = (n_cat + n_cols - 1) // n_cols\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n    axes = axes.ravel() if n_cat > 1 else [axes]\n    \n    for idx, feature in enumerate(categorical_features):\n        # Cross-tabulation\n        ct = pd.crosstab(train[feature], train['loan_paid_back'], normalize='index') * 100\n        \n        ct.plot(kind='bar', ax=axes[idx], color=['#FF6B6B', '#4ECDC4'], width=0.8)\n        axes[idx].set_title(f'üè∑Ô∏è {feature} vs Target', fontweight='bold', fontsize=12)\n        axes[idx].set_xlabel(feature, fontweight='bold')\n        axes[idx].set_ylabel('Percentage (%)', fontweight='bold')\n        axes[idx].legend(['Not Paid (0)', 'Paid (1)'], loc='best')\n        axes[idx].grid(alpha=0.3, axis='y')\n        axes[idx].tick_params(axis='x', rotation=45)\n    \n    # Hide unused subplots\n    for idx in range(len(categorical_features), len(axes)):\n        axes[idx].axis('off')\n    \n    plt.suptitle('üîç Categorical Features Analysis', fontsize=16, fontweight='bold', y=1.01)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"‚ÑπÔ∏è No categorical features found in the dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:53.473867Z","iopub.execute_input":"2025-11-02T14:16:53.474058Z","iopub.status.idle":"2025-11-02T14:16:55.2983Z","shell.execute_reply.started":"2025-11-02T14:16:53.474043Z","shell.execute_reply":"2025-11-02T14:16:55.297523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 9: Feature Engineering\n# ==========================================\nprint(\"‚öôÔ∏è Starting Feature Engineering...\")\n\ndef create_features(df):\n    \"\"\"Create new features for the dataset\"\"\"\n    df = df.copy()\n    \n    # Example feature engineering (adjust based on actual columns)\n    # You'll need to customize this based on the actual features in your dataset\n    \n    # 1. Interaction features\n    if 'person_income' in df.columns and 'loan_amnt' in df.columns:\n        df['income_to_loan_ratio'] = df['person_income'] / (df['loan_amnt'] + 1)\n        df['loan_to_income_pct'] = (df['loan_amnt'] / (df['person_income'] + 1)) * 100\n    \n    # 2. Polynomial features for important columns\n    if 'person_age' in df.columns:\n        df['age_squared'] = df['person_age'] ** 2\n        df['age_log'] = np.log1p(df['person_age'])\n    \n    # 3. Binning continuous features\n    if 'person_income' in df.columns:\n        df['income_bin'] = pd.qcut(df['person_income'], q=5, labels=False, duplicates='drop')\n    \n    # 4. Statistical features (if multiple related columns exist)\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    if 'id' in numeric_cols:\n        numeric_cols.remove('id')\n    if 'loan_paid_back' in numeric_cols:\n        numeric_cols.remove('loan_paid_back')\n    \n    if len(numeric_cols) >= 3:\n        df['feature_sum'] = df[numeric_cols].sum(axis=1)\n        df['feature_mean'] = df[numeric_cols].mean(axis=1)\n        df['feature_std'] = df[numeric_cols].std(axis=1)\n        df['feature_max'] = df[numeric_cols].max(axis=1)\n        df['feature_min'] = df[numeric_cols].min(axis=1)\n    \n    return df\n\n# Apply feature engineering\ntrain_fe = create_features(train)\ntest_fe = create_features(test)\n\nprint(f\"‚úÖ Feature Engineering Complete!\")\nprint(f\"   ‚Ä¢ Original features: {train.shape[1]}\")\nprint(f\"   ‚Ä¢ New features: {train_fe.shape[1]}\")\nprint(f\"   ‚Ä¢ Features added: {train_fe.shape[1] - train.shape[1]}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:55.299044Z","iopub.execute_input":"2025-11-02T14:16:55.299221Z","iopub.status.idle":"2025-11-02T14:16:55.979002Z","shell.execute_reply.started":"2025-11-02T14:16:55.299207Z","shell.execute_reply":"2025-11-02T14:16:55.978173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 10: Data Preparation\n# ==========================================\nprint(\"üîß Preparing data for modeling...\")\n\n# Separate features and target\nX = train_fe.drop(['id', 'loan_paid_back'], axis=1)\ny = train_fe['loan_paid_back']\nX_test = test_fe.drop(['id'], axis=1)\n\n# Handle categorical features\ncategorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\nif len(categorical_cols) > 0:\n    print(f\"üìù Encoding {len(categorical_cols)} categorical features...\")\n    le_dict = {}\n    \n    for col in categorical_cols:\n        le = LabelEncoder()\n        X[col] = le.fit_transform(X[col].astype(str))\n        X_test[col] = le.transform(X_test[col].astype(str))\n        le_dict[col] = le\n\n# Ensure all columns are aligned\nX_test = X_test[X.columns]\n\nprint(f\"‚úÖ Data prepared successfully!\")\nprint(f\"   ‚Ä¢ Training samples: {X.shape[0]:,}\")\nprint(f\"   ‚Ä¢ Test samples: {X_test.shape[0]:,}\")\nprint(f\"   ‚Ä¢ Total features: {X.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:55.981391Z","iopub.execute_input":"2025-11-02T14:16:55.98163Z","iopub.status.idle":"2025-11-02T14:16:56.897645Z","shell.execute_reply.started":"2025-11-02T14:16:55.981612Z","shell.execute_reply":"2025-11-02T14:16:56.89679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 11: Model Training - LightGBM\n# ==========================================\nimport joblib\nimport pickle\nimport os\n\n# Create directory for saving models\nos.makedirs('models', exist_ok=True)\n\nprint(\"üöÄ Training LightGBM Model...\")\n\nlgbm_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'n_estimators': 1000,\n    'random_state': 42\n}\n\n# Cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nlgbm_scores = []\nlgbm_predictions = np.zeros(len(X_test))\nlgbm_models = []  # Store all fold models\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = LGBMClassifier(**lgbm_params)\n    model.fit(X_train_fold, y_train_fold, \n              eval_set=[(X_val_fold, y_val_fold)],\n              callbacks=[])\n    \n    val_pred = model.predict_proba(X_val_fold)[:, 1]\n    score = roc_auc_score(y_val_fold, val_pred)\n    lgbm_scores.append(score)\n    lgbm_models.append(model)  # Save model\n    \n    lgbm_predictions += model.predict_proba(X_test)[:, 1] / skf.n_splits\n    \n    # Save each fold model\n    joblib.dump(model, f'models/lgbm_fold_{fold}.pkl')\n    \n    print(f\"   Fold {fold} - ROC AUC: {score:.6f}\")\n\nprint(f\"\\nüìä LightGBM CV Score: {np.mean(lgbm_scores):.6f} (+/- {np.std(lgbm_scores):.6f})\")\nprint(f\"üíæ Saved 5 LightGBM models to 'models/' directory\")\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:16:56.898449Z","iopub.execute_input":"2025-11-02T14:16:56.898705Z","iopub.status.idle":"2025-11-02T14:20:53.141098Z","shell.execute_reply.started":"2025-11-02T14:16:56.898687Z","shell.execute_reply":"2025-11-02T14:20:53.140419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 12: Model Training - XGBoost\n# ==========================================\nprint(\"üöÄ Training XGBoost Model...\")\n\nxgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'learning_rate': 0.05,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'n_estimators': 1000,\n    'random_state': 42,\n    'verbosity': 0\n}\n\nxgb_scores = []\nxgb_predictions = np.zeros(len(X_test))\nxgb_models = []  # Store all fold models\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train_fold, y_train_fold,\n              eval_set=[(X_val_fold, y_val_fold)],\n              verbose=False)\n    \n    val_pred = model.predict_proba(X_val_fold)[:, 1]\n    score = roc_auc_score(y_val_fold, val_pred)\n    xgb_scores.append(score)\n    xgb_models.append(model)  # Save model\n    \n    xgb_predictions += model.predict_proba(X_test)[:, 1] / skf.n_splits\n    \n    # Save each fold model\n    joblib.dump(model, f'models/xgb_fold_{fold}.pkl')\n    \n    print(f\"   Fold {fold} - ROC AUC: {score:.6f}\")\n\nprint(f\"\\nüìä XGBoost CV Score: {np.mean(xgb_scores):.6f} (+/- {np.std(xgb_scores):.6f})\")\nprint(f\"üíæ Saved 5 XGBoost models to 'models/' directory\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:20:53.141765Z","iopub.execute_input":"2025-11-02T14:20:53.14197Z","iopub.status.idle":"2025-11-02T14:24:01.850945Z","shell.execute_reply.started":"2025-11-02T14:20:53.141953Z","shell.execute_reply":"2025-11-02T14:24:01.850144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 13: Model Training - CatBoost\n# ==========================================\nprint(\"üöÄ Training CatBoost Model...\")\n\ncb_params = {\n    'iterations': 1000,\n    'learning_rate': 0.05,\n    'depth': 6,\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'random_seed': 42,\n    'verbose': False\n}\n\ncb_scores = []\ncb_predictions = np.zeros(len(X_test))\ncb_models = []  # Store all fold models\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = CatBoostClassifier(**cb_params)\n    model.fit(X_train_fold, y_train_fold,\n              eval_set=(X_val_fold, y_val_fold),\n              verbose=False)\n    \n    val_pred = model.predict_proba(X_val_fold)[:, 1]\n    score = roc_auc_score(y_val_fold, val_pred)\n    cb_scores.append(score)\n    cb_models.append(model)  # Save model\n    \n    cb_predictions += model.predict_proba(X_test)[:, 1] / skf.n_splits\n    \n    # Save each fold model\n    model.save_model(f'models/catboost_fold_{fold}.cbm')\n    \n    print(f\"   Fold {fold} - ROC AUC: {score:.6f}\")\n\nprint(f\"\\nüìä CatBoost CV Score: {np.mean(cb_scores):.6f} (+/- {np.std(cb_scores):.6f})\")\nprint(f\"üíæ Saved 5 CatBoost models to 'models/' directory\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:24:01.85186Z","iopub.execute_input":"2025-11-02T14:24:01.85216Z","iopub.status.idle":"2025-11-02T14:28:42.504057Z","shell.execute_reply.started":"2025-11-02T14:24:01.852137Z","shell.execute_reply":"2025-11-02T14:28:42.503352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 14: Model Performance Comparison\n# ==========================================\nmodel_scores = {\n    'LightGBM': (np.mean(lgbm_scores), np.std(lgbm_scores)),\n    'XGBoost': (np.mean(xgb_scores), np.std(xgb_scores)),\n    'CatBoost': (np.mean(cb_scores), np.std(cb_scores))\n}\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nmodels = list(model_scores.keys())\nmeans = [model_scores[m][0] for m in models]\nstds = [model_scores[m][1] for m in models]\n\nbars = ax.bar(models, means, yerr=stds, capsize=10, \n              color=['#FF6B6B', '#4ECDC4', '#95E1D3'], \n              edgecolor='black', linewidth=2, alpha=0.8)\n\nax.set_ylabel('ROC AUC Score', fontsize=12, fontweight='bold')\nax.set_title('üèÜ Model Performance Comparison (5-Fold CV)', fontsize=14, fontweight='bold')\nax.set_ylim([min(means) - 0.01, max(means) + 0.01])\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{mean:.6f}\\n¬±{std:.6f}',\n            ha='center', va='bottom', fontweight='bold', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìä FINAL MODEL SCORES\")\nprint(\"=\"*60)\nfor model_name, (mean, std) in model_scores.items():\n    print(f\"{model_name:12s}: {mean:.6f} (+/- {std:.6f})\")\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:28:42.504792Z","iopub.execute_input":"2025-11-02T14:28:42.505087Z","iopub.status.idle":"2025-11-02T14:28:42.691249Z","shell.execute_reply.started":"2025-11-02T14:28:42.505067Z","shell.execute_reply":"2025-11-02T14:28:42.690542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 15: Ensemble Predictions\n# ==========================================\nprint(\"üéØ Creating Ensemble Predictions...\")\n\n# Simple average ensemble\nensemble_predictions = (lgbm_predictions + xgb_predictions + cb_predictions) / 3\n\n# Weighted ensemble (weights based on CV scores)\nweights = np.array([np.mean(lgbm_scores), np.mean(xgb_scores), np.mean(cb_scores)])\nweights = weights / weights.sum()\n\nweighted_ensemble = (\n    lgbm_predictions * weights[0] + \n    xgb_predictions * weights[1] + \n    cb_predictions * weights[2]\n)\n\nprint(f\"‚úÖ Ensemble predictions created!\")\nprint(f\"   ‚Ä¢ Model weights: LightGBM={weights[0]:.3f}, XGBoost={weights[1]:.3f}, CatBoost={weights[2]:.3f}\")\nprint(f\"   ‚Ä¢ Prediction range: [{weighted_ensemble.min():.4f}, {weighted_ensemble.max():.4f}]\")\n\n# Visualize prediction distribution\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].hist(ensemble_predictions, bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)\naxes[0].set_xlabel('Predicted Probability', fontweight='bold')\naxes[0].set_ylabel('Frequency', fontweight='bold')\naxes[0].set_title('üìä Simple Average Ensemble Distribution', fontweight='bold')\naxes[0].grid(alpha=0.3)\n\naxes[1].hist(weighted_ensemble, bins=50, color='#FF6B6B', edgecolor='black', alpha=0.7)\naxes[1].set_xlabel('Predicted Probability', fontweight='bold')\naxes[1].set_ylabel('Frequency', fontweight='bold')\naxes[1].set_title('üìä Weighted Ensemble Distribution', fontweight='bold')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:28:42.692204Z","iopub.execute_input":"2025-11-02T14:28:42.692479Z","iopub.status.idle":"2025-11-02T14:28:43.095213Z","shell.execute_reply.started":"2025-11-02T14:28:42.692455Z","shell.execute_reply":"2025-11-02T14:28:43.094542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 16: Create Submission File\n# ==========================================\nprint(\"üì§ Creating submission file...\")\n\n# Use weighted ensemble for final submission\nsubmission['loan_paid_back'] = weighted_ensemble\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"‚úÖ Submission file created successfully!\")\nprint(f\"\\nüìã Submission Preview:\")\nprint(submission.head(10))\nprint(f\"\\nüìä Submission Statistics:\")\nprint(submission['loan_paid_back'].describe())\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ NOTEBOOK EXECUTION COMPLETE!\")\nprint(\"=\"*60)\nprint(\"üìÅ Submission file: submission.csv\")\nprint(\"üèÜ Expected LB Score: ~{:.4f}\".format(np.mean([np.mean(lgbm_scores), \n                                                       np.mean(xgb_scores), \n                                                       np.mean(cb_scores)])))\nprint(\"=\"*60)\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Download submission.csv\")\nprint(\"   2. Submit to Kaggle\")\nprint(\"   3. Check leaderboard score\")\nprint(\"   4. Iterate and improve! üöÄ\")\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:28:43.095928Z","iopub.execute_input":"2025-11-02T14:28:43.096106Z","iopub.status.idle":"2025-11-02T14:28:43.612227Z","shell.execute_reply.started":"2025-11-02T14:28:43.096092Z","shell.execute_reply":"2025-11-02T14:28:43.611573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 17: Model Saving Summary & Metadata\n# ==========================================\nprint(\"üíæ MODEL SAVING SUMMARY\")\nprint(\"=\"*60)\n\n# Save model metadata\nmodel_metadata = {\n    'competition': 'Playground Series S5E11 - Loan Payback Prediction',\n    'date_trained': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'models': {\n        'lgbm': {\n            'params': lgbm_params,\n            'cv_scores': lgbm_scores,\n            'mean_score': np.mean(lgbm_scores),\n            'std_score': np.std(lgbm_scores),\n            'files': [f'models/lgbm_fold_{i}.pkl' for i in range(1, 6)]\n        },\n        'xgb': {\n            'params': xgb_params,\n            'cv_scores': xgb_scores,\n            'mean_score': np.mean(xgb_scores),\n            'std_score': np.std(xgb_scores),\n            'files': [f'models/xgb_fold_{i}.pkl' for i in range(1, 6)]\n        },\n        'catboost': {\n            'params': cb_params,\n            'cv_scores': cb_scores,\n            'mean_score': np.mean(cb_scores),\n            'std_score': np.std(cb_scores),\n            'files': [f'models/catboost_fold_{i}.cbm' for i in range(1, 6)]\n        }\n    },\n    'ensemble_weights': {\n        'lgbm': float(weights[0]),\n        'xgb': float(weights[1]),\n        'catboost': float(weights[2])\n    },\n    'feature_names': X.columns.tolist(),\n    'n_features': X.shape[1]\n}\n\n# Save metadata as JSON\nimport json\nwith open('models/model_metadata.json', 'w') as f:\n    json.dump(model_metadata, f, indent=4)\n\nprint(f\"‚úÖ Saved {len(lgbm_models)} LightGBM models\")\nprint(f\"‚úÖ Saved {len(xgb_models)} XGBoost models\")\nprint(f\"‚úÖ Saved {len(cb_models)} CatBoost models\")\nprint(f\"‚úÖ Saved model metadata: models/model_metadata.json\")\nprint(f\"\\nüìÅ Total files saved: {len(lgbm_models) + len(xgb_models) + len(cb_models) + 1}\")\nprint(\"=\"*60)\n\n# List all saved files\nprint(\"\\nüìÇ Saved Model Files:\")\nprint(\"-\" * 60)\nfor model_type in ['lgbm', 'xgb', 'catboost']:\n    print(f\"\\n{model_type.upper()}:\")\n    for fold in range(1, 6):\n        if model_type == 'catboost':\n            filename = f'models/catboost_fold_{fold}.cbm'\n        else:\n            filename = f'models/{model_type}_fold_{fold}.pkl'\n        \n        if os.path.exists(filename):\n            size = os.path.getsize(filename) / (1024 * 1024)  # Convert to MB\n            print(f\"   ‚úì {filename} ({size:.2f} MB)\")\n\nprint(f\"\\n   ‚úì models/model_metadata.json\")\nprint(\"=\"*60)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:28:43.612993Z","iopub.execute_input":"2025-11-02T14:28:43.613218Z","iopub.status.idle":"2025-11-02T14:28:43.626562Z","shell.execute_reply.started":"2025-11-02T14:28:43.613201Z","shell.execute_reply":"2025-11-02T14:28:43.625831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 18: Load Saved Models (Example)\n# ==========================================\nprint(\"üì• EXAMPLE: HOW TO LOAD SAVED MODELS\")\nprint(\"=\"*60)\n\nprint(\"\"\"\n# To load and use the saved models later:\n\n# 1. Load LightGBM models\nimport joblib\nlgbm_model_fold1 = joblib.load('models/lgbm_fold_1.pkl')\npredictions_lgbm = lgbm_model_fold1.predict_proba(X_test)[:, 1]\n\n# 2. Load XGBoost models\nxgb_model_fold1 = joblib.load('models/xgb_fold_1.pkl')\npredictions_xgb = xgb_model_fold1.predict_proba(X_test)[:, 1]\n\n# 3. Load CatBoost models\nfrom catboost import CatBoostClassifier\ncb_model_fold1 = CatBoostClassifier()\ncb_model_fold1.load_model('models/catboost_fold_1.cbm')\npredictions_cb = cb_model_fold1.predict_proba(X_test)[:, 1]\n\n# 4. Load all models and create ensemble\nimport json\nwith open('models/model_metadata.json', 'r') as f:\n    metadata = json.load(f)\n\nensemble_weights = metadata['ensemble_weights']\nprint(f\"Ensemble weights: {ensemble_weights}\")\n\n# 5. Recreate predictions from all folds\nall_lgbm_preds = []\nfor fold in range(1, 6):\n    model = joblib.load(f'models/lgbm_fold_{fold}.pkl')\n    preds = model.predict_proba(X_test)[:, 1]\n    all_lgbm_preds.append(preds)\n\nlgbm_ensemble = np.mean(all_lgbm_preds, axis=0)\n\"\"\")\n\nprint(\"=\"*60)\n\n# Example: Actually load one model to verify\nprint(\"\\nüîç Verification: Loading one model as example...\")\ntry:\n    test_model = joblib.load('models/lgbm_fold_1.pkl')\n    print(\"‚úÖ Successfully loaded models/lgbm_fold_1.pkl\")\n    print(f\"   Model type: {type(test_model).__name__}\")\n    print(f\"   Number of features: {test_model.n_features_in_}\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéØ All models saved and ready for future use!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:28:43.627432Z","iopub.execute_input":"2025-11-02T14:28:43.627697Z","iopub.status.idle":"2025-11-02T14:28:43.667122Z","shell.execute_reply.started":"2025-11-02T14:28:43.627672Z","shell.execute_reply":"2025-11-02T14:28:43.666553Z"}},"outputs":[],"execution_count":null}]}