{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================================\n# IMPORT LIBRARIES AND CONFIGURE ENVIRONMENT\n# ================================================================\n# Ignore unnecessary warnings\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Import essential libraries\nimport pandas as pd\nimport numpy as np\n\n# Import utilities for feature combinations and modeling\nfrom itertools import combinations\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Import PyTorch modules for neural networks\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# LOAD DATASETS\n# ================================================================\n# Load the training dataset\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\n\n# Load the testing dataset\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\n\n# Load the original reference dataset\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\n# Display dataset shapes for verification\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)\nprint('Orig Shape:', orig.shape)","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# DEFINE TARGET AND CATEGORICAL VARIABLES\n# ================================================================\n# Define the target variable\nTARGET = 'loan_paid_back'\n\n# Define categorical columns\nCATS = [\n    'gender',\n    'marital_status',\n    'education_level',\n    'employment_status',\n    'loan_purpose',\n    'grade_subgrade'\n]\n\n# Define base features excluding id and target\nBASE = [col for col in train.columns if col not in ['id', TARGET]]","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# CREATE INTERACTION FEATURES\n# ================================================================\n# Initialize list to store interaction feature names\nINTER = []\n\n# Create pairwise combinations of base features\nfor col1, col2 in combinations(BASE, 2):\n\n    # Define the new interaction feature name\n    new_col_name = f'{col1}_{col2}'\n\n    # Add to interaction list\n    INTER.append(new_col_name)\n\n    # Concatenate values across all datasets\n    for df in [train, test, orig]:\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n\n# Display count of generated interaction features\nprint(f'{len(INTER)} Interaction Features Created.')","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# ADD MEAN AND COUNT STATISTICS FROM ORIGINAL DATA\n# ================================================================\n# Initialize list to store new features\nORIG = []\n\n# Generate mean and count features from the original dataset\nfor col in BASE:\n\n    # Compute mean target per category\n    mean_map = orig.groupby(col)[TARGET].mean()\n    new_mean_col_name = f\"orig_mean_{col}\"\n    mean_map.name = new_mean_col_name\n\n    # Merge mean feature into train and test\n    train = train.merge(mean_map, on=col, how='left')\n    test = test.merge(mean_map, on=col, how='left')\n    ORIG.append(new_mean_col_name)\n\n    # Compute count per category\n    count_map = orig.groupby(col).size().reset_index(name=f\"orig_count_{col}\")\n\n    # Merge count feature into train and test\n    train = train.merge(count_map, on=col, how='left')\n    test = test.merge(count_map, on=col, how='left')\n    ORIG.append(f\"orig_count_{col}\")\n\n# Display number of original features created\nprint(f'{len(ORIG)} Orig Features Created.')","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# COMBINE ALL FEATURE GROUPS\n# ================================================================\n# Combine base, original, and interaction features\nFEATURES = BASE + ORIG + INTER\n\n# Display final feature count\nprint(f'{len(FEATURES)} Total Features.')","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# DEFINE DATA MATRICES AND CROSS-VALIDATION STRATEGY\n# ================================================================\n# Create training feature matrix\nX = train[FEATURES]\n\n# Create target vector\ny = train[TARGET]\n\n# Define cross-validation splits\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# DEFINE TARGET ENCODER CLASS\n# ================================================================\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    # Initialize encoder with parameters\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n        # Store list of columns to encode\n        self.cols_to_encode = cols_to_encode\n\n        # Store aggregation methods\n        self.aggs = aggs\n\n        # Store number of cross-validation folds\n        self.cv = cv\n\n        # Store smoothing parameter\n        self.smooth = smooth\n\n        # Flag to determine if original columns should be dropped\n        self.drop_original = drop_original\n\n        # Initialize mapping dictionaries\n        self.mappings_ = {}\n\n        # Initialize global statistics\n        self.global_stats_ = {}\n\n\n    # Fit method to learn mappings from full data\n    def fit(self, X, y):\n        # Create a temporary DataFrame with target column\n        temp_df = X.copy()\n        temp_df['target'] = y\n\n        # Compute global statistics for each aggregation\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n\n        # Compute category-specific mappings for each column and aggregation\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n\n        # Return fitted encoder\n        return self\n\n\n    # Transform method to encode unseen data using learned mappings\n    def transform(self, X):\n        # Create a copy of the input DataFrame\n        X_transformed = X.copy()\n\n        # Iterate over each column and aggregation function\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n\n                # Define new encoded column name\n                new_col_name = f'TE_{col}_{agg_func}'\n\n                # Retrieve learned mapping\n                map_series = self.mappings_[col][agg_func]\n\n                # Map categories to target-based encoding\n                X_transformed[new_col_name] = X[col].map(map_series)\n\n                # Replace unseen values with global statistic\n                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n\n        # Drop original columns if specified\n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n\n        # Return transformed DataFrame\n        return X_transformed\n\n\n    # Fit and transform method with internal cross-validation\n    def fit_transform(self, X, y):\n        # Fit encoder on full dataset\n        self.fit(X, y)\n\n        # Create an empty DataFrame to store encoded values\n        encoded_features = pd.DataFrame(index=X.index)\n\n        # Initialize K-Fold cross-validation\n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n\n        # Perform internal CV to avoid target leakage\n        for train_idx, val_idx in kf.split(X, y):\n\n            # Split into training and validation folds\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx]\n\n            # Create temporary DataFrame with target column\n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n\n            # Encode each specified column\n            for col in self.cols_to_encode:\n\n                # Iterate through aggregation functions\n                for agg_func in self.aggs:\n\n                    # Define new encoded column name\n                    new_col_name = f'TE_{col}_{agg_func}'\n\n                    # Compute global statistic for the fold\n                    fold_global_stat = y_train.agg(agg_func)\n\n                    # Compute category-specific statistics\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n\n                    # Apply smoothing only for mean encoding\n                    if agg_func == 'mean':\n\n                        # Compute category sample counts\n                        counts = temp_df_train.groupby(col)['target'].count()\n\n                        # Determine smoothing parameter\n                        m = self.smooth\n                        if self.smooth == 'auto':\n                            variance_between = mapping.var()\n                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n                            m = avg_variance_within / variance_between if variance_between > 0 else 0\n\n                        # Compute smoothed mean\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n\n                        # Map validation data\n                        encoded_values = X_val[col].map(smoothed_mapping)\n\n                    # Handle non-mean aggregations\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n\n                    # Fill missing categories with fold-level global statistic\n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n\n        # Merge encoded features into original DataFrame\n        X_transformed = X.copy()\n\n        # Attach encoded columns to the transformed DataFrame\n        for col in encoded_features.columns:\n            X_transformed[col] = encoded_features[col]\n\n        # Drop original columns if required\n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n\n        # Return transformed DataFrame\n        return X_transformed","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# DEFINE PYTORCH DATASET CLASS\n# ================================================================\nclass TabularDataset(Dataset):\n    # Initialize dataset\n    def __init__(self, features, targets=None):\n        self.features = torch.tensor(features.values, dtype=torch.float32)\n        if targets is not None:\n            self.targets = torch.tensor(targets.values, dtype=torch.float32)\n        else:\n            self.targets = None\n\n    # Return dataset length\n    def __len__(self):\n        return len(self.features)\n\n    # Return item by index\n    def __getitem__(self, idx):\n        if self.targets is not None:\n            return self.features[idx], self.targets[idx]\n        return self.features[idx]","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# DEFINE NEURAL NETWORK MODEL\n# ================================================================\nclass MLPModel(nn.Module):\n    # Initialize network layers\n    def __init__(self, input_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.out = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    # Define forward pass\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        return self.out(x)","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# DEFINE TRAINING HELPER FUNCTIONS\n# ================================================================\n# ------------------------------------------------\n# Train the model for one epoch\n# ------------------------------------------------\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    # Set model to training mode\n    model.train()\n\n    # Initialize total running loss\n    running_loss = 0.0\n\n    # Iterate through each batch in the data loader\n    for features, targets in loader:\n\n        # Move features and targets to the selected device\n        features = features.to(device)\n        targets = targets.to(device).unsqueeze(1)\n\n        # Reset gradients before backward pass\n        optimizer.zero_grad()\n\n        # Perform forward pass to obtain model outputs\n        logits = model(features)\n\n        # Compute loss between predictions and targets\n        loss = criterion(logits, targets)\n\n        # Backpropagate gradients\n        loss.backward()\n\n        # Update model parameters\n        optimizer.step()\n\n        # Accumulate batch loss\n        running_loss += loss.item() * features.size(0)\n\n    # Compute average epoch loss\n    return running_loss / len(loader.dataset)\n\n\n# ------------------------------------------------\n# Evaluate model performance on validation data\n# ------------------------------------------------\ndef evaluate(model, loader, device):\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize list to store predictions\n    preds = []\n\n    # Disable gradient computation for efficiency\n    with torch.no_grad():\n\n        # Iterate through validation batches\n        for features, _ in loader:\n\n            # Move features to the selected device\n            features = features.to(device)\n\n            # Perform forward pass to obtain raw logits\n            logits = model(features)\n\n            # Convert logits to probabilities\n            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n\n            # Append batch predictions\n            preds.append(probs)\n\n    # Concatenate all batch predictions\n    return np.concatenate(preds)\n\n\n# ------------------------------------------------\n# Perform inference on test data\n# ------------------------------------------------\ndef infer_test(model, loader, device):\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize list to store predictions\n    preds = []\n\n    # Disable gradient computation for inference\n    with torch.no_grad():\n\n        # Iterate through test batches\n        for features in loader:\n\n            # Move features to the selected device\n            features = features.to(device)\n\n            # Perform forward pass to obtain raw logits\n            logits = model(features)\n\n            # Convert logits to probabilities\n            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n\n            # Append batch predictions\n            preds.append(probs)\n\n    # Concatenate all batch predictions\n    return np.concatenate(preds)","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# DEFINE CROSS-VALIDATION TRAINING LOOP\n# ================================================================\ndef run_cv(X, y, test_df, skf, inter_cols, cat_cols):\n    # Select computing device (use GPU if available)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize arrays for out-of-fold and test predictions\n    oof_preds = np.zeros(len(X))\n    test_preds = np.zeros(len(test_df))\n\n    # Set training hyperparameters\n    num_epochs = 24\n    batch_size = 384\n    lr = 5e-4\n\n    # Loop through each fold in cross-validation\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n\n        # Print the current fold number\n        print(f'--- Fold {fold}/{skf.n_splits} ---')\n\n        # Split data into training and validation subsets\n        X_train, y_train = X.iloc[train_idx].copy(), y.iloc[train_idx].copy()\n        X_val, y_val = X.iloc[val_idx].copy(), y.iloc[val_idx].copy()\n        X_test = test_df.copy()\n\n        # Initialize the target encoder\n        te = TargetEncoder(\n            cols_to_encode=inter_cols,\n            cv=5,\n            smooth='auto',\n            aggs=['mean'],\n            drop_original=True\n        )\n\n        # Apply target encoding on the training data\n        X_train = te.fit_transform(X_train, y_train)\n\n        # Transform validation and test data using learned mappings\n        X_val = te.transform(X_val)\n        X_test = te.transform(X_test)\n\n        # Convert categorical columns to numerical codes\n        for col in cat_cols:\n            X_train[col] = X_train[col].astype('category').cat.codes\n            X_val[col] = X_val[col].astype('category').cat.codes\n            X_test[col] = X_test[col].astype('category').cat.codes\n\n        # Fill any missing values with zeros\n        X_train = X_train.fillna(0)\n        X_val = X_val.fillna(0)\n        X_test = X_test.fillna(0)\n\n        # Create PyTorch datasets for train, validation, and test\n        train_ds = TabularDataset(X_train, y_train)\n        val_ds = TabularDataset(X_val, y_val)\n        test_ds = TabularDataset(X_test)\n\n        # Create data loaders for efficient batching\n        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n        test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n\n        # Determine the number of input features\n        input_dim = X_train.shape[1]\n\n        # Initialize the neural network model\n        model = MLPModel(input_dim=input_dim).to(device)\n\n        # Define the binary cross-entropy loss function\n        criterion = nn.BCEWithLogitsLoss()\n\n        # Define the optimizer for model training\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n        # Initialize variables to track the best model per fold\n        best_auc = -np.inf\n        best_val_preds = None\n\n        # Train the model for a fixed number of epochs\n        for epoch in range(1, num_epochs + 1):\n\n            # Perform one training epoch\n            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n\n            # Evaluate model performance on the validation set\n            val_preds = evaluate(model, val_loader, device)\n            val_auc = roc_auc_score(y_val, val_preds)\n\n            # Print epoch results (training loss and validation AUC)\n            print(f'Epoch {epoch:02d} - loss: {train_loss:.4f} - val_auc: {val_auc:.4f}')\n\n            # Update best model if AUC improves\n            if val_auc > best_auc:\n                best_auc = val_auc\n                best_val_preds = val_preds.copy()\n\n        # Store best validation predictions in out-of-fold array\n        oof_preds[val_idx] = best_val_preds\n\n        # Generate predictions on test data for the current fold\n        fold_test_preds = infer_test(model, test_loader, device)\n\n        # Average test predictions across all folds\n        test_preds += fold_test_preds / skf.n_splits\n\n        # Print final AUC for this fold\n        print(f'Fold {fold} AUC: {best_auc:.4f}')\n\n    # Compute overall OOF AUC score\n    overall_auc = roc_auc_score(y, oof_preds)\n\n    # Display final summary results\n    print('====================')\n    print(f'Overall OOF AUC: {overall_auc:.4f}')\n    print('====================')\n\n    # Return out-of-fold predictions, test predictions, and overall AUC\n    return oof_preds, test_preds, overall_auc","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# MAIN EXECUTION FUNCTION\n# ================================================================\ndef main():\n    # Run cross-validation\n    oof_preds, test_preds, overall_auc = run_cv(X, y, test[FEATURES], skf, INTER, CATS)\n\n    # Save out-of-fold predictions\n    pd.DataFrame({'id': train.id, TARGET: oof_preds}).to_csv(\n        f'oof_torch_cv_{overall_auc:.5f}.csv', index=False\n    )\n\n    # Save test predictions\n    pd.DataFrame({'id': test.id, TARGET: test_preds}).to_csv(\n        'submission.csv', index=False\n    )","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# EXECUTION ENTRY POINT\n# ================================================================\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"516f741f-3bbb-47ee-95bb-dc7485e36f46","_cell_guid":"f16f15ed-ac12-4e10-bbff-425429c333c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}