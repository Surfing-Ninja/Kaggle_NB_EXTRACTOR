{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672},{"sourceId":13582697,"sourceType":"datasetVersion","datasetId":8629306},{"sourceId":13617019,"sourceType":"datasetVersion","datasetId":8635402}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":5939.274657,"end_time":"2025-11-09T12:44:33.049114","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-09T11:05:33.774457","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"402b9551","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nimport warnings\nimport gc\nimport time\n\n# --- Core Scikit-learn ---\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# --- NEW: MLP Model ---\nfrom sklearn.neural_network import MLPClassifier \n\n# --- GBDT Models ---\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier, Pool\n\n# --- Blending ---\nfrom scipy.optimize import minimize\nfrom scipy.stats import rankdata\n\nwarnings.simplefilter('ignore')\nprint(\"=\"*80)\nprint(\"ðŸš€ ULTIMATE 5-MODEL STACK (REVERSE-ENGINEERED) ðŸš€\")\nprint(\"=\"*80)","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:05:37.340705Z","iopub.status.busy":"2025-11-09T11:05:37.340501Z","iopub.status.idle":"2025-11-09T11:05:44.76195Z","shell.execute_reply":"2025-11-09T11:05:44.761086Z"},"papermill":{"duration":7.427094,"end_time":"2025-11-09T11:05:44.763375","exception":false,"start_time":"2025-11-09T11:05:37.336281","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"35db8066","cell_type":"code","source":"# ============================================================================\n# CONFIGURATION\n# ============================================================================\nSEED_LIST = [42, 123]  # Multi-seed for stability\nN_SPLITS = 5           # 5-Folds for speed\nTARGET = 'loan_paid_back'","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:05:44.770524Z","iopub.status.busy":"2025-11-09T11:05:44.769818Z","iopub.status.idle":"2025-11-09T11:05:44.773613Z","shell.execute_reply":"2025-11-09T11:05:44.773045Z"},"papermill":{"duration":0.008104,"end_time":"2025-11-09T11:05:44.774624","exception":false,"start_time":"2025-11-09T11:05:44.76652","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"39758a6d","cell_type":"code","source":"# ============================================================================\n# LOAD DATA\n# ============================================================================\nprint(\"\\n[LOADING DATA]\")\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n\n# âš ï¸ CRITICAL: Store original lengths BEFORE any modifications\nTRAIN_LEN = len(train)\nTEST_LEN = len(test)\n\nprint(f\"Train: {train.shape}, Test: {test.shape}, Orig: {orig.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:05:44.780813Z","iopub.status.busy":"2025-11-09T11:05:44.780416Z","iopub.status.idle":"2025-11-09T11:05:46.438371Z","shell.execute_reply":"2025-11-09T11:05:46.437477Z"},"papermill":{"duration":1.662384,"end_time":"2025-11-09T11:05:46.439635","exception":false,"start_time":"2025-11-09T11:05:44.777251","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"d25797e5","cell_type":"code","source":"# ============================================================================\n# FEATURE ENGINEERING (FROM PRO MODEL)\n# ============================================================================\nprint(\"\\n[STARTING FEATURE ENGINEERING...]\")\nBASE = [col for col in train.columns if col not in ['id', TARGET]]\nCATS = ['gender', 'marital_status', 'education_level', 'employment_status', \n        'loan_purpose', 'grade_subgrade']\nNUMS = [col for col in BASE if col not in CATS]\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1) ADVANCED ORIG STATISTICS\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n[1/4] Advanced Orig Statistics...\")\nORIG_STATS = []\nfor col in BASE:\n    for stat in ['mean', 'std', 'min', 'max', 'median']:\n        stat_map = orig.groupby(col)[TARGET].agg(stat)\n        stat_map.name = f\"orig_{stat}_{col}\"\n        train = train.merge(stat_map, on=col, how='left')\n        test = test.merge(stat_map, on=col, how='left')\n        ORIG_STATS.append(f\"orig_{stat}_{col}\")\n    \n    count_map = orig.groupby(col).size().reset_index(name=f\"orig_count_{col}\")\n    train = train.merge(count_map, on=col, how='left')\n    test = test.merge(count_map, on=col, how='left')\n    ORIG_STATS.append(f\"orig_count_{col}\")\nprint(f\"âœ“ Created {len(ORIG_STATS)} advanced orig features\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2) BIGRAM & 3-WAY INTERACTIONS\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n[2/4] Interaction Features...\")\n\n# Add placeholder target for concatenation\ntest[TARGET] = -1\n# Concat ALL 3 datasets for robust feature creation\ncombine = pd.concat([train, test, orig], axis=0, ignore_index=True)\n\nINTER = []\nTE_BASE = [col for col in BASE if col not in ['annual_income', 'loan_amount']]\nfor col1, col2 in combinations(TE_BASE, 2):\n    new_col = f'{col1}_{col2}'\n    INTER.append(new_col)\n    combine[new_col] = combine[col1].astype(str) + \"_\" + combine[col2].astype(str)\n\nINTER_3WAY = []\ntriplets = [\n    ('grade_subgrade', 'employment_status', 'loan_purpose'),\n    ('grade_subgrade', 'debt_to_income_ratio', 'credit_score'),\n    ('loan_purpose', 'education_level', 'marital_status')]\nfor col1, col2, col3 in triplets:\n    new_col = f'{col1}_{col2}_{col3}'\n    INTER_3WAY.append(new_col)\n    combine[new_col] = (combine[col1].astype(str) + \"_\" + \n                       combine[col2].astype(str) + \"_\" + \n                       combine[col3].astype(str))\nprint(f\"âœ“ Created {len(INTER)} bigrams + {len(INTER_3WAY)} 3-way interactions\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 3) QUANTILE FEATURES\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n[3/4] Quantile Features...\")\nQFEAT = []\nfor col in ['annual_income', 'loan_amount', 'credit_score']:\n    # Create quantiles from the training data only to prevent leakage\n    quantiles = np.percentile(combine.iloc[:TRAIN_LEN][col].dropna(), np.arange(0, 101, 5))\n    qcol = f'{col}_quantile'\n    combine[qcol] = np.digitize(combine[col], quantiles)\n    QFEAT.append(qcol)\nprint(f\"âœ“ Created {len(QFEAT)} quantile features\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 4) ROUNDING FEATURES\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n[4/4] Rounding Features...\")\nROUND = []\nrounding_levels = {'1s': 0, '10s': -1, '100s': -2}\nfor col in ['annual_income', 'loan_amount']:\n    for suffix, level in rounding_levels.items():\n        new_col = f\"{col}_ROUND_{suffix}\"\n        ROUND.append(new_col)\n        combine[new_col] = combine[col].round(level).astype(int)\nprint(f\"âœ“ Created {len(ROUND)} rounding features\")\nprint(f\"\\nâœ… Feature Engineering Complete\")","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:05:46.446485Z","iopub.status.busy":"2025-11-09T11:05:46.446209Z","iopub.status.idle":"2025-11-09T11:06:23.048636Z","shell.execute_reply":"2025-11-09T11:06:23.047854Z"},"papermill":{"duration":36.607262,"end_time":"2025-11-09T11:06:23.04983","exception":false,"start_time":"2025-11-09T11:05:46.442568","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"930d6453","cell_type":"code","source":"# ============================================================================\n# âš ï¸ CRITICAL FIX: PROPER DATA SPLITTING âš ï¸\n# ============================================================================\nprint('\\n' + '='*80)\nprint('SPLITTING DATA (FIXED INDEXING)')\nprint('='*80)\n\n# Split using EXACT row counts stored at the beginning\ntrain = combine.iloc[:TRAIN_LEN].copy()\ntest = combine.iloc[TRAIN_LEN:TRAIN_LEN + TEST_LEN].copy()\norig = combine.iloc[TRAIN_LEN + TEST_LEN:].copy() # Keep orig for GBDT augmentation\n\n# Verify splits\nprint(f\"âœ“ Train: {train.shape} (expected {TRAIN_LEN} rows)\")\nprint(f\"âœ“ Test: {test.shape} (expected {TEST_LEN} rows)\")\nprint(f\"âœ“ Orig: {orig.shape} (expected {orig.shape[0]} rows)\")\n\n# Remove placeholder target from test\ntest = test.drop(columns=[TARGET])\ndel combine\ngc.collect()\nprint(f'\\nâœ… Data split verified successfully!')\n\n# ============================================================================\n# FINAL FEATURE SETS\n# ============================================================================\n# All new string/quantile features to be Target Encoded\nCOLS_TO_TE = INTER + INTER_3WAY + QFEAT + ROUND\n\n# Features for GBDTs (XGB, LGBM, CAT)\nFEATURES_GBDT_BASE = BASE + ORIG_STATS + QFEAT + ROUND\n\n# Features for Linear/MLP models\nCATS_LR_MLP = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nNUMS_LR_MLP = NUMS + ORIG_STATS # Use original nums + all new numeric orig stats\nFEATURES_LR_MLP_BASE = NUMS_LR_MLP + CATS_LR_MLP\n\nprint(f\"\\nTotal features to Target Encode: {len(COLS_TO_TE)}\")","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:06:23.056906Z","iopub.status.busy":"2025-11-09T11:06:23.056703Z","iopub.status.idle":"2025-11-09T11:06:26.643854Z","shell.execute_reply":"2025-11-09T11:06:26.642888Z"},"papermill":{"duration":3.592119,"end_time":"2025-11-09T11:06:26.645123","exception":false,"start_time":"2025-11-09T11:06:23.053004","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"f52d97ff","cell_type":"code","source":"# ============================================================================\n# TARGET ENCODER CLASS (CPU-SAFE, ROBUST)\n# ============================================================================\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth=1.0, drop_original=False):\n        self.cols_to_encode = cols_to_encode\n        self.aggs = aggs\n        self.cv = cv\n        self.smooth = smooth\n        self.drop_original = drop_original\n        self.mappings_ = {}\n        self.global_stats_ = {}\n\n    def fit(self, X, y):\n        temp_df = X.copy()\n        temp_df['target'] = y\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n                new_col_name = f'TE_{col}_{agg_func}'\n                map_series = self.mappings_[col][agg_func]\n                X_transformed[new_col_name] = X[col].map(map_series)\n                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n        return X_transformed\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        encoded_features = pd.DataFrame(index=X.index)\n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n        \n        for train_idx, val_idx in kf.split(X, y):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx]\n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n            \n            for col in self.cols_to_encode:\n                for agg_func in self.aggs:\n                    new_col_name = f'TE_{col}_{agg_func}'\n                    fold_global_stat = y_train.agg(agg_func)\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n                    \n                    if agg_func == 'mean':\n                        counts = temp_df_train.groupby(col)['target'].count()\n                        m = self.smooth\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n                        encoded_values = X_val[col].map(smoothed_mapping)\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n                    \n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n        \n        X_transformed = X.copy()\n        for col in encoded_features.columns:\n            if encoded_features[col].isnull().any():\n                encoded_features[col].fillna(self.global_stats_[agg_func], inplace=True)\n            X_transformed[col] = encoded_features[col]\n        \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n        \n        return X_transformed\n\nprint(\"\\nâœ“ TargetEncoder class defined\")","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:06:26.653474Z","iopub.status.busy":"2025-11-09T11:06:26.653241Z","iopub.status.idle":"2025-11-09T11:06:26.664672Z","shell.execute_reply":"2025-11-09T11:06:26.664089Z"},"papermill":{"duration":0.016723,"end_time":"2025-11-09T11:06:26.665676","exception":false,"start_time":"2025-11-09T11:06:26.648953","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"2b19006e","cell_type":"code","source":"# ============================================================================\n# CRITICAL FIX: DTYPE CONVERSION FUNCTION\n# ============================================================================\ndef fix_dtypes_for_lgb(df, categorical_cols):\n    \"\"\"Fix dtypes for LightGBM compatibility.\"\"\"\n    df_fixed = df.copy()\n    for col in df_fixed.columns:\n        if col in categorical_cols:\n            df_fixed[col] = df_fixed[col].astype('category')\n        elif df_fixed[col].dtype == 'object':\n            # Factorize any remaining object columns\n            df_fixed[col], _ = pd.factorize(df_fixed[col])\n    return df_fixed\n\nprint(\"âœ“ LGBM Dtype Fix function defined\")","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:06:26.672939Z","iopub.status.busy":"2025-11-09T11:06:26.672539Z","iopub.status.idle":"2025-11-09T11:06:26.677102Z","shell.execute_reply":"2025-11-09T11:06:26.676452Z"},"papermill":{"duration":0.009375,"end_time":"2025-11-09T11:06:26.678255","exception":false,"start_time":"2025-11-09T11:06:26.66888","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"7089a4ed","cell_type":"code","source":"# ============================================================================\n# L1 TRAINING FUNCTION (ALL 5 MODELS)\n# ============================================================================\ndef train_models(seed):\n    \"\"\"Train all 5 models with proper FE and dtype handling for a given seed\"\"\"\n    \n    print(f\"\\n{'â”€'*80}\")\n    print(f\"SEED: {seed} - STARTING TRAINING\")\n    print(f\"{'â”€'*80}\")\n    \n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n    \n    # Init OOF/Test arrays for this seed\n    oof_lgb = np.zeros(TRAIN_LEN)\n    test_lgb = np.zeros(TEST_LEN)\n    oof_cat = np.zeros(TRAIN_LEN)\n    test_cat = np.zeros(TEST_LEN)\n    oof_xgb = np.zeros(TRAIN_LEN)\n    test_xgb = np.zeros(TEST_LEN)\n    oof_mlp = np.zeros(TRAIN_LEN)\n    test_mlp = np.zeros(TEST_LEN)\n    oof_lr = np.zeros(TRAIN_LEN)\n    test_lr = np.zeros(TEST_LEN)\n\n    # We must loop fold by fold because the FE is different for GBDTs vs Linear/MLP\n    for fold, (train_idx, val_idx) in enumerate(skf.split(train, train[TARGET]), 1):\n        print(f\"\\n--- Fold {fold}/{N_SPLITS} ---\")\n        \n        # -------------------------------------------------\n        # 1. Create Data for this Fold\n        # -------------------------------------------------\n        X_train_fold = train.iloc[train_idx].copy()\n        y_train_fold = train.iloc[train_idx][TARGET].copy() # Base train target\n        X_valid_fold = train.iloc[val_idx].copy()\n        y_valid_fold = train.iloc[val_idx][TARGET].copy() # Base validation target\n        X_test_fold = test.copy()\n\n        # -------------------------------------------------\n        # 2. Create GBDT data (LGBM, CAT, XGB)\n        # -------------------------------------------------\n        \n        # Add orig data for GBDT training\n        Xy_train_fold_gbdt = pd.concat([X_train_fold, orig], axis=0, ignore_index=True)\n        y_train_fold_gbdt = Xy_train_fold_gbdt[TARGET] # This is the augmented training target\n\n        # Apply Target Encoding\n        te_gbdt = TargetEncoder(cols_to_encode=COLS_TO_TE, cv=5, smooth=1.0, aggs=['mean'], drop_original=True)\n        X_train_gbdt = te_gbdt.fit_transform(Xy_train_fold_gbdt, y_train_fold_gbdt)\n        X_valid_gbdt = te_gbdt.transform(X_valid_fold.copy())\n        X_test_gbdt = te_gbdt.transform(X_test_fold.copy())\n\n        # Define GBDT features\n        FEATURES_GBDT_FINAL = [col for col in X_train_gbdt.columns if col not in ['id', TARGET]]\n        \n        # -------------------------------------------------\n        # 3. Create LR/MLP data (Linear, NN)\n        # -------------------------------------------------\n        \n        # Apply Target Encoding (this \"feeds\" the models)\n        te_linear = TargetEncoder(cols_to_encode=COLS_TO_TE, cv=5, smooth=1.0, aggs=['mean'], drop_original=True)\n        X_train_linear = te_linear.fit_transform(X_train_fold.copy(), y_train_fold) # Use base train fold for fit\n        X_valid_linear = te_linear.transform(X_valid_fold.copy())\n        X_test_linear = te_linear.transform(X_test_fold.copy())\n        \n        # Get list of new TE features\n        TE_FEATURES = [col for col in X_train_linear.columns if 'TE_' in col]\n        \n        # Define final LR/MLP feature list\n        NUMS_LR_MLP_FINAL = NUMS_LR_MLP + TE_FEATURES\n        FEATURES_LR_MLP_FINAL = NUMS_LR_MLP_FINAL + CATS_LR_MLP\n\n        X_train_linear = X_train_linear[FEATURES_LR_MLP_FINAL].copy()\n        X_valid_linear = X_valid_linear[FEATURES_LR_MLP_FINAL].copy()\n        X_test_linear = X_test_linear[FEATURES_LR_MLP_FINAL].copy()\n        \n        # Fill NaNs\n        X_train_linear[NUMS_LR_MLP_FINAL] = X_train_linear[NUMS_LR_MLP_FINAL].fillna(0)\n        X_valid_linear[NUMS_LR_MLP_FINAL] = X_valid_linear[NUMS_LR_MLP_FINAL].fillna(0)\n        X_test_linear[NUMS_LR_MLP_FINAL] = X_test_linear[NUMS_LR_MLP_FINAL].fillna(0)\n        \n        # One-Hot Encode\n        X_train_linear = pd.get_dummies(X_train_linear, columns=CATS_LR_MLP, drop_first=True)\n        X_valid_linear = pd.get_dummies(X_valid_linear, columns=CATS_LR_MLP, drop_first=True)\n        X_test_linear = pd.get_dummies(X_test_linear, columns=CATS_LR_MLP, drop_first=True)\n        \n        # Align\n        X_train_linear, X_valid_linear = X_train_linear.align(X_valid_linear, join='left', axis=1, fill_value=0)\n        X_train_linear, X_test_linear = X_train_linear.align(X_test_linear, join='left', axis=1, fill_value=0)\n        \n        # -------------------------------------------------\n        # 4. Train Models\n        # -------------------------------------------------\n        \n        # [1/5] LightGBM\n        print(\"  [1/5] Training LGBM...\")\n        params_lgb = {\n            'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n            'learning_rate': 0.008, 'num_leaves': 32, 'max_depth': 6,\n            'min_child_samples': 20, 'subsample': 0.75, 'colsample_bytree': 0.35,\n            'reg_alpha': 1.2, 'reg_lambda': 5.5, 'random_state': seed,\n            'device': 'gpu', 'verbose': -1, 'n_estimators': 12000\n        }\n        lgb_cat_cols = [c for c in CATS + QFEAT if c in X_train_gbdt.columns]\n        X_train_lgb = fix_dtypes_for_lgb(X_train_gbdt[FEATURES_GBDT_FINAL], lgb_cat_cols)\n        X_valid_lgb = fix_dtypes_for_lgb(X_valid_gbdt[FEATURES_GBDT_FINAL], lgb_cat_cols)\n        X_test_lgb = fix_dtypes_for_lgb(X_test_gbdt[FEATURES_GBDT_FINAL], lgb_cat_cols)\n\n        model_lgb = LGBMClassifier(**params_lgb)\n        model_lgb.fit(X_train_lgb, y_train_fold_gbdt, \n                      eval_set=[(X_valid_lgb, y_valid_fold)],\n                      callbacks=[lgb.early_stopping(200, verbose=False)],\n                      categorical_feature=[c for c in lgb_cat_cols if c in X_train_lgb.columns])\n        \n        oof_lgb[val_idx] = model_lgb.predict_proba(X_valid_lgb)[:, 1]\n        test_lgb += model_lgb.predict_proba(X_test_lgb)[:, 1] / N_SPLITS\n        del X_train_lgb, X_valid_lgb, X_test_lgb, model_lgb\n        \n        # [2/5] CatBoost\n        print(\"  [2/5] Training CatBoost...\")\n        params_cat = {\n            'iterations': 2500, 'learning_rate': 0.02, 'depth': 6, 'l2_leaf_reg': 3.5,\n            'random_strength': 2.0, 'bagging_temperature': 0.5, 'task_type': 'GPU',\n            'loss_function': 'Logloss', 'eval_metric': 'AUC', 'random_seed': seed,\n            'early_stopping_rounds': 150, 'verbose': False\n        }\n        cat_cols_for_catboost = [c for c in CATS + QFEAT if c in X_train_gbdt.columns]\n        cat_indices = [X_train_gbdt[FEATURES_GBDT_FINAL].columns.get_loc(c) for c in cat_cols_for_catboost]\n        \n        train_pool = Pool(X_train_gbdt[FEATURES_GBDT_FINAL], y_train_fold_gbdt, cat_features=cat_indices)\n        val_pool = Pool(X_valid_gbdt[FEATURES_GBDT_FINAL], y_valid_fold, cat_features=cat_indices)\n        test_pool = Pool(X_test_gbdt[FEATURES_GBDT_FINAL], cat_features=cat_indices)\n        \n        model_cat = CatBoostClassifier(**params_cat)\n        model_cat.fit(train_pool, eval_set=val_pool)\n        \n        oof_cat[val_idx] = model_cat.predict_proba(val_pool)[:, 1]\n        test_cat += model_cat.predict_proba(test_pool)[:, 1] / N_SPLITS\n        del train_pool, val_pool, test_pool, model_cat\n        \n        # [3/5] XGBoost\n        print(\"  [3/5] Training XGBoost...\")\n        params_xgb = {\n            'objective': 'binary:logistic', 'eval_metric': 'auc', 'max_depth': 6,\n            'min_child_weight': 5, 'colsample_bytree': 0.35, 'colsample_bylevel': 0.65,\n            'subsample': 0.70, 'reg_alpha': 1.2, 'reg_lambda': 4.5, 'gamma': 0.4,\n            'learning_rate': 0.008, 'n_estimators': 15000, 'early_stopping_rounds': 250,\n            'random_state': seed, 'n_jobs': -1, 'enable_categorical': True,\n            'device': 'cuda', 'tree_method': 'hist'\n        }\n        xgb_cat_cols = [c for c in CATS + QFEAT if c in X_train_gbdt.columns]\n        \n        X_train_xgb = X_train_gbdt[FEATURES_GBDT_FINAL].copy()\n        X_valid_xgb = X_valid_gbdt[FEATURES_GBDT_FINAL].copy()\n        X_test_xgb = X_test_gbdt[FEATURES_GBDT_FINAL].copy()\n        \n        for cat_col in xgb_cat_cols:\n            X_train_xgb[cat_col] = X_train_xgb[cat_col].astype('category')\n            X_valid_xgb[cat_col] = X_valid_xgb[cat_col].astype('category')\n            X_test_xgb[cat_col] = X_test_xgb[cat_col].astype('category')\n        \n        model_xgb = XGBClassifier(**params_xgb)\n        model_xgb.fit(X_train_xgb, y_train_fold_gbdt, eval_set=[(X_valid_xgb, y_valid_fold)], verbose=False)\n        \n        oof_xgb[val_idx] = model_xgb.predict_proba(X_valid_xgb)[:, 1]\n        test_xgb += model_xgb.predict_proba(X_test_xgb)[:, 1] / N_SPLITS\n        del X_train_xgb, X_valid_xgb, X_test_xgb, model_xgb\n        \n        # [4/5] MLPClassifier\n        print(\"  [4/5] Training MLP...\")\n        mlp_params = {\n            'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'adam',\n            'batch_size': 256, 'learning_rate': 'adaptive', 'max_iter': 1000,\n            'early_stopping': True, 'n_iter_no_change': 20,\n            'validation_fraction': 0.1, 'random_state': seed\n        }\n        pipe_mlp = Pipeline([\n            ('scaler', StandardScaler()),\n            ('model', MLPClassifier(**mlp_params))\n        ])\n        \n        # --- FIX: Use y_train_fold (the base fold target) ---\n        pipe_mlp.fit(X_train_linear, y_train_fold)\n        \n        oof_mlp[val_idx] = pipe_mlp.predict_proba(X_valid_linear)[:, 1]\n        test_mlp += pipe_mlp.predict_proba(X_test_linear)[:, 1] / N_SPLITS\n        del pipe_mlp\n        \n        # [5/5] Logistic Regression\n        print(\"  [5/5] Training LR...\")\n        pipe_lr = Pipeline([\n            ('scaler', StandardScaler()),\n            ('model', LogisticRegression(C=0.1, max_iter=1000, random_state=seed, solver='liblinear'))\n        ])\n        \n        # --- FIX: Use y_train_fold (the base fold target) ---\n        pipe_lr.fit(X_train_linear, y_train_fold)\n        \n        oof_lr[val_idx] = pipe_lr.predict_proba(X_valid_linear)[:, 1]\n        test_lr += pipe_lr.predict_proba(X_test_linear)[:, 1] / N_SPLITS\n        del pipe_lr\n        \n        # Cleanup\n        print(f\"  Fold {fold} complete.\")\n        del X_train_gbdt, y_train_fold_gbdt, X_valid_gbdt, X_test_gbdt\n        del X_train_linear, y_valid_fold, X_valid_linear, X_test_linear\n        del X_train_fold, y_train_fold, X_valid_fold, X_test_fold\n        gc.collect()\n\n    # Print fold scores\n    print(f\"\\nSEED {seed} CV Scores:\")\n    print(f\"  LGBM: {roc_auc_score(train[TARGET], oof_lgb):.5f}\")\n    print(f\"  CAT:  {roc_auc_score(train[TARGET], oof_cat):.5f}\")\n    print(f\"  XGB:  {roc_auc_score(train[TARGET], oof_xgb):.5f}\")\n    print(f\"  MLP:  {roc_auc_score(train[TARGET], oof_mlp):.5f}\")\n    print(f\"  LR:   {roc_auc_score(train[TARGET], oof_lr):.5f}\")\n    \n    return oof_lgb, test_lgb, oof_cat, test_cat, oof_xgb, test_xgb, oof_mlp, test_mlp, oof_lr, test_lr","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:06:26.685675Z","iopub.status.busy":"2025-11-09T11:06:26.685229Z","iopub.status.idle":"2025-11-09T11:06:26.706065Z","shell.execute_reply":"2025-11-09T11:06:26.705563Z"},"papermill":{"duration":0.025672,"end_time":"2025-11-09T11:06:26.707104","exception":false,"start_time":"2025-11-09T11:06:26.681432","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"3f1aa6ed","cell_type":"code","source":"# ============================================================================\n# RUN TRAINING FOR ALL SEEDS\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING MULTI-SEED 5-MODEL ENSEMBLE\")\nprint(\"=\"*80)\nstart_time = time.time()\n\noof_lgbs, test_lgbs = [], []\noof_cats, test_cats = [], []\noof_xgbs, test_xgbs = [], []\noof_mlps, test_mlps = [], []\noof_lrs, test_lrs = [], []\n\nfor seed in SEED_LIST:\n    out_lgb, tst_lgb, out_cat, tst_cat, out_xgb, tst_xgb, out_mlp, tst_mlp, out_lr, tst_lr = train_models(seed)\n    \n    oof_lgbs.append(out_lgb)\n    test_lgbs.append(tst_lgb)\n    oof_cats.append(out_cat)\n    test_cats.append(tst_cat)\n    oof_xgbs.append(out_xgb)\n    test_xgbs.append(tst_xgb)\n    oof_mlps.append(out_mlp)\n    test_mlps.append(tst_mlp)\n    oof_lrs.append(out_lr)\n    test_lrs.append(tst_lr)","metadata":{"execution":{"iopub.execute_input":"2025-11-09T11:06:26.714308Z","iopub.status.busy":"2025-11-09T11:06:26.71396Z","iopub.status.idle":"2025-11-09T12:43:31.082332Z","shell.execute_reply":"2025-11-09T12:43:31.081516Z"},"papermill":{"duration":5824.373187,"end_time":"2025-11-09T12:43:31.083591","exception":false,"start_time":"2025-11-09T11:06:26.710404","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"4d8d59d0","cell_type":"code","source":"# ============================================================================\n# AVERAGE ACROSS SEEDS\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"AVERAGING PREDICTIONS ACROSS SEEDS\")\nprint(\"=\"*80)\n\noof_lgb_avg = np.mean(oof_lgbs, axis=0)\ntest_lgb_avg = np.mean(test_lgbs, axis=0)\noof_cat_avg = np.mean(oof_cats, axis=0)\ntest_cat_avg = np.mean(test_cats, axis=0)\noof_xgb_avg = np.mean(oof_xgbs, axis=0)\ntest_xgb_avg = np.mean(test_xgbs, axis=0)\noof_mlp_avg = np.mean(oof_mlps, axis=0)\ntest_mlp_avg = np.mean(test_mlps, axis=0)\noof_lr_avg = np.mean(oof_lrs, axis=0)\ntest_lr_avg = np.mean(test_lrs, axis=0)\n\ncv_lgb_final = roc_auc_score(train[TARGET], oof_lgb_avg)\ncv_cat_final = roc_auc_score(train[TARGET], oof_cat_avg)\ncv_xgb_final = roc_auc_score(train[TARGET], oof_xgb_avg)\ncv_mlp_final = roc_auc_score(train[TARGET], oof_mlp_avg)\ncv_lr_final = roc_auc_score(train[TARGET], oof_lr_avg)\n\nprint(f\"LGBM Multi-Seed CV: {cv_lgb_final:.5f}\")\nprint(f\"CatBoost Multi-Seed CV: {cv_cat_final:.5f}\")\nprint(f\"XGBoost Multi-Seed CV:  {cv_xgb_final:.5f}\")\nprint(f\"MLP Multi-Seed CV:      {cv_mlp_final:.5f}\")\nprint(f\"LR Multi-Seed CV:       {cv_lr_final:.5f}\")\n\n# This is our L2 training data\nall_oof_l1 = [oof_lgb_avg, oof_cat_avg, oof_xgb_avg, oof_mlp_avg, oof_lr_avg]\nall_test_l1 = [test_lgb_avg, test_cat_avg, test_xgb_avg, test_mlp_avg, test_lr_avg]","metadata":{"execution":{"iopub.execute_input":"2025-11-09T12:43:31.098885Z","iopub.status.busy":"2025-11-09T12:43:31.098661Z","iopub.status.idle":"2025-11-09T12:43:31.773479Z","shell.execute_reply":"2025-11-09T12:43:31.772492Z"},"papermill":{"duration":0.683592,"end_time":"2025-11-09T12:43:31.774726","exception":false,"start_time":"2025-11-09T12:43:31.091134","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"1705ead2","cell_type":"code","source":"# ============================================================================\n# LAYER 2: RANKING & ADVANCED STACKING\n# ============================================================================\nprint('\\n' + '='*80)\nprint('LAYER 2: RANKING & ADVANCED STACKING (5 MODELS)')\nprint('='*80)\n\nmeta_train = np.column_stack(all_oof_l1)\nmeta_test = np.column_stack(all_test_l1)\n\nprint(\"Converting OOF and Test predictions to ranks...\")\nmeta_train_ranked = np.zeros_like(meta_train)\nmeta_test_ranked = np.zeros_like(meta_test)\n\nfor i in range(meta_train.shape[1]):\n    meta_train_ranked[:, i] = rankdata(meta_train[:, i])\n    meta_test_ranked[:, i] = rankdata(meta_test[:, i])\n\n# --- Method 1: L2 LGBM Stacker (on Ranks) ---\nprint('\\nTraining L2 LGBM Stacker...')\nl2_lgbm_params = {\n    'objective': 'binary', 'metric': 'auc', 'n_estimators': 1000,\n    'learning_rate': 0.02, 'num_leaves': 7, 'max_depth': 3,\n    'subsample': 0.8, 'colsample_bytree': 0.7, 'random_state': SEED_LIST[0],\n    'n_jobs': -1, 'verbose': -1, 'device': 'gpu'\n}\n\n# We can use a simple KFold for the stacker\nkf_meta = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED_LIST[0]+1)\noof_preds_l2_lgbm = np.zeros(TRAIN_LEN)\ntest_preds_l2_lgbm = np.zeros(TEST_LEN)\n\nfor fold, (train_idx, val_idx) in enumerate(kf_meta.split(meta_train_ranked, train[TARGET])):\n    l2_model = LGBMClassifier(**l2_lgbm_params)\n    l2_model.fit(meta_train_ranked[train_idx], train[TARGET].iloc[train_idx],\n                 eval_set=[(meta_train_ranked[val_idx], train[TARGET].iloc[val_idx])],\n                 callbacks=[lgb.early_stopping(100, verbose=False)])\n    \n    oof_preds_l2_lgbm[val_idx] = l2_model.predict_proba(meta_train_ranked[val_idx])[:, 1]\n    test_preds_l2_lgbm += l2_model.predict_proba(meta_test_ranked)[:, 1] / N_SPLITS\n\ncv_l2_lgbm = roc_auc_score(train[TARGET], oof_preds_l2_lgbm)\nprint(f'L2 LGBM Stacker Final CV: {cv_l2_lgbm:.5f}')\n\n# --- Method 2: Optimized Weighted-Average (on Ranks) ---\nprint('\\nOptimizing blend weights (on Ranks)...')\n\ndef get_auc_ranked(weights):\n    w = weights / np.sum(weights)\n    # Order: [LGBM, CAT, XGB, MLP, LR]\n    weighted_oof_ranked = (w[0] * meta_train_ranked[:, 0] + # LGBM\n                           w[1] * meta_train_ranked[:, 1] + # CAT\n                           w[2] * meta_train_ranked[:, 2] + # XGB\n                           w[3] * meta_train_ranked[:, 3] + # MLP\n                           w[4] * meta_train_ranked[:, 4])  # LR\n    return -roc_auc_score(train[TARGET], weighted_oof_ranked)\n\ninitial_weights = [0.2, 0.2, 0.2, 0.2, 0.2]\nbounds = [(0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]\nopt_result = minimize(get_auc_ranked, initial_weights, method='Nelder-Mead', bounds=bounds)\n\noptimized_weights_ranked = opt_result.x / np.sum(opt_result.x)\ncv_optimized_ranked = -opt_result.fun \n\nprint(f\"Optimized Weights (Ranked): LGBM={optimized_weights_ranked[0]:.4f}, CAT={optimized_weights_ranked[1]:.4f}, XGB={optimized_weights_ranked[2]:.4f}, MLP={optimized_weights_ranked[3]:.4f}, LR={optimized_weights_ranked[4]:.4f}\")\nprint(f\"Optimized Rank Blend CV: {cv_optimized_ranked:.5f}\")","metadata":{"execution":{"iopub.execute_input":"2025-11-09T12:43:31.789482Z","iopub.status.busy":"2025-11-09T12:43:31.789257Z","iopub.status.idle":"2025-11-09T12:44:29.471966Z","shell.execute_reply":"2025-11-09T12:44:29.471269Z"},"papermill":{"duration":57.691429,"end_time":"2025-11-09T12:44:29.473306","exception":false,"start_time":"2025-11-09T12:43:31.781877","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"8f4b76ec","cell_type":"code","source":"# ============================================================================\n# FINAL ENSEMBLE & SAVE\n# ============================================================================\nprint('\\n' + '='*80)\nprint('FINAL ENSEMBLE (ON RANKS)')\nprint('='*80)\n\n# --- Method 3: Simple Rank Average ---\nsimple_avg_oof_ranked = np.mean(meta_train_ranked, axis=1)\ncv_simple_ranked = roc_auc_score(train[TARGET], simple_avg_oof_ranked)\n\n# Calculate test predictions for all 3 rank-based methods\ntest_preds_simple_ranked = np.mean(meta_test_ranked, axis=1)\n\ntest_preds_optimized_ranked = (optimized_weights_ranked[0] * meta_test_ranked[:, 0] + \n                               optimized_weights_ranked[1] * meta_test_ranked[:, 1] + \n                               optimized_weights_ranked[2] * meta_test_ranked[:, 2] +\n                               optimized_weights_ranked[3] * meta_test_ranked[:, 3] +\n                               optimized_weights_ranked[4] * meta_test_ranked[:, 4])\n            \n# Find the best method\nbest_blend_score = cv_simple_ranked\nbest_blend_test = test_preds_simple_ranked\nblend_name = 'Simple Rank Average'\n\nif cv_l2_lgbm > best_blend_score:\n    best_blend_score = cv_l2_lgbm\n    best_blend_test = test_preds_l2_lgbm\n    blend_name = 'L2 LGBM Stacker (on Ranks)'\n\nif cv_optimized_ranked > best_blend_score:\n    best_blend_score = cv_optimized_ranked\n    best_blend_test = test_preds_optimized_ranked\n    blend_name = 'Optimized Rank Average'\n\nprint(f\"\\n--- Base Models (Multi-Seed CV) ---\")\nprint(f'  LightGBM: {cv_lgb_final:.5f}')\nprint(f'  CatBoost: {cv_cat_final:.5f}')\nprint(f'  XGBoost: {cv_xgb_final:.5f}')\nprint(f'  MLPClassifier: {cv_mlp_final:.5f}')\nprint(f'  Logistic Regression: {cv_lr_final:.5f}')\nprint(f'\\n--- Ensemble Methods (on Ranks) ---')\nprint(f'  Simple Rank Average: {cv_simple_ranked:.5f}')\nprint(f'  Optimized Rank Average: {cv_optimized_ranked:.5f}')\nprint(f'  L2 LGBM Stacker: {cv_l2_lgbm:.5f}')\nprint(f'\\n--- Best Method ---')\nprint(f'  {blend_name}: {best_blend_score:.5f}')\n\n# ============================================================================\n# SAVE\n# ============================================================================\nsubmission['id'] = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')['id']\nsubmission[TARGET] = best_blend_test\nsubmission.to_csv('submission.csv', index=False)\n\noof_df = pd.DataFrame({'id': train['id'], TARGET: best_blend_score})\noof_df.to_csv(f'oof_predictions_{best_blend_score:.5f}.csv', index=False)\n\nprint(f'\\n{\"=\"*80}')\nprint('âœ… SAVED: submission.csv')\nprint(f'âœ… SAVED: oof_predictions_{best_blend_score:.5f}.csv')\nprint(f\"Runtime: {(time.time() - start_time)/60:.1f} minutes\")\nprint(f'\\n{\"=\"*80}\\nFINAL STABLE CV SCORE: {best_blend_score:.5f}')\nprint('='*80)","metadata":{"execution":{"iopub.execute_input":"2025-11-09T12:44:29.488651Z","iopub.status.busy":"2025-11-09T12:44:29.488419Z","iopub.status.idle":"2025-11-09T12:44:31.606999Z","shell.execute_reply":"2025-11-09T12:44:31.606127Z"},"papermill":{"duration":2.127739,"end_time":"2025-11-09T12:44:31.608204","exception":false,"start_time":"2025-11-09T12:44:29.480465","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"3610fb4b","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006924,"end_time":"2025-11-09T12:44:31.622654","exception":false,"start_time":"2025-11-09T12:44:31.61573","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}