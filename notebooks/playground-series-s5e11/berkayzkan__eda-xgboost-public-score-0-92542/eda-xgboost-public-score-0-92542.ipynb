{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loan Paid Back Prediction\n\nThis notebook contains a comprehensive process of analysis, feature engineering, and modeling to solve a loan repayment (binary classification) problem.\n\n**Primary Goal:** To predict the `loan_paid_back` probabilities for the `test.csv` data using models trained on `train.csv`.\n\n---\n\n## üìä Notebook Workflow\n\nThis work consists of several main sections:\n\n### 1. Data Loading and Preparation\n* The `train.csv`, `test.csv`, and `loan_dataset_20000.csv` (as `orig` for feature engineering) datasets are loaded.\n* The target variable (`TARGET`) and categorical features (`CATS`) are defined.\n* A validation set is split to monitor model performance.\n\n### 2. Exploratory Data Analysis (EDA)\n* **Numerical Features:** The **Pearson Correlation** with the target variable is examined.\n* **Categorical Features:** The relationship with the target variable is measured using the **Cramer's V** metric.\n* **Visualization:** The features with the strongest relationships are visualized using bar charts and a correlation heatmap.\n\n### 3. Feature Engineering\nThis is one of the most critical steps in this notebook. To prevent data leakage, two types of powerful features are derived using the `orig` (external) dataset:\n* **Target Encoding (`orig_mean_...`):** The average effect of each category on the target variable.\n* **Frequency Encoding (`orig_count_...`):** The frequency (or rarity) of each category in the dataset.\n\n### 4. Modeling: XGBoost (Base Model 1)\n* An **XGBoost** model is trained using all base (`BASE`) and derived (`ORIG`) features.\n* The model is set to handle categorical features directly (`enable_categorical=True`).\n* The training progress (Train vs Val AUC) and feature importances are visualized.\n* ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:25.151237Z","iopub.execute_input":"2025-11-02T20:51:25.152158Z","iopub.status.idle":"2025-11-02T20:51:25.428127Z","shell.execute_reply.started":"2025-11-02T20:51:25.152133Z","shell.execute_reply":"2025-11-02T20:51:25.427259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np\n\ntrain_df = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\nprint('Train Shape:', train_df.shape)\nprint('Test Shape:', test_df.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:25.429287Z","iopub.execute_input":"2025-11-02T20:51:25.429594Z","iopub.status.idle":"2025-11-02T20:51:27.304447Z","shell.execute_reply.started":"2025-11-02T20:51:25.429577Z","shell.execute_reply":"2025-11-02T20:51:27.303559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nTARGET = 'loan_paid_back'  #boolean\nCATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nBASE = [col for col in train_df.columns if col not in ['id', TARGET]]\n\n# √áalƒ±≈üma kopyalarƒ± olu≈ütur\ntrain = train_df.copy()\ntest = test_df.copy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:27.305312Z","iopub.execute_input":"2025-11-02T20:51:27.305601Z","iopub.status.idle":"2025-11-02T20:51:27.362061Z","shell.execute_reply.started":"2025-11-02T20:51:27.305575Z","shell.execute_reply":"2025-11-02T20:51:27.361235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ORIG = []\n\nfor col in BASE:\n    # MEAN\n    mean_map = orig.groupby(col)[TARGET].mean()\n    new_mean_col_name = f\"orig_mean_{col}\"\n    mean_map.name = new_mean_col_name\n    \n    train = train.merge(mean_map, on=col, how='left')\n    test = test.merge(mean_map, on=col, how='left')\n    ORIG.append(new_mean_col_name)\n\n    # COUNT\n    new_count_col_name = f\"orig_count_{col}\"\n    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n    \n    train = train.merge(count_map, on=col, how='left')\n    test = test.merge(count_map, on=col, how='left')\n    ORIG.append(new_count_col_name)\n\nprint(len(ORIG), 'Orig Features Created!!')\n\nFEATURES = BASE + ORIG\nprint(len(FEATURES), 'Total Features.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:27.363744Z","iopub.execute_input":"2025-11-02T20:51:27.364043Z","iopub.status.idle":"2025-11-02T20:51:31.372885Z","shell.execute_reply.started":"2025-11-02T20:51:27.364025Z","shell.execute_reply":"2025-11-02T20:51:31.372112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train[FEATURES], train[TARGET].astype(int), \n    test_size=0.2, random_state=42, stratify=train[TARGET]\n)\n\nprint(f'Train split (with ORIG features): {X_train.shape}')\nprint(f'Val split (with ORIG features): {X_val.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:31.37374Z","iopub.execute_input":"2025-11-02T20:51:31.374067Z","iopub.status.idle":"2025-11-02T20:51:31.782331Z","shell.execute_reply.started":"2025-11-02T20:51:31.374044Z","shell.execute_reply":"2025-11-02T20:51:31.781617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# numeric correlations\nnumeric_cols = train[BASE].select_dtypes(include=[np.number]).columns.tolist()\nnumeric_corr = pd.Series(dtype=float)\nif numeric_cols:\n    numeric_corr = train[numeric_cols].corrwith(train[TARGET].astype(int)).sort_values(ascending=False)\n\n# categorical associations (Cramer's V)\ncat_cols = [c for c in BASE if c not in numeric_cols]\n\n# try to use scipy for chi2_contingency; fallback to a safe zero if unavailable\ntry:\n    from scipy.stats import chi2_contingency\n    def cramers_v(x, y):\n        confusion = pd.crosstab(x, y)\n        if confusion.size == 0:\n            return 0.0\n        chi2 = chi2_contingency(confusion)[0]\n        n = confusion.sum().sum()\n        if n == 0:\n            return 0.0\n        phi2 = chi2 / n\n        r, k = confusion.shape\n        # bias correction\n        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n        rcorr = r - ((r-1)**2)/(n-1)\n        kcorr = k - ((k-1)**2)/(n-1)\n        denom = min((kcorr-1), (rcorr-1))\n        return (phi2corr / denom)**0.5 if denom > 0 else 0.0\nexcept Exception:\n    def cramers_v(x, y):\n        # fallback: compute association ratio by encoding categories to codes and using Pearson\n        x_codes = x.astype('category').cat.codes\n        return abs(x_codes.corr(y.astype(int)))\n\ncat_assoc = {}\nfor c in cat_cols:\n    col = train[c].fillna('NA')\n    cat_assoc[c] = cramers_v(col, train[TARGET].astype(int))\n\ncat_series = pd.Series(cat_assoc).sort_values(ascending=False)\n\n# Display summaries\nprint('Numeric features (Pearson correlation with target):')\nif not numeric_corr.empty:\n    print(numeric_corr.to_string())\nelse:\n    print('  No numeric features found in BASE.')\n\nprint('\\nTop categorical associations (Cramer\\'s V or fallback):')\nif not cat_series.empty:\n    print(cat_series.head(20).to_string())\nelse:\n    print('  No categorical features found in BASE.')\n\n# combined ranking by absolute strength\ncombined = pd.concat([\n    numeric_corr.abs().rename('strength'),\n    cat_series.abs().rename('strength')\n]).sort_values(ascending=False)\n\nprint('\\nTop features by absolute association strength with the target:')\nprint(combined.head(20).to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:31.782946Z","iopub.execute_input":"2025-11-02T20:51:31.783234Z","iopub.status.idle":"2025-11-02T20:51:32.515925Z","shell.execute_reply.started":"2025-11-02T20:51:31.783216Z","shell.execute_reply":"2025-11-02T20:51:32.515326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualization 1: Numeric correlations\nif not numeric_corr.empty:\n    fig, ax = plt.subplots(figsize=(10, max(4, len(numeric_corr) * 0.3)))\n    numeric_corr.plot(kind='barh', ax=ax, color=['green' if x > 0 else 'red' for x in numeric_corr])\n    ax.set_title('Pearson Correlations: Numeric Features vs Target', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Correlation Coefficient', fontsize=12)\n    ax.set_ylabel('Feature', fontsize=12)\n    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No numeric features to visualize.\")\n\n# Visualization 2: Categorical associations\nif not cat_series.empty:\n    fig, ax = plt.subplots(figsize=(10, max(4, len(cat_series) * 0.3)))\n    cat_series.plot(kind='barh', ax=ax, color='steelblue')\n    ax.set_title(\"Cramer's V: Categorical Features vs Target\", fontsize=14, fontweight='bold')\n    ax.set_xlabel(\"Cramer's V (Association Strength)\", fontsize=12)\n    ax.set_ylabel('Feature', fontsize=12)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No categorical features to visualize.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:32.516607Z","iopub.execute_input":"2025-11-02T20:51:32.516814Z","iopub.status.idle":"2025-11-02T20:51:33.321462Z","shell.execute_reply.started":"2025-11-02T20:51:32.516797Z","shell.execute_reply":"2025-11-02T20:51:33.320705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization 3: Combined importance \nif not combined.empty:\n    fig, ax = plt.subplots(figsize=(10, max(5, len(combined) * 0.25)))\n    top_n = min(20, len(combined))\n    top_combined = combined.head(top_n)\n    top_combined.plot(kind='barh', ax=ax, color='coral')\n    ax.set_title(f'Top {top_n} Features by Association Strength with Target', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Association Strength (Absolute Value)', fontsize=12)\n    ax.set_ylabel('Feature', fontsize=12)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No features to visualize.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:33.322318Z","iopub.execute_input":"2025-11-02T20:51:33.322797Z","iopub.status.idle":"2025-11-02T20:51:33.519644Z","shell.execute_reply.started":"2025-11-02T20:51:33.322773Z","shell.execute_reply":"2025-11-02T20:51:33.518893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntrain_encoded = train[BASE].copy()\n\nle_dict = {}\nfor col in cat_cols:\n    le = LabelEncoder()\n    train_encoded[col] = le.fit_transform(train[col].fillna('NA'))\n    le_dict[col] = le\n\ntrain_encoded[TARGET] = train[TARGET].astype(int)\n\ncorr_matrix = train_encoded.corr()\n\nplt.figure(figsize=(14, 12))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\nplt.title('Correlation matrix', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\ntarget_corr = corr_matrix[TARGET].sort_values(ascending=False)\nprint(f'\\n{TARGET} ile korelasyonlar (label encoded):')\nprint(target_corr.to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:33.520485Z","iopub.execute_input":"2025-11-02T20:51:33.520823Z","iopub.status.idle":"2025-11-02T20:51:34.944712Z","shell.execute_reply.started":"2025-11-02T20:51:33.520801Z","shell.execute_reply":"2025-11-02T20:51:34.943928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n\n# preprocess function\ndef preprocess_features(df, features, cat_cols, numeric_cols):\n    \"\"\"Prepare Categoric and Numerical Features\"\"\"\n    df_processed = df[features].copy()\n    \n    # Kategorik kolonlar\n    for col in cat_cols:\n        if col in df_processed.columns:\n            df_processed[col] = df_processed[col].fillna('NA').astype('category')\n    \n    # Numeric kolonlar\n    for col in numeric_cols:\n        if col in df_processed.columns:\n            df_processed[col] = df_processed[col].fillna(0)\n    \n    return df_processed\n\n# select numerical colons\nnumeric_cols_full = [col for col in FEATURES if col not in cat_cols]\n\nprint(f'Total features: {len(FEATURES)}')\nprint(f'  - BASE features: {len(BASE)}')\nprint(f'  - ORIG features: {len(ORIG)}')\nprint(f'Numeric features: {len(numeric_cols_full)}')\nprint(f'Categorical features: {len(cat_cols)}')\n\n\nX_train_full = preprocess_features(train, FEATURES, cat_cols, numeric_cols_full)\ny_train_full = train[TARGET].astype(int)\n\nX_test = preprocess_features(test, FEATURES, cat_cols, numeric_cols_full)\n\nX_train_split = preprocess_features(X_train, FEATURES, cat_cols, numeric_cols_full)\nX_val_split = preprocess_features(X_val, FEATURES, cat_cols, numeric_cols_full)\n\nprint(f'\\nTraining set: {X_train_full.shape}')\nprint(f'Test set: {X_test.shape}')\nprint(f'Train split: {X_train_split.shape}')\nprint(f'Val split: {X_val_split.shape}')\n\n# XGBoost model parameters\nxgb_params = {\n    'n_estimators': 10000,\n    'max_depth': 4,\n    'learning_rate': 0.010433357477511243,\n    'tree_method': 'hist',\n    'device': 'cuda',\n    'eval_metric': 'auc',\n    'objective': 'binary:logistic',\n    'random_state': 42,\n    'min_child_weight': 20,\n    'subsample': 0.8879829126651821,\n    'colsample_bytree': 0.5543148418738543,\n    'gamma': 0.6845363006652688,\n    'reg_alpha': 0.2399421158144976,\n    'reg_lambda': 0.28254661049782354,\n    'enable_categorical': True,\n    'early_stopping_rounds': 100,\n}\n\n# Train XGBoost model\nprint('\\nXGBoost training contuinue...')\nmodel = XGBClassifier(**xgb_params)\n\nmodel.fit(\n    X_train_full, y_train_full,\n    eval_set=[(X_train_split, y_train), (X_val_split, y_val)],\n    verbose=1000\n)\n\nprint('Model training completed!')\n\n# Predict on test set\npred = model.predict_proba(X_test)[:, 1]\n\n# Prepare submission\nsubmission = pd.DataFrame({\n    \"id\": test[\"id\"],\n    TARGET: pred\n})\n\n\n# Save submission file\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(f'\\nSubmission file saved!: submission.csv')\nprint(f'Submission shape: {submission.shape}')\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:51:34.946446Z","iopub.execute_input":"2025-11-02T20:51:34.94666Z","iopub.status.idle":"2025-11-02T20:53:19.052869Z","shell.execute_reply.started":"2025-11-02T20:51:34.946644Z","shell.execute_reply":"2025-11-02T20:53:19.052307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# XGBoost feature importance ve loss visualization\nimport matplotlib.pyplot as plt\n\n# Feature importance \nfeature_importance = pd.DataFrame({\n    'feature': X_train_full.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint('Feature Importance:')\nprint(feature_importance.head(20).to_string())\n\n# 2 subplot: feature importance + loss curves\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n# 1. Feature importance plot (colorful + logarithmic)\ntop_features = feature_importance.head(20)\n\n\nimport matplotlib.cm as cm\ncolors = cm.viridis(np.linspace(0, 1, len(top_features)))\n\n# Horizontal bar plot\nbars = axes[0].barh(range(len(top_features)), top_features['importance'], color=colors, edgecolor='black', linewidth=0.5)\naxes[0].set_yticks(range(len(top_features)))\naxes[0].set_yticklabels(top_features['feature'])\naxes[0].set_xlabel('Importance (log scale)', fontsize=12)\naxes[0].set_ylabel('Feature', fontsize=12)\naxes[0].set_title('Top Feature Importances (XGBoost) on log scale', fontsize=14, fontweight='bold')\naxes[0].set_xscale('log')  # Logaritmik √∂l√ßek\naxes[0].invert_yaxis()\naxes[0].grid(axis='x', alpha=0.3, which='both')\n\nfor i, (bar, val) in enumerate(zip(bars, top_features['importance'])):\n    axes[0].text(val, bar.get_y() + bar.get_height()/2, \n                f'{val:.4f}', \n                ha='left', va='center', fontsize=9, fontweight='bold')\n\n# Train & Validation Loss curves\nresults = model.evals_result()\ntrain_loss = results['validation_0']['auc']\nval_loss = results['validation_1']['auc']\nepochs = range(len(train_loss))\n\naxes[1].plot(epochs, train_loss, 'b-', label='Train AUC', linewidth=2, alpha=0.8)\naxes[1].plot(epochs, val_loss, 'r-', label='Validation AUC', linewidth=2, alpha=0.8)\naxes[1].set_xlabel('Iteration', fontsize=12)\naxes[1].set_ylabel('AUC Score', fontsize=12)\naxes[1].set_title('XGBoost Training Progress: Train vs Val AUC', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f'\\nFinal Train AUC: {train_loss[-1]:.4f}')\nprint(f'Final Validation AUC: {val_loss[-1]:.4f}')\nprint(f'Total iterations: {len(train_loss)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:53:19.053603Z","iopub.execute_input":"2025-11-02T20:53:19.053788Z","iopub.status.idle":"2025-11-02T20:53:19.846594Z","shell.execute_reply.started":"2025-11-02T20:53:19.053774Z","shell.execute_reply":"2025-11-02T20:53:19.845916Z"}},"outputs":[],"execution_count":null}]}