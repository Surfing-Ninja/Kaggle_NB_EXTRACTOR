{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**A precision-tuned XGBoost workflow focused on stability, feature logic, and reliable CV performance.**\n\nThis notebook builds a highly optimized XGBoost model for the Kaggle Playground Series S5E11 competition. It begins by loading the training and test data and performing a small domain-specific transformation on the grade_subgrade feature to extract the numerical subgrade. The code identifies all base features aside from the target and ID, and divides them into categorical and numeric groups. It then creates several forms of feature engineering: leakage-safe out-of-fold target mean encoding, frequency encoding for all columns, and quantile-based bin features for numeric columns. These engineered features are concatenated with the raw features, and some previously-tested columns are optionally removed. All remaining categorical features are cast to pandas categorical type so XGBoost can use its built-in categorical handling.\n\nNext, the script aligns feature columns between train and test to ensure consistency. It then uses xgb.cv to estimate an optimal number of boosting rounds with early stopping, using one representative set of jittered hyperparameters. After this first estimate, the code performs a small “micro-sweep” around the chosen number of estimators (best, +10, +20) to find the value that maximizes out-of-fold AUC with a strong parameter configuration. The best number of estimators identified in this sweep becomes the final n_estimators for all models in the ensemble.\n\nThe notebook then trains a small internal ensemble of XGBoost models using several different seeds and parameter jitters. Each model is trained on the full training data and predicts probabilities on the test data. To tune the final ensemble blending method, the script also builds out-of-fold predictions for each ensemble member using Stratified K-Fold. From these OOF predictions it constructs two meta-signals: the average probability of all ensemble predictions and the average rank of those predictions. The script then searches for a blending weight β (between 0.20 and 0.40) that maximizes OOF AUC when mixing probabilities and ranks. This produces a more stable and robust final prediction method.\n\nFinally, the tuned blend is applied to the ensemble’s predictions on the test set, and the resulting probabilities are clipped into a valid range and written into submission.csv. The script concludes by printing the reference CV AUC from xgb.cv, the best result from the micro-sweep, and the final blended OOF AUC used to drive the ensemble selection.","metadata":{}},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Imports**\n\nThis section loads all required libraries for the modeling pipeline. It brings in core Python tools for file handling, randomness control, system operations, and memory management. NumPy and pandas are included to handle numerical processing and tabular data. XGBoost and its classifier interface provide the gradient-boosted tree model used throughout the workflow, including native support for categorical features. From scikit-learn, it imports utilities for cross-validation and the AUC metric used for evaluation. Warning messages are then disabled to keep the notebook output focused and uncluttered.","metadata":{}},{"cell_type":"code","source":"# Imports\nimport os, gc, warnings, math, random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Config**\n\nThis section defines the paths to the training and test files within the Kaggle input directory. It sets the names of the ID column and the target column used in the prediction task. It also specifies the number of folds for cross-validation and establishes a fixed random seed to ensure reproducible results.","metadata":{}},{"cell_type":"code","source":"# Config\nDATA_DIR = Path(\"/kaggle/input/playground-series-s5e11\")\nTRAIN_PATH = DATA_DIR / \"train.csv\"\nTEST_PATH  = DATA_DIR / \"test.csv\"\n\nID_COL = \"id\"\nTARGET = \"loan_paid_back\"\n\nN_FOLDS = 7\nSEED = 42","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Boosting budget & early stopping used by xgb.cv to find a good n_estimators**\n\nThis section sets the upper limit for how many boosting rounds XGBoost is allowed to explore during cross-validation. It also defines the early stopping threshold, which halts training if performance does not improve for a specified number of rounds, helping identify an efficient and well-performing number of estimators.","metadata":{}},{"cell_type":"code","source":"# Boosting budget & early stopping used by xgb.cv to find a good n_estimators\nNUM_BOOST_ROUND = 20000\nEARLY_STOP = 50","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Small internal ensemble: same model with slight param jitters and different seeds**\n\nThis section specifies a set of random seeds and small parameter variations used to train several versions of the same XGBoost model under slightly different conditions. The seeds provide controlled randomness, while the jittered parameters introduce subtle changes in tree structure and regularization strength. These variations allow the model to be trained from multiple stable perspectives, improving robustness without relying on external blending or combining unrelated models.","metadata":{}},{"cell_type":"code","source":"# Small internal ensemble: same model with slight param jitters and different seeds\nENSEMBLE_SEEDS = [42, 7, 19, 77, 123]\nJITTERS = [\n    dict(max_leaves=4,  min_child_weight=89, reg_alpha=1.4, reg_lambda=5.9),\n    dict(max_leaves=4,  min_child_weight=82, reg_alpha=1.1, reg_lambda=6.3),\n    dict(max_leaves=5,  min_child_weight=95, reg_alpha=1.6, reg_lambda=5.6),\n    dict(max_leaves=5,  min_child_weight=88, reg_alpha=1.3, reg_lambda=6.1),\n    dict(max_leaves=4,  min_child_weight=92, reg_alpha=1.2, reg_lambda=6.0),\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Base XGBoost params (hist + categorical); regularization/leaves are provided via jitters**\n\nThis section defines the baseline XGBoost configuration used throughout the workflow. It enables the efficient histogram tree method and lets the library take advantage of the GPU when possible. The model is set for binary classification, uses AUC as its evaluation metric, and outputs calibrated logistic probabilities. All row and feature sampling parameters are set to use the full dataset, eliminating randomness from subsampling and keeping the training behavior consistent. Structural and regularization parameters are intentionally omitted here, because they are later supplied through small, controlled parameter variations designed to improve stability.","metadata":{}},{"cell_type":"code","source":"# Base XGBoost params (hist + categorical); regularization/leaves are provided via jitters\nBASE_PARAMS = {\n    'tree_method': 'hist',\n    'device': 'cuda',            # falls back if no GPU; keep 'auto' predictor to let XGB decide\n    'predictor': 'auto',\n    'eval_metric': 'auc',\n    'objective': 'binary:logistic',\n    'subsample': 1.0,\n    'colsample_bytree': 1.0,\n    'colsample_bylevel': 1.0,\n    'colsample_bynode': 1.0,\n    'gamma': 0.0,\n    'scale_pos_weight': 1.0,\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Reproducible randomness for Python, NumPy, and hashing**\n\nThis section ensures that every run of the notebook behaves the same way. It locks Python’s hashing, aligns the standard random generator, and sets NumPy’s seed to a fixed value.\n\nBy synchronizing these sources of randomness, the pipeline produces identical data splits, parameter choices, and model outputs across repeated executions.","metadata":{}},{"cell_type":"code","source":"# Reproducible randomness for Python, NumPy, and hashing\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Helpers**\n\nThis helper module provides the building blocks for a clean, leakage-safe training flow. It starts with read_data, which simply loads the train and test files. The target_encoding routine then creates strictly out-of-fold target means using stratified folds so that each training row receives a target mean computed without seeing itself, while the test set gets a single global mapping, preventing leakage by design. Complementing that, create_frequency_and_bins adds lightweight signal: frequency encodings for every column and quantile bins at multiple resolutions for numeric features, with safeguards for constant or tricky distributions.\n\nTo make XGBoost’s native categorical handling work correctly, enable_categoricals casts chosen columns to pandas’ categorical dtype. For model length selection, do_cv_nround runs xgb.cv with early stopping and returns the boosting round at which the cross-validated AUC peaks, giving a data-driven estimate of n_estimators. Finally, oof_auc_for_n performs a focused, stratified out-of-fold evaluation for a specific number of estimators and parameter set, enabling a small, reliable micro-sweep around the cross-validated peak to lock in a stable training budget.","metadata":{}},{"cell_type":"code","source":"# Helpers\ndef read_data():\n    \"\"\"Load train/test CSVs from Kaggle input path.\"\"\"\n    train = pd.read_csv(TRAIN_PATH)\n    test  = pd.read_csv(TEST_PATH)\n    return train, test\n\ndef target_encoding(train, test, cols, target_col, n_splits=10, seed=42):\n    \"\"\"\n    Out-of-fold target mean encoding for leakage-safe training.\n    - Uses StratifiedKFold for stable class balance per fold.\n    - Applies global mapping to test.\n    \"\"\"\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    te_train = pd.DataFrame(index=train.index)\n    te_test  = pd.DataFrame(index=test.index)\n\n    y = train[target_col].values\n    for col in cols:\n        oof_vals = np.zeros(len(train), dtype=float)\n        for tr_idx, va_idx in kf.split(train, y):\n            tr_fold = train.iloc[tr_idx]\n            va_fold = train.iloc[va_idx]\n            mean_map = tr_fold.groupby(col)[target_col].mean()\n            oof_vals[va_idx] = va_fold[col].map(mean_map).astype(float)\n        te_train[f\"mean_{col}\"] = oof_vals\n\n        # Global mapping for test rows (no leakage)\n        global_map = train.groupby(col)[target_col].mean()\n        te_test[f\"mean_{col}\"] = test[col].map(global_map).astype(float)\n\n    return te_train, te_test\n\ndef create_frequency_and_bins(train, test, cols, num_cols):\n    \"\"\"\n    Lightweight encodings:\n    - Frequency encoding for all columns.\n    - Quantile bins (5/10/15) for numeric columns to add coarse order information.\n    \"\"\"\n    tr_new = pd.DataFrame(index=train.index)\n    te_new = pd.DataFrame(index=test.index)\n\n    for col in cols:\n        # Frequency (fallback to mean frequency for unseen test values)\n        freq = train[col].value_counts()\n        tr_new[f\"{col}_freq\"] = train[col].map(freq).astype(float)\n        te_new[f\"{col}_freq\"] = test[col].map(freq).astype(float).fillna(freq.mean())\n\n        # Quantile bins only for numeric columns; protect against constant columns\n        if col in num_cols:\n            for q in [5, 10, 15]:\n                try:\n                    tr_bins, bins = pd.qcut(train[col], q=q, labels=False, retbins=True, duplicates=\"drop\")\n                    tr_new[f\"{col}_bin{q}\"] = tr_bins.astype(float)\n                    te_new[f\"{col}_bin{q}\"] = pd.cut(test[col], bins=bins, labels=False, include_lowest=True).astype(float)\n                except Exception:\n                    tr_new[f\"{col}_bin{q}\"] = 0.0\n                    te_new[f\"{col}_bin{q}\"] = 0.0\n    return tr_new, te_new\n\ndef enable_categoricals(df, cat_cols):\n    \"\"\"Cast listed columns to pandas 'category' so XGBoost can use enable_categorical=True.\"\"\"\n    for c in cat_cols:\n        if df[c].dtype.name != \"category\":\n            df[c] = df[c].astype(\"category\")\n    return df\n\ndef do_cv_nround(train_df, features, target, base_params):\n    \"\"\"\n    Estimate a good number of boosting rounds via xgb.cv with early stopping.\n    Returns (best_round, best_auc) based on test-auc-mean peak.\n    \"\"\"\n    dtrain = xgb.DMatrix(train_df[features], label=train_df[target], enable_categorical=True)\n    cv = xgb.cv(\n        params=base_params,\n        dtrain=dtrain,\n        nfold=N_FOLDS,\n        num_boost_round=NUM_BOOST_ROUND,\n        metrics='auc',\n        verbose_eval=False,\n        early_stopping_rounds=EARLY_STOP,\n        seed=SEED,\n        shuffle=True,\n        stratified=True,\n    )\n    best_round = int(cv['test-auc-mean'].idxmax())\n    best_auc   = float(cv['test-auc-mean'][best_round])\n    print(f\"[CV] Best round: {best_round} | Best CV AUC: {best_auc:.7f}\")\n    return best_round, best_auc\n\ndef oof_auc_for_n(X, y, n_estimators, params):\n    \"\"\"\n    Compute OOF AUC for a given n_estimators and params using StratifiedKFold.\n    This is a small micro-sweep to refine around the cv-chosen round.\n    \"\"\"\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof = np.zeros(len(X), dtype=\"float32\")\n    for tr_idx, va_idx in skf.split(X, y):\n        X_tr_f, y_tr_f = X.iloc[tr_idx], y[tr_idx]\n        X_va_f, y_va_f = X.iloc[va_idx], y[va_idx]\n        m = XGBClassifier(**params, n_estimators=n_estimators, enable_categorical=True)\n        m.fit(X_tr_f, y_tr_f)\n        oof[va_idx] = m.predict_proba(X_va_f)[:, 1].astype(\"float32\")\n        del m\n        gc.collect()\n    return roc_auc_score(y, oof)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font color = \"DeepSkyBlue\">**Main**\n\nThe main function begins by loading the training and test data, displaying their shapes, and extracting a useful numeric feature from the grade_subgrade field. It then prepares the full feature set by identifying categorical and numeric columns, generating leakage-safe target encodings, and adding frequency and quantile-bin based transformations. After combining these engineered features with the original ones, it removes a small group of columns that previous experimentation showed to be less beneficial, ensures proper categorical typing for XGBoost, and synchronizes the train and test matrices so that both share the same structure.\n\nOnce the data is ready, the function determines an effective number of boosting rounds by running cross-validation with a representative parameter configuration. It refines this estimate through a short sweep around the cross-validated peak, selecting the value that yields the strongest out-of-fold performance. Using this finalized boosting length, the method trains several stability-oriented model runs that differ only in seed and slight parameter variations, collecting both their test-time predictions and their out-of-fold predictions on the training data.\n\nFrom these training-side predictions, the function creates two complementary signals: the average predicted probability and the average rank of the predictions. It then evaluates a small set of mixing weights to identify the most reliable combination of these signals in terms of AUC. Finally, it applies the chosen weight to the test-side predictions, clips them into a valid probability range, saves them as submission.csv, and prints key reference metrics drawn from cross-validation, the sweep, and the final combined output.","metadata":{}},{"cell_type":"code","source":"# Main\ndef main():\n    train, test = read_data()\n    print(f\"train: {train.shape} | test: {test.shape}\")\n\n    # Minimal domain feature: extract numeric subgrade from 'grade_subgrade' (e.g., 'A7' -> 7)\n    train['subgrade'] = train['grade_subgrade'].str[1:].astype(int)\n    test['subgrade']  = test['grade_subgrade'].str[1:].astype(int)\n\n    # Build feature list excluding ID and target\n    base_cols = train.drop(columns=[TARGET, ID_COL]).columns.tolist()\n\n    # Split into categorical vs numeric for later encodings\n    cat_cols = [c for c in base_cols if train[c].dtype in [\"object\", \"category\"]]\n    num_cols = [c for c in base_cols if c not in cat_cols]\n\n    # Leakage-safe target encoding on all base columns (categorical & numeric)\n    te_tr, te_te = target_encoding(train, test, base_cols, TARGET, n_splits=10, seed=SEED)\n\n    # Frequency + quantile-bin encodings\n    fq_tr, fq_te = create_frequency_and_bins(train, test, base_cols, num_cols)\n\n    # Concatenate original features with encodings\n    X_tr = pd.concat([train[base_cols], te_tr, fq_tr], axis=1)\n    X_te = pd.concat([test[base_cols],  te_te, fq_te], axis=1)\n\n    # Optional feature drops kept from prior experimentation\n    drops = [\n        \"education_level\",\"loan_purpose\",\"grade_subgrade\",\"interest_rate\",\"marital_status\",\n        \"employment_status_freq\", \"credit_score_bin5\", \"loan_amount_bin5\", \"debt_to_income_ratio_bin5\"\n    ]\n    drops = [d for d in drops if d in X_tr.columns]\n    X_tr = X_tr.drop(columns=drops, errors=\"ignore\")\n    X_te = X_te.drop(columns=drops, errors=\"ignore\")\n\n    # Ensure XGBoost categorical support by casting objects/categories\n    cat_all = [c for c in X_tr.columns if X_tr[c].dtype in [\"object\",\"category\"]]\n    X_tr = enable_categoricals(X_tr, cat_all)\n    X_te = enable_categoricals(X_te, cat_all)\n\n    # Align columns between train and test for safety\n    common_cols = [c for c in X_tr.columns if c in X_te.columns]\n    X_tr = X_tr[common_cols]\n    X_te = X_te[common_cols]\n\n    print(f\"Final feature count: {X_tr.shape[1]}\")\n\n    # Find a good n_estimators via xgb.cv using a representative jitter probe\n    base_for_cv = BASE_PARAMS.copy()\n    probe = dict(max_leaves=4, min_child_weight=89, reg_alpha=1.4, reg_lambda=5.9)\n    base_for_cv.update(probe)\n    best_round, best_auc = do_cv_nround(pd.concat([X_tr, train[[TARGET]]], axis=1), common_cols, TARGET, base_for_cv)\n\n    # Micro-sweep around the cv peak to lock n_estimators (best, +10, +20)\n    y = train[TARGET].values\n    strong = BASE_PARAMS.copy()\n    strong.update(probe)\n    strong[\"random_state\"] = SEED\n\n    candidates = [best_round, best_round + 10, best_round + 20]\n    best_n, best_n_auc = None, -1.0\n    print(f\"[n-sweep] candidates: {candidates}\")\n    for n in candidates:\n        auc_n = oof_auc_for_n(X_tr[common_cols], y, n_estimators=n, params=strong)\n        print(f\"  n_estimators={n} -> OOF AUC={auc_n:.6f}\")\n        if auc_n > best_n_auc:\n            best_n_auc = auc_n\n            best_n = n\n\n    n_estimators = int(best_n)\n    print(f\"[n-sweep] chosen n_estimators={n_estimators} | OOF AUC={best_n_auc:.6f}\")\n\n    # Train the internal ensemble (same n_estimators, jittered params, different seeds)\n    preds = []\n    for seed, jitter in zip(ENSEMBLE_SEEDS, JITTERS):\n        params = BASE_PARAMS.copy()\n        params.update(jitter)\n        params[\"random_state\"] = seed\n\n        model = XGBClassifier(\n            **params,\n            n_estimators=n_estimators,\n            enable_categorical=True\n        )\n        model.fit(X_tr, train[TARGET])\n        pred = model.predict_proba(X_te)[:, 1].astype(\"float32\")\n        preds.append(pred)\n        del model\n        gc.collect()\n\n    # Build OOF stack for the same ensemble to tune a simple convex blend (prob vs rank)\n    print(\"[OOF] Building internal OOF for blend beta …\")\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof_stack = np.zeros((len(train), len(preds)), dtype=\"float32\")\n\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tr, y), 1):\n        X_tr_f, y_tr_f = X_tr.iloc[tr_idx], y[tr_idx]\n        X_va_f, y_va_f = X_tr.iloc[va_idx], y[va_idx]\n        for m_idx, jitter in enumerate(JITTERS):\n            params = BASE_PARAMS.copy()\n            params.update(jitter)\n            params[\"random_state\"] = ENSEMBLE_SEEDS[m_idx]\n\n            m = XGBClassifier(\n                **params,\n                n_estimators=n_estimators,\n                enable_categorical=True\n            )\n            m.fit(X_tr_f, y_tr_f)\n            oof_stack[va_idx, m_idx] = m.predict_proba(X_va_f)[:, 1].astype(\"float32\")\n            del m\n        auc_fold = roc_auc_score(y_va_f, oof_stack[va_idx].mean(axis=1))\n        print(f\"  Fold {fold}: mean-prob AUC = {auc_fold:.6f}\")\n        gc.collect()\n\n    # Tune beta for convex combination of mean probabilities and mean ranks (robustness)\n    prob_oof = oof_stack.mean(axis=1)\n    ranks = np.column_stack([pd.Series(oof_stack[:, i]).rank(method=\"average\").values for i in range(oof_stack.shape[1])])\n    rank_oof = (ranks.mean(axis=1) - ranks.min()) / (ranks.max() - ranks.min() + 1e-12)\n\n    beta_grid = [0.20, 0.25, 0.30, 0.35, 0.40]\n    best_beta, best_beta_auc = 0.25, -1.0\n    for b in beta_grid:\n        mix = (1-b)*prob_oof + b*rank_oof\n        auc = roc_auc_score(y, mix)\n        if auc > best_beta_auc:\n            best_beta_auc = auc\n            best_beta = b\n    print(f\"[Blend] Best beta={best_beta} | OOF AUC={best_beta_auc:.6f}\")\n\n    # Apply tuned beta to test predictions and write submission\n    prob_test = np.mean(preds, axis=0)\n    ranks_te = np.column_stack([pd.Series(preds[i]).rank(method=\"average\").values for i in range(len(preds))])\n    rank_test = (ranks_te.mean(axis=1) - ranks_te.min()) / (ranks_te.max() - ranks_te.min() + 1e-12)\n    final_pred = (1-best_beta)*prob_test + best_beta*rank_test\n    final_pred = np.clip(final_pred, 0.0, 1.0).astype(\"float32\")\n\n    sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET: final_pred})\n    sub.to_csv(\"submission.csv\", index=False)\n    print(\"\\n[DONE] Wrote submission.csv\")\n    print(f\"CV ref AUC (probe): {best_auc:.6f} | OOF AUC (n-sweep best): {best_n_auc:.6f} | Blend OOF AUC: {best_beta_auc:.6f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}