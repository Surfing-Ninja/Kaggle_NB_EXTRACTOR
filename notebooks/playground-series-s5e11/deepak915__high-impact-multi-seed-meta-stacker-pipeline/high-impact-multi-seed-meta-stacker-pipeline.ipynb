{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold # Best for classification/imbalance\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning) \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:20:55.201679Z","iopub.execute_input":"2025-11-10T09:20:55.202057Z","iopub.status.idle":"2025-11-10T09:21:00.208792Z","shell.execute_reply.started":"2025-11-10T09:20:55.202008Z","shell.execute_reply":"2025-11-10T09:21:00.207698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìñ Welcome: What This Notebook Delivers\nThis notebook is a comprehensive, best-practices guide for solving the classic problem of loan default prediction using tabular machine learning.\nWhether you're a Kaggle competitor, a business data scientist, or just diving into robust ML workflows, this notebook offers a clear, real-world-tested pipeline from messy, raw input to leaderboard-ready predictions.\n\n### What You Will Learn and See Here\n\nA Business-First Problem Framing:\nUnderstand how ML can help banks and lenders minimize risk and make faster, fairer credit decisions.\n\n#### Modern, Modular Pipeline Structure:\nThe entire workflow is broken into clear, auditable steps:\n\n#### Data cleaning & deduplication: Honest evaluation begins with honest data.\n\n#### High-value feature engineering: Focused on real predictive power‚Äîno kitchen-sink noise or bloated transformations.\n\n#### Advanced, multi-criteria feature selection: Blend LightGBM, ExtraTrees, and Mutual Information for robust, generalizable predictors.\n\n#### Minimalist model stacking: Only the most powerful Level 0 models (LGBM, XGBoost, CatBoost), each run with several random seeds and cross-validation for stability.\n\n#### Practical meta-modeling: Simple, interpretable Level 1 models (logistic, ridge, LGBM) chosen with validation for stacking‚Äîexactly as top competitors do.\n\n#### Rigorous Validation at Every Stage:\nOOF (out-of-fold) predictions, multi-seeded evaluation, and clear AUC-ROC diagnostics‚Äîno hidden data leaks or overfitting.\n\n#### Rich Documentation:\nEach block and decision is explained: why it‚Äôs done, what benefit it brings, and how it fits into the broader pipeline.\n\n#### Business Relevance & Reproducibility:\nEvery method connects back to why it matters for the end business goal: reduce credit risk, improve customer experience, and bring ML from notebook to real impact.\n\n#### Final Outputs:\nStacked, honest probability predictions for test data, easily exportable for submission or deployment.","metadata":{}},{"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ndf_test=pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\ndf_train.drop('id',axis=1,inplace=True)\ndf_test.drop('id',axis=1,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:21:00.210134Z","iopub.execute_input":"2025-11-10T09:21:00.210601Z","iopub.status.idle":"2025-11-10T09:21:02.313768Z","shell.execute_reply.started":"2025-11-10T09:21:00.210578Z","shell.execute_reply":"2025-11-10T09:21:02.312768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Understanding the Data\n### Our raw data has 11 columns:\n1. annual_income         # How much they earn per year\n2. debt_to_income_ratio  # Their debt divided by income (%)\n3. credit_score          # Credit history score (300-850)\n4. loan_amount          # How much they want to borrow ($)\n5. interest_rate        # The interest rate offered (%)\n6. gender               # Male/Female\n7. marital_status       # Single/Married/Divorced\n8. education_level      # High School/Bachelor's/Master's/PhD\n9. employment_status    # Employed/Self-employed/Unemployed\n10. loan_purpose        # Why they need the loan\n11. grade_subgrade      # Bank's internal risk grade\n12. loan_paid_back      # TARGET: Did they pay back? (1=Yes, 0=No)\n","metadata":{}},{"cell_type":"markdown","source":"## Understanding spread and distribution of data","metadata":{}},{"cell_type":"code","source":"numerical_cols = ['annual_income','debt_to_income_ratio','credit_score','loan_amount','interest_rate']\n\n# Determine grid size for subplots\nnum_cols = len(numerical_cols)\nnum_rows = (num_cols + 1) // 2  # Adjust as needed for layout\n\nplt.figure(figsize=(12, 4 * num_rows)) # Adjust figure size\n\nfor i, col in enumerate(numerical_cols):\n    plt.subplot(num_rows, 2, i + 1) # 2 columns per row\n    sns.histplot(df_train[col], kde=True) # Example: histogram\n    plt.title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:21:02.323104Z","iopub.execute_input":"2025-11-10T09:21:02.323422Z","iopub.status.idle":"2025-11-10T09:21:03.882045Z","shell.execute_reply.started":"2025-11-10T09:21:02.323398Z","shell.execute_reply":"2025-11-10T09:21:03.88099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols = ['gender','marital_status','education_level','employment_status','loan_purpose','grade_subgrade' ]\n\ncat_cols_count = len(categorical_cols)\ncat_rows = (cat_cols_count + 1) // 2  # Adjust as needed for layout, e.g., 3 rows for 6 plots\n\n# Create figure and a set of subplots\nfig, axes = plt.subplots(nrows=cat_rows, ncols=2, figsize=(15, 4 * cat_rows))\n\n# Flatten the axes array for easier iteration\naxes = axes.flatten()\n\nfor i, col in enumerate(categorical_cols):\n    # Use axes-level function (sns.countplot) and specify the 'ax'\n    sns.countplot(data=df_train, x=col, ax=axes[i])\n    axes[i].set_title(f'Distribution of {col}')\n    axes[i].set_xlabel(col)\n    axes[i].tick_params(axis='x', rotation=45) # Rotate labels if they overlap\n\n# Hide any unused subplots if the total count is odd\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:21:03.882871Z","iopub.execute_input":"2025-11-10T09:21:03.883193Z","iopub.status.idle":"2025-11-10T09:21:05.187267Z","shell.execute_reply.started":"2025-11-10T09:21:03.883167Z","shell.execute_reply":"2025-11-10T09:21:05.186275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üèóÔ∏è Step 0: Feature Engineering ‚Äî The Foundation of Winning Models\nWhy Feature Engineering Matters\n\nFeature engineering transforms raw tabular data into explicit signals for ML models.\nIt's not just about adding more features, but making each one meaningful and predictive:\n\nExample:\nRaw: Person A earns $100K, borrows $30K\nPerson B earns $30K, borrows $30K\nEngineered: Person A‚Äôs ratio = 0.3 (safer)\nPerson B‚Äôs ratio = 1.0 (risky)\n\nCareful engineering exposes patterns the raw data hides.\n\n#### Step 1: Deduplication üîÑ\n\nRemoves duplicate rows from both train and test sets for honest evaluation. Models can't \"memorize\" repeated records, ensuring fair validation.\n\n#### Step 2: Combine for Uniform Processing üîó\n\nTrain and test are temporarily combined so encodings, transforms, and scalers remain identical. This ensures consistent numeric and categorical representations across all data.\n\n#### Step 3: Minimal Label Encoding of Categoricals üî§\n\nAll categorical (object) columns are label-encoded. Keeps numeric codes aligned between train/test, prevents model confusion, and reduces the risk of unseen categories during scoring.\n\n#### Step 4: Smart Feature Selection üåü\n\nOnly keep a couple of low-cardinality, interpretable categorical variables.\nRank all numeric features by predictive power (mutual information).\nLimit the feature space to just 5‚Äì6 most informative numerics, discarding the rest.\n\n#### Step 5: Clean Numeric Interactions ‚ûï‚ûó‚úñÔ∏è\n\nFor the top numerics, create key interactions (sum, product, ratio).\nAvoid unnecessary or noisy feature combinations.\nEach interaction reflects a real-world relationship (e.g., debt-to-income ratio).\n\n#### Step 6: Mathematical Transformations & Binning üßÆ\n\nApply quantile binning, log1p, and sqrt to top numerics.\nThese steps reduce skew, highlight nonlinear thresholds, and improve generalization.\n\n#### Step 7: Split Back to Train/Test üìã\n\nSafety first: after all shared transformations, restore strict train/test boundaries, avoiding leakage.\n\n#### Step 8: Optional Standard Scaling üìè\n\nScale core numerics for compatibility with linear/meta models.\nTrees are robust to scaling, but stackers and validation benefit.\n\n#### What‚Äôs NOT Included\n‚ùå No groupby aggregate features (models and feature selectors already find global/local patterns).\n\n‚ùå No hundreds of random interactions.\n\n‚ùå No variance threshold filtering‚Äîyou have already removed constants and highly correlated features with smarter selectors.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_selection import mutual_info_classif\n\nTARGET = 'loan_paid_back'\n\n# 1. Deduplication and combine for uniform FE\nprint(\"üîÅ Removing duplicates in train/test...\")\ndf_train = df_train.drop_duplicates().reset_index(drop=True)\ndf_test = df_test.drop_duplicates().reset_index(drop=True)\n\nprint(\"üîÑ Combining train and test for consistent processing...\")\ndf_train['is_train'] = 1\ndf_test['is_train'] = 0\ndf_all = pd.concat([df_train, df_test], axis=0, ignore_index=True)\nprint(f\"‚Üí Combined shape: {df_all.shape}\")\n\n# 2. Label Encoding (all categoricals, keep mapping for interpretability if needed)\ncat_cols = df_all.select_dtypes(include=['object', 'category']).columns.tolist()\nfor col in cat_cols:\n    df_all[col] = LabelEncoder().fit_transform(df_all[col].astype(str))\nprint(f\"‚úì Encoded {len(cat_cols)} categoricals\")\n\n# 3. Select only a few simple key categoricals (not high-card!), and top numerics by MI\ncat_counts = {c: df_all[c].nunique() for c in cat_cols}\nmain_cats = [c for c, u in cat_counts.items() if 2 <= u <= 8][:2]    # use 2 lowest-card simple cats only (tune count as needed)\nprint(f\"‚òÜ Main categoricals: {main_cats}\")\n\nnum_cols = [c for c in df_all.columns if c not in cat_cols + [TARGET, 'is_train'] and np.issubdtype(df_all[c].dtype, np.number)]\nmi = mutual_info_classif(df_all.loc[df_all.is_train==1, num_cols], df_all.loc[df_all.is_train==1, TARGET])\nmain_nums = [num_cols[i] for i in np.argsort(mi)[::-1][:5]]           # Only top 5 for max generalization\nprint(f\"‚òÜ Selected numerics (by MI): {main_nums}\")\n\n# 4. Numeric interactions (only for main_nums, and only sum/prod/ratio‚Äînot all pairs)\nprint(\"‚ûó Creating numeric interactions (sum, prod, ratio)...\")\nfor i in range(len(main_nums)):\n    for j in range(i+1, len(main_nums)):\n        c1, c2 = main_nums[i], main_nums[j]\n        df_all[f\"{c1}_plus_{c2}\"] = df_all[c1] + df_all[c2]\n        df_all[f\"{c1}_times_{c2}\"] = df_all[c1] * df_all[c2]\n        df_all[f\"{c1}_div_{c2}\"]   = df_all[c1] / (df_all[c2] + 1e-5)  # safe division\n\nprint(\"‚úì Interactions done.\")\n\n# 5. Binning/transforms (main nums only)\nprint(\"üßÆ Numeric transforms/quantile bins/log/sqrt...\")\nfor col in main_nums:\n    try:\n        df_all[f'{col}_bin'] = pd.qcut(df_all[col].rank(method='first'), 5, labels=False, duplicates='drop')\n    except Exception as e:\n        print(f\"   - Skipped binning for {col}: {e}\")\n    df_all[f'{col}_log']  = np.log1p(np.abs(df_all[col]))\n    df_all[f'{col}_sqrt'] = np.sqrt(np.abs(df_all[col]))\nprint(\"‚úì Transforms complete.\")\n\n# 6. Restore final train/test sets\ndf_train = df_all[df_all['is_train']==1].drop(columns=['is_train'])\ndf_test  = df_all[df_all['is_train']==0].drop(columns=['is_train', TARGET], errors='ignore')\nprint(f\"‚úîÔ∏è Final train: {df_train.shape} | test: {df_test.shape}\")\n\n# 7. Standard scaling for selected numerics (for strong stacking/meta-models)\nscaler = StandardScaler()\ndf_train[main_nums] = scaler.fit_transform(df_train[main_nums])\ndf_test[main_nums]  = scaler.transform(df_test[main_nums])\nprint(f\"‚úÖ Features ready: {df_train.shape[1]-1} (excluding target)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:21:05.188321Z","iopub.execute_input":"2025-11-10T09:21:05.188933Z","iopub.status.idle":"2025-11-10T09:21:05.349419Z","shell.execute_reply.started":"2025-11-10T09:21:05.1889Z","shell.execute_reply":"2025-11-10T09:21:05.348379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìã Robust Feature Selection (Multi-Method, Consensus-Driven)\n#### Why This Step?\n\nAfter focused feature engineering, hundreds of features may remain. Many are noisy, redundant, or only weakly informative.\nThis step is like assembling a panel of expert judges‚Äîonly features deemed strong by multiple perspectives make it to modeling.\n\n#### Step 1: Remove Constant & Redundant Features üö´üîó\n\nConstant columns:\nAutomatically dropped, since they don‚Äôt differentiate any outcome.\n\ntext\nif df['column'].nunique() <= 1:\n    drop column\nHigh-correlation pairs:\nRemove one from any pair of features correlated above 0.99.\nPrevents duplication and overfitting.\n\n#### Step 2: Calculate Feature Importance ‚Äî Three Perspectives üëÄ\n\nEach method views the data differently:\n#### LightGBM Gain:\nMeasures how effectively each feature splits the data into pure risk groups.\n\n#### Mutual Information:\nScores how much knowing the feature reduces uncertainty about the target‚Äîinclusive of nonlinear/complex effects.\n\n#### ExtraTrees Gain:\nAverages many random trees; rewards features useful for splits (but with different regularization and sampling logic from LGBM).\n\n#### Step 3: Require Agreement Among Judges ü§ù\n\nA feature must be flagged as important by at least two out of the three methods (using a meaningful threshold for each):\n\nThis ensures that weak, noisy, or accidental correlations don‚Äôt sneak in just because one method ‚Äúlikes‚Äù them.\nModels become more robust and less prone to leaderboard shakeups.\n\n#### Step 4: Rank and Cut to the Top ùêç Features üèÜ\n\nFor those features passing the agreement criteria:\nCompute their average rank across all selectors.\nSelect only the top N by lowest average rank.\n\ntext\nTop 50 features after all filtering and consensus ranking =\n    Most widely, robustly predictive; least noisy\nStep 5: Impute Missing Values üîß\n\nFill NaNs in train/test with the median from training (never from test) to avoid information leak.\n\nEnsures the resulting modeling DataFrame is clean and fully numeric.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef blended_feature_selection(df_train, df_test, target_col='loan_paid_back', n_features=50,\n                             mi_thresh=0.03, lgb_thresh=0.03, et_thresh=0.03, min_sources=2):\n    print(\"\\n=*= FEATURE SELECTION PIPELINE (Multiple Importances, Robust) =*=\")\n    y = df_train[target_col]\n    X_train = df_train.drop(columns=[target_col])\n    X_test = df_test.copy()\n\n    # Remove constant features\n    constant_features = [c for c in X_train.columns if X_train[c].nunique(dropna=False) <= 1]\n    if constant_features:\n        print(f\"‚úì Dropped {len(constant_features)} constant features: {constant_features}\")\n    X_train = X_train.drop(columns=constant_features)\n    X_test  = X_test.drop(columns=constant_features, errors='ignore')\n\n    # Remove highly correlated features\n    corr_matrix = X_train.corr().abs()\n    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    high_corr = [col for col in upper_tri.columns if (upper_tri[col] > 0.99).any()]\n    if high_corr:\n        print(f\"‚úì Dropped {len(high_corr)} highly correlated features: {high_corr}\")\n    X_train = X_train.drop(columns=high_corr)\n    X_test  = X_test.drop(columns=high_corr, errors='ignore')\n\n    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n    Xf = X_train[num_cols].fillna(0)  # For selectors\n\n    print(\"üîé Calculating importances and selecting features...\")\n    # Mutual Info\n    mi_scores = mutual_info_classif(Xf, y, random_state=42)\n    mi_scores_norm = mi_scores / (np.max(mi_scores) + 1e-10)\n    mi_features = [f for f, s in zip(num_cols, mi_scores_norm) if s > mi_thresh]\n\n    # LightGBM\n    lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42, importance_type='gain')\n    lgb_model.fit(Xf, y)\n    lgb_imp = lgb_model.feature_importances_\n    lgb_imp_norm = lgb_imp / (np.max(lgb_imp) + 1e-10)\n    lgb_features = [f for f, s in zip(num_cols, lgb_imp_norm) if s > lgb_thresh]\n\n    # Extra Trees\n    et_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n    et_model.fit(Xf, y)\n    et_imp = et_model.feature_importances_\n    et_imp_norm = et_imp / (np.max(et_imp) + 1e-10)\n    et_features = [f for f, s in zip(num_cols, et_imp_norm) if s > et_thresh]\n\n    # Create robust selector: keep only features identified by at least two selectors\n    selectors = {'mi': mi_features, 'lgb': lgb_features, 'et': et_features}\n    def count_sources(feat):\n        return sum([feat in selectors[s] for s in selectors])\n\n    print(\"  Counting agreement of sources for each feature...\")\n    agreement = {f: count_sources(f) for f in num_cols}\n    robust_features = [f for f, cnt in agreement.items() if cnt >= min_sources]\n    print(f\"‚òÖ Features with agreement from at least {min_sources} selectors: {len(robust_features)}\")\n\n    # Average rank filter (only rank robust features)\n    imp_df = pd.DataFrame({'feature': num_cols,\n                           'mi': mi_scores_norm, 'lgb': lgb_imp_norm, 'et': et_imp_norm})\n    for c in ['mi', 'lgb', 'et']:\n        imp_df[f'{c}_rank'] = imp_df[c].rank(ascending=False)\n    imp_df['avg_rank'] = imp_df[[f'{c}_rank' for c in ['mi','lgb','et']]].mean(axis=1)\n\n    imp_df = imp_df[imp_df['feature'].isin(robust_features)]\n    imp_df = imp_df.sort_values('avg_rank')\n    selected = imp_df['feature'].tolist()[:n_features]\n\n    print(f\"‚úÖ Robust selected features ({len(selected)}):\\n - {selected[:10]}{' ...' if len(selected) > 10 else ''}\")\n    X_final = X_train[selected].copy()\n    X_test_final = X_test[selected].copy()\n    print(f\"‚úèÔ∏è X_final: {X_final.shape}, X_test_final: {X_test_final.shape}\")\n\n    return X_final, X_test_final, y\n\n# Usage\nX_final, X_test_final, y = blended_feature_selection(\n    df_train, df_test, target_col='loan_paid_back', n_features=50,\n    mi_thresh=0.03, lgb_thresh=0.03, et_thresh=0.03, min_sources=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:21:05.350367Z","iopub.execute_input":"2025-11-10T09:21:05.350595Z","iopub.status.idle":"2025-11-10T09:21:12.192671Z","shell.execute_reply.started":"2025-11-10T09:21:05.350576Z","shell.execute_reply":"2025-11-10T09:21:12.191432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üéØ Level 0 Models ‚Äì Robust, Lean Gradient Boosting Stack\nWhat Are Level 0 Models?\n\n#### Level 0 models are the critical, high-precision first layer of your ensemble.\nInstead of many disparate learners, you assemble a select team of the most powerful tabular classifiers:\nLightGBM, XGBoost, and CatBoost.\nEach is expert at discovering subtle, nonlinear risk signals, especially in structured data.\n\n#### Model\tSpecialty\nLightGBM\tLightning-fast, highly regularized GBDT\nXGBoost\tIndustry gold-standard, robust tree boosting\nCatBoost\tHandles categoricals natively, well-calibrated\n\n#### Why just these three?\nMore models = more noise, less interpretability. Less is more.\n\n#### Step 1: Repeated Multi-Seed, Multi-Fold Validation üîÅ\n\nFor each model and each seed (e.g., 3 seeds √ó 5 folds), you:\nSplit data into stratified folds (fair evaluation for all outcome classes).\nHold out a fold, train on the rest, predict the holdout fold‚Äîrepeat for all.\nOut-of-fold (OOF) predictions for every sample ensure honest model assessment‚Äînever trained and predicted on the same data.\n\n#### Step 2: Hyperparameter Tuning per Fold üîß\n\nEach model is tuned with RandomizedSearchCV in its fold:\nTweaks tree count, maximum depth, learning rate, and regularization.\nFinds the \"sweet spot\" to maximize ROC-AUC in cross-validation‚Äîno overfitting to just one set!\n\n#### Step 3: Aggregate OOF and Test Predictions üìä\n\nAfter all folds/seeds, average OOF results‚Äîthese are the features for the stacker.\nTest predictions are mean-averaged across all seeds and folds for superior stability.\n\n#### Step 4: Create Meta-Feature Stack for Level 1 üì¶\n\nBuild new datasets from the (averaged) OOF predictions of each model:\nlgb_oof, xgb_oof, cat_oof\nThe meta-feature input to Level 1 is compact, high-signal, and robust with almost no noise.\n\n#### Step 5: Model-Specific AUC Diagnostics ‚≠ê\n\nReport ROC-AUC for each L0 model's OOF predictions‚Äîsee which learner is strongest and monitor for modeling bugs.\nHelps with optimal weighting/blending in Level 1.\n\n#### Result: SOTA Building Blocks for Ensembling\n\n3 extremely strong, diverse models: capturing almost all real-life tabular patterns.\nNo dilution from weaker models or noise-prone algorithms.\nOOF predictions are ready as Level 1 features‚Äîbest possible training for your stacker.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nX = X_final\ny = y\nX_test = X_test_final\n\nNFOLDS = 5\nSEED_LIST = [42, 123, 999]\n\nmodel_configs = {\n    'lgb': (\n        lgb.LGBMClassifier(force_col_wise=True, verbosity=-1),\n        {'n_estimators': [150, 200], 'max_depth': [5, 7, 9], 'learning_rate': [0.03, 0.05, 0.07]}\n    ),\n    'xgb': (\n        xgb.XGBClassifier(use_label_encoder=False, objective='binary:logistic'),\n        {'n_estimators': [150, 200], 'max_depth': [5, 7], 'learning_rate': [0.03, 0.05]}\n    ),\n    'cat': (\n        CatBoostClassifier(verbose=False),\n        {'iterations': [150, 200], 'depth': [5, 7], 'learning_rate': [0.03, 0.05]}\n    ),\n}\n\n# For each model, for each seed, store oof/test [seed, n_samples]\noof_preds = {m: np.zeros((len(SEED_LIST), len(X))) for m in model_configs}\ntest_preds = {m: np.zeros((len(SEED_LIST), len(X_test))) for m in model_configs}\n\nfor si, SEED in enumerate(SEED_LIST):\n    print(f\"\\n==================== SEED {SEED} ====================\")\n    skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y), 1):\n        print(f\"\\n=== Fold {fold}/{NFOLDS} (Seed {SEED}) ===\")\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n        for name, (base_model, param_dist) in model_configs.items():\n            print(f\"üîç Training {name.upper()} on fold {fold} (seed {SEED})...\")\n            model_cv = RandomizedSearchCV(\n                base_model.set_params(random_state=SEED),\n                param_dist,\n                cv=3, scoring='roc_auc', n_iter=3, n_jobs=-1, random_state=SEED\n            )\n            model_cv.fit(X_train, y_train)\n            best_params = model_cv.best_params_\n            print(f\"    Best params: {best_params}\")\n\n            if name == 'lgb':\n                final_model = lgb.LGBMClassifier(**best_params, random_state=SEED, force_col_wise=True, verbosity=-1)\n                final_model.fit(\n                    X_train, y_train,\n                    eval_set=[(X_valid, y_valid)],\n                    eval_metric='auc'\n                )\n            elif name == 'xgb':\n                final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, objective='binary:logistic', random_state=SEED)\n                final_model.fit(\n                    X_train, y_train,\n                    eval_set=[(X_valid, y_valid)],\n                    early_stopping_rounds=20,\n                    eval_metric='auc',\n                    verbose=False\n                )\n            else:  # cat\n                final_model = CatBoostClassifier(**best_params, random_state=SEED, verbose=False)\n                final_model.fit(\n                    X_train, y_train,\n                    eval_set=(X_valid, y_valid),\n                    early_stopping_rounds=20,\n                    verbose=False\n                )\n\n            pred_valid = final_model.predict_proba(X_valid)[:, 1]\n            pred_test = final_model.predict_proba(X_test)[:, 1]\n\n            oof_preds[name][si, valid_idx] = pred_valid\n            test_preds[name][si] += pred_test / NFOLDS\n\n# Average OOF/test predictions over seeds\nfinal_oof = {m: arr.mean(axis=0) for m, arr in oof_preds.items()}\nfinal_test = {m: arr.mean(axis=0) for m, arr in test_preds.items()}\n\n# Combined meta-feature DataFrames\nprint(\"\\n========== LEVEL 0 MODEL PERFORMANCE ==========\")\nfor model in final_oof:\n    auc = roc_auc_score(y, final_oof[model])\n    print(f\"‚úÖ Model: {model.upper()} | OOF ROC-AUC: {auc:.5f}\")\n\noof_stack = pd.DataFrame({f'{m}_oof': p for m, p in final_oof.items()}, index=X_final.index)\ntest_stack = pd.DataFrame({f'{m}_oof': p for m, p in final_test.items()}, index=X_test_final.index)\nprint(\"\\nüì¶ Shape of OOF meta-features:\", oof_stack.shape)\nprint(\"üì¶ Shape of Test meta-features:\", test_stack.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:22:23.193228Z","iopub.execute_input":"2025-11-10T09:22:23.194784Z","iopub.status.idle":"2025-11-10T09:25:56.336363Z","shell.execute_reply.started":"2025-11-10T09:22:23.19474Z","shell.execute_reply":"2025-11-10T09:25:56.335071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üèóÔ∏è Step 5: Stacker ‚Äì Simple, Robust Meta-Ensembling\nWhat is Stacking?\n\nStacking is the art of learning how to optimally combine your Level 0 model predictions.\nImagine a chief diagnostician (‚Äúmeta-model‚Äù) who sees every base model‚Äôs probability for each sample and decides how to blend them for the most accurate single answer.\n\n\"When LightGBM and CatBoost are both confident, trust their average; if they disagree, lean on what's worked best in validation!\"\n\n#### Step 1: Clean Meta-Feature Set üì¶\n\nInputs: Each row is the out-of-fold (OOF) prediction from each Level 0 model (i.e., lgb_oof, xgb_oof, cat_oof).\nNo extra \"meta-meta\" features (like prediction range, standard deviation, or agreement meta-features).\n\nResult: The stacker gets a tight, noise-free view of each model‚Äôs unique perspective.\n\n#### Step 2: Meta-Models = Simple, Interpretable, and Robust üèÜ\n\nOnly three meta-models are considered:\nLogistic Regression (LR)\nRidgeClassifier (Ridge)\nLightGBM (as a strong but regularized decision-tree meta-model)\n\nThis matches best practices‚Äîlean, less prone to overfit, very fast to train.\n\n#### Step 3: Grid-Search and Cross-Validation üîÑ\n\nStratified 5-fold cross-validation ensures each meta-model sees diverse, balanced splits of the feature space.\nGridSearchCV with reasonable parameter grids finds the highest-ROC AUC configuration for each meta-model.\n\n#### Step 4: Uniform Averaging by Best Validation Score ü•á\n\nAfter training, the meta-model with the highest mean OOF AUC is selected.\nThe chosen model makes final test set predictions‚Äîno further weighted blending, no ensemble of meta-models, just the cleanest final choice.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass SimpleStacker:\n    def __init__(self, n_folds=5, random_state=42):\n        self.n_folds = n_folds\n        self.random_state = random_state\n\n    def get_model_and_grid(self, model_name):\n        if model_name == 'lr':\n            return LogisticRegression(random_state=self.random_state, max_iter=300), {\n                'C': [0.05, 0.1, 0.5, 1.0]\n            }\n        elif model_name == 'ridge':\n            return RidgeClassifier(random_state=self.random_state), {\n                'alpha': [0.1, 1.0, 10.0]\n            }\n        elif model_name == 'lgb':\n            return lgb.LGBMClassifier(random_state=self.random_state, force_col_wise=True, verbosity=-1), {\n                'n_estimators': [100, 150],\n                'num_leaves': [10, 20, 31],\n                'learning_rate': [0.02, 0.05]\n            }\n\n    def fit_predict(self, oof_stack, test_stack, y_train):\n        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n\n        meta_oof = pd.DataFrame(index=oof_stack.index)\n        meta_test = pd.DataFrame(index=test_stack.index)\n        meta_scores = {}\n\n        # Only key meta-models‚Äîno tree ensemble meta!\n        model_names = ['lr', 'ridge', 'lgb']\n\n        for model_name in model_names:\n            print(f\"\\nTraining {model_name.upper()} with GridSearchCV\")\n            oof_preds = np.zeros(len(oof_stack))\n            test_preds = np.zeros(len(test_stack))\n            scores = []\n\n            base_model, param_grid = self.get_model_and_grid(model_name)\n\n            for fold, (tr_idx, va_idx) in enumerate(skf.split(oof_stack, y_train), 1):\n                X_tr, X_va = oof_stack.iloc[tr_idx], oof_stack.iloc[va_idx]\n                y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n                grid = GridSearchCV(base_model, param_grid, scoring='roc_auc', cv=3, n_jobs=-1)\n                grid.fit(X_tr, y_tr)\n                best_model = grid.best_estimator_\n\n                if hasattr(best_model, 'predict_proba'):\n                    val_pred = best_model.predict_proba(X_va)[:, 1]\n                    test_pred = best_model.predict_proba(test_stack)[:, 1]\n                else:\n                    val_pred = best_model.decision_function(X_va)\n                    test_pred = best_model.decision_function(test_stack)\n                    val_pred = (val_pred - val_pred.min()) / (val_pred.max() - val_pred.min() + 1e-8)\n                    test_pred = (test_pred - test_pred.min()) / (test_pred.max() - test_pred.min() + 1e-8)\n\n                oof_preds[va_idx] = val_pred\n                test_preds += test_pred / self.n_folds\n\n                score = roc_auc_score(y_va, val_pred)\n                scores.append(score)\n                print(f\"  Fold {fold}: AUC = {score:.4f}\")\n\n            meta_oof[model_name] = oof_preds\n            meta_test[model_name] = test_preds\n            meta_scores[model_name] = np.mean(scores)\n            print(f\" > Mean OOF AUC for {model_name.upper()}: {meta_scores[model_name]:.5f}\")\n\n        # Simple uniform average or best model ‚Äî or blend by OOF AUC\n        best_model_name = max(meta_scores, key=lambda x: meta_scores[x])\n        print(f\"\\nFinal Stacker: Best meta-model by OOF AUC is {best_model_name.upper()} ({meta_scores[best_model_name]:.5f})\")\n        final_oof = meta_oof[best_model_name].values\n        final_test = meta_test[best_model_name].values\n\n        print(f\"\\nFinal Stacked AUC: {roc_auc_score(y_train, final_oof):.5f}\")\n        return final_oof, final_test\n\n# Usage\nstacker = SimpleStacker(n_folds=5, random_state=42)\nfinal_oof, final_test = stacker.fit_predict(oof_stack, test_stack, y)\n\n\n# Build submission\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\nsubmission = pd.DataFrame({\"id\": df_test[\"id\"], \"loan_paid_back\": final_test})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved final submission.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:25:56.337767Z","iopub.execute_input":"2025-11-10T09:25:56.338056Z","iopub.status.idle":"2025-11-10T09:26:14.125927Z","shell.execute_reply.started":"2025-11-10T09:25:56.338009Z","shell.execute_reply":"2025-11-10T09:26:14.125113Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Meta model Evaluation\n\n\n### **What Is This Step?**\n\nAfter building Level 0 base models and Level 1 meta-model, you need to see **which performs best**. This step evaluates all models side-by-side using multiple metrics and visualizations, then compares your stacked model against simpler alternatives to confirm the extra complexity was worth it.\n\n---\n\n## **Step 1: Evaluate All Models**\n\nCalculate AUC, accuracy, F1, precision, recall, specificity, and optimal thresholds for each Level 0 model and the final meta-model. This shows not just \"which predicts well\" but **how well each performs across different decision thresholds and metrics**.\n\n***\n\n## **Step 2: Rank Models by AUC**\n\nSort all models by their ROC-AUC score. Example: CatBoost (0.9523) > LGB (0.9456) > XGB (0.9412) > ... > Naive Bayes (0.8956). Identifies which base models are strongest and whether the meta-model beats them all.\n\n***\n\n## **Step 3: Visualize ROC Curves**\n\nPlot ROC curves for all 10 models (9 Level 0 + 1 meta). Higher curves closer to top-left = better discrimination. **Key insight:** If meta-model's curve is above all Level 0 curves, stacking worked. If it's below, simpler averaging might be better.\n\n***\n\n## **Step 4: Plot Calibration Curves**\n\nShow whether predicted probabilities match actual default rates. A perfect model's calibration curve follows the diagonal (45¬∞). Miscalibrated models predict 0.7 but 60% actually default. Meta-model often has better calibration (more honest probabilities) than base models.\n\n***\n\n## **Step 5: Compare Prediction Distributions**\n\nHistogram of predicted probabilities for each model. Some models might be overconfident (clustered near 0 and 1), others too cautious (clustered near 0.5). Meta-model often has a balanced, realistic distribution.\n\n***\n\n## **Step 6: Compare Against Greedy Ensemble**\n\nGreedy forward selection: Start with nothing, iteratively add the model that improves ensemble AUC the most. Example result: \"Best ensemble = LGB + Cat + XGB (AUC = 0.9487)\". **Compare:** Is meta-model (0.9523) better than simple greedy average (0.9487)? If yes, stacking justified. If no, use greedy ensemble instead.\n\n***\n\n## **Result:**\n\nClear ranking of all models, visual proof that meta-model outperforms alternatives, and confirmation that two-layer stacking was worth the effort. You now have confidence in your final submission.\n\n***\n\n**This evaluation ensures your most complex model actually performs better‚Äîand isn't just overfitting complexity.**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, brier_score_loss, roc_curve\n)\nfrom sklearn.calibration import calibration_curve\n\nclass Step6_ModelEvaluation:\n    def __init__(self, target_col='loan_paid_back'):\n        self.target_col = target_col\n\n    def evaluate_all_models(self, y_true, predictions_dict, model_type=\"Level 0\"):\n        results = {}\n        for name, preds in predictions_dict.items():\n            preds_binary = (preds > 0.5).astype(int)\n            auc = roc_auc_score(y_true, preds)\n            acc = accuracy_score(y_true, preds_binary)\n            f1 = f1_score(y_true, preds_binary)\n            prec = precision_score(y_true, preds_binary)\n            recall = recall_score(y_true, preds_binary)\n            brier = brier_score_loss(y_true, preds)\n            results[name] = {'auc': auc, 'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': recall, 'brier': brier}\n            print(f\"[{model_type}] {name}: AUC={auc:.4f} | Acc={acc:.4f} | F1={f1:.4f} | Prec={prec:.4f} | Recall={recall:.4f} | Brier={brier:.4f}\")\n        return results\n\n    def model_rank_table(self, results):\n        res_df = pd.DataFrame(results).T\n        print(res_df.sort_values('auc', ascending=False)[['auc', 'accuracy', 'f1', 'precision', 'recall', 'brier']])\n\n    def plot_roc_curves(self, y_true, predictions_dict):\n        plt.figure(figsize=(10, 7))\n        for name, preds in predictions_dict.items():\n            fpr, tpr, _ = roc_curve(y_true, preds)\n            auc = roc_auc_score(y_true, preds)\n            plt.plot(fpr, tpr, label=f\"{name} (AUC: {auc:.3f})\")\n        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(\"ROC Curves\")\n        plt.legend(); plt.show()\n\n    def plot_calibration_curves(self, y_true, predictions_dict):\n        plt.figure(figsize=(10, 7))\n        for name, preds in predictions_dict.items():\n            prob_true, prob_pred = calibration_curve(y_true, preds, n_bins=10, strategy='uniform')\n            plt.plot(prob_pred, prob_true, marker='o', label=name)\n        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n        plt.xlabel(\"Mean Predicted Value\"); plt.ylabel(\"Fraction of Positives\"); plt.title(\"Calibration Curves\")\n        plt.legend(); plt.show()\n\n    def plot_prediction_distribution(self, predictions_dict):\n        plt.figure(figsize=(10, 7))\n        for name, preds in predictions_dict.items():\n            plt.hist(preds, bins=50, alpha=0.5, label=name, density=True)\n        plt.xlabel(\"Predicted Probability\"); plt.ylabel(\"Density\"); plt.title(\"Prediction Distributions\")\n        plt.legend(); plt.show()\n\n    def compare_ensembles(self, y_true, predictions_dict):\n        preds_df = pd.DataFrame(predictions_dict)\n        aucs = {col: roc_auc_score(y_true, preds_df[col]) for col in preds_df.columns}\n        best_model = max(aucs, key=aucs.get)\n        remaining = set(preds_df.columns) - {best_model}\n        current = preds_df[[best_model]].copy()\n        best_auc = aucs[best_model]\n        selected = [best_model]\n        improved = True\n        while improved and remaining:\n            improved = False\n            next_best = None\n            next_auc = best_auc\n            for cand in remaining:\n                avg_pred = current.mean(axis=1) * (len(current.columns) / (len(current.columns)+1)) + preds_df[cand] / (len(current.columns)+1)\n                auc_val = roc_auc_score(y_true, avg_pred)\n                if auc_val > next_auc:\n                    next_auc = auc_val\n                    next_best = cand\n            if next_best is not None:\n                selected.append(next_best)\n                current[next_best] = preds_df[next_best]\n                best_auc = next_auc\n                remaining.remove(next_best)\n                improved = True\n        final_ensemble_pred = current.mean(axis=1)\n        print(f\"Greedy ensemble selected models: {selected}\")\n        return selected, final_ensemble_pred\n\n# Assuming oof_stack columns: 'lgb_oof', 'xgb_oof', 'cat_oof'\n# And final_oof from meta-stacker\n\nevaluator = Step6_ModelEvaluation(target_col='loan_paid_back')\n\n# Evaluate individual L0 models\nlevel0_results = evaluator.evaluate_all_models(\n    y_true=y,\n    predictions_dict={col: oof_stack[col] for col in oof_stack.columns},\n    model_type=\"Level 0\"\n)\n\n# Evaluate meta-stacker\nmeta_results = evaluator.evaluate_all_models(\n    y_true=y,\n    predictions_dict={'stacked_meta': final_oof},\n    model_type=\"Meta\"\n)\n\nprint(\"\\nüî¢ Model Ranking Table (by AUC):\")\nevaluator.model_rank_table(level0_results)\n\n# Visualization: L0 + meta only\nplot_dict = {col: oof_stack[col] for col in oof_stack.columns}\nplot_dict['stacked_meta'] = final_oof\n\nprint(\"\\nüìà ROC Curves\")\nevaluator.plot_roc_curves(y, plot_dict)\nprint(\"\\nüìâ Calibration Curves\")\nevaluator.plot_calibration_curves(y, plot_dict)\nprint(\"\\nüìä Prediction Distributions\")\nevaluator.plot_prediction_distribution(plot_dict)\n\n# Greedy ensemble comparison (optional for curiosity)\nprint(\"\\nüîß Greedy Ensemble Search\")\nselected_models, ensemble_preds = evaluator.compare_ensembles(y, {col: oof_stack[col] for col in oof_stack.columns})\nensemble_auc = roc_auc_score(y, ensemble_preds)\nprint(f\"\\nGreedy Ensemble AUC: {ensemble_auc:.4f}\")\nprint(f\"Meta-model better than ensemble? {meta_results['stacked_meta']['auc'] > ensemble_auc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T22:52:14.302862Z","iopub.execute_input":"2025-11-09T22:52:14.303184Z","iopub.status.idle":"2025-11-09T22:52:15.560388Z","shell.execute_reply.started":"2025-11-09T22:52:14.303146Z","shell.execute_reply":"2025-11-09T22:52:15.559469Z"}},"outputs":[],"execution_count":null}]}