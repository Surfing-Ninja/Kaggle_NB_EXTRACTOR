{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:24:44.317518Z","iopub.execute_input":"2025-11-02T20:24:44.317871Z","iopub.status.idle":"2025-11-02T20:24:46.806824Z","shell.execute_reply.started":"2025-11-02T20:24:44.317844Z","shell.execute_reply":"2025-11-02T20:24:46.805667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n=================================================================================\nENHANCED LOAN PAYBACK PREDICTION - TOP LEADERBOARD STRATEGY\nKaggle Playground Series S5E11\nTarget: 0.93+ ROC AUC\n=================================================================================\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tabulate import tabulate\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Model imports\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\n\n# Styling\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\npd.set_option('display.precision', 5)\n\nprint(\"=\"*85)\nprint(\" üöÄ ENHANCED LOAN PAYBACK PREDICTION - TOP LEADERBOARD STRATEGY\".center(85))\nprint(\"=\"*85)\n\n# =============================================================================\n# 1. DATA LOADING\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" üìÇ PHASE 1: DATA LOADING\".center(85))\nprint(\"=\"*85)\n\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\n\nprint(f\"\\n‚úì Train: {train.shape[0]:,} rows √ó {train.shape[1]} columns\")\nprint(f\"‚úì Test:  {test.shape[0]:,} rows √ó {test.shape[1]} columns\")\n\n# =============================================================================\n# 2. ADVANCED FEATURE ENGINEERING\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" üîß PHASE 2: ADVANCED FEATURE ENGINEERING\".center(85))\nprint(\"=\"*85)\n\ndef parse_grade_subgrade(df):\n    \"\"\"Parse grade_subgrade into letter and number components\"\"\"\n    df = df.copy()\n    df['grade_letter'] = df['grade_subgrade'].str[0]\n    df['grade_number'] = df['grade_subgrade'].str[1:].astype(int)\n    \n    # Create grade ranking (A1=1, A2=2, ..., G5=35)\n    grade_rank = {'A': 0, 'B': 5, 'C': 10, 'D': 15, 'E': 20, 'F': 25, 'G': 30}\n    df['grade_rank'] = df['grade_letter'].map(grade_rank) + df['grade_number']\n    \n    return df\n\ndef create_elite_features(df, is_train=True):\n    \"\"\"Create advanced engineered features for top performance\"\"\"\n    df = df.copy()\n    \n    # Parse grade subgrade\n    df = parse_grade_subgrade(df)\n    \n    # Financial ratio features\n    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n    df['loan_to_income_pct'] = (df['loan_amount'] / df['annual_income']) * 100\n    df['available_income'] = df['annual_income'] * (1 - df['debt_to_income_ratio'])\n    df['loan_burden'] = df['loan_amount'] / df['available_income']\n    \n    # Credit risk metrics\n    df['credit_risk_v1'] = (\n        df['credit_score'] * 0.4 - \n        df['debt_to_income_ratio'] * 1000 * 0.3 - \n        df['interest_rate'] * 10 * 0.2 -\n        df['grade_rank'] * 5 * 0.1\n    )\n    \n    df['credit_score_norm'] = (df['credit_score'] - 395) / (849 - 395)\n    df['credit_interest_diff'] = df['credit_score_norm'] - (df['interest_rate'] / 20)\n    \n    # Payment capacity features\n    df['monthly_income'] = df['annual_income'] / 12\n    \n    monthly_rate = df['interest_rate'] / 100 / 12\n    n_payments = 60\n    df['estimated_monthly_payment'] = (\n        df['loan_amount'] * monthly_rate * (1 + monthly_rate)**n_payments\n    ) / ((1 + monthly_rate)**n_payments - 1)\n    \n    df['payment_to_income_ratio'] = df['estimated_monthly_payment'] / df['monthly_income']\n    df['disposable_after_loan'] = df['available_income'] / 12 - df['estimated_monthly_payment']\n    df['payment_stress'] = df['estimated_monthly_payment'] / (df['available_income'] / 12)\n    \n    # Interaction features\n    df['credit_x_income'] = df['credit_score'] * np.log1p(df['annual_income'])\n    df['debt_x_interest'] = df['debt_to_income_ratio'] * df['interest_rate']\n    df['grade_x_credit'] = df['grade_rank'] * (850 - df['credit_score'])\n    df['loan_x_interest'] = np.log1p(df['loan_amount']) * df['interest_rate']\n    \n    # Polynomial features\n    df['credit_score_sq'] = df['credit_score'] ** 2\n    df['debt_ratio_sq'] = df['debt_to_income_ratio'] ** 2\n    df['interest_rate_sq'] = df['interest_rate'] ** 2\n    \n    # Categorical frequency encoding\n    categorical_cols = ['gender', 'marital_status', 'education_level', \n                       'employment_status', 'loan_purpose', 'grade_letter']\n    \n    for col in categorical_cols:\n        freq = df[col].value_counts(normalize=True)\n        df[f'{col}_freq'] = df[col].map(freq)\n    \n    # Risk flags\n    df['high_risk_flag'] = (\n        (df['credit_score'] < 600) | \n        (df['debt_to_income_ratio'] > 0.4) |\n        (df['grade_rank'] > 20)\n    ).astype(int)\n    \n    df['excellent_credit'] = (df['credit_score'] >= 750).astype(int)\n    df['low_debt_burden'] = (df['debt_to_income_ratio'] <= 0.2).astype(int)\n    df['employed_flag'] = (df['employment_status'] == 'Employed').astype(int)\n    df['high_income'] = (df['annual_income'] >= df['annual_income'].median()).astype(int)\n    df['small_loan'] = (df['loan_amount'] <= df['loan_amount'].quantile(0.3)).astype(int)\n    \n    df['composite_risk'] = (\n        df['high_risk_flag'] * 3 -\n        df['excellent_credit'] * 2 -\n        df['low_debt_burden'] * 2 -\n        df['employed_flag'] * 1\n    )\n    \n    # Binning features\n    numeric_for_binning = ['annual_income', 'debt_to_income_ratio', 'credit_score', \n                           'loan_amount', 'interest_rate']\n    \n    for col in numeric_for_binning:\n        for q in [5, 10]:\n            try:\n                df[f'{col}_bin{q}'] = pd.qcut(df[col], q=q, labels=False, duplicates='drop')\n            except:\n                df[f'{col}_bin{q}'] = 0\n    \n    return df\n\nprint(\"\\n‚öôÔ∏è  Creating elite feature set...\")\ntrain_fe = create_elite_features(train, is_train=True)\ntest_fe = create_elite_features(test, is_train=False)\nprint(f\"‚úì Created {len([c for c in train_fe.columns if c not in train.columns])} new features\")\n\n# =============================================================================\n# 3. TARGET ENCODING FOR CATEGORICAL VARIABLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" üéØ PHASE 3: ADVANCED CATEGORICAL ENCODING\".center(85))\nprint(\"=\"*85)\n\ncategorical_cols = ['gender', 'marital_status', 'education_level', \n                   'employment_status', 'loan_purpose', 'grade_subgrade', 'grade_letter']\n\ny = train_fe['loan_paid_back']\nX_train = train_fe.drop(['id', 'loan_paid_back'], axis=1)\nX_test = test_fe.drop(['id'], axis=1)\ntest_ids = test_fe['id']\n\nprint(\"\\n‚öôÔ∏è  Applying target encoding with cross-validation...\")\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\nfor col in categorical_cols:\n    if col in X_train.columns:\n        X_train[f'{col}_target_enc'] = 0.0\n        X_test[f'{col}_target_enc'] = 0.0\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y)):\n            target_mean = y.iloc[train_idx].groupby(X_train[col].iloc[train_idx]).mean()\n            X_train.loc[val_idx, f'{col}_target_enc'] = X_train.loc[val_idx, col].map(target_mean)\n        \n        global_mean = y.mean()\n        X_train[f'{col}_target_enc'].fillna(global_mean, inplace=True)\n        \n        target_mean_full = y.groupby(X_train[col]).mean()\n        X_test[f'{col}_target_enc'] = X_test[col].map(target_mean_full).fillna(global_mean)\n        \n        print(f\"‚úì Target encoded: {col}\")\n\n# Label encoding\nlabel_encoders = {}\nfor col in categorical_cols:\n    if col in X_train.columns:\n        le = LabelEncoder()\n        X_train[col] = le.fit_transform(X_train[col].astype(str))\n        X_test[col] = le.transform(X_test[col].astype(str))\n        label_encoders[col] = le\n\nprint(f\"\\n‚úì Final feature count: {X_train.shape[1]}\")\n\n# =============================================================================\n# 4. ENSEMBLE MODEL TRAINING\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" ü§ñ PHASE 4: MULTI-MODEL ENSEMBLE TRAINING\".center(85))\nprint(\"=\"*85)\n\nscale_pos_weight = (y == 0).sum() / (y == 1).sum()\nprint(f\"\\n‚öñÔ∏è  Class imbalance ratio: {scale_pos_weight:.4f}\")\n\nxgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'tree_method': 'hist',\n    'device': 'cuda',\n    'random_state': 42,\n    'learning_rate': 0.01,\n    'max_depth': 7,\n    'min_child_weight': 80,\n    'subsample': 0.85,\n    'colsample_bytree': 0.85,\n    'colsample_bylevel': 0.85,\n    'colsample_bynode': 0.85,\n    'gamma': 0.1,\n    'reg_alpha': 2.0,\n    'reg_lambda': 5.0,\n    'scale_pos_weight': scale_pos_weight,\n    'n_estimators': 3000,\n    'early_stopping_rounds': 100\n}\n\nlgb_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'max_depth': 7,\n    'min_child_weight': 70,\n    'reg_alpha': 2.0,\n    'reg_lambda': 5.0,\n    'scale_pos_weight': scale_pos_weight,\n    'random_state': 42,\n    'verbose': -1,\n    'n_estimators': 3000\n}\n\ncat_params = {\n    'iterations': 3000,\n    'learning_rate': 0.01,\n    'depth': 7,\n    'l2_leaf_reg': 5,\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'random_seed': 42,\n    'verbose': False,\n    'early_stopping_rounds': 100,\n    'scale_pos_weight': scale_pos_weight\n}\n\nprint(\"\\nüîÑ Training ensemble with 5-Fold Stratified CV...\")\nprint(\"‚îÄ\" * 85)\n\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\noof_xgb = np.zeros(len(X_train))\noof_lgb = np.zeros(len(X_train))\noof_cat = np.zeros(len(X_train))\n\ntest_xgb = np.zeros(len(X_test))\ntest_lgb = np.zeros(len(X_test))\ntest_cat = np.zeros(len(X_test))\n\nfeature_importance = []\ncv_scores = {'xgb': [], 'lgb': [], 'cat': [], 'ensemble': []}\n\n# Store ROC curves for visualization\nroc_data = {'xgb': [], 'lgb': [], 'cat': [], 'ensemble': []}\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y), 1):\n    print(f\"\\n{'Fold ' + str(fold):^85}\")\n    print(\"‚îÄ\" * 85)\n    \n    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # XGBoost\n    print(\"Training XGBoost...\", end=\" \")\n    xgb_model = XGBClassifier(**xgb_params)\n    xgb_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n    oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n    test_xgb += xgb_model.predict_proba(X_test)[:, 1] / n_folds\n    xgb_score = roc_auc_score(y_val, oof_xgb[val_idx])\n    cv_scores['xgb'].append(xgb_score)\n    print(f\"AUC: {xgb_score:.6f}\")\n    \n    fpr, tpr, _ = roc_curve(y_val, oof_xgb[val_idx])\n    roc_data['xgb'].append((fpr, tpr, xgb_score))\n    \n    # LightGBM\n    print(\"Training LightGBM...\", end=\" \")\n    lgb_model = lgb.LGBMClassifier(**lgb_params)\n    lgb_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                  callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])\n    oof_lgb[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n    test_lgb += lgb_model.predict_proba(X_test)[:, 1] / n_folds\n    lgb_score = roc_auc_score(y_val, oof_lgb[val_idx])\n    cv_scores['lgb'].append(lgb_score)\n    print(f\"AUC: {lgb_score:.6f}\")\n    \n    fpr, tpr, _ = roc_curve(y_val, oof_lgb[val_idx])\n    roc_data['lgb'].append((fpr, tpr, lgb_score))\n    \n    # CatBoost\n    print(\"Training CatBoost...\", end=\" \")\n    cat_model = CatBoostClassifier(**cat_params)\n    cat_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=False)\n    oof_cat[val_idx] = cat_model.predict_proba(X_val)[:, 1]\n    test_cat += cat_model.predict_proba(X_test)[:, 1] / n_folds\n    cat_score = roc_auc_score(y_val, oof_cat[val_idx])\n    cv_scores['cat'].append(cat_score)\n    print(f\"AUC: {cat_score:.6f}\")\n    \n    fpr, tpr, _ = roc_curve(y_val, oof_cat[val_idx])\n    roc_data['cat'].append((fpr, tpr, cat_score))\n    \n    # Ensemble\n    oof_ensemble = (oof_xgb[val_idx] * 0.4 + oof_lgb[val_idx] * 0.35 + oof_cat[val_idx] * 0.25)\n    ensemble_score = roc_auc_score(y_val, oof_ensemble)\n    cv_scores['ensemble'].append(ensemble_score)\n    print(f\"Ensemble AUC: {ensemble_score:.6f}\")\n    \n    fpr, tpr, _ = roc_curve(y_val, oof_ensemble)\n    roc_data['ensemble'].append((fpr, tpr, ensemble_score))\n    \n    # Feature importance\n    fold_importance = pd.DataFrame({\n        'feature': X_train.columns,\n        'importance': xgb_model.feature_importances_,\n        'fold': fold\n    })\n    feature_importance.append(fold_importance)\n    \n    gc.collect()\n\n# Overall scores\noof_ensemble_final = (oof_xgb * 0.4 + oof_lgb * 0.35 + oof_cat * 0.25)\noverall_score = roc_auc_score(y, oof_ensemble_final)\n\n# =============================================================================\n# 5. RESULTS SUMMARY\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" üìä CROSS-VALIDATION RESULTS\".center(85))\nprint(\"=\"*85)\n\nresults_table = [\n    ['XGBoost Mean', f\"{np.mean(cv_scores['xgb']):.6f}\", f\"¬±{np.std(cv_scores['xgb']):.6f}\"],\n    ['LightGBM Mean', f\"{np.mean(cv_scores['lgb']):.6f}\", f\"¬±{np.std(cv_scores['lgb']):.6f}\"],\n    ['CatBoost Mean', f\"{np.mean(cv_scores['cat']):.6f}\", f\"¬±{np.std(cv_scores['cat']):.6f}\"],\n    ['‚îÄ'*20, '‚îÄ'*15, '‚îÄ'*15],\n    ['Ensemble Mean', f\"{np.mean(cv_scores['ensemble']):.6f}\", f\"¬±{np.std(cv_scores['ensemble']):.6f}\"],\n    ['Overall OOF', f\"{overall_score:.6f}\", '']\n]\n\nprint(\"\\n\" + tabulate(results_table, headers=['Model', 'ROC AUC', 'Std'], tablefmt='fancy_grid'))\n\n# =============================================================================\n# 6. OPTIMIZED ENSEMBLE WEIGHTS\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" ‚öñÔ∏è  PHASE 5: OPTIMIZING ENSEMBLE WEIGHTS\".center(85))\nprint(\"=\"*85)\n\nbest_score = 0\nbest_weights = (0.4, 0.35, 0.25)\n\nprint(\"\\nSearching for optimal weights...\")\nfor w1 in np.arange(0.3, 0.5, 0.05):\n    for w2 in np.arange(0.25, 0.45, 0.05):\n        w3 = 1 - w1 - w2\n        if w3 < 0.2 or w3 > 0.4:\n            continue\n        \n        oof_weighted = oof_xgb * w1 + oof_lgb * w2 + oof_cat * w3\n        score = roc_auc_score(y, oof_weighted)\n        \n        if score > best_score:\n            best_score = score\n            best_weights = (w1, w2, w3)\n\nprint(f\"\\n‚úì Best weights: XGB={best_weights[0]:.2f}, LGB={best_weights[1]:.2f}, CAT={best_weights[2]:.2f}\")\nprint(f\"‚úì Best OOF Score: {best_score:.6f}\")\n\ntest_ensemble = test_xgb * best_weights[0] + test_lgb * best_weights[1] + test_cat * best_weights[2]\n\n# =============================================================================\n# 7. STUNNING VISUALIZATIONS\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" üìä PHASE 6: GENERATING STUNNING VISUALIZATIONS\".center(85))\nprint(\"=\"*85)\n\n# Aggregate feature importance\nfi_df = pd.concat(feature_importance)\nfi_agg = fi_df.groupby('feature')['importance'].mean().sort_values(ascending=False).head(20)\n\n# Create comprehensive visualization\nfig = plt.figure(figsize=(24, 16))\nfig.suptitle('üöÄ Elite Loan Payback Prediction - Comprehensive Analysis Dashboard', \n             fontsize=20, fontweight='bold', y=0.995)\n\n# Color schemes\ncolors_models = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\ncolors_gradient = plt.cm.viridis(np.linspace(0, 1, 5))\n\n# =================== ROW 1 ===================\n# 1. Feature Importance (Top 20)\nax1 = plt.subplot(3, 4, 1)\nbars = ax1.barh(range(len(fi_agg)), fi_agg.values, color=plt.cm.plasma(np.linspace(0.3, 0.9, len(fi_agg))))\nax1.set_yticks(range(len(fi_agg)))\nax1.set_yticklabels(fi_agg.index, fontsize=9)\nax1.set_xlabel('Importance Score', fontsize=11, fontweight='bold')\nax1.set_title('üèÜ Top 20 Feature Importance', fontsize=13, fontweight='bold', pad=10)\nax1.invert_yaxis()\nax1.grid(axis='x', alpha=0.3, linestyle='--')\nfor i, (feat, imp) in enumerate(fi_agg.items()):\n    ax1.text(imp, i, f' {imp:.4f}', va='center', fontsize=8)\n\n# 2. Model Performance Comparison\nax2 = plt.subplot(3, 4, 2)\nmodels = ['XGBoost', 'LightGBM', 'CatBoost', 'Ensemble']\nmeans = [np.mean(cv_scores['xgb']), np.mean(cv_scores['lgb']), \n         np.mean(cv_scores['cat']), np.mean(cv_scores['ensemble'])]\nstds = [np.std(cv_scores['xgb']), np.std(cv_scores['lgb']), \n        np.std(cv_scores['cat']), np.std(cv_scores['ensemble'])]\nx_pos = np.arange(len(models))\nbars = ax2.bar(x_pos, means, yerr=stds, capsize=5, color=colors_models, alpha=0.8, edgecolor='black', linewidth=1.5)\nax2.set_xticks(x_pos)\nax2.set_xticklabels(models, fontsize=10, fontweight='bold')\nax2.set_ylabel('ROC AUC Score', fontsize=11, fontweight='bold')\nax2.set_title('üìä Model Performance Comparison', fontsize=13, fontweight='bold', pad=10)\nax2.set_ylim([0.910, 0.920])\nax2.grid(axis='y', alpha=0.3, linestyle='--')\nfor i, (m, s) in enumerate(zip(means, stds)):\n    ax2.text(i, m + s + 0.0005, f'{m:.5f}', ha='center', fontsize=9, fontweight='bold')\n\n# 3. ROC Curves - All Models\nax3 = plt.subplot(3, 4, 3)\nmodel_names = ['XGBoost', 'LightGBM', 'CatBoost', 'Ensemble']\nfor idx, (model_key, model_name, color) in enumerate(zip(['xgb', 'lgb', 'cat', 'ensemble'], \n                                                          model_names, colors_models)):\n    mean_fpr = np.linspace(0, 1, 100)\n    tprs = []\n    for fpr, tpr, score in roc_data[model_key]:\n        tprs.append(np.interp(mean_fpr, fpr, tpr))\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = np.mean(cv_scores[model_key])\n    ax3.plot(mean_fpr, mean_tpr, color=color, linewidth=2.5, \n             label=f'{model_name} (AUC={mean_auc:.4f})', alpha=0.9)\n\nax3.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier', alpha=0.5)\nax3.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\nax3.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\nax3.set_title('üìà ROC Curves - Model Comparison', fontsize=13, fontweight='bold', pad=10)\nax3.legend(loc='lower right', fontsize=9, framealpha=0.9)\nax3.grid(alpha=0.3, linestyle='--')\n\n# 4. CV Fold Scores\nax4 = plt.subplot(3, 4, 4)\nfold_nums = np.arange(1, 6)\nwidth = 0.2\nax4.bar(fold_nums - 1.5*width, cv_scores['xgb'], width, label='XGBoost', color=colors_models[0], alpha=0.8)\nax4.bar(fold_nums - 0.5*width, cv_scores['lgb'], width, label='LightGBM', color=colors_models[1], alpha=0.8)\nax4.bar(fold_nums + 0.5*width, cv_scores['cat'], width, label='CatBoost', color=colors_models[2], alpha=0.8)\nax4.bar(fold_nums + 1.5*width, cv_scores['ensemble'], width, label='Ensemble', color=colors_models[3], alpha=0.8)\nax4.axhline(y=overall_score, color='red', linestyle='--', linewidth=2, label=f'Overall: {overall_score:.5f}')\nax4.set_xlabel('Fold Number', fontsize=11, fontweight='bold')\nax4.set_ylabel('ROC AUC Score', fontsize=11, fontweight='bold')\nax4.set_title('üîÑ Cross-Validation Fold Scores', fontsize=13, fontweight='bold', pad=10)\nax4.set_xticks(fold_nums)\nax4.legend(fontsize=8, loc='lower right', framealpha=0.9)\nax4.grid(axis='y', alpha=0.3, linestyle='--')\n\n# =================== ROW 2 ===================\n# 5. Target Distribution\nax5 = plt.subplot(3, 4, 5)\ntarget_counts = train['loan_paid_back'].value_counts()\ncolors_pie = ['#FF6B6B', '#51CF66']\nwedges, texts, autotexts = ax5.pie(target_counts, labels=['Not Paid', 'Paid Back'], \n                                     autopct='%1.1f%%', colors=colors_pie, startangle=90,\n                                     textprops={'fontsize': 11, 'fontweight': 'bold'},\n                                     explode=(0.05, 0.05), shadow=True)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(12)\nax5.set_title('üéØ Target Distribution', fontsize=13, fontweight='bold', pad=10)\n\n# 6. Credit Score Distribution by Target\nax6 = plt.subplot(3, 4, 6)\npaid = train[train['loan_paid_back']==1]['credit_score']\nnot_paid = train[train['loan_paid_back']==0]['credit_score']\nax6.hist(paid, bins=50, alpha=0.7, label='Paid Back', color='#51CF66', edgecolor='black', linewidth=0.5)\nax6.hist(not_paid, bins=50, alpha=0.7, label='Not Paid', color='#FF6B6B', edgecolor='black', linewidth=0.5)\nax6.axvline(paid.mean(), color='green', linestyle='--', linewidth=2, label=f'Paid Mean: {paid.mean():.0f}')\nax6.axvline(not_paid.mean(), color='red', linestyle='--', linewidth=2, label=f'Not Paid Mean: {not_paid.mean():.0f}')\nax6.set_xlabel('Credit Score', fontsize=11, fontweight='bold')\nax6.set_ylabel('Frequency', fontsize=11, fontweight='bold')\nax6.set_title('üí≥ Credit Score Distribution', fontsize=13, fontweight='bold', pad=10)\nax6.legend(fontsize=9, framealpha=0.9)\nax6.grid(alpha=0.3, linestyle='--')\n\n# 7. Debt-to-Income Ratio Distribution\nax7 = plt.subplot(3, 4, 7)\npaid_dti = train[train['loan_paid_back']==1]['debt_to_income_ratio']\nnot_paid_dti = train[train['loan_paid_back']==0]['debt_to_income_ratio']\nax7.hist(paid_dti, bins=50, alpha=0.7, label='Paid Back', color='#51CF66', edgecolor='black', linewidth=0.5)\nax7.hist(not_paid_dti, bins=50, alpha=0.7, label='Not Paid', color='#FF6B6B', edgecolor='black', linewidth=0.5)\nax7.axvline(paid_dti.mean(), color='green', linestyle='--', linewidth=2, label=f'Paid Mean: {paid_dti.mean():.2f}')\nax7.axvline(not_paid_dti.mean(), color='red', linestyle='--', linewidth=2, label=f'Not Paid Mean: {not_paid_dti.mean():.2f}')\nax7.set_xlabel('Debt-to-Income Ratio', fontsize=11, fontweight='bold')\nax7.set_ylabel('Frequency', fontsize=11, fontweight='bold')\nax7.set_title('üí∞ Debt-to-Income Distribution', fontsize=13, fontweight='bold', pad=10)\nax7.legend(fontsize=9, framealpha=0.9)\nax7.grid(alpha=0.3, linestyle='--')\n\n# 8. Interest Rate vs Credit Score\nax8 = plt.subplot(3, 4, 8)\npaid_sample = train[train['loan_paid_back']==1].sample(n=min(5000, len(train[train['loan_paid_back']==1])), random_state=42)\nnot_paid_sample = train[train['loan_paid_back']==0].sample(n=min(5000, len(train[train['loan_paid_back']==0])), random_state=42)\nax8.scatter(paid_sample['credit_score'], paid_sample['interest_rate'], \n           alpha=0.4, s=10, color='#51CF66', label='Paid Back')\nax8.scatter(not_paid_sample['credit_score'], not_paid_sample['interest_rate'], \n           alpha=0.4, s=10, color='#FF6B6B', label='Not Paid')\nax8.set_xlabel('Credit Score', fontsize=11, fontweight='bold')\nax8.set_ylabel('Interest Rate (%)', fontsize=11, fontweight='bold')\nax8.set_title('üìâ Interest Rate vs Credit Score', fontsize=13, fontweight='bold', pad=10)\nax8.legend(fontsize=9, framealpha=0.9)\nax8.grid(alpha=0.3, linestyle='--')\n\n# =================== ROW 3 ===================\n# 9. Prediction Distribution\nax9 = plt.subplot(3, 4, 9)\nax9.hist(test_ensemble, bins=50, color='#4ECDC4', alpha=0.8, edgecolor='black', linewidth=1)\nax9.axvline(test_ensemble.mean(), color='red', linestyle='--', linewidth=2, \n           label=f'Mean: {test_ensemble.mean():.4f}')\nax9.axvline(np.median(test_ensemble), color='orange', linestyle='--', linewidth=2, \n           label=f'Median: {np.median(test_ensemble):.4f}')\nax9.set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')\nax9.set_ylabel('Frequency', fontsize=11, fontweight='bold')\nax9.set_title('üé≤ Test Prediction Distribution', fontsize=13, fontweight='bold', pad=10)\nax9.legend(fontsize=9, framealpha=0.9)\nax9.grid(axis='y', alpha=0.3, linestyle='--')\n\n# 10. Employment Status Impact\nax10 = plt.subplot(3, 4, 10)\nemp_payback = train.groupby('employment_status')['loan_paid_back'].agg(['mean', 'count'])\nemp_payback = emp_payback.sort_values('mean', ascending=True)\nbars = ax10.barh(emp_payback.index, emp_payback['mean'], \n                color=plt.cm.RdYlGn(emp_payback['mean']), alpha=0.8, edgecolor='black', linewidth=1)\nax10.set_xlabel('Payback Rate', fontsize=11, fontweight='bold')\nax10.set_title('üëî Employment Status Impact', fontsize=13, fontweight='bold', pad=10)\nax10.grid(axis='x', alpha=0.3, linestyle='--')\nfor i, (idx, row) in enumerate(emp_payback.iterrows()):\n    ax10.text(row['mean'], i, f\" {row['mean']:.2%} (n={row['count']:,})\", \n             va='center', fontsize=9, fontweight='bold')\n\n# 11. Loan Purpose Impact\nax11 = plt.subplot(3, 4, 11)\npurpose_payback = train.groupby('loan_purpose')['loan_paid_back'].agg(['mean', 'count'])\npurpose_payback = purpose_payback.sort_values('mean', ascending=False)\nbars = ax11.bar(range(len(purpose_payback)), purpose_payback['mean'], \n               color=plt.cm.viridis(np.linspace(0.2, 0.9, len(purpose_payback))), \n               alpha=0.8, edgecolor='black', linewidth=1)\nax11.set_xticks(range(len(purpose_payback)))\nax11.set_xticklabels(purpose_payback.index, rotation=45, ha='right', fontsize=9)\nax11.set_ylabel('Payback Rate', fontsize=11, fontweight='bold')\nax11.set_title('üéØ Loan Purpose Impact', fontsize=13, fontweight='bold', pad=10)\nax11.grid(axis='y', alpha=0.3, linestyle='--')\nfor i, (idx, row) in enumerate(purpose_payback.iterrows()):\n    ax11.text(i, row['mean'] + 0.01, f\"{row['mean']:.2%}\", ha='center', fontsize=8, fontweight='bold')\n\n# 12. Model Weights Visualization\nax12 = plt.subplot(3, 4, 12)\nweights_data = {\n    'XGBoost': best_weights[0],\n    'LightGBM': best_weights[1],\n    'CatBoost': best_weights[2]\n}\nwedges, texts, autotexts = ax12.pie(weights_data.values(), labels=weights_data.keys(), \n                                      autopct='%1.1f%%', colors=colors_models[:3], startangle=90,\n                                      textprops={'fontsize': 11, 'fontweight': 'bold'},\n                                      explode=(0.05, 0.05, 0.05), shadow=True)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(12)\nax12.set_title('‚öñÔ∏è Optimized Ensemble Weights', fontsize=13, fontweight='bold', pad=10)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('elite_loan_prediction_dashboard.png', dpi=300, bbox_inches='tight', facecolor='white')\nprint(\"\\n‚úì Main dashboard saved: 'elite_loan_prediction_dashboard.png'\")\nplt.show()\n\n# Additional Visualization: Correlation Heatmap\nprint(\"\\n‚öôÔ∏è  Generating correlation heatmap...\")\nfig2, ax = plt.subplots(figsize=(16, 14))\n# Select key features for correlation\nkey_features = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', \n                'interest_rate', 'income_to_loan_ratio', 'credit_risk_v1', \n                'payment_to_income_ratio', 'grade_rank', 'loan_paid_back']\ncorr_data = train_fe[key_features].corr()\nmask = np.triu(np.ones_like(corr_data, dtype=bool))\nsns.heatmap(corr_data, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n            ax=ax, vmin=-1, vmax=1)\nax.set_title('üî• Feature Correlation Heatmap - Key Variables', \n            fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight', facecolor='white')\nprint(\"‚úì Correlation heatmap saved: 'correlation_heatmap.png'\")\nplt.show()\n\n# Additional Visualization: Grade Distribution\nprint(\"\\n‚öôÔ∏è  Generating grade distribution analysis...\")\nfig3, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig3.suptitle('üìä Credit Grade Analysis', fontsize=18, fontweight='bold', y=0.995)\n\n# Grade letter distribution\nax_g1 = axes[0, 0]\ngrade_dist = train['grade_subgrade'].str[0].value_counts().sort_index()\nbars = ax_g1.bar(grade_dist.index, grade_dist.values, \n                color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(grade_dist))),\n                alpha=0.8, edgecolor='black', linewidth=1.5)\nax_g1.set_xlabel('Grade Letter', fontsize=12, fontweight='bold')\nax_g1.set_ylabel('Count', fontsize=12, fontweight='bold')\nax_g1.set_title('Grade Letter Distribution', fontsize=13, fontweight='bold')\nax_g1.grid(axis='y', alpha=0.3, linestyle='--')\nfor bar in bars:\n    height = bar.get_height()\n    ax_g1.text(bar.get_x() + bar.get_width()/2., height,\n              f'{int(height):,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\n# Grade vs Payback Rate\nax_g2 = axes[0, 1]\ngrade_payback = train_fe.groupby('grade_letter')['loan_paid_back'].agg(['mean', 'count'])\ngrade_payback = grade_payback.sort_index()\nax_g2_twin = ax_g2.twinx()\nbars = ax_g2.bar(grade_payback.index, grade_payback['mean'], \n               color=plt.cm.RdYlGn(grade_payback['mean']), alpha=0.8, \n               edgecolor='black', linewidth=1.5, label='Payback Rate')\nline = ax_g2_twin.plot(grade_payback.index, grade_payback['count'], \n                      color='red', marker='o', linewidth=2, markersize=8, label='Count')\nax_g2.set_xlabel('Grade Letter', fontsize=12, fontweight='bold')\nax_g2.set_ylabel('Payback Rate', fontsize=12, fontweight='bold', color='black')\nax_g2_twin.set_ylabel('Count', fontsize=12, fontweight='bold', color='red')\nax_g2.set_title('Grade vs Payback Rate', fontsize=13, fontweight='bold')\nax_g2.set_ylim([0.5, 1.0])\nax_g2.grid(axis='y', alpha=0.3, linestyle='--')\nax_g2.legend(loc='upper left', fontsize=9)\nax_g2_twin.legend(loc='upper right', fontsize=9)\n\n# Interest Rate by Grade\nax_g3 = axes[1, 0]\ngrade_interest = train_fe.groupby('grade_letter')['interest_rate'].mean().sort_index()\nbars = ax_g3.bar(grade_interest.index, grade_interest.values,\n               color=plt.cm.plasma(np.linspace(0.2, 0.9, len(grade_interest))),\n               alpha=0.8, edgecolor='black', linewidth=1.5)\nax_g3.set_xlabel('Grade Letter', fontsize=12, fontweight='bold')\nax_g3.set_ylabel('Average Interest Rate (%)', fontsize=12, fontweight='bold')\nax_g3.set_title('Interest Rate by Grade', fontsize=13, fontweight='bold')\nax_g3.grid(axis='y', alpha=0.3, linestyle='--')\nfor bar, val in zip(bars, grade_interest.values):\n    ax_g3.text(bar.get_x() + bar.get_width()/2., val,\n              f'{val:.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\n# Credit Score by Grade\nax_g4 = axes[1, 1]\ngrade_credit = train_fe.groupby('grade_letter')['credit_score'].mean().sort_index()\nbars = ax_g4.bar(grade_credit.index, grade_credit.values,\n               color=plt.cm.viridis(np.linspace(0.2, 0.9, len(grade_credit))),\n               alpha=0.8, edgecolor='black', linewidth=1.5)\nax_g4.set_xlabel('Grade Letter', fontsize=12, fontweight='bold')\nax_g4.set_ylabel('Average Credit Score', fontsize=12, fontweight='bold')\nax_g4.set_title('Credit Score by Grade', fontsize=13, fontweight='bold')\nax_g4.grid(axis='y', alpha=0.3, linestyle='--')\nfor bar, val in zip(bars, grade_credit.values):\n    ax_g4.text(bar.get_x() + bar.get_width()/2., val,\n              f'{val:.0f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('grade_analysis.png', dpi=300, bbox_inches='tight', facecolor='white')\nprint(\"‚úì Grade analysis saved: 'grade_analysis.png'\")\nplt.show()\n\n# =============================================================================\n# 8. FEATURE IMPORTANCE\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" ‚≠ê PHASE 7: TOP FEATURES\".center(85))\nprint(\"=\"*85)\n\nprint(\"\\nüèÜ Top 20 Most Important Features:\")\nfi_table = [[feat, f'{imp:.6f}'] for feat, imp in fi_agg.items()]\nprint(tabulate(fi_table, headers=['Feature', 'Importance'], tablefmt='fancy_grid'))\n\n# =============================================================================\n# 9. SUBMISSION\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" üíæ PHASE 8: CREATING SUBMISSION\".center(85))\nprint(\"=\"*85)\n\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'loan_paid_back': test_ensemble\n})\n\nsubmission.to_csv('submission_elite.csv', index=False)\n\nprint(f\"\\n‚úì Submission created: submission_elite.csv\")\nprint(f\"‚úì Shape: {submission.shape}\")\nprint(f\"\\nüìä Prediction Statistics:\")\npred_stats = [\n    ['Mean', f\"{test_ensemble.mean():.6f}\"],\n    ['Median', f\"{np.median(test_ensemble):.6f}\"],\n    ['Min', f\"{test_ensemble.min():.6f}\"],\n    ['Max', f\"{test_ensemble.max():.6f}\"],\n    ['Std', f\"{test_ensemble.std():.6f}\"]\n]\nprint(tabulate(pred_stats, headers=['Statistic', 'Value'], tablefmt='grid'))\n\nprint(\"\\n\" + tabulate(submission.head(10), headers='keys', tablefmt='grid', showindex=False))\n\n# =============================================================================\n# FINAL SUMMARY\n# =============================================================================\nprint(\"\\n\" + \"=\"*85)\nprint(\" üéâ ELITE PIPELINE COMPLETE\".center(85))\nprint(\"=\"*85)\n\nsummary = [\n    ['Features Engineered', f\"{X_train.shape[1]}\"],\n    ['Models in Ensemble', '3 (XGBoost + LightGBM + CatBoost)'],\n    ['CV Strategy', '5-Fold Stratified'],\n    ['Best OOF Score', f\"{best_score:.6f}\"],\n    ['Expected LB', f\"~{best_score - 0.001:.4f} to {best_score + 0.001:.4f}\"],\n    ['Improvement vs Baseline', f\"+{(best_score - 0.9152)*100:.2f}%\"]\n]\n\nprint(\"\\n\" + tabulate(summary, headers=['Metric', 'Value'], tablefmt='fancy_grid'))\nprint(\"\\n\" + \"=\"*85)\nprint(\" üöÄ Ready for Kaggle submission!\".center(85))\nprint(\"=\"*85)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T20:32:56.772523Z","iopub.execute_input":"2025-11-02T20:32:56.772825Z","iopub.status.idle":"2025-11-02T20:33:24.64768Z","shell.execute_reply.started":"2025-11-02T20:32:56.772805Z","shell.execute_reply":"2025-11-02T20:33:24.646037Z"}},"outputs":[],"execution_count":null}]}