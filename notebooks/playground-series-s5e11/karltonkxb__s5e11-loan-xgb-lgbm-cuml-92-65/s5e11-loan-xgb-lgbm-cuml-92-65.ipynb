{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":216.441762,"end_time":"2025-11-01T07:09:40.502238","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-01T07:06:04.060476","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna, json\n\nimport lightgbm as lgb\n\nfrom itertools import combinations\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom cuml import LogisticRegression\nfrom cuml.preprocessing.TargetEncoder import TargetEncoder\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-10T16:19:01.448073Z","iopub.execute_input":"2025-11-10T16:19:01.448393Z","iopub.status.idle":"2025-11-10T16:19:01.453538Z","shell.execute_reply.started":"2025-11-10T16:19:01.448369Z","shell.execute_reply":"2025-11-10T16:19:01.452892Z"},"papermill":{"duration":0.010917,"end_time":"2025-11-01T07:06:07.577035","exception":false,"start_time":"2025-11-01T07:06:07.566118","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2025-11-10T16:19:01.455348Z","iopub.execute_input":"2025-11-10T16:19:01.455573Z","iopub.status.idle":"2025-11-10T16:19:02.467427Z","shell.execute_reply.started":"2025-11-10T16:19:01.455558Z","shell.execute_reply":"2025-11-10T16:19:02.466723Z"},"papermill":{"duration":3.199948,"end_time":"2025-11-01T07:06:10.779255","exception":false,"start_time":"2025-11-01T07:06:07.579307","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# downcast numerical columns\n\ndef downcasting(data: pd.DataFrame, verbose: bool=True) -> pd.DataFrame:\n\n    mem_before = data.memory_usage().sum() / 1024**2\n    if verbose:\n        print(f\"Memory usage of dataframe is {mem_before:.2f} MB\")\n            \n    for col in data.select_dtypes(include=[\"number\"]).columns:\n        if pd.api.types.is_integer_dtype(data[col]):\n            data[col] = pd.to_numeric(data[col], downcast=\"integer\")\n        \n        elif pd.api.types.is_float_dtype(data[col]):\n            data[col] = pd.to_numeric(data[col], downcast=\"float\")\n\n    mem_after = data.memory_usage().sum() / 1024**2\n    if verbose:\n        print(f\"Memory usage after optimization is: {mem_after:.2f} MB\")\n        print(f\"Decreased by {(100 * (mem_before - mem_after) / mem_before):.1f}%\\n\")\n\n    \n    return data\n\n# train = downcasting(train)\n# test = downcasting(test)\n# orig = downcasting(orig)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:02.468611Z","iopub.execute_input":"2025-11-10T16:19:02.468844Z","iopub.status.idle":"2025-11-10T16:19:02.47441Z","shell.execute_reply.started":"2025-11-10T16:19:02.468818Z","shell.execute_reply":"2025-11-10T16:19:02.473812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:02.47502Z","iopub.execute_input":"2025-11-10T16:19:02.475257Z","iopub.status.idle":"2025-11-10T16:19:02.495984Z","shell.execute_reply.started":"2025-11-10T16:19:02.475232Z","shell.execute_reply":"2025-11-10T16:19:02.495352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:02.496668Z","iopub.execute_input":"2025-11-10T16:19:02.496924Z","iopub.status.idle":"2025-11-10T16:19:02.516537Z","shell.execute_reply.started":"2025-11-10T16:19:02.496904Z","shell.execute_reply":"2025-11-10T16:19:02.515818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Analaysis","metadata":{}},{"cell_type":"code","source":"target = 'loan_paid_back'\ncats = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n\n# collect only common columns\ncommon_columns = [col for col in train.columns if col not in ['id', target]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:02.518205Z","iopub.execute_input":"2025-11-10T16:19:02.518441Z","iopub.status.idle":"2025-11-10T16:19:02.526248Z","shell.execute_reply.started":"2025-11-10T16:19:02.518425Z","shell.execute_reply":"2025-11-10T16:19:02.525631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to compare train and test dataframes\ndef compare_dataframes(train_df, test_df, cols):\n    report_data = []\n    \n    all_cols = cols\n\n    for col in all_cols:\n        row_data = {'Column': col}\n        \n        if col in train_df.columns and col in test_df.columns:\n            if pd.api.types.is_numeric_dtype(train_df[col]):\n                train_stats = train_df[col].describe()\n                test_stats = test_df[col].describe()\n                \n                row_data.update({\n                    'Data Type': 'Numeric',\n                    'Train Mean': train_stats['mean'],\n                    'Test Mean': test_stats['mean'],\n                    'Train Median': train_stats['50%'],\n                    'Test Median': test_stats['50%'],\n                    'Train Std': train_stats['std'],\n                    'Test Std': test_stats['std']\n                })\n            else:\n                train_counts = train_df[col].value_counts(normalize=True)\n                test_counts = test_df[col].value_counts(normalize=True)\n                \n                row_data.update({\n                    'Data Type': 'Categorical',\n                    'Train Unique': len(train_df[col].unique()),\n                    'Test Unique': len(test_df[col].unique()),\n                    'Train Mode': train_df[col].mode()[0],\n                    'Test Mode': test_df[col].mode()[0]\n                })\n\n            if 'Train Mean' in row_data:\n                mean_diff_pct = (abs(row_data['Train Mean'] - row_data['Test Mean']) / \n                                 np.mean([row_data['Train Mean'], row_data['Test Mean']])) * 100\n                row_data['Mean Diff %'] = f\"{mean_diff_pct:.2f}%\"\n            \n            row_data['Train Count'] = train_df[col].count()\n            row_data['Test Count'] = test_df[col].count()\n        \n        report_data.append(row_data)\n\n    report_df = pd.DataFrame(report_data).apply(lambda x: round(x, 2), axis = 0)\n    return report_df.set_index('Column')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:02.526815Z","iopub.execute_input":"2025-11-10T16:19:02.527003Z","iopub.status.idle":"2025-11-10T16:19:02.535492Z","shell.execute_reply.started":"2025-11-10T16:19:02.526989Z","shell.execute_reply":"2025-11-10T16:19:02.53475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"report = compare_dataframes(train, test, common_columns)\nreport","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:02.536195Z","iopub.execute_input":"2025-11-10T16:19:02.53643Z","iopub.status.idle":"2025-11-10T16:19:03.780145Z","shell.execute_reply.started":"2025-11-10T16:19:02.536405Z","shell.execute_reply":"2025-11-10T16:19:03.779362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compare_dataframes(train, orig, common_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:03.781029Z","iopub.execute_input":"2025-11-10T16:19:03.781398Z","iopub.status.idle":"2025-11-10T16:19:04.681389Z","shell.execute_reply.started":"2025-11-10T16:19:03.781374Z","shell.execute_reply":"2025-11-10T16:19:04.680678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\nnum_features = len(cols)\n\nfig, axs = plt.subplots(2, 5, figsize=(20, 10))\n\nfig.suptitle('Feature Distribution Analysis', fontsize=18, y=1.02)\n\nfor i in range(num_features):\n    sns.histplot(\n        data=train,\n        x=cols[i],\n        bins=20,\n        ax=axs[0, i],\n        color='lightgreen',\n        edgecolor='black',\n        kde=True\n    )\n    axs[0, i].set_title(cols[i])\n\n\n# try logged version of columns\ntrain_copy = train.copy(deep=True)\nfor col in cols:\n    train_copy[col] = np.log1p(train_copy[col])\n    \nfor i in range(num_features):\n    sns.histplot(\n        data=train_copy,\n        x=cols[i],\n        bins=20,\n        ax=axs[1, i],\n        color='lightgreen',\n        edgecolor='black',\n        kde=True\n    )\n    axs[1, i].set_title(f\"{cols[i]} - logged verion\")\n\n\n\nplt.tight_layout(rect=[0, 0, 1, 0.98]) \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:04.682221Z","iopub.execute_input":"2025-11-10T16:19:04.682518Z","iopub.status.idle":"2025-11-10T16:19:27.7282Z","shell.execute_reply.started":"2025-11-10T16:19:04.682495Z","shell.execute_reply":"2025-11-10T16:19:27.727368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split grade_subgrade into 2 columns \ntrain['grade'] = train['grade_subgrade'].str[0]\ntest['grade'] = test['grade_subgrade'].str[0]\norig['grade'] = orig['grade_subgrade'].str[0]\n\ntrain['grade1'] = train['grade_subgrade'].str[1]\ntest['grade1'] = test['grade_subgrade'].str[1]\norig['grade1'] = orig['grade_subgrade'].str[1]\n\n# add to common columns\ncommon_columns.append('grade')\ncommon_columns.append('grade1')\n\nprint(\"<------ Train ------>\\n\")\nfor col in common_columns:\n    print(f\"{col}: {train[col].nunique()} unique values\")\n\nprint('\\n<------ Test ------>\\n')\nfor col in common_columns:\n    print(f\"{col}: {test[col].nunique()} unique values\")\n\nprint('\\n<------ Orig ------>\\n')\nfor col in common_columns:\n    print(f\"{col}: {orig[col].nunique()} unique values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:27.72899Z","iopub.execute_input":"2025-11-10T16:19:27.7292Z","iopub.status.idle":"2025-11-10T16:19:28.461842Z","shell.execute_reply.started":"2025-11-10T16:19:27.729183Z","shell.execute_reply":"2025-11-10T16:19:28.461106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\ncorr = train.corr(numeric_only= True).round(3)\n\nsns.heatmap(corr, cmap = 'crest', annot = True)\nplt.title('Non categorical Feature correlation Heatmap', fontsize = 15, pad=10)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:28.462551Z","iopub.execute_input":"2025-11-10T16:19:28.462739Z","iopub.status.idle":"2025-11-10T16:19:28.862356Z","shell.execute_reply.started":"2025-11-10T16:19:28.462726Z","shell.execute_reply":"2025-11-10T16:19:28.861693Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering\n\nThe below feature engineering is used from the Chris Deotte notebook: [https://www.kaggle.com/code/cdeotte/xgboost-using-original-data-cv-0-976](https://www.kaggle.com/code/cdeotte/xgboost-using-original-data-cv-0-976)","metadata":{}},{"cell_type":"code","source":"# credit score categories\n\ndef map_fico_tier(score):\n    \"\"\"Maps a credit score to its corresponding FICO tier.\"\"\"\n    if score >= 800:\n        return 'Exceptional'\n    elif score >= 740:\n        return 'Very Good'\n    elif score >= 670:\n        return 'Good'\n    elif score >= 580:\n        return 'Fair'\n    else: # Below 580\n        return 'Poor'\n\ndef map_vantage_tier(score):\n    \"\"\"Maps a credit score to its corresponding VantageScore tier.\"\"\"\n    if score >= 781:\n        return 'Excellent'\n    elif score >= 661:\n        return 'Good'\n    elif score >= 601:\n        return 'Fair'\n    elif score >= 500:\n        return 'Poor'\n    else: # Below 500\n        return 'Very Poor'\n\n# Creates two new categorical features based on FICO and VantageScore ranges \n# using the existing 'credit_score' column in both train and test DataFrames.\n\nfor data in [train, test, orig]:\n    data['credit_score_FICO_tier'] = data['credit_score'].apply(map_fico_tier)\n    data['credit_score_Vantage_tier'] = data['credit_score'].apply(map_vantage_tier)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:28.863209Z","iopub.execute_input":"2025-11-10T16:19:28.863481Z","iopub.status.idle":"2025-11-10T16:19:29.177627Z","shell.execute_reply.started":"2025-11-10T16:19:28.863458Z","shell.execute_reply":"2025-11-10T16:19:29.176986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ROUND = []\nrounding_levels = {'1s': 0, '10s': -1, '100s': -2}\n\nfor col in ['annual_income', 'loan_amount']:\n    for suffix, level in rounding_levels.items():\n        new_col_name = f'{col}_ROUND_{suffix}'\n        ROUND.append(new_col_name)\n        for df in [train, test, orig]:\n            df[new_col_name] = df[col].round(level).astype(int)\n\nprint(f'{len(ROUND)} ROUND Features created.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:29.178403Z","iopub.execute_input":"2025-11-10T16:19:29.178927Z","iopub.status.idle":"2025-11-10T16:19:29.231603Z","shell.execute_reply.started":"2025-11-10T16:19:29.1789Z","shell.execute_reply":"2025-11-10T16:19:29.230846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# let's check newly created columns unique values\nrounded_cols = ['annual_income', 'loan_amount', 'annual_income_ROUND_1s', 'annual_income_ROUND_10s', 'annual_income_ROUND_100s',\n                'loan_amount_ROUND_1s', 'loan_amount_ROUND_10s', 'loan_amount_ROUND_100s']\n\nfor col in rounded_cols:\n    print(f\"{col}: {train[col].nunique()} unique values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:19:29.233545Z","iopub.execute_input":"2025-11-10T16:19:29.233802Z","iopub.status.idle":"2025-11-10T16:19:29.284746Z","shell.execute_reply.started":"2025-11-10T16:19:29.233786Z","shell.execute_reply":"2025-11-10T16:19:29.284094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test['loan_paid_back'] = -1\n\ncombine_cols = ['annual_income', 'loan_amount', 'debt_to_income_ratio', 'credit_score', 'grade', 'grade1',\n        'annual_income_ROUND_1s', 'annual_income_ROUND_10s', 'annual_income_ROUND_100s',\n        'loan_amount_ROUND_1s', 'loan_amount_ROUND_10s', 'loan_amount_ROUND_100s',\n       'interest_rate', 'gender', 'marital_status', 'education_level', 'credit_score_FICO_tier', 'credit_score_Vantage_tier',\n       'employment_status', 'loan_purpose', 'grade_subgrade', 'loan_paid_back',]\n\ncombine = pd.concat([train[combine_cols], test[combine_cols], orig[combine_cols]],axis=0)\n\n\nCATS = []\nNUMS = []\nfor c in combine.columns[:-1]:\n    t = \"CAT\"\n    if combine[c].dtype=='object':\n        CATS.append(c)\n    else:\n        if c not in rounded_cols: # don't mix high cardinality cols\n            NUMS.append(c) \n        t = \"NUM\"\n    n = combine[c].nunique()\n    na = combine[c].isna().sum()\n    print(f\"[{t}] {c} has {n} unique and {na} NA\")\nprint(\"CATS:\", CATS )\nprint(\"NUMS:\", NUMS )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:23:51.071781Z","iopub.execute_input":"2025-11-10T16:23:51.072205Z","iopub.status.idle":"2025-11-10T16:23:52.100103Z","shell.execute_reply.started":"2025-11-10T16:23:51.072164Z","shell.execute_reply":"2025-11-10T16:23:52.099438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (LE - Label Encode)\n\nWe will label encode all categorical columns. Also we will make a duplicate of each numerical column and treat the copy as a categorical column.\n","metadata":{}},{"cell_type":"code","source":"CATS1 = []\nSIZES = {}\nfor c in NUMS + CATS:\n    n = c\n    if c in NUMS: \n        n = f\"{c}2\"\n        CATS1.append(n)\n    combine[n],_ = combine[c].factorize()\n    SIZES[n] = combine[n].max()+1\n\n    combine[c] = combine[c].astype('int32')\n    combine[n] = combine[n].astype('int32')\n\nprint(\"New CATS:\", CATS1 )\nprint(\"Cardinality of all CATS:\", SIZES )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:23:58.551779Z","iopub.execute_input":"2025-11-10T16:23:58.552297Z","iopub.status.idle":"2025-11-10T16:23:59.026624Z","shell.execute_reply.started":"2025-11-10T16:23:58.552272Z","shell.execute_reply":"2025-11-10T16:23:59.025757Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (Combine Column Pairs)\n\nWe will create a new categorical column from every pair of existing categorical columns. The original categorical columns have been label encoded into integers from 0 to N-1 each. Therefore we can create a new column with unique integers using the formula **new_cols[name] = combine[c1] * SIZES[c2] + combine[c2].**\n","metadata":{}},{"cell_type":"code","source":"pairs = combinations(CATS + CATS1, 2)\nnew_cols = {}\nCATS2 = []\n\nfor c1, c2 in pairs:\n    name = \"_\".join(sorted((c1, c2)))\n    new_cols[name] = combine[c1] * SIZES[c2] + combine[c2]\n    CATS2.append(name)\nif new_cols:\n    new_df = pd.DataFrame(new_cols)         \n    combine = pd.concat([combine, new_df], axis=1) \n\nprint(f\"Created {len(CATS2)} new CAT columns\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:25:11.372646Z","iopub.execute_input":"2025-11-10T16:25:11.373441Z","iopub.status.idle":"2025-11-10T16:25:11.946272Z","shell.execute_reply.started":"2025-11-10T16:25:11.373414Z","shell.execute_reply":"2025-11-10T16:25:11.945579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (CE - Count Encoding)\nWe now have 55 categorical columns. We will count encode each of them and create 55 new columns.","metadata":{}},{"cell_type":"code","source":"\nCE_FEATS = CATS + CATS1 + CATS2\nCE = []\nnew_cols = {}\nfor col in CE_FEATS:\n    nm_col = f\"CE_{col.upper()}\"\n    if nm_col not in combine.columns:\n        new_cols[nm_col] = combine.groupby(col)[\"loan_paid_back\"].transform(\"count\").astype(\"int32\")\n        CE.append(nm_col)\ntmp_df = pd.DataFrame(new_cols)\ncombine = pd.concat([combine, tmp_df], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:25:15.000598Z","iopub.execute_input":"2025-11-10T16:25:15.00118Z","iopub.status.idle":"2025-11-10T16:25:16.653153Z","shell.execute_reply.started":"2025-11-10T16:25:15.001152Z","shell.execute_reply":"2025-11-10T16:25:16.652301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = combine.iloc[:len(train)]\ntest = combine.iloc[len(train):len(train)+len(test)]\norig = combine.iloc[-len(orig):]\ndel combine\nprint(\"Train shape\", train.shape,\"Test shape\", test.shape,\"Original shape\", orig.shape )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:25:41.891418Z","iopub.execute_input":"2025-11-10T16:25:41.89215Z","iopub.status.idle":"2025-11-10T16:25:41.903492Z","shell.execute_reply.started":"2025-11-10T16:25:41.892123Z","shell.execute_reply":"2025-11-10T16:25:41.90277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = NUMS+rounded_cols+CATS+CATS1+CATS2+CE\nprint(f\"We have {len(FEATURES)} features.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:26:09.755148Z","iopub.execute_input":"2025-11-10T16:26:09.755733Z","iopub.status.idle":"2025-11-10T16:26:09.760028Z","shell.execute_reply.started":"2025-11-10T16:26:09.755708Z","shell.execute_reply":"2025-11-10T16:26:09.759373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loader for QuantileDMatrix\nWe need the following data loader to use XGB's memory efficient `QuantileDMatrix`. This data loader yields batches of data on GPU for XGB.","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nclass IterLoadForDMatrix(xgb.core.DataIter):\n    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n        self.features = features\n        self.target = target\n        self.df = df\n        self.it = 0 \n        self.batch_size = batch_size\n        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n        super().__init__()\n\n    def reset(self):\n        '''Reset the iterator'''\n        self.it = 0\n\n    def next(self, input_data):\n        '''Yield next batch of data.'''\n        if self.it == self.batches:\n            return 0 # Return 0 when there's no more batch.\n        \n        a = self.it * self.batch_size\n        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n        #dt = cudf.DataFrame(self.df.iloc[a:b])\n        dt = self.df.iloc[a:b]\n        input_data(data=dt[self.features], label=dt[self.target]) \n        self.it += 1\n        return 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:26:13.00027Z","iopub.execute_input":"2025-11-10T16:26:13.000999Z","iopub.status.idle":"2025-11-10T16:26:13.0072Z","shell.execute_reply.started":"2025-11-10T16:26:13.000978Z","shell.execute_reply":"2025-11-10T16:26:13.006464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train XGB w/ Original Data as Rows\nWe will now train XGBoost with adding original data as new rows. Inside each KFold for loop iteration, we will concatenate the original data to the train data as new rows. \n\nWe will also use a `QuantileDMatrix` instead of a `DMatrix`. This reduces memory usage because the data will be loaded into GPU VRAM in batches and compressed in batches. This is different than the basic `DMatrix` which tries to load all the train data into GPU VRAM at the same time which spikes the memory and can cause OOM.","metadata":{}},{"cell_type":"code","source":"FEATURES = NUMS+rounded_cols+CATS+CATS1+CATS2+CE\nprint(f\"We have {len(FEATURES)} features.\")\n\nFOLDS = 5\nSEED = 42\n\nparams = {\n    \"objective\": \"binary:logistic\",  \n    \"eval_metric\": \"auc\",           \n    \"learning_rate\": 0.01,\n    \"max_depth\": 0,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.7,\n    \"seed\": SEED,\n    \"device\": \"cuda\",\n    \"grow_policy\": \"lossguide\", \n    \"max_leaves\": 32,          \n\n    'scale_pos_weight': 0.8, # usefull for unbalanced data\n    \"min_samples_split\": 5,\n    'lambda': 5.0, \n    'alpha': 2.5,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:28:09.344621Z","iopub.execute_input":"2025-11-10T16:28:09.345369Z","iopub.status.idle":"2025-11-10T16:28:09.35048Z","shell.execute_reply.started":"2025-11-10T16:28:09.345332Z","shell.execute_reply":"2025-11-10T16:28:09.349763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_preds = np.zeros(len(train))\ntest_preds = np.zeros(len(test))\n\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n    print(\"#\"*25)\n    print(f\"### Fold {fold+1} ###\")\n    print(\"#\"*25)\n\n    Xy_train = train.iloc[train_idx][ FEATURES+['loan_paid_back'] ].copy()\n    Xy_more = orig[ FEATURES+['loan_paid_back'] ]\n    for k in range(1):\n        Xy_train = pd.concat([Xy_train,Xy_more],axis=0,ignore_index=True)\n    \n    X_valid = train.iloc[val_idx][FEATURES].copy()\n    y_valid = train.iloc[val_idx]['loan_paid_back']\n    X_test = test[FEATURES].copy()\n\n    CC = CATS1+CATS2\n    print(f\"Target encoding {len(CC)} features... \",end=\"\")\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n        TE0 = TargetEncoder(n_folds=10, smooth=1, split_method='random', stat='mean')\n        Xy_train[c] = TE0.fit_transform(Xy_train[c],Xy_train['loan_paid_back']).astype('float32')\n        X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        X_test[c] = TE0.transform(X_test[c]).astype('float32')\n    print()\n\n    Xy_train[CATS] = Xy_train[CATS].astype('category')\n    X_valid[CATS] = X_valid[CATS].astype('category')\n    X_test[CATS] = X_test[CATS].astype('category')\n\n    Xy_train = IterLoadForDMatrix(Xy_train, FEATURES, 'loan_paid_back')\n    dtrain = xgb.QuantileDMatrix(Xy_train, enable_categorical=True, max_bin=256)\n    dval   = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n    dtest  = xgb.DMatrix(X_test, enable_categorical=True)\n\n    model = xgb.train(\n        params=params,\n        dtrain=dtrain,\n        num_boost_round=10_000,\n        evals=[(dtrain, \"train\"), (dval, \"valid\")],\n        early_stopping_rounds=300,\n        verbose_eval=300\n    )\n\n    oof_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n    test_preds += model.predict(dtest, iteration_range=(0, model.best_iteration + 1)) / FOLDS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:28:17.854392Z","iopub.execute_input":"2025-11-10T16:28:17.855115Z","iopub.status.idle":"2025-11-10T16:37:50.144139Z","shell.execute_reply.started":"2025-11-10T16:28:17.855089Z","shell.execute_reply":"2025-11-10T16:37:50.143457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CV Score","metadata":{}},{"cell_type":"code","source":"m = roc_auc_score(train.loan_paid_back, oof_preds)\nprint(f\"XGB with Original Data as rows CV = {m}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:37:50.145599Z","iopub.execute_input":"2025-11-10T16:37:50.145949Z","iopub.status.idle":"2025-11-10T16:37:50.289617Z","shell.execute_reply.started":"2025-11-10T16:37:50.145929Z","shell.execute_reply":"2025-11-10T16:37:50.288918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5))\nxgb.plot_importance(model, max_num_features=20, importance_type='gain',ax=ax)\nplt.title(\"Top 20 Feature Importances (XGBoost)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:37:50.290408Z","iopub.execute_input":"2025-11-10T16:37:50.290626Z","iopub.status.idle":"2025-11-10T16:37:50.607211Z","shell.execute_reply.started":"2025-11-10T16:37:50.290609Z","shell.execute_reply":"2025-11-10T16:37:50.606361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save results\nsubmission['loan_paid_back'] = test_preds\nsubmission.to_csv('submission_orig_as_rows.csv', index=False) \nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:37:50.60858Z","iopub.execute_input":"2025-11-10T16:37:50.60881Z","iopub.status.idle":"2025-11-10T16:37:51.116556Z","shell.execute_reply.started":"2025-11-10T16:37:50.608783Z","shell.execute_reply":"2025-11-10T16:37:51.115932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Previous Version of Notebook - XGB & LGBM","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n\n\ntarget = 'loan_paid_back'\ncats = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n\n# collect only common columns\ncommon_columns = [col for col in train.columns if col not in ['id', target]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:08.735439Z","iopub.execute_input":"2025-11-10T16:49:08.736065Z","iopub.status.idle":"2025-11-10T16:49:10.716796Z","shell.execute_reply.started":"2025-11-10T16:49:08.736044Z","shell.execute_reply":"2025-11-10T16:49:10.716177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# log of numeric features\ncols = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\ncols_logged = []\n\nfor col in cols:\n    for df in [train, test, orig]:\n        df[f\"{col}_logged\"] = np.log1p(df[col])\n    \n    cols_logged.append(f\"{col}_logged\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:10.717993Z","iopub.execute_input":"2025-11-10T16:49:10.71819Z","iopub.status.idle":"2025-11-10T16:49:10.763745Z","shell.execute_reply.started":"2025-11-10T16:49:10.718175Z","shell.execute_reply":"2025-11-10T16:49:10.763137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"some feature engineering creative techniques from [https://www.kaggle.com/code/analyticaobscura/s5e11-loan-payback-xgb-lgbm-ann](https://www.kaggle.com/code/analyticaobscura/s5e11-loan-payback-xgb-lgbm-ann)","metadata":{}},{"cell_type":"code","source":"def credit_features(train, test):\n\n    first_cols = set(train.columns.to_list())\n    \n    train['loan_to_income'] = train['loan_amount'] / (train['annual_income'] + 1)\n    test['loan_to_income'] = test['loan_amount'] / (test['annual_income'] + 1)\n    \n    train['total_debt'] = train['debt_to_income_ratio'] * train['annual_income']\n    test['total_debt'] = test['debt_to_income_ratio'] * test['annual_income']\n    \n    train['available_income'] = train['annual_income'] * (1 - train['debt_to_income_ratio'])\n    test['available_income'] = test['annual_income'] * (1 - test['debt_to_income_ratio'])\n    \n    train['affordability'] = train['available_income'] / (train['loan_amount'] + 1)\n    test['affordability'] = test['available_income'] / (test['loan_amount'] + 1)\n    \n    train['monthly_payment'] = train['loan_amount'] * (1 + train['interest_rate']/100) / 12\n    test['monthly_payment'] = test['loan_amount'] * (1 + test['interest_rate']/100) / 12\n    \n    train['payment_to_income'] = train['monthly_payment'] / (train['annual_income']/12 + 1)\n    test['payment_to_income'] = test['monthly_payment'] / (test['annual_income']/12 + 1)\n    \n    train['risk_score'] = (train['debt_to_income_ratio'] * 40 + \n                           (1 - train['credit_score']/850) * 30 + train['interest_rate'] * 2)\n    test['risk_score'] = (test['debt_to_income_ratio'] * 40 + \n                          (1 - test['credit_score']/850) * 30 + test['interest_rate'] * 2)\n    \n    train['grade_number'] = train['grade_subgrade'].str[1].astype(int)\n    test['grade_number'] = test['grade_subgrade'].str[1].astype(int)\n\n    train['grade'] = train['grade_subgrade'].str[0]\n    test['grade'] = test['grade_subgrade'].str[0]\n    \n    grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n    train['grade_rank'] = train['grade'].map(grade_map)\n    test['grade_rank'] = test['grade'].map(grade_map)\n    \n    train['grade_combined'] = train['grade_rank'] * 10 + train['grade_number']\n    test['grade_combined'] = test['grade_rank'] * 10 + test['grade_number']\n    \n    train['credit_interest'] = train['credit_score'] * train['interest_rate'] / 100\n    test['credit_interest'] = test['credit_score'] * test['interest_rate'] / 100\n    \n    train['income_credit'] = np.log1p(train['annual_income']) * train['credit_score'] / 1000\n    test['income_credit'] = np.log1p(test['annual_income']) * test['credit_score'] / 1000\n    \n    train['debt_loan'] = train['debt_to_income_ratio'] * np.log1p(train['loan_amount'])\n    test['debt_loan'] = test['debt_to_income_ratio'] * np.log1p(test['loan_amount'])\n\n    created_columns = list(set(train.columns.to_list()) ^ first_cols)\n    \n    print(f\"{len(created_columns)} Features created\")\n    \n    return train, test, created_columns\n\ntrain, test, new_cols = credit_features(train, test)\n\ncats.append('grade')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:10.764544Z","iopub.execute_input":"2025-11-10T16:49:10.764801Z","iopub.status.idle":"2025-11-10T16:49:11.304045Z","shell.execute_reply.started":"2025-11-10T16:49:10.764779Z","shell.execute_reply":"2025-11-10T16:49:11.30329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# credit score categories\n\ndef map_fico_tier(score):\n    \"\"\"Maps a credit score to its corresponding FICO tier.\"\"\"\n    if score >= 800:\n        return 'Exceptional'\n    elif score >= 740:\n        return 'Very Good'\n    elif score >= 670:\n        return 'Good'\n    elif score >= 580:\n        return 'Fair'\n    else: # Below 580\n        return 'Poor'\n\ndef map_vantage_tier(score):\n    \"\"\"Maps a credit score to its corresponding VantageScore tier.\"\"\"\n    if score >= 781:\n        return 'Excellent'\n    elif score >= 661:\n        return 'Good'\n    elif score >= 601:\n        return 'Fair'\n    elif score >= 500:\n        return 'Poor'\n    else: # Below 500\n        return 'Very Poor'\n\n# Creates two new categorical features based on FICO and VantageScore ranges \n# using the existing 'credit_score' column in both train and test DataFrames.\n\ntrain['credit_score_FICO_tier'] = train['credit_score'].apply(map_fico_tier).astype('category')\ntrain['credit_score_Vantage_tier'] = train['credit_score'].apply(map_vantage_tier).astype('category')\ntest['credit_score_FICO_tier'] = test['credit_score'].apply(map_fico_tier).astype('category')\ntest['credit_score_Vantage_tier'] = test['credit_score'].apply(map_vantage_tier).astype('category')\norig['credit_score_FICO_tier'] = orig['credit_score'].apply(map_fico_tier).astype('category')\norig['credit_score_Vantage_tier'] = orig['credit_score'].apply(map_vantage_tier).astype('category')\n\n\ncats.append('credit_score_FICO_tier')\ncats.append('credit_score_Vantage_tier')\n\ncommon_columns.append('credit_score_FICO_tier')\ncommon_columns.append('credit_score_Vantage_tier')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:11.306118Z","iopub.execute_input":"2025-11-10T16:49:11.306475Z","iopub.status.idle":"2025-11-10T16:49:11.659868Z","shell.execute_reply.started":"2025-11-10T16:49:11.306451Z","shell.execute_reply":"2025-11-10T16:49:11.659254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\n\ncorr = train.corr(numeric_only= True).round(3)\n\nsns.heatmap(corr, cmap = 'crest', annot = True)\nplt.title('Non categorical Feature correlation Heatmap', fontsize = 15, pad=10)\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:11.660512Z","iopub.execute_input":"2025-11-10T16:49:11.660694Z","iopub.status.idle":"2025-11-10T16:49:13.931848Z","shell.execute_reply.started":"2025-11-10T16:49:11.66068Z","shell.execute_reply":"2025-11-10T16:49:13.931123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig['grade'] = orig['grade_subgrade'].str[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:13.932713Z","iopub.execute_input":"2025-11-10T16:49:13.932985Z","iopub.status.idle":"2025-11-10T16:49:13.942195Z","shell.execute_reply.started":"2025-11-10T16:49:13.932961Z","shell.execute_reply":"2025-11-10T16:49:13.941634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INTER = []\ninter_cols = [col for col in common_columns if col not in ['annual_income', 'loan_amount']] # remmove high cardinality columns\n\nfor col1, col2 in combinations(list(set(inter_cols + cats)), 2):\n    new_col_name = f'{col1}_{col2}'\n    INTER.append(new_col_name)\n    for df in [train, test, orig]:\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n\nprint(f'{len(INTER)} Features.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:13.943013Z","iopub.execute_input":"2025-11-10T16:49:13.943266Z","iopub.status.idle":"2025-11-10T16:49:37.984545Z","shell.execute_reply.started":"2025-11-10T16:49:13.943245Z","shell.execute_reply":"2025-11-10T16:49:37.983698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ROUND = []\nrounding_levels = {'1s': 0, '10s': -1, '100s': -2}\n\nfor col in ['annual_income', 'loan_amount']:\n    for suffix, level in rounding_levels.items():\n        new_col_name = f'{col}_ROUND_{suffix}'\n        ROUND.append(new_col_name)\n        for df in [train, test, orig]:\n            df[new_col_name] = df[col].round(level).astype(int)\n\nprint(f'{len(ROUND)} ROUND Features created.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:37.985218Z","iopub.execute_input":"2025-11-10T16:49:37.985423Z","iopub.status.idle":"2025-11-10T16:49:38.043135Z","shell.execute_reply.started":"2025-11-10T16:49:37.985408Z","shell.execute_reply":"2025-11-10T16:49:38.042353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ORIG = []\n\nfor col in common_columns:\n    # MEAN\n    mean_map = orig.groupby(col)[target].mean()\n    new_mean_col_name = f\"orig_mean_{col}\"\n    mean_map.name = new_mean_col_name\n    \n    train = train.merge(mean_map, on=col, how='left')\n    test = test.merge(mean_map, on=col, how='left')\n    ORIG.append(new_mean_col_name)\n\n    # COUNT\n    new_count_col_name = f\"orig_count_{col}\"\n    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n    \n    train = train.merge(count_map, on=col, how='left')\n    test = test.merge(count_map, on=col, how='left')\n    ORIG.append(new_count_col_name)\n\nprint(len(ORIG), 'Orig Features Created!!')","metadata":{"papermill":{"duration":3.954077,"end_time":"2025-11-01T07:06:14.745786","exception":false,"start_time":"2025-11-01T07:06:10.791709","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:49:38.044036Z","iopub.execute_input":"2025-11-10T16:49:38.044384Z","iopub.status.idle":"2025-11-10T16:50:16.964001Z","shell.execute_reply.started":"2025-11-10T16:49:38.04436Z","shell.execute_reply":"2025-11-10T16:50:16.963263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = common_columns + ORIG + INTER + cols_logged + new_cols + ROUND\nprint(len(FEATURES), 'Features will be used')","metadata":{"papermill":{"duration":0.008335,"end_time":"2025-11-01T07:06:14.757559","exception":false,"start_time":"2025-11-01T07:06:14.749224","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:50:16.965819Z","iopub.execute_input":"2025-11-10T16:50:16.966048Z","iopub.status.idle":"2025-11-10T16:50:16.970409Z","shell.execute_reply.started":"2025-11-10T16:50:16.966031Z","shell.execute_reply":"2025-11-10T16:50:16.969531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"X = train[FEATURES].copy(deep = True)\ny = train[target].copy(deep = True)\n\ntest = test.drop(columns=['id'], axis=1).copy(deep=True)","metadata":{"papermill":{"duration":0.061551,"end_time":"2025-11-01T07:06:14.821588","exception":false,"start_time":"2025-11-01T07:06:14.760037","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:50:16.971233Z","iopub.execute_input":"2025-11-10T16:50:16.971508Z","iopub.status.idle":"2025-11-10T16:50:19.276103Z","shell.execute_reply.started":"2025-11-10T16:50:16.971486Z","shell.execute_reply":"2025-11-10T16:50:19.275461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 7,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'n_estimators': 10000,\n    'learning_rate': 0.01,\n    'early_stopping_rounds': 300,\n    'random_state': 42,\n    'n_jobs': -1,\n    'device': 'cuda',\n    'enable_categorical': True,\n    \"grow_policy\": \"lossguide\", \n\n    'scale_pos_weight': 0.8, # usefull for unbalanced data\n    \"min_samples_split\": 5,\n    'lambda': 3.0, \n    'alpha': 2.0,\n    'max_bin': 512\n}\n\nparams_lgbm = {\n    'n_estimators': 5000,\n    'learning_rate': 0.01,\n    'num_leaves': 128,\n    'max_depth': 7,\n    'colsample_bytree': 0.8,\n    'categorical_feature':cats,\n    'subsample': 0.7,\n    'reg_alpha': 3,\n    'reg_lambda': 1,\n    'random_state': 42,\n    'max_bin': 512,\n    'n_jobs': -1,\n    'metric': 'auc',\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'verbosity': -1,\n    'scale_pos_weight' : 0.79, # mainly for unbalanced binary data\n}","metadata":{"papermill":{"duration":0.008055,"end_time":"2025-11-01T07:06:16.122037","exception":false,"start_time":"2025-11-01T07:06:16.113982","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:51:39.079404Z","iopub.execute_input":"2025-11-10T16:51:39.079946Z","iopub.status.idle":"2025-11-10T16:51:39.085655Z","shell.execute_reply.started":"2025-11-10T16:51:39.079924Z","shell.execute_reply":"2025-11-10T16:51:39.08486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cuml Target Encoding Practical Notebook: [https://github.com/rapidsai/cuml/blob/branch-0.16/notebooks/target_encoder_walkthrough.ipynb](https://github.com/rapidsai/cuml/blob/branch-0.16/notebooks/target_encoder_walkthrough.ipynb)","metadata":{}},{"cell_type":"code","source":"N_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n\noof_preds_xgb = np.zeros(len(X))\ntest_preds_xgb = np.zeros(len(test))\n\noof_preds_lgb = np.zeros(len(X))\ntest_preds_lgb = np.zeros(len(test))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f'--- Fold {fold}/{N_SPLITS} ---')\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    X_test = test[FEATURES].copy()\n\n    for col in INTER:\n        TE = TargetEncoder(n_folds=10, smooth=1.5, split_method='random', stat='mean')\n    \n        X_train[col] = TE.fit_transform(X_train[[col]], y_train)\n        X_val[col] = TE.transform(X_val[[col]])\n        X_test[col] = TE.transform(X_test[[col]])\n\n    X_train[cats] = X_train[cats].astype('category')\n    X_val[cats] = X_val[cats].astype('category')\n    X_test[cats] = X_test[cats].astype('category')\n\n    model = XGBClassifier(**params)\n    \n    model.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              verbose=500)\n\n    val_preds = model.predict_proba(X_val)[:, 1]\n    oof_preds_xgb[val_idx] = val_preds\n    \n    fold_score = roc_auc_score(y_val, val_preds)\n    print(f'XGB Fold {fold} AUC: {fold_score:.4f}')\n    test_preds_xgb += model.predict_proba(X_test)[:, 1] / N_SPLITS\n\n    print('---------------------------------------------')\n    \n    model_lgb = LGBMClassifier(**params_lgbm)\n    \n    model_lgb.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              callbacks=[lgb.early_stopping(stopping_rounds=300)],\n              # verbose=500\n            )\n\n    val_preds_lgb = model_lgb.predict_proba(X_val)[:, 1]\n    oof_preds_lgb[val_idx] = val_preds_lgb\n    \n    fold_score = roc_auc_score(y_val, val_preds_lgb)\n    print(f'LGB Fold {fold} AUC: {fold_score:.4f}\\n')\n    test_preds_lgb += model.predict_proba(X_test)[:, 1] / N_SPLITS\n\noverall_auc = roc_auc_score(y,oof_preds_xgb )\nprint(f'====================')\nprint(f'XGB Overall OOF AUC: {overall_auc:.4f}')\nprint(f'====================')\nprint(f'XGB Overall OOF AUC: {roc_auc_score(y, oof_preds_lgb):.4f}')\nprint(f'====================')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T16:51:49.017629Z","iopub.execute_input":"2025-11-10T16:51:49.018132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importances = model.feature_importances_\nimportance_df = pd.DataFrame({\n    'feature': FEATURES, \n    'importance': feature_importances,\n    'importance_lgb': model_lgb.feature_importances_\n})\n\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.suptitle('Feature Importance Comparison', fontsize=18, y=1.02)\n\n\nsns.barplot(data=importance_df.sort_values('importance', ascending=False).head(20),\n            x='importance',           \n            y='feature',\n            ax=axs[0],                \n            edgecolor='black',\n           )\naxs[0].set_title(\"XGBoost Model Feature Importance\") \n\nsns.barplot(data=importance_df.sort_values('importance_lgb', ascending=False).head(20),\n            x='importance_lgb',       \n            y='feature',\n            ax=axs[1],                \n            edgecolor='black',\n           )\naxs[1].set_title(\"LGBoost Model Feature Importance\") \n\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":1.015442,"end_time":"2025-11-01T07:09:38.382962","exception":false,"start_time":"2025-11-01T07:09:37.36752","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importances = model.feature_importances_\nimportance_df = pd.DataFrame({\n    'feature': FEATURES, \n    'importance': feature_importances,\n    'importance_lgb': model_lgb.feature_importances_\n})\n\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\nfig.suptitle('Feature Importance Comparison - least important features', fontsize=18, y=1.02)\n\n\nsns.barplot(data=importance_df.sort_values('importance', ascending=False).tail(20),\n            x='importance',           \n            y='feature',\n            ax=axs[0],                \n            edgecolor='black',\n           )\naxs[0].set_title(\"XGBoost Model Feature Importance\") \n\nsns.barplot(data=importance_df.sort_values('importance_lgb', ascending=False).tail(20),\n            x='importance_lgb',       \n            y='feature',\n            ax=axs[1],                \n            edgecolor='black',\n           )\naxs[1].set_title(\"LGBoost Model Feature Importance\") \n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for pred in [oof_preds_xgb, oof_preds_lgb, oof_preds]:\n#         print(f\"{pred} auc score : {roc_auc_score(y, pred):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save results\nsubmission['loan_paid_back'] = test_preds_xgb\nsubmission.to_csv('submission_xgb.csv', index=False) \n\n# save results\nsubmission['loan_paid_back'] = test_preds_lgb\nsubmission.to_csv('submission_lgb.csv', index=False) ","metadata":{"papermill":{"duration":0.005866,"end_time":"2025-11-01T07:09:39.978772","exception":false,"start_time":"2025-11-01T07:09:39.972906","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save final results\nsubmission['loan_paid_back'] = (test_preds_xgb + test_preds_lgb + test_preds) / 3\nsubmission.to_csv('submission.csv', index=False) \nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}