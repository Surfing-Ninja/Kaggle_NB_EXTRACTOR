{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook is heavily inspired by the excellent work from Chris Deotte: [First Place Single Model LB 38.81](https://www.kaggle.com/code/cdeotte/first-place-single-model-lb-38-81).\n\nThe purpose of this notebook is to provide a baseline and reference for an approach that improves the accuracy of a single XGBoost model through effective feature engineering.\n\nIn this notebook, we specifically add rounded features for large numerical values (such as `annual_income` and `loan_amount`) and features created using Target Encoding. However, there are many other features worth trying. If you are looking for more ideas, the feature engineering concepts presented in the original notebook mentioned above are an outstanding resource.\n\nAlso, the special notebook [S5E11 6x faster with RAPIDS](https://www.kaggle.com/code/onodera/s5e11-6x-faster-with-rapids) serves as an excellent reference for GPU acceleration.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:28.820394Z","iopub.execute_input":"2025-11-08T01:34:28.820655Z","iopub.status.idle":"2025-11-08T01:34:28.83232Z","shell.execute_reply.started":"2025-11-08T01:34:28.820638Z","shell.execute_reply":"2025-11-08T01:34:28.831654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np\n\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)\nprint('Orig Shape:', orig.shape)\n\ntrain.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:28.833422Z","iopub.execute_input":"2025-11-08T01:34:28.833654Z","iopub.status.idle":"2025-11-08T01:34:28.995515Z","shell.execute_reply.started":"2025-11-08T01:34:28.833633Z","shell.execute_reply":"2025-11-08T01:34:28.994935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET = 'loan_paid_back'\nBASE = [col for col in train.columns if col not in ['id', TARGET]]\nCATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nNUMS = [col for col in BASE if col not in CATS]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:28.996197Z","iopub.execute_input":"2025-11-08T01:34:28.996483Z","iopub.status.idle":"2025-11-08T01:34:29.002786Z","shell.execute_reply.started":"2025-11-08T01:34:28.996466Z","shell.execute_reply":"2025-11-08T01:34:29.002183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Add Features","metadata":{}},{"cell_type":"markdown","source":"## 1.Bigram Features","metadata":{}},{"cell_type":"code","source":"train[NUMS].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:29.004351Z","iopub.execute_input":"2025-11-08T01:34:29.004634Z","iopub.status.idle":"2025-11-08T01:34:29.076259Z","shell.execute_reply.started":"2025-11-08T01:34:29.004612Z","shell.execute_reply":"2025-11-08T01:34:29.075596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import combinations\n\nINTER = []\n\nTE_BASE = [col for col in BASE if col not in ['annual_income', 'loan_amount']]\nfor col1, col2 in combinations(TE_BASE, 2):\n    new_col_name = f'{col1}_{col2}'\n    INTER.append(new_col_name)\n    for df in [train, test, orig]:\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n        \nprint(f'{len(INTER)} INTER Features created.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:29.07692Z","iopub.execute_input":"2025-11-08T01:34:29.077116Z","iopub.status.idle":"2025-11-08T01:34:29.582017Z","shell.execute_reply.started":"2025-11-08T01:34:29.077088Z","shell.execute_reply":"2025-11-08T01:34:29.581275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.ROUND Features","metadata":{}},{"cell_type":"code","source":"ROUND = []\n\nrounding_levels = {\n    '1s': 0,   \n    '10s': -1,\n}\n\nfor col in ['annual_income', 'loan_amount']:\n    for suffix, level in rounding_levels.items():\n        new_col_name = f'{col}_ROUND_{suffix}'\n        ROUND.append(new_col_name)\n        \n        for df in [train, test, orig]:\n            df[new_col_name] = df[col].round(level).astype(int)\n\nprint(f'{len(ROUND)} ROUND Features created.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:29.582791Z","iopub.execute_input":"2025-11-08T01:34:29.583045Z","iopub.status.idle":"2025-11-08T01:34:29.602902Z","shell.execute_reply.started":"2025-11-08T01:34:29.583023Z","shell.execute_reply":"2025-11-08T01:34:29.602362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.Orig Features","metadata":{}},{"cell_type":"code","source":"ORIG = []\n\nfor col in BASE:\n    # MEAN\n    mean_map = orig.groupby(col)[TARGET].mean()\n    new_mean_col_name = f\"orig_mean_{col}\"\n    mean_map.name = new_mean_col_name\n    \n    train = train.merge(mean_map, on=col, how='left')\n    test = test.merge(mean_map, on=col, how='left')\n    ORIG.append(new_mean_col_name)\n\n    # COUNT\n    new_count_col_name = f\"orig_count_{col}\"\n    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n    \n    train = train.merge(count_map, on=col, how='left')\n    test = test.merge(count_map, on=col, how='left')\n    ORIG.append(new_count_col_name)\n\nprint(f'{len(ORIG)} ORIG Features created.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:29.603462Z","iopub.execute_input":"2025-11-08T01:34:29.603618Z","iopub.status.idle":"2025-11-08T01:34:31.722449Z","shell.execute_reply.started":"2025-11-08T01:34:29.603606Z","shell.execute_reply":"2025-11-08T01:34:31.721699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = BASE + ORIG + INTER + ROUND\nprint(len(FEATURES), 'Features.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:31.723102Z","iopub.execute_input":"2025-11-08T01:34:31.723332Z","iopub.status.idle":"2025-11-08T01:34:31.727565Z","shell.execute_reply.started":"2025-11-08T01:34:31.723318Z","shell.execute_reply":"2025-11-08T01:34:31.726905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"X = train[FEATURES]\ny = train[TARGET]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:31.728359Z","iopub.execute_input":"2025-11-08T01:34:31.728661Z","iopub.status.idle":"2025-11-08T01:34:31.741543Z","shell.execute_reply.started":"2025-11-08T01:34:31.728644Z","shell.execute_reply":"2025-11-08T01:34:31.740966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\n\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:31.743472Z","iopub.execute_input":"2025-11-08T01:34:31.743941Z","iopub.status.idle":"2025-11-08T01:34:31.754437Z","shell.execute_reply.started":"2025-11-08T01:34:31.743915Z","shell.execute_reply":"2025-11-08T01:34:31.753785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:31.754974Z","iopub.execute_input":"2025-11-08T01:34:31.755222Z","iopub.status.idle":"2025-11-08T01:34:31.767151Z","shell.execute_reply.started":"2025-11-08T01:34:31.755199Z","shell.execute_reply":"2025-11-08T01:34:31.766565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'colsample_bytree': 0.3,\n    'subsample': 0.55,\n    'n_estimators': 10000,\n    'learning_rate': 0.01,\n    'early_stopping_rounds': 200,\n    'random_state': 42,\n    'n_jobs': -1,\n    'enable_categorical': True,\n    'device': 'cuda',\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:31.76793Z","iopub.execute_input":"2025-11-08T01:34:31.768164Z","iopub.status.idle":"2025-11-08T01:34:31.779719Z","shell.execute_reply.started":"2025-11-08T01:34:31.768141Z","shell.execute_reply":"2025-11-08T01:34:31.779165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TargetEncoder Class","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Target Encoder that supports multiple aggregation functions,\n    internal cross-validation for leakage prevention, and smoothing.\n\n    Parameters\n    ----------\n    cols_to_encode : list of str\n        List of column names to be target encoded.\n\n    aggs : list of str, default=['mean']\n        List of aggregation functions to apply. Any function accepted by\n        pandas' `.agg()` method is supported, such as:\n        'mean', 'std', 'var', 'min', 'max', 'skew', 'nunique', \n        'count', 'sum', 'median'.\n        Smoothing is applied only to the 'mean' aggregation.\n\n    cv : int, default=5\n        Number of folds for cross-validation in fit_transform.\n\n    smooth : float or 'auto', default='auto'\n        The smoothing parameter `m`. A larger value puts more weight on the \n        global mean. If 'auto', an empirical Bayes estimate is used.\n        \n    drop_original : bool, default=False\n        If True, the original columns to be encoded are dropped.\n    \"\"\"\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n        self.cols_to_encode = cols_to_encode\n        self.aggs = aggs\n        self.cv = cv\n        self.smooth = smooth\n        self.drop_original = drop_original\n        self.mappings_ = {}\n        self.global_stats_ = {}\n\n    def fit(self, X, y):\n        \"\"\"\n        Learn mappings from the entire dataset.\n        These mappings are used for the transform method on validation/test data.\n        \"\"\"\n        temp_df = X.copy()\n        temp_df['target'] = y\n\n        # Learn global statistics for each aggregation\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n\n        # Learn category-specific mappings\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n        \n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Apply learned mappings to the data.\n        Unseen categories are filled with global statistics.\n        \"\"\"\n        X_transformed = X.copy()\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n                new_col_name = f'TE_{col}_{agg_func}'\n                map_series = self.mappings_[col][agg_func]\n                X_transformed[new_col_name] = X[col].map(map_series)\n                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n        \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        Fit and transform the data using internal cross-validation to prevent leakage.\n        \"\"\"\n        # First, fit on the entire dataset to get global mappings for transform method\n        self.fit(X, y)\n\n        # Initialize an empty DataFrame to store encoded features\n        encoded_features = pd.DataFrame(index=X.index)\n        \n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n\n        for train_idx, val_idx in kf.split(X, y):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx]\n            \n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n\n            for col in self.cols_to_encode:\n                # --- Calculate mappings only on the training part of the fold ---\n                for agg_func in self.aggs:\n                    new_col_name = f'TE_{col}_{agg_func}'\n                    \n                    # Calculate global stat for this fold\n                    fold_global_stat = y_train.agg(agg_func)\n                    \n                    # Calculate category stats for this fold\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n\n                    # --- Apply smoothing only for 'mean' aggregation ---\n                    if agg_func == 'mean':\n                        counts = temp_df_train.groupby(col)['target'].count()\n                        \n                        m = self.smooth\n                        if self.smooth == 'auto':\n                            # Empirical Bayes smoothing\n                            variance_between = mapping.var()\n                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n                            if variance_between > 0:\n                                m = avg_variance_within / variance_between\n                            else:\n                                m = 0  # No smoothing if no variance between groups\n                        \n                        # Apply smoothing formula\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n                        encoded_values = X_val[col].map(smoothed_mapping)\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n                    \n                    # Store encoded values for the validation fold\n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n\n        # Merge with original DataFrame\n        X_transformed = X.copy()\n        for col in encoded_features.columns:\n            X_transformed[col] = encoded_features[col]\n            \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:31.780364Z","iopub.execute_input":"2025-11-08T01:34:31.780605Z","iopub.status.idle":"2025-11-08T01:34:31.794522Z","shell.execute_reply.started":"2025-11-08T01:34:31.780588Z","shell.execute_reply":"2025-11-08T01:34:31.793912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(test))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f'--- Fold {fold}/{N_SPLITS} ---')\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    X_test = test[FEATURES].copy()\n\n    TE = TargetEncoder(cols_to_encode=INTER, cv=5, smooth=1.0, aggs=['mean'], drop_original=True)\n    X_train = TE.fit_transform(X_train, y_train)\n    X_val = TE.transform(X_val)\n    X_test = TE.transform(X_test)\n\n    TE2 = TargetEncoder(cols_to_encode=ROUND, cv=5, smooth=1.0, aggs=['mean'], drop_original=False)\n    X_train = TE2.fit_transform(X_train, y_train)\n    X_val = TE2.transform(X_val)\n    X_test = TE2.transform(X_test)\n\n    X_train[CATS] = X_train[CATS].astype('category')\n    X_val[CATS] = X_val[CATS].astype('category')\n    X_test[CATS] = X_test[CATS].astype('category')\n    \n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              verbose=1000)\n\n    val_preds = model.predict_proba(X_val)[:, 1]\n    oof_preds[val_idx] = val_preds\n    \n    fold_score = roc_auc_score(y_val, val_preds)\n    print(f'Fold {fold} AUC: {fold_score:.4f}')\n    test_preds += model.predict_proba(X_test)[:, 1] / N_SPLITS\n\noverall_auc = roc_auc_score(y, oof_preds)\nprint(f'====================')\nprint(f'Overall OOF AUC: {overall_auc:.4f}')\nprint(f'====================')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T01:34:31.795153Z","iopub.execute_input":"2025-11-08T01:34:31.795346Z","execution_failed":"2025-11-08T01:35:43.947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfeature_importances = model.feature_importances_\n\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns, \n    'importance': feature_importances\n})\n\nimportance_df = importance_df.sort_values('importance', ascending=False)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12, 20))\nsns.barplot(x='importance', \n            y='feature', \n            data=importance_df.head(50)) \nplt.title('Feature Importance (Fold5 model)')\nplt.xlabel('Importance Score')\nplt.ylabel('Features')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T01:35:43.948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save OOF/Test_Preds","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'id': train.id, TARGET: oof_preds}).to_csv(f'oof_xgb_cv_{overall_auc}.csv', index=False)\npd.DataFrame({'id': test.id, TARGET: test_preds}).to_csv(f'test_xgb_cv_{overall_auc}.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T01:35:43.948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}