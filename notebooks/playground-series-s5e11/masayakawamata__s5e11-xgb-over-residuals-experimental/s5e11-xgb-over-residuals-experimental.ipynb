{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is an experiment based on [my baseline code](https://www.kaggle.com/code/masayakawamata/s5e11-xgb-baseline?scriptVersionId=272568850) and inspired by [this discussion](https://www.kaggle.com/competitions/playground-series-s5e11/discussion/614986).\n\nThe method implements \"XGB over Residuals\" by:\n1.  Training a Logistic Regression model on the **original** dataset.\n2.  Acquiring the logits (predictions before activation) from that model.\n3.  Passing these logits to the main XGBoost model using the `base_margin` parameter.\n\n---\n\n### Current Results & Purpose\n\nWhile I have **not yet achieved a score improvement** with this specific setup, I suspect the underlying concept could be powerful. A better score might be achievable through modifications, such as:\n\n* Using a Neural Network or another GBDT (instead of LogReg) to generate the `base_margin`.\n* Training the Stage 1 model on a feature-engineered dataset to capture more of the original data's signal, which might significantly unlock the potential of this \"over residuals\" approach.\n\nI hope this notebook can serve as a useful baseline for your own experiments with this technique.\n\nIf you manage to improve performance with this method, I would be very grateful if you could share your findings in the comments or in a post-competition write-up, as your insights would be highly valuable.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:54:26.885033Z","iopub.execute_input":"2025-11-08T03:54:26.885357Z","iopub.status.idle":"2025-11-08T03:54:26.894233Z","shell.execute_reply.started":"2025-11-08T03:54:26.88532Z","shell.execute_reply":"2025-11-08T03:54:26.893085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd\n\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)\nprint('Orig Shape:', orig.shape)\n\ntrain.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:54:26.896548Z","iopub.execute_input":"2025-11-08T03:54:26.896841Z","iopub.status.idle":"2025-11-08T03:54:31.839703Z","shell.execute_reply.started":"2025-11-08T03:54:26.896819Z","shell.execute_reply":"2025-11-08T03:54:31.838756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET = 'loan_paid_back'\nBASE = [col for col in train.columns if col not in ['id', TARGET]]\nCATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nNUMS = [col for col in BASE if col not in CATS]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:54:31.841042Z","iopub.execute_input":"2025-11-08T03:54:31.841409Z","iopub.status.idle":"2025-11-08T03:54:31.847167Z","shell.execute_reply.started":"2025-11-08T03:54:31.841373Z","shell.execute_reply":"2025-11-08T03:54:31.845957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LogReg on Original Data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\npreprocessor_ohe = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATS),\n    ],\n    remainder='passthrough' \n)\n\npipeline_logreg = Pipeline(steps=[\n    ('preprocess', preprocessor_ohe),\n    ('scale', StandardScaler()),\n    ('model', LogisticRegression(C=48, max_iter=1200, random_state=42))\n])\n\nX_orig = orig[BASE]\ny_orig = orig[TARGET]\n\npipeline_logreg.fit(X_orig, y_orig)\n\nprint(\"Getting logits (decision_function) for train and test data...\")\ntrain['logreg_logit'] = pipeline_logreg.decision_function(train[BASE])\ntest['logreg_logit'] = pipeline_logreg.decision_function(test[BASE])\ntrain[['id', 'logreg_logit']].head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:55:22.529903Z","iopub.execute_input":"2025-11-08T03:55:22.530213Z","iopub.status.idle":"2025-11-08T03:55:26.289762Z","shell.execute_reply.started":"2025-11-08T03:55:22.530191Z","shell.execute_reply":"2025-11-08T03:55:26.288775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = BASE + ['logreg_logit']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:55:26.291383Z","iopub.execute_input":"2025-11-08T03:55:26.291856Z","iopub.status.idle":"2025-11-08T03:55:26.296442Z","shell.execute_reply.started":"2025-11-08T03:55:26.291833Z","shell.execute_reply":"2025-11-08T03:55:26.295585Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"X = train[FEATURES]\ny = train[TARGET]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:55:28.430492Z","iopub.execute_input":"2025-11-08T03:55:28.430862Z","iopub.status.idle":"2025-11-08T03:55:28.464393Z","shell.execute_reply.started":"2025-11-08T03:55:28.430836Z","shell.execute_reply":"2025-11-08T03:55:28.463406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:55:28.81705Z","iopub.execute_input":"2025-11-08T03:55:28.817394Z","iopub.status.idle":"2025-11-08T03:55:29.097605Z","shell.execute_reply.started":"2025-11-08T03:55:28.81737Z","shell.execute_reply":"2025-11-08T03:55:29.096582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 5,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'n_estimators': 10000,\n    'learning_rate': 0.01,\n    'early_stopping_rounds': 200,\n    'random_state': 42,\n    'n_jobs': -1,\n    'enable_categorical': True,\n    'device': 'cuda',\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:55:30.615652Z","iopub.execute_input":"2025-11-08T03:55:30.616062Z","iopub.status.idle":"2025-11-08T03:55:30.622521Z","shell.execute_reply.started":"2025-11-08T03:55:30.61603Z","shell.execute_reply":"2025-11-08T03:55:30.621164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(test))\n\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f'--- Fold {fold}/{N_SPLITS} ---')\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    X_test = test[FEATURES].copy()\n\n    X_train[CATS] = X_train[CATS].astype('category')\n    X_val[CATS] = X_val[CATS].astype('category')\n    X_test[CATS] = X_test[CATS].astype('category')\n\n    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n    dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n    dtest = xgb.DMatrix(X_test, enable_categorical=True)\n    \n    dtrain.set_base_margin(X_train['logreg_logit'])\n    dval.set_base_margin(X_val['logreg_logit'])\n    dtest.set_base_margin(test['logreg_logit']) \n    \n    evals = [(dtrain, 'train'), (dval, 'eval')]\n    \n    model = xgb.train(\n        params,\n        dtrain,\n        evals=evals,\n        num_boost_round=10000,\n        early_stopping_rounds=params.get('early_stopping_rounds', 200),\n        verbose_eval=1000, \n    )   \n    \n    fold_val_preds = model.predict(dval, iteration_range=(0, model.best_iteration))\n    fold_test_preds = model.predict(dtest, iteration_range=(0, model.best_iteration))\n    \n    oof_preds[val_idx] = fold_val_preds \n    \n    fold_score = roc_auc_score(y_val, fold_val_preds)\n    print(f'Fold {fold} AUC: {fold_score:.4f}')\n    \n    test_preds += fold_test_preds / N_SPLITS \n\noverall_auc = roc_auc_score(y, oof_preds)\nprint(f'====================')\nprint(f'Overall OOF AUC: {overall_auc:.4f}')\nprint(f'====================')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:55:31.291645Z","iopub.execute_input":"2025-11-08T03:55:31.291984Z","execution_failed":"2025-11-08T03:57:51.032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfeature_importances_dict = model.get_score(importance_type='gain')\nimportance_df = pd.DataFrame({\n    'feature': feature_importances_dict.keys(),\n    'importance': feature_importances_dict.values()\n})\n\nimportance_df = importance_df.sort_values('importance', ascending=False)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12, 20))\nsns.barplot(x='importance', \n            y='feature', \n            data=importance_df.head(50))\nplt.title(f'Feature Importance (gain) - Last Model')\nplt.xlabel('Importance Score (Gain)')\nplt.ylabel('Features')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-08T03:57:51.032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame({'id': train.id, TARGET: oof_preds}).to_csv(f'oof_xgb_over_resid_cv_{overall_auc}.csv', index=False)\npd.DataFrame({'id': test.id, TARGET: test_preds}).to_csv(f'test_xgb_over_resid_cv_{overall_auc}.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:54:31.910871Z","iopub.status.idle":"2025-11-08T03:54:31.91131Z","shell.execute_reply.started":"2025-11-08T03:54:31.911067Z","shell.execute_reply":"2025-11-08T03:54:31.911083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import StratifiedKFold, cross_val_score\n# from sklearn.metrics import roc_auc_score\n\n# preprocessor_ohe = ColumnTransformer(\n#     transformers=[\n#         ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATS),\n#     ],\n#     remainder='passthrough' \n# )\n\n# X_orig = orig[BASE]\n# y_orig = orig[TARGET]\n\n# def objective(trial):\n#     logreg_c = trial.suggest_float('C', 1e-4, 1e2, log=True)\n#     logreg_max_iter = trial.suggest_int('max_iter', 100, 2000, step=100)\n\n#     pipeline = Pipeline(steps=[\n#         ('preprocess', preprocessor_ohe),\n#         ('scale', StandardScaler()),\n#         ('model', LogisticRegression(\n#             C=logreg_c,\n#             max_iter=logreg_max_iter,\n#             solver='liblinear', \n#             random_state=42\n#         ))\n#     ])\n\n#     N_SPLITS = 5\n#     skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n    \n#     try:\n#         scores = cross_val_score(pipeline, X_orig, y_orig, cv=skf, scoring='roc_auc')\n#         return np.mean(scores)\n    \n#     except Exception as e:\n#         print(f\"Trial failed with error: {e}\")\n#         return 0.0\n# N_TRIALS = 100\n\n# print(f\"Optuna tuning started (N_TRIALS={N_TRIALS}, CV_SPLITS=5)...\")\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n\n# print(\"\\nOptuna tuning finished.\")\n# print(f\"Best trial AUC: {study.best_value:.6f}\")\n# print(f\"Best params: {study.best_params}\")\n\n# best_params = study.best_params\n\n# final_pipeline = Pipeline(steps=[\n#     ('preprocess', preprocessor_ohe),\n#     ('scale', StandardScaler()),\n#     ('model', LogisticRegression(\n#         C=best_params['C'],\n#         max_iter=best_params['max_iter'],\n#         solver='liblinear', \n#         random_state=42\n#     ))\n# ])\n\n# final_pipeline.fit(X_orig, y_orig)\n# train['logreg_logit'] = final_pipeline.decision_function(train[BASE])\n# test['logreg_logit'] = final_pipeline.decision_function(test[BASE])\n\n# display(train[['id', 'logreg_logit']].head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T03:54:31.913328Z","iopub.status.idle":"2025-11-08T03:54:31.913745Z","shell.execute_reply.started":"2025-11-08T03:54:31.913563Z","shell.execute_reply":"2025-11-08T03:54:31.913581Z"}},"outputs":[],"execution_count":null}]}