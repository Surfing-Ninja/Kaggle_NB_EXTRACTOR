{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672},{"sourceId":13671694,"sourceType":"datasetVersion","datasetId":8625730}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade scikit-learn scikit-learn==1.7.2 xgboost==3.1.1 lightgbm==4.6.0 catboost==1.2.8 numpy==1.26.4 scipy==1.14.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:11:20.292119Z","iopub.execute_input":"2025-11-09T20:11:20.292475Z","iopub.status.idle":"2025-11-09T20:11:26.687211Z","shell.execute_reply.started":"2025-11-09T20:11:20.292448Z","shell.execute_reply":"2025-11-09T20:11:26.686077Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n# %load_ext cudf.pandas\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, label_binarize, OrdinalEncoder, QuantileTransformer, TargetEncoder, RobustScaler\nfrom category_encoders import CatBoostEncoder, MEstimateEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, HistGradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression, LinearRegression, BayesianRidge, Ridge, ElasticNet, Lasso\n\nfrom sklearn import set_config\nimport os\n\nimport optuna\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, root_mean_squared_error, mean_squared_error, precision_recall_curve, make_scorer, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, matthews_corrcoef, r2_score\nfrom scipy.stats import norm, skew\n\nfrom colorama import Fore, Style, init\nfrom copy import deepcopy\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold, KFold, RepeatedKFold, cross_val_score, StratifiedGroupKFold\nfrom sklearn.isotonic import IsotonicRegression\nfrom xgboost import DMatrix, XGBClassifier, XGBRegressor\nimport xgboost as xgb\nfrom lightgbm import log_evaluation, early_stopping, LGBMClassifier, LGBMRegressor, Dataset\nimport lightgbm\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nfrom tqdm.notebook import tqdm\nfrom optuna.samplers import TPESampler, CmaEsSampler\nfrom optuna.pruners import HyperbandPruner\nfrom functools import partial\nfrom IPython.display import display_html, clear_output\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.compose import ColumnTransformer\nimport gc\nimport re\nfrom typing import Literal, NamedTuple\nfrom itertools import combinations\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.inspection import permutation_importance\n\nimport keras\nfrom keras.models import Sequential\nfrom keras import layers\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nimport math\nimport random\nfrom copy import deepcopy\nfrom typing import Any, Literal, NamedTuple, Optional\n\nimport scipy.special\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport sklearn.preprocessing\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nimport torch\nimport torch.nn as nn\nimport torch.optim\nfrom torch import Tensor\nfrom tqdm.std import tqdm\nfrom itertools import combinations\nfrom dataclasses import dataclass, field\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:11:26.69013Z","iopub.execute_input":"2025-11-09T20:11:26.690485Z","iopub.status.idle":"2025-11-09T20:11:26.707375Z","shell.execute_reply.started":"2025-11-09T20:11:26.690454Z","shell.execute_reply":"2025-11-09T20:11:26.705976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Configuration</p>","metadata":{}},{"cell_type":"code","source":"class Config:\n    target = 'loan_paid_back'\n    train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv', index_col='id')\n    test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv', index_col='id')\n    submission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n    orig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    state = 42\n    n_splits = 10\n    early_stop = 200\n    metric = 'roc_auc'\n    task_type = \"binary\"\n    task_is_regression = task_type == 'regression'\n    if task_is_regression:\n        n_classes = None\n    else:\n        n_classes = train[target].nunique()\n        labels = list(train[target].unique())\n\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=state)\n\n    outliers = False\n    log_trf = False\n    missing = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:11:26.708449Z","iopub.execute_input":"2025-11-09T20:11:26.708744Z","iopub.status.idle":"2025-11-09T20:11:28.176605Z","shell.execute_reply.started":"2025-11-09T20:11:26.708722Z","shell.execute_reply":"2025-11-09T20:11:28.174949Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">EDA</p>","metadata":{}},{"cell_type":"code","source":"class EDA(Config):\n    \n    def __init__(self):\n        super().__init__()\n\n        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n        self.data_info()\n        self.heatmap()\n        self.dist_plots()\n        self.cat_feature_plots()\n        if self.task_is_regression:\n            self.target_plot()\n        else:\n            self.target_pie()\n                \n    def data_info(self):\n        \n        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n            table_style = [{'selector': 'th:not(.index_name)',\n                            'props': [('background-color', '#3cb371'),\n                                      ('color', '#FFFFFF'),\n                                      ('font-weight', 'bold'),\n                                      ('border', '1px solid #DCDCDC'),\n                                      ('text-align', 'center')]\n                            }, \n                            {'selector': 'tbody td',\n                             'props': [('border', '1px solid #DCDCDC'),\n                                       ('font-weight', 'normal')]\n                            }]\n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} head\\n')\n            display(data.head().style.set_table_styles(table_style))\n                           \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} info\\n'+Style.RESET_ALL)               \n            display(data.info())\n                           \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} describe\\n')\n            display(data.describe().drop(index='count', columns=self.target, errors = 'ignore').T\n                    .style.set_table_styles(table_style).format('{:.3f}'))\n            \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} missing values\\n'+Style.RESET_ALL)               \n            display(data.isna().sum())\n        return self\n    \n    def heatmap(self):\n        print(Style.BRIGHT+Fore.GREEN+f'\\nCorrelation Heatmap\\n')\n        plt.figure(figsize=(6, 6))\n        corr = self.train[self.num_features+[self.target]].corr(method='pearson')\n        sns.heatmap(corr, fmt = '0.4f', cmap = 'Greens', square=True, annot=True, linewidths=1, cbar=False)\n        plt.show()\n        \n    def dist_plots(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nDistribution analysis\\n\")\n        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'), \n                        self.test[self.num_features].assign(Source = 'Test'),], \n                        axis=0, ignore_index = True)\n\n        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (18, len(self.num_features) * 6), \n                                 gridspec_kw = {'hspace': 0.3, \n                                                'wspace': 0.2, \n                                                'width_ratios': [0.70, 0.30]\n                                               }\n                                )\n        for i,col in enumerate(self.num_features):\n            ax = axes[i,0]\n            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source', \n                        palette = ['#3cb371', 'r'], ax = ax, linewidth = 2\n                       )\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col}\")\n            ax.grid()\n\n            ax = axes[i,1]\n            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n                        linewidth = 1, fliersize= 1,\n                        ax = ax, palette=['#3cb371', 'r']\n                       )\n            ax.set_title(f\"\\n{col}\")\n            ax.set(xlabel = '', ylabel = '')\n            ax.tick_params(axis='both', which='major')\n            ax.set_xticklabels(['Train', 'Test'])\n\n        plt.tight_layout()\n        plt.show()\n               \n    def cat_feature_plots(self):\n        fig, axes = plt.subplots(max(len(self.cat_features), 1), 2 ,figsize = (18, len(self.cat_features) * 6), \n                                 gridspec_kw = {'hspace': 0.5, \n                                                'wspace': 0.2,\n                                               }\n                                )\n        if len(self.cat_features) == 1:\n            axes = np.array([axes])\n            \n        for i, col in enumerate(self.cat_features):\n            ax = axes[i,0]\n            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='#3cb371')\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col} Train\")\n            \n            ax = axes[i,1]\n            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='r')\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col} Test\")\n\n        plt.tight_layout()\n        plt.show()\n\n    def target_pie(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nTarget feature distribution\\n\")\n        targets = self.train[self.target]\n        plt.figure(figsize=(6, 6))\n        plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=sns.color_palette('viridis', len(targets.value_counts())))\n        plt.show()\n\n    def target_plot(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nTarget feature distribution\\n\")\n        \n        fig, axes = plt.subplots(1, 2 ,figsize = (14, 6), \n                                 gridspec_kw = {'hspace': 0.3, \n                                                'wspace': 0.2, \n                                                'width_ratios': [0.70, 0.30]\n                                               }\n                                )\n        ax = axes[0]\n        sns.kdeplot(data = self.train[self.target], \n                    color = '#3cb371', ax = ax, linewidth = 2\n                   )\n        ax.set(xlabel = '', ylabel = '')\n        ax.set_title(f\"\\n{self.target}\")\n        ax.grid()\n\n        ax = axes[1]\n        sns.boxplot(data = self.train, y = self.target, width = 0.5,\n                    linewidth = 1, fliersize= 1,\n                    ax = ax, color = '#3cb371'\n                   )\n        ax.set_title(f\"\\n{self.target}\")\n        ax.set(xlabel = '', ylabel = '')\n        ax.tick_params(axis='both', which='major')\n\n        plt.tight_layout()\n        plt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:11:28.177952Z","iopub.execute_input":"2025-11-09T20:11:28.1783Z","iopub.status.idle":"2025-11-09T20:11:28.205473Z","shell.execute_reply.started":"2025-11-09T20:11:28.178263Z","shell.execute_reply":"2025-11-09T20:11:28.204131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eda = EDA()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:11:28.20875Z","iopub.execute_input":"2025-11-09T20:11:28.209139Z","iopub.status.idle":"2025-11-09T20:11:59.966446Z","shell.execute_reply.started":"2025-11-09T20:11:28.209106Z","shell.execute_reply":"2025-11-09T20:11:59.965393Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Preprocessing</p>","metadata":{}},{"cell_type":"code","source":"class Preprocessing(Config):\n    \n    def __init__(self, n_splits=5, random_state=42, smoothing=20):\n        super().__init__()\n        self.global_stats = {}\n        self.encodings = {}\n        self.freq_encodings = {}\n        self.count_encodings = {}\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.smoothing = smoothing\n\n    def fit_transform(self):\n        self.prepare_data()\n        if self.missing:\n            self.missing_values()\n\n        combine = pd.concat([self.X, self.test])\n        combine = self.feature_engineering(combine)\n        self.X = combine.iloc[:len(self.X)].copy()\n        self.test = combine.iloc[len(self.X):].copy()\n\n        self.num_features = self.test.select_dtypes(exclude=['object', 'bool', 'category']).columns.tolist()\n        self.cat_features = self.test.select_dtypes(include=['object', 'bool','category']).columns.tolist()\n\n        if self.outliers:\n            self.remove_outliers()\n        if self.log_trf:\n            self.log_transformation()\n\n        return self.X, self.y, self.test, self.cat_features, self.num_features\n\n    def prepare_data(self):\n        self.train_raw = self.train.copy()\n        self.y = self.train[self.target]\n        self.X = self.train.drop(self.target, axis=1)\n\n        self.num_features = self.X.select_dtypes(exclude=['object', 'bool']).columns.tolist()\n        self.cat_features = self.X.select_dtypes(include=['object', 'bool']).columns.tolist()\n\n    def feature_engineering(self, data):\n        df = data.copy()\n        highcard = ['annual_income', 'loan_amount']\n        lowcard = [col for col in self.num_features if col not in highcard]\n        \n        global_stats = {'mean': self.orig[self.target].mean(), 'count': 0}\n        for c in self.num_features + self.cat_features:\n            for a in ['mean', 'count']:\n                col = f'{c}_org_{a}'\n                tmp = (self.orig.groupby(c)[self.target]\n                       .agg(a)\n                       .rename(col)\n                       .reset_index())\n                df = df.merge(tmp, on=c, how='left')\n                df[col] = df[col].fillna(global_stats[a])\n\n        for c in self.num_features:\n            df[f\"Log_{c}\"] = np.log1p(df[c])\n            df[f\"{c}_sq\"] = df[c]**2\n\n        self.numtocat_features = []\n        for c in lowcard:\n            df[f\"{c}_cat\"], _ = df[c].factorize()\n            df[f\"{c}_cat\"] = df[f\"{c}_cat\"].astype('category')\n            self.numtocat_features.append(f\"{c}_cat\")\n\n        for c in highcard:\n            df[f'{c}_round'] = df[c].round(0)\n            df[f\"{c}_round\"], _ = pd.factorize(df[f\"{c}_round\"])\n            df[f\"{c}_round\"] = df[f\"{c}_round\"].astype('category')\n            self.numtocat_features.append(f\"{c}_round\")\n            df[f'{c}_thousands'] = df[c].round(-3)\n            df[f\"{c}_thousands\"], _ = pd.factorize(df[f\"{c}_thousands\"])\n            df[f\"{c}_thousands\"] = df[f\"{c}_thousands\"].astype('category')\n            self.numtocat_features.append(f\"{c}_thousands\")\n\n        df['grade_number'] = df['grade_subgrade'].str[1].astype(int)\n        grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n        df['grade_rank'] = df['grade_subgrade'].str[0].map(grade_map)\n\n        all_cats = self.numtocat_features+self.cat_features\n        for c in all_cats:\n            freqs = df[c].value_counts(normalize=True)\n            df[f\"{c}_fe\"] = df[c].map(freqs)\n\n        df[self.cat_features] = df[self.cat_features].astype('category')\n        return df\n\n    def log_transformation(self):\n        self.y = np.log1p(self.y)\n\n    def remove_outliers(self):\n        Q1 = self.y.quantile(0.25)\n        Q3 = self.y.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_limit = Q1 - 1.5 * IQR\n        upper_limit = Q3 + 1.5 * IQR\n        mask = (self.y >= lower_limit) & (self.y <= upper_limit)\n        self.X = self.X[mask]\n        self.y = self.y[mask]\n        self.X.reset_index(drop=True, inplace=True)\n\n    def missing_values(self):\n        self.X[self.cat_features] = self.X[self.cat_features].fillna('NaN')\n        self.test[self.cat_features] = self.test[self.cat_features].fillna('NaN')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:11:59.967445Z","iopub.execute_input":"2025-11-09T20:11:59.967843Z","iopub.status.idle":"2025-11-09T20:11:59.990703Z","shell.execute_reply.started":"2025-11-09T20:11:59.967813Z","shell.execute_reply":"2025-11-09T20:11:59.989434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, test, cat_features, num_features = Preprocessing().fit_transform()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:11:59.991543Z","iopub.execute_input":"2025-11-09T20:11:59.991839Z","iopub.status.idle":"2025-11-09T20:12:08.55937Z","shell.execute_reply.started":"2025-11-09T20:11:59.99182Z","shell.execute_reply":"2025-11-09T20:12:08.558379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Models</p>","metadata":{}},{"cell_type":"code","source":"models = {\n    'XGB_TE16': XGBClassifier(**{'tree_method': 'hist',\n                                 'n_estimators': 10000,\n                                 'objective': 'binary:logistic',\n                                 'random_state': Config.state,\n                                 'enable_categorical': True,\n                                 'verbosity': 0,\n                                 'eval_metric': 'auc',\n                                 'booster': 'gbtree',\n                                 'n_jobs': -1,\n                                 'learning_rate': 0.01,\n                                 \"device\": \"cuda\",\n                                 'lambda': 0.31304742836116506,\n                                 'alpha': 4.446958778347978, \n                                 'colsample_bytree': 0.10024697719858375,\n                                 'subsample': 0.6689970856296092, \n                                 'max_depth': 8,\n                                 'min_child_weight': 2,\n                                 \"min_samples_split\": 5,\n                                 'max_bin': 512\n                               }),\n    'XGB6': XGBClassifier(**{'tree_method': 'hist',\n                             'n_estimators': 10000,\n                             'objective': 'binary:logistic',\n                             'random_state': Config.state,\n                             'enable_categorical': True,\n                             'verbosity': 0,\n                             'eval_metric': 'auc',\n                             'booster': 'gbtree',\n                             'n_jobs': -1,\n                             'learning_rate': 0.01,\n                             \"device\": \"cuda\",\n                             'lambda': 1.7433102083275247,\n                             'alpha': 1.648659627164799,\n                             'colsample_bytree': 0.10650947532921384,\n                             'subsample': 0.9225209864222714,\n                             'max_depth': 4, \n                             'min_child_weight': 3,\n                             'max_bin': 512,\n                           }),\n    'XGB6_cl': XGBClassifier(**{'tree_method': 'hist',\n                                'n_estimators': 10000,\n                                'objective': 'binary:logistic',\n                                'random_state': Config.state,\n                                'enable_categorical': True,\n                                'verbosity': 0,\n                                'eval_metric': 'logloss',\n                                'booster': 'gbtree',\n                                'n_jobs': -1,\n                                'learning_rate': 0.01,\n                                \"device\": \"cuda\",\n                                'lambda': 1.7433102083275247,\n                                'alpha': 1.648659627164799,\n                                'colsample_bytree': 0.10650947532921384,\n                                'subsample': 0.9225209864222714,\n                                'max_depth': 4, \n                                'min_child_weight': 3,\n                                'max_bin': 512,\n                               }),\n    'LGBM2': LGBMClassifier(**{'random_state': Config.state,\n                               'early_stopping_round': Config.early_stop,\n                               'verbose': -1,\n                               'n_estimators': 10000,\n                               'metric': 'auc',\n                               'objective': 'binary',\n                               'max_bin': 500,\n                               'max_depth': 5, \n                               'learning_rate': 0.03064219130978399, \n                               'min_child_samples': 190,\n                               'subsample': 0.4736164696953288,\n                               'colsample_bytree': 0.21677995910966458,\n                               'num_leaves': 318,\n                               'reg_alpha': 4.818090956944737,\n                               'reg_lambda': 0.016624049322167257,\n                              }),\n    'LGBM3': LGBMClassifier(**{'random_state': Config.state,\n                               'early_stopping_round': Config.early_stop,\n                               'verbose': -1,\n                               'n_estimators': 10000,\n                               'metric': 'AUC',\n                               'objective': 'binary',\n                               'learning_rate': 0.01,\n                               'max_depth': 5,\n                               'min_child_samples': 162, \n                               'subsample': 0.44186829450007403,\n                               'colsample_bytree': 0.23231047438980407,\n                               'num_leaves': 332, \n                               'reg_alpha': 0.049317572788057186,\n                               'reg_lambda': 7.073507415197327,\n                               'max_bin': 500,\n                              }),\n    'HGB3': HistGradientBoostingClassifier(**{'max_iter': 10000,\n                                              'random_state': Config.state,\n                                              'early_stopping': True,\n                                              'categorical_features': \"from_dtype\",\n                                              'learning_rate': 0.01,\n                                              'loss': 'log_loss',\n                                              'scoring': 'loss',\n                                              'l2_regularization': 0.011425355549456015,\n                                              'max_depth': 4,\n                                              'max_leaf_nodes': 85,\n                                              'min_samples_leaf': 50\n                                            }),\n    'predict':_,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:26:36.240465Z","iopub.execute_input":"2025-11-09T20:26:36.242911Z","iopub.status.idle":"2025-11-09T20:26:36.261002Z","shell.execute_reply.started":"2025-11-09T20:26:36.242859Z","shell.execute_reply":"2025-11-09T20:26:36.259827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Model Training</p>","metadata":{}},{"cell_type":"code","source":"class FeatureEncoder:\n    def __init__(self, num_features, cat_features):\n        self.num_features = num_features\n        self.cat_features = cat_features\n        self.ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n        self.scaler = StandardScaler()\n        self.ohe_cols = None\n\n    def fit(self, X):\n        self.ohe.fit(X[self.cat_features])\n        self.ohe_cols = self.ohe.get_feature_names_out(self.cat_features)\n        self.scaler.fit(X[self.num_features])\n        \n    def transform_fold(self, X_train, X_val, X_test):\n        def transform(X):\n            X[self.num_features] = self.scaler.transform(X[self.num_features])\n\n            X_ohe = self.ohe.transform(X[self.cat_features])\n            X_ohe_df = pd.DataFrame(X_ohe, columns=self.ohe_cols, index=X.index)   \n            X = pd.concat([X.drop(columns=self.cat_features).reset_index(drop=True),\n                                        X_ohe_df.reset_index(drop=True)], axis=1)\n            return X\n        return transform(X_train), transform(X_val), transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:15:48.880355Z","iopub.execute_input":"2025-11-09T20:15:48.880898Z","iopub.status.idle":"2025-11-09T20:15:48.894375Z","shell.execute_reply.started":"2025-11-09T20:15:48.880857Z","shell.execute_reply":"2025-11-09T20:15:48.892279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer(Config):\n    \n    def __init__(self, X, y, test, models, num_features, cat_features, training=True):\n        self.X = X\n        self.test = test\n        self.y = y\n        self.models = models\n        self.training = training\n        self.scores = pd.DataFrame(columns=['Score'], dtype=float)\n        self.OOF_preds = pd.DataFrame(dtype=float)\n        self.TEST_preds = pd.DataFrame(dtype=float)\n        self.num_features = num_features\n        self.cat_features = cat_features\n\n    def ScoreMetric(self, y_true, y_pred):\n        if self.metric == 'roc_auc':\n            return roc_auc_score(y_true, y_pred, multi_class=\"ovr\") if self.n_classes > 2 else roc_auc_score(y_true, y_pred)\n        elif self.metric == 'accuracy':\n            return accuracy_score(y_true, y_pred)\n        elif self.metric == 'f1':\n            return f1_score(y_true, y_pred, average='weighted') if self.n_classes > 2 else f1_score(y_true, y_pred)\n        elif self.metric == 'precision':\n            return precision_score(y_true, y_pred, average='weighted') if self.n_classes > 2 else precision_score(y_true, y_pred)\n        elif self.metric == 'recall':\n            return recall_score(y_true, y_pred, average='weighted') if self.n_classes > 2 else recall_score(y_true, y_pred)\n        elif self.metric == 'mae':\n            return mean_absolute_error(y_true, y_pred)\n        elif self.metric == 'r2':\n            return r2_score(y_true, y_pred)\n        elif self.metric == 'rmse':\n            return root_mean_squared_error(y_true, y_pred)\n        elif self.metric == 'rmsle':\n            return root_mean_squared_error(y_true, y_pred)\n        elif self.metric == 'mse':\n            return mean_squared_error(y_true, y_pred, squared=True)\n\n    def train(self, model, X, y, test, model_name):\n        oof_pred = np.zeros(X.shape[0], dtype=float)\n        test_pred = np.zeros(test.shape[0], dtype=float)\n\n        print('='*20)\n        print(model_name)\n        params=model.get_params()\n        for n_fold, (train_id, valid_id) in enumerate(self.folds.split(X, y)):\n            features = X.columns.to_list()\n\n            X_train = X[features].loc[train_id].copy()\n            y_train = y[train_id]\n            X_val = X[features].iloc[valid_id].copy()\n            y_val = y[valid_id]\n            X_test = test[features].copy()             \n\n            if model_name != 'Ensemble':\n                TE = TargetEncoder(random_state=42, shuffle=True, cv=5, smooth=15)\n                X_train[self.cat_features] = te.fit_transform(X_train[self.cat_features], y_train).astype('float32')\n                X_val[self.cat_features] = te.transform(X_val[self.cat_features]).astype('float32')\n                X_test[self.cat_features] = te.transform(X_test[self.cat_features]).astype('float32')\n            \n            print(f'Fold {n_fold+1}')\n            \n            if \"LGBM\" in model_name:\n                X_train = lightgbm.Dataset(X_train, label=y_train, categorical_feature=self.cat_features)\n                val_dataset = lightgbm.Dataset(X_val, label=y_val, categorical_feature=self.cat_features)\n                model = lightgbm.train(\n                    params=params,\n                    train_set=X_train,\n                    valid_sets=[val_dataset]\n                )\n\n            elif any(model in model_name for model in [\"NN\", \"TabM\"]):\n                model.num_features = X_train.select_dtypes(exclude=['category']).columns.tolist()\n                model.cat_features = X_train.select_dtypes(include=['category']).columns.tolist()\n                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n                \n            elif \"XGB\" in model_name:\n                X_train = DMatrix(X_train, label=y_train, enable_categorical=True)\n                X_val   = DMatrix(X_val, label=y_val, enable_categorical=True)\n                X_test  = DMatrix(X_test, enable_categorical=True)\n                model = xgb.train(\n                    params=params,\n                    dtrain=X_train,\n                    evals=[(X_val, \"valid\")],\n                    num_boost_round=100_000,\n                    early_stopping_rounds=200,\n                    verbose_eval=False\n                )\n\n            elif \"CAT\" in model_name:\n                X_train = Pool(X_train, label=y_train, cat_features=self.cat_features)\n                X_val = Pool(X_val, label=y_val, cat_features=self.cat_features)\n                X_test = Pool(test, cat_features=self.cat_features)\n                model.fit(X_train, eval_set=X_val, verbose=False)\n                \n            elif any(model in model_name for model in [\"HGB\", \"YDF\"]):\n                model.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n                \n            elif \"Ensemble\" in model_name:\n                model = Pipeline([\n                    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n                    (\"ridge\", model)\n                ])\n                model.fit(X_train, y_train)\n                \n            else:\n                encoder = FeatureEncoder(num_features=self.num_features, cat_features=self.cat_features)\n                encoder.fit(X)\n                X_train, X_val, X_test = encoder.transform_fold(X_train, X_val, X_test)\n          \n                model.fit(X_train, y_train)\n\n            if self.task_type == \"regression\" :\n                y_pred_val = model.predict(X_val)           \n                test_pred += model.predict(X_test) / self.n_splits\n            elif self.task_type == \"binary\" :\n                y_pred_val = model.predict_proba(X_val)[:, 1]            \n                test_pred += model.predict_proba(X_test)[:, 1] / self.n_splits\n            elif self.task_type == \"multiclass\" :\n                y_pred_val = model.predict_proba(X_val)            \n                test_pred += model.predict_proba(X_test) / self.n_splits\n                \n            oof_pred[valid_id] = y_pred_val\n            score = self.ScoreMetric(y_val, y_pred_val)\n            print(score)\n            self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score\n\n        self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n\n        return oof_pred, test_pred\n\n    def run(self):\n        for model_name, model in tqdm(self.models.items()):\n\n            if self.training:                \n                X = self.X.copy()\n                test = self.test.copy()\n\n                oof_pred, test_pred = self.train(model, X, self.y, test, model_name)\n                pd.DataFrame(oof_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_oof.csv', index=False)\n                pd.DataFrame(test_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_test.csv', index=False)\n            \n            else:\n                oof_pred = pd.read_csv(f'/kaggle/input/loan-models/{model_name}_oof.csv')\n                test_pred = pd.read_csv(f'/kaggle/input/loan-models/{model_name}_test.csv')\n                if 'predict' in model_name:\n                    oof_pred = oof_pred['loan_paid_back']\n                    test_pred = test_pred['loan_paid_back']\n                for n_fold, (train_id, valid_id) in enumerate(self.folds.split(oof_pred, self.y)):\n                    y_pred_val, y_val = oof_pred.loc[valid_id], self.y.loc[valid_id]\n                    self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = self.ScoreMetric(y_val, y_pred_val)\n                self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n\n            self.OOF_preds[f'{model_name}'] = oof_pred\n            self.TEST_preds[f'{model_name}'] = test_pred\n            \n        if len(self.models)>1:\n            if self.task_is_regression:\n                meta_model = LinearRegression()\n            else:\n                meta_model = LogisticRegression(C = 0.1, random_state = self.state, max_iter = 1000)\n            \n            self.OOF_preds[\"Ensemble\"], self.TEST_preds[\"Ensemble\"] = self.train(meta_model, self.OOF_preds, y, self.TEST_preds, 'Ensemble')            \n            self.scores = self.scores.sort_values('Score')\n            self.score_bar()\n            self.plot_result(self.OOF_preds[\"Ensemble\"])\n            return self.TEST_preds[\"Ensemble\"]\n        else:\n            print(Style.BRIGHT+Fore.GREEN+f'{model_name} score {self.scores.loc[f\"{model_name}\", \"Score\"]:.7f}\\n')\n            self.plot_result(self.OOF_preds[f'{model_name}'])\n            return self.TEST_preds[f'{model_name}']\n            \n    def score_bar(self):\n        plt.figure(figsize=(18, 7))      \n        colors = ['#3cb371' if i != 'Ensemble' else 'r' for i in self.scores.Score.index]\n        hbars = plt.barh(self.scores.index, self.scores.Score, color=colors, height=0.8)\n        plt.bar_label(hbars, fmt='%.6f')\n        plt.ylabel('Models')\n        plt.xlabel('Score')\n        plt.show()\n        \n    def plot_result(self, oof):\n        if self.task_is_regression:\n            cmap = LinearSegmentedColormap.from_list(\"red2green\", [\"#3cb371\", \"r\"], N=10)\n            fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n            \n            errors = np.abs(y - oof)\n            axes[0].scatter(y, oof, c=errors, cmap=cmap, alpha=0.5, s=5)\n            axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n            axes[0].set_xlabel('Actual')\n            axes[0].set_ylabel('Predicted')\n            axes[0].set_title('Actual vs. Predicted')\n            \n            residuals = y - oof\n            axes[1].scatter(oof, residuals, c=errors, cmap=cmap, alpha=0.5, s=5)\n            axes[1].axhline(y=0, color='black', linestyle='--', lw=2)\n            axes[1].set_xlabel('Predicted Values')\n            axes[1].set_ylabel('Residuals')\n            axes[1].set_title('Residual Plot')\n            \n            plt.tight_layout()\n            plt.show()\n        else:\n            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n    \n            for col in self.OOF_preds:\n                RocCurveDisplay.from_predictions(self.y, self.OOF_preds[col], name=f\"{col}\", ax=axes[0])            \n            axes[0].plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n            axes[0].set_xlabel('False Positive Rate')\n            axes[0].set_ylabel('True Positive Rate')\n            axes[0].set_title('ROC')\n            axes[0].legend(loc=\"lower right\")\n            \n            ConfusionMatrixDisplay.from_predictions(y, (oof>=0.5).astype(int), display_labels=self.labels, colorbar=False, ax=axes[1], cmap = 'Greens')\n            axes[1].set_title('Confusion Matrix')\n            \n            plt.tight_layout()\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:21:50.929419Z","iopub.execute_input":"2025-11-09T20:21:50.929818Z","iopub.status.idle":"2025-11-09T20:21:50.972267Z","shell.execute_reply.started":"2025-11-09T20:21:50.929793Z","shell.execute_reply":"2025-11-09T20:21:50.970824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(X, y, test, models, num_features, cat_features, training=False)\nTEST_preds = trainer.run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:26:49.121418Z","iopub.execute_input":"2025-11-09T20:26:49.122547Z","iopub.status.idle":"2025-11-09T20:27:10.595978Z","shell.execute_reply.started":"2025-11-09T20:26:49.122517Z","shell.execute_reply":"2025-11-09T20:27:10.594555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"border-radius: 40px; color: white; font-weight: bold; font-size: 150%; text-align: center; background-color:#3cb371; padding: 5px 5px 5px 5px;\">Submission</p>","metadata":{}},{"cell_type":"code","source":"submission = Config.submission\nsubmission[Config.target] = TEST_preds\nsubmission.to_csv(\"submission.csv\", index=False)\n\ndisplay(submission.head())\nplt.figure(figsize=(14, 8))\nsns.distplot(submission[Config.target], bins=100, hist_kws={'alpha': 1, 'color': '#3cb371'}, kde_kws={'color': 'red', 'linewidth': 2})\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T20:22:43.206083Z","iopub.execute_input":"2025-11-09T20:22:43.207687Z","iopub.status.idle":"2025-11-09T20:22:45.952622Z","shell.execute_reply.started":"2025-11-09T20:22:43.207644Z","shell.execute_reply":"2025-11-09T20:22:45.951648Z"}},"outputs":[],"execution_count":null}]}