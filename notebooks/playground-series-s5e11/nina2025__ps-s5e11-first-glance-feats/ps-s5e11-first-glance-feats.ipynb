{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":272824123,"sourceType":"kernelVersion"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":193.167856,"end_time":"2025-10-28T00:13:10.62552","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-28T00:09:57.457664","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### PS-s4e11 - [Predicting Loan Payback](https://www.kaggle.com/competitions/playground-series-s5e11/code?competitionId=91722&sortBy=scoreDescending&excludeNonAccessedDatasources=true)\n\nPlayground Series - Season 5, Episode 11\n\n&nbsp;\n\nIn notebook [PS-s5e11 | first glance.1](https://www.kaggle.com/code/nina2025/ps-s5e11-first-glance-1/edit) was deleted several feat-columns one by one,\nand in this notebook PS-s5e11 | first glance.2], we'll do the opposite â€“ add several feat-columns one by one.\n\nPassing this whole thing through the h-blend gateway. For this, the top work - [Simple XGBoost s5e11](https://www.kaggle.com/code/yousefelshahat2/simple-xgboost-s5e11) of the current moment by expert - [yousef Elshahat](https://www.kaggle.com/yousefelshahat2)\n\nLet's see what happens.\n\n&nbsp;\n\nPublic solutions:\n\n||[LB](https://www.kaggle.com/competitions/playground-series-s5e11/leaderboard)| &nbsp; | | &nbsp; | &nbsp; | &nbsp; |\n|:-|:-| :-: | :-: | :-: | :-: | :-: |\n|  | [0.92407](https://www.kaggle.com/code/yousefelshahat2/simple-xgboost-s5e11?scriptVersionId=272522813) |&nbsp;v.6&nbsp;| original - [Simple XGBoost s5e11](https://www.kaggle.com/code/yousefelshahat2/simple-xgboost-s5e11) | expert | [yousef Elshahat](https://www.kaggle.com/yousefelshahat2) | World |\n|||||||\n| | | | **main weights** | **sort** | **correct weights** |\n| | [0.92429](https://www.kaggle.com/code/nina2025/ps-s5e11-first-glance-2?scriptVersionId=272604654) | v.[1](#) | [ 0.20 +0.20 +0.20 +0.20 +0.20 ] |[asc/desc](#blend)|[+0,21,+0.11, -0,07,-0.10,-0.15]|\n| | [0.98432](https://www.kaggle.com/code/nina2025/ps-s5e11-first-glance-2?scriptVersionId=272607416) | v.[2](#) | [ 0.40 +0.15 +0.15 +0.15 +0.15 ] |[asc/desc](#blend)|[+0,21,+0.11, -0,07,-0.10,-0.15]|\n|||||||\n|  | [0.92443](https://www.kaggle.com/code/nina2025/ps-s5e11-first-glance-1-feats?scriptVersionId=272824123) |&nbsp;v.4&nbsp;| original - FE &nbsp;-&nbsp; [PS-s5e11 . first glance.1 - feats](https://www.kaggle.com/code/nina2025/ps-s5e11-first-glance-1-feats) | master | [F.A.Nina](https://www.kaggle.com/code/nina2025) | Georgia |\n|||||||\n| | [?]() | v.[3](#) | ( original - FE &nbsp;**x**&nbsp; FE + original ) . [ 50% &nbsp;**x**&nbsp; 50% ] |[asc/desc](#blend)|[ +0,05,-0.05 ]|","metadata":{}},{"cell_type":"code","source":"pip install --upgrade xgboost scikit-learn","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-02T11:24:17.912727Z","iopub.execute_input":"2025-11-02T11:24:17.913039Z","iopub.status.idle":"2025-11-02T11:24:37.104096Z","shell.execute_reply.started":"2025-11-02T11:24:17.913018Z","shell.execute_reply":"2025-11-02T11:24:37.103332Z"},"papermill":{"duration":13.691795,"end_time":"2025-10-28T00:10:15.020035","exception":false,"start_time":"2025-10-28T00:10:01.32824","status":"completed"},"scrolled":true,"tags":[],"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb; from xgboost import XGBRegressor\n\nimport warnings; warnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-02T11:25:00.544479Z","iopub.execute_input":"2025-11-02T11:25:00.544731Z","iopub.status.idle":"2025-11-02T11:25:01.409427Z","shell.execute_reply.started":"2025-11-02T11:25:00.544708Z","shell.execute_reply":"2025-11-02T11:25:01.408521Z"},"papermill":{"duration":3.157009,"end_time":"2025-10-28T00:10:18.182342","exception":false,"start_time":"2025-10-28T00:10:15.025333","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\n\nprint( df['gender']           .unique() )\nprint( df['marital_status']   .unique() )\nprint( df['education_level']  .unique() )\nprint( df['employment_status'].unique() )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:25:04.095156Z","iopub.execute_input":"2025-11-02T11:25:04.095865Z","iopub.status.idle":"2025-11-02T11:25:05.489708Z","shell.execute_reply.started":"2025-11-02T11:25:04.09584Z","shell.execute_reply":"2025-11-02T11:25:05.488917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fe_map(df):\n    _map_gender = {\n        'Female'  : 1.01, \n        'Male'    : 1.02,\n        'Other'   : 1.03\n    }\n    _map_marital_status = {\n        'Single'  : 2.011, \n        'Married' : 2.012,\n        \"Divorced\": 2.013,\n        'Widowed' : 2.014 \n    }\n    _map_education_level = {\n        \"High School\": 3.11,\n        \"Master's\"   : 3.12,\n        \"Bachelor's\" : 3.13,\n        'PhD'        : 3.14,\n        'Other'      : 3.15 \n    }\n    _map_employment_status = {\n        'Self-employed': 4.001, \n        'Employed'     : 4.002, \n        'Unemployed'   : 4.003, \n        'Retired'      : 4.004,\n        'Student'      : 4.005 \n    }\n    df['_map_f1'] = df['gender']           .map(_map_gender)\n    df['_map_f2'] = df['marital_status']   .map(_map_marital_status)\n    df['_map_f3'] = df['education_level']  .map(_map_education_level)\n    df['_map_f4'] = df['employment_status'].map(_map_employment_status)\n    return df\n\ndef _fe_1(df):\n    df['_feat_1_1'] = df['_map_f4'] * df['_map_f1']\n    df['_feat_1_2'] = df['_map_f4'] * df['_map_f2']\n    df['_feat_1_3'] = df['_map_f4'] * df['_map_f3']\n    return df\n\ndef _fe_2(df):\n    df['_feat_2_1'] = df['_map_f4'] * df['_map_f1'] * df['_map_f2']\n    df['_feat_2_2'] = df['_map_f4'] * df['_map_f1'] * df['_map_f3']\n    df['_feat_2_3'] = df['_map_f4'] * df['_map_f2'] * df['_map_f3']\n    return df\n\ndef _fe_3(df):\n    df['_feat_3_1'] = df['_map_f1'] * df['_map_f2']\n    df['_feat_3_2'] = df['_map_f1'] * df['_map_f3'] \n    df['_feat_3_3'] = df['_map_f1'] * df['_map_f4']\n    df['_feat_3_4'] = df['_map_f2'] * df['_map_f3']\n    df['_feat_3_5'] = df['_map_f2'] * df['_map_f4']\n    df['_feat_3_3'] = df['_map_f3'] * df['_map_f4']\n    return df\n\ndef _fe_4(df):\n    df['_feat_4_1'] = df['_map_f4'] * df['_map_f1']/(df['_map_f2'] + df['_map_f3'] +0.0000001)\n    df['_feat_4_2'] = df['_map_f4'] * df['_map_f2']/(df['_map_f1'] + df['_map_f3'] +0.0000001)\n    df['_feat_4_3'] = df['_map_f4'] * df['_map_f3']/(df['_map_f2'] + df['_map_f1'] +0.0000001)\n    return df\n\ndef fen(df):\n    df = fe_map(df)\n    df =  _fe_1(df)\n    df =  _fe_2(df)\n    df =  _fe_3(df)\n    df =  _fe_4(df)\n    df.drop(columns='_map_f1', inplace=True)\n    df.drop(columns='_map_f2', inplace=True)\n    df.drop(columns='_map_f3', inplace=True)\n    df.drop(columns='_map_f4', inplace=True)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:25:09.009105Z","iopub.execute_input":"2025-11-02T11:25:09.009412Z","iopub.status.idle":"2025-11-02T11:25:09.020146Z","shell.execute_reply.started":"2025-11-02T11:25:09.009391Z","shell.execute_reply":"2025-11-02T11:25:09.019554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_subm(file_submission):\n\n    df = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\n    df_test = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\n    target = df.columns.tolist()[-1]\n    print(df.shape)\n    df.head()\n\n    df      = fen(df)\n    df_test = fen(df_test)\n    \n    def create_frequency_features(train_df, test_df, cols, num, cat):\n        \"\"\"\n        Add frequency and binning features to the dataset.\n        \n        - For each column, create <col>_freq = how often each value appears in train data.\n        - For numeric columns, split values into 5 and 10 quantile bins (groups) to show rank or range.\n        \"\"\"\n        train, test = train_df.copy(), test_df.copy()\n    \n        for col in cols:\n            # Frequency encoding: how common each value is\n            freq = train[col].value_counts(normalize=True)\n            train[f\"{col}_freq\"] = train[col].map(freq)\n            test[f\"{col}_freq\"] = test[col].map(freq).fillna(train[f\"{col}_freq\"].mean())\n    \n            # Binning: group numeric values into quantiles\n            if col in num:\n                for q in [5, 10, 15]:\n                    try:\n                        train[f\"{col}_bin{q}\"], bins = pd.qcut(train[col], q=q, labels=False, retbins=True, duplicates=\"drop\")\n                        test[f\"{col}_bin{q}\"] = pd.cut(test[col], bins=bins, labels=False, include_lowest=True)\n                    except Exception:\n                        train[f\"{col}_bin{q}\"] = test[f\"{col}_bin{q}\"] = 0\n    \n        new_num = train.drop(columns=cat+[target]).columns.tolist()\n        return train, test\n    \n    \n    # Identify feature\n    cols = df.drop(columns=target).columns.tolist()\n    \n    # Categorical features\n    cat = [col for col in cols if df[col].dtype in [\"object\",\"category\"] and col != target]\n    \n    # Numerical features\n    num = [col for col in cols if df[col].dtype not in [\"object\",\"category\",\"bool\"] and col not in [\"id\", target]]\n    \n    # Creating new features based on the frequency of numerical features\n    df, df_test = create_frequency_features(df, df_test, cols, num, cat)\n    \n    # Preparing categorical features\n    df[cat], df_test[cat] = df[cat].astype(\"category\"), df_test[cat].astype(\"category\")\n    \n    # Dropping ID and duplicates\n    df.drop(columns=\"id\", inplace=True)\n    df.drop_duplicates(inplace=True)\n    \n    \n    # print(df.columns.tolist()); df.head()\n    \n    \n    dtrain = xgb.DMatrix(\n        df.drop(columns=target),\n        label=df[target],\n        enable_categorical=True\n    )\n    \n    # Define XGBoost parameters for classification\n    xgb_params = {\n        'tree_method': 'hist',\n        'device': 'cuda',\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic',\n        'random_state': 42,\n        'max_depth': 4,\n        'scale_pos_weight':1,\n        'min_child_weight': 89,\n    }\n    \n    # Run cross-validation\n    cv_results = xgb.cv(\n        params=xgb_params,\n        dtrain=dtrain,\n        nfold=5,\n        num_boost_round=2000,\n        metrics='auc',\n        verbose_eval=100,\n        early_stopping_rounds=50,\n    )\n    \n    # Display last few CV results\n    print(cv_results.tail())\n    \n    # Extract best boosting round\n    best_round = cv_results['test-auc-mean'].idxmax()\n    best_auc = cv_results['test-auc-mean'][best_round]\n    print(f\"Best round: {best_round}, Best CV AUC: {best_auc:.7f}\")\n    \n    \n    # putting the n_estimator at the average early stopping point to avoid overfitting\n    last_round = len(cv_results) - 1\n    xgb_params[\"n_estimators\"] = last_round + 10\n    \n    \n    # Prepare training data\n    X_train = df.drop(columns=target)\n    y_train = df[target]\n    \n    # Train XGBoost model\n    model = XGBRegressor(**xgb_params, enable_categorical=True)\n    model.fit(X_train, y_train)\n\n    \n    if hasattr(model, \"feature_importances_\"):\n        import seaborn as sns, matplotlib.pyplot as plt\n        importances = model.feature_importances_\n        importance_df = pd.DataFrame({'feature': X_train.columns,'importance': importances})\n        importance_df = importance_df.sort_values('importance', ascending=False)\n        plt.style.use('fivethirtyeight')\n        plt.figure(figsize=(12, 20))\n        sns.barplot(x='importance',y='feature', data=importance_df.head(50)) \n        plt.title('Feature Importance')\n        plt.xlabel('Importance Score')\n        plt.ylabel('Features')\n        plt.tight_layout()\n        plt.show()\n    \n    \n    # Predict on test set\n    pred = model.predict(df_test.drop(columns = \"id\"))\n    \n    # Prepare submission\n    sub = pd.DataFrame({\n        \"id\": df_test[\"id\"],\n        target: pred\n    })\n    \n    # Save submission file\n    sub.to_csv(file_submission, index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-02T11:25:15.012542Z","iopub.execute_input":"2025-11-02T11:25:15.012821Z","iopub.status.idle":"2025-11-02T11:25:15.026037Z","shell.execute_reply.started":"2025-11-02T11:25:15.0128Z","shell.execute_reply":"2025-11-02T11:25:15.025489Z"},"papermill":{"duration":1.355719,"end_time":"2025-10-28T00:10:19.542837","exception":false,"start_time":"2025-10-28T00:10:18.187118","status":"completed"},"tags":[],"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"make_subm(f\"subm_plus.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:25:18.290493Z","iopub.execute_input":"2025-11-02T11:25:18.290996Z","iopub.status.idle":"2025-11-02T11:26:04.45125Z","shell.execute_reply.started":"2025-11-02T11:25:18.290976Z","shell.execute_reply":"2025-11-02T11:26:04.450658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os,ast,copy\nimport numpy as np\nimport pandas as pd\n\nfrom bokeh.plotting import figure, gridplot \nfrom bokeh.io import output_file, show, output_notebook\noutput_notebook()\n\ndef color_scheme(dk,color):\n    colors    = ['red','green','blue']\n    clr_alls  = ['gold','crimson',\"forestgreen\",'mediumblue',\"magenta\"]\n    clr_alls2 = ['crimson','blue']\n    clr_alls3 = ['darkmagenta',\"forestgreen\",'mediumblue']\n    clr_alls4 = ['crimson',\"darkgreen\",'mediumblue','brown']\n    clr_alls4m= ['red',\"forestgreen\",'mediumblue',\"darkmagenta\"]\n    clr_alls4j= ['red',\"green\",'blue',\"sienna\"]\n    clr_alls4i= ['crimson',\"green\",'mediumblue',\"chocolate\"]\n    clr_alls5 = ['red',\"forestgreen\",'mediumblue',\"darkmagenta\",'crimson']\n    clr_Red   = [\"firebrick\",\"orangered\",\"crimson\",'tomato',\"red\"]\n    clr_Red4  = [\"firebrick\",\"orangered\",\"crimson\",'red']\n    clr_Red31 = [\"crimson\",'red','gold']\n    clr_Red13 = ['gold',\"crimson\",'red']\n    clr_Green = [\"darkgreen\",\"limegreen\",\"green\",'lime',\"forestgreen\"]\n    clr_Green2= ['olivedrab',\"darkgreen\",\"forestgreen\"]\n    clr_Green3= [\"darkmagenta\",'olivedrab',\"darkgreen\"]\n    clr_Green4= [\"darkgreen\",\"forestgreen\",\"limegreen\",\"lime\"]\n    clr_Blue  = ['midnightblue',\"royalblue\",\"mediumblue\",\"blue\",\"steelblue\",'cyan']\n    clr_Blue4 = ['midnightblue',\"royalblue\",\"mediumblue\",\"deepskyblue\"]\n    clr_Brown = [\"maroon\",\"sienna\",\"chocolate\",\"sandybrown\",'brown']\n    clr_Brown3= [\"maroon\",\"sienna\",\"sandybrown\"]\n    clr_Brown4= [\"maroon\",\"sienna\",\"chocolate\",\"sandybrown\"]\n    clr_Brown5= [\"maroon\",\"sienna\",\"chocolate\",\"sandybrown\",'gold']\n    clr_Two   = ['crimson','mediumblue']\n    clr_Two2  = ['crimson','darkgreen']\n    clr_tes3  = ['limegreen',\"magenta\",'red']\n    clr_tes3b = ['darkmagenta',\"magenta\",'red']\n    clr_tes6  = ['limegreen'] + clr_Brown\n    clr_tes7  = ['limegreen'] + clr_Brown4 + [\"magenta\"]+[\"darkmagenta\"]\n    clr_tes8  = clr_Red4 + clr_Blue4\n    clr_tes9  = clr_Red4 + ['darkmagenta'] + clr_Blue4\n    clr_tes10 = clr_Brown + clr_Green\n    clr_tes11 = clr_Brown + ['red','darkmagenta'] + clr_Green\n    l = len(dk['subm'])\n    if color == 'Two2'  : colors = clr_Two2   [0:l]\n    if color == 'Two'   : colors = clr_Two    [0:l]\n    if color == 'alls'  : colors = clr_alls   [0:l]\n    if color == 'alls2' : colors = clr_alls2  [0:l]\n    if color == 'alls3' : colors = clr_alls3  [0:l]\n    if color == 'alls4' : colors = clr_alls4  [0:l]\n    if color == 'alls4m': colors = clr_alls4m [0:l]\n    if color == 'alls4i': colors = clr_alls4i [0:l]\n    if color == 'alls4j': colors = clr_alls4j [0:l]\n    if color == 'alls5' : colors = clr_alls5  [0:l]\n    if color == 'red'   : colors = clr_Red    [0:l]\n    if color == 'red4'  : colors = clr_Red4   [0:l]\n    if color == 'red31' : colors = clr_Red31  [0:l]\n    if color == 'red13' : colors = clr_Red13  [0:l]\n    if color == 'green' : colors = clr_Green  [0:l]\n    if color == 'green2': colors = clr_Green2 [0:l]\n    if color == 'green3': colors = clr_Green3 [0:l]\n    if color == 'green4': colors = clr_Green4 [0:l]\n    if color == 'blue'  : colors = clr_Blue   [0:l]\n    if color == 'blue4' : colors = clr_Blue4  [0:l]\n    if color == 'brown' : colors = clr_Brown  [0:l]\n    if color == 'brown3': colors = clr_Brown3 [0:l]\n    if color == 'brown4': colors = clr_Brown4 [0:l]\n    if color == 'brown5': colors = clr_Brown5 [0:l]\n    if color == 'tes3'  : colors = clr_tes3   [0:l]\n    if color == 'tes3b' : colors = clr_tes3b  [0:l]\n    if color == 'tes6'  : colors = clr_tes6   [0:l]\n    if color == 'tes7'  : colors = clr_tes7   [0:l]\n    if color == 'tes8'  : colors = clr_tes8   [0:l]\n    if color == 'tes9'  : colors = clr_tes9   [0:l]\n    if color == 'tes10' : colors = clr_tes10  [0:l]\n    if color == 'tes11' : colors = clr_tes11  [0:l]\n    return colors\n\n\ndef bokeh_show(\n        params,\n        df_cross,\n        colors, \n        show_figures1, \n        show_figures2, wps_fig2,\n        color_cross):\n    \n    def dossier(js,subms,cols):\n        def quant(i,js,subms,cols):\n            return {\"c\" : i, \"q\" : sum([1 for subm in cols[i] if subm == subms[js]])}\n        return {\n            'name' : subms[js],\n            'q_in' : [quant(i,js,subms,cols) for i in range(len(subms))]\n        }\n    alls = pd.read_csv(f'tida_desc.csv')\n    matrix = [ast.literal_eval(str(row.alls)) for row in alls.itertuples()]\n    subms = sorted(matrix[0])\n    cols = [[data[i] for data in matrix] for i in range(len(subms))]\n    df_subms = pd.DataFrame({f'col_{i}': [x[i] for x in matrix] for i in range(len(subms))})\n    dossiers = [dossier(js,subms,cols) for js in range(len(subms))]\n    subm_names = [one_dossier['name'] for one_dossier in dossiers]\n    figures1,qss,i = [],[],0\n    height = 100 if len(colors)==2\\\n        else 134 if len(colors)==3 else (154 if len(colors)==4 else 174)\n    for one_dossier in dossiers: \n        i_col = 'alls. ' + str(one_dossier['q_in'][i]['c'])\n        qs = [one['q'] for one in one_dossier['q_in']]\n        x_names = [name.replace(\"Group\",\"\").replace(\"subm_\",\"\") for name in subm_names]\n        width = 157  if len(colors) == 5\\\n            else (121 if len(colors) == 8\\\n            else (131 if len(colors) == 9\\\n            else (141 if len(colors) == 10\\\n            else (171 if len(colors) == 11 else 100))))\n        f = figure(x_range=x_names,width=width, height=height, title=i_col)\n        f.vbar(x=x_names, width=0.585, top=qs, color=colors)\n        figures1.append(f)\n        qss.append(qs)\n        i+=1\n    grid = gridplot([figures1])\n    output_file('tida_alls.html')\n    if show_figures1 == True: show(grid)\n    sub_wts = params['subwts']\n    main_wts = [subm['weight'] for subm in params['subm']]\n    mms,acc_mass = [],[]\n    for j in range(len(dossiers)):\n        one_dossier = dossiers[j]\n        qs = [one['q'] for one in one_dossier['q_in']]\n        mm = [qs[h] * (main_wts[j] + sub_wts[h]) for h in range(len(sub_wts))]\n        mass = sum(mm)\n        mms.append(mm)\n        acc_mass.append(round(mass))                        #subm_names[::-1]\n    y_names = [name + \" - \" + str(mass) for name,mass in zip(subm_names,acc_mass)]\n    f1 = figure(y_range=y_names, width=313, height=height, title='relations of general masses')\n    f1.hbar(y=y_names, height=0.585, right=acc_mass, left=0, color=colors)\n    output_file('tida_alls2.html')\n    alls = [f'alls.{i}' for i in range(len(dossiers))]\n    subm = [f'sub{i}'   for i in range(len(dossiers))] \n    mmsT  = np.asarray(mms).T\n    data = {'cols' : alls}\n    for i in range(len(dossiers)): data[f'sub{i}'] = mmsT[i,:]\n    f2 = figure(y_range=alls, height=height, width=274, title=\" ( relations of columns masses )\")\n    f2.hbar_stack(subm, y='cols', height=0.585, color=colors, source=data)\n    qssT  = np.asarray(qss).T\n    data = {'cols' : alls}\n    for i in range(len(dossiers)): data[f'sub{i}'] = qssT[i,:]\n    f3 = figure(y_range=alls, height=height, width=215, title=\"ratios in columns\")\n    f3.hbar_stack(subm, y='cols', height=0.585, color=colors, source=data)\n    grid = gridplot([[f3,f2,f1]])\n    show(grid)\n    if show_figures2 == True:\n        def read(params,i):\n            FiN = params[\"path\"] + params[\"subm\"][i][\"name\"] + \".csv\"\n            target_name_back = {'target':params[\"target\"],'pred':params[\"target\"]}\n            return pd.read_csv(FiN).rename(columns=target_name_back)\n        dfs = [read(params,i) for i in range(len(params[\"subm\"]))] + [df_cross]\n        f   = figure(width=800, height=274)\n        f.title.text = 'Click on legend entries to mute the corresponding lines'\n        b,e        = 21000,21121\n        line_x     = [dfs[i][b:e]['id']            for i in range(len(dfs))]\n        line_y     = [dfs[i][b:e]['loan_paid_back'] for i in range(len(dfs))]\n        color      = colors + [color_cross]\n        alpha      = [0.8 for i in range(len(dfs)-1)] + [0.95]\n        lws        = [1.0 for i in range(len(dfs)-1)] + [1.00]\n        legend = subm_names + ['cross']\n        for i in range(len(legend)):\n            f.line(line_x[i], line_y[i], line_width=lws[i], color=color[i], alpha=alpha[i],\n                   muted_color='white',legend_label=legend[i])\n        f.legend.location = \"top_left\"\n        f.legend.click_policy=\"mute\"\n        show(f)\n\n\ndef h_blend(params,color,cross='silver',\n            figures1=False,figures2=False,wf2=555,\n            details=False):\n\n    color_cross = cross\n\n    dk = copy.deepcopy(params)\n\n    show_details,show_figures1,show_figures2 = details,figures1,figures2\n\n    file_short_names = [subm['name'] for subm in params['subm']]\n    type_sort    = params['type_sort'][0]\n    dk['asc']    = params['type_sort'][1]\n    dk['desc']   = params['type_sort'][2]\n    dk['id']     = params['id_target'][0]\n    dk['target'] = params['id_target'][1]\n# ------------------------------------------------------------------------\n    def read(dk,i):\n        tnm = dk[\"subm\"][i][\"name\"]\n        FiN = dk[\"path\"] + tnm + \".csv\"\n        return pd.read_csv(FiN).rename(columns={\n            'target':tnm, 'pred':tnm, dk[\"target\"]:tnm})\n        \n    def merge(dfs_subm):\n        df_subms = pd.merge(dfs_subm[0],  dfs_subm[1], on=[dk['id']])\n        for i in range(2, len(dk[\"subm\"])): \n            df_subms = pd.merge(df_subms, dfs_subm[i], on=[dk['id']])\n        return df_subms\n        \n    def da(dk,sorting_direction,show_details):\n        \n        df_subms = merge([read(dk,i) for i in range(len(dk[\"subm\"]))])\n        cols = [col for col in df_subms.columns if col != dk['id']]\n        short_name_cols = [c for c in cols]\n        \n        def alls1(x, sd=sorting_direction,cs=cols):\n            reverse = True if sd=='desc' else False\n            tes = {c: x[c] for c in cs}.items()\n            subms_sorted = [t[0] for t in sorted(tes,key=lambda k:k[1],reverse=reverse)]\n            return subms_sorted\n\n        import random\n\n        def alls2(x, sd=sorting_direction,cs=cols):\n            reverse = True if sd=='desc' else False\n            tes = {c: x[c] for c in cs}.items()\n            subms_random = [t[0] for t in tes]\n            random.shuffle(subms_random)\n            return subms_random\n\n        alls = alls1 if type_sort == 'asc/desc' else alls2\n            \n        def summa(x,cs,wts,ic_alls): \n            return sum([x[cs[j]] * (wts[0][j] + wts[1][ic_alls[j]]) for j in range(len(cs))])\n            \n        wts = [[[e['weight'] for e in dk[\"subm\"]], [w for w in dk[\"subwts\" ]]]]\n          \n        def correct(x, cs=cols, wts=wts):\n            i = [x['alls'].index(c) for c in short_name_cols]\n            return summa(x,cs,wts[0],i)\n\n        if len(wts) == 1:\n            correct_sub_weights = [wt for wt in dk[\"subwts\"]]\n            weights = [subm['weight'] for subm in dk[\"subm\"]]\n            def correct(x, cs=cols, w=weights, cw=correct_sub_weights):\n                ic = [x['alls'].index(c) for c in short_name_cols]\n                cS = [x[cols[j]] * (w[j] + cw[ic[j]]) for j in range(len(cols))]\n                return sum(cS)\n                   \n        def amxm(x, cs=cols):\n            list_values = x[cs].to_list()\n            mxm = abs(max(list_values)-min(list_values))\n            return mxm\n\n        if len(wts) > 1:\n            df_subms['mx-m']   = df_subms.apply(lambda x: amxm   (x), axis=1)\n        df_subms['alls']       = df_subms.apply(lambda x: alls   (x), axis=1)\n        df_subms[dk[\"target\"]] = df_subms.apply(lambda x: correct(x), axis=1)\n        schema_rename = { old_nc:new_shnc for old_nc, new_shnc in zip(cols, short_name_cols) }\n        df_subms = df_subms.rename(columns=schema_rename)\n        df_subms = df_subms.rename(columns={dk[\"target\"]:\"ensemble\"})\n        df_subms.insert(loc=1, column=' _ ', value=['   '] * len(df_subms))\n        df_subms[' _ '] = df_subms[' _ '].astype(str)\n        pd.set_option('display.max_rows',100)\n        pd.set_option('display.float_format', '{:.5f}'.format)\n        vcols = [dk['id']]+[' _ '] + short_name_cols + [' _ ']+['alls']+[' _ ']+['ensemble']\n        if len(wts) > 1: vcols.append([' _ '] + ['mx-m'])\n        df_subms = df_subms[vcols]\n        if show_details: display(df_subms.head(5))\n        pd.set_option('display.float_format', '{:.5f}'.format)\n        df_subms = df_subms.rename(columns={\"ensemble\":dk[\"target\"]})\n        df_subms.to_csv(f'tida_{sorting_direction}.csv', index=False)\n        return df_subms[[dk['id'],dk['target']]]\n   \n    def ensemble_da(dk,        show_details): \n        dfD    = da(dk,'desc', show_details)\n        dfA    = da(dk,'asc',  show_details)\n        dfA[dk['target']] = dk['desc']*dfD[dk['target']] + dfA[dk['target']]*dk['asc']\n        return dfA\n\n    da = ensemble_da(dk,show_details)\n    colors = color_scheme(dk, color)\n    bokeh_show(dk, da, colors, show_figures1, show_figures2, wf2, color_cross)\n    return  da\n\n\ndef matrix_vs(path,fs_names):\n    def load(path,fs_names):\n        dfs = [pd.read_csv(path + name_subm +'.csv') for name_subm in fs_names]\n        for i in range(len(dfs)):\n            dfs[i] = dfs[i].rename(columns={\"loan_paid_back\": f'{fs_names[i]}'})\n        dfsm = pd.merge(dfs[0], dfs[1], on=\"id\")\n        for i in range(2,len(dfs)):\n            dfsm = pd.merge(dfsm,dfs[i],on='id')\n        return dfsm   \n    def make_list_vs(fs_names):\n        list = []\n        for i in range(0,len(fs_names)-1):\n            for j in range(i+1,len(fs_names)):\n                list.append(fs_names[i] + \"_vs_\" + fs_names[j])\n        return list\n    def get_mvs(dfs, list_vs):\n        def get_abs_distance(x,t1,t2):\n            return abs(x[t1]-x[t2])\n        for vs in list_vs:\n            t = vs.split('_vs_')\n            dfs[vs] = dfs.apply(lambda x: get_abs_distance(x,t[0],t[1]), axis=1)\n        return dfs   \n    def distance_vs(name, st_names, list_vs, dfs):\n        distances = []\n        for st in st_names:\n            vs_between = name + \"_vs_\" + st\n            if vs_between not in list_vs:\n                distances.append(0)\n            else: distances.append(round(dfs[vs_between].sum()))\n        return distances\n    dfs = load(path,fs_names)\n    list_vs = make_list_vs(fs_names)\n    mvs = get_mvs(dfs, list_vs)\n    m1 = pd.DataFrame({'subm':fs_names})\n    m2 = pd.DataFrame({ name :distance_vs(name, fs_names, list_vs, mvs) for name in fs_names})\n    matrix = pd.concat([m1,m2],axis=1)\n    return matrix\n\n\ndef display_distances(params):\n    files = [subm['name'] for subm in params['subm']]\n    distances = matrix_vs ( params['path'], files )            \n    display(distances)","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-02T11:39:00.248113Z","iopub.execute_input":"2025-11-02T11:39:00.248414Z","iopub.status.idle":"2025-11-02T11:39:00.297957Z","shell.execute_reply.started":"2025-11-02T11:39:00.248394Z","shell.execute_reply":"2025-11-02T11:39:00.297316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# subm_minus.LB is already known = 0.92443\ndf = pd.read_csv('/kaggle/input/ps-s5e11-first-glance-1-feats/submission.csv')\ndf . to_csv('subm_minus.csv',index=False)\n\npath = f'/kaggle/working/subm_'\n\nparams = {\n      'path'     : path,            \n      'id_target': ['id',\"loan_paid_back\"],          \n      'type_sort': ['asc/desc', 0.30,0.70],\n      'subwts'   : [ +0.05,-0.05 ],\n      'subm'     : [\n         { 'name': f'minus','weight':+0.50 },\n         { 'name': f'plus', 'weight':+0.50 },]\n}\n\ndf_cross = h_blend(params,color='alls2',figures1=True,figures2=True,details=True)\n\ndisplay_distances(params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:39:04.312999Z","iopub.execute_input":"2025-11-02T11:39:04.313541Z","iopub.status.idle":"2025-11-02T11:39:23.448303Z","shell.execute_reply.started":"2025-11-02T11:39:04.31352Z","shell.execute_reply":"2025-11-02T11:39:23.44746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submit","metadata":{}},{"cell_type":"code","source":"df_cross . to_csv('submission.csv',index=False)\n\ndf_cross","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:40:51.852409Z","iopub.execute_input":"2025-11-02T11:40:51.85271Z","iopub.status.idle":"2025-11-02T11:40:52.324352Z","shell.execute_reply.started":"2025-11-02T11:40:51.85269Z","shell.execute_reply":"2025-11-02T11:40:52.323518Z"}},"outputs":[],"execution_count":null}]}