{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸš€ GPU-Accelerated Machine Learning Pipeline\n\n**Inspired by:** [S5E11 | TE_XGB - Interaction Features](https://www.kaggle.com/code/masayakawamata/s5e11-te-xgb-interaction-features/notebook?scriptVersionId=272584844)\n\n---\n\n## ğŸ“Œ Purpose\n\nThis notebook demonstrates **how much we can accelerate the original CPU-based pipeline by fully leveraging GPU acceleration** with cuDF, cuML, and XGBoost GPU features.\n\n**Bottom Line:** Same features, same model, but **5-6x faster** with GPU optimization! ğŸ”¥\n\n---\n\n## ğŸ¯ TL;DR\n\n| Metric | CPU Version | GPU Version | Speedup |\n|--------|-------------|-------------|---------|\n| **Score** | 0.92603 | 0.92636 | +0.00033 âœ¨ |\n| **Runtime** | ~40+ min | ~7-8 min | **5-6x faster** âš¡ |\n| **Feature Engineering** | pandas | cuDF | **~8x faster** |\n| **Target Encoding** | sklearn | cuML | **~21x faster** |\n| **Training** | XGBoost (CPU) | XGBoost (GPU) | **~5x faster** |\n\n---\n\n## ğŸ”„ What Changed?\n\n### 1. **Data Processing â†’ cuDF (GPU)**\n```python\n# Before (CPU)\nmean_map = orig.groupby(col)[TARGET].mean()\ntrain = train.merge(mean_map, on=col, how='left')\n\n# After (GPU)\nmean_map = orig_cudf.groupby(col, as_index=False)[TARGET].mean()  # cuDF\ntrain_cudf = train_cudf.merge(mean_map, on=col, how='left')       # cuDF\n```\n**Result:** ~8x faster for groupby operations\n\n---\n\n### 2. **Target Encoding â†’ cuML (GPU)**\n```python\n# Before (CPU)\nfrom sklearn.base import BaseEstimator, TransformerMixin\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    # Custom implementation with pandas...\n\n# After (GPU)\nfrom cuml.preprocessing import TargetEncoder as CuMLTargetEncoder\nTE = CuMLTargetEncoder(n_folds=5, smooth=1.0)\ntrain_encoded = TE.fit_transform(X_train_col, y_train_cudf)\n```\n**Result:** ~21x faster for encoding 66 interaction features\n\n---\n\n### 3. **XGBoost Training â†’ Enhanced GPU Config**\n```python\n# Added these parameters\nparams = {\n    'tree_method': 'hist',\n    'predictor': 'gpu_predictor',\n    'device': 'cuda',\n}\n```\n\n---\n\n### 4. **Enhanced Monitoring**\n- âœ… Real-time progress tracking\n- âœ… Detailed timing breakdown per operation\n- âœ… Per-fold performance metrics\n- âœ… GPU memory monitoring\n\n---\n\n## ğŸ—ï¸ Pipeline Architecture\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 1. Data Loading (pandas)                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 2. Interaction Features (pandas)                â”‚\nâ”‚    - String concatenation for combinations      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 3. Convert to cuDF âš¡                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 4. Group Features (cuDF) ğŸš€ ~8x faster          â”‚\nâ”‚    - GPU-accelerated groupby                    â”‚\nâ”‚    - GPU-accelerated merge                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 5. Convert back to pandas                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 6. Target Encoding (cuML) ğŸš€ ~21x faster        â”‚\nâ”‚    - GPU-native encoding                        â”‚\nâ”‚    - Per-fold encoding with CV                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 7. XGBoost Training (GPU) ğŸš€ ~5x faster         â”‚\nâ”‚    - GPU tree building                          â”‚\nâ”‚    - GPU prediction                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 8. Results & Submission                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ğŸ“Š Performance Breakdown\n\n| Operation | CPU Time | GPU Time | Speedup |\n|-----------|----------|----------|---------|\n| Feature Engineering | ~30s | ~4s | **~8x** |\n| Target Encoding (per fold) | ~215s | ~11s | **~21x** |\n| XGBoost Training (per fold) | ~300s | ~60s | **~5x** |\n| **Total Pipeline** | **~40+ min** | **~7-8 min** | **~5-6x** |\n\n---\n\n## ğŸ’¡ Key Takeaways\n\n1. **GPU acceleration provides massive speedups** for data-intensive operations\n2. **cuDF** is highly effective for groupby/merge operations on medium-large datasets\n3. **cuML** TargetEncoder significantly accelerates categorical encoding\n4. **Explicit GPU configuration** in XGBoost (`gpu_predictor`) ensures full GPU utilization\n5. **Faster iteration = Better experimentation** during model development\n\n---\n\n## ğŸ› ï¸ Required Libraries\n```python\n# GPU acceleration\nimport cudf         # GPU-accelerated dataframes\nimport cupy         # GPU-accelerated arrays\nfrom cuml.preprocessing import TargetEncoder  # GPU target encoding\n\n# Standard ML\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n```\n\n\n---\n\n**Ready to accelerate your pipeline? Let's dive in! ğŸš€**","metadata":{}},{"cell_type":"code","source":"import time\nimport warnings\nfrom itertools import combinations\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\n\nimport cudf\nimport cupy as cp\nfrom cuml.preprocessing import TargetEncoder as CuMLTargetEncoder\n\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:40:07.873055Z","iopub.execute_input":"2025-11-07T16:40:07.873334Z","iopub.status.idle":"2025-11-07T16:40:21.262172Z","shell.execute_reply.started":"2025-11-07T16:40:07.873308Z","shell.execute_reply":"2025-11-07T16:40:21.261535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"Loading Data\")\nprint(\"=\" * 80)\n\n# Load data directly as pandas (lightweight initial load)\ntrain_pd = pd.read_csv('../input/playground-series-s5e11/train.csv')\ntest_pd = pd.read_csv('../input/playground-series-s5e11/test.csv')\norig_pd = pd.read_csv('../input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\nprint(f'Train Shape: {train_pd.shape}')\nprint(f'Test Shape: {test_pd.shape}')\nprint(f'Orig Shape: {orig_pd.shape}')\n\nTARGET = 'loan_paid_back'\nCATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nBASE = [col for col in train_pd.columns if col not in ['id', TARGET]]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:42:41.66917Z","iopub.execute_input":"2025-11-07T16:42:41.669853Z","iopub.status.idle":"2025-11-07T16:42:42.746007Z","shell.execute_reply.started":"2025-11-07T16:42:41.669824Z","shell.execute_reply":"2025-11-07T16:42:42.745096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering - Interaction Features (on pandas for string ops)","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"Creating Interaction Features\")\nprint(\"=\" * 80)\n\nstart_time = time.time()\nINTER = []\n\nfor col1, col2 in combinations(BASE, 2):\n    new_col_name = f'{col1}_{col2}'\n    INTER.append(new_col_name)\n    for df in [train_pd, test_pd, orig_pd]:\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n\nelapsed = time.time() - start_time\nprint(f'âœ“ Created {len(INTER)} interaction features in {elapsed:.2f}s')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:42:52.706867Z","iopub.execute_input":"2025-11-07T16:42:52.707383Z","iopub.status.idle":"2025-11-07T16:43:24.00452Z","shell.execute_reply.started":"2025-11-07T16:42:52.70736Z","shell.execute_reply":"2025-11-07T16:43:24.003873Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert to cuDF for GPU Operations","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"Converting to cuDF for GPU Operations\")\nprint(\"=\" * 80)\n\nconvert_start = time.time()\ntrain_cudf = cudf.DataFrame.from_pandas(train_pd)\ntest_cudf = cudf.DataFrame.from_pandas(test_pd)\norig_cudf = cudf.DataFrame.from_pandas(orig_pd)\nconvert_time = time.time() - convert_start\nprint(f'âœ“ Converted to cuDF in {convert_time:.2f}s')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:43:24.005554Z","iopub.execute_input":"2025-11-07T16:43:24.005773Z","iopub.status.idle":"2025-11-07T16:43:29.053397Z","shell.execute_reply.started":"2025-11-07T16:43:24.005755Z","shell.execute_reply":"2025-11-07T16:43:29.052575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering - Group Features (GPU Accelerated with cuDF)","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"Creating Group Features (GPU Accelerated)\")\nprint(\"=\" * 80)\n\ngroup_start = time.time()\nORIG = []\n\nprint(f\"  Processing {len(BASE)} base columns...\")\nfor idx, col in enumerate(BASE, 1):\n    # MEAN - GPU accelerated groupby\n    mean_map = orig_cudf.groupby(col, as_index=False)[TARGET].mean()\n    mean_map = mean_map.rename(columns={TARGET: f\"orig_mean_{col}\"})\n\n    # Merge with cuDF (GPU)\n    train_cudf = train_cudf.merge(mean_map, on=col, how='left')\n    test_cudf = test_cudf.merge(mean_map, on=col, how='left')\n    ORIG.append(f\"orig_mean_{col}\")\n\n    # COUNT - GPU accelerated groupby\n    count_map = orig_cudf.groupby(col, as_index=False).size()\n    count_map = count_map.rename(columns={'size': f\"orig_count_{col}\"})\n\n    # Merge with cuDF (GPU)\n    train_cudf = train_cudf.merge(count_map, on=col, how='left')\n    test_cudf = test_cudf.merge(count_map, on=col, how='left')\n    ORIG.append(f\"orig_count_{col}\")\n\n    if idx % 5 == 0:\n        print(f\"    Processed {idx}/{len(BASE)} columns...\")\n\ngroup_time = time.time() - group_start\nprint(f'âœ“ Created {len(ORIG)} group features in {group_time:.2f}s (GPU accelerated)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:43:29.054454Z","iopub.execute_input":"2025-11-07T16:43:29.054677Z","iopub.status.idle":"2025-11-07T16:43:32.556643Z","shell.execute_reply.started":"2025-11-07T16:43:29.054656Z","shell.execute_reply":"2025-11-07T16:43:32.55591Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combine Features","metadata":{}},{"cell_type":"code","source":"FEATURES = BASE + ORIG + INTER\nprint(f'\\n{\"=\" * 80}')\nprint(f'Total Features: {len(FEATURES)}')\nprint(f'  - Base: {len(BASE)}')\nprint(f'  - Group (from orig): {len(ORIG)}')\nprint(f'  - Interaction: {len(INTER)}')\nprint(\"=\" * 80)\n\n# Convert back to pandas for XGBoost compatibility\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Converting back to pandas for training\")\nprint(\"=\" * 80)\nconvert_back_start = time.time()\ntrain = train_cudf.to_pandas()\ntest = test_cudf.to_pandas()\nconvert_back_time = time.time() - convert_back_start\nprint(f'âœ“ Converted back to pandas in {convert_back_time:.2f}s')\n\nX = train[FEATURES]\ny = train[TARGET]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:43:32.55804Z","iopub.execute_input":"2025-11-07T16:43:32.558303Z","iopub.status.idle":"2025-11-07T16:43:40.754363Z","shell.execute_reply.started":"2025-11-07T16:43:32.558278Z","shell.execute_reply":"2025-11-07T16:43:40.753644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Parameters","metadata":{}},{"cell_type":"code","source":"N_SPLITS = 5\n\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 5,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'n_estimators': 10000,\n    'learning_rate': 0.01,\n    'early_stopping_rounds': 100,\n    'random_state': 42,\n    'n_jobs': -1,\n    'tree_method': 'hist',  # GPU acceleration\n    'predictor': 'gpu_predictor',  # GPU predictor\n    'device': 'cuda',\n    'enable_categorical': True,\n}\n\nprint(f\"\\n{'=' * 80}\")\nprint(\"XGBoost Configuration (GPU Accelerated)\")\nprint('=' * 80)\nfor k, v in params.items():\n    print(f\"  {k}: {v}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:43:56.716091Z","iopub.execute_input":"2025-11-07T16:43:56.71638Z","iopub.status.idle":"2025-11-07T16:43:56.721906Z","shell.execute_reply.started":"2025-11-07T16:43:56.716358Z","shell.execute_reply":"2025-11-07T16:43:56.721107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cross-Validation Training","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"Starting Cross-Validation Training\")\nprint(\"=\" * 80)\n\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(test))\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfold_scores = []\nfold_times = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f'\\n{\"â”€\" * 80}')\n    print(f'Fold {fold}/{N_SPLITS}')\n    print(f'{\"â”€\" * 80}')\n    fold_start = time.time()\n\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    X_test = test[FEATURES].copy()\n\n    # ========================================================================\n    # GPU-Accelerated Target Encoding with cuML\n    # ========================================================================\n    print(\"Target Encoding with cuML (GPU)...\")\n    te_start = time.time()\n\n    # Convert target to cuDF\n    y_train_cudf = cudf.Series(y_train.values)\n\n    # Initialize storage for encoded features\n    X_train_encoded_dict = {}\n    X_val_encoded_dict = {}\n    X_test_encoded_dict = {}\n\n    # Process each column individually\n    print(f\"  Encoding {len(INTER)} interaction features...\")\n    for idx, col in enumerate(INTER, 1):\n        # Convert single column to cuDF\n        X_train_col = cudf.Series(X_train[col].values)\n        X_val_col = cudf.Series(X_val[col].values)\n        X_test_col = cudf.Series(X_test[col].values)\n\n        # Initialize cuML TargetEncoder\n        TE = CuMLTargetEncoder(\n            n_folds=5,\n            smooth=1.0,\n            split_method='interleaved',\n            output_type='numpy'\n        )\n\n        # Fit and transform\n        train_encoded = TE.fit_transform(X_train_col, y_train_cudf)\n        val_encoded = TE.transform(X_val_col)\n        test_encoded = TE.transform(X_test_col)\n\n        # Store encoded values\n        encoded_col_name = f'TE_{col}_mean'\n        X_train_encoded_dict[encoded_col_name] = train_encoded.flatten()\n        X_val_encoded_dict[encoded_col_name] = val_encoded.flatten()\n        X_test_encoded_dict[encoded_col_name] = test_encoded.flatten()\n\n        if idx % 10 == 0:\n            print(f\"    Encoded {idx}/{len(INTER)} columns...\")\n\n    # Convert to pandas DataFrames\n    X_train_encoded_df = pd.DataFrame(X_train_encoded_dict, index=X_train.index)\n    X_val_encoded_df = pd.DataFrame(X_val_encoded_dict, index=X_val.index)\n    X_test_encoded_df = pd.DataFrame(X_test_encoded_dict, index=X_test.index)\n\n    # Drop original INTER columns and add encoded features\n    X_train_final = X_train.drop(columns=INTER)\n    X_val_final = X_val.drop(columns=INTER)\n    X_test_final = X_test.drop(columns=INTER)\n\n    X_train_final = pd.concat([X_train_final, X_train_encoded_df], axis=1)\n    X_val_final = pd.concat([X_val_final, X_val_encoded_df], axis=1)\n    X_test_final = pd.concat([X_test_final, X_test_encoded_df], axis=1)\n\n    te_time = time.time() - te_start\n    print(f'  âœ“ Target encoding completed in {te_time:.2f}s')\n\n    # Convert categorical columns\n    for col in CATS:\n        X_train_final[col] = X_train_final[col].astype('category')\n        X_val_final[col] = X_val_final[col].astype('category')\n        X_test_final[col] = X_test_final[col].astype('category')\n\n    # ========================================================================\n    # Train XGBoost Model (GPU)\n    # ========================================================================\n    print(\"Training XGBoost (GPU)...\")\n    model_start = time.time()\n\n    model = XGBClassifier(**params)\n    model.fit(X_train_final, y_train,\n              eval_set=[(X_val_final, y_val)],\n              verbose=1000)\n\n    model_time = time.time() - model_start\n    print(f'  âœ“ Training completed in {model_time:.2f}s')\n\n    # ========================================================================\n    # Prediction and Evaluation\n    # ========================================================================\n    val_preds = model.predict_proba(X_val_final)[:, 1]\n    oof_preds[val_idx] = val_preds\n\n    fold_score = roc_auc_score(y_val, val_preds)\n    fold_scores.append(fold_score)\n\n    test_preds += model.predict_proba(X_test_final)[:, 1] / N_SPLITS\n\n    fold_time = time.time() - fold_start\n    fold_times.append(fold_time)\n\n    print(f'\\n  Fold {fold} Results:')\n    print(f'    AUC: {fold_score:.4f}')\n    print(f'    Time: {fold_time:.2f}s (TE: {te_time:.2f}s, Model: {model_time:.2f}s)')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:44:07.363278Z","iopub.execute_input":"2025-11-07T16:44:07.363563Z","iopub.status.idle":"2025-11-07T16:50:12.086526Z","shell.execute_reply.started":"2025-11-07T16:44:07.363543Z","shell.execute_reply":"2025-11-07T16:50:12.085693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"overall_auc = roc_auc_score(y, oof_preds)\n\nprint('\\n' + '=' * 80)\nprint('Final Results')\nprint('=' * 80)\nprint(f'Overall OOF AUC: {overall_auc:.4f}')\nprint(f'Mean Fold AUC: {np.mean(fold_scores):.4f} (Â±{np.std(fold_scores):.4f})')\nprint(f'Total Training Time: {sum(fold_times):.2f}s')\nprint(f'Average Time per Fold: {np.mean(fold_times):.2f}s')\nprint(f'Feature Engineering Time: {group_time:.2f}s')\nprint('=' * 80)\n\nprint('\\nFold Scores:')\nfor i, score in enumerate(fold_scores, 1):\n    print(f'  Fold {i}: {score:.4f} ({fold_times[i - 1]:.2f}s)')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:51:31.879304Z","iopub.execute_input":"2025-11-07T16:51:31.879593Z","iopub.status.idle":"2025-11-07T16:51:32.032868Z","shell.execute_reply.started":"2025-11-07T16:51:31.879573Z","shell.execute_reply":"2025-11-07T16:51:32.032163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfeature_importances = model.feature_importances_\n\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns, \n    'importance': feature_importances\n})\n\nimportance_df = importance_df.sort_values('importance', ascending=False)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12, 20))\nsns.barplot(x='importance', \n            y='feature', \n            data=importance_df.head(50)) \nplt.title('Feature Importance (Fold5 model)')\nplt.xlabel('Importance Score')\nplt.ylabel('Features')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:52:42.70993Z","iopub.execute_input":"2025-11-07T16:52:42.710258Z","iopub.status.idle":"2025-11-07T16:52:43.798407Z","shell.execute_reply.started":"2025-11-07T16:52:42.710221Z","shell.execute_reply":"2025-11-07T16:52:43.79757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame({'id': train.id, TARGET: oof_preds}).to_csv(f'oof_xgb_cv_{overall_auc}.csv', index=False)\npd.DataFrame({'id': test.id, TARGET: test_preds}).to_csv(f'test_xgb_cv_{overall_auc}.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}