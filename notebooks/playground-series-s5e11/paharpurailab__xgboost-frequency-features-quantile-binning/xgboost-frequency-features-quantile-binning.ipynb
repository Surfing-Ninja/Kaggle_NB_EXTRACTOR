{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† XGBoost + Frequency Features + Quantile Binning\n\n### üöÄ Playground Series S5E11 ‚Äî Loan Paid Back Prediction\n\nThis notebook trains a strong **XGBoost model** with enhanced feature engineering using **frequency encoding** and **quantile binning**, followed by **cross-validation** to find the best boosting round. Finally, it generates a submission file for Kaggle.\n\n---\n\n## üì¶ 1. Import Libraries\n\n```python\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n```\n\n---\n\n## üìÇ 2. Load Data\n\n```python\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\ntarget = train.columns[-1]\n```\n\n---\n\n## ‚öôÔ∏è 3. Feature Engineering ‚Äî Frequency & Quantile Binning\n\n```python\ndef create_frequency_features(train_df, test_df, cols, num, cat):\n    train, test = train_df.copy(), test_df.copy()\n\n    for col in cols:\n        # Frequency encoding\n        freq = train[col].value_counts(normalize=True)\n        train[f\"{col}_freq\"] = train[col].map(freq)\n        test[f\"{col}_freq\"] = test[col].map(freq).fillna(train[f\"{col}_freq\"].mean())\n\n        # Quantile binning for numeric columns\n        if col in num:\n            for q in [5, 10, 15]:\n                try:\n                    train[f\"{col}_bin{q}\"], bins = pd.qcut(train[col], q=q, labels=False, retbins=True, duplicates=\"drop\")\n                    test[f\"{col}_bin{q}\"] = pd.cut(test[col], bins=bins, labels=False, include_lowest=True)\n                except Exception:\n                    train[f\"{col}_bin{q}\"] = test[f\"{col}_bin{q}\"] = 0\n\n    return train, test\n```\n\n---\n\n## üîç 4. Identify Categorical and Numerical Columns\n\n```python\ncols = train.drop(columns=target).columns.tolist()\ncat = [col for col in cols if train[col].dtype in [\"object\", \"category\"]]\nnum = [col for col in cols if train[col].dtype not in [\"object\", \"category\", \"bool\"] and col not in [\"id\", target]]\n```\n\n---\n\n## üß© 5. Create Enhanced Features\n\n```python\ntrain, test = create_frequency_features(train, test.copy(), cols, num, cat)\ntrain[cat], test[cat] = train[cat].astype(\"category\"), test[cat].astype(\"category\")\ntrain.drop(columns=\"id\", inplace=True)\ntrain.drop_duplicates(inplace=True)\n```\n\n---\n\n## ‚ö° 6. XGBoost Cross-Validation\n\n```python\ndtrain = xgb.DMatrix(\n    train.drop(columns=target),\n    label=train[target],\n    enable_categorical=True\n)\n\nparams = {\n    'tree_method': 'hist',\n    'device': 'cuda',\n    'eval_metric': 'auc',\n    'objective': 'binary:logistic',\n    'random_state': 42,\n    'max_depth': 4,\n    'scale_pos_weight': 1\n}\n\ncv_results = xgb.cv(\n    params=params,\n    dtrain=dtrain,\n    nfold=5,\n    num_boost_round=2000,\n    metrics='auc',\n    verbose_eval=False,\n    early_stopping_rounds=50\n)\n\nbest_round = cv_results['test-auc-mean'].idxmax()\nbest_auc = cv_results['test-auc-mean'][best_round]\n```\n\n---\n\n## üß† 7. Train Final XGBoost Model\n\n```python\nmodel = XGBClassifier(**params, enable_categorical=True, n_estimators=best_round)\nmodel.fit(train.drop(columns=target), train[target])\n```\n\n---\n\n## üìä 8. Predict and Create Submission\n\n```python\npred = model.predict_proba(test.drop(columns=\"id\"))[:, 1]\n\nsub = pd.DataFrame({\n    \"id\": test[\"id\"],\n    target: pred\n})\n\nsub.to_csv(\"submission.csv\", index=False)\n```\n\n---\n\n## üèÅ Result\n\n‚úÖ **Model:** XGBoost with Frequency + Quantile Features\n‚úÖ **Evaluation Metric:** AUC\n‚úÖ **Output:** `submission.csv`\n\n---\n\nWould you like me to add a **second section for Optuna tuning** (with visualization of best parameters and progress bar)?\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\ntarget = train.columns[-1]\n\ndef create_frequency_features(train_df, test_df, cols, num, cat):\n    train, test = train_df.copy(), test_df.copy()\n\n    for col in cols:\n        freq = train[col].value_counts(normalize=True)\n        train[f\"{col}_freq\"] = train[col].map(freq)\n        test[f\"{col}_freq\"] = test[col].map(freq).fillna(train[f\"{col}_freq\"].mean())\n\n        if col in num:\n            for q in [5, 10, 15]:\n                try:\n                    train[f\"{col}_bin{q}\"], bins = pd.qcut(train[col], q=q, labels=False, retbins=True, duplicates=\"drop\")\n                    test[f\"{col}_bin{q}\"] = pd.cut(test[col], bins=bins, labels=False, include_lowest=True)\n                except Exception:\n                    train[f\"{col}_bin{q}\"] = test[f\"{col}_bin{q}\"] = 0\n\n    new_num = train.drop(columns=cat + [target]).columns.tolist()\n    return train, test\n\ncols = train.drop(columns=target).columns.tolist()\ncat = [col for col in cols if train[col].dtype in [\"object\", \"category\"] and col != target]\nnum = [col for col in cols if train[col].dtype not in [\"object\", \"category\", \"bool\"] and col not in [\"id\", target]]\n\ntrain, test = create_frequency_features(train, test.copy(), cols, num, cat)\ntrain[cat], test[cat] = train[cat].astype(\"category\"), test[cat].astype(\"category\")\ntrain.drop(columns=\"id\", inplace=True)\ntrain.drop_duplicates(inplace=True)\n\ndtrain = xgb.DMatrix(\n    train.drop(columns=target),\n    label=train[target],\n    enable_categorical=True\n)\n\nparams = {\n    'tree_method': 'hist',\n    'device': 'cuda',\n    'eval_metric': 'auc',\n    'objective': 'binary:logistic',\n    'random_state': 42,\n    'max_depth': 4,\n    'scale_pos_weight': 1\n}\n\ncv_results = xgb.cv(\n    params=params,\n    dtrain=dtrain,\n    nfold=5,\n    num_boost_round=2000,\n    metrics='auc',\n    verbose_eval=False,\n    early_stopping_rounds=50\n)\n\nbest_round = cv_results['test-auc-mean'].idxmax()\nbest_auc = cv_results['test-auc-mean'][best_round]\n\nmodel = XGBClassifier(**params, enable_categorical=True, n_estimators=best_round)\nmodel.fit(train.drop(columns=target), train[target])\n\npred = model.predict_proba(test.drop(columns=\"id\"))[:, 1]\n\nsub = pd.DataFrame({\n    \"id\": test[\"id\"],\n    target: pred\n})\nsub.to_csv(\"submission.csv\", index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:52:33.383486Z","iopub.execute_input":"2025-11-01T04:52:33.384058Z","iopub.status.idle":"2025-11-01T04:52:59.743955Z","shell.execute_reply.started":"2025-11-01T04:52:33.384009Z","shell.execute_reply":"2025-11-01T04:52:59.743199Z"}},"outputs":[],"execution_count":null}]}