{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672},{"sourceId":273334165,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **FOREWORD**\n\nThanks to the baseline work [here](https://www.kaggle.com/code/masayakawamata/s5e11-te-xgb-interaction-features). I add a couple of ML models to the original work and complete my baseline work\n\nThis is my starter work for the November Playground 2025 challenge using the train and original as rows and interaction features with mean and count encoders. <br>I also add a few quantile binner features for numeric columns from [here](https://www.kaggle.com/code/yousefelshahat2/simple-xgboost-only-competition-data-s5e11/notebook)","metadata":{}},{"cell_type":"markdown","source":"# **IMPORTS**","metadata":{}},{"cell_type":"code","source":"import warnings, torch\nimport pandas as pd, numpy as np\nwarnings.simplefilter('ignore')\nfrom itertools import combinations\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom xgboost import XGBClassifier as XGBC\nfrom lightgbm import LGBMClassifier as LGBMC, log_evaluation, early_stopping\nfrom catboost import CatBoostClassifier as CBC\nfrom sklearn.metrics import *\n\nfrom sklearn.base import BaseEstimator, TransformerMixin","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T05:57:18.636942Z","iopub.execute_input":"2025-11-03T05:57:18.637507Z","iopub.status.idle":"2025-11-03T05:57:18.642147Z","shell.execute_reply.started":"2025-11-03T05:57:18.637482Z","shell.execute_reply":"2025-11-03T05:57:18.641242Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_req = False\n\nif test_req :\n    print(\"THIS IS A SYNTAX CHECK RUN\")\n    nest = 150\n    es   = 50\nelse:\n    nest = 12000\n    es   = 300","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T05:57:20.879242Z","iopub.execute_input":"2025-11-03T05:57:20.879915Z","iopub.status.idle":"2025-11-03T05:57:20.884222Z","shell.execute_reply.started":"2025-11-03T05:57:20.879892Z","shell.execute_reply":"2025-11-03T05:57:20.883379Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)\nprint('Orig Shape:', orig.shape)\n\nTARGET = 'loan_paid_back'\nCATS   = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nBASE   = [col for col in train.columns if col not in ['id', TARGET]]\nNUMS   = list(set(BASE) - set(CATS))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T05:57:23.156143Z","iopub.execute_input":"2025-11-03T05:57:23.156377Z","iopub.status.idle":"2025-11-03T05:57:24.669527Z","shell.execute_reply.started":"2025-11-03T05:57:23.156361Z","shell.execute_reply":"2025-11-03T05:57:24.668685Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **FEATURE ENGINEERING**\n\n- Original as columns\n- Combination interactions\n- k-bins discretization","metadata":{}},{"cell_type":"code","source":"\nINTER = []\n\nfor col1, col2 in tqdm(combinations(BASE, 2)):\n    new_col_name = f'{col1}_{col2}'\n    INTER.append(new_col_name)\n    for df in [train, test, orig] :\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n        \nprint(f'{len(INTER)} Features')\n\nfor col1, col2, col3 in combinations(CATS, 3 ):\n    new_col_name = f'{col1}_{col2}_{col3}'\n    INTER.append(new_col_name)\n    for df in [train, test, orig]:\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str) + df[col3].astype(str)\n        \nprint(f'{len(INTER)} Features')\n\nORIG = []\n\nfor col in BASE:\n    # MEAN\n    mean_map = orig.groupby(col)[TARGET].mean()\n    new_mean_col_name = f\"orig_mean_{col}\"\n    mean_map.name = new_mean_col_name\n    \n    train = train.merge(mean_map, on=col, how='left')\n    test = test.merge(mean_map, on=col, how='left')\n    ORIG.append(new_mean_col_name)\n\n    # COUNT\n    new_count_col_name = f\"orig_count_{col}\"\n    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n    \n    train = train.merge(count_map, on=col, how='left')\n    test  = test.merge(count_map, on=col, how='left')\n    ORIG.append(new_count_col_name)\n\nprint(len(ORIG), 'Orig Features Created!!')\n\nBINS = []\nfor col in NUMS :\n    for q in [5, 10, 15, 20, 25]:\n        try:\n            train_bins, bins  = pd.qcut(\n                df[col], q=q, labels=False, retbins=True, duplicates=\"drop\"\n            )\n            train[f\"{col}_bin{q}\"] = train_bins\n            \n            test[f\"{col}_bin{q}\"] = pd.cut(\n                df_test[col], bins=bins, labels=False, include_lowest=True\n            )\n            \n        except Exception:\n            train[f\"{col}_bin{q}\"] = 0\n            test[f\"{col}_bin{q}\"] = 0\n\n        BINS.append(f\"{col}_bin{q}\")\n\nprint(len(BINS), 'Discretization Features Created!!')","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-03T05:57:27.524811Z","iopub.execute_input":"2025-11-03T05:57:27.525618Z","iopub.status.idle":"2025-11-03T05:58:39.094077Z","shell.execute_reply.started":"2025-11-03T05:57:27.525589Z","shell.execute_reply":"2025-11-03T05:58:39.093222Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = BASE + ORIG + INTER + BINS\nprint(len(FEATURES), 'Features.')\n\nX = train[FEATURES]\ny = train[TARGET]\n\nwith np.printoptions(linewidth = 150, threshold = 10000):\n    print(f\"\\n\\n---> Selected features\\n\\n\")\n    print(np.array(FEATURES))\n    print()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-03T05:58:59.420549Z","iopub.execute_input":"2025-11-03T05:58:59.421143Z","iopub.status.idle":"2025-11-03T05:59:00.168714Z","shell.execute_reply.started":"2025-11-03T05:58:59.421118Z","shell.execute_reply":"2025-11-03T05:59:00.168085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **MODEL TRAINING**","metadata":{}},{"cell_type":"code","source":"\nN_SPLITS = 5\nskf      = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n\nMdl_Master = \\\n{     \n f'XGB1C'  : [\n              XGBC(**{ \"objective\"            : \"binary:logistic\",\n                       \"eval_metric\"          : \"auc\",\n                       'device'               : \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n                       'learning_rate'        : 0.005,\n                       'n_estimators'         : nest,\n                       'max_depth'            : 7,\n                       'subsample'            : 0.90,\n                       'colsample_bytree'     : 0.55,\n                       'reg_lambda'           : 1.85,\n                       'reg_alpha'            : 0.01,\n                       'verbosity'            : 0,\n                       'random_state'         : 42,\n                       'enable_categorical'   : True,\n                       'early_stopping_rounds' : es,\n                      } \n                   ),\n              {\"verbose\" : 0}\n             ],\n}\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:23:25.838835Z","iopub.execute_input":"2025-11-03T06:23:25.83918Z","iopub.status.idle":"2025-11-03T06:23:25.849094Z","shell.execute_reply.started":"2025-11-03T06:23:25.839156Z","shell.execute_reply":"2025-11-03T06:23:25.848391Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Target Encoder that supports multiple aggregation functions,\n    internal cross-validation for leakage prevention, and smoothing.\n\n    Parameters\n    ----------\n    cols_to_encode : list of str\n        List of column names to be target encoded.\n\n    aggs : list of str, default=['mean']\n        List of aggregation functions to apply. Any function accepted by\n        pandas' `.agg()` method is supported, such as:\n        'mean', 'std', 'var', 'min', 'max', 'skew', 'nunique', \n        'count', 'sum', 'median'.\n        Smoothing is applied only to the 'mean' aggregation.\n\n    cv : int, default=5\n        Number of folds for cross-validation in fit_transform.\n\n    smooth : float or 'auto', default='auto'\n        The smoothing parameter `m`. A larger value puts more weight on the \n        global mean. If 'auto', an empirical Bayes estimate is used.\n        \n    drop_original : bool, default=False\n        If True, the original columns to be encoded are dropped.\n    \"\"\"\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n        self.cols_to_encode = cols_to_encode\n        self.aggs = aggs\n        self.cv = cv\n        self.smooth = smooth\n        self.drop_original = drop_original\n        self.mappings_ = {}\n        self.global_stats_ = {}\n\n    def fit(self, X, y):\n        \"\"\"\n        Learn mappings from the entire dataset.\n        These mappings are used for the transform method on validation/test data.\n        \"\"\"\n        temp_df = X.copy()\n        temp_df['target'] = y\n\n        # Learn global statistics for each aggregation\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n\n        # Learn category-specific mappings\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n        \n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Apply learned mappings to the data.\n        Unseen categories are filled with global statistics.\n        \"\"\"\n        X_transformed = X.copy()\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n                new_col_name = f'TE_{col}_{agg_func}'\n                map_series = self.mappings_[col][agg_func]\n                X_transformed[new_col_name] = X[col].map(map_series)\n                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n        \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        Fit and transform the data using internal cross-validation to prevent leakage.\n        \"\"\"\n        # First, fit on the entire dataset to get global mappings for transform method\n        self.fit(X, y)\n\n        # Initialize an empty DataFrame to store encoded features\n        encoded_features = pd.DataFrame(index=X.index)\n        \n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n\n        for train_idx, val_idx in kf.split(X, y):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx]\n            \n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n\n            for col in self.cols_to_encode:\n                # --- Calculate mappings only on the training part of the fold ---\n                for agg_func in self.aggs:\n                    new_col_name = f'TE_{col}_{agg_func}'\n                    \n                    # Calculate global stat for this fold\n                    fold_global_stat = y_train.agg(agg_func)\n                    \n                    # Calculate category stats for this fold\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n\n                    # --- Apply smoothing only for 'mean' aggregation ---\n                    if agg_func == 'mean':\n                        counts = temp_df_train.groupby(col)['target'].count()\n                        \n                        m = self.smooth\n                        if self.smooth == 'auto':\n                            # Empirical Bayes smoothing\n                            variance_between = mapping.var()\n                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n                            if variance_between > 0:\n                                m = avg_variance_within / variance_between\n                            else:\n                                m = 0  # No smoothing if no variance between groups\n                        \n                        # Apply smoothing formula\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n                        encoded_values = X_val[col].map(smoothed_mapping)\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n                    \n                    # Store encoded values for the validation fold\n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n\n        # Merge with original DataFrame\n        X_transformed = X.copy()\n        for col in encoded_features.columns:\n            X_transformed[col] = encoded_features[col]\n            \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:23:29.65355Z","iopub.execute_input":"2025-11-03T06:23:29.654258Z","iopub.status.idle":"2025-11-03T06:23:29.666941Z","shell.execute_reply.started":"2025-11-03T06:23:29.654233Z","shell.execute_reply":"2025-11-03T06:23:29.666192Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nOOF_Preds    = []\nMdl_Preds    = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    \n    print(f'--- Fold {fold}/{N_SPLITS} ---')\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    X_test         = test[FEATURES].copy()\n\n    TE      = TargetEncoder(\n        cols_to_encode=INTER, \n        cv=5, \n        smooth='auto', \n        aggs=['mean'], \n        drop_original=True\n    )\n    \n    X_train = TE.fit_transform(X_train, y_train)\n    X_val   = TE.transform(X_val)\n    X_test  = TE.transform(X_test)\n\n    X_train[CATS] = X_train[CATS].astype('category')\n    X_val[CATS]   = X_val[CATS].astype('category')\n    X_test[CATS]  = X_test[CATS].astype('category')\n\n    oof_preds , test_preds = [], []\n    \n    for method, (model, fit_params) in Mdl_Master.items():\n  \n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], **fit_params)\n        val_preds = pd.Series(model.predict_proba(X_val)[:, 1], name = method, index = val_idx)\n        oof_preds.append(val_preds)\n        test_preds.append(pd.Series( model.predict_proba(X_test)[:, 1], name = method))\n\n        print(f\"---> Model {method} fitted successfully\")\n\n    oof_preds  = pd.concat(oof_preds, axis=1)\n    test_preds = pd.concat(test_preds, axis=1) \n\n    OOF_Preds.append(oof_preds)\n    Mdl_Preds.append(test_preds)\n\nOOF_Preds = pd.concat(OOF_Preds, axis=0, ignore_index = False).sort_index(ascending = True)\nMdl_Preds = pd.concat(Mdl_Preds, axis=0, ignore_index = False).groupby(level = 0).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:23:33.472561Z","iopub.execute_input":"2025-11-03T06:23:33.473071Z","iopub.status.idle":"2025-11-03T06:38:38.006232Z","shell.execute_reply.started":"2025-11-03T06:23:33.473048Z","shell.execute_reply":"2025-11-03T06:38:38.005375Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **ENSEMBLE**\n\nLet's do a simple average for now and then submit. Ensembling is a key step for Playground competitions and this step needs to be improved","metadata":{}},{"cell_type":"code","source":"for col in OOF_Preds.columns :\n    score  = roc_auc_score(y, OOF_Preds[col])\n    print(f\"---> Score = {score:,.8f} | {col}\")\n\noof_preds = np.mean(OOF_Preds.values,  axis=1)\nscore     = roc_auc_score(y, oof_preds)\nprint(f\"\\n---> Average Ensemble Score = {score:,.8f}\\n\")\n\ntest_preds = Mdl_Preds.mean(axis = 1).values\n\nsub_fl = pd.read_csv(\n    f\"/kaggle/input/playground-series-s5e11/sample_submission.csv\",\n    index_col = \"id\",\n)\n\n# Blending with same CV scheme public notebook\nsub_public = pd.read_csv(\n    f\"/kaggle/input/ps5e11-agentic-ai-solution-single-xgb/submission_4.csv\"\n)[\"loan_paid_back\"].values.flatten()\n\nsub_fl[\"loan_paid_back\"] = test_preds * 0.20 + 0.80 * sub_public\n\nsub_fl.to_csv(f\"submission.csv\", index = True)\n\nprint()\n!head submission.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:46:07.241384Z","iopub.execute_input":"2025-11-03T06:46:07.241995Z","iopub.status.idle":"2025-11-03T06:46:08.390193Z","shell.execute_reply.started":"2025-11-03T06:46:07.241971Z","shell.execute_reply":"2025-11-03T06:46:08.389191Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null}]}