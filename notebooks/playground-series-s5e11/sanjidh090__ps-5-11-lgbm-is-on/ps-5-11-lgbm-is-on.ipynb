{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:48:14.459258Z","iopub.execute_input":"2025-11-04T14:48:14.459562Z","iopub.status.idle":"2025-11-04T14:48:16.075045Z","shell.execute_reply.started":"2025-11-04T14:48:14.459539Z","shell.execute_reply":"2025-11-04T14:48:16.074325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Use the path that works in your environment.\n# Assuming you are running this where the files are accessible.\ntry:\n    df_train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\n    \n    print(\"--- Loaded df_train Columns ---\")\n    # Print all column names\n    print(df_train.columns.tolist())\n    \n    # Print the first few rows to check if the data looks correct\n    print(\"\\n--- df_train Head (First 5 Rows) ---\")\n    print(df_train.head())\n\nexcept FileNotFoundError:\n    print(\"Error: The file path '/kaggle/input/playground-series-s5e11/train.csv' was not found.\")\n    print(\"Please check the path and make sure you are loading the correct file.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:48:36.820128Z","iopub.execute_input":"2025-11-04T14:48:36.82041Z","iopub.status.idle":"2025-11-04T14:48:38.229711Z","shell.execute_reply.started":"2025-11-04T14:48:36.820391Z","shell.execute_reply":"2025-11-04T14:48:38.228992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport os\nimport sys\n\n# --- 0. Configuration and Environment Check ---\n\n# Check if a GPU environment is available, otherwise default to CPU\ntry:\n    # A simple check for the necessary CUDA environment (may fail on some setups)\n    import cupy\n    XGB_DEVICE = 'cuda' \n    XGB_TREE_METHOD = 'gpu_hist'\n    print(\"Using GPU (CUDA) environment.\")\nexcept:\n    XGB_DEVICE = 'cpu'\n    XGB_TREE_METHOD = 'hist'\n    print(\"GPU environment not detected or failed to initialize. Using CPU.\")\n\n\n# Correct TARGET variable name based on your file's columns\nTARGET = 'loan_paid_back' \nN_FOLDS = 5 # Number of folds for Cross-Validation and OOF Encoding\n\n# Best hyper-parameters found by Optuna in the original notebook (AUC: 0.926017)\n# Note: n_jobs=-1 is handled implicitly by device='cuda' or nthread in 'cpu'\nOPTIMAL_PARAMS = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'tree_method': XGB_TREE_METHOD,\n    'device': XGB_DEVICE,\n    'n_estimators': 2000,\n    'learning_rate': 0.015112,\n    'max_depth': 12,\n    'min_child_weight': 11,\n    'gamma': 0.00755,\n    'subsample': 0.887,\n    'colsample_bytree': 0.835,\n    'colsample_bylevel': 0.812,\n    'colsample_bynode': 0.701,\n    'reg_alpha': 0.000305,\n    'reg_lambda': 0.000495,\n    'seed': 42,\n}\n\n# --- 1. Data Loading ---\nINPUT_PATH = '/kaggle/input/playground-series-s5e11/'\nprint(f\"Loading data from: {INPUT_PATH}\")\n\ntry:\n    df_train = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'))\n    df_test = pd.read_csv(os.path.join(INPUT_PATH, 'test.csv'))\n    \n    y = df_train[TARGET].astype(int)\n    test_ids = df_test['id']\n    df_train.drop(TARGET, axis=1, inplace=True)\n    \n    # Combine for feature engineering, dropping 'id' as it's not a feature\n    df_full = pd.concat([df_train.drop('id', axis=1), df_test.drop('id', axis=1)], ignore_index=True)\n\n    print(f\"Train shape: {df_train.shape}, Test shape: {df_test.shape}\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error loading files. Check the INPUT_PATH and file names: {e}\")\n    sys.exit()\n\n\n# --- 2. Advanced Feature Engineering (Replicating Notebook Concepts) ---\n\nprint(\"\\n--- Starting Advanced Feature Engineering ---\")\n\nCAT_COLS = ['gender', 'marital_status', 'education_level', \n            'employment_status', 'loan_purpose', 'grade_subgrade']\nNUM_COLS = ['annual_income', 'debt_to_income_ratio', 'credit_score', \n            'loan_amount', 'interest_rate']\n\n# Fill NaNs for the full dataset BEFORE splitting\nfor col in NUM_COLS:\n    # Missingness Indicator: Fill NaNs with -999\n    df_full[col].fillna(-999, inplace=True)\n\nfor col in CAT_COLS:\n    # Categorical NaN handling: Treat NaNs as a new category 'Missing'\n    df_full[col].fillna('Missing', inplace=True)\n    \n    # --- Frequency Encoding ---\n    count_map = df_full[col].value_counts().to_dict()\n    df_full[f'{col}_freq_enc'] = df_full[col].map(count_map)\n\n# --- Group Mean Deviation Feature Creation (Interaction Stats) ---\nfor group_col in ['grade_subgrade', 'education_level']:\n    for agg_col in ['annual_income', 'credit_score']:\n        # Calculate the mean of 'agg_col' for each category in 'group_col'\n        group_mean = df_full.groupby(group_col)[agg_col].transform('mean')\n        \n        # Deviation = Value - Group Mean\n        df_full[f'{agg_col}_dev_by_{group_col}'] = df_full[agg_col] - group_mean\n        \nprint(\"Feature Engineering Complete. Full shape:\", df_full.shape)\n\n# --- Separate back into training and testing sets ---\nX = df_full.iloc[:len(y)]\nX_test = df_full.iloc[len(y):]\n\n\n# --- 3. Out-of-Fold (OOF) Target Encoding ---\n\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\noof_encoded_cols = [f'{col}_oof_enc' for col in CAT_COLS]\noof_df = pd.DataFrame(index=X.index, columns=oof_encoded_cols)\nglobal_mean = y.mean() # Mean of the whole target used for unknown/rare categories\n\nprint(f\"Starting {N_FOLDS}-Fold OOF Target Encoding...\")\nfor train_idx, val_idx in kf.split(X, y):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold = y.iloc[train_idx]\n    \n    for col in CAT_COLS:\n        # Calculate target mean on the training fold\n        target_mean_map = y_train_fold.groupby(X_train_fold[col]).mean()\n        \n        # Apply the mean to the validation fold\n        oof_df.loc[val_idx, f'{col}_oof_enc'] = X_val_fold[col].map(target_mean_map)\n\n# Add the OOF encoded features to the training set\nX = pd.concat([X, oof_df], axis=1)\n\n# For the test set, use the global target mean map\nfor col in CAT_COLS:\n    global_target_mean = y.groupby(X[col]).mean()\n    X_test[f'{col}_oof_enc'] = X_test[col].map(global_target_mean)\n\n# --- CRITICAL FIX: Ensure OOF columns are float and handle all remaining NaNs ---\nfor col in oof_encoded_cols:\n    X[col] = X[col].astype(float)\n    X_test[col] = X_test[col].astype(float)\n\n    # Fill NaNs (for categories not seen in the fold or new in test set) with the overall target mean\n    X[col].fillna(global_mean, inplace=True)\n    X_test[col].fillna(global_mean, inplace=True)\n\n# Drop original categorical columns now that they are fully encoded\nX.drop(CAT_COLS, axis=1, inplace=True)\nX_test.drop(CAT_COLS, axis=1, inplace=True)\n\n# Final check of column alignment (CRITICAL for XGBoost)\nX_test = X_test[X.columns]\n\n\n# --- 4. Model Training (Single XGBoost Model) ---\nprint(\"\\n--- Starting XGBoost Training ---\")\n\n# Pass optimal parameters including the determined device and tree method\nmodel = xgb.XGBClassifier(**OPTIMAL_PARAMS)\n\n# Train the model\nmodel.fit(\n    X, y,\n    eval_set=[(X, y)],\n    early_stopping_rounds=100,\n    verbose=False\n)\n\nprint(f\"Model trained with {model.best_iteration} boosting rounds.\")\nprint(f\"Training AUC: {roc_auc_score(y, model.predict_proba(X)[:, 1]):.6f}\")\n\n\n# --- 5. Prediction and Submission ---\n# Predict probabilities on the test set\npredictions = model.predict_proba(X_test)[:, 1]\n\n# Create submission file\nsubmission_df = pd.DataFrame({'id': test_ids, TARGET: predictions})\nsubmission_df.to_csv('submission_advanced_xgb.csv', index=False)\n\nprint(\"\\n--- Submission File Created ---\")\nprint(\"File: submission_advanced_xgb.csv\")\nprint(f\"Prediction head:\\n{submission_df.head()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:58:34.839977Z","iopub.execute_input":"2025-11-04T14:58:34.840717Z","iopub.status.idle":"2025-11-04T14:59:42.565389Z","shell.execute_reply.started":"2025-11-04T14:58:34.840682Z","shell.execute_reply":"2025-11-04T14:59:42.564408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_auc_score\nimport os\n\n# --- 0. Configuration and Hyperparameters ---\n\nTARGET = 'loan_paid_back' \nXGB_DEVICE = 'cuda' \nXGB_TREE_METHOD = 'hist' if XGB_DEVICE == 'cpu' else 'gpu_hist'\nN_FOLDS = 5 \nGLOBAL_SEED = 42\n\nOPTIMAL_PARAMS = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'tree_method': XGB_TREE_METHOD,\n    'device': XGB_DEVICE,\n    'n_estimators': 2000,\n    'learning_rate': 0.015112,\n    'max_depth': 12,\n    'min_child_weight': 11,\n    'gamma': 0.00755,\n    'subsample': 0.887,\n    'colsample_bytree': 0.835,\n    'colsample_bylevel': 0.812,\n    'colsample_bynode': 0.701,\n    'reg_alpha': 0.000305,\n    'reg_lambda': 0.000495,\n    'seed': GLOBAL_SEED,\n    'n_jobs': -1,\n}\n\n# --- 1. Data Loading ---\nINPUT_PATH = '/kaggle/input/playground-series-s5e11/'\n\ntry:\n    df_train = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'))\n    df_test = pd.read_csv(os.path.join(INPUT_PATH, 'test.csv'))\n    \n    y = df_train[TARGET].astype(int)\n    test_ids = df_test['id']\n    df_train.drop(TARGET, axis=1, inplace=True)\n    \n    df_full = pd.concat([df_train.drop('id', axis=1), df_test.drop('id', axis=1)], ignore_index=True)\n\nexcept FileNotFoundError as e:\n    print(f\"Error loading files. Check the INPUT_PATH: {e}\")\n    exit()\n\n\n# --- 2. Advanced Feature Engineering (With Missingness Flags) ---\n\nprint(\"\\n--- Starting Advanced Feature Engineering ---\")\n\nCAT_COLS = ['gender', 'marital_status', 'education_level', \n            'employment_status', 'loan_purpose', 'grade_subgrade']\nNUM_COLS = ['annual_income', 'debt_to_income_ratio', 'credit_score', \n            'loan_amount', 'interest_rate']\n\n# Create Missingness Flags (Crucial for high AUC models)\nfor col in NUM_COLS:\n    df_full[f'{col}_is_missing'] = df_full[col].isna().astype(int)\n    # Fill actual NaN values with -999 (Sentinel value for XGBoost)\n    df_full[col].fillna(-999, inplace=True)\n\n# Categorical NaN handling: Treat NaNs as a new category 'Missing'\nfor col in CAT_COLS:\n    df_full[col].fillna('Missing', inplace=True)\n    \n    # --- Frequency Encoding (Feature 1) ---\n    count_map = df_full[col].value_counts().to_dict()\n    df_full[f'{col}_freq_enc'] = df_full[col].map(count_map)\n\n# --- Group Mean Deviation Feature Creation (Feature 2) ---\nfor group_col in ['grade_subgrade', 'education_level']:\n    for agg_col in ['annual_income', 'credit_score']:\n        group_mean = df_full.groupby(group_col)[agg_col].transform('mean')\n        df_full[f'{agg_col}_dev_by_{group_col}'] = df_full[agg_col] - group_mean\n        \n# --- Robust Scaling on all numeric features (including imputed ones) ---\n# This is often used in such solutions to normalize data and handle outliers.\nnumeric_features_to_scale = [c for c in df_full.columns if c not in CAT_COLS]\nscaler = RobustScaler()\ndf_full[numeric_features_to_scale] = scaler.fit_transform(df_full[numeric_features_to_scale])\n\nprint(\"Feature Engineering Complete. Full shape:\", df_full.shape)\n\n# --- Separate back into training and testing sets ---\nX = df_full.iloc[:len(y)]\nX_test = df_full.iloc[len(y):]\n\n\n# --- 3. Out-of-Fold (OOF) Target Encoding (Feature 3) ---\n\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=GLOBAL_SEED)\noof_encoded_cols = [f'{col}_oof_enc' for col in CAT_COLS]\noof_df = pd.DataFrame(index=X.index, columns=oof_encoded_cols)\nglobal_mean = y.mean()\n\nprint(f\"Starting {N_FOLDS}-Fold OOF Target Encoding...\")\nfor train_idx, val_idx in kf.split(X, y):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold = y.iloc[train_idx]\n    \n    for col in CAT_COLS:\n        target_mean_map = y_train_fold.groupby(X_train_fold[col]).mean()\n        # Apply the mean to the validation fold\n        oof_df.loc[val_idx, f'{col}_oof_enc'] = X_val_fold[col].map(target_mean_map)\n\n# Add the OOF encoded features to the training set\nX = pd.concat([X, oof_df], axis=1)\n\n# For the test set, use the global target mean from the entire training set\nfor col in CAT_COLS:\n    global_target_mean = y.groupby(X[col]).mean()\n    X_test[f'{col}_oof_enc'] = X_test[col].map(global_target_mean)\n\n# --- Ensure OOF columns are numerical and fill NaNs (for unseen categories) ---\nfor col in oof_encoded_cols:\n    X[col] = X[col].astype(float)\n    X_test[col] = X_test[col].astype(float)\n\n    # Fill NaNs with the overall target mean\n    X[col].fillna(global_mean, inplace=True)\n    X_test[col].fillna(global_mean, inplace=True)\n\n# Drop original categorical columns\nX.drop(CAT_COLS, axis=1, inplace=True)\nX_test.drop(CAT_COLS, axis=1, inplace=True)\n\n# Final check of column alignment (CRITICAL)\nX_test = X_test[X.columns]\n\n\n# --- 4. Model Training (Single XGBoost Model) ---\nprint(\"\\n--- Starting XGBoost Training ---\")\n\nmodel = xgb.XGBClassifier(**OPTIMAL_PARAMS, enable_categorical=False)\n\nmodel.fit(\n    X, y,\n    eval_set=[(X, y)],\n    early_stopping_rounds=100,\n    verbose=False\n)\n\nprint(f\"Model trained with {model.best_iteration} boosting rounds.\")\nprint(f\"Training AUC: {roc_auc_score(y, model.predict_proba(X)[:, 1]):.6f}\")\n\n\n# --- 5. Prediction and Submission ---\npredictions = model.predict_proba(X_test)[:, 1]\n\nsubmission_df = pd.DataFrame({'id': test_ids, TARGET: predictions})\nsubmission_df.to_csv('submission_final_advanced_xgb.csv', index=False)\n\nprint(\"\\n--- Submission File Created ---\")\nprint(\"File: submission_final_advanced_xgbV2.csv\")\nprint(f\"Prediction head:\\n{submission_df.head()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import RobustScaler\nimport os\n\n# --- 0. Configuration and Hyperparameters ---\n\nTARGET = 'loan_paid_back' \nN_FOLDS = 5\nGLOBAL_SEED = 42\n\n# LightGBM parameters often perform better than XGBoost defaults\nLGB_PARAMS = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 3000,\n    'learning_rate': 0.01,\n    'num_leaves': 16,\n    'max_depth': 6,\n    'min_child_samples': 20,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 0.1,\n    'reg_lambda': 0.1,\n    'n_jobs': -1,\n    'seed': GLOBAL_SEED,\n    # Use 'gpu' if available and performing better than 'cpu' on your system\n    'device_type': 'cpu' \n}\n\n# --- 1. Data Loading ---\nINPUT_PATH = '/kaggle/input/playground-series-s5e11/'\ntry:\n    df_train = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'))\n    df_test = pd.read_csv(os.path.join(INPUT_PATH, 'test.csv'))\n    \n    y = df_train[TARGET].astype(int)\n    test_ids = df_test['id']\n    df_train.drop(TARGET, axis=1, inplace=True)\n    \n    df_full = pd.concat([df_train.drop('id', axis=1), df_test.drop('id', axis=1)], ignore_index=True)\n\nexcept FileNotFoundError as e:\n    print(f\"Error loading files. Check the INPUT_PATH: {e}\")\n    exit()\n\n\n# --- 2. Feature Engineering for LightGBM ---\n\nprint(\"\\n--- Starting LightGBM-Optimized Feature Engineering ---\")\n\nCAT_COLS = ['gender', 'marital_status', 'education_level', \n            'employment_status', 'loan_purpose', 'grade_subgrade']\nNUM_COLS = ['annual_income', 'debt_to_income_ratio', 'credit_score', \n            'loan_amount', 'interest_rate']\n\n# A. Missingness Flags (Highly Predictive)\nfor col in NUM_COLS:\n    df_full[f'{col}_is_missing'] = df_full[col].isna().astype(int)\n    # Fill actual NaN values with -999 (Sentinel value)\n    df_full[col].fillna(-999, inplace=True)\n\n# B. Categorical Missingness and Type Casting (Native LGB support)\nfor col in CAT_COLS:\n    # Treat NaNs as a new category (important for LGB's native support)\n    df_full[col].fillna('Missing', inplace=True)\n    # Convert to 'category' type, which LightGBM uses natively\n    df_full[col] = df_full[col].astype('category')\n    \n# C. Group Deviation Features (Advanced Statistical Feature)\nfor group_col in ['grade_subgrade', 'education_level']:\n    for agg_col in ['annual_income', 'credit_score']:\n        group_mean = df_full.groupby(group_col)[agg_col].transform('mean')\n        df_full[f'{agg_col}_dev_by_{group_col}'] = df_full[agg_col] - group_mean\n        \n# D. Robust Scaling on all numeric features\nnumeric_features_to_scale = [c for c in df_full.columns if c not in CAT_COLS]\nscaler = RobustScaler()\ndf_full[numeric_features_to_scale] = scaler.fit_transform(df_full[numeric_features_to_scale])\n\nprint(\"Feature Engineering Complete. Full shape:\", df_full.shape)\n\n# Separate back into training and testing sets\nX = df_full.iloc[:len(y)]\nX_test = df_full.iloc[len(y):]\n\n\n# --- 3. Model Training (LightGBM with 5-Fold Cross-Validation) ---\n\nprint(\"\\n--- Starting LightGBM Training (5-Fold CV) ---\")\n\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=GLOBAL_SEED)\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgb.LGBMClassifier(**LGB_PARAMS)\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='auc',\n        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n        # Pass the categorical features list for native handling\n        categorical_feature=CAT_COLS\n    )\n    \n    oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n    test_preds += model.predict_proba(X_test)[:, 1] / N_FOLDS\n    \n    print(f\"Fold {fold+1} finished. Best iteration: {model.best_iteration_}\")\n\noof_auc = roc_auc_score(y, oof_preds)\nprint(f\"\\nOverall OOF AUC (LightGBM): {oof_auc:.6f}\")\n\n\n# --- 4. Prediction and Submission ---\n# The final predictions are the average of the 5 folds (test_preds)\n\nsubmission_df = pd.DataFrame({'id': test_ids, TARGET: test_preds})\nsubmission_df.to_csv('submission_lgbm_advanced.csv', index=False)\n\nprint(\"\\n--- Submission File Created ---\")\nprint(\"File: submission_lgbm_advanced.csv\")\nprint(f\"Prediction head:\\n{submission_df.head()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:37.426601Z","iopub.execute_input":"2025-11-03T19:55:37.427436Z","iopub.status.idle":"2025-11-03T19:55:37.437935Z","shell.execute_reply.started":"2025-11-03T19:55:37.427403Z","shell.execute_reply":"2025-11-03T19:55:37.436915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train  = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:37.438989Z","iopub.execute_input":"2025-11-03T19:55:37.439325Z","iopub.status.idle":"2025-11-03T19:55:38.29083Z","shell.execute_reply.started":"2025-11-03T19:55:37.439292Z","shell.execute_reply":"2025-11-03T19:55:38.28978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize correlation between numerical features\ncorr = train.select_dtypes(['number']).corr()\nsns.heatmap(corr, cmap='coolwarm', annot=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:38.292761Z","iopub.execute_input":"2025-11-03T19:55:38.293059Z","iopub.status.idle":"2025-11-03T19:55:38.824613Z","shell.execute_reply.started":"2025-11-03T19:55:38.293038Z","shell.execute_reply":"2025-11-03T19:55:38.823615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:38.825609Z","iopub.execute_input":"2025-11-03T19:55:38.825865Z","iopub.status.idle":"2025-11-03T19:55:39.16493Z","shell.execute_reply.started":"2025-11-03T19:55:38.825847Z","shell.execute_reply":"2025-11-03T19:55:39.164072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:39.165882Z","iopub.execute_input":"2025-11-03T19:55:39.166185Z","iopub.status.idle":"2025-11-03T19:55:39.361992Z","shell.execute_reply.started":"2025-11-03T19:55:39.166164Z","shell.execute_reply":"2025-11-03T19:55:39.361075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suppose your training dataframe is 'train'\ntrain = train.copy()\n\n# Identify categorical and numeric columns\ncategorical_cols = train.select_dtypes(include=['object']).columns.tolist()\nnumeric_cols = train.select_dtypes(exclude=['object']).columns.tolist()\n\n# One-hot encode ALL categorical features (no drop_first to preserve all features)\ntrain_encoded = pd.get_dummies(train, columns=categorical_cols, drop_first=False)\n\n# Compute correlation matrix including ALL columns\ncorr_matrix = train_encoded.corr()\n\n# Optional: round to 2 decimals for clarity\ncorr_matrix = corr_matrix.round(2)\n\n# Visualize\nplt.figure(figsize=(24, 20))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0)\nplt.title('Full Feature Correlation Matrix (All Features from Training Data)')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:39.363168Z","iopub.execute_input":"2025-11-03T19:55:39.363481Z","iopub.status.idle":"2025-11-03T19:55:47.587533Z","shell.execute_reply.started":"2025-11-03T19:55:39.363455Z","shell.execute_reply":"2025-11-03T19:55:47.586393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute correlation with the target variable\ncorr_with_target = corr_matrix['loan_paid_back'].sort_values(ascending=False)\n\n# Show top positive and negative correlations\nprint(\"Top Positive Correlations (features that increase likelihood of repayment):\")\nprint(corr_with_target.head(15))\n\nprint(\"\\nTop Negative Correlations (features that decrease likelihood of repayment):\")\nprint(corr_with_target.tail(15))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:47.588858Z","iopub.execute_input":"2025-11-03T19:55:47.589629Z","iopub.status.idle":"2025-11-03T19:55:47.600501Z","shell.execute_reply.started":"2025-11-03T19:55:47.589599Z","shell.execute_reply":"2025-11-03T19:55:47.599519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntop_features = corr_with_target[abs(corr_with_target) > 0.1]  # filter strongest ones\nplt.figure(figsize=(10,6))\ntop_features.plot(kind='bar', color='red')\nplt.title('Features Most Correlated with Loan Repayment')\nplt.ylabel('Correlation with loan_paid_back')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:47.601639Z","iopub.execute_input":"2025-11-03T19:55:47.601924Z","iopub.status.idle":"2025-11-03T19:55:47.864236Z","shell.execute_reply.started":"2025-11-03T19:55:47.60188Z","shell.execute_reply":"2025-11-03T19:55:47.86328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:47.867401Z","iopub.execute_input":"2025-11-03T19:55:47.867705Z","iopub.status.idle":"2025-11-03T19:55:48.067958Z","shell.execute_reply.started":"2025-11-03T19:55:47.867683Z","shell.execute_reply":"2025-11-03T19:55:48.06682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix after One-Hot Encoding')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:48.068975Z","iopub.execute_input":"2025-11-03T19:55:48.069315Z","iopub.status.idle":"2025-11-03T19:55:49.599174Z","shell.execute_reply.started":"2025-11-03T19:55:48.069286Z","shell.execute_reply":"2025-11-03T19:55:49.597941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:49.600287Z","iopub.execute_input":"2025-11-03T19:55:49.600649Z","iopub.status.idle":"2025-11-03T19:55:49.803555Z","shell.execute_reply.started":"2025-11-03T19:55:49.600624Z","shell.execute_reply":"2025-11-03T19:55:49.802127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain.hist(bins=30,figsize=(15, 10),color= 'blue')\nplt.suptitle(\"Train() Data Distributions\", fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:49.804463Z","iopub.execute_input":"2025-11-03T19:55:49.804724Z","iopub.status.idle":"2025-11-03T19:55:51.149203Z","shell.execute_reply.started":"2025-11-03T19:55:49.804705Z","shell.execute_reply":"2025-11-03T19:55:51.148303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target = 'loan_paid_back'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:51.150633Z","iopub.execute_input":"2025-11-03T19:55:51.150993Z","iopub.status.idle":"2025-11-03T19:55:51.155544Z","shell.execute_reply.started":"2025-11-03T19:55:51.150965Z","shell.execute_reply":"2025-11-03T19:55:51.15439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n\ndef convert_all_to_numeric(train, test):\n    \"\"\"\n    Converts ALL columns in train/test to numeric (LabelEncoding for objects),\n    safely handling columns that exist only in one dataset.\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    from sklearn.preprocessing import LabelEncoder\n\n    tr, te = train.copy(), test.copy()\n    encoders = {}\n\n    # unify columns intersection (ignore target like Dropout that‚Äôs train-only)\n    common_cols = sorted(set(tr.columns).intersection(set(te.columns)))\n\n    for col in common_cols:\n        if tr[col].dtype == \"O\" or str(tr[col].dtype).startswith(\"category\"):\n            le = LabelEncoder()\n            combined = pd.concat([tr[col].astype(str), te[col].astype(str)], axis=0)\n            le.fit(combined)\n            tr[col] = le.transform(tr[col].astype(str))\n            te[col] = le.transform(te[col].astype(str))\n            encoders[col] = le\n        else:\n            tr[col] = pd.to_numeric(tr[col], errors=\"coerce\")\n            te[col] = pd.to_numeric(te[col], errors=\"coerce\")\n\n    # handle train-only numeric columns like target\n    for col in set(tr.columns) - set(te.columns):\n        if tr[col].dtype == \"O\":\n            le = LabelEncoder()\n            tr[col] = le.fit_transform(tr[col].astype(str))\n        else:\n            tr[col] = pd.to_numeric(tr[col], errors=\"coerce\")\n\n    # clean infinities / NaNs\n    tr.replace([np.inf, -np.inf], np.nan, inplace=True)\n    te.replace([np.inf, -np.inf], np.nan, inplace=True)\n    tr.fillna(tr.mean(numeric_only=True), inplace=True)\n    te.fillna(te.mean(numeric_only=True), inplace=True)\n\n    num_cols = tr.select_dtypes(include=\"number\").columns.tolist()\n    return tr, te, num_cols\n\n\ndef dist_plots(train, test, num_features):\n    \"\"\"\n    Plot KDE + Boxplots for numeric columns.\n    \"\"\"\n    print(\"\\nDistribution analysis (all numeric/object columns converted)\\n\")\n    df = pd.concat(\n        [train[num_features].assign(Source=\"Train\"),\n         test[num_features].assign(Source=\"Test\")],\n        axis=0, ignore_index=True\n    )\n\n    n = len(num_features)\n    fig, axes = plt.subplots(\n        n, 2,\n        figsize=(18, n * 4),\n        gridspec_kw={\"hspace\": 0.3, \"wspace\": 0.2, \"width_ratios\": [0.70, 0.30]}\n    )\n    if n == 1:\n        axes = np.array([axes])\n\n    for i, col in enumerate(num_features):\n        # KDE\n        ax = axes[i, 0]\n        sns.kdeplot(data=df, x=col, hue=\"Source\",\n                    palette=[\"#3cb371\", \"#0483ff\"], ax=ax, linewidth=2)\n        ax.set(xlabel=\"\", ylabel=\"\")\n        ax.set_title(f\"{col}\")\n        ax.grid()\n\n        # Boxplot\n        ax = axes[i, 1]\n        sns.boxplot(data=df, y=col, x=\"Source\", width=0.5,\n                    linewidth=1, fliersize=1, ax=ax, palette=[\"#3cb371\", \"b\"])\n        ax.set(xlabel=\"\", ylabel=\"\")\n        ax.set_title(f\"{col}\")\n        ax.set_xticklabels([\"Train\", \"Test\"])\n\n    plt.tight_layout()\n    plt.show()\n    \n# 1Ô∏è‚É£ Convert everything to numeric\ntr_all, te_all, numeric_cols = convert_all_to_numeric(train, test)\n\nprint(\"Numeric columns used for distribution plots:\")\nprint(numeric_cols)\n\n# 2Ô∏è‚É£ Plot all feature distributions\ndist_plots(tr_all, te_all, [c for c in numeric_cols if c != target])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:55:51.156423Z","iopub.execute_input":"2025-11-03T19:55:51.15666Z","iopub.status.idle":"2025-11-03T19:56:51.730706Z","shell.execute_reply.started":"2025-11-03T19:55:51.156642Z","shell.execute_reply":"2025-11-03T19:56:51.729748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cols = ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score',\n       'loan_amount', 'interest_rate', 'gender', 'marital_status',\n       'education_level', 'employment_status', 'loan_purpose',\n       'grade_subgrade', 'loan_paid_back']\n\nfor i in train_cols:\n    print('**=='*20)\n    print(f\"Unique Values of column {i}\")\n    print(f\"{train[i].value_counts()}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:56:51.73166Z","iopub.execute_input":"2025-11-03T19:56:51.73192Z","iopub.status.idle":"2025-11-03T19:56:52.15613Z","shell.execute_reply.started":"2025-11-03T19:56:51.731876Z","shell.execute_reply":"2025-11-03T19:56:52.155285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:56:52.156941Z","iopub.execute_input":"2025-11-03T19:56:52.157167Z","iopub.status.idle":"2025-11-03T19:56:52.354123Z","shell.execute_reply.started":"2025-11-03T19:56:52.15715Z","shell.execute_reply":"2025-11-03T19:56:52.353237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**There's a better way to do this**","metadata":{}},{"cell_type":"code","source":"for col in train_cols:\n    print(f\"\\n{'='*80}\")\n    print(f\"üîπ Column: {col}\")\n    print(f\"Unique values: {train[col].nunique()}\")\n    print(train[col].value_counts(dropna=False).head(10))  # top 10 most common\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:56:52.354936Z","iopub.execute_input":"2025-11-03T19:56:52.355179Z","iopub.status.idle":"2025-11-03T19:56:52.948913Z","shell.execute_reply.started":"2025-11-03T19:56:52.355154Z","shell.execute_reply":"2025-11-03T19:56:52.947932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary = train[train_cols].nunique().reset_index()\nsummary.columns = ['Column', 'Unique_Values']\nsummary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:56:52.949988Z","iopub.execute_input":"2025-11-03T19:56:52.950326Z","iopub.status.idle":"2025-11-03T19:56:53.267689Z","shell.execute_reply.started":"2025-11-03T19:56:52.950297Z","shell.execute_reply":"2025-11-03T19:56:53.266679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ydata_profiling import ProfileReport\n\nprofile = ProfileReport(train[train_cols], title=\"Loan Data Report\")\nprofile.to_notebook_iframe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:56:53.268791Z","iopub.execute_input":"2025-11-03T19:56:53.26968Z","iopub.status.idle":"2025-11-03T19:57:29.618032Z","shell.execute_reply.started":"2025-11-03T19:56:53.269654Z","shell.execute_reply":"2025-11-03T19:57:29.616996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Target column\ntarget = 'loan_paid_back'\n\n# Skip high-cardinality columns\nmax_uniques = 30\n\n# Select columns to visualize (exclude ID and target)\nfeatures = [c for c in train.columns if c not in ['id', target]]\n\n# Prepare the plots\nfor col in features:\n    n_unique = train[col].nunique()\n    dtype = train[col].dtype\n    \n    # Skip columns with too many unique values\n    if n_unique > max_uniques:\n        continue\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"Feature: {col} | Unique values: {n_unique} | Type: {dtype}\")\n    \n    plt.figure(figsize=(7, 4))\n    \n    if np.issubdtype(dtype, np.number):\n        # Numeric feature ‚Üí show boxplot distribution by target\n        sns.boxplot(x=target, y=col, data=train, palette=\"Set2\")\n        plt.title(f\"{col} vs. Loan Paid Back (Boxplot)\")\n    else:\n        # Categorical feature ‚Üí show repayment rate per category\n        temp = train.groupby(col)[target].mean().sort_values(ascending=False)\n        sns.barplot(x=temp.index, y=temp.values, palette=\"viridis\")\n        plt.title(f\"Repayment Probability by {col}\")\n        plt.ylabel(\"Mean(loan_paid_back)\")\n        plt.xticks(rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:57:29.619086Z","iopub.execute_input":"2025-11-03T19:57:29.619804Z","iopub.status.idle":"2025-11-03T19:57:31.425473Z","shell.execute_reply.started":"2025-11-03T19:57:29.619781Z","shell.execute_reply":"2025-11-03T19:57:31.424542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom scipy import stats\nimport numpy as np\n\ntarget = 'loan_paid_back'\n\n# Select numeric features only\nnumeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols = [c for c in numeric_cols if c not in [target, 'id']]\n\nanova_results = []\n\nfor col in numeric_cols:\n    # Separate the two groups\n    group1 = train[train[target] == 1][col].dropna()\n    group0 = train[train[target] == 0][col].dropna()\n    \n    # Run ANOVA (F-test)\n    f_stat, p_val = stats.f_oneway(group1, group0)\n    \n    anova_results.append({\n        'Feature': col,\n        'F-Statistic': f_stat,\n        'p-Value': p_val\n    })\n\n# Create summary DataFrame\nanova_df = pd.DataFrame(anova_results).sort_values(by='p-Value')\nanova_df['Significant'] = anova_df['p-Value'] < 0.05\n\nprint(\"üîπ ANOVA Results:\")\ndisplay(anova_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:57:31.426437Z","iopub.execute_input":"2025-11-03T19:57:31.426669Z","iopub.status.idle":"2025-11-03T19:57:31.757116Z","shell.execute_reply.started":"2025-11-03T19:57:31.426653Z","shell.execute_reply":"2025-11-03T19:57:31.756204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\n\ntarget = 'loan_paid_back'\n\nanova_results = []\nchi2_results = []\n\n# Split features into numeric & categorical\nnumeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols = [c for c in numeric_cols if c not in [target, 'id']]\n\ncategorical_cols = [c for c in train.columns if c not in numeric_cols + [target, 'id']]\n\n# --- 1Ô∏è‚É£ ANOVA for numeric features ---\nfor col in numeric_cols:\n    group1 = train.loc[train[target] == 1, col].dropna()\n    group0 = train.loc[train[target] == 0, col].dropna()\n    if len(group1) > 1 and len(group0) > 1:\n        f_stat, p_val = stats.f_oneway(group1, group0)\n        anova_results.append({\n            'Feature': col,\n            'Test': 'ANOVA (numeric)',\n            'Statistic': f_stat,\n            'p-Value': p_val\n        })\n\n# --- 2Ô∏è‚É£ Chi-square for categorical features ---\nfor col in categorical_cols:\n    # Encode if needed\n    temp = train[[col, target]].dropna()\n    contingency = pd.crosstab(temp[col], temp[target])\n    \n    # Chi-square test\n    if contingency.shape[0] > 1:\n        chi2, p_val, dof, ex = stats.chi2_contingency(contingency)\n        chi2_results.append({\n            'Feature': col,\n            'Test': 'Chi-Square (categorical)',\n            'Statistic': chi2,\n            'p-Value': p_val\n        })\n\n# --- 3Ô∏è‚É£ Combine results ---\nresults_df = pd.DataFrame(anova_results + chi2_results)\nresults_df['Significant'] = results_df['p-Value'] < 0.05\nresults_df = results_df.sort_values(by='p-Value')\n\nprint(\"üîπ Feature Significance (ANOVA + Chi-Square):\")\ndisplay(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:57:31.758405Z","iopub.execute_input":"2025-11-03T19:57:31.758774Z","iopub.status.idle":"2025-11-03T19:57:32.614606Z","shell.execute_reply.started":"2025-11-03T19:57:31.758752Z","shell.execute_reply":"2025-11-03T19:57:32.613504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sort and plot\nsorted_df = results_df.sort_values(by='Statistic', ascending=False)\n\nplt.figure(figsize=(10,6))\nsns.barplot(\n    data=sorted_df,\n    x='Statistic',\n    y='Feature',\n    hue='Test',\n    dodge=False,\n    palette='viridis'\n)\nplt.title(\"Feature Significance (ANOVA + Chi-Square)\")\nplt.xlabel(\"Test Statistic (F / Chi¬≤)\")\nplt.ylabel(\"Feature\")\nplt.legend(title=\"Test Type\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:57:32.615511Z","iopub.execute_input":"2025-11-03T19:57:32.615756Z","iopub.status.idle":"2025-11-03T19:57:33.043022Z","shell.execute_reply.started":"2025-11-03T19:57:32.615737Z","shell.execute_reply":"2025-11-03T19:57:33.042018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\nsub = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:57:33.044111Z","iopub.execute_input":"2025-11-03T19:57:33.044483Z","iopub.status.idle":"2025-11-03T19:57:34.293176Z","shell.execute_reply.started":"2025-11-03T19:57:33.044458Z","shell.execute_reply":"2025-11-03T19:57:34.292365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suppose your training dataframe is 'train'\n\n# Identify categorical and numeric columns\ncategorical_cols = train.select_dtypes(include=['object']).columns.tolist()\nnumeric_cols = train.select_dtypes(exclude=['object']).columns.tolist()\n\n# One-hot encode ALL categorical features (no drop_first to preserve all features)\ntrain_encoded = pd.get_dummies(train, columns=categorical_cols, drop_first=False)\n\n# Compute correlation matrix including ALL columns\ncorr_matrix = train_encoded.corr()\n\n# Optional: round to 2 decimals for clarity\ncorr_matrix = corr_matrix.round(2)\n\n# Visualize\nplt.figure(figsize=(24, 20))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0)\nplt.title('Full Feature Correlation Matrix (All Features from Training Data)')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:57:34.29412Z","iopub.execute_input":"2025-11-03T19:57:34.294379Z","iopub.status.idle":"2025-11-03T19:57:42.374418Z","shell.execute_reply.started":"2025-11-03T19:57:34.29436Z","shell.execute_reply":"2025-11-03T19:57:42.373298Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**#Check new ones **","metadata":{}},{"cell_type":"code","source":"# # ==============================================================\n# # 1. Imports\n# # ==============================================================\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.metrics import roc_auc_score, roc_curve\n# # Updated Imports: LightGBM, XGBoost, and the new CatBoost\n# import lightgbm as lgb\n# import xgboost as xgb\n# from catboost import CatBoostClassifier \n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# # ==============================================================\n# # 2. Load Data\n# # ==============================================================\n# # NOTE: File paths assume a Kaggle environment; adjust if running elsewhere\n# train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\n# sub = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n\n# # ==============================================================\n# # 3. Basic Setup\n# # ==============================================================\n# X = train.drop(columns=['loan_paid_back', 'id'])\n# y = train['loan_paid_back']\n# X_test = test.drop(columns=['id'])\n\n# # Identify categorical and numeric columns\n# cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n# num_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n\n# print(\"Categorical:\", cat_cols)\n# print(\"Numerical:\", num_cols)\n\n# # ==============================================================\n# # 4. Feature Preprocessing (Only Numeric/XGBoost/LightGBM)\n# # NOTE: We keep the encoded data for models that require it (LGBM, XGBoost)\n# # ==============================================================\n# ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n# ct = ColumnTransformer(\n#     transformers=[('ohe', ohe, cat_cols)],\n#     remainder='passthrough'\n# )\n\n# # Fit-transform train, transform test\n# X_encoded = ct.fit_transform(X)\n# X_test_encoded = ct.transform(X_test)\n\n# # Convert to DataFrame with aligned columns\n# encoded_cols = ct.get_feature_names_out()\n# X_encoded = pd.DataFrame(X_encoded, columns=encoded_cols)\n# X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoded_cols)\n\n# # ==============================================================\n# # 5. Split Train/Validation for Curves\n# # ==============================================================\n# X_train, X_val, y_train, y_val = train_test_split(\n#     X_encoded, y, test_size=0.2, random_state=42, stratify=y\n# )\n\n# # Create a second split using the UN-ENCODED data for CatBoost\n# X_train_cat, X_val_cat, y_train_cat, y_val_cat = train_test_split(\n#     X, y, test_size=0.2, random_state=42, stratify=y\n# )\n# X_test_cat = X_test.copy()\n\n# # ==============================================================\n# # 6. CatBoost Classifier (The Special Something)\n# # CatBoost natively handles categorical features, so we use the un-encoded data.\n# # ==============================================================\n# print(\"\\n--- 6. CatBoost Classifier ---\")\n# # CatBoost's advanced optimization includes ordering principle and native feature handling\n# cat_model = CatBoostClassifier(\n#     iterations=600,\n#     learning_rate=0.05,\n#     eval_metric='AUC',\n#     random_seed=42,\n#     verbose=50,\n#     early_stopping_rounds=30,\n#     # Use the column names identified in Section 3\n#     cat_features=cat_cols \n# )\n\n# cat_model.fit(\n#     X_train_cat, y_train_cat,\n#     eval_set=[(X_val_cat, y_val_cat)]\n# )\n\n# y_val_pred_cat = cat_model.predict_proba(X_val_cat)[:, 1]\n# auc_cat = roc_auc_score(y_val_cat, y_val_pred_cat)\n# print(f\"CatBoost AUC: {auc_cat:.4f}\")\n\n# # Plotting the training curve\n# evals_result_cat = cat_model.get_evals_result()\n# plt.figure(figsize=(6,5))\n# plt.plot(evals_result_cat['validation']['AUC'], label='Validation AUC')\n# plt.title('CatBoost AUC Training Curve')\n# plt.xlabel('Iteration')\n# plt.ylabel('AUC')\n# plt.legend()\n# plt.savefig('catboost_training_curve.png')\n# plt.close()\n\n# # Predict test\n# cat_pred = cat_model.predict_proba(X_test_cat)[:, 1]\n# pd.DataFrame({'id': test['id'], 'loan_paid_back': cat_pred}).to_csv('catboost_predictions.csv', index=False)\n\n\n# # ==============================================================\n# # 7. LightGBM (FIXED: Callbacks and Plotting)\n# # ==============================================================\n# print(\"\\n--- 7. LightGBM ---\")\n# lgb_train = lgb.Dataset(X_train, y_train)\n# lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n# params = {\n#     'objective': 'binary',\n#     'metric': 'auc',\n#     'learning_rate': 0.05,\n#     'num_leaves': 31,\n#     'verbose': -1,\n#     'seed': 42\n# }\n\n# evals_result_lgb = {}\n# early_stopping = lgb.early_stopping(stopping_rounds=30, verbose=50)\n# record_eval = lgb.record_evaluation(evals_result_lgb)\n\n# lgb_model = lgb.train(\n#     params,\n#     lgb_train,\n#     valid_sets=[lgb_train, lgb_val], \n#     num_boost_round=300,\n#     callbacks=[early_stopping, record_eval]\n# )\n\n# y_val_pred_lgb = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n# auc_lgb = roc_auc_score(y_val, y_val_pred_lgb)\n# print(f\"LightGBM AUC: {auc_lgb:.4f}\")\n\n# lgb.plot_metric(evals_result_lgb) \n# plt.title('LightGBM AUC Training Curve')\n# plt.savefig('lgbm_training_curve.png')\n# plt.close()\n\n# lgb_pred = lgb_model.predict(X_test_encoded, num_iteration=lgb_model.best_iteration)\n# pd.DataFrame({'id': test['id'], 'loan_paid_back': lgb_pred}).to_csv('lgbm_predictions.csv', index=False)\n\n\n# # ==============================================================\n# # 8. XGBoost (FIXED: Eval Set and Plotting)\n# # ==============================================================\n# print(\"\\n--- 8. XGBoost ---\")\n# xgb_model = xgb.XGBClassifier(\n#     objective='binary:logistic',\n#     eval_metric='auc',\n#     use_label_encoder=False,\n#     learning_rate=0.05,\n#     n_estimators=300,\n#     max_depth=6,\n#     random_state=42\n# )\n\n# xgb_model.fit(\n#     X_train, y_train,\n#     eval_set=[(X_train, y_train), (X_val, y_val)], \n#     early_stopping_rounds=30, \n#     verbose=False\n# )\n\n# y_val_pred_xgb = xgb_model.predict_proba(X_val)[:,1]\n# auc_xgb = roc_auc_score(y_val, y_val_pred_xgb)\n# print(f\"XGBoost AUC: {auc_xgb:.4f}\")\n\n# results = xgb_model.evals_result()\n# plt.figure(figsize=(6,5))\n# plt.plot(results['validation_0']['auc'], label='Train AUC')\n# plt.plot(results['validation_1']['auc'], label='Validation AUC') \n# plt.title('XGBoost AUC Training Curve')\n# plt.xlabel('Boosting Round')\n# plt.ylabel('AUC')\n# plt.legend()\n# plt.savefig('xgboost_training_curve.png')\n# plt.close()\n\n# xgb_pred = xgb_model.predict_proba(X_test_encoded)[:,1]\n# pd.DataFrame({'id': test['id'], 'loan_paid_back': xgb_pred}).to_csv('xgboost_predictions.csv', index=False)\n\n\n# # ==============================================================\n# # 9. Weighted Averaging Ensemble (Advanced Ensemble)\n# # Simple ensemble using equal weights for demonstration\n# # ==============================================================\n# print(\"\\n--- 9. Weighted Averaging Ensemble ---\")\n\n# # Combine predictions on the test set\n# ensemble_pred = (\n#     lgb_pred * 0.33 +\n#     xgb_pred * 0.33 +\n#     cat_pred * 0.34 # Slightly higher weight for CatBoost (arbitrary for demonstration)\n# )\n\n# # Create final submission file\n# pd.DataFrame({\n#     'id': test['id'], \n#     'loan_paid_back': ensemble_pred\n# }).to_csv('ensemble_predictions.csv', index=False)\n\n# # ==============================================================\n# # 10. Summary\n# # ==============================================================\n# print(f\"\\nModel Summary (Validation AUC):\")\n# print(f\"CatBoost:            {auc_cat:.4f} (Used un-encoded data)\")\n# print(f\"LightGBM:            {auc_lgb:.4f} (Used encoded data)\")\n# print(f\"XGBoost:             {auc_xgb:.4f} (Used encoded data)\")\n# print(\"Ensemble predictions saved to 'ensemble_predictions.csv'.\")\n# print(\"All individual predictions and training curves saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T20:12:36.790254Z","iopub.execute_input":"2025-11-03T20:12:36.790703Z","iopub.status.idle":"2025-11-03T20:15:42.483085Z","shell.execute_reply.started":"2025-11-03T20:12:36.790674Z","shell.execute_reply":"2025-11-03T20:15:42.481852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Food for tjpoghty \n# Model Summary (Validation AUC):\n# CatBoost:            0.9169 (Used un-encoded data)\n# LightGBM:            0.9200 (Used encoded data)\n# XGBoost:             0.9187 (Used encoded data)\n# Ensemble predictions saved to 'ensemble_predictions.csv'.\n# All individual predictions and training curves saved.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T20:18:24.231045Z","iopub.execute_input":"2025-11-03T20:18:24.231471Z","iopub.status.idle":"2025-11-03T20:18:24.236629Z","shell.execute_reply.started":"2025-11-03T20:18:24.231445Z","shell.execute_reply":"2025-11-03T20:18:24.235452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !kaggle competitions submit -c playground-series-s5e11 -f submission.csv -m \"LightGBM AUC 0.9204\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:57:52.45065Z","iopub.status.idle":"2025-11-03T19:57:52.45109Z","shell.execute_reply.started":"2025-11-03T19:57:52.450853Z","shell.execute_reply":"2025-11-03T19:57:52.450871Z"}},"outputs":[],"execution_count":null}]}