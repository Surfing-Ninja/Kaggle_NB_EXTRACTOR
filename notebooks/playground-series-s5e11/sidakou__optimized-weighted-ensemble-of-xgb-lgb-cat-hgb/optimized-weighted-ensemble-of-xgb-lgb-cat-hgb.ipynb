{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":512.007606,"end_time":"2025-11-02T17:23:53.689565","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-02T17:15:21.681959","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1a0be8c9fba84bbc9b95a787f8c9c08a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2718090e6dbf44408cdc4e5597796a0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9777327f0fd947638a932f8ea0bc33aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_dd09a193a1b4447f97b71c56eddfcc50","placeholder":"‚Äã","style":"IPY_MODEL_1a0be8c9fba84bbc9b95a787f8c9c08a","tabbable":null,"tooltip":null,"value":"‚Äá2000/2000‚Äá[03:04&lt;00:00,‚Äá‚Äá8.74it/s]"}},"b01846f641fa4489ad7fb88bb0df40b2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c58ea0149740438fbab97949607974b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fea31c5124cb4ca1b38b0f9750987a42","placeholder":"‚Äã","style":"IPY_MODEL_2718090e6dbf44408cdc4e5597796a0d","tabbable":null,"tooltip":null,"value":"Best‚Äátrial:‚Äá1320.‚ÄáBest‚Äávalue:‚Äá0.922947:‚Äá100%"}},"c92600bc5f224f00b9b947ac82c3b99d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b01846f641fa4489ad7fb88bb0df40b2","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9e5d7adc97940b592a10d75e60d6161","tabbable":null,"tooltip":null,"value":2000}},"c9e5d7adc97940b592a10d75e60d6161":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd09a193a1b4447f97b71c56eddfcc50":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0d59bd3ef8043048d32dfebecf38c72":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f53647676f3d4ac790fcc4e72d3d674c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c58ea0149740438fbab97949607974b2","IPY_MODEL_c92600bc5f224f00b9b947ac82c3b99d","IPY_MODEL_9777327f0fd947638a932f8ea0bc33aa"],"layout":"IPY_MODEL_f0d59bd3ef8043048d32dfebecf38c72","tabbable":null,"tooltip":null}},"fea31c5124cb4ca1b38b0f9750987a42":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#f0f8ff; padding:20px; border-radius:10px; line-height:1.6;\">\n\n<h1 style=\"text-align:center; color:#0b3d91;\"> XGB, LGB, CAT & HGB Ensemble for Loan Payback</h1>\n\n<p>This notebook explores an <strong>ensemble of four powerful tree-based models</strong> ‚Äî \n<strong>XGBoost</strong>, <strong>LightGBM</strong>, <strong>CatBoost</strong>, and \n<strong>HistGradientBoosting</strong> ‚Äî to predict <strong>loan payback probability</strong>.</p>\n\n<p>Each model is trained and optimized individually using <strong>Optuna</strong>,  \nand their predictions are blended with optimized weights to achieve the best <strong>AUC score</strong>.</p>\n\n<h2 style=\"color:#0b3d91;\">‚ú® Highlights</h2>\n<ul>\n<li>üìä Comprehensive feature preprocessing & scaling</li>\n<li>üå≤ Comparison of 4 major gradient boosting frameworks</li>\n<li>‚öôÔ∏è Optuna-based hyperparameter and weight optimization</li>\n<li>üìà Ensemble prediction for improved robustness</li>\n</ul>\n\n<p>Let‚Äôs see how the <strong>forest of four trees</strong> performs on the <strong>loan repayment task!</strong></p>\n\n</div>","metadata":{"papermill":{"duration":0.006662,"end_time":"2025-11-02T17:15:28.32001","exception":false,"start_time":"2025-11-02T17:15:28.313348","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ===============================\n# üìö Library Imports\n# ===============================\n\n# --- Basic libraries ---\nimport os                  # File and directory operations\nimport numpy as np         # Numerical computations and array operations\nimport pandas as pd        # DataFrame manipulation\n\n# --- Visualization ---\nimport seaborn as sns      # Statistical data visualization\nimport matplotlib.pyplot as plt  # Plotting library\n\n# --- Preprocessing ---\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split ,KFold # Split dataset into train and validation sets\n\n# --- Machine Learning Models ---\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n# --- Evaluation Metrics ---\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # Regression metrics\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n)  # Classification metrics\n\n# --- Hyperparameter Optimization ---\nimport optuna  # Automatic hyperparameter tuning\n\n\n# --- Kaggle-specific: Display input file paths ---\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))  # Print dataset file paths","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-03T01:46:32.346458Z","iopub.execute_input":"2025-11-03T01:46:32.346805Z","iopub.status.idle":"2025-11-03T01:46:32.355821Z","shell.execute_reply.started":"2025-11-03T01:46:32.346782Z","shell.execute_reply":"2025-11-03T01:46:32.355004Z"},"papermill":{"duration":16.382197,"end_time":"2025-11-02T17:15:44.708829","exception":false,"start_time":"2025-11-02T17:15:28.326632","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#f0f8ff; padding:20px; border-radius:10px; line-height:1.6;\">\n\n<h2 style=\"color:#0b3d91;\">üìä Data Overview</h2>\n\n<p>We begin by <strong>exploring the dataset</strong> to understand its structure and key characteristics.  \nUsing <code>info()</code> and <code>describe()</code>, we examine:</p>\n\n<ul>\n<li><strong>Data types</strong> of each feature</li>\n<li><strong>Missing values</strong> (if any)</li>\n<li><strong>Summary statistics</strong> such as mean, standard deviation, and range</li>\n</ul>\n\n<p>Additionally, a <strong>correlation heatmap</strong> is visualized to identify potential relationships between features.</p>\n\n<hr style=\"border:1px solid #d1ecf1;\">\n\n<p> This step ensures that the dataset is <strong>clean</strong>, <strong>well-structured</strong>, and <strong>free from strong multicollinearity</strong>, providing a reliable foundation for model training.</p>\n\n</div>","metadata":{"papermill":{"duration":0.005501,"end_time":"2025-11-02T17:15:44.720581","exception":false,"start_time":"2025-11-02T17:15:44.71508","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# üìÇ Data Loading \ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\npredict = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:44.733789Z","iopub.status.busy":"2025-11-02T17:15:44.733146Z","iopub.status.idle":"2025-11-02T17:15:47.157318Z","shell.execute_reply":"2025-11-02T17:15:47.156124Z"},"papermill":{"duration":2.433107,"end_time":"2025-11-02T17:15:47.159497","exception":false,"start_time":"2025-11-02T17:15:44.72639","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:47.173048Z","iopub.status.busy":"2025-11-02T17:15:47.172708Z","iopub.status.idle":"2025-11-02T17:15:47.211641Z","shell.execute_reply":"2025-11-02T17:15:47.210226Z"},"papermill":{"duration":0.048286,"end_time":"2025-11-02T17:15:47.213909","exception":false,"start_time":"2025-11-02T17:15:47.165623","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plots a heatmap showing correlations between numerical features.\ncorr = train.select_dtypes(['number']).corr()\nsns.heatmap(corr, cmap='coolwarm', annot=True)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:47.474465Z","iopub.status.busy":"2025-11-02T17:15:47.474135Z","iopub.status.idle":"2025-11-02T17:15:48.182434Z","shell.execute_reply":"2025-11-02T17:15:48.181103Z"},"papermill":{"duration":0.717856,"end_time":"2025-11-02T17:15:48.184216","exception":false,"start_time":"2025-11-02T17:15:47.46636","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#f0f8ff; padding:20px; border-radius:10px; line-height:1.6;\">\n\n<h2 style=\"color:#0b3d91;\">üß© Feature Engineering & One-Hot Encoding</h2>\n\n<p>We enhance the dataset by <strong>creating additional meaningful features</strong> and transforming categorical variables into a numerical format suitable for model training.</p>\n\n<ul>\n<li><strong>Boolean columns</strong> are converted into integer values (<code>0</code> and <code>1</code>).</li>\n<li><strong>One-hot encoding</strong> is applied to categorical features to ensure full compatibility with all tree-based models.</li>\n<li>These transformations help models capture <strong>nonlinear relationships</strong> and handle <strong>categorical diversity</strong> effectively.</li>\n<li><strong>Target mean encoding and count encoding</strong> are added efficiently using K-Fold cross-validation:\n    <ul>\n        <li><em>Target mean encoding</em> calculates the average of the target variable for each category, while avoiding data leakage by computing it fold by fold.</li>\n        <li><em>Count encoding</em> represents each category by its frequency in the training set.</li>\n        <li>Both encodings are applied to training and prediction datasets simultaneously to maintain consistency.</li>\n        <li>All new features are concatenated at once to <strong>avoid DataFrame fragmentation</strong> and improve computational efficiency.</li>\n    </ul>\n</li>\n</ul>\n\n<hr style=\"border:1px solid #d1ecf1;\">\n\n<p> <em>Feature engineering plays a crucial role in improving model performance by providing richer, more informative inputs for learning.</em></p>\n\n</div>\n","metadata":{"papermill":{"duration":0.006997,"end_time":"2025-11-02T17:15:48.199634","exception":false,"start_time":"2025-11-02T17:15:48.192637","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create new features from grade_subgrade and remove unnecessary columns\ndef create_features(df):\n    df['grade'] = df['grade_subgrade'].str[0]\n    df['subgrade'] = df['grade_subgrade'].str[1:].astype(int)\n    \n    return df\n\ntrain = create_features(train)\npredict = create_features(predict)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:48.217898Z","iopub.status.busy":"2025-11-02T17:15:48.217505Z","iopub.status.idle":"2025-11-02T17:15:48.734587Z","shell.execute_reply":"2025-11-02T17:15:48.733587Z"},"papermill":{"duration":0.528589,"end_time":"2025-11-02T17:15:48.736307","exception":false,"start_time":"2025-11-02T17:15:48.207718","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-Hot Encoding\ndef one_hot_encode(df):\n    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n    df = pd.get_dummies(df, columns=object_cols, drop_first=False)\n    return df\n\ntrain = one_hot_encode(train)\npredict = one_hot_encode(predict)\n\nmissing_cols = set(train.columns) - set(predict.columns)\nfor col in missing_cols:\n    predict[col] = 0\n\npredict = predict[train.columns]\npredict = predict.drop(columns=['loan_paid_back'])","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:48.752642Z","iopub.status.busy":"2025-11-02T17:15:48.752328Z","iopub.status.idle":"2025-11-02T17:15:49.375804Z","shell.execute_reply":"2025-11-02T17:15:49.374319Z"},"papermill":{"duration":0.634194,"end_time":"2025-11-02T17:15:49.377919","exception":false,"start_time":"2025-11-02T17:15:48.743725","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert boolean columns\n\ndef bool_to_int(df):\n    bool_columns = df.select_dtypes(include='bool').columns\n    for col in bool_columns:\n        df[col] = df[col].astype(int)\n    return df\n\ntrain = bool_to_int(train)\npredict = bool_to_int(predict)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:49.394832Z","iopub.status.busy":"2025-11-02T17:15:49.394436Z","iopub.status.idle":"2025-11-02T17:15:49.703408Z","shell.execute_reply":"2025-11-02T17:15:49.702241Z"},"papermill":{"duration":0.319726,"end_time":"2025-11-02T17:15:49.70527","exception":false,"start_time":"2025-11-02T17:15:49.385544","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add target mean encoding and count encoding features efficiently\n# This version avoids DataFrame fragmentation by concatenating columns at once\n\ndef add_target_count_features(train, predict, target_col, n_splits=10):\n    BASE = [c for c in train.columns if c not in [target_col]]\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    mean_features = pd.DataFrame(index=train.index)\n    count_features = pd.DataFrame(index=train.index)\n    mean_features_pred = pd.DataFrame(index=predict.index)\n    count_features_pred = pd.DataFrame(index=predict.index)\n\n    for col in BASE:\n        if train[col].isnull().all():\n            continue\n\n        # === Mean Encoding with K-Fold (leakage prevention) ===\n        mean_encoded = np.zeros(len(train))\n        for tr_idx, val_idx in kf.split(train):\n            tr_fold = train.iloc[tr_idx]\n            val_fold = train.iloc[val_idx]\n            mean_map = tr_fold.groupby(col)[target_col].mean()\n            mean_encoded[val_idx] = val_fold[col].map(mean_map)\n\n        mean_features[f'mean_{col}'] = mean_encoded\n\n        # Apply global mean mapping to prediction data\n        global_mean = train.groupby(col)[target_col].mean()\n        mean_features_pred[f'mean_{col}'] = predict[col].map(global_mean)\n\n        # === Count Encoding ===\n        count_map = train[col].value_counts().to_dict()\n        count_features[f'count_{col}'] = train[col].map(count_map)\n        count_features_pred[f'count_{col}'] = predict[col].map(count_map)\n\n    # === Concatenate all features at once to avoid fragmentation ===\n    train = pd.concat([train, mean_features, count_features], axis=1)\n    predict = pd.concat([predict, mean_features_pred, count_features_pred], axis=1)\n\n    # Defragment DataFrames for better performance\n    train = train.copy()\n    predict = predict.copy()\n\n    print(f\"{len(mean_features.columns) + len(count_features.columns)} features created!\")\n    return train, predict\n\n\ntrain, predict = add_target_count_features(train, predict, target_col='loan_paid_back')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#f0f8ff; padding:20px; border-radius:10px; line-height:1.6;\">\n\n<h2 style=\"color:#0b3d91;\">‚öñÔ∏è Data Scaling and Train-Test Split</h2>\n\n<p>To ensure <strong>consistent feature scaling</strong> across all variables, we apply <strong>standardization</strong> to numerical features.  \nThis process helps stabilize training and improves model convergence, especially for algorithms sensitive to feature magnitude.</p>\n\n<p>After scaling, the dataset is <strong>split into training and validation sets</strong>, allowing for an <strong>unbiased comparison</strong> of model performance under identical conditions.</p>\n\n<hr style=\"border:1px solid #d1ecf1;\">\n\n<p><em>Proper scaling and data splitting ensure fair and reliable evaluation across all tree-based models.</em></p>\n\n</div>","metadata":{"papermill":{"duration":0.008549,"end_time":"2025-11-02T17:15:49.721177","exception":false,"start_time":"2025-11-02T17:15:49.712628","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def split_data(df,test_size=0.2,random_state=42):\n    X = df.drop(columns=['id','loan_paid_back'])\n    y = df['loan_paid_back']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = split_data(train)\n\npredict_X = predict.copy()\npredict_X = predict_X.drop(columns=['id'])","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:49.73951Z","iopub.status.busy":"2025-11-02T17:15:49.739204Z","iopub.status.idle":"2025-11-02T17:15:51.278246Z","shell.execute_reply":"2025-11-02T17:15:51.276916Z"},"papermill":{"duration":1.549739,"end_time":"2025-11-02T17:15:51.280003","exception":false,"start_time":"2025-11-02T17:15:49.730264","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#f0f8ff; padding:20px; border-radius:10px; line-height:1.6;\">\n\n<h2 style=\"color:#0b3d91;\">üå≤ Model Training</h2>\n\n<p>We train <strong>four optimized tree-based models</strong> ‚Äî  \n<strong>XGBoost</strong>, <strong>LightGBM</strong>, <strong>CatBoost</strong>, and <strong>HistGradientBoosting</strong> ‚Äî on the same dataset to evaluate their predictive performance.</p>\n\n<p>Each model is built using <strong>hyperparameters optimized via tuning techniques such as Optuna</strong>, ensuring both <strong>strong generalization</strong> and a <strong>fair comparison</strong> across all algorithms.</p>\n\n</div>","metadata":{"papermill":{"duration":0.007092,"end_time":"2025-11-02T17:15:51.295346","exception":false,"start_time":"2025-11-02T17:15:51.288254","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## XGBOOST","metadata":{"papermill":{"duration":0.013004,"end_time":"2025-11-02T17:15:51.315563","exception":false,"start_time":"2025-11-02T17:15:51.302559","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# === Function to build an XGBoost model ===\ndef build_xgboost_model(n_estimators=1000, max_depth=5, learning_rate=0.1, random_state=37):\n\n    # Create an XGBoost classifier\n    model = xgb.XGBClassifier(\n        objective=\"binary:logistic\",  # Binary classification (output = probability)\n        n_estimators=n_estimators,    # Number of trees\n        max_depth=max_depth,          # Depth of each tree (controls model complexity)\n        learning_rate=learning_rate,  # Learning rate (smaller = slower but more stable learning)\n        random_state=random_state,    # Set random seed for reproducibility\n        eval_metric=\"auc\"             # Evaluation metric = AUC (used during training)\n    )\n    return model  # Return the constructed model\n\n\n# === Build and train the model ===\nxgb_model = build_xgboost_model()     # Initialize the model with default parameters\nxgb_model.fit(X_train, y_train)       # Train the model on training data (features and labels)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:15:51.338441Z","iopub.status.busy":"2025-11-02T17:15:51.338117Z","iopub.status.idle":"2025-11-02T17:16:10.898067Z","shell.execute_reply":"2025-11-02T17:16:10.896946Z"},"papermill":{"duration":19.570771,"end_time":"2025-11-02T17:16:10.899818","exception":false,"start_time":"2025-11-02T17:15:51.329047","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CATBOOST","metadata":{"papermill":{"duration":0.00739,"end_time":"2025-11-02T17:16:10.916636","exception":false,"start_time":"2025-11-02T17:16:10.909246","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# === Function to build a CatBoost model ===\ndef build_catboost_model(iterations=1000, depth=5, learning_rate=0.1, random_state=37):\n\n    # Create a CatBoost classifier\n    model = CatBoostClassifier(\n        iterations=iterations,     # Number of boosting rounds\n        depth=depth,               # Depth of each decision tree\n        learning_rate=learning_rate,  # Learning rate for boosting\n        random_seed=random_state,     # Random seed for reproducibility\n        eval_metric=\"AUC\",            # Evaluation metric = AUC\n        loss_function=\"Logloss\",      # Binary classification loss function\n        verbose=False,                 # Suppress training output\n        allow_writing_files=False\n\n    )\n    return model  # Return the constructed model\n\n\n# === Build and train the model ===\ncat_model = build_catboost_model()   # Initialize the model with default parameters\ncat_model.fit(X_train, y_train)      # Train the model on training data (features and labels)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:16:10.933644Z","iopub.status.busy":"2025-11-02T17:16:10.933002Z","iopub.status.idle":"2025-11-02T17:16:41.643657Z","shell.execute_reply":"2025-11-02T17:16:41.642346Z"},"papermill":{"duration":30.732985,"end_time":"2025-11-02T17:16:41.657198","exception":false,"start_time":"2025-11-02T17:16:10.924213","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LIGHTGBM","metadata":{"papermill":{"duration":0.010954,"end_time":"2025-11-02T17:16:41.676872","exception":false,"start_time":"2025-11-02T17:16:41.665918","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# === Function to build a LightGBM model ===\ndef build_lightgbm_model(n_estimators=2000, max_depth=-1, learning_rate=0.05, random_state=66):\n\n\n    # Create a LightGBM classifier\n    model = lgb.LGBMClassifier(\n        objective=\"binary\",          # Binary classification\n        n_estimators=n_estimators,   # Number of boosting rounds\n        max_depth=max_depth,         # Maximum depth of each tree\n        learning_rate=learning_rate, # Step size for gradient boosting\n        random_state=random_state,   # Set random seed\n        metric=\"auc\",                 # Evaluation metric = AUC\n        verbose = -1\n    )\n    return model  # Return the constructed model\n\n\n# === Build and train the model ===\nlgb_model = build_lightgbm_model()   # Initialize the model with default parameters\nlgb_model.fit(X_train, y_train)      # Train the model on training data (features and labels)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:16:41.696538Z","iopub.status.busy":"2025-11-02T17:16:41.696185Z","iopub.status.idle":"2025-11-02T17:16:54.572889Z","shell.execute_reply":"2025-11-02T17:16:54.57164Z"},"papermill":{"duration":12.88881,"end_time":"2025-11-02T17:16:54.57547","exception":false,"start_time":"2025-11-02T17:16:41.68666","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## HistGradientBoosting","metadata":{"papermill":{"duration":0.007818,"end_time":"2025-11-02T17:16:54.592702","exception":false,"start_time":"2025-11-02T17:16:54.584884","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# === Function to build a HistGradientBoosting model === # best 37 200\ndef build_hgb_model(max_iter=200, max_depth=None, random_state=33):\n    model = HistGradientBoostingClassifier(\n        max_iter=max_iter,   # Number of boosting iterations\n        max_depth=max_depth, # Maximum depth of trees\n        random_state=random_state\n    )\n    return model\n\n# === Build and train the model ===\nhgb_model = build_hgb_model()\nhgb_model.fit(X_train, y_train) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:37:30.218763Z","iopub.execute_input":"2025-11-03T01:37:30.219705Z","iopub.status.idle":"2025-11-03T01:37:30.306767Z","shell.execute_reply.started":"2025-11-03T01:37:30.219661Z","shell.execute_reply":"2025-11-03T01:37:30.305535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#f0f8ff; padding:20px; border-radius:10px; line-height:1.6;\">\n\n<h2 style=\"color:#0b3d91;\">üìä Model Predictions and Performance Evaluation</h2>\n\n<p>For each trained model ‚Äî <strong>XGBoost</strong>, <strong>LightGBM</strong>, <strong>CatBoost</strong>, and <strong>HistGradientBoosting</strong> ‚Äî  \nwe generate <strong>predicted probabilities</strong> and evaluate their performance on the <strong>test dataset</strong> using key binary classification metrics:  \n<strong>Accuracy</strong>, <strong>Precision</strong>, <strong>Recall</strong>, <strong>F1 Score</strong>, and <strong>AUC (Area Under the ROC Curve)</strong>.</p>\n\n<p>This step allows us to <strong>quantify each model‚Äôs classification capability</strong> and identify their respective <strong>strengths and weaknesses</strong> before constructing the final <strong>ensemble model</strong>.</p>\n\n<hr style=\"border:1px solid #d1ecf1;\">\n\n<p><em>By comparing these metrics side by side, we can determine which model performs best individually and assess where blending may provide further improvements.</em></p>\n\n</div>","metadata":{"papermill":{"duration":0.007924,"end_time":"2025-11-02T17:20:17.091838","exception":false,"start_time":"2025-11-02T17:20:17.083914","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def evaluate_metrics(y_true, y_pred_proba):\n    results = []\n\n    # Binarize predictions using a 0.5 threshold\n    y_pred = (y_pred_proba >= 0.5).astype(int)\n\n    # Function to calculate evaluation metrics\n    def calculate_metrics(y_true, y_pred, y_pred_proba):\n        accuracy  = accuracy_score(y_true, y_pred)\n        precision = precision_score(y_true, y_pred, zero_division=0)\n        recall    = recall_score(y_true, y_pred)\n        f1        = f1_score(y_true, y_pred)\n        auc       = roc_auc_score(y_true, y_pred_proba)  \n\n        return {\n            'Accuracy': accuracy,\n            'Precision': precision,\n            'Recall': recall,\n            'F1': f1,\n            'AUC': auc,\n        }\n\n    results.append(calculate_metrics(y_true, y_pred, y_pred_proba))\n    return pd.DataFrame(results)\n","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:20:17.110007Z","iopub.status.busy":"2025-11-02T17:20:17.109648Z","iopub.status.idle":"2025-11-02T17:20:17.116672Z","shell.execute_reply":"2025-11-02T17:20:17.115546Z"},"papermill":{"duration":0.018437,"end_time":"2025-11-02T17:20:17.118367","exception":false,"start_time":"2025-11-02T17:20:17.09993","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_prob = xgb_model.predict_proba(X_test)[:, 1] \ncat_prob = cat_model.predict_proba(X_test)[:, 1]\nlgb_prob = lgb_model.predict_proba(X_test)[:, 1]\nhgb_prob  = hgb_model.predict_proba(X_test)[:, 1]\n\n\nxgb_pred = xgb_model.predict(X_test)\ncat_pred = cat_model.predict(X_test)\nlgb_pred = lgb_model.predict(X_test)\nhgb_pred = hgb_model.predict(X_test)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:20:17.138487Z","iopub.status.busy":"2025-11-02T17:20:17.13814Z","iopub.status.idle":"2025-11-02T17:20:26.143395Z","shell.execute_reply":"2025-11-02T17:20:26.142244Z"},"papermill":{"duration":9.016793,"end_time":"2025-11-02T17:20:26.145272","exception":false,"start_time":"2025-11-02T17:20:17.128479","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_results = evaluate_metrics(y_test, xgb_prob).assign(Model=\"XGBoost\")\ncat_results = evaluate_metrics(y_test, cat_prob).assign(Model=\"CatBoost\")\nlgb_results = evaluate_metrics(y_test, lgb_prob).assign(Model=\"LightGBM\")\nhgb_results  = evaluate_metrics(y_test, hgb_prob).assign(Model=\"HistGradientBoosting\")\n\nresults = pd.concat([xgb_results, cat_results, lgb_results, hgb_results], ignore_index=True)\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC']]\n\ndisplay(results)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:20:26.166544Z","iopub.status.busy":"2025-11-02T17:20:26.165737Z","iopub.status.idle":"2025-11-02T17:20:27.514026Z","shell.execute_reply":"2025-11-02T17:20:27.51066Z"},"papermill":{"duration":1.360498,"end_time":"2025-11-02T17:20:27.516711","exception":false,"start_time":"2025-11-02T17:20:26.156213","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = {\n    'XGBoost': xgb_pred,\n    'CatBoost': cat_pred,\n    'LightGBM': lgb_pred,\n    'HistGradientBoosting': hgb_pred\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10)) \naxes = axes.flatten()\n\nfor ax, (name, pred) in zip(axes, models.items()):\n    cm = confusion_matrix(y_test, pred, normalize='true')\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n    ax.set_title(f'{name}')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#f0f8ff; padding:20px; border-radius:10px; line-height:1.6;\">\n\n<h2 style=\"color:#0b3d91;\">üß† Ensemble Learning with Optimized Weights (Optuna)</h2>\n\n<p>To enhance <strong>classification performance</strong>, we perform a <strong>weighted ensemble</strong> of four tree-based models:  \n<strong>XGBoost</strong>, <strong>LightGBM</strong>, <strong>CatBoost</strong>, and <strong>HistGradientBoosting</strong>.</p>\n\n<p>We use <strong>Optuna</strong> to automatically search for the optimal combination of weights that maximizes the <strong>AUC</strong> on the validation set.  \nEach model‚Äôs predicted probability is linearly combined according to the optimized weights.</p>\n\n<hr style=\"border:1px solid #d1ecf1;\">\n\n<h3 style=\"color:#0b3d91;\">üîç Optimization Process</h3>\n<ol>\n<li>Define the search space for each model‚Äôs weight (<code>0.0‚Äì1.0</code>).</li>\n<li>Normalize weights so that their total equals 1.</li>\n<li>Compute the weighted average of predicted probabilities.</li>\n<li>Evaluate the result with <strong>AUC</strong>.</li>\n<li>Repeat the process using Optuna‚Äôs <strong>TPE sampler</strong> to find the best combination.</li>\n</ol>\n\n<h3 style=\"color:#0b3d91;\">üìä Final Ensemble Output</h3>\n<p>The final ensemble uses the best weight combination found by Optuna to produce <strong>final predicted probabilities</strong>,  \nand the performance is evaluated with <strong>Accuracy, Precision, Recall, F1 Score, and AUC</strong>.</p>\n\n</div>","metadata":{"papermill":{"duration":0.008807,"end_time":"2025-11-02T17:20:27.535088","exception":false,"start_time":"2025-11-02T17:20:27.526281","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Optimization Function (Maximize AUC) ---\ndef optimize_weight(trial):\n    # Suggest weights for each model between 0 and 1\n    w_xgb = trial.suggest_float('xgb_weight', 0.0, 1.0)\n    w_lgb = trial.suggest_float('lgb_weight', 0.0, 1.0)\n    w_hgb  = trial.suggest_float('hgb_weight', 0.0, 1.0)\n    w_cat = trial.suggest_float('cat_weight', 0.0, 1.0)\n\n    total_weight = w_xgb + w_lgb + w_hgb + w_cat\n    if total_weight == 0:\n        return 0.5  # Return neutral AUC if all weights are zero\n\n    # Normalize weights so they sum to 1\n    w_xgb /= total_weight\n    w_lgb /= total_weight\n    w_hgb  /= total_weight\n    w_cat /= total_weight\n\n    # --- Use probability outputs ---\n    final_prob = (\n        w_xgb * xgb_prob +\n        w_lgb * lgb_prob +\n        w_hgb  * hgb_prob +\n        w_cat * cat_prob\n    )\n\n    # Handle edge case where y_test has only one class\n    try:\n        auc = roc_auc_score(y_test, final_prob)\n    except ValueError:\n        auc = 0.5\n\n    return auc  # Optuna will maximize this\n\n\n# --- Run Optuna Study (maximize AUC) ---\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(optimize_weight, n_trials=1000, show_progress_bar=True)\n\n# --- Retrieve Best Weights ---\nbest_params = study.best_params\nbest_w_xgb = best_params['xgb_weight']\nbest_w_lgb = best_params['lgb_weight']\nbest_w_hgb  = best_params['hgb_weight']\nbest_w_cat = best_params['cat_weight']\n\n# Normalize again to ensure sum = 1\ntotal_weight = best_w_xgb + best_w_lgb + best_w_hgb + best_w_cat\nbest_w_xgb /= total_weight\nbest_w_lgb /= total_weight\nbest_w_hgb  /= total_weight\nbest_w_cat /= total_weight\n\n# --- Final Weighted Probability Prediction ---\nfinal_prob = (\n    best_w_xgb * xgb_prob +\n    best_w_lgb * lgb_prob +\n    best_w_hgb  * hgb_prob +\n    best_w_cat * cat_prob\n)\n\n# --- Compute Metrics ---\ntry:\n    best_auc = roc_auc_score(y_test, final_prob)\nexcept ValueError:\n    best_auc = 0.5\n\n# --- Convert to Binary Predictions (threshold = 0.5) ---\nthreshold = 0.5\nfinal_pred_binary = (final_prob >= threshold).astype(int)\n\n# --- Display Results ---\nprint(\"\\n=== Optimized Weights ===\")\nprint(f\"XGBoost: {best_w_xgb:.4f}\")\nprint(f\"LightGBM: {best_w_lgb:.4f}\")\nprint(f\"HistGradientBoosting: {best_w_hgb:.4f}\")\nprint(f\"CatBoost: {best_w_cat:.4f}\")\nprint(f\"\\nFinal AUC: {best_auc:.4f}\")\n\n# --- Additional Output ---\nprint(\"\\n=== Example of Predictions ===\")\nprint(\"Probabilities (final_prob):\", final_prob[:10])\nprint(\"Binary (final_pred_binary):\", final_pred_binary[:10])\n","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:20:27.555449Z","iopub.status.busy":"2025-11-02T17:20:27.554962Z","iopub.status.idle":"2025-11-02T17:23:31.642733Z","shell.execute_reply":"2025-11-02T17:23:31.641458Z"},"papermill":{"duration":184.100498,"end_time":"2025-11-02T17:23:31.644617","exception":false,"start_time":"2025-11-02T17:20:27.544119","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_results = evaluate_metrics(y_test, xgb_prob).assign(Model=\"XGBoost\")\ncat_results = evaluate_metrics(y_test, cat_prob).assign(Model=\"CatBoost\")\nlgb_results = evaluate_metrics(y_test, lgb_prob).assign(Model=\"LightGBM\")\nhgb_results  = evaluate_metrics(y_test, hgb_prob).assign(Model=\"HistGradientBoosting\")\nencode_results = evaluate_metrics(y_test, final_prob).assign(Model=\"Optimized Ensemble\")\n\nresults = pd.concat([xgb_results, cat_results, lgb_results, hgb_results,encode_results], ignore_index=True)\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC']]\n\ndisplay(results)","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:23:31.665844Z","iopub.status.busy":"2025-11-02T17:23:31.664684Z","iopub.status.idle":"2025-11-02T17:23:33.142604Z","shell.execute_reply":"2025-11-02T17:23:33.140946Z"},"papermill":{"duration":1.48976,"end_time":"2025-11-02T17:23:33.144558","exception":false,"start_time":"2025-11-02T17:23:31.654798","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion(y_true, y_pred, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n    cm = confusion_matrix(y_true, y_pred, normalize='true' if normalize else None)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap=cmap)\n    plt.title(title)\n    plt.show()\n\nencode_results_prob = cat_model.predict_proba(predict_X)[:, 1]\nplot_confusion(y_test, final_pred_binary, normalize=True, title='Normalized Confusion Matrix')","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:23:33.165609Z","iopub.status.busy":"2025-11-02T17:23:33.165272Z","iopub.status.idle":"2025-11-02T17:23:33.590788Z","shell.execute_reply":"2025-11-02T17:23:33.58973Z"},"papermill":{"duration":0.439219,"end_time":"2025-11-02T17:23:33.592777","exception":false,"start_time":"2025-11-02T17:23:33.153558","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit","metadata":{"papermill":{"duration":0.009009,"end_time":"2025-11-02T17:23:33.611201","exception":false,"start_time":"2025-11-02T17:23:33.602192","status":"completed"},"tags":[]}},{"cell_type":"code","source":"xgb_pred_new = xgb_model.predict_proba(predict_X)[:, 1] \nlgb_pred_new = lgb_model.predict_proba(predict_X)[:, 1]\nhgb_pred_new  = hgb_model.predict_proba(predict_X)[:, 1]\ncat_pred_new = cat_model.predict_proba(predict_X)[:, 1]\n\nfinal_pred_new = (\n    best_w_xgb * xgb_pred_new +\n    best_w_lgb * lgb_pred_new +\n    best_w_hgb  * hgb_pred_new +\n    best_w_cat * cat_pred_new\n)\npredict_df = pd.DataFrame(final_pred_new, columns=['loan_paid_back_proba'])\n\nsubmission = pd.concat([predict['id'], predict_df], axis=1)\n\ndisplay(submission.head())\nprint(submission.isnull().sum())\n","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:23:33.63252Z","iopub.status.busy":"2025-11-02T17:23:33.632161Z","iopub.status.idle":"2025-11-02T17:23:51.509067Z","shell.execute_reply":"2025-11-02T17:23:51.508027Z"},"papermill":{"duration":17.889976,"end_time":"2025-11-02T17:23:51.510761","exception":false,"start_time":"2025-11-02T17:23:33.620785","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Save to CSV for Kaggle submission ---\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\n‚úÖ Submission file saved as 'submission.csv'\")","metadata":{"execution":{"iopub.execute_input":"2025-11-02T17:23:51.532157Z","iopub.status.busy":"2025-11-02T17:23:51.53176Z","iopub.status.idle":"2025-11-02T17:23:52.149595Z","shell.execute_reply":"2025-11-02T17:23:52.147968Z"},"papermill":{"duration":0.63111,"end_time":"2025-11-02T17:23:52.151767","exception":false,"start_time":"2025-11-02T17:23:51.520657","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}