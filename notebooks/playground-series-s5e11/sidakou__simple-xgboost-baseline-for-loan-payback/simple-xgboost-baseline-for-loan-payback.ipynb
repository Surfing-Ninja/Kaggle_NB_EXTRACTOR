{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#e8f5e9; padding:10px; border-radius:5px; line-height:1.6;\">\n\n<h1 style=\"color:#1b5e20;\">XGBoost Model for Loan Payback Prediction</h1>\n\n<p>This notebook builds a <strong>simple and effective XGBoost model</strong> for predicting loan repayment probability.<br>\nIt aims to serve as a <strong>beginner-friendly baseline</strong> for the <em>Predicting Loan Payback</em> competition.</p>\n\n<h3 style=\"color:#2e7d32;\">ðŸ“‹ Workflow</h3>\n<ol>\n  <li>Import necessary libraries</li>\n  <li>Load and inspect the dataset</li>\n  <li>Handle missing values and encode categorical features</li>\n  <li>Train an XGBoost model</li>\n  <li>Evaluate model performance</li>\n  <li>Create submission file</li>\n</ol>\n\n<h3 style=\"color:#2e7d32;\">ðŸŽ¯ Goal</h3>\n<p>To provide a clear and reproducible baseline that can be easily extended with feature engineering or model tuning.</p>\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n\n<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n1. Import Libraries\n</h2>","metadata":{}},{"cell_type":"code","source":"# Load libraries and check dataset paths\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay\n)\nimport shap\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:02:45.891809Z","iopub.execute_input":"2025-11-02T12:02:45.89215Z","iopub.status.idle":"2025-11-02T12:02:45.900505Z","shell.execute_reply.started":"2025-11-02T12:02:45.892127Z","shell.execute_reply":"2025-11-02T12:02:45.899395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n2. Load Dataset\n</h2>","metadata":{}},{"cell_type":"code","source":"# Load training and test datasets\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\npredict = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:02:45.902172Z","iopub.execute_input":"2025-11-02T12:02:45.902596Z","iopub.status.idle":"2025-11-02T12:02:47.50651Z","shell.execute_reply.started":"2025-11-02T12:02:45.902574Z","shell.execute_reply":"2025-11-02T12:02:47.505413Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n3. Data Overview\n</h2>\n","metadata":{}},{"cell_type":"code","source":"# Visualize correlation between numerical features\ncorr = train.select_dtypes(['number']).corr()\nsns.heatmap(corr, cmap='coolwarm', annot=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:02:47.507687Z","iopub.execute_input":"2025-11-02T12:02:47.508017Z","iopub.status.idle":"2025-11-02T12:02:48.070136Z","shell.execute_reply.started":"2025-11-02T12:02:47.507987Z","shell.execute_reply":"2025-11-02T12:02:48.069162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n4. Data Preprocessing\n</h2>","metadata":{}},{"cell_type":"code","source":"# Create new features from grade_subgrade and remove unnecessary columns\ndef create_features(df):\n    df['grade'] = df['grade_subgrade'].str[0]\n    df['subgrade'] = df['grade_subgrade'].str[1:].astype(int)\n    \n    return df\n\ntrain = create_features(train)\npredict = create_features(predict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:02:48.072018Z","iopub.execute_input":"2025-11-02T12:02:48.072287Z","iopub.status.idle":"2025-11-02T12:02:48.579377Z","shell.execute_reply.started":"2025-11-02T12:02:48.072266Z","shell.execute_reply":"2025-11-02T12:02:48.578137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def delete_features(df):\n    df = df.drop(columns=(['grade_subgrade','gender','marital_status']))   \n    return df\n    \ntrain = delete_features(train)\npredict = delete_features(predict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-Hot Encoding\ndef one_hot_encode(df):\n    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n    df = pd.get_dummies(df, columns=object_cols, drop_first=False)\n    return df\n\ntrain = one_hot_encode(train)\npredict = one_hot_encode(predict)\n\nmissing_cols = set(train.columns) - set(predict.columns)\nfor col in missing_cols:\n    predict[col] = 0\n\npredict = predict[train.columns]\npredict = predict.drop(columns=['loan_paid_back'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:02:48.580252Z","iopub.execute_input":"2025-11-02T12:02:48.580644Z","iopub.status.idle":"2025-11-02T12:02:49.261781Z","shell.execute_reply.started":"2025-11-02T12:02:48.580622Z","shell.execute_reply":"2025-11-02T12:02:49.260523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert boolean columns to integersã€€\ndef bool_to_int(df):\n    bool_columns = df.select_dtypes(include='bool').columns\n    for col in bool_columns:\n        df[col] = df[col].astype(int)\n    return df\n\ntrain = bool_to_int(train)\npredict = bool_to_int(predict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:02:49.262929Z","iopub.execute_input":"2025-11-02T12:02:49.263247Z","iopub.status.idle":"2025-11-02T12:02:49.666924Z","shell.execute_reply.started":"2025-11-02T12:02:49.263224Z","shell.execute_reply":"2025-11-02T12:02:49.665796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add target mean encoding and count encoding features efficiently\n# This version avoids DataFrame fragmentation by concatenating columns at once\n\ndef add_target_count_features(train, predict, target_col, n_splits=10):\n    BASE = [c for c in train.columns if c not in [target_col]]\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    mean_features = pd.DataFrame(index=train.index)\n    count_features = pd.DataFrame(index=train.index)\n    mean_features_pred = pd.DataFrame(index=predict.index)\n    count_features_pred = pd.DataFrame(index=predict.index)\n\n    for col in BASE:\n        if train[col].isnull().all():\n            continue\n\n        # === Mean Encoding with K-Fold (leakage prevention) ===\n        mean_encoded = np.zeros(len(train))\n        for tr_idx, val_idx in kf.split(train):\n            tr_fold = train.iloc[tr_idx]\n            val_fold = train.iloc[val_idx]\n            mean_map = tr_fold.groupby(col)[target_col].mean()\n            mean_encoded[val_idx] = val_fold[col].map(mean_map)\n\n        mean_features[f'mean_{col}'] = mean_encoded\n\n        # Apply global mean mapping to prediction data\n        global_mean = train.groupby(col)[target_col].mean()\n        mean_features_pred[f'mean_{col}'] = predict[col].map(global_mean)\n\n        # === Count Encoding ===\n        count_map = train[col].value_counts().to_dict()\n        count_features[f'count_{col}'] = train[col].map(count_map)\n        count_features_pred[f'count_{col}'] = predict[col].map(count_map)\n\n    # === Concatenate all features at once to avoid fragmentation ===\n    train = pd.concat([train, mean_features, count_features], axis=1)\n    predict = pd.concat([predict, mean_features_pred, count_features_pred], axis=1)\n\n    # Defragment DataFrames for better performance\n    train = train.copy()\n    predict = predict.copy()\n\n    print(f\"{len(mean_features.columns) + len(count_features.columns)} features created!\")\n    return train, predict\n\n\ntrain, predict = add_target_count_features(train, predict, target_col='loan_paid_back')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:02:49.66787Z","iopub.execute_input":"2025-11-02T12:02:49.668183Z","iopub.status.idle":"2025-11-02T12:03:02.499282Z","shell.execute_reply.started":"2025-11-02T12:02:49.668152Z","shell.execute_reply":"2025-11-02T12:03:02.497396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split dataset into training and testing sets\ndef split_data(df,test_size=0.2,random_state=42):\n    X = df.drop(columns=['id','loan_paid_back'])\n    y = df['loan_paid_back']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = split_data(train)\n\npredict_X = predict.copy()\npredict_X = predict_X.drop(columns=['id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.499977Z","iopub.status.idle":"2025-11-02T12:03:02.500279Z","shell.execute_reply.started":"2025-11-02T12:03:02.50014Z","shell.execute_reply":"2025-11-02T12:03:02.500153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n5. Model Training (XGBoost)\n</h2>\n","metadata":{}},{"cell_type":"code","source":"# Build and train an XGBoost model\ndef build_xgboost_model(n_estimators=1000,max_depth=5,learning_rate=0.1,random_state=67):\n    \n    model = xgb.XGBClassifier(\n        objective=\"binary:logistic\",\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        random_state=random_state,\n        eval_metric=\"auc\"\n    )\n    return model\n\n\nxgb_model = build_xgboost_model()\n\nxgb_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.502654Z","iopub.status.idle":"2025-11-02T12:03:02.503019Z","shell.execute_reply.started":"2025-11-02T12:03:02.502839Z","shell.execute_reply":"2025-11-02T12:03:02.502854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explain XGBoost model using SHAP values\ndef explain_model(model):\n    explainer = shap.Explainer(model)\n    shap_values = explainer(X_test)\n    \n    shap.plots.waterfall(shap_values[0])\n\n    shap.plots.beeswarm(shap_values)\n\nexplain_model(xgb_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.504539Z","iopub.status.idle":"2025-11-02T12:03:02.504877Z","shell.execute_reply.started":"2025-11-02T12:03:02.504731Z","shell.execute_reply":"2025-11-02T12:03:02.504748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n 6. Evaluation\n</h2>\n","metadata":{}},{"cell_type":"code","source":"# Evaluate model performance using common classification metrics\ndef evaluate_metrics(y_true, y_pred_proba):\n    results = []\n\n    y_pred = (y_pred_proba >= 0.5).astype(int)\n\n    def calculate_metrics(y_true, y_pred, y_pred_proba):\n        accuracy  = accuracy_score(y_true, y_pred)\n        precision = precision_score(y_true, y_pred, zero_division=0)\n        recall    = recall_score(y_true, y_pred)\n        f1        = f1_score(y_true, y_pred)\n        auc       = roc_auc_score(y_true, y_pred_proba)\n\n        return {\n            'Accuracy': accuracy,\n            'Precision': precision,\n            'Recall': recall,\n            'F1': f1,\n            'AUC': auc,\n        }\n\n    results.append(calculate_metrics(y_true, y_pred, y_pred_proba))\n    return pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.50111Z","iopub.status.idle":"2025-11-02T12:03:02.501409Z","shell.execute_reply.started":"2025-11-02T12:03:02.501248Z","shell.execute_reply":"2025-11-02T12:03:02.501259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate XGBoost model predictionsã€€\ntrain_predict_probs = xgb_model.predict_proba(X_test)[:, 1]\n\nthreshold = 0.5\ntrain_predict_binary = (train_predict_probs >= threshold).astype(int)\n\nresults = evaluate_metrics(y_test, train_predict_probs)\ndisplay(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.506124Z","iopub.status.idle":"2025-11-02T12:03:02.506479Z","shell.execute_reply.started":"2025-11-02T12:03:02.506272Z","shell.execute_reply":"2025-11-02T12:03:02.506287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot confusion matrix for model predictions\ndef plot_confusion(y_true, y_pred, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n    cm = confusion_matrix(y_true, y_pred, normalize='true' if normalize else None)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap=cmap)\n    plt.title(title)\n    plt.show()\n\nplot_confusion(y_test, train_predict_binary, normalize=True, title='Normalized Confusion Matrix')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.508623Z","iopub.status.idle":"2025-11-02T12:03:02.508928Z","shell.execute_reply.started":"2025-11-02T12:03:02.508798Z","shell.execute_reply":"2025-11-02T12:03:02.508811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n 7. Submission\n</h2>\n","metadata":{}},{"cell_type":"code","source":"# Generate submission file from CatBoost predictions\npredict_y_probs = xgb_model.predict_proba(predict_X)[:, 1]\npredict_df = pd.DataFrame(predict_y_probs, columns=['loan_paid_back'])\nsubmission = pd.concat([predict['id'], predict_df], axis=1)\n\ndisplay(submission.head())\nprint(submission.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.510032Z","iopub.status.idle":"2025-11-02T12:03:02.510466Z","shell.execute_reply.started":"2025-11-02T12:03:02.510223Z","shell.execute_reply":"2025-11-02T12:03:02.510239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as 'submission.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:03:02.511315Z","iopub.status.idle":"2025-11-02T12:03:02.511712Z","shell.execute_reply.started":"2025-11-02T12:03:02.51153Z","shell.execute_reply":"2025-11-02T12:03:02.511547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#e8f5e9; padding:10px; border-radius:5px;\">\n\n<h2 style=\"text-align:center; color:#2e7d32; font-weight:700; margin-top:0;\">\n8. Conclusion\n</h2>\n\n- This simple XGBoost model achieved solid baseline performance.  \n- The confusion matrix shows high precision and recall on positive cases.  \n- Future improvements could include:\n  - Hyperparameter tuning with Optuna  \n  - Feature scaling or interaction terms  \n  - Model calibration for better probability prediction  \n\nThis notebook can be a **good starting point** for anyone joining the *Predicting Loan Payback* competition.\n","metadata":{}}]}