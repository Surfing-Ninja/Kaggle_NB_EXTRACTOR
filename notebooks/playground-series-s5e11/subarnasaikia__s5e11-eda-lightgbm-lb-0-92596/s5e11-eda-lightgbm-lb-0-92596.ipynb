{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loan Default Prediction - Baseline Training Model\n## Goal: Predict probability of loan repayment (AUC-ROC optimization)\n\nThis notebook provides a strong baseline with:\n- Comprehensive EDA\n- Feature engineering\n- LightGBM model with cross-validation\n- Submission generation\n\nIn this notebook I'm using the concept of target encoding from [this](https://www.kaggle.com/code/masayakawamata/s5e11-te-xgb-interaction-features/notebook) notebook. [Here](https://www.kaggle.com/code/masayakawamata/s5e11-te-xgb-interaction-features/comments#3310033) the target encoding feature is explained clearly.\n\nTo train multiple models I use [this](https://www.kaggle.com/code/ravi20076/playgrounds5e11-public-baseline-v1/notebook) notebook's reference.\n","metadata":{}},{"cell_type":"code","source":"import warnings, torch\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:24.999995Z","iopub.execute_input":"2025-11-02T21:01:25.000241Z","iopub.status.idle":"2025-11-02T21:01:25.004859Z","shell.execute_reply.started":"2025-11-02T21:01:25.000186Z","shell.execute_reply":"2025-11-02T21:01:25.003991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom xgboost import XGBClassifier as XGBC\nfrom lightgbm import LGBMClassifier as LGBMC, log_evaluation, early_stopping\nfrom catboost import CatBoostClassifier as CBC\nfrom sklearn.metrics import *\n\n\n# Set random seed for reproducibility\nSEED = 42\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:25.005877Z","iopub.execute_input":"2025-11-02T21:01:25.006638Z","iopub.status.idle":"2025-11-02T21:01:28.210703Z","shell.execute_reply.started":"2025-11-02T21:01:25.006605Z","shell.execute_reply":"2025-11-02T21:01:28.209637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_req = False\n\nif test_req :\n    print(\"THIS IS A SYNTAX CHECK RUN\")\n    nest = 150\n    es   = 50\nelse:\n    nest = 12000\n    es   = 300","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:28.211651Z","iopub.execute_input":"2025-11-02T21:01:28.21228Z","iopub.status.idle":"2025-11-02T21:01:28.217367Z","shell.execute_reply.started":"2025-11-02T21:01:28.212254Z","shell.execute_reply":"2025-11-02T21:01:28.216268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Load Data","metadata":{}},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/playground-series-s5e11/train.csv\"\nTEST_PATH = \"/kaggle/input/playground-series-s5e11/test.csv\"\nORIG_PATH = \"/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv\"\nSAMPLE_SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e11/sample_submission.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:28.221936Z","iopub.execute_input":"2025-11-02T21:01:28.222291Z","iopub.status.idle":"2025-11-02T21:01:28.234112Z","shell.execute_reply.started":"2025-11-02T21:01:28.22227Z","shell.execute_reply":"2025-11-02T21:01:28.233362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\norig = pd.read_csv(ORIG_PATH)\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\nprint(f\"Orig shape: {orig.shape}\")\nprint(f\"\\nTrain columns: {train.columns.tolist()}\\n\")\nprint(f\"Test columns: {test.columns.tolist()}\\n\")\nprint(f\"Orig columns: {orig.columns.tolist()}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:28.234934Z","iopub.execute_input":"2025-11-02T21:01:28.235168Z","iopub.status.idle":"2025-11-02T21:01:29.922367Z","shell.execute_reply.started":"2025-11-02T21:01:28.235149Z","shell.execute_reply":"2025-11-02T21:01:29.921392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Basic info\nprint(\"=\" * 50)\nprint(\"TRAINING DATA INFO\")\nprint(\"=\" * 50)\ndisplay(train.info())\nprint(\"\\n\" + \"=\" * 50)\nprint(\"STATISTICAL SUMMARY\")\nprint(\"=\" * 50)\ndisplay(train.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:29.923999Z","iopub.execute_input":"2025-11-02T21:01:29.924379Z","iopub.status.idle":"2025-11-02T21:01:30.315957Z","shell.execute_reply.started":"2025-11-02T21:01:29.924345Z","shell.execute_reply":"2025-11-02T21:01:30.314816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\nprint(\"Missing values in train:\")\ndisplay(train.isnull().sum())\nprint(\"\\nMissing values in test:\")\ndisplay(test.isnull().sum())\nprint(\"\\nMissing values in orig:\")\ndisplay(orig.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:32.937007Z","iopub.execute_input":"2025-11-02T21:01:32.93763Z","iopub.status.idle":"2025-11-02T21:01:33.227435Z","shell.execute_reply.started":"2025-11-02T21:01:32.937597Z","shell.execute_reply":"2025-11-02T21:01:33.226599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target distribution\nprint(\"Target distribution:\")\nprint(train['loan_paid_back'].value_counts(normalize=True))\n\nplt.figure(figsize=(8, 5))\ntrain['loan_paid_back'].value_counts().plot(kind='bar', color=['salmon', 'skyblue'])\nplt.title('Loan Repayment Distribution', fontsize=14, fontweight='bold')\nplt.xlabel('Loan Paid Back (0=No, 1=Yes)')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:35.784839Z","iopub.execute_input":"2025-11-02T21:01:35.785192Z","iopub.status.idle":"2025-11-02T21:01:35.987987Z","shell.execute_reply.started":"2025-11-02T21:01:35.785168Z","shell.execute_reply":"2025-11-02T21:01:35.986955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Numerical features distribution\nnumerical_cols = ['annual_income', 'debt_to_income_ratio', 'credit_score', \n                  'loan_amount', 'interest_rate']\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_cols):\n    axes[i].hist(train[col], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'{col}', fontweight='bold')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Frequency')\n\naxes[-1].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:38.305552Z","iopub.execute_input":"2025-11-02T21:01:38.305875Z","iopub.status.idle":"2025-11-02T21:01:39.776396Z","shell.execute_reply.started":"2025-11-02T21:01:38.305849Z","shell.execute_reply":"2025-11-02T21:01:39.775457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation with target\ncorrelations = train[numerical_cols + ['loan_paid_back']].corr()['loan_paid_back'].sort_values(ascending=False)\nprint(\"Correlation with target:\")\nprint(correlations)\n\nplt.figure(figsize=(8, 6))\ncorrelations[:-1].plot(kind='barh', color='coral')\nplt.title('Feature Correlation with Loan Repayment', fontsize=14, fontweight='bold')\nplt.xlabel('Correlation Coefficient')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:42.946668Z","iopub.execute_input":"2025-11-02T21:01:42.947143Z","iopub.status.idle":"2025-11-02T21:01:43.298196Z","shell.execute_reply.started":"2025-11-02T21:01:42.947113Z","shell.execute_reply":"2025-11-02T21:01:43.296491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Categorical features analysis\nCATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n\nprint(\"Categorical feature cardinality:\")\nfor col in CATS:\n    print(f\"{col}: {train[col].nunique()} unique values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:45.716193Z","iopub.execute_input":"2025-11-02T21:01:45.716598Z","iopub.status.idle":"2025-11-02T21:01:45.912403Z","shell.execute_reply.started":"2025-11-02T21:01:45.716557Z","shell.execute_reply":"2025-11-02T21:01:45.911566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target rate by categorical features (sample for a few)\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nsample_cats = ['gender', 'marital_status', 'education_level', 'employment_status']\n\nfor i, col in enumerate(sample_cats):\n    target_rate = train.groupby(col)['loan_paid_back'].mean().sort_values()\n    target_rate.plot(kind='barh', ax=axes[i], color='teal')\n    axes[i].set_title(f'Repayment Rate by {col}', fontweight='bold')\n    axes[i].set_xlabel('Repayment Rate')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:48.233442Z","iopub.execute_input":"2025-11-02T21:01:48.234314Z","iopub.status.idle":"2025-11-02T21:01:49.039809Z","shell.execute_reply.started":"2025-11-02T21:01:48.234279Z","shell.execute_reply":"2025-11-02T21:01:49.038789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Identify feature types\nTARGET = 'loan_paid_back'\nid_col = 'id'\n\n# Features to drop\ndrop_cols = [id_col, TARGET]\n\n# Get feature columns\nBASE = [col for col in train.columns if col not in drop_cols]\n\n# Separate numerical and categorical\ncategorical_features = train[BASE].select_dtypes(include=['object', 'category']).columns.tolist()\nnumerical_features = train[BASE].select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nprint(f\"Total features: {len(BASE)}\")\nprint(f\"Numerical: {len(numerical_features)}\")\nprint(f\"Categorical: {len(categorical_features)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:52.186139Z","iopub.execute_input":"2025-11-02T21:01:52.186817Z","iopub.status.idle":"2025-11-02T21:01:52.344634Z","shell.execute_reply.started":"2025-11-02T21:01:52.186786Z","shell.execute_reply":"2025-11-02T21:01:52.343762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://www.kaggle.com/code/masayakawamata/s5e11-te-xgb-interaction-features?scriptVersionId=272584844&cellId=5\n\nfrom itertools import combinations\n\nINTER = []\n\nfor col1, col2 in combinations(BASE, 2):\n    new_col_name = f'{col1}_{col2}'\n    INTER.append(new_col_name)\n    for df in [train, test, orig]:\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n        \n        \nprint(f'{len(INTER)} Features.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:01:54.674244Z","iopub.execute_input":"2025-11-02T21:01:54.674555Z","iopub.status.idle":"2025-11-02T21:02:31.562694Z","shell.execute_reply.started":"2025-11-02T21:01:54.674532Z","shell.execute_reply":"2025-11-02T21:02:31.561626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ORIG = []\n\nfor col in BASE:\n    # MEAN\n    mean_map = orig.groupby(col)[TARGET].mean()\n    new_mean_col_name = f\"orig_mean_{col}\"\n    mean_map.name = new_mean_col_name\n    \n    train = train.merge(mean_map, on=col, how='left')\n    test = test.merge(mean_map, on=col, how='left')\n    orig = orig.merge(mean_map, on=col, how='left')\n    ORIG.append(new_mean_col_name)\n\n    # COUNT\n    new_count_col_name = f\"orig_count_{col}\"\n    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n    \n    train = train.merge(count_map, on=col, how='left')\n    test = test.merge(count_map, on=col, how='left')\n    orig = orig.merge(count_map, on=col, how='left')\n    ORIG.append(new_count_col_name)\n\nprint(len(ORIG), 'Orig Features Created!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:02:31.564434Z","iopub.execute_input":"2025-11-02T21:02:31.564693Z","iopub.status.idle":"2025-11-02T21:03:02.390626Z","shell.execute_reply.started":"2025-11-02T21:02:31.564673Z","shell.execute_reply":"2025-11-02T21:03:02.389613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_features(df):\n    \"\"\"Create additional features from existing ones\"\"\"\n    df = df.copy()\n    \n    # Income-based features\n    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n    df['loan_to_income_pct'] = (df['loan_amount'] / df['annual_income']) * 100\n    \n    # Risk features\n    df['high_dti'] = (df['debt_to_income_ratio'] > 0.4).astype(int)\n    df['low_credit'] = (df['credit_score'] < 650).astype(int)\n    df['high_interest'] = (df['interest_rate'] > 15).astype(int)\n    \n    # Combined risk score\n    df['risk_score'] = df['high_dti'] + df['low_credit'] + df['high_interest']\n    \n    # Credit score bins\n    df['credit_score_bin'] = pd.cut(df['credit_score'], \n                                      bins=[0, 580, 670, 740, 800, 900],\n                                      labels=['poor', 'fair', 'good', 'very_good', 'excellent'])\n    \n    # Income bins\n    df['income_bin'] = pd.qcut(df['annual_income'], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n    \n    # Extract grade from grade_subgrade (e.g., 'A1' -> 'A')\n    df['grade'] = df['grade_subgrade'].str[0]\n    \n    return df\n\n\nNEW = ['income_to_loan_ratio', 'loan_to_income_pct', 'high_dti', 'low_credit', 'high_interest', 'risk_score',\n       'credit_score_bin', 'income_bin', 'grade']\nCATS += ['credit_score_bin', 'income_bin', 'grade']\n# Apply feature engineering\ntrain = create_features(train)\ntest = create_features(test)\norig = create_features(orig)\n\nprint(\"New features created!\")\nprint(f\"Train shape after feature engineering: {train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:03:02.391722Z","iopub.execute_input":"2025-11-02T21:03:02.392159Z","iopub.status.idle":"2025-11-02T21:03:03.962981Z","shell.execute_reply.started":"2025-11-02T21:03:02.392127Z","shell.execute_reply":"2025-11-02T21:03:03.961961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = BASE + ORIG + INTER + NEW\nprint(len(FEATURES), 'Features.')\ncategorical_features = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()\nprint(len(categorical_features), 'categorical_features.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:03:03.964788Z","iopub.execute_input":"2025-11-02T21:03:03.965152Z","iopub.status.idle":"2025-11-02T21:03:07.816258Z","shell.execute_reply.started":"2025-11-02T21:03:03.965128Z","shell.execute_reply":"2025-11-02T21:03:07.815341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Prepare Data for Modeling","metadata":{}},{"cell_type":"code","source":"# Keep only necessary columns from orig (same structure as train)\norig_trimmed = orig[[*FEATURES, TARGET]].copy()\n\n# Combine both dataframes\ncombined = pd.concat([train, orig_trimmed], ignore_index=True)\n\n# Drop duplicate rows based on FEATURES + TARGET\ncombined = combined.drop_duplicates(subset=FEATURES + [TARGET], keep='first').reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:03:07.817142Z","iopub.execute_input":"2025-11-02T21:03:07.817482Z","iopub.status.idle":"2025-11-02T21:03:18.788625Z","shell.execute_reply.started":"2025-11-02T21:03:07.81746Z","shell.execute_reply":"2025-11-02T21:03:18.787747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode categorical variables\nlabel_encoders = {}\n\nfor col in CATS:\n    le = LabelEncoder()\n    # Fit on combined train+test to handle unseen categories\n    combined_new = pd.concat([combined[col], test[col]], axis=0)\n    le.fit(combined_new.astype(str))\n    \n    combined[col] = le.transform(combined[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n    \n    label_encoders[col] = le\n\nprint(\"Categorical encoding completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:03:18.789489Z","iopub.execute_input":"2025-11-02T21:03:18.789728Z","iopub.status.idle":"2025-11-02T21:03:21.016464Z","shell.execute_reply.started":"2025-11-02T21:03:18.7897Z","shell.execute_reply":"2025-11-02T21:03:21.015373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare X and y\n\n# 4️⃣ Extract final X and y\nX = combined[FEATURES]\ny = combined[TARGET]\nX_test = test[FEATURES]\n\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T21:03:21.017634Z","iopub.execute_input":"2025-11-02T21:03:21.017994Z","iopub.status.idle":"2025-11-02T21:03:21.839223Z","shell.execute_reply.started":"2025-11-02T21:03:21.017964Z","shell.execute_reply":"2025-11-02T21:03:21.838263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Target Encoder that supports multiple aggregation functions,\n    internal cross-validation for leakage prevention, and smoothing.\n\n    Parameters\n    ----------\n    cols_to_encode : list of str\n        List of column names to be target encoded.\n\n    aggs : list of str, default=['mean']\n        List of aggregation functions to apply. Any function accepted by\n        pandas' `.agg()` method is supported, such as:\n        'mean', 'std', 'var', 'min', 'max', 'skew', 'nunique', \n        'count', 'sum', 'median'.\n        Smoothing is applied only to the 'mean' aggregation.\n\n    cv : int, default=5\n        Number of folds for cross-validation in fit_transform.\n\n    smooth : float or 'auto', default='auto'\n        The smoothing parameter `m`. A larger value puts more weight on the \n        global mean. If 'auto', an empirical Bayes estimate is used.\n        \n    drop_original : bool, default=False\n        If True, the original columns to be encoded are dropped.\n    \"\"\"\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n        self.cols_to_encode = cols_to_encode\n        self.aggs = aggs\n        self.cv = cv\n        self.smooth = smooth\n        self.drop_original = drop_original\n        self.mappings_ = {}\n        self.global_stats_ = {}\n\n    def fit(self, X, y):\n        \"\"\"\n        Learn mappings from the entire dataset.\n        These mappings are used for the transform method on validation/test data.\n        \"\"\"\n        temp_df = X.copy()\n        temp_df['target'] = y\n\n        # Learn global statistics for each aggregation\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n\n        # Learn category-specific mappings\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n        \n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Apply learned mappings to the data.\n        Unseen categories are filled with global statistics.\n        \"\"\"\n        X_transformed = X.copy()\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n                new_col_name = f'TE_{col}_{agg_func}'\n                map_series = self.mappings_[col][agg_func]\n                X_transformed[new_col_name] = X[col].map(map_series)\n                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n        \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        Fit and transform the data using internal cross-validation to prevent leakage.\n        \"\"\"\n        # First, fit on the entire dataset to get global mappings for transform method\n        self.fit(X, y)\n\n        # Initialize an empty DataFrame to store encoded features\n        encoded_features = pd.DataFrame(index=X.index)\n        \n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n\n        for train_idx, val_idx in kf.split(X, y):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx]\n            \n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n\n            for col in self.cols_to_encode:\n                # --- Calculate mappings only on the training part of the fold ---\n                for agg_func in self.aggs:\n                    new_col_name = f'TE_{col}_{agg_func}'\n                    \n                    # Calculate global stat for this fold\n                    fold_global_stat = y_train.agg(agg_func)\n                    \n                    # Calculate category stats for this fold\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n\n                    # --- Apply smoothing only for 'mean' aggregation ---\n                    if agg_func == 'mean':\n                        counts = temp_df_train.groupby(col)['target'].count()\n                        \n                        m = self.smooth\n                        if self.smooth == 'auto':\n                            # Empirical Bayes smoothing\n                            variance_between = mapping.var()\n                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n                            if variance_between > 0:\n                                m = avg_variance_within / variance_between\n                            else:\n                                m = 0  # No smoothing if no variance between groups\n                        \n                        # Apply smoothing formula\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n                        encoded_values = X_val[col].map(smoothed_mapping)\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n                    \n                    # Store encoded values for the validation fold\n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n\n        # Merge with original DataFrame\n        X_transformed = X.copy()\n        for col in encoded_features.columns:\n            X_transformed[col] = encoded_features[col]\n            \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model with Cross-Validation","metadata":{}},{"cell_type":"code","source":"# # LightGBM parameters\n# params = {\n#     'objective': 'binary',\n#     'metric': 'auc',\n#     'boosting_type': 'gbdt',\n#     'learning_rate': 0.05,\n#     'num_leaves': 31,\n#     'max_depth': -1,\n#     'min_child_samples': 20,\n#     'subsample': 0.8,\n#     'subsample_freq': 1,\n#     'colsample_bytree': 0.8,\n#     'reg_alpha': 0.1,\n#     'reg_lambda': 0.1,\n#     'random_state': SEED,\n#     'n_jobs': -1,\n#     'verbose': -1\n# }\n\n# Cross-validation setup\nN_FOLDS = 5\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration / containers ---\nMdl_Master = {\n    'XGB1C': [\n        XGBC(**{\n            \"objective\": \"binary:logistic\",\n            \"eval_metric\": \"auc\",\n            \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n            \"learning_rate\": 0.01,\n            \"n_estimators\": nest,\n            \"max_depth\": 8,\n            \"subsample\": 0.90,\n            \"colsample_bytree\": 0.75,\n            \"reg_lambda\": 0.75,\n            \"reg_alpha\": 0.001,\n            \"verbosity\": 0,\n            \"random_state\": 42,\n            \"enable_categorical\": True,\n            \"early_stopping_rounds\": es,\n        }),\n        {\"verbose\": 0}  # extra fit params for this model (if used)\n    ],\n\n    'LGBM1C': [\n        LGBMC(**{\n            \"objective\": \"binary\",\n            \"eval_metric\": \"auc\",\n            \"device\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n            \"learning_rate\": 0.01,\n            \"n_estimators\": nest,\n            \"max_depth\": 7,\n            \"subsample\": 0.90,\n            \"colsample_bytree\": 0.60,\n            \"reg_lambda\": 1.25,\n            \"reg_alpha\": 0.001,\n            \"verbosity\": -1,  # suppress verbose LGBM logs\n            \"random_state\": 42,\n        }),\n        {\n            \"callbacks\": [\n                log_evaluation(period=100),  # print evaluation every 100 rounds\n                early_stopping(es, verbose=False)\n            ]\n        }\n    ],\n\n    'LGBM2C': [\n        LGBMC(**{\n            \"objective\": \"binary\",\n            \"data_sample_strategy\": \"goss\",\n            \"eval_metric\": \"auc\",\n            \"device\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n            \"learning_rate\": 0.01,\n            \"n_estimators\": nest,\n            \"max_depth\": 6,\n            \"subsample\": 0.825,\n            \"colsample_bytree\": 0.55,\n            \"reg_lambda\": 0.85,\n            \"reg_alpha\": 0.001,\n            \"verbosity\": -1,\n            \"random_state\": 42,\n        }),\n        {\n            \"callbacks\": [\n                log_evaluation(period=100),\n                early_stopping(es, verbose=False)\n            ]\n        }\n    ],\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Storage\nn_samples = len(X)\noof_predictions = np.zeros(n_samples, dtype=float)  # final ensemble oof\ntest_predictions = None  # will init after we know X_test length\nfeature_importance = pd.DataFrame()\ncv_scores_model = {k: [] for k in Mdl_Master.keys()}  # per-model CV scores\nensemble_cv_scores = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting cross-validation training...\")\nprint(\"=\" * 60)\n\n# silence non-critical warnings (be careful with this globally; you might want a context)\nwarnings.filterwarnings(\"ignore\")\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\nFold {fold}/{N_FOLDS}\")\n\n    # Split data\n    X_train_fold, X_val_fold = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n    y_train_fold, y_val_fold = y.iloc[train_idx].copy(), y.iloc[val_idx].copy()\n    X_test_fold = test[FEATURES].copy()  # keep same name\n\n    # Target encoding (fit only on train part)\n    TE = TargetEncoder(cols_to_encode=INTER, cv=5, smooth='auto', aggs=['mean'], drop_original=True)\n    X_train_fold = TE.fit_transform(X_train_fold, y_train_fold)\n    X_val_fold = TE.transform(X_val_fold)\n    X_test_transformed = TE.transform(X_test_fold)\n\n    # Initialize containers for this fold\n    per_model_val_preds = []\n    per_model_test_preds = []\n\n    # initialize test_predictions vector once (we know length of X_test_transformed)\n    if test_predictions is None:\n        test_predictions = np.zeros(len(X_test_transformed), dtype=float)\n\n    for method, (model, fit_params) in Mdl_Master.items():\n        # Use a local copy to avoid accidental mutation\n        local_fit_params = dict(fit_params)\n\n        # If callbacks/logging present in fit_params, we've already set log_evaluation(period=100)\n        # Fit model (safely pass eval_set and any callbacks)\n        model.fit(\n            X_train_fold,\n            y_train_fold,\n            eval_set=[(X_val_fold, y_val_fold)],\n            **local_fit_params\n        )\n\n        # Probabilistic predictions for validation and test\n        val_pred = pd.Series(model.predict_proba(X_val_fold)[:, 1], index=val_idx, name=method)\n        test_pred = pd.Series(model.predict_proba(X_test_transformed)[:, 1], name=method)\n\n        per_model_val_preds.append(val_pred)\n        per_model_test_preds.append(test_pred)\n\n        # model-level AUC\n        model_auc = roc_auc_score(y_val_fold, val_pred.values)\n        cv_scores_model[method].append(model_auc)\n        print(f\"Fold {fold}_{method} AUC: {model_auc:.6f}\")\n        print(f\"---> Model {method} fitted successfully\")\n\n        # Feature importance: be robust to different shapes/names after TE\n        feat_names = list(X_train_fold.columns)\n        fi = getattr(model, \"feature_importances_\", None)\n        if fi is not None and len(fi) == len(feat_names):\n            fold_importance = pd.DataFrame({\n                'feature': feat_names,\n                'importance': fi,\n                'fold': fold,\n                'model': method\n            })\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n        else:\n            # if model doesn't provide importances or shapes mismatch, skip/notify\n            print(f\"Warning: feature_importances_ not available or shape mismatch for {method}; skipping FI for this model.\")\n\n    # combine per-model preds into DataFrame indexed by val_idx\n    val_preds_df = pd.concat(per_model_val_preds, axis=1)\n    test_preds_df = pd.concat(per_model_test_preds, axis=1)\n\n    # Ensemble strategy: simple mean across models (you can change to weighted average)\n    ensemble_val = val_preds_df.mean(axis=1)\n    ensemble_test = test_preds_df.mean(axis=1)\n\n    # store ensemble OOF predictions\n    oof_predictions[val_idx] = ensemble_val.values\n\n    # accumulate test predictions (averaged across folds)\n    test_predictions += ensemble_test.values / N_FOLDS\n\n    # also record ensemble fold AUC\n    ens_auc = roc_auc_score(y_val_fold, ensemble_val.values)\n    ensemble_cv_scores.append(ens_auc)\n    print(f\"Fold {fold} Ensemble AUC: {ens_auc:.6f}\")\n\nprint(\"\\n\" + \"=\" * 60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summaries\n# per-model mean+std\nfor method, scores in cv_scores_model.items():\n    if scores:\n        print(f\"{method} CV AUC: {np.mean(scores):.6f} (+/- {np.std(scores):.6f})\")\n# ensemble summary\nprint(f\"Ensemble Mean CV AUC: {np.mean(ensemble_cv_scores):.6f} (+/- {np.std(ensemble_cv_scores):.6f})\")\n\nprint(\"=\" * 60)\n\n# Optionally: create a DataFrame for oof predictions and check length\noof_df = pd.DataFrame({'oof_pred': oof_predictions, 'target': y.values})\nassert len(oof_df) == n_samples, \"OOF size mismatch\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Overall OOF AUC\noof_auc = roc_auc_score(y, oof_predictions)\nprint(f\"Out-of-Fold AUC: {oof_auc:.6f}\")\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y, oof_predictions)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {oof_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curve - Loan Default Prediction', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature importance\nimportance_df = feature_importance.groupby('feature')['importance'].mean().sort_values(ascending=False).reset_index()\n\nplt.figure(figsize=(10, 8))\ntop_n = 20\nplt.barh(range(top_n), importance_df['importance'][:top_n], color='steelblue')\nplt.yticks(range(top_n), importance_df['feature'][:top_n])\nplt.xlabel('Average Importance (Gain)', fontsize=12)\nplt.title(f'Top {top_n} Feature Importance', fontsize=14, fontweight='bold')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(importance_df.head(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Generate Submission File","metadata":{}},{"cell_type":"code","source":"# Create submission dataframe\nsubmission = pd.DataFrame({\n    'id': test[id_col],\n    'loan_paid_back': test_predictions\n})\n\n# Verify submission format\nprint(\"Submission shape:\", submission.shape)\nprint(\"\\nFirst few predictions:\")\nprint(submission.head(10))\nprint(\"\\nPrediction statistics:\")\nprint(submission['loan_paid_back'].describe())\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\nSubmission file saved as 'submission.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}