{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #fff; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 10px; \n        font-size: 36px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        üè¶ Predicting Loan Payback üí∞\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# üìÇ Overview\n\n* **Background** <br>\n  Assessing a borrower‚Äôs **loan repayment capability** is a fundamental task that helps financial institutions reduce default risk and promote financial inclusion.\n  This dataset provides demographic, financial, credit history, and loan-related information to model **the likelihood of loan repayment**.\n\n* **Goal of the Project** <br>\n  The main objective is to **predict whether a borrower will pay back a loan** based on demographic, income, debt, credit, and loan characteristics.\n  The insights support **loan approval decisions** and **risk management strategies**.\n\n**Key Features**\n\n| Feature                | Description                                                         | Type / Range                                              |\n| ---------------------- | ------------------------------------------------------------------- | --------------------------------------------------------- |\n| `id`                   | Unique loan identifier                                              | string                                                    |\n| `annual_income`        | Annual income of the borrower                                       | float                                                     |\n| `debt_to_income_ratio` | Ratio of total debt to income (DTI)                                 | float                                                     |\n| `credit_score`         | Borrower‚Äôs credit score                                             | int                                                       |\n| `loan_amount`          | Amount of loan requested                                            | float                                                     |\n| `interest_rate`        | Annual nominal interest rate (%)                                    | float                                                     |\n| `gender`               | Gender of the borrower                                              | {Male, Female, Other}                                     |\n| `marital_status`       | Marital status                                                      | {Single, Married, Divorced, Widowed}                      |\n| `education_level`      | Educational background                                              | {High School, Bachelor‚Äôs, Master‚Äôs, PhD, Other}           |\n| `employment_status`    | Employment type                                                     | {Employed, Self-employed, Unemployed, Student, Retired}   |\n| `loan_purpose`         | Purpose of the loan                                                 | {Debt consolidation, Car, Business, Home, Medical, Other} |\n| `grade_subgrade`       | Assigned loan grade or subgrade (e.g., A1‚ÄìC3)                       | categorical                                               |\n| `loan_paid_back`       | **Target variable:** Loan repayment status (1 = Paid, 0 = Not Paid) | binary                                                 |\n\n**Files Provided**\n\n* `train.csv`: Training dataset (with target `Status`).\n* `test.csv`: Test dataset (without target variable).\n* `sample_submission.csv`: Template for submission.\n\n(Source: [Kaggle Competition ‚Äì Playground Series S5E11](https://www.kaggle.com/competitions/playground-series-s5e11))","metadata":{}},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Import Libraries</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Import Libraries\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"!pip install statsmodels > pip_log_statsmodels.txt 2>&1\n!pip install scikit_posthocs > pip_log_scikit_posthocs.txt 2>&1\n!pip install pingouin > pip_log_pingouin.txt 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:10.736019Z","iopub.execute_input":"2025-11-01T15:23:10.736221Z","iopub.status.idle":"2025-11-01T15:23:22.352825Z","shell.execute_reply.started":"2025-11-01T15:23:10.736204Z","shell.execute_reply":"2025-11-01T15:23:22.352035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Core data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\n\n# Statistical functions\nfrom scipy.stats import skew, kurtosis, probplot\n\n# Display utilities for Jupyter notebooks\nfrom IPython.display import display, HTML\n\n# Machine learning preprocessing and modeling\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# Metrics\nfrom sklearn.metrics import (roc_curve, roc_auc_score, classification_report, confusion_matrix,\n                             precision_recall_curve, auc, average_precision_score, log_loss)\n\n# Statistical\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import probplot\nfrom scipy.stats import kruskal\nimport scikit_posthocs as sp\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom scipy.stats import levene\nfrom scipy import stats\nimport pingouin as pg\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import mannwhitneyu\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", 500) # To display all the columns of dataframe\npd.set_option(\"max_colwidth\", None) # To set the width of the column to maximum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:22.354659Z","iopub.execute_input":"2025-11-01T15:23:22.354899Z","iopub.status.idle":"2025-11-01T15:23:31.486605Z","shell.execute_reply.started":"2025-11-01T15:23:22.35488Z","shell.execute_reply":"2025-11-01T15:23:31.485934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    SEED = 42\n    MAX_ITER = 50000\n    N_SPLIT = 1\n    TEST_SIZE = 0.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:31.487269Z","iopub.execute_input":"2025-11-01T15:23:31.487524Z","iopub.status.idle":"2025-11-01T15:23:31.491666Z","shell.execute_reply.started":"2025-11-01T15:23:31.487502Z","shell.execute_reply":"2025-11-01T15:23:31.490877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Import Libraries</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Load Data\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Load the datasets\ndf_train = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\n\n# Verify shapes\nprint(\"Train Data Shape:\", df_train.shape)\nprint(\"\\nTest Data Shape:\", df_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:31.492372Z","iopub.execute_input":"2025-11-01T15:23:31.492597Z","iopub.status.idle":"2025-11-01T15:23:33.191933Z","shell.execute_reply.started":"2025-11-01T15:23:31.492581Z","shell.execute_reply":"2025-11-01T15:23:33.191274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Data Preview and Info</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Data Preview and Info\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Display few rows of each dataset\nprint(\"Train Data Preview:\")\ndisplay(df_train.head())\n\nprint(\"\\nTest Data Preview:\")\ndisplay(df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:33.192741Z","iopub.execute_input":"2025-11-01T15:23:33.193068Z","iopub.status.idle":"2025-11-01T15:23:33.234361Z","shell.execute_reply.started":"2025-11-01T15:23:33.193047Z","shell.execute_reply":"2025-11-01T15:23:33.233491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display information about the DataFrames\nprint(\"Train Data Info:\")\ndf_train.info()\n\nprint(\"\\nTest Data Info:\")\ndf_test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:33.235276Z","iopub.execute_input":"2025-11-01T15:23:33.235541Z","iopub.status.idle":"2025-11-01T15:23:33.496987Z","shell.execute_reply.started":"2025-11-01T15:23:33.235513Z","shell.execute_reply":"2025-11-01T15:23:33.4962Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Insights from Initial Data Exploration\n\n**Dataset Size and Structure**\n\n* **Train:** 593994 samples ¬∑ 13 columns (including target `loan_paid_back`)\n* **Test:** 254569 samples ¬∑ 12 columns (no `loan_paid_back`), ready for prediction.\n\n**Feature Overview**\n\n* **Numerical features:**\n  `annual_income`, `debt_to_income_ratio`, `credit_score`, `loan_amount`, `interest_rate`\n\n* **Categorical features:**\n  `id`, `gender`, `marital_status`, `education_level`, `employment_status`, `loan_purpose`, `grade_subgrade`\n\n* **Target variable:**\n  `loan_paid_back`\n\n**Data Consistency**\n\n* Data types are consistent (`float64`/`int64` for numeric, `object` for categorical).\n* Column alignment is perfect across Train and Test ‚Äî no schema drift or feature mismatch.\n* The column `id` is not meaningful for analysis. So this column is not really a part of the information we should care about. We can drop this column for both train and test data.","metadata":{}},{"cell_type":"code","source":"df_train.drop(\"id\", axis=1, inplace=True)\nlist_test_id = df_test[\"id\"].copy().to_list()\ndf_test.drop(\"id\", axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:33.49776Z","iopub.execute_input":"2025-11-01T15:23:33.498043Z","iopub.status.idle":"2025-11-01T15:23:33.569424Z","shell.execute_reply.started":"2025-11-01T15:23:33.498016Z","shell.execute_reply":"2025-11-01T15:23:33.568586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_features = [\"annual_income\", \"debt_to_income_ratio\", \"credit_score\", \"loan_amount\", \"interest_rate\"]\ncat_features = [\"gender\", \"marital_status\", \"education_level\", \"employment_status\", \"loan_purpose\", \"grade_subgrade\"]\nprint(\"Train Data describe:\")\ncm = sns.light_palette(\"green\", as_cmap=True)\ndisplay(df_train[num_features].describe().T.style.background_gradient(cmap=cm))\n\nprint(\"\\nTest Data describe:\")\ndisplay(df_test[num_features].describe().T.style.background_gradient(cmap=cm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:33.571902Z","iopub.execute_input":"2025-11-01T15:23:33.57218Z","iopub.status.idle":"2025-11-01T15:23:33.84784Z","shell.execute_reply.started":"2025-11-01T15:23:33.572163Z","shell.execute_reply":"2025-11-01T15:23:33.84726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerical Feature Summary ‚Äî Data Analyst Perspective\n\n* The *Train* and *Test* datasets share very similar structures and distributions, ensuring consistency during model training and evaluation.\n\n**Feature Details:**\n\n| Feature                  | Remarks                                                                                                                    |\n| ------------------------ | -------------------------------------------------------------------------------------------------------------------------- |\n| **annual_income**        | The average annual income is around **48,200** USD. It has a wide distribution, with the highest income close to **400,000** USD.  |\n| **debt_to_income_ratio** | The average debt-to-income ratio is approximately 6.8%. The distribution is reasonable, with a maximum value of **99%**.       |\n| **credit_score**         | The average credit score is around 694, ranging from 300 to 850. The distribution is fairly uniform.                       |\n| **loan_amount**          | The average loan amount is around 15,000 USD. The test data shows a lower standard deviation, indicating less variability. |\n| **interest_rate**        | The average interest rate is approximately 12.5%, with similar distributions in both datasets.                             |","metadata":{}},{"cell_type":"code","source":"def convert_cat(features, df):\n    for feature in features:\n        if feature in df.columns:\n            df[feature] = df[feature].astype(\"category\")\nconvert_cat(cat_features, df=df_train)\nconvert_cat(cat_features, df=df_test)\n\nprint(\"Train Data describe:\")\ndisplay(df_train[cat_features].describe().T.style.background_gradient(cmap=\"Greens\", subset=[\"unique\", \"freq\"]))\n\nprint(\"\\nTest Data describe:\")\ndisplay(df_test[cat_features].describe().T.style.background_gradient(cmap=\"Greens\", subset=[\"unique\", \"freq\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:33.848482Z","iopub.execute_input":"2025-11-01T15:23:33.848799Z","iopub.status.idle":"2025-11-01T15:23:34.163667Z","shell.execute_reply.started":"2025-11-01T15:23:33.848782Z","shell.execute_reply":"2025-11-01T15:23:34.163079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical Feature Summary ‚Äî Data Analyst Perspective\n\n**Overview:**\n\n* Both *Train* and *Test* datasets share similar distributions across categorical variables, ensuring representativeness during model training and evaluation.\n\n**Feature Details:**\n\n| Feature               | Remarks                                                                         |\n| --------------------- | ------------------------------------------------------------------------------- |\n| **gender**            | Contains 3 categories, with **Female** being the most common (~51%).              |\n| **marital_status**    | Consists of 4 groups, with **Single** having the highest proportion (~48%).       |\n| **education_level**   | Includes 5 levels, where **Bachelor‚Äôs** is the most frequent (~47%).              |\n| **employment_status** | Comprises 5 categories, with **Employed** making up the majority (~76%).          |\n| **loan_purpose**      | Covers 8 loan purposes, with **Debt consolidation** being the most common (~31%). |\n| **grade_subgrade**    | Contains 30 classification levels, with **C3** appearing most frequently (~10%).  |","metadata":{}},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Data Quality Check</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Data Quality Check\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## Missing Value","metadata":{}},{"cell_type":"code","source":"def displayNULL(df, dataset_name=None):\n    total_rows = len(df)\n\n    # Replace blank strings with NaN for completeness\n    df_null_check = df.replace(r\"^\\s*$\", np.nan, regex=True)\n\n    missing_df = df_null_check.isnull().sum().reset_index()\n    missing_df.columns = [\"Feature\", \"Missing_Count\"]\n    missing_df = missing_df[missing_df[\"Missing_Count\"] > 0]\n    missing_df[\"Missing_%\"] = (missing_df[\"Missing_Count\"] / total_rows * 100).round(2)\n    missing_df = missing_df.sort_values(by=\"Missing_Count\", ascending=False).reset_index(drop=True)\n\n    total_missing = missing_df[\"Missing_Count\"].sum()\n\n    print(\"=\" * 40)\n    if dataset_name:\n        print(f\"üîé Missing Value Summary for: {dataset_name}\")\n    else:\n        print(\"üîé Missing Value Summary:\")\n    print(\"=\" * 40)\n    \n    if total_missing == 0:\n        print(f\"‚úÖ No missing values detected in {total_rows:,} rows.\")\n    else:\n        try:\n            from tabulate import tabulate\n            print(tabulate(missing_df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, colalign=(\"left\", \"left\", \"left\")))\n        except ImportError:\n            print(missing_df.to_string(index=False))\n        \n        print(f\"\\n‚ö†Ô∏è  Total missing values: {total_missing:,} out of {total_rows:,} rows.\")\n\nprint(\"Missing value train dataset: \")\ndisplayNULL(df_train, dataset_name=\"Train Set\")\n\nprint(\"\\nMissing value test dataset: \")\ndisplayNULL(df_test, dataset_name=\"Test Set\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:34.164422Z","iopub.execute_input":"2025-11-01T15:23:34.164641Z","iopub.status.idle":"2025-11-01T15:23:36.463756Z","shell.execute_reply.started":"2025-11-01T15:23:34.164626Z","shell.execute_reply":"2025-11-01T15:23:36.463135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checking duplicate Value","metadata":{}},{"cell_type":"code","source":"def check_duplicates_report(df, dataset_name):\n    duplicates_count = df.duplicated().sum()\n    total_rows = len(df)\n\n    print(\"=\" * 40)\n    print(f\"üîç {dataset_name} Duplicate Analysis\")\n    print(\"=\" * 40)\n\n    if duplicates_count == 0:\n        print(f\"‚úÖ No duplicates found in {total_rows:,} rows\")\n    else:\n        print(f\"‚ö†Ô∏è  {duplicates_count} duplicates found ({duplicates_count/total_rows:.2%})\")\n        print(f\"    Total rows affected: {duplicates_count:,}/{total_rows:,}\")\n\ndatasets = {\n    \"Training Data\": df_train,\n    \"Test Data\": df_test\n}\n\nduplicate_summary = {}\nfor name, data in datasets.items():\n    check_duplicates_report(data, name)\n    duplicate_summary[name] = {\n        \"duplicates\": data.duplicated().sum(),\n        \"total_rows\": len(data)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:36.464591Z","iopub.execute_input":"2025-11-01T15:23:36.464883Z","iopub.status.idle":"2025-11-01T15:23:36.953938Z","shell.execute_reply.started":"2025-11-01T15:23:36.464859Z","shell.execute_reply":"2025-11-01T15:23:36.953313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checking Outlier Value","metadata":{}},{"cell_type":"code","source":"def checking_outlier(list_feature, df, dataset_name):\n    print(\"=\" * 40)\n    print(f\"üîç {dataset_name} Checking outlier\")\n    print(\"=\" * 40)\n    outlier_info = []\n    for feature in list_feature:\n        Q1 = df[feature].quantile(0.25)\n        Q3 = df[feature].quantile(0.75)\n        IQR = Q3 - Q1\n\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n\n        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)][feature]\n        if len(outliers) == 0:\n            pass\n        else:\n            outlier_info.append({\n            \"Feature\": feature,\n            \"Outlier Count\": len(outliers),\n            # \"Outlier Detail\": outliers.tolist()\n            })\n    return pd.DataFrame(outlier_info)\n\nchecking_outlier(list_feature=num_features, df=df_train, dataset_name=\"Training data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:36.954678Z","iopub.execute_input":"2025-11-01T15:23:36.954891Z","iopub.status.idle":"2025-11-01T15:23:37.082336Z","shell.execute_reply.started":"2025-11-01T15:23:36.954875Z","shell.execute_reply":"2025-11-01T15:23:37.081708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checking_outlier(list_feature=num_features, df=df_test, dataset_name=\"Test data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:37.082974Z","iopub.execute_input":"2025-11-01T15:23:37.083162Z","iopub.status.idle":"2025-11-01T15:23:37.144302Z","shell.execute_reply.started":"2025-11-01T15:23:37.083148Z","shell.execute_reply":"2025-11-01T15:23:37.143563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Quality Insights: Missing Values, Duplicates and Outliers.\n\n**Missing Values Analysis**\n\n* We conducted a thorough examination for missing values across the **train**, **test**, and **original** datasets.\n\n* **No missing values** were detected ‚Äî ensuring clean synthetic data for model training.\n\n**Duplicate Records Analysis**\n\n* We performed a check for exact duplicate rows that could artificially inflate performance or introduce bias.\n\n* **No duplicates** detected across the **train** and **test** datasets. ‚Äî ensuring test predictions are made on unique samples.\n\n**Outlier Analysis**\n\n* We also examined the datasets for checking outliers.\n* **The outliers** were found in dataset at features. But we can not remove them since these outliers reflect reality.\n* Both **train** and **test** datasets contain **notable outliers** in `annual_income`, `debt_to_income_ratio`, `credit_score`, `loan_amount`, `interest_rate`.\n\n**Conclusion**\n\n* The **training and test datasets** exhibit excellent data quality with **no missing or duplicate entries**.\n* The outlier value exist across the **train** and **test** datasets.","metadata":{}},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Exploratory Data Analysis</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Exploratory Data Analysis\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def color(n_colors=2, tone=\"diverging\"):\n    stop = 1\n    if tone == \"diverging\":\n        cmap = sns.diverging_palette(0, 230, as_cmap=True)\n        stop = 0.9\n    elif tone == \"pastel\":\n        cmap = sns.color_palette(\"pastel\")\n    elif tone == \"muted\":\n        cmap = sns.color_palette(\"muted\")\n    elif tone == \"husl\":\n        cmap = sns.color_palette(\"husl\")\n    elif tone == \"Dark2\":\n        cmap = sns.color_palette(\"Dark2\")\n    elif tone == \"viridis\":\n        cmap = sns.color_palette(\"viridis\")\n    elif tone == \"crest\":\n        cmap = sns.color_palette(\"crest\")\n    elif tone == \"Paired\":\n        cmap = sns.color_palette(\"Paired\")\n    elif tone == \"rocket\":\n        cmap = sns.color_palette(\"rocket\")\n    elif tone == \"rocket_r\":\n        cmap = sns.color_palette(\"rocket_r\")\n    elif tone == \"mako\":\n        cmap = sns.color_palette(\"mako\")\n    elif tone == \"RdYlGn\":\n        cmap = sns.color_palette(\"RdYlGn\")\n    elif tone == \"modern\":\n        cmap = sns.color_palette([\"#E63946\",\"#F1FAEE\",\"#A8DADC\",\"#457B9D\",\"#1D3557\"])\n    positions = np.linspace(0, stop, n_colors)\n    return [cmap(p) for p in positions] if callable(cmap) else cmap[:n_colors]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:37.145095Z","iopub.execute_input":"2025-11-01T15:23:37.145359Z","iopub.status.idle":"2025-11-01T15:23:37.151323Z","shell.execute_reply.started":"2025-11-01T15:23:37.145343Z","shell.execute_reply":"2025-11-01T15:23:37.150758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cal_ChiSquare(cat_feature, target_feature, df, show_expected=False, show_residuals=False):\n    \"\"\"\n    Perform a Chi-Square test of independence to evaluate whether two categorical variables\n    are statistically associated (i.e., dependent) or independent from each other.\n\n    This function tests the null hypothesis that the two categorical variables are independent.\n    It prints the test statistic, degrees of freedom, p-value, and an interpretation based on the p-value.\n    Optionally, it displays the expected frequency table under independence, and standardized residuals\n    (including a heatmap) which help to identify specific group-level deviations.\n\n    Parameters\n    ----------\n    cat_feature : str\n        Name of the first categorical variable (typically the feature).\n\n    target_feature : str\n        Name of the second categorical variable (typically the target label).\n\n    df : pd.DataFrame\n        The input DataFrame containing the data.\n\n    show_expected : bool, default=False\n        If True, prints the expected frequencies under the assumption of independence.\n\n    show_residuals : bool, default=False\n        If True, prints the standardized residuals and shows them as a heatmap\n        to identify where the strongest associations/deviations occur.\n\n    Returns\n    -------\n    None\n        Prints the Chi-Square test result, including statistical significance interpretation.\n        Optionally prints expected values and standardized residuals.\n\n    Notes\n    -----\n    - Hypotheses:\n        H‚ÇÄ (Null):     The two variables are independent (no association).\n        H‚ÇÅ (Alt.):      There is a dependency or association between the variables.\n\n    - Interpretation:\n        If p-value < 0.05 ‚Üí Reject H‚ÇÄ ‚Üí Conclude that the variables are significantly associated.\n        If p-value ‚â• 0.05 ‚Üí Fail to reject H‚ÇÄ ‚Üí No statistically significant association found.\n\n    - Standardized residuals:\n        - Values > +2 or < -2 indicate strong deviation from expected frequency (local dependency).\n        - Useful for identifying specific group-level contributions to the overall Chi-Square result.\n\n    References\n    ----------\n    - https://en.wikipedia.org/wiki/Chi-squared_test\n    - https://www.scribbr.com/statistics/chi-square-test-of-independence/\n    \"\"\"\n    print(f\"\\nüîç Chi-Square Test of Independence: '{cat_feature}' vs. '{target_feature}'\")\n\n    # Contingency table\n    crosstab = pd.crosstab(df[cat_feature], df[target_feature])\n    chi2, p, dof, expected = chi2_contingency(crosstab)\n\n    print(f\"Chi-squared statistic: {chi2:.3f}\")\n    print(f\"Degrees of freedom: {dof}\")\n    print(f\"p-value: {p:.6f}\")\n\n    if p < 0.05:\n        print(\"‚úÖ Result: p-value < 0.05 ‚Üí Reject H‚ÇÄ\")\n        print(f\"‚Üí There is a **statistically significant association** between '{cat_feature}' and '{target_feature}'.\")\n    else:\n        print(\"‚ùé Result: p-value ‚â• 0.05 ‚Üí Fail to reject H‚ÇÄ\")\n        print(f\"‚Üí No statistically significant association between '{cat_feature}' and '{target_feature}'.\")\n\n    # Optional: show expected frequencies\n    if show_expected:\n        print(\"\\nüìä Expected Frequencies:\")\n        print(pd.DataFrame(expected, index=crosstab.index, columns=crosstab.columns))\n    else:\n        pass\n\n    # Optional: show standardized residuals\n    if show_residuals:\n        # cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n        residuals = (crosstab - expected) / np.sqrt(expected)\n        print(\"\\nüìà Standardized Residuals:\")\n        print(round(residuals, 2))\n\n        # Heatmap of residuals\n        plt.figure(figsize=(10, 7))\n        sns.heatmap(residuals, annot=True, cmap=\"RdYlGn\", center=0, fmt=\".2f\", linewidths=0.5)\n        plt.title(f\"Standardized Residuals Heatmap: {cat_feature} vs {target_feature}\", weight=\"bold\", fontsize=13, pad=25)\n        plt.ylabel(cat_feature)\n        plt.xlabel(target_feature)\n        plt.tight_layout()\n        plt.show()\n\ndef perform_kruskal_test(df, categorical_feature, numeric_feature):\n    \"\"\"\n    Perform the Kruskal-Wallis H-test to determine whether there are statistically\n    significant differences in the distribution of a numeric variable across\n    three or more independent groups.\n\n    If the result is significant (p < 0.05), Dunn's post-hoc test with Bonferroni correction\n    is performed to identify which group pairs differ.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input dataset containing the categorical and numerical variables.\n\n    categorical_feature : str\n        The name of the categorical feature that defines the groups.\n\n    numeric_feature : str\n        The name of the numeric feature to be compared across groups.\n\n    Returns\n    -------\n    None\n        Prints the Kruskal-Wallis H-statistic, p-value, interpretation, and\n        optionally the results of Dunn's post-hoc test.\n\n    Notes\n    -----\n    - H‚ÇÄ (null hypothesis): The distribution of the numeric variable is the same across all groups.\n    - H‚ÇÅ (alternative hypothesis): At least one group has a different distribution.\n    - If p < 0.05 ‚Üí reject H‚ÇÄ ‚Üí use Dunn‚Äôs test to explore specific group differences.\n    - Kruskal-Wallis is a non-parametric alternative to one-way ANOVA.\n    - It does not assume normality, but assumes:\n        1. Independent samples\n        2. Ordinal or continuous response variable\n        3. Similar shapes of distributions\n\n    Requirements\n    ------------\n    - `scipy.stats.kruskal`\n    - `scikit-posthocs` package for Dunn‚Äôs test (`import scikit_posthocs as sp`)\n\n    References\n    ----------\n    - https://www.geeksforgeeks.org/kruskal-wallis-test/\n    - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html\n    - https://scikit-posthocs.readthedocs.io/en/latest/index.html\n    \"\"\"\n\n    # Extract values\n    groups = df[categorical_feature].dropna().unique()\n    if len(groups) < 3:\n        print(f\"‚ùå Error: Kruskal-Wallis H-test requires 3 or more groups.\")\n        return\n    else:\n        print(f\"\\nüîç Kruskal-Wallis Test: {numeric_feature} ~ {categorical_feature}\")\n        data_groups = [df[df[categorical_feature] == g][numeric_feature].dropna() for g in groups]\n\n        # Perform kruskal\n        stat, p = kruskal(*data_groups)\n\n        print(f\"Kruskal-Wallis H-statistic: {stat:.3f}\")\n        print(f\"p-value: {p}\")\n\n        if p < 0.05:\n            print(\"üü¢ Significant difference found. Running Dunn's Post-Hoc Test...\")\n            dunn_result = sp.posthoc_dunn(df, val_col=numeric_feature, group_col=categorical_feature, p_adjust=\"bonferroni\")\n            print(dunn_result)\n        else:\n            print(\"\\n‚ÑπÔ∏è No significant difference found (p >= 0.05)\")\n\ndef check_normality_with_plots(df, feature, target_feature, threshold_skew_1=0.5, threshold_skew_2=1.0,\n                               threshold_kurt=1.5, ncols=2):\n    \"\"\"\n    Check the normality of numerical features *within each group* of a categorical feature,\n    using Skewness, Kurtosis, and Q‚ÄìQ plots. \n    If non-normality is detected in any group, automatically perform Kruskal‚ÄìWallis test.\n\n    ---\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input dataset containing both numeric and categorical features.\n\n    feature : numeric\n        Numerical columns to test (e.g. [\"Temparature\"]).\n\n    target_feature : str\n        Categorical variable name (e.g. \"Fertilizer_Name\").\n\n    threshold_skew_1 : float, default = 0.5\n        Threshold for approximately symmetric (|skew| ‚â§ 0.5).\n\n    threshold_skew_2 : float, default = 1.0\n        Threshold for moderate skewness (0.5 < |skew| ‚â§ 1.0).\n\n    threshold_kurt : float, default = 1.5\n        Absolute kurtosis threshold for approximate normality.\n\n    ncols : int, default = 2\n        Number of Q‚ÄìQ plots per row.\n    \"\"\"\n\n    results = []\n    non_normal_detected = False\n\n    print(f\"\\n Checking normality of numeric feature(s) by target feature: '{target_feature}'\")\n\n    # ===  Evaluate normality within each group ===\n    print(f\"\\nüîπ Feature: {feature}\")\n\n    for grp, subset in df.groupby(target_feature):\n        data = subset[feature].dropna()\n        sk = skew(data)\n        kt = kurtosis(data)\n        abs_sk = abs(sk)\n        abs_kt = abs(kt)\n\n        # Skewness interpretation\n        if abs_sk <= threshold_skew_1:\n            skew_remark = \"Approximately symmetric\"\n        elif abs_sk <= threshold_skew_2:\n            skew_remark = \"Moderately skewed\"\n        else:\n            skew_remark = \"Highly skewed\"\n\n        # Kurtosis interpretation\n        if abs_kt < threshold_kurt:\n            kurt_remark = \"Normal tails\"\n        else:\n            kurt_remark = \"Heavy/light tails\"\n\n        remark = f\"{skew_remark}, {kurt_remark}\"\n        results.append({\n            \"Feature\": feature,\n            \"Group\": grp,\n            \"Skewness\": f\"{sk:.4f}\",\n            \"Kurtosis\": f\"{kt:.4f}\",\n            \"Remark\": remark\n        })\n\n        # Flag if any group is not approximately normal\n        if not (abs_sk <= threshold_skew_1 and abs_kt <= threshold_kurt):\n            non_normal_detected = True\n\n    # === Visual Q‚ÄìQ plots ===\n    n_groups = df[target_feature].nunique()\n    nrows = int(np.ceil(n_groups / ncols))\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols, 4.5 * nrows))\n    axes = np.array(axes).reshape(-1)\n\n    for i, grp in enumerate(df[target_feature].unique()):\n        ax = axes[i]\n        data = df.loc[df[target_feature] == grp, feature].dropna()\n        probplot(data, dist=\"norm\", plot=ax)\n        ax.set_title(f\"{feature} ‚Äî {grp}\", fontsize=11, weight=\"bold\")\n        ax.grid(alpha=0.3)\n\n    for j in range(i + 1, len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.suptitle(f\"Q‚ÄìQ Plots of {feature} by {target_feature}\", fontsize=13, weight=\"bold\", y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    # === Display results table ===\n    df_result = pd.DataFrame(results)\n    cm = sns.light_palette(\"green\", as_cmap=True)\n    styled = (\n        df_result.style\n        .background_gradient(subset=[\"Skewness\"], cmap=cm, vmin=-1, vmax=1)\n        .background_gradient(subset=[\"Kurtosis\"], cmap=cm, vmin=-1.5, vmax=1.5)\n        .set_caption(\n            f'<b><span style=\"font-size:14px; text-align:center; display:block;\">'\n            f'Skewness & Kurtosis of {feature} by {target_feature}'\n            f'</span></b>'\n        )\n        .set_table_attributes('style=\"width:80%; margin:auto;\"')\n    )\n    display(styled)\n\n    if non_normal_detected == True:\n        print(\"\\n‚ö†Ô∏è At least one group deviates from normality ‚Üí Running Kruskal‚ÄìWallis test or Mann‚ÄìWhitney U test...\")\n    else:\n        print(\"\\n‚úÖ All groups approximately follow normal distribution.\")\n\n    return non_normal_detected\n\ndef perform_anova_with_tukey(df, numeric_feature, categorical_feature, typ=2):\n    \"\"\"\n    Perform a One-Way ANOVA test to determine whether there are statistically\n    significant differences between the means of three or more independent groups.\n\n    If the ANOVA test is significant (p < 0.05), Tukey's HSD post-hoc test is performed\n    to identify which specific pairs of groups differ from each other.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input dataset containing the numeric and categorical features.\n\n    numeric_feature : str\n        The name of the numerical (continuous) response variable.\n\n    categorical_feature : str\n        The name of the categorical (independent) variable used to group the data.\n\n    typ : int, optional (default=2)\n        The type of sum of squares to use in the ANOVA test:\n        - Type I (1): Sequential.\n        - Type II (2): Default and commonly used for balanced designs.\n        - Type III (3): Use when model includes interaction terms or unbalanced data.\n\n    Returns\n    -------\n    None\n        Prints the ANOVA table, p-value, interpretation, and (if significant) the Tukey HSD test summary.\n\n    Notes\n    -----\n    - H‚ÇÄ (null hypothesis): All group means are equal.\n    - H‚ÇÅ (alternative hypothesis): At least one group mean is different.\n    - If p < 0.05 ‚Üí reject H‚ÇÄ ‚Üí perform Tukey‚Äôs HSD to find which groups differ.\n    - Assumptions:\n        1. Independence of observations\n        2. Normally distributed groups (Shapiro or Anderson test can check this)\n        3. Homogeneity of variances (Levene's test)\n\n    References\n    ----------\n    - https://www.scribbr.com/statistics/one-way-anova/\n    - https://en.wikipedia.org/wiki/Analysis_of_variance\n    - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.tukey_hsd.html\n    \"\"\"\n\n    # Extract unique groups\n    groups = df[categorical_feature].dropna().unique()\n\n    if len(groups) < 3:\n        print(f\"‚ùå Error: ANOVA requires 3 or more groups.\")\n        return\n    else:\n        print(f\"\\nüîç ANOVA Test: {numeric_feature} ~ {categorical_feature} (Type {typ})\")\n\n        # Fit OLS model\n        model = ols(f\"{numeric_feature} ~ C({categorical_feature})\", data=df).fit()\n\n        # Perform ANOVA\n        anova_table = anova_lm(model, typ=typ)\n        print(\"\\nüìä ANOVA Table:\")\n        print(anova_table)\n\n        # Extract p-value\n        p_value = anova_table[\"PR(>F)\"].iloc[0]\n\n        if p_value < 0.05:\n            print(\"\\n‚úÖ Significant difference found (p < 0.05)\")\n            print(\"‚û°Ô∏è Performing Tukey's HSD post-hoc test:\")\n\n            tukey = pairwise_tukeyhsd(df[numeric_feature], df[categorical_feature])\n            print(tukey.summary())\n        else:\n            print(\"\\n‚ÑπÔ∏è No significant difference found (p >= 0.05)\")\n\ndef perform_welch_anova(df, numeric_feature, categorical_feature):\n    \"\"\"\n    Perform Welch‚Äôs ANOVA test to compare group means when the assumption of equal variances\n    is violated but normality approximately holds.\n\n    This version of ANOVA adjusts for unequal variances and sample sizes across groups.\n    If the Welch‚Äôs ANOVA is significant (p < 0.05), a Games‚ÄìHowell post-hoc test is performed\n    to identify which specific group pairs differ significantly.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input dataset containing both numeric and categorical variables.\n\n    numeric_feature : str\n        The name of the continuous (dependent) variable.\n\n    categorical_feature : str\n        The name of the categorical (independent) variable representing group membership.\n\n    Returns\n    -------\n    None\n        Prints Welch‚Äôs ANOVA summary, p-value interpretation, and Games‚ÄìHowell post-hoc results.\n\n    Notes\n    -----\n    - H‚ÇÄ (null hypothesis): All group means are equal.\n    - H‚ÇÅ (alternative hypothesis): At least one group mean differs.\n    - If p < 0.05 ‚Üí reject H‚ÇÄ ‚Üí perform Games‚ÄìHowell test.\n    - Assumptions:\n        1. Groups are independent.\n        2. Data within each group are approximately normal.\n        3. Variances are not necessarily equal (heteroscedasticity allowed).\n\n    Key Differences vs Classical ANOVA\n    ----------------------------------\n    - Welch‚Äôs ANOVA does **not assume equal variances**.\n    - More robust when sample sizes and variances differ across groups.\n    - Use **Games‚ÄìHowell post-hoc test** instead of Tukey HSD.\n\n    References\n    ----------\n    - Welch, B. L. (1951). \"On the comparison of several mean values: an alternative approach.\"\n      Biometrika, 38(3/4), 330‚Äì336.\n    - Games, P. A., & Howell, J. F. (1976). \"Pairwise multiple comparison procedures with unequal N‚Äôs and/or variances.\"\n      Journal of Educational Statistics, 1(2), 113‚Äì125.\n    \"\"\"\n\n    # Drop NaN rows\n    df = df[[numeric_feature, categorical_feature]].dropna()\n\n    # Extract group values\n    groups = [df.loc[df[categorical_feature] == g, numeric_feature] for g in df[categorical_feature].unique()]\n\n    if len(groups) < 3:\n        print(\"‚ùå Error: Welch‚Äôs ANOVA requires 3 or more groups.\")\n        return\n\n    print(f\"\\nüîç Welch‚Äôs ANOVA Test: {numeric_feature} ~ {categorical_feature}\")\n    print(\"Testing mean differences under heteroscedasticity assumption...\")\n\n    # Perform Welch's ANOVA (scipy.stats)\n    welch_result = stats.f_oneway(*groups)\n    print(\"\\nWelch‚Äôs ANOVA Result:\")\n    print(f\"F-statistic = {welch_result.statistic:.4f},  p-value = {welch_result.pvalue:.6f}\")\n\n    # Interpret result\n    if welch_result.pvalue < 0.05:\n        print(\"\\n‚úÖ Significant difference found (p < 0.05)\")\n        print(\"‚û°Ô∏è Performing Games‚ÄìHowell post-hoc test:\\n\")\n\n        # Perform Games‚ÄìHowell post-hoc test (robust for unequal variances)\n        gh_result = pg.pairwise_gameshowell(dv=numeric_feature, between=categorical_feature, data=df)\n        # display(gh_result)\n\n        display(HTML(\"<b>Games‚ÄìHowell Post-hoc Test (adjusted p-values)</b>\"))\n        display(gh_result.style.background_gradient(cmap=cm).format(precision=4).set_table_attributes('style=\"width:80%; margin:auto;\"'))\n    else:\n        print(\"\\n‚ÑπÔ∏è No significant difference found (p ‚â• 0.05)\")\n\n\ndef check_homogeneity_of_variance(df, feature, target_feature, alpha=0.05, ratio_threshold=2.0):\n    \"\"\"\n    Check homogeneity of variances across groups using Levene‚Äôs test (median-centered).\n    Also computes variance ratios and provides practical interpretation.\n\n    ---\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input dataset containing numeric and categorical features.\n    feature : str\n        Numeric variable to test (e.g. \"Temparature\").\n    target_feature : str\n        Categorical grouping variable (e.g. \"Fertilizer_Name\").\n    alpha : float, default = 0.05\n        Significance level for hypothesis testing.\n    ratio_threshold : float, default = 2.0\n        Threshold for maximum acceptable variance ratio (max(var)/min(var)).\n        If ratio > threshold ‚Üí indicates heteroscedasticity in practice.\n\n    ---\n    Returns\n    -------\n    dict\n        Dictionary with test statistic, p-value, variance ratio, and recommendation.\n\n    ---\n    Interpretation Logic\n    ---------------------\n    Step 1: Statistical Test\n        - H‚ÇÄ: All group variances are equal.\n        - H‚ÇÅ: At least one group has a different variance.\n        - Levene‚Äôs Test (center='median') is robust to non-normality.\n\n    Step 2: Practical Variance Ratio\n        - ratio = max(var_i) / min(var_i)\n        - < 2 ‚Üí practically equal\n        - 2‚Äì4 ‚Üí moderate difference\n        - > 4 ‚Üí strong heterogeneity\n\n    Step 3: Recommendation\n        - If p > 0.05 AND ratio < 2 ‚Üí ANOVA suitable\n        - If p < 0.05 BUT ratio < 2 ‚Üí Statistical diff, but practically negligible ‚Üí still OK for ANOVA\n        - If ratio ‚â• 2 OR p < 0.05  ‚Üí  Use Welch‚Äôs ANOVA or Kruskal‚ÄìWallis\n    \"\"\"\n\n    # Group data by category\n    groups = [df.loc[df[target_feature] == g, feature].dropna() for g in df[target_feature].unique()]\n\n    # Perform Levene‚Äôs Test (robust version)\n    stat, p = levene(*groups, center=\"mean\")\n\n    # Compute variance ratio (max/min)\n    variances = [np.var(g, ddof=1) for g in groups]\n    ratio = max(variances) / min(variances)\n    anova_use = False\n    is_homogeneous_variances = False\n    # Determine interpretation\n    if p > alpha and ratio < ratio_threshold:\n        status = \"‚úÖ Homogeneous variances.\"\n        recommendation = \"Use One-Way ANOVA or Independent Two-Sample T-Test.\"\n        is_homogeneous_variances = True\n        anova_use = True\n    elif p < alpha and ratio < ratio_threshold:\n        status = \"‚ö†Ô∏è Statistically significant difference, but practically small ‚Äî ANOVA or T-Test still acceptable.\"\n        recommendation = \"Use Welch‚Äôs ANOVA or Welch‚Äôs T-Test.\"\n        anova_use = True\n    else:\n        status = \"üö® Variances differ substantially ‚Äî Use non-parametric test.\"\n        recommendation = \"Use Kruskal‚ÄìWallis or Mann‚ÄìWhitney U test (Wilcoxon rank-sum test).\"\n\n    # Display summary table\n    summary_df = pd.DataFrame({\n        \"Metric\": [\"Levene‚Äôs Statistic\", \"p-value\", \"Max/Min Variance Ratio\"],\n        \"Value\": [f\"{stat:.4f}\", f\"{p:.6f}\", f\"{ratio:.2f}\"]\n    })\n    display(summary_df.style\n            .background_gradient(subset=[\"Value\"], cmap=\"Greens\")\n            .set_caption(\n        f'<b><span style=\"font-size:14px; text-align:center; display:block;\">'\n        f'Homogeneity of Variance ‚Äî {feature} by {target_feature}</span></b>'\n    ).set_table_attributes('style=\"width:70%; margin:auto;\"'))\n\n    # Print interpretation\n    print(\"\\nüîç Interpretation:\")\n    print(f\"   {status}\")\n    print(f\"   Recommendation ‚Üí {recommendation}\")\n\n    return anova_use, is_homogeneous_variances\n\ndef cal_mannwhitneyu(dataframe, categorical_feature, num_feature):\n    \"\"\"\n    Perform the Mann‚ÄìWhitney U test (Wilcoxon rank-sum test) to assess whether there \n    is a statistically significant difference in the distribution of a numerical feature \n    between two independent groups defined by a binary categorical feature.\n\n    The function also compares medians, calculates the effect size (r), provides interpretation,\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        The input DataFrame containing the data.\n\n    categorical_feature : str\n        Column name of the categorical feature (must contain exactly 2 unique values).\n\n    num_feature : str\n        Column name of the numerical feature to compare.\n\n    Returns\n    -------\n    None\n        Prints the U statistic, p-value, medians, Z-score, effect size r, and interpretation.\n\n    Notes\n    -----\n    - H‚ÇÄ (Null Hypothesis): The two groups have the same distribution.\n    - H‚ÇÅ (Alternative Hypothesis): The distributions are different.\n    - If p ‚â§ 0.05 ‚Üí reject H‚ÇÄ ‚Üí significant difference.\n    - Effect size r helps interpret how strong the difference is:\n        * Small ~0.1, Medium ~0.3, Large ‚â•0.5\n    \"\"\"\n\n    groups = dataframe[categorical_feature].dropna().unique()\n\n    if len(groups) != 2:\n        print(f\"‚ùå Error: Mann-Whitney U test requires exactly 2 groups, but found {len(groups)}.\")\n        return\n\n    print(f\"üîç Mann‚ÄìWhitney U Test for '{num_feature}' by '{categorical_feature}'\\n\")\n\n    group1 = dataframe[dataframe[categorical_feature] == groups[0]][num_feature].dropna()\n    group2 = dataframe[dataframe[categorical_feature] == groups[1]][num_feature].dropna()\n\n    stat, p = mannwhitneyu(group1, group2, alternative=\"two-sided\")\n\n    print(f\"U statistic : {stat}\")\n    print(f\"p-value     : {p}\")\n\n    # Interpretation\n    if p <= 0.05:\n        print(\"\\n‚úÖ Result: Statistically significant difference between the two groups (Reject H‚ÇÄ).\")\n        median1 = group1.median()\n        median2 = group2.median()\n        if median1 > median2:\n            print(f\" Interpretation: Group '{groups[0]}' has a higher median '{num_feature}' than Group '{groups[1]}'.\")\n        elif median1 < median2:\n            print(f\" Interpretation: Group '{groups[1]}' has a higher median '{num_feature}' than Group '{groups[0]}'.\")\n        else:\n            print(\" Interpretation: The medians are equal, but distributions may still differ.\")\n    else:\n        print(\"\\n‚ö™ Result: No statistically significant difference between the two groups (Fail to reject H‚ÇÄ).\")\n\ndef t_test_with_cohens_d(data, categorical_feature, num_feature, equal_var=False):\n    \"\"\"\n    Perform an Independent Two-Sample T-Test and compute Cohen's d to evaluate \n    the difference between two independent groups on a numeric variable.\n\n    Supports both:\n    - Student‚Äôs T-Test (equal variances)\n    - Welch‚Äôs T-Test (unequal variances, default)\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The input DataFrame containing the categorical and numerical features.\n\n    categorical_feature : str\n        The name of the categorical column used to define the two groups (must have exactly 2 unique values).\n\n    num_feature : str\n        The name of the numerical feature to compare between the two groups.\n\n    equal_var : bool, optional (default=False)\n        If True ‚Üí Student‚Äôs t-test (equal variances).\n        If False ‚Üí Welch‚Äôs t-test (unequal variances).\n\n    Returns\n    -------\n    None\n        Prints the t-statistic, p-value, Cohen‚Äôs d, and interpretation of the effect size.\n\n    Notes\n    -----\n    - H‚ÇÄ (null hypothesis): The two groups have equal means.\n    - H‚ÇÅ (alternative): The group means differ significantly.\n    - Cohen's d interpretation:\n        - 0.2 ‚Üí small effect\n        - 0.5 ‚Üí medium effect\n        - 0.8+ ‚Üí large effect\n    - Welch‚Äôs t-test is recommended when group variances are unequal (default setting).\n\n    References\n    ----------\n    - https://www.scribbr.com/statistics/t-test/\n    - https://en.wikipedia.org/wiki/Welch%27s_t-test\n    - https://en.wikipedia.org/wiki/Cohen%27s_d\n    \"\"\"\n\n    # Extract unique groups\n    groups = data[categorical_feature].dropna().unique()\n\n    if len(groups) != 2:\n        print(f\"‚ùå Error: Independent T-Test requires exactly 2 groups.\")\n        return\n\n    print(f\"üîç Independent Two-Sample T-Test: {num_feature} ~ {categorical_feature}\")\n    print(f\"‚Üí Test Type: {'Student‚Äôs T-Test (equal variances)' if equal_var else 'Welch‚Äôs T-Test (unequal variances)'}\")\n\n    # Extract values\n    x1 = data[data[categorical_feature] == groups[0]][num_feature].dropna()\n    x2 = data[data[categorical_feature] == groups[1]][num_feature].dropna()\n\n    # Run T-Test\n    t_stat, p_value = ttest_ind(x1, x2, equal_var=equal_var)\n\n    # Calculate Cohen's d (different formulas depending on variance assumption)\n    nx1, nx2 = len(x1), len(x2)\n    s1, s2 = np.var(x1, ddof=1), np.var(x2, ddof=1)\n\n    if equal_var:\n        # --- Student‚Äôs T-Test version (pooled variance)\n        pooled_std = np.sqrt(((nx1 - 1) * s1 + (nx2 - 1) * s2) / (nx1 + nx2 - 2))\n        cohens_d = (np.mean(x1) - np.mean(x2)) / pooled_std\n    else:\n        # --- Welch‚Äôs T-Test version (average variance)\n        s_pooled = np.sqrt((s1 + s2) / 2)\n        cohens_d = (np.mean(x1) - np.mean(x2)) / s_pooled\n\n    # Output\n    print(f\"\\nComparing groups: '{groups[0]}' vs. '{groups[1]}'\")\n    print(f\"t-statistic: {t_stat:.3f}\")\n    print(f\"p-value: {p_value:.6f}\")\n    print(f\"Cohen's d: {cohens_d:.3f}\")\n\n    # Significance interpretation\n    if p_value < 0.05:\n        print(\"\\n‚úÖ Significant difference found (p < 0.05)\")\n    else:\n        print(\"\\n‚ÑπÔ∏è No significant difference found (p ‚â• 0.05)\")\n\n    # Effect size interpretation\n    if abs(cohens_d) < 0.2:\n        size = \"small\"\n    elif abs(cohens_d) < 0.5:\n        size = \"medium\"\n    else:\n        size = \"large\"\n\n    print(f\"Effect size interpretation: {size} effect ({abs(cohens_d)})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:37.152334Z","iopub.execute_input":"2025-11-01T15:23:37.15259Z","iopub.status.idle":"2025-11-01T15:23:37.192784Z","shell.execute_reply.started":"2025-11-01T15:23:37.152573Z","shell.execute_reply":"2025-11-01T15:23:37.192251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loan Distribution","metadata":{}},{"cell_type":"code","source":"df_train[\"loan_paid_back\"] = df_train[\"loan_paid_back\"].map({0: \"Not paid\", 1: \"Paid\"})\n\n# Prepare data and colors\nstatus_counts = df_train[\"loan_paid_back\"].value_counts().sort_index()\norder = status_counts.index.tolist()\ncolors = color(n_colors=len(order), tone=\"RdYlGn\")\npalette = dict(zip(order, colors))\n\n# Create subplots\nfig, ax = plt.subplots(1, 2, figsize=(15, 6))\n\n# --- Pie chart ---\nax[0].pie(status_counts, labels=order, colors=colors, autopct=\"%1.2f%%\", startangle=150, shadow=True)\nax[0].set_title(\"Proportion of Paid vs Not Paid Loans\", fontweight=\"bold\", fontsize=14, pad=20)\n\n# --- Count plot ---\nsns.countplot(data=df_train, x=\"loan_paid_back\", order=order, palette=palette, ax=ax[1])\nax[1].set_title(\"Count of Paid vs Not Paid Loans\", fontweight=\"bold\", fontsize=14, pad=20)\nfor container in ax[1].containers:\n    ax[1].bar_label(container, fmt=\"%d\", label_type=\"edge\", fontsize=10)\nax[1].set(xlabel=\"Loan Status\", ylabel=\"Frequency\")\nsns.despine(ax=ax[1])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:37.193662Z","iopub.execute_input":"2025-11-01T15:23:37.193903Z","iopub.status.idle":"2025-11-01T15:23:37.742189Z","shell.execute_reply.started":"2025-11-01T15:23:37.193883Z","shell.execute_reply":"2025-11-01T15:23:37.741526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Loan Repayment Status**\n* The majority of borrowers have fully repaid their loans ‚Äî the *Paid* class accounts for nearly 80%.\n**Details:**\n* **Repayment Rate:**\n  * *Paid*: 79.88%\n  * *Not paid*: 20.12%\n* **Count:**\n  * *Paid*: 474,494 loans\n  * *Not paid*: 115,000 loans\n**Interpretation:**\n* The dataset shows a **class imbalance** between repayment statuses.","metadata":{}},{"cell_type":"markdown","source":"## Numerical Feature Distributions","metadata":{}},{"cell_type":"code","source":"def plot_numerical_features(df_train, df_test, num_features):\n    colors = color(n_colors=2, tone=\"RdYlGn\")\n    n = len(num_features)\n\n    fig, axes = plt.subplots(n, 2, figsize=(12, n * 4))\n    axes = np.array(axes).reshape(n, 2)\n\n    for i, feature in enumerate(num_features):\n        sns.histplot(data=df_train[feature], color=colors[0], bins=20, kde=True, ax=axes[i, 0], label=\"Train data\")\n        sns.histplot(data=df_test[feature], color=colors[1], bins=20, kde=True, ax=axes[i, 0], label=\"Test data\")\n        axes[i, 0].set_title(f\"Histogram of {feature}\", pad=15, weight=\"bold\", fontsize=14)\n        axes[i, 0].legend()\n        axes[i, 0].set_ylabel(\"\")\n        sns.despine(left=False, bottom=False, ax=axes[i, 0])\n\n        df_plot = pd.concat([\n            pd.DataFrame({\"Dataset\": \"Train data\", feature: df_train[feature]}),\n            pd.DataFrame({\"Dataset\": \"Test data\", feature: df_test[feature]})\n        ]).reset_index(drop=True)\n\n        sns.boxplot(data=df_plot, x=feature, y=\"Dataset\", palette=colors, orient=\"h\", ax=axes[i, 1])\n        axes[i, 1].set_title(f\"Horizontal Box plot of {feature}\", pad=15, weight=\"bold\", fontsize=14)\n        sns.despine(left=False, bottom=False, ax=axes[i, 1])\n\n    plt.tight_layout()\n    plt.show()\n\nplot_numerical_features(df_train = df_train, df_test = df_test, num_features=num_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:37.743064Z","iopub.execute_input":"2025-11-01T15:23:37.743312Z","iopub.status.idle":"2025-11-01T15:23:57.794091Z","shell.execute_reply.started":"2025-11-01T15:23:37.743295Z","shell.execute_reply":"2025-11-01T15:23:57.793365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_skewness(data, dataset_name, numerical_features = num_features, highlight=True, sort=True):\n    skewness_dict = {}\n    skew_feature = []\n    for feature in numerical_features:\n        skew = data[feature].skew(skipna=True)\n        skewness_dict[feature] = skew\n\n    skew_df = pd.DataFrame.from_dict(skewness_dict, orient=\"index\", columns=[\"Skewness\"])\n    if sort:\n        skew_df = skew_df.reindex(skew_df[\"Skewness\"].abs().sort_values(ascending=False).index)\n    else:\n        pass\n\n    print(f\"\\nüîç Skewness for {dataset_name}:\")\n    print(\"-\"*70)\n    print(f\"{'Feature':<30} | {'Skewness':<9} | {'Remark'}\")\n    print(\"-\"*70)\n    for feature, row in skew_df.iterrows():\n        skew = row[\"Skewness\"]\n        abs_skew = abs(skew)\n        if abs_skew > 1:\n            remark = \"Highly skewed\"\n            color = \"\\033[91m\"\n        elif abs_skew > 0.5:\n            remark = \"Moderately skewed\"\n            color = \"\\033[93m\"\n        else:\n            remark = \"Approximately symmetric\"\n            color = \"\"\n        endc = \"\\033[0m\" if color else \"\"\n        if highlight and color:\n            print(f\"{color}{feature:<30} | {skew:>+9.4f} | {remark}{endc}\")\n            skew_feature.append(feature)\n        else:\n            print(f\"{feature:<30} | {skew:>+9.4f} | {remark}\")\n    print(\"-\"*70)\n    return skew_feature, skew_df\n\nskew_feature_train, skew_train_df = check_skewness(df_train, \"Train Data\")\nskew_feature_test, skew_test_df = check_skewness(df_test, \"Test Data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:57.79507Z","iopub.execute_input":"2025-11-01T15:23:57.795469Z","iopub.status.idle":"2025-11-01T15:23:57.849229Z","shell.execute_reply.started":"2025-11-01T15:23:57.795443Z","shell.execute_reply":"2025-11-01T15:23:57.848498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insights on Numerical Feature Distributions\n\n* Includes **6 numerical features**: `annual_income`, `debt_to_income_ratio`, `loan_amount`, `credit_score`, `interest_rate`.\n* **Distributions are highly consistent across Train, Test datasets**, with no visible shifts or irregularities.\n* **Histograms** show mostly **uniform or mildly oscillating patterns**, indicating stable value ranges without strong peaks.\n* **Boxplots** confirm that the **median and interquartile ranges align closely** among all datasets ‚Äî no apparent outliers.\n* `annual_income` and `debt_to_income_ratio` is slightly **right-skewed**, meaning **log transformation or power transformation** should be applied to highly skewed features to improve **normality and model performance**.","metadata":{}},{"cell_type":"markdown","source":"## Correlation Analysis of Numerical Features","metadata":{}},{"cell_type":"code","source":"def plot_correlation(df_train, df_test, train_name=\"Train Data\", test_name=\"Test Data\", figsize=(24, 10)):\n    corr_train = df_train.corr(numeric_only=True)\n    corr_test = df_test.corr(numeric_only=True)\n\n    mask_train = np.triu(np.ones_like(corr_train, dtype=bool))\n    adjusted_mask_train = mask_train[1:, :-1]\n    adjusted_cereal_corr_train = corr_train.iloc[1:, :-1]\n\n    mask_test = np.triu(np.ones_like(corr_test, dtype=bool))\n    adjusted_mask_test = mask_test[1:, :-1]\n    adjusted_cereal_corr_test = corr_test.iloc[1:, :-1]\n\n    cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n    fig, ax = plt.subplots(1, 2, figsize=figsize)\n\n    sns.heatmap(data=adjusted_cereal_corr_train, mask=adjusted_mask_train,\n                annot=True, fmt=\".2f\", cmap=cmap,\n                vmin=-1, vmax=1, linecolor=\"white\", linewidths=0.5, ax=ax[0])\n    ax[0].set_title(f\"Correlation Heatmap of {train_name}\", fontsize=16, weight=\"bold\", loc=\"center\", pad=15)\n\n    sns.heatmap(data=adjusted_cereal_corr_test, mask=adjusted_mask_test,\n                annot=True, fmt=\".2f\", cmap=cmap,\n                vmin=-1, vmax=1, linecolor=\"white\", linewidths=0.5, ax=ax[1])\n    ax[1].set_title(f\"Correlation Heatmap of {test_name}\", fontsize=16, weight=\"bold\", loc=\"center\", pad=15)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_correlation(df_train=df_train.drop(columns=\"loan_paid_back\", axis=1),\n                 df_test=df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:57.850077Z","iopub.execute_input":"2025-11-01T15:23:57.850412Z","iopub.status.idle":"2025-11-01T15:23:58.655096Z","shell.execute_reply.started":"2025-11-01T15:23:57.850395Z","shell.execute_reply":"2025-11-01T15:23:58.654282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Correlation Matrix of Financial Variables**\n\n**Overview:**\n\n* Most variables show very low correlations with each other ‚Üí minimal risk of multicollinearity.\n* The most notable relationship is between *credit_score* and *interest_rate*.\n\n**Key Highlights:**\n\n* **interest_rate ‚Üî credit_score**: Strong negative correlation **(-0.54)** ‚Üí the higher the credit score, the lower the interest rate.\n* Other variable pairs have correlation coefficients close to **0**, indicating no clear linear relationship.","metadata":{}},{"cell_type":"markdown","source":"## Categorical Feature Distributions","metadata":{}},{"cell_type":"code","source":"def map_grade_category(grade):\n    if isinstance(grade, str):\n        if grade.startswith((\"A\", \"B\")):\n            return \"High\"\n        elif grade.startswith((\"C\", \"D\")):\n            return \"Medium\"\n        elif grade.startswith((\"E\", \"F\")):\n            return \"Low\"\n    return \"Unknown\"\n\ndf_train[\"grade_category\"] = df_train[\"grade_subgrade\"].map(map_grade_category)\ndf_test[\"grade_category\"] = df_test[\"grade_subgrade\"].map(map_grade_category)\ndf_train.drop(columns=\"grade_subgrade\", axis=1, inplace=True)\ndf_test.drop(columns=\"grade_subgrade\", axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:58.655925Z","iopub.execute_input":"2025-11-01T15:23:58.656175Z","iopub.status.idle":"2025-11-01T15:23:58.697231Z","shell.execute_reply.started":"2025-11-01T15:23:58.656158Z","shell.execute_reply":"2025-11-01T15:23:58.696597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_categorical_distribution_across_datasets(train_data, test_data, feature, tone=\"RdYlGn\"):\n    \"\"\"\n    Compare categorical feature distributions between Train and Test datasets.\n    Creates side-by-side bar charts and donut (pie) charts for visual comparison.\n    \"\"\"\n\n    # ----- Gather all unique categories from both datasets -----\n    combined_categories = (\n        pd.concat([train_data[feature], test_data[feature]])\n        .dropna()\n        .astype(str)\n        .unique()\n    )\n    order = sorted(combined_categories)\n\n    # Convert feature to string for consistent plotting\n    tdf = train_data.copy()\n    vdf = test_data.copy()\n    tdf[feature] = tdf[feature].astype(str)\n    vdf[feature] = vdf[feature].astype(str)\n\n    # ----- Build consistent color palette -----\n    colors = color(n_colors=len(order), tone=tone)\n    palette = dict(zip(order, colors))\n\n    fig, ax = plt.subplots(2, 2, figsize=(18, 10))\n    datasets = [(tdf, \"Train\"), (vdf, \"Test\")]\n\n    # ----- Bar charts -----\n    for i, (data, name) in enumerate(datasets):\n        # Detect any missing categories in palette and add new colors dynamically\n        unique_vals = set(data[feature].unique())\n        missing_keys = unique_vals - set(palette.keys())\n        if missing_keys:\n            extra_colors = color(n_colors=len(missing_keys), tone=tone)\n            palette.update(dict(zip(missing_keys, extra_colors)))\n\n        sns.countplot(data=data, x=feature,  palette=palette, ax=ax[i, 0], order=order)\n        ax[i, 0].set_title(f\"{name} Data: {feature} Counts\", fontsize=12, pad=15, weight=\"bold\")\n        ax[i, 0].set_xlabel(feature)\n        ax[i, 0].set_ylabel(\"Count\")\n        ax[i, 0].set_axisbelow(True)\n        sns.despine(ax=ax[i, 0])\n\n        # Annotate each bar\n        for p in ax[i, 0].patches:\n            height = int(p.get_height())\n            x = p.get_x() + p.get_width() / 2\n            y = p.get_height()\n            ax[i, 0].annotate(f\"{height}\", (x, y), ha=\"center\", va=\"bottom\", fontsize=9)\n\n    # ----- Donut (pie) charts -----\n    for i, (data, name) in enumerate(datasets):\n        counts = data[feature].value_counts().reindex(order, fill_value=0)\n        # Guarantee every label has a color (even unseen)\n        pie_colors = [palette.get(lbl, \"#cccccc\") for lbl in order]\n\n        wedges, texts, autotexts = ax[i, 1].pie(\n            counts.values,\n            labels=order,\n            autopct=\"%1.1f%%\",\n            startangle=90,\n            colors=pie_colors,\n            textprops={\"fontsize\": 11},\n            radius=1.2,\n            shadow=True,\n        )\n        centre_circle = plt.Circle((0, 0), 0.70, fc=\"white\")\n        ax[i, 1].add_artist(centre_circle)\n        ax[i, 1].set_title(f\"{name} Data: {feature} Distribution (%)\",\n                           fontsize=12, pad=15, weight=\"bold\")\n        ax[i, 1].axis(\"equal\")\n\n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.3)\n    plt.show()\n\n\n# ----- Run for all categorical features -----\ncat_features = [\"gender\", \"marital_status\", \"education_level\", \"employment_status\", \"loan_purpose\", \"grade_category\"]\nfor feature in cat_features:\n    plot_categorical_distribution_across_datasets(df_train, df_test, feature)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:23:58.697942Z","iopub.execute_input":"2025-11-01T15:23:58.698202Z","iopub.status.idle":"2025-11-01T15:24:06.68501Z","shell.execute_reply.started":"2025-11-01T15:23:58.69818Z","shell.execute_reply":"2025-11-01T15:24:06.684232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Overall Consistency in Categorical Feature Distributions**\n\n* The **count plots and percentage distribution charts** of categorical variables across the **Train and Test datasets** show a **high degree of consistency**.\n* There are **no noticeable signs of distributional shift** or **sampling bias** across datasets.","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Analysis","metadata":{}},{"cell_type":"code","source":"def perform_statical_testing(feature: str, df: pd.DataFrame = df_train,  target_feature: str = \"loan_paid_back\") -> None:\n    \"\"\"\n    Perform statistical tests (normality and Kruskal-Wallis) \n    to evaluate whether there are significant differences \n    in the distribution of a numerical feature across categories \n    of the target variable.\n\n    Args:\n        feature (str): Name of the numerical feature to be tested.\n        df (pd.DataFrame): Dataset containing both numerical and target columns.\n        target_feature (str): Name of the target categorical feature.\n\n    Returns:\n        None: Prints or displays statistical test results.\n    \"\"\"\n    # Perform normality test (e.g., Shapiro-Wilk or D‚ÄôAgostino test) for feature distribution\n    non_normal_detected = check_normality_with_plots(df=df, feature=feature, target_feature=target_feature)\n    total_categories = df[target_feature].nunique()\n    if total_categories > 2:\n        if non_normal_detected == True:\n            perform_kruskal_test(df=df, categorical_feature=target_feature,\n                                numeric_feature=feature)\n        else:\n            anove_use, is_homogeneous_variances = check_homogeneity_of_variance(df=df, feature=feature,\n                                                                                target_feature=target_feature)\n            if anove_use and is_homogeneous_variances:\n                perform_anova_with_tukey(df=df, numeric_feature=feature,\n                                        categorical_feature=target_feature)\n            elif anove_use and is_homogeneous_variances == False:\n                perform_welch_anova(df=df, numeric_feature=feature, categorical_feature=target_feature)\n            else:\n                perform_kruskal_test(df=df, categorical_feature=target_feature,\n                        numeric_feature=feature)\n    else:\n        if non_normal_detected == True:\n            cal_mannwhitneyu(dataframe=df, categorical_feature=target_feature, num_feature=feature)\n        else:\n            anove_use, is_homogeneous_variances = check_homogeneity_of_variance(df=df, feature=feature,\n                                                                                target_feature=target_feature)\n            if anove_use and is_homogeneous_variances:\n                t_test_with_cohens_d(data=df, categorical_feature=target_feature, num_feature=feature, equal_var=True)\n            elif anove_use and is_homogeneous_variances == False:\n                t_test_with_cohens_d(data=df, categorical_feature=target_feature, num_feature=feature, equal_var=False)\n            else:\n                cal_mannwhitneyu(dataframe=df, categorical_feature=target_feature, num_feature=feature)\n\ndef plot_numerical_distribution(feature: str, df: pd.DataFrame = df_train,\n                                target_feature: str = \"loan_paid_back\", order: list = None) -> None:\n    \"\"\"\n    Perform statistical testing and visualize the distribution of a numerical feature \n    across different classes of the target variable using violin plots and summary statistics.\n\n    The function executes:\n      1. Statistical tests.\n      2. Summary table with mean, median, std per category.\n      3. Violin plot for visualizing feature distributions across classes.\n\n    Args:\n        feature (str): The name of the numerical feature to analyze.\n        df (pd.DataFrame): Input dataframe containing numerical & target features.\n        target_feature (str): Target variable name (categorical feature).\n        order (list, optional): Custom ordering for category display in the plot.\n\n    Returns:\n        None: Displays statistical summaries and plots directly.\n    \"\"\"\n\n    # Compute summary statistics for each Fertilizer category\n    df_summary_feature = (\n        df.groupby(by=target_feature, as_index=False)\n        .agg(\n            Count=(feature, \"count\"),\n            Mean=(feature, \"mean\"),\n            Median=(feature, \"median\"),\n            Std=(feature, \"std\")\n        )\n        .sort_values(by=\"Mean\", ascending=False).reset_index(drop=True)\n    )\n\n    # Compute global statistics for the entire feature\n    summary_data = [\n        (\"Overall Mean\", f\"{df[feature].mean():.2f}\"),\n        (\"Overall Median\", f\"{df[feature].median()}\"),\n        (\"Overall Std\", f\"{df[feature].std():.2f}\")\n    ]\n\n    # Display overall statistics in HTML format for better notebook visualization\n    summary_html = \"<ul>\" + \"\".join([\n        f\"<li><b>{k}:</b> {v}</li>\" for k, v in summary_data\n    ]) + \"</ul>\"\n    display(HTML(summary_html))\n\n    # Display detailed summary per category as styled dataframe\n    display(\n        df_summary_feature.style.background_gradient(cmap=cm)\n        .set_table_attributes('style=\"width:75%; margin:auto;\"')\n    )\n\n    # Run statistical significance testing\n    perform_statical_testing(feature=feature, target_feature=target_feature)\n\n    # Visualize distribution via violin plot\n    plt.figure(figsize=(10, 6))\n    sns.violinplot(x=target_feature, y=feature, data=df, hue=target_feature, order=order,\n                   palette=color(n_colors=df[target_feature].nunique(), tone=\"RdYlGn\"))\n    \n    plt.title(f\"Violin plot of {feature} distribution by {target_feature}\", pad=15, weight=\"bold\")\n    plt.xlabel(target_feature, labelpad=10)\n    plt.ylabel(feature, labelpad=10)\n    plt.legend().remove()\n    sns.despine(left=False, bottom=False)\n    plt.tight_layout()\n    plt.show()\n\nfor feature in num_features:\n    display(HTML(f\"<h2 style='text-align:center; font-size:22px; color:green;'><b>Distribution of {feature} by Loan Status</b></h2>\"))\n    plot_numerical_distribution(feature=feature, df = df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:06.685819Z","iopub.execute_input":"2025-11-01T15:24:06.686079Z","iopub.status.idle":"2025-11-01T15:24:26.048661Z","shell.execute_reply.started":"2025-11-01T15:24:06.686061Z","shell.execute_reply":"2025-11-01T15:24:26.047839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insight Numerical Features vs Loan Status","metadata":{}},{"cell_type":"markdown","source":"* **`annual_income`** <br>\nBorrowers who fully repaid their loans (*Paid*) tend to have **higher median incomes**, suggesting that income level plays a meaningful role in repayment capability.\n\n* **`debt_to_income_ratio`** <br>\nThe *Not paid* group shows a **higher median debt-to-income ratio**, indicating that heavier debt burdens may increase the likelihood of missed payments.\n\n* **`credit_score`** <br>\nBorrowers in the *Paid* group have **higher average credit scores**, reinforcing that creditworthiness is a strong predictor of successful loan repayment.\n\n* **`loan_amount`** <br>\nWhile there is a statistical difference in loan amounts between the two groups, the **actual effect is negligible**, implying that loan size alone doesn‚Äôt strongly influence repayment behavior.\n\n* **`interest_rate`** <br>\nLoans that were *Not paid* tend to have **higher average interest rates**, suggesting that higher borrowing costs could contribute to default risk.","metadata":{}},{"cell_type":"code","source":"def bivariate_percent_plot(cat, target_feature, df, figsize=(15, 6), order=None):\n    display(HTML(f\"<h2 style='text-align:center; font-size:22px; color:green;'><b>Distribution of {cat} by {target_feature}</b></h2>\"))\n    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=False, figsize=figsize)\n\n    # === Data processing ===\n    grouped = df.groupby([cat, target_feature]).size().unstack(fill_value=0)\n\n    # 1) Define a fixed hue order (adjust if needed)\n    target_order = [c for c in [\"Paid\", \"Not paid\"] if c in grouped.columns]\n    # Fallback if the labels are 0/1 or have different names\n    if not target_order:\n        target_order = list(grouped.columns)\n\n    # 2) Calculate percentages row-wise and reorder columns by target_order\n    percentages = grouped.div(grouped.sum(axis=1), axis=0)[target_order] * 100\n\n    # 3) Define X-axis category order\n    if order is not None:\n        percentages = percentages.loc[order]\n        labels = order\n    else:\n        labels = percentages.index.tolist()\n\n    # === Consistent color palette for both charts ===\n    base_colors = color(n_colors=len(target_order), tone=\"RdYlGn\")\n    color_map = dict(zip(target_order, base_colors))\n\n    # === Plot 1: Stacked bar chart (percentage) ===\n    bottom = np.zeros(len(percentages))\n    for cls in target_order:\n        ax[0].bar(percentages.index, percentages[cls].values, bottom=bottom,\n                  label=cls, color=color_map[cls])\n        bottom += percentages[cls].values\n\n    # Add percentage labels\n    for container in ax[0].containers:\n        ax[0].bar_label(container, fmt=\"%1.0f%%\", label_type=\"center\",\n                        fontsize=9, color=\"black\", weight=\"bold\")\n\n    ax[0].set_title(f\"Percentage of {target_feature} by {cat}\", fontsize=14, weight=\"bold\")\n    ax[0].set_xlabel(f\"{cat}\", fontsize=12)\n    ax[0].set_ylabel(f\"% {target_feature} Rate\", fontsize=12)\n    ax[0].set_xticklabels(labels=labels, rotation=45)\n    sns.despine(left=False, bottom=False, ax=ax[0])\n    ax[0].legend().remove()\n\n    # === Plot 2: Count plot (using same color_map + hue_order) ===\n    sns.countplot(data=df, hue=target_feature, x=cat,\n                  order=labels, hue_order=target_order,\n                  palette=color_map, ax=ax[1])\n\n    for container in ax[1].containers:\n        ax[1].bar_label(container, fmt=\"%d\", label_type=\"edge\",\n                        fontsize=10, weight=\"bold\")\n\n    ax[1].set_title(f\"{target_feature} by {cat}\", fontsize=14, weight=\"bold\")\n    ax[1].set_xlabel(f\"{cat}\", fontsize=12)\n    ax[1].set_ylabel(\"Number of Customers\", fontsize=12)\n    ax[1].legend(title=target_feature, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    ax[1].set_xticklabels(labels=ax[1].get_xticklabels(), rotation=45)\n    sns.despine(left=False, bottom=False, ax=ax[1])\n\n    plt.tight_layout()\n    plt.show()\n\n    # === Chi-Square Test ===\n    cal_ChiSquare(cat_feature=cat, target_feature=target_feature, df=df, show_residuals=True)\n\n# === Run for all categorical features ===\nfor feature in cat_features:\n    bivariate_percent_plot(cat=feature, target_feature=\"loan_paid_back\", df=df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:26.049573Z","iopub.execute_input":"2025-11-01T15:24:26.049926Z","iopub.status.idle":"2025-11-01T15:24:31.148087Z","shell.execute_reply.started":"2025-11-01T15:24:26.049902Z","shell.execute_reply":"2025-11-01T15:24:31.147351Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insight for Categorical Features vs Loan Status\n\n* `gender` <br>\n**Male** borrowers show higher default tendencies, whereas **females** are more consistent in repayment.\n\n* `marital_status` <br>\nBorrowers‚Äô **marital status does not meaningfully** influence their repayment outcomes.\n\n* `education_level` <br>\nBorrowers‚Äô **education level** shows a clear link to repayment behavior ‚Äî those with a **Bachelor‚Äôs** degree are more likely to default, while **High School and PhD graduates** tend to repay more reliably.\n\n* `employment_status` <br>\nEmployment status strongly influences repayment behavior ‚Äî **Unemployed** and **Student** borrowers are more likely to default, while **Employed**, **Retired**, and **Self-employed** individuals show stronger repayment patterns.\n\n* `loan_purpose` <br>\nLoan purpose plays a clear role in repayment behavior ‚Äî borrowers taking loans for **Education** and **Medical** reasons are more likely to default, while those borrowing for Home or Business purposes tend to repay more reliably.\n\n* `grade_category` <br>\n**Credit grade is strongly linked to repayment behavioR**‚Äî borrowers with High grades are more likely to repay, while those with Low grades show higher default tendencies.","metadata":{}},{"cell_type":"markdown","source":"### Overall Picture\n\n| **Feature**              | **Summary**                                                                                                                                                                                            |\n| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **annual_income**        | Borrowers who fully repaid (*Paid*) have **higher median income**, suggesting **income level positively influences repayment ability**.                                                                |\n| **debt_to_income_ratio** | *Not paid* borrowers show **higher median debt-to-income ratios**, indicating **heavier debt burdens increase default risk**.                                                                          |\n| **credit_score**         | *Paid* borrowers have **higher average credit scores**, confirming **creditworthiness as a key driver of repayment success**.                                                                          |\n| **loan_amount**          | While average loan amounts differ slightly, the **practical effect is minimal**, suggesting **loan size alone doesn‚Äôt determine repayment behavior**.                                                  |\n| **interest_rate**        | *Not paid* borrowers face **higher average interest rates**, implying **higher borrowing costs may contribute to default risk**.                                                                       |\n| **gender**               | **Male borrowers show higher default tendencies**, while **female borrowers tend to repay more consistently**.                                                                                         |\n| **marital_status**       | **Marital status has little impact** ‚Äî repayment behavior remains **largely consistent across marital groups**.                                                                                        |\n| **education_level**      | **Education level correlates with repayment** ‚Äî *Bachelor‚Äôs* borrowers default more often, while *High School* and *PhD* groups **repay more reliably**.                                               |\n| **employment_status**    | **Employment status strongly affects repayment** ‚Äî *Unemployed* and *Student* borrowers default more, while *Employed*, *Retired*, and *Self-employed* borrowers **show stronger repayment behavior**. |\n| **loan_purpose**         | **Loan purpose clearly influences repayment** ‚Äî *Education* and *Medical* loans default more, while *Home* and *Business* loans **show higher repayment rates**.                                       |\n| **grade_category**       | **Credit grade is strongly linked to repayment** ‚Äî *High-grade* borrowers repay more reliably, while *Low-grade* borrowers **exhibit higher default tendencies**.                                      |\n","metadata":{}},{"cell_type":"markdown","source":"## Business Questions","metadata":{}},{"cell_type":"code","source":"df_bq = df_train.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:31.149027Z","iopub.execute_input":"2025-11-01T15:24:31.149263Z","iopub.status.idle":"2025-11-01T15:24:31.174083Z","shell.execute_reply.started":"2025-11-01T15:24:31.149245Z","shell.execute_reply":"2025-11-01T15:24:31.173134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Customer Segmentation & Risk Profiling","metadata":{}},{"cell_type":"markdown","source":"#### How does the combination of income level and credit score affect loan repayment behavior?","metadata":{}},{"cell_type":"code","source":"bins = [0, 40000, 80000, 150000, 400000]\nlabels = [\"Low\", \"Lower-Middle\", \"Upper-Middle\", \"High\"]\ndf_bq[\"annual_income_group\"] = pd.cut(df_bq[\"annual_income\"], bins=bins, labels=labels, include_lowest=True)\n\nbins = [395, 580, 670, 750, 850]\nlabels = [\"Low\", \"Fair\", \"Good\", \"Excellent\"]\ndf_bq[\"credit_score_group\"] = pd.cut(df_bq[\"credit_score\"], bins=bins, labels=labels, include_lowest=True)\n\ndf_bq_income_group_credit_score_group = pd.crosstab(\n    [df_bq[\"annual_income_group\"], df_bq[\"credit_score_group\"]],\n    df_bq[\"loan_paid_back\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_income_group_credit_score_group)\n\ncontingency = pd.crosstab(\n    [df_bq[\"annual_income_group\"], df_bq[\"credit_score_group\"]],\n    df_bq[\"loan_paid_back\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index,\n                            columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(10,6))\nsns.heatmap(\n    std_resid_df,\n    annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0,\n    cbar_kws={\"label\": \"Standardized Residual\"}\n)\nplt.title(\"Standardized Residuals Heatmap: Income Group or Credit Score Group vs Loan Status\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Income Group or Credit Score Group | Credit Score\")\nplt.xlabel(\"Loan Status\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:31.175109Z","iopub.execute_input":"2025-11-01T15:24:31.175378Z","iopub.status.idle":"2025-11-01T15:24:31.675519Z","shell.execute_reply.started":"2025-11-01T15:24:31.175359Z","shell.execute_reply":"2025-11-01T15:24:31.674797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings:**\n\n* A **strong association** exists between the *income‚Äìcredit score combination* and *repayment behavior*.\n* Borrowers with **low income but high credit scores** tend to **repay more reliably**,\n  while **high-income but low-credit borrowers** show **higher default risk**.","metadata":{}},{"cell_type":"markdown","source":"#### Are certain combinations of debt-to-income ratio and interest rate driving higher default rates?","metadata":{}},{"cell_type":"code","source":"bins_dti = [0, 0.15, 0.30, 0.45, 1]\nlabels_dti = [\"Low\", \"Medium\", \"High\", \"Very High\"]\ndf_bq[\"DTI_group\"] = pd.cut(df_bq[\"debt_to_income_ratio\"], bins=bins_dti, labels=labels_dti, include_lowest=True)\n\nbins_rate = [3.2, 8.0, 12.0, 16.0, 21.0]\nlabels_rate = [\"Low\", \"Moderate\", \"High\", \"Very High\"]\ndf_bq[\"Rate_group\"] = pd.cut(df_bq[\"interest_rate\"], bins=bins_rate, labels=labels_rate, include_lowest=True)\n\ndf_bq[\"DTI_Rate_combo\"] = df_bq[\"DTI_group\"].astype(str) + \" | \" + df_bq[\"Rate_group\"].astype(str)\n\ncombo_summary = (df_bq.groupby(\"DTI_Rate_combo\")[\"loan_paid_back\"].value_counts(normalize=True).unstack(fill_value=0)* 100)\n\nplt.figure(figsize=(10,6))\nsns.heatmap(\n    combo_summary[[\"Not paid\"]],\n    annot=True, fmt=\".1f\", cmap=\"Reds\", cbar_kws={'label': '% Default'},\n)\nplt.title(\"Default Rate (%) by DTI and Interest Rate Combination\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"\")\nplt.ylabel(\"DTI_Rate_combo\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:31.679667Z","iopub.execute_input":"2025-11-01T15:24:31.67988Z","iopub.status.idle":"2025-11-01T15:24:32.33999Z","shell.execute_reply.started":"2025-11-01T15:24:31.679865Z","shell.execute_reply":"2025-11-01T15:24:32.339226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* **Default risk rises sharply when either DTI or interest rate increases.**\n* Borrowers with **High DTI (‚â•0.30)** and **Moderate+ interest rates (>8%)** show **a 70%+ default rate**, marking the **critical risk zone**.","metadata":{}},{"cell_type":"markdown","source":"#### Which employment and education segments represent the lowest credit risk?","metadata":{}},{"cell_type":"code","source":"df_bq_education_employment = pd.crosstab(\n    [df_bq[\"education_level\"], df_bq[\"employment_status\"]],\n    df_bq[\"loan_paid_back\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_education_employment)\n\ncontingency = pd.crosstab(\n    [df_bq[\"education_level\"], df_bq[\"employment_status\"]],\n    df_bq[\"loan_paid_back\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(11,7))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Education or Employment vs Loan Status\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Education or Employment | Loan Status\")\nplt.xlabel(\"Loan Status\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:32.340674Z","iopub.execute_input":"2025-11-01T15:24:32.340866Z","iopub.status.idle":"2025-11-01T15:24:32.918595Z","shell.execute_reply.started":"2025-11-01T15:24:32.340852Z","shell.execute_reply":"2025-11-01T15:24:32.917764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* **Borrowers who are employed and have at least a High School education show the lowest credit risk.**\n* The **Bachelor‚Äôs‚ÄìEmployed** group performs best, with **higher-than-expected repayment** and **significantly lower default residuals.**\n* This segment represents the **most reliable customer profile** for loan approval and retention strategies.","metadata":{}},{"cell_type":"markdown","source":"### Loan Product Optimization","metadata":{}},{"cell_type":"markdown","source":"#### How does loan purpose interact with interest rate to influence repayment?","metadata":{}},{"cell_type":"code","source":"df_bq_loan_purpose_rate_group = pd.crosstab(\n    [df_bq[\"loan_purpose\"], df_bq[\"Rate_group\"]],\n    df_bq[\"loan_paid_back\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_loan_purpose_rate_group)\n\ncontingency = pd.crosstab(\n    [df_bq[\"loan_purpose\"], df_bq[\"Rate_group\"]],\n    df_bq[\"loan_paid_back\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(11,8))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Loan Purpose or Rate Group vs Loan Status\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Loan Purpose or Rate Group | Loan Status\")\nplt.xlabel(\"Loan Status\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:32.919422Z","iopub.execute_input":"2025-11-01T15:24:32.919631Z","iopub.status.idle":"2025-11-01T15:24:33.745428Z","shell.execute_reply.started":"2025-11-01T15:24:32.919616Z","shell.execute_reply":"2025-11-01T15:24:33.744715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings:**\n\n* **Borrowers using loans for education or debt consolidation become significantly riskier as interest rates rise ‚Äî interest rate amplifies default risk.**\n* In contrast, **business and home loan borrowers maintain strong repayment behavior at low‚Äìmoderate rates.**\n* Pricing and risk policy should tighten for **rate-sensitive purposes** like Education & Debt Consolidation.","metadata":{}},{"cell_type":"markdown","source":"### Interest Rate Strategy & Credit Policy","metadata":{}},{"cell_type":"markdown","source":"#### Is the current interest rate structure fair across different credit grades?","metadata":{}},{"cell_type":"code","source":"df_bq_rate_credit_score_group = pd.crosstab(\n    [df_bq[\"Rate_group\"], df_bq[\"credit_score_group\"]],\n    df_bq[\"loan_paid_back\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_rate_credit_score_group)\n\ncontingency = pd.crosstab(\n    [df_bq[\"Rate_group\"], df_bq[\"credit_score_group\"]],\n    df_bq[\"loan_paid_back\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(11,8))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Rate Group or Credit Score Group vs Loan Status\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Rate Group or Credit Score Group | Loan Status\")\nplt.xlabel(\"Loan Status\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:33.746295Z","iopub.execute_input":"2025-11-01T15:24:33.74655Z","iopub.status.idle":"2025-11-01T15:24:34.239165Z","shell.execute_reply.started":"2025-11-01T15:24:33.746531Z","shell.execute_reply":"2025-11-01T15:24:34.238428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* **Some borrower segments are over-charged relative to their true risk** ‚Äî e.g., *moderate-rate borrowers with good credit* show stronger-than-expected repayment.\n* Meanwhile, **high-rate borrowers with only fair credit exhibit sharply higher default risk**, suggesting pricing is aligned for that group.","metadata":{}},{"cell_type":"markdown","source":"#### Would adjusting interest rates for borrowers with high DTI but good credit scores improve repayment performance?","metadata":{}},{"cell_type":"code","source":"df_bq_rate_dti_credit_group = pd.crosstab(\n    [df_bq[\"Rate_group\"], df_bq[\"DTI_group\"], df_bq[\"credit_score_group\"]],\n    df_bq[\"loan_paid_back\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_rate_dti_credit_group)\n\ncontingency = pd.crosstab(\n    [df_bq[\"Rate_group\"], df_bq[\"DTI_group\"], df_bq[\"credit_score_group\"]],\n    df_bq[\"loan_paid_back\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(15,15))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Rate Group or DTI Group or Credit Score Group vs Loan Status\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Rate Group or DTI Group or Credit Score Group | Loan Status\")\nplt.xlabel(\"Loan Status\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:34.240047Z","iopub.execute_input":"2025-11-01T15:24:34.24028Z","iopub.status.idle":"2025-11-01T15:24:35.303878Z","shell.execute_reply.started":"2025-11-01T15:24:34.240263Z","shell.execute_reply":"2025-11-01T15:24:35.303239Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* Borrowers with **high DTI but strong credit scores** who are charged **high interest rates** show **higher-than-expected default rates**\n* However, similar borrowers placed in **lower or moderate interest-rate tiers** demonstrate **better-than-expected repayment performance**","metadata":{}},{"cell_type":"markdown","source":"### Behavioral & Demographic Analysis","metadata":{}},{"cell_type":"markdown","source":"#### Do gender and marital status jointly influence repayment patterns?","metadata":{}},{"cell_type":"code","source":"df_bq_gender_marital_status = pd.crosstab(\n    [df_bq[\"gender\"], df_bq[\"marital_status\"]],\n    df_bq[\"loan_paid_back\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_gender_marital_status)\n\ncontingency = pd.crosstab(\n    [df_bq[\"gender\"], df_bq[\"marital_status\"]],\n    df_bq[\"loan_paid_back\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(11,8))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Gender or Marital Status vs Loan Status\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Gender or Marital Status | Loan Status\")\nplt.xlabel(\"Loan Status\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:35.304855Z","iopub.execute_input":"2025-11-01T15:24:35.305107Z","iopub.status.idle":"2025-11-01T15:24:35.743285Z","shell.execute_reply.started":"2025-11-01T15:24:35.305088Z","shell.execute_reply":"2025-11-01T15:24:35.74249Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* **Repayment behavior varies across gender √ó marital status segments.**\n* Married women and single women show **stronger repayment discipline**, while married and single men show **elevated default tendencies**.","metadata":{}},{"cell_type":"markdown","source":"#### How do credit grades differ across education and employment levels?","metadata":{}},{"cell_type":"code","source":"df_bq_rate_dti_credit_group = pd.crosstab(\n    [df_bq[\"education_level\"], df_bq[\"employment_status\"]],\n    df_bq[\"grade_category\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_rate_dti_credit_group)\n\ncontingency = pd.crosstab(\n    [df_bq[\"education_level\"], df_bq[\"employment_status\"]],\n    df_bq[\"grade_category\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(15,15))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Education or Employment vs Grade Category\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Education or Employment | Grade Category\")\nplt.xlabel(\"Grade Category\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:35.744231Z","iopub.execute_input":"2025-11-01T15:24:35.744464Z","iopub.status.idle":"2025-11-01T15:24:36.42846Z","shell.execute_reply.started":"2025-11-01T15:24:35.744448Z","shell.execute_reply":"2025-11-01T15:24:36.427842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* **Higher education + employment ‚Üí higher likelihood of strong credit grades.**\n* **Unemployment ‚Üí significantly weaker credit performance, regardless of education level.**\n* **Employment status is a stronger differentiator of credit quality than education.**","metadata":{}},{"cell_type":"markdown","source":"### Actionable Insights for Risk & Marketing Teams","metadata":{}},{"cell_type":"markdown","source":"#### Which customer segments (by education, income, and purpose) are most responsive to loan offers with moderate interest rates?","metadata":{}},{"cell_type":"code","source":"df_bq_group = pd.crosstab(\n    [df_bq[\"education_level\"], df_bq[\"annual_income_group\"], df_bq[\"loan_purpose\"]],\n    df_bq[\"Rate_group\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_group)\n\ncontingency = pd.crosstab(\n    [df_bq[\"education_level\"], df_bq[\"annual_income_group\"], df_bq[\"loan_purpose\"]],\n    df_bq[\"Rate_group\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(15,35))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Education or Annual Income or Loan Purpose vs Rate Group\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Education or Annual Income or Loan Purpose | Rate Group\")\nplt.xlabel(\"Rate Group\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:36.429579Z","iopub.execute_input":"2025-11-01T15:24:36.430119Z","iopub.status.idle":"2025-11-01T15:24:39.581768Z","shell.execute_reply.started":"2025-11-01T15:24:36.430096Z","shell.execute_reply":"2025-11-01T15:24:39.580802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* **Higher education + stable employment ‚Üí higher likelihood of being in better credit grades than expected.**\n* **Unemployment ‚Üí strongly weaker credit performance, regardless of education level.**\n* **Employment status is a stronger differentiator of credit quality than education level.**","metadata":{}},{"cell_type":"markdown","source":"#### Can combining credit grade, loan purpose, and employment status help build a more accurate risk scoring model?","metadata":{}},{"cell_type":"code","source":"df_bq_group = pd.crosstab(\n    [df_bq[\"grade_category\"], df_bq[\"loan_purpose\"], df_bq[\"employment_status\"]],\n    df_bq[\"loan_paid_back\"],\n    normalize=\"index\"\n) * 100\n\ndisplay(df_bq_group)\n\ncontingency = pd.crosstab(\n    [df_bq[\"grade_category\"], df_bq[\"loan_purpose\"], df_bq[\"employment_status\"]],\n    df_bq[\"loan_paid_back\"]\n)\nchi2, p, dof, ex = chi2_contingency(contingency)\nprint(\"Chi2:\", chi2, \"p-value:\", p)\n\n# 1) Chi-square test + expected counts\nchi2, p, dof, expected = chi2_contingency(contingency.values)\nexpected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n\nprint(f\"Chi2: {chi2:.4f} | dof: {dof} | p-value: {p:.6f}\")\n\n# 2) Standardized residuals (adjusted)\n#    r_ij = (O_ij - E_ij) / sqrt(E_ij * (1 - row_prob_i) * (1 - col_prob_j))\nrow_sums = contingency.sum(axis=1).values[:, None]        # shape (R,1)\ncol_sums = contingency.sum(axis=0).values[None, :]        # shape (1,C)\ngrand_total = contingency.values.sum()\n\nrow_prob = row_sums / grand_total               # R x 1\ncol_prob = col_sums / grand_total               # 1 x C\n\ndenom = np.sqrt(expected * (1 - row_prob) * (1 - col_prob))\nstd_resid = (contingency.values - expected) / denom\n\nstd_resid_df = pd.DataFrame(std_resid, index=contingency.index, columns=contingency.columns)\n\n# 3) Heatmap ‚Äî display nicely the multiindex\nplt.figure(figsize=(15,35))\nsns.heatmap(std_resid_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", center=0, cbar_kws={\"label\": \"Standardized Residual\"})\nplt.title(\"Standardized Residuals Heatmap: Grade Category or Loan Purpose or Employment Status vs Loan Status\", weight=\"bold\", fontsize=14, pad=20)\nplt.ylabel(\"Grade Category or Loan Purpose or Employment Status | Loan Status\")\nplt.xlabel(\"Loan Status\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:39.582817Z","iopub.execute_input":"2025-11-01T15:24:39.583408Z","iopub.status.idle":"2025-11-01T15:24:41.581983Z","shell.execute_reply.started":"2025-11-01T15:24:39.583377Z","shell.execute_reply":"2025-11-01T15:24:41.58121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key findings**\n\n* **Credit grade alone is not sufficient** ‚Äî unemployment or debt consolidation can overturn strong credit signals.\n* **Employment stability is a critical risk differentiator**.\n* **Loan purpose (especially debt consolidation) amplifies risk when combined with lower credit grades or unemployment.**","metadata":{}},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Data Preprocessing</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Data Preprocessing\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Ability to afford the loan\ndf_train[\"income_to_loan_ratio\"] = df_train[\"annual_income\"] / (df_train[\"loan_amount\"] + 1)\ndf_test[\"income_to_loan_ratio\"] = df_test[\"annual_income\"] / (df_test[\"loan_amount\"] + 1)\n\n# Loan burden level\ndf_train[\"loan_burden_score\"] = df_train[\"loan_amount\"] / (df_train[\"annual_income\"] + 1)\ndf_test[\"loan_burden_score\"] = df_test[\"loan_amount\"] / (df_test[\"annual_income\"] + 1)\n\n# Credit score normalized by income\ndf_train[\"credit_to_income\"] = df_train[\"credit_score\"] / (df_train[\"annual_income\"] + 1)\ndf_test[\"credit_to_income\"] = df_test[\"credit_score\"] / (df_test[\"annual_income\"] + 1)\n\n# Additional interest cost\ndf_train[\"interest_burden\"] = (df_train[\"interest_rate\"]) * df_train[\"loan_amount\"]\ndf_test[\"interest_burden\"] = (df_test[\"interest_rate\"]) * df_test[\"loan_amount\"]\n\n# Rate adjusted by credit strength\ndf_train[\"normalized_interest\"] = (df_train[\"interest_rate\"]) * df_train[\"credit_score\"]\ndf_test[\"normalized_interest\"] = (df_test[\"interest_rate\"]) * df_test[\"credit_score\"]\n\n# Combined DTI √ó credit risk\ndf_train[\"dti_credit_ratio\"] = df_train[\"debt_to_income_ratio\"] * (850 - df_train[\"credit_score\"])\ndf_test[\"dti_credit_ratio\"] = df_test[\"debt_to_income_ratio\"] * (850 - df_test[\"credit_score\"])\n\n# Income relative to credit\ndf_train[\"income_credit_ratio\"] = df_train[\"annual_income\"] / (df_train[\"credit_score\"] + 1)\ndf_test[\"income_credit_ratio\"] = df_test[\"annual_income\"] / (df_test[\"credit_score\"] + 1)\n\n# Loan size relative to rating\ndf_train[\"loan_credit_ratio\"] = df_train[\"loan_amount\"] / (df_train[\"credit_score\"] + 1)\ndf_test[\"loan_credit_ratio\"] = df_test[\"loan_amount\"] / (df_test[\"credit_score\"] + 1)\n\n# Estimated disposable income\ndf_train[\"free_income_est\"] = df_train[\"annual_income\"] * (1 - df_train[\"debt_to_income_ratio\"])\ndf_test[\"free_income_est\"] = df_test[\"annual_income\"] * (1 - df_test[\"debt_to_income_ratio\"])\n\n# Payment stress level\ndf_train[\"stress_score\"] = (df_train[\"loan_amount\"] / (df_train[\"annual_income\"] + 1)) * df_train[\"debt_to_income_ratio\"]\ndf_test[\"stress_score\"] = (df_test[\"loan_amount\"] / (df_test[\"annual_income\"] + 1)) * df_test[\"debt_to_income_ratio\"]\n\n# Stress inflated by interest rate\ndf_train[\"risk_pressure\"] = df_train[\"stress_score\"] * df_train[\"interest_rate\"]\ndf_test[\"risk_pressure\"] = df_test[\"stress_score\"] * df_test[\"interest_rate\"]\n\n# Credit score adjusted for leverage\ndf_train[\"adjusted_credit\"] = df_train[\"credit_score\"] - (df_train[\"debt_to_income_ratio\"] * 200)\ndf_test[\"adjusted_credit\"] = df_test[\"credit_score\"] - (df_test[\"debt_to_income_ratio\"] * 200)\n\n# Credit quality per loan size\ndf_train[\"credit_to_loan\"] = df_train[\"credit_score\"] / (df_train[\"loan_amount\"] + 1)\ndf_test[\"credit_to_loan\"] = df_test[\"credit_score\"] / (df_test[\"loan_amount\"] + 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:41.582878Z","iopub.execute_input":"2025-11-01T15:24:41.583161Z","iopub.status.idle":"2025-11-01T15:24:41.65397Z","shell.execute_reply.started":"2025-11-01T15:24:41.583141Z","shell.execute_reply":"2025-11-01T15:24:41.653169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_features = [\"annual_income\", \"debt_to_income_ratio\", \"credit_score\", \"loan_amount\", \"interest_rate\", \"income_to_loan_ratio\", \n                \"loan_burden_score\", \"credit_to_income\", \"interest_burden\", \"normalized_interest\", \"dti_credit_ratio\", \n                \"income_credit_ratio\", \"loan_credit_ratio\", \"free_income_est\", \"stress_score\", \"risk_pressure\", \"adjusted_credit\", \"credit_to_loan\"]\nnew_num_features = [\"income_to_loan_ratio\", \"loan_burden_score\", \"credit_to_income\", \"interest_burden\", \"normalized_interest\", \"dti_credit_ratio\", \n                \"income_credit_ratio\", \"loan_credit_ratio\", \"free_income_est\", \"stress_score\", \"risk_pressure\", \"adjusted_credit\", \"credit_to_loan\"]\ncat_features = [\"gender\", \"marital_status\", \"education_level\", \"employment_status\", \"loan_purpose\", \"grade_category\"]\n\nfor feature in new_num_features:\n    display(HTML(f\"<h2 style='text-align:center; font-size:22px; color:green;'><b>Distribution of {feature} by Loan Status</b></h2>\"))\n    plot_numerical_distribution(feature=feature, df = df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:24:41.654811Z","iopub.execute_input":"2025-11-01T15:24:41.655084Z","iopub.status.idle":"2025-11-01T15:25:31.983296Z","shell.execute_reply.started":"2025-11-01T15:24:41.655062Z","shell.execute_reply":"2025-11-01T15:25:31.982589Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* `income_to_loan_ratio` <br>\nBorrowers with a higher income-to-loan ratio demonstrate better repayment performance. This indicates that stronger financial capacity relative to loan size is a key factor in reducing default risk.\n\n* `loan_burden_score`<br>\nBorrowers with a higher Loan Burden Score are more likely to **fail to repay**. This indicates that higher debt pressure significantly increases the **risk of default**.\n\n* `credit_to_income`<br>\nBorrowers with a **higher Credit-to-Income Ratio** are more likely to **repay successfully**. This suggests that individuals with stronger credit access relative to income tend to be **more financially stable** and present **lower default risk**.\n\n* `interest_burden`<br>\nBorrowers with a higher **Interest Burden** are more likely to **default**, though the effect size is **relatively small**, indicating that interest cost plays a **secondary role** in default risk.\n\n* `normalized_interest`<br>\nBorrowers with higher **Normalized Interest** are more likely to **default**, but the effect is **very minimal**, suggesting interest level is a **minor contributor** to credit risk compared to other financial indicators.\n\n* `dti_credit_ratio`<br>\nBorrowers with a higher **DTI-Credit Ratio** are more likely to **default**. A higher debt load relative to available credit is a strong indicator of **increased default risk**.\n\n* `income_credit_ratio`<br>\nBorrowers in the **Not paid** group exhibit a **higher Income‚ÄìCredit Ratio**, indicating that a higher income-to-credit ratio **does not necessarily imply stronger repayment capacity**. This may reflect cases where borrowers **over-leverage beyond their credit capacity**, increasing default risk.\n\n* `loan_credit_ratio`<br>\nBorrowers who **defaulted** tend to have a **higher Loan‚ÄìCredit Ratio**, indicating that borrowing closer to credit limits is associated with **higher default risk**. However, the **effect size is small**, suggesting this ratio is a **supporting indicator** rather than a primary driver.\n\n* `free_income_est`<br>\nBorrowers who **successfully repaid (Paid)** exhibit **higher Free Income Estimate**, indicating that those with **greater disposable income after essentials** tend to have **better repayment performance**. This highlights the importance of **positive free cash flow** in reducing default risk.\n\n* `stress_score`<br>\nBorrowers who **defaulted** exhibit **higher Stress Scores**, suggesting that elevated financial and psychological stress correlates with **higher default risk**. This implies that financial stress may serve as an **early warning indicator** in credit risk assessment.\n\n* `risk_pressure`<br>\nBorrowers who **defaulted** show **higher Risk Pressure**, indicating that greater financial pressure is associated with **increased default likelihood**. This reinforces the importance of financial-stress indicators as **key signals in credit risk monitoring**.\n\n* `adjusted_credit`<br>\nBorrowers who **repaid** show significantly **higher Adjusted Credit**, indicating that stronger credit profiles are strongly associated with **better repayment behavior**. With a **large effect size (Cohen's d = 0.809)**, this is one of the **strongest predictors** of non-default.\n\n* `credit_to_loan`<br>\nBorrowers who **repaid** have a **higher Credit-to-Loan Ratio**, indicating that those with greater credit capacity relative to their loan are **less likely to default**. This ratio reflects **stronger credit health** and **greater financial cushion**.","metadata":{}},{"cell_type":"code","source":"plot_correlation(df_train=df_train.drop(columns=\"loan_paid_back\", axis=1),\n                 df_test=df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:31.984101Z","iopub.execute_input":"2025-11-01T15:25:31.984311Z","iopub.status.idle":"2025-11-01T15:25:34.048563Z","shell.execute_reply.started":"2025-11-01T15:25:31.984295Z","shell.execute_reply":"2025-11-01T15:25:34.047742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Correlation Heatmap Analysis**\n\n* **Goal**: Compare relationships between financial and demographic features in train and test sets.\n* **Key observations**:\n\n  * *Bucketed* features (e.g., `income_range_bucket`, `fico_score_bucket`, `employment_length_bucket`) strongly correlate with their original variables ‚Üí confirms meaningful grouping.\n  * `interest_rate` correlates with `fico_score` and `debt_to_income_ratio` ‚Üí aligns with financial logic.\n  * `loan_amount` correlates with `adjusted_gross_income` ‚Üí income impacts loan size.\n* **Train/Test comparison**: Correlation patterns are similar ‚Üí model can generalize consistently.","metadata":{}},{"cell_type":"markdown","source":"## Overall Picture\n\n| **Feature**              | **Summary Insight**                                                                                  |\n| ------------------------ | ---------------------------------------------------------------------------------------------------- |\n| **annual_income**        | Higher income ‚Üí better repayment ability                                                             |\n| **debt_to_income_ratio** | Higher DTI ‚Üí higher default risk                                                                     |\n| **credit_score**         | Higher credit score ‚Üí strong repayment predictor                                                     |\n| **loan_amount**          | Loan size has minimal effect on repayment                                                            |\n| **interest_rate**        | Higher interest rates ‚Üí higher default risk                                                          |\n| **interest_burden**      | Higher interest burden ‚Üí higher default risk (small effect)                                          |\n| **normalized_interest**  | Higher normalized interest ‚Üí slightly higher default risk (very small effect)                        |\n| **dti_credit_ratio**     | Higher DTI-to-credit ratio ‚Üí increased default risk                                                  |\n| **income_credit_ratio**  | Higher income-to-credit in *Not paid* ‚Üí may reflect over-borrowing vs credit capacity                |\n| **loan_credit_ratio**    | Higher loan-to-credit ratio ‚Üí higher default risk (small effect)                                     |\n| **free_income_est**      | Higher free income ‚Üí better repayment capability                                                     |\n| **stress_score**         | Higher stress score ‚Üí higher default risk                                                            |\n| **risk_pressure**        | Higher risk pressure ‚Üí higher default probability                                                    |\n| **adjusted_credit**      | Higher adjusted credit ‚Üí **strong repayment predictor (large effect)**                               |\n| **credit_to_loan**       | Higher credit-to-loan ratio ‚Üí stronger ability to repay                                              |\n| **income_to_loan_ratio** | **Higher income-to-loan ‚Üí better repayment** (stronger financial capacity reduces default risk)      |\n| **loan_burden_score**    | **Higher loan burden ‚Üí higher default risk** (greater pressure increases chance of failure to repay) |\n| **credit_to_income**     | **Higher credit-to-income ‚Üí better repayment** (better credit access ‚Üí stronger stability)           |\n| **gender**               | Males default more; females repay more consistently                                                  |\n| **marital_status**       | Repayment behavior consistent across marital groups                                                  |\n| **education_level**      | HS/PhD repay better; Bachelor's default more                                                         |\n| **employment_status**    | Unemployed/Students default more; employed/retired repay better                                      |\n| **loan_purpose**         | Education/Medical loans default more; Home/Business repay better                                     |\n| **grade_category**       | Lower credit grades ‚Üí higher default risk                                                            |","metadata":{}},{"cell_type":"markdown","source":"## Re-checking Skew","metadata":{}},{"cell_type":"code","source":"skew_feature_train, skew_train_df = check_skewness(df_train, \"Train Data\", numerical_features=num_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:34.049386Z","iopub.execute_input":"2025-11-01T15:25:34.049612Z","iopub.status.idle":"2025-11-01T15:25:34.150844Z","shell.execute_reply.started":"2025-11-01T15:25:34.049595Z","shell.execute_reply":"2025-11-01T15:25:34.150098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"skew_feature_test, skew_test_df = check_skewness(df_test, \"Test Data\", numerical_features=num_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:34.151657Z","iopub.execute_input":"2025-11-01T15:25:34.151939Z","iopub.status.idle":"2025-11-01T15:25:34.201696Z","shell.execute_reply.started":"2025-11-01T15:25:34.151916Z","shell.execute_reply":"2025-11-01T15:25:34.20093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\ndef handle_skewed_features(\n    df,\n    zero_threshold=0.9,\n    skew_threshold=0.5,\n    num_features=None,\n    exclude_cols=None\n):\n    \"\"\"\n    Handle skewed numerical features by applying appropriate transformations.\n\n    Parameters:\n    - df: pandas.DataFrame\n    - zero_threshold: float (default=0.9)\n    - skew_threshold: float (default=0.5)\n    - num_features: list of numerical columns to consider\n    - exclude_cols: list of columns to skip entirely\n\n    Returns:\n    - df: transformed DataFrame\n    - transformed_cols: list of new feature names\n    - high_zero_cols: list of sparse features (> zero_threshold)\n    - skewed_cols: list of auto‚Äëdetected skewed features\n    \"\"\"\n    df = df.copy()\n    if num_features is None:\n        raise ValueError(\"`num_features` must be provided\")\n    if exclude_cols is None:\n        exclude_cols = []\n\n    # 1) pick the numeric cols to scan\n    numerical_cols = [c for c in num_features if c not in exclude_cols]\n\n    # 2) detect ultra‚Äësparse\n    zero_ratios = (df[numerical_cols] == 0).sum() / len(df)\n    high_zero_cols = zero_ratios[zero_ratios > zero_threshold].index.tolist()\n\n    # 3) compute skew\n    skew_vals = df[numerical_cols].apply(lambda s: skew(s.dropna()))\n    auto_skewed = skew_vals[abs(skew_vals) > skew_threshold].index.tolist()\n\n    # 4) union these with your forced list\n    to_transform = list(set(auto_skewed))\n\n    transformed_cols = []\n    dropped_cols     = []\n\n    for col in to_transform:\n        # if it's sparse ‚Üí binary+log\n        if col in high_zero_cols:\n            df[f\"Has_{col}\"] = (df[col] > 0).astype(int)\n            df[f\"Log_{col}\"] = df[col].map(lambda x: np.log1p(x) if x > 0 else 0)\n            transformed_cols += [f\"Has_{col}\", f\"Log_{col}\"]\n            dropped_cols.append(col)\n        # if it's discrete small‚Äëcardinality, skip transform but keep\n        elif df[col].nunique() <= 5:\n            # do nothing (we still keep raw col in df)\n            continue\n        # otherwise apply Yeo‚ÄëJohnson\n        else:\n            pt = PowerTransformer(method=\"yeo-johnson\")\n            arr = df[[col]].values  # shape (n,1)\n            df[f\"PT_{col}\"] = pt.fit_transform(arr)\n            transformed_cols.append(f\"PT_{col}\")\n            dropped_cols.append(col)\n\n    # drop originals for any column we did transform\n    df.drop(columns=dropped_cols, inplace=True)\n\n    return df, transformed_cols, high_zero_cols, auto_skewed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:34.202615Z","iopub.execute_input":"2025-11-01T15:25:34.202877Z","iopub.status.idle":"2025-11-01T15:25:34.211432Z","shell.execute_reply.started":"2025-11-01T15:25:34.20286Z","shell.execute_reply":"2025-11-01T15:25:34.210758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_train_df, transformed_columns, sparse_columns, skewed_columns = handle_skewed_features(df=df_train, num_features=skew_feature_train)\nnum_features = [\"PT_annual_income\", \"PT_debt_to_income_ratio\", \"credit_score\", \"loan_amount\", \"interest_rate\", \"PT_income_to_loan_ratio\", \n                \"PT_loan_burden_score\", \"PT_credit_to_income\", \"interest_burden\", \"normalized_interest\", \"PT_dti_credit_ratio\", \n                \"PT_income_credit_ratio\", \"loan_credit_ratio\", \"PT_free_income_est\", \"PT_stress_score\", \"PT_risk_pressure\", \"adjusted_credit\", \"PT_credit_to_loan\"]\nskew_feature_train, skew_train_df = check_skewness(processed_train_df, \"Train Data\", numerical_features=num_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:34.212249Z","iopub.execute_input":"2025-11-01T15:25:34.212476Z","iopub.status.idle":"2025-11-01T15:25:37.348576Z","shell.execute_reply.started":"2025-11-01T15:25:34.21246Z","shell.execute_reply":"2025-11-01T15:25:37.347823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_test_df, transformed_columns_test, sparse_columns_test, skewed_columns_test = handle_skewed_features(df=df_test, num_features=skew_feature_test)\nskew_feature_test, skew_test_df = check_skewness(data=processed_test_df, numerical_features=num_features,\n                                                   dataset_name= \"Test data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:37.349377Z","iopub.execute_input":"2025-11-01T15:25:37.349851Z","iopub.status.idle":"2025-11-01T15:25:38.569309Z","shell.execute_reply.started":"2025-11-01T15:25:37.349825Z","shell.execute_reply":"2025-11-01T15:25:38.56857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations from the Skewness Table (`processed_train_df` and `processed_test_df`):**\n\n‚úÖ **Approximately Symmetric**:\n\n* Several features, such as:\n\n  * `PT_credit_to_loan`, `PT_stress_score`, `PT_credit_to_income`, `PT_risk_pressure`, `PT_debt_to_income_ratio`, `PT_loan_burden_score`, `PT_income_to_loan_ratio`, `PT_free_income_est`, `PT_income_credit_ratio`, `PT_dti_credit_ratio` and `PT_annual_income` close to zero.\n  * This indicates that **Yeo-Johnson transformation was effective** for these features.","metadata":{}},{"cell_type":"markdown","source":"## Re-check Outliers","metadata":{}},{"cell_type":"code","source":"checking_outlier(list_feature=num_features, df=processed_train_df, dataset_name=\"Data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:38.570133Z","iopub.execute_input":"2025-11-01T15:25:38.570701Z","iopub.status.idle":"2025-11-01T15:25:39.001704Z","shell.execute_reply.started":"2025-11-01T15:25:38.570679Z","shell.execute_reply":"2025-11-01T15:25:39.000885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checking_outlier(list_feature=num_features, df=processed_test_df, dataset_name=\"Data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:39.002556Z","iopub.execute_input":"2025-11-01T15:25:39.002919Z","iopub.status.idle":"2025-11-01T15:25:39.210835Z","shell.execute_reply.started":"2025-11-01T15:25:39.002893Z","shell.execute_reply":"2025-11-01T15:25:39.210259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display information about the DataFrames\nprint(\"Train Data Info:\")\nprocessed_train_df.info()\n\nprint(\"\\nTest Data Info:\")\nprocessed_test_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:39.211533Z","iopub.execute_input":"2025-11-01T15:25:39.21178Z","iopub.status.idle":"2025-11-01T15:25:39.336679Z","shell.execute_reply.started":"2025-11-01T15:25:39.211761Z","shell.execute_reply":"2025-11-01T15:25:39.335999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_train_df[\"loan_paid_back\"] = processed_train_df[\"loan_paid_back\"].map({\"Not paid\": 0, \"Paid\": 1}).astype(int)\n# We need to update the data for the columns, this helps to reduce memory.\nprocessed_train_df = processed_train_df.astype({\n    \"credit_score\": \"int16\",\n    \"loan_amount\": \"float16\",\n    \"interest_rate\": \"float16\",\n    \"loan_paid_back\": \"int8\",\n    \"interest_burden\": \"float32\",\n    \"normalized_interest\": \"float16\",\n    \"loan_credit_ratio\": \"float16\",\n    \"adjusted_credit\": \"float16\",\n    \"PT_loan_burden_score\": \"float16\",\n    \"PT_dti_credit_ratio\": \"float16\",\n    \"PT_credit_to_income\": \"float16\",\n    \"PT_free_income_est\": \"float16\",\n    \"PT_annual_income\": \"float16\",\n    \"PT_credit_to_loan\": \"float16\",\n    \"PT_stress_score\": \"float16\",\n    \"PT_income_credit_ratio\": \"float16\",\n    \"PT_income_to_loan_ratio\": \"float16\",\n    \"PT_risk_pressure\": \"float16\",\n    \"grade_category\": \"category\"\n})\n\nprocessed_test_df = processed_test_df.astype({\n    \"credit_score\": \"int16\",\n    \"loan_amount\": \"float16\",\n    \"interest_rate\": \"float16\",\n    \"interest_burden\": \"float32\",\n    \"normalized_interest\": \"float16\",\n    \"loan_credit_ratio\": \"float16\",\n    \"adjusted_credit\": \"float16\",\n    \"PT_loan_burden_score\": \"float16\",\n    \"PT_dti_credit_ratio\": \"float16\",\n    \"PT_credit_to_income\": \"float16\",\n    \"PT_free_income_est\": \"float16\",\n    \"PT_annual_income\": \"float16\",\n    \"PT_credit_to_loan\": \"float16\",\n    \"PT_stress_score\": \"float16\",\n    \"PT_income_credit_ratio\": \"float16\",\n    \"PT_income_to_loan_ratio\": \"float16\",\n    \"PT_risk_pressure\": \"float16\",\n    \"grade_category\": \"category\"\n})\n\n# Display information about the DataFrames\nprint(\"Train Data Info:\")\nprocessed_train_df.info()\n\nprint(\"\\nTest Data Info:\")\nprocessed_test_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:39.337361Z","iopub.execute_input":"2025-11-01T15:25:39.337575Z","iopub.status.idle":"2025-11-01T15:25:39.556304Z","shell.execute_reply.started":"2025-11-01T15:25:39.337559Z","shell.execute_reply":"2025-11-01T15:25:39.555495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Test Set\n\nüî¥ Suppose you chatted with experts who told you that the **credit_score** is a very important attribute to predict **loan_paid_back**. <br>\nüî¥ We may want to ensure that the test set is representative of the various categories of study hours per day in the whole dataset. Since the study hours per day is a continuous numerical attribute, we first need to create an category attribute.","metadata":{}},{"cell_type":"code","source":"processed_train_df[\"credit_score_cat\"] = pd.qcut(processed_train_df[\"credit_score\"],\n                                              q=4,\n                                              labels=[1, 2, 3, 4])\n\nplt.figure(figsize=(8, 5))\nsns.histplot(data=processed_train_df, x=\"credit_score_cat\", color=\"lightblue\", edgecolor=\"black\")\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.title(\"Distribution of credit_score_cat\", fontsize=14, weight=\"bold\",pad=20)\nplt.xlabel(\"credit_score_cat\", fontsize=12)\nplt.ylabel(\"\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:39.557023Z","iopub.execute_input":"2025-11-01T15:25:39.557222Z","iopub.status.idle":"2025-11-01T15:25:39.943232Z","shell.execute_reply.started":"2025-11-01T15:25:39.557206Z","shell.execute_reply":"2025-11-01T15:25:39.942505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=Config.N_SPLIT, test_size=Config.TEST_SIZE, \n                               random_state=Config.SEED)\nfor train_index, val_index in split.split(processed_train_df, processed_train_df[\"credit_score_cat\"]):\n    start_train_set = processed_train_df.loc[train_index]\n    start_val_set = processed_train_df.loc[val_index]\n\n# Now we should remove the credit_score_cat attribute so the data is back to its original state:\nfor set_ in (start_train_set, start_val_set): \n    set_.drop(\"credit_score_cat\", axis=1, inplace=True)\n\ndf_train_new = start_train_set.drop(\"loan_paid_back\", axis=1)\ndf_train_label = start_train_set[\"loan_paid_back\"].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:39.943958Z","iopub.execute_input":"2025-11-01T15:25:39.944163Z","iopub.status.idle":"2025-11-01T15:25:40.255334Z","shell.execute_reply.started":"2025-11-01T15:25:39.944148Z","shell.execute_reply":"2025-11-01T15:25:40.254698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scale and Encode","metadata":{}},{"cell_type":"code","source":"list_standard = [\"PT_debt_to_income_ratio\", \"credit_score\", \"PT_loan_burden_score\", \"PT_credit_to_income\", \"PT_stress_score\", \"PT_risk_pressure\", \n                \"PT_credit_to_loan\"]\n\nlist_robust = [\"loan_amount\", \"interest_rate\", \"PT_income_to_loan_ratio\", \"interest_burden\", \"PT_annual_income\",\n               \"normalized_interest\", \"PT_dti_credit_ratio\", \"PT_income_credit_ratio\", \"loan_credit_ratio\", \"PT_free_income_est\", \"adjusted_credit\"]\n\nstandard_transfomer = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\nrobust_transfomer = Pipeline(steps=[\n    (\"scaler\", RobustScaler()),\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\ncat_transfomer = Pipeline(steps=[\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num_standard\", standard_transfomer, list_standard),\n        (\"num_robust\", robust_transfomer, list_robust),\n        (\"cat\", cat_transfomer, cat_features),\n    ]\n)\n\npreprocessor.fit(df_train_new)\n\ndf_train_new_prepared = preprocessor.transform(df_train_new)\nlist_feature_prepared = preprocessor.get_feature_names_out().tolist()\nclean_features = [col.replace(\"num_standard__\", \"\").replace(\"num_robust__\", \"\").replace(\"cat__\", \"\").replace(\"PT_\", \"\") for col in list_feature_prepared]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:40.256166Z","iopub.execute_input":"2025-11-01T15:25:40.257Z","iopub.status.idle":"2025-11-01T15:25:42.970804Z","shell.execute_reply.started":"2025-11-01T15:25:40.256973Z","shell.execute_reply":"2025-11-01T15:25:42.970117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Metric</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Metric\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"**AUC-ROC Overview**\n\n**AUC-ROC** evaluates a model‚Äôs ability to **distinguish between two classes**.\n\n* **ROC curve** plots **TPR vs FPR**\n* **AUC** = Area under ROC curve ‚Üí **ranking quality of predictions**\n\n**Why AUC-ROC?**\n\n* Robust to **class imbalance**\n* Measures **ranking ability**, not fixed threshold (0.5)\n* Common metric in **credit risk & fraud prediction**\n\n> Higher AUC ‚Üí better separation between *Paid* vs *Default*\n\n**Interpretation**\n\n| AUC Score     | Meaning                  |\n| ------------- | ------------------------ |\n| **1.0**       | Perfect model            |\n| **0.9+**      | Excellent discrimination |\n| **0.7 ‚Äì 0.9** | Good                     |\n| **0.5**       | Random guessing          |\n| **< 0.5**     | Worse than random        |\n\n(Source: [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic))","metadata":{}},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Modeling</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Modeling\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"def shap_plot(model, X_test, list_feature, type = None):\n     # https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137/\n    if hasattr(X_test, \"toarray\"):\n        X_test = X_test.toarray()\n    X_test_sample = pd.DataFrame(X_test, columns=list_feature)\n    explainer = shap.Explainer(model.predict, X_test_sample)\n    shap_values = explainer(X_test_sample)\n    if type ==\"bar\":\n        shap_importance = np.abs(shap_values.values).mean(axis=0)\n        shap_df = pd.DataFrame({\"feature\": X_test_sample.columns, \"importance\": shap_importance})\n        shap_df = shap_df.sort_values(\"importance\", ascending=False).head(20)\n        plt.figure(figsize=(12, 6))\n        sns.barplot(x=shap_df[\"importance\"], y=shap_df[\"feature\"], palette=\"viridis\", order=shap_df[\"feature\"])\n        plt.xlabel(\"mean(|SHAP value|)\")\n        plt.title(\"SHAP Feature Importance\", fontsize=14, weight=\"bold\", pad=20)\n        plt.tight_layout()\n        plt.show()\n    else:\n        shap.summary_plot(shap_values, X_test_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:42.97169Z","iopub.execute_input":"2025-11-01T15:25:42.972054Z","iopub.status.idle":"2025-11-01T15:25:42.978625Z","shell.execute_reply.started":"2025-11-01T15:25:42.972035Z","shell.execute_reply":"2025-11-01T15:25:42.977998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_ROC_confusionMatrix(estimator, X_val, y_val, figsize):\n    y_pred_prob = estimator.predict_proba(X_val)[:, 1]  # Probability of positive class\n    y_pred = estimator.predict(X_val)\n\n    fig, ax = plt.subplots(nrows=2, ncols=2, sharey=False, figsize=figsize)\n\n    # Plot 1\n    # Calculate ROC\n    fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n    rocScore = roc_auc_score(y_val, y_pred_prob)\n\n    ax[0, 0].plot(fpr, tpr, label=f\"{estimator.__class__.__name__} (AUC = {rocScore:.2f})\")\n    ax[0, 0].plot([0, 1], [0, 1], \"b--\")\n    ax[0, 0].set_xlabel(\"False Positive Rate\")\n    ax[0, 0].set_ylabel(\"True Positive Rate\")\n    ax[0, 0].set_title(f\"ROC ({estimator.__class__.__name__})\", fontsize=14, weight=\"bold\", pad=20)\n    ax[0, 0].legend()\n\n    # Plot 2\n    confusionMatrix = confusion_matrix(y_val, y_pred)\n    sns.heatmap(confusionMatrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax[0, 1])\n    ax[0, 1].set_title(f\"Confusion Matrix ({estimator.__class__.__name__})\", fontsize=14, weight=\"bold\", pad=20)\n    ax[0, 1].set_xlabel(\"Prediction\")\n    ax[0, 1].set_ylabel(\"Actual\")\n\n    # plot 3\n    avg_prec = average_precision_score(y_val, y_pred_prob)   \n    precision, recall, thresholds_pr = precision_recall_curve(y_val, y_pred_prob)\n    ax[1, 0].plot(recall, precision, label=f\"PR Curve (AP = {avg_prec:.3f})\")\n    ax[1, 0].set_xlabel(\"Recall\")\n    ax[1, 0].set_ylabel(\"Precision\")\n    ax[1, 0].set_title(\"Precision-Recall Curve\", fontsize=14, weight=\"bold\", pad=20)\n    ax[1, 0].legend()\n\n    ax.flat[-1].set_visible(False)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(classification_report(y_val, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:42.979511Z","iopub.execute_input":"2025-11-01T15:25:42.979782Z","iopub.status.idle":"2025-11-01T15:25:42.99623Z","shell.execute_reply.started":"2025-11-01T15:25:42.97976Z","shell.execute_reply":"2025-11-01T15:25:42.995548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to evaluate models\ndef evaluate_model(model, X_train, X_val, y_train, y_val, figsize = (15, 6), show_shap_plot = False):\n    print(f\"Evaluating {model.__class__.__name__}...\")\n    model.fit(X_train, y_train)\n    plot_ROC_confusionMatrix(estimator = model, X_val = X_val, y_val = y_val, figsize = figsize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:42.996934Z","iopub.execute_input":"2025-11-01T15:25:42.997179Z","iopub.status.idle":"2025-11-01T15:25:43.013079Z","shell.execute_reply.started":"2025-11-01T15:25:42.997164Z","shell.execute_reply":"2025-11-01T15:25:43.012458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_val = start_val_set.drop(\"loan_paid_back\", axis=1)\ny_val = start_val_set[\"loan_paid_back\"].copy()\nX_val_prepared = preprocessor.transform(X_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:43.013825Z","iopub.execute_input":"2025-11-01T15:25:43.014204Z","iopub.status.idle":"2025-11-01T15:25:43.216815Z","shell.execute_reply.started":"2025-11-01T15:25:43.014176Z","shell.execute_reply":"2025-11-01T15:25:43.215933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGBClassifier","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nparam_xgb = {\n\"lambda\": 0.0028334499645967606, \n\"alpha\": 6.173470071867061, \n\"max_depth\": 3, \n\"eta\": 0.11987979274427926, \n\"subsample\": 0.9194731846804935, \n\"colsample_bytree\": 0.6174129520077346, \n\"min_child_weight\": 5, \n\"gamma\": 0.9449756382275054, \n\"n_estimators\": 1800,\n\"n_jobs\": -1,\n\"verbosity\": 0,\n\"random_state\": Config.SEED,\n\"use_label_encoder\": False,\n\"objective\": \"binary:logistic\",\n\"eval_metric\": \"auc\",\n\"tree_method\": \"hist\",\n\"booster\": \"gbtree\"\n}\n\nmodel_xgb = xgb.XGBClassifier(**param_xgb)\n\nevaluate_model(model = model_xgb, X_train=df_train_new_prepared, X_val=X_val_prepared,\n               y_train=df_train_label, y_val=y_val, figsize=(15, 10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:25:43.217767Z","iopub.execute_input":"2025-11-01T15:25:43.218066Z","iopub.status.idle":"2025-11-01T15:26:20.411586Z","shell.execute_reply.started":"2025-11-01T15:25:43.218041Z","shell.execute_reply":"2025-11-01T15:26:20.411014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CatBoostClassifier","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n# After running optuna\nparam_cb = {\n\"iterations\": 1383, \n\"learning_rate\": 0.22775461488679877, \n\"depth\": 5, \n\"l2_leaf_reg\": 7.46314929623761, \n\"random_strength\": 1.5904542174434636e-05, \n\"bagging_temperature\": 0.03502831981387006, \n\"border_count\": 252,\n\"loss_function\": \"Logloss\",\n\"eval_metric\": \"AUC\",\n\"verbose\": 0,\n\"random_seed\": Config.SEED,\n\"bootstrap_type\": \"Bayesian\",\n\"thread_count\": -1,\n\"grow_policy\": \"Lossguide\"\n}\n\nmodel_cb = CatBoostClassifier(**param_cb)\nevaluate_model(model = model_cb, X_train=df_train_new_prepared, X_val=X_val_prepared,\n               y_train=df_train_label, y_val=y_val, figsize=(15, 10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:26:20.412307Z","iopub.execute_input":"2025-11-01T15:26:20.412562Z","iopub.status.idle":"2025-11-01T15:27:43.218347Z","shell.execute_reply.started":"2025-11-01T15:26:20.412544Z","shell.execute_reply":"2025-11-01T15:27:43.217709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LGBMClassifier","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nparams_lgbm = {\n\"objective\": \"binary\",\n\"metric\": \"binary_logloss\",\n\"boosting_type\": \"gbdt\",\n\"num_leaves\": 31,\n\"learning_rate\": 0.0322942967545754,\n\"feature_fraction\": 0.6236144085285287,\n\"bagging_fraction\": 0.9596685778433888,\n\"bagging_freq\": 3,\n\"max_depth\": 15,\n\"min_child_samples\": 20,\n\"subsample\": 0.782964614940435,\n\"colsample_bytree\": 0.7330716143099598,\n\"reg_alpha\": 0.24890188410341635,\n\"reg_lambda\": 0.004657445631362826,\n\"random_state\": Config.SEED,\n\"verbose\": -1,\n\"n_jobs\": -1,\n\"n_estimators\": 3000\n}\nmodel_lgbm = LGBMClassifier(**params_lgbm)\n\nevaluate_model(model = model_lgbm, X_train=df_train_new_prepared, X_val=X_val_prepared,\n               y_train=df_train_label, y_val=y_val, figsize=(15, 10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:27:43.219103Z","iopub.execute_input":"2025-11-01T15:27:43.219423Z","iopub.status.idle":"2025-11-01T15:28:44.729508Z","shell.execute_reply.started":"2025-11-01T15:27:43.219403Z","shell.execute_reply":"2025-11-01T15:28:44.728837Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VotingClassifier","metadata":{}},{"cell_type":"code","source":"# Collect predictions (probabilities instead of labels) ---\nests = [(\"cb\", model_cb), (\"xgb\", model_xgb), (\"lgbm\", model_lgbm)]\n\npreds = {name: m.predict_proba(X_val_prepared)[:, 1] for name, m in ests}\n\nauc_each = {name: roc_auc_score(y_val, preds[name]) for name,_ in ests}\ndisplay(auc_each)\n\nA = np.column_stack([preds[name] for name,_ in ests])  # shape (n_val, n_models)\ndef obj_w(trial):\n    w = np.array([trial.suggest_float(f\"w_{i}\", 0.0, 5.0) for i in range(A.shape[1])])\n    if w.sum() == 0: \n        return 1e6\n    y_hat = A.dot(w / w.sum())  # weighted average probs\n    return roc_auc_score(y_val, y_hat)\n\nstudy_w = optuna.create_study(direction=\"maximize\")  \nstudy_w.optimize(obj_w, n_trials=1000, show_progress_bar=True)\n\nw = np.array([study_w.best_params[f\"w_{i}\"] for i in range(A.shape[1])])\nweights = (w / w.sum()).tolist()\nprint(\"Best weights (normalized):\", weights)\nprint(\"Best AUC:\", study_w.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:28:44.730274Z","iopub.execute_input":"2025-11-01T15:28:44.7309Z","iopub.status.idle":"2025-11-01T15:29:29.886025Z","shell.execute_reply.started":"2025-11-01T15:28:44.730881Z","shell.execute_reply":"2025-11-01T15:29:29.885115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nvoting_clf_soft = VotingClassifier(estimators=[(\"cb\", model_cb), (\"lgbm\", model_lgbm), (\"xgb\", model_xgb)], weights=weights, voting=\"soft\", n_jobs=-1)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.SEED)\n\ncv_scores = cross_val_score(voting_clf_soft, X=df_train_new_prepared, y=df_train_label, cv=kfold, scoring=\"roc_auc\",  n_jobs=-1)\nprint(f\"Cross-validated ROC-AUC (mean ¬± std): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n\nevaluate_model(model = voting_clf_soft, X_train=df_train_new_prepared, X_val=X_val_prepared,\n               y_train=df_train_label, y_val=y_val, figsize=(15, 10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T15:29:29.8872Z","iopub.execute_input":"2025-11-01T15:29:29.887584Z","execution_failed":"2025-11-01T15:44:36.704Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Prepare Final Submission File</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Prepare Final Submission File\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"df_test_prepared = preprocessor.transform(processed_test_df)\n\n# Generate predicted probabilities for the test set\ny_pred_test_prob_cat = voting_clf_soft.predict_proba(df_test_prepared)\nloan_status = y_pred_test_prob_cat[:, 1]\n\n# Prepare submission file\nsubmission = pd.DataFrame({\n    \"id\": list_test_id,\n    \"loan_paid_back\": loan_status\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"\\nSubmission file saved!\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T15:44:36.705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot distribution of predicted probabilities\nplt.figure(figsize=(10, 6))\nsns.histplot(loan_status, bins=30, kde=True)\nplt.title(\"Distribution of Predicted Loan Paid Back Probabilities\", weight=\"bold\", pad=15, fontsize=12)\nplt.xlabel(\"Predicted Probability of Loan Paid Back\")\nsns.despine(left=False, bottom=False, right=False)\nplt.ylabel(\"Frequency\")\nplt.xlim(0, 1)  # Limit x-axis to [0, 1]\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T15:44:36.706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert probabilities to binary predictions using a threshold (e.g., 0.5)\nbinary_predictions = (loan_status > 0.5).astype(int)\n\n# Plot distribution of binary predictions\nplt.figure(figsize=(8, 5))\nsns.countplot(x=binary_predictions.flatten(), palette= \"RdYlGn\")\nplt.title(\"Distribution of Predicted Loan Paid Back\", weight=\"bold\", pad=15, fontsize=12)\nplt.xlabel(\"Loan Paid Back (0: Not paid, 1: Paid)\")\nplt.ylabel(\"\")\nsns.despine(left=False, bottom=False)\nplt.xticks(ticks=[0, 1], labels=[\"Not paid\", \"Paid\"])\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T15:44:36.707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shap_plot(model=voting_clf_soft.named_estimators_[\"xgb\"], X_test=df_test_prepared[:1500], list_feature=clean_features, type=\"bar\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T15:44:36.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shap_plot(model=voting_clf_soft.named_estimators_[\"xgb\"], X_test=df_test_prepared[:1500], list_feature=clean_features)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T15:44:36.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n# <span style=\"color:transparent;\">Conclusion</span>\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        Conclusion\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"**Key Features Driving Repayment Ability:**\n\n* **employment_status**: Unemployed & students ‚Üí highest default risk; employed & retired ‚Üí strong repayment.\n* **adjusted_credit** & **credit_score**: High credit scores ‚Üí strong repayment ability.\n* **annual_income** & **free_income_est**: High income & free cash ‚Üí better repayment.\n\n**Features Increasing Default Risk:**\n\n* **debt_to_income_ratio** & **dti_credit_ratio**: High debt ratios ‚Üí higher risk.\n* **interest_rate** & **interest_burden**: High rates & interest burden ‚Üí higher default probability.\n* **stress_score** & **risk_pressure**: High stress & risk pressure ‚Üí higher default likelihood.\n\n**Features with Low or Unclear Impact:**\n\n* **loan_amount**: Loan size ‚Üí minor impact on repayment.\n* **marital_status**: Stable repayment across marital groups.\n* **normalized_interest**, **loan_credit_ratio**: Small effect on risk.\n\n**Demographics & Loan Purpose:**\n\n* **education_level**: HS/PhD ‚Üí better repayment; Bachelor ‚Üí higher default.\n* **loan_purpose**: Education/medical loans ‚Üí higher default; home/business loans ‚Üí better repayment.\n* **gender**: Male ‚Üí slightly higher default risk than female.","metadata":{}},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n<div style=\"\n    border-radius: 15px; \n    border: 2px solid #003366; \n    padding: 10px; \n    background: linear-gradient(135deg, #3a0ca3, #7209b7 30%, #f72585 80%);\n    text-align: center; \n    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\n\">\n    <h1 style=\"\n        color: #FFFFFF; \n        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n        font-weight: bold; \n        margin-bottom: 5px; \n        font-size: 28px; \n        font-family: 'Roboto', sans-serif;\n        letter-spacing: 1px;\n    \">\n        üôè Thanks for Reading! üöÄ\n    </h1>\n    <p style=\"color: #ffffff; font-size: 18px; text-align: center;\">\n        Happy Coding! üôåüòä\n    </p>\n</div>\n","metadata":{}}]}