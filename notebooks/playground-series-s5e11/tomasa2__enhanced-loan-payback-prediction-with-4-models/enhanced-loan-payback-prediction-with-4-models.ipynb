{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Enhanced Loan Payback Prediction with Multiple Models and Ensemble\nIn this notebook :\n- XGBoost model \n- CatBoost model\n- Random Forest model\n- LightGBM model\n- Model ensemble approach","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:07.219427Z","iopub.execute_input":"2025-11-08T09:00:07.219696Z","iopub.status.idle":"2025-11-08T09:00:07.564531Z","shell.execute_reply.started":"2025-11-08T09:00:07.219674Z","shell.execute_reply":"2025-11-08T09:00:07.563903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Importing Libraries & Loading Data","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import skew\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set some display options for better visualization\npd.set_option('display.max_columns', None)\nsns.set_style('whitegrid')\n\nprint(\"Libraries imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:14.0166Z","iopub.execute_input":"2025-11-08T09:11:14.016865Z","iopub.status.idle":"2025-11-08T09:11:20.047722Z","shell.execute_reply.started":"2025-11-08T09:11:14.016846Z","shell.execute_reply":"2025-11-08T09:11:20.046993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- In this version of notebook lets try to use original dataset and implement it into pipeline (so far not working as intended )\n- loan-prediction-dataset-2025","metadata":{}},{"cell_type":"code","source":"# Load the data\ndf_train = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\ndf_sub = pd.read_csv(\"/kaggle/input/playground-series-s5e11/sample_submission.csv\")\ndf_orig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\nprint(f\"Training data shape: {df_train.shape}\")\nprint(f\"Test data shape: {df_test.shape}\")\nprint(f\"Original dataset shape: {df_orig.shape}\")\nprint(f\"\\nTarget distribution:\")\nprint(df_train['loan_paid_back'].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:20.048818Z","iopub.execute_input":"2025-11-08T09:11:20.049283Z","iopub.status.idle":"2025-11-08T09:11:21.76085Z","shell.execute_reply.started":"2025-11-08T09:11:20.049265Z","shell.execute_reply":"2025-11-08T09:11:21.760023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"print(\"Training Data Head:\")\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:19.655447Z","iopub.execute_input":"2025-11-08T09:00:19.655675Z","iopub.status.idle":"2025-11-08T09:00:19.680821Z","shell.execute_reply.started":"2025-11-08T09:00:19.65565Z","shell.execute_reply":"2025-11-08T09:00:19.679843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nTraining Data Info:\")\ndf_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:19.682368Z","iopub.execute_input":"2025-11-08T09:00:19.682822Z","iopub.status.idle":"2025-11-08T09:00:19.976003Z","shell.execute_reply.started":"2025-11-08T09:00:19.682798Z","shell.execute_reply":"2025-11-08T09:00:19.975425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nMissing Values in Train Data:\")\nprint(df_train.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:19.976645Z","iopub.execute_input":"2025-11-08T09:00:19.976852Z","iopub.status.idle":"2025-11-08T09:00:20.136683Z","shell.execute_reply.started":"2025-11-08T09:00:19.976834Z","shell.execute_reply":"2025-11-08T09:00:20.136141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nMissing Values in Test Data:\")\nprint(df_test.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:20.137308Z","iopub.execute_input":"2025-11-08T09:00:20.1376Z","iopub.status.idle":"2025-11-08T09:00:20.207781Z","shell.execute_reply.started":"2025-11-08T09:00:20.137584Z","shell.execute_reply":"2025-11-08T09:00:20.20707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Descriptive statistics for numerical columns\ndf_train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:20.208519Z","iopub.execute_input":"2025-11-08T09:00:20.208785Z","iopub.status.idle":"2025-11-08T09:00:20.356492Z","shell.execute_reply.started":"2025-11-08T09:00:20.208762Z","shell.execute_reply":"2025-11-08T09:00:20.355885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the Target Variable","metadata":{}},{"cell_type":"code","source":"# Distribution of the target variable 'accident_risk'\nplt.figure(figsize=(10, 6))\nsns.countplot(x='loan_paid_back', data=df_train, palette='pastel', edgecolor='black')\nplt.title('Distribution of Loan Payback')\nplt.xlabel('Loan Payback')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:20.357194Z","iopub.execute_input":"2025-11-08T09:00:20.357467Z","iopub.status.idle":"2025-11-08T09:00:20.582679Z","shell.execute_reply.started":"2025-11-08T09:00:20.357445Z","shell.execute_reply":"2025-11-08T09:00:20.582006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyzing Categorical Features","metadata":{}},{"cell_type":"code","source":"categorical_features = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\nprint(categorical_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:20.584188Z","iopub.execute_input":"2025-11-08T09:00:20.584457Z","iopub.status.idle":"2025-11-08T09:00:20.623543Z","shell.execute_reply.started":"2025-11-08T09:00:20.58444Z","shell.execute_reply":"2025-11-08T09:00:20.622748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# A more compact view of categorical features vs the target\nfig, axes = plt.subplots(3, 2, figsize=(16, 10))\naxes = axes.flatten()\ncmap = plt.get_cmap('magma')\ncolors = cmap([0.9, 0.66, 0.33])\ntarget = 'loan_paid_back'\n\nfor i, col in enumerate(categorical_features):\n    grouped = df_train.groupby(col)[target].mean()\n    axes[i].bar(grouped.index.astype(str), grouped.values, color=colors)\n    axes[i].set_ylabel(f'Mean {target}')\n    axes[i].set_title(f'{col} vs {target}')\n    axes[i].tick_params(axis='x', rotation=45)\n    \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:20.624788Z","iopub.execute_input":"2025-11-08T09:00:20.625093Z","iopub.status.idle":"2025-11-08T09:00:22.00383Z","shell.execute_reply.started":"2025-11-08T09:00:20.625076Z","shell.execute_reply":"2025-11-08T09:00:22.003054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Skeweness and Outliers ","metadata":{}},{"cell_type":"code","source":"numerical_features = df_train.select_dtypes(exclude=['object', 'category']).columns.tolist()\nnumerical_features = [col for col in numerical_features if col not in ['id', 'loan_paid_back']]\nprint(numerical_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:29.801272Z","iopub.execute_input":"2025-11-08T09:11:29.801818Z","iopub.status.idle":"2025-11-08T09:11:29.812498Z","shell.execute_reply.started":"2025-11-08T09:11:29.801794Z","shell.execute_reply":"2025-11-08T09:11:29.811886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through all numerical features\nfor col in numerical_features:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # --- Left: Distribution (Histogram + KDE) ---\n    sns.histplot(df_train[col], kde=True, ax=axes[0], color='skyblue')\n    axes[0].set_title(f\"Distribution of {col}\", fontsize=12)\n    axes[0].set_xlabel(col)\n    axes[0].set_ylabel(\"Frequency\")\n    \n    # --- Right: Boxplot (Outliers) ---\n    sns.boxplot(x=df_train[col], ax=axes[1], color='lightcoral')\n    axes[1].set_title(f\"Boxplot of {col}\", fontsize=12)\n    axes[1].set_xlabel(col)\n    \n    # Clean layout\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:30.376071Z","iopub.execute_input":"2025-11-08T09:11:30.376557Z","iopub.status.idle":"2025-11-08T09:11:45.920969Z","shell.execute_reply.started":"2025-11-08T09:11:30.376536Z","shell.execute_reply":"2025-11-08T09:11:45.920278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nskew_values_train = df_train[numerical_features].apply(lambda x: skew(x.dropna()))\nskew_values_test = df_test[numerical_features].apply(lambda x: skew(x.dropna()))\nprint('Skeweness in df_train')\nprint(skew_values_train.sort_values(ascending=False))\nprint('\\n')\nprint('Skeweness in df_test')\nprint(skew_values_test.sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:45.922227Z","iopub.execute_input":"2025-11-08T09:11:45.922457Z","iopub.status.idle":"2025-11-08T09:11:46.01239Z","shell.execute_reply.started":"2025-11-08T09:11:45.92244Z","shell.execute_reply":"2025-11-08T09:11:46.011635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # dealing with skeweness\n# skewed_cols = skew_values_train[abs(skew_values_train) > 1].index.tolist()\n# print(\"Highly skewed columns:\", skewed_cols)\n\n# for col in skewed_cols:\n#     df_train[col] = np.log1p(df_train[col])\n#     df_test[col]  = np.log1p(df_test[col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:46.013106Z","iopub.execute_input":"2025-11-08T09:11:46.013378Z","iopub.status.idle":"2025-11-08T09:11:46.016541Z","shell.execute_reply.started":"2025-11-08T09:11:46.013353Z","shell.execute_reply":"2025-11-08T09:11:46.015908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # dealing with outliers \n# for col in numerical_features:\n#     lower = df_train[col].quantile(0.01)\n#     upper = df_train[col].quantile(0.99)\n#     df_train[col] = df_train[col].clip(lower, upper)\n#     df_test[col]  = df_test[col].clip(lower, upper)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:46.018115Z","iopub.execute_input":"2025-11-08T09:11:46.018365Z","iopub.status.idle":"2025-11-08T09:11:46.028441Z","shell.execute_reply.started":"2025-11-08T09:11:46.018348Z","shell.execute_reply":"2025-11-08T09:11:46.027922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Feature Engineering ","metadata":{}},{"cell_type":"code","source":"# Save the target variable and IDs\ny = df_train['loan_paid_back']\ntest_ids = df_test['id']\n\n# Drop unnecessary columns\ndf_train_processed = df_train.drop(['id', 'loan_paid_back'], axis=1)\ndf_test_processed = df_test.drop(['id'], axis=1)\n\n# Combine for preprocessing\ncombined_df = pd.concat([df_train_processed, df_test_processed], axis=0, ignore_index=True)\nprint(f\"Combined dataset shape: {combined_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:11:46.029275Z","iopub.execute_input":"2025-11-08T09:11:46.029505Z","iopub.status.idle":"2025-11-08T09:11:46.155857Z","shell.execute_reply.started":"2025-11-08T09:11:46.02949Z","shell.execute_reply":"2025-11-08T09:11:46.155213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Engineering \n\n# 1. Create ratio features\ncombined_df['loan_to_income_ratio'] = combined_df['loan_amount'] / (combined_df['annual_income'] + 1)\ncombined_df['dti_loan_ratio'] = combined_df['debt_to_income_ratio'] * combined_df['loan_amount']\ncombined_df['credit_score_income_ratio'] = combined_df['credit_score'] / (combined_df['annual_income'] + 1)\ncombined_df['interest_loan_amount'] = combined_df['interest_rate'] * combined_df['loan_amount']\n\n# 2. Credit score binning (!!categorical feature!!)\n#combined_df['credit_score_bin'] = pd.cut(combined_df['credit_score'], \n#                                          bins=[0, 580, 669, 739, 799, 850],\n#                                          labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n\n# 3. Income binning  (!!categorical feature!!)\n#combined_df['income_bin'] = pd.qcut(combined_df['annual_income'], q=5, \n#                                     labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n\n# 4. Risk score combinations\ncombined_df['credit_debt_score'] = combined_df['credit_score'] / (combined_df['debt_to_income_ratio'] + 0.01)\ncombined_df['affordability_index'] = combined_df['annual_income'] / (combined_df['loan_amount'] * (1 + combined_df['interest_rate']/100))\n\n# 6. Test interaction features \ncombined_df['income_to_loan_ratio'] = combined_df['annual_income'] / (combined_df['loan_amount'] + 1)\ncombined_df['debt_adjusted_income'] = combined_df['annual_income'] * (1 - combined_df['debt_to_income_ratio'])\ncombined_df['credit_interest_interaction'] = combined_df['credit_score'] * combined_df['interest_rate']\ncombined_df['loan_burden'] = (combined_df['loan_amount'] * combined_df['interest_rate']) / combined_df['annual_income']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:00:58.981329Z","iopub.execute_input":"2025-11-08T09:00:58.982086Z","iopub.status.idle":"2025-11-08T09:00:59.0347Z","shell.execute_reply.started":"2025-11-08T09:00:58.982061Z","shell.execute_reply":"2025-11-08T09:00:59.03389Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Target Encoding ","metadata":{}},{"cell_type":"code","source":"import category_encoders as ce\n\n\nprint(\"Target encoding will be performed within cross-validation to prevent data leakage\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:01.807173Z","iopub.execute_input":"2025-11-08T09:01:01.807446Z","iopub.status.idle":"2025-11-08T09:01:02.273333Z","shell.execute_reply.started":"2025-11-08T09:01:01.807426Z","shell.execute_reply":"2025-11-08T09:01:02.272487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate back into training and testing sets\nX = combined_df.iloc[:len(df_train)]\nX_test = combined_df.iloc[len(df_train):]\n\nprint(f\"Training data shape before correlation: {X.shape}\")\nprint(f\"Test data shape before correlation: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:02.274465Z","iopub.execute_input":"2025-11-08T09:01:02.275228Z","iopub.status.idle":"2025-11-08T09:01:02.279858Z","shell.execute_reply.started":"2025-11-08T09:01:02.275208Z","shell.execute_reply":"2025-11-08T09:01:02.279164Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ADD Features From Original Dataset  ","metadata":{}},{"cell_type":"markdown","source":"-  will not use this part, I made a mistake in implementing the new features . Work in progress ","metadata":{}},{"cell_type":"code","source":"# print(\"Adding features from external original dataset...\")\n# global_orig_mean = df_orig['loan_paid_back'].mean()\n# orig_feature_names = []\n\n# # ============================================\n# # ONLY for CATEGORICAL features \n# # ============================================\n# for col in categorical_features:  # ← Changed from orig_cols\n#     if col not in df_orig.columns:\n#         continue\n    \n#     # Smoothed mean target encoding\n#     mean_col = f\"orig_mean_{col}\"\n#     smoothing = 10\n#     mean_map = df_orig.groupby(col)['loan_paid_back'] \\\n#                       .apply(lambda x: (x.mean() * len(x) + global_orig_mean * smoothing) /\n#                                        (len(x) + smoothing))\n    \n#     combined_df[mean_col] = combined_df[col].map(mean_map)\n#     combined_df[mean_col] = combined_df[mean_col].fillna(global_orig_mean)\n#     orig_feature_names.append(mean_col)\n    \n#     # Count encoding (fixed)\n#     count_col = f\"orig_count_{col}\"\n#     count_map = df_orig.groupby(col).size()  # ← Removed ['loan_paid_back']\n#     combined_df[count_col] = combined_df[col].map(count_map)\n#     combined_df[count_col] = combined_df[count_col].fillna(0)\n#     orig_feature_names.append(count_col)\n\n# # ============================================  \n# # For NUMERICAL features - use percentiles instead\n# # ============================================\n# for col in numerical_features:  # ← Added separate loop\n#     if col not in df_orig.columns:\n#         continue\n    \n#     # Percentile feature (less sparse than groupby)\n#     pct_col = f\"orig_pct_{col}\"\n#     combined_df[pct_col] = combined_df[col].apply(\n#         lambda x: (df_orig[col] <= x).mean() * 100 if pd.notna(x) else 50\n#     )\n#     orig_feature_names.append(pct_col)\n\n# print(f\"Added {len(orig_feature_names)} orig-based features.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:09.929359Z","iopub.execute_input":"2025-11-08T09:01:09.929611Z","iopub.status.idle":"2025-11-08T09:01:09.934043Z","shell.execute_reply.started":"2025-11-08T09:01:09.929593Z","shell.execute_reply":"2025-11-08T09:01:09.933235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:12.244761Z","iopub.execute_input":"2025-11-08T09:01:12.245356Z","iopub.status.idle":"2025-11-08T09:01:12.265514Z","shell.execute_reply.started":"2025-11-08T09:01:12.245335Z","shell.execute_reply":"2025-11-08T09:01:12.26488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dropping Highly Correlated Features \n ","metadata":{}},{"cell_type":"code","source":"\n# Remove highly correlated features (threshold = 0.85)\ncorr_matrix = combined_df.select_dtypes(include=[np.number]).corr().abs()\ncorr_matrix = corr_matrix.fillna(0)\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nthreshold = 0.85\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint(f\"Number of features to drop due to correlation > {threshold}: {len(to_drop)}\")\nprint(\"Dropped features:\", to_drop)\n\n# Create filtered dataframe with dropped columns\ncombined_df_filtered = combined_df.drop(columns=to_drop)\n\n# because I created new catagorical features let make new updated list \ncategorical_features = combined_df_filtered.select_dtypes(include=['object', 'category']).columns.tolist()\n\n#categorical_features = combined_df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# IMPORTANT: propagate the dropped-columns filtering back to df_train_fe and df_test_fe\n# so that later target-encoding inside CV and model training use the same feature set.\n# This prevents mismatches between the features used to create CV splits and the features\n# actually used during training.\n\ndf_train_fe = combined_df_filtered.iloc[:len(df_train)].reset_index(drop=True)\ndf_test_fe = combined_df_filtered.iloc[len(df_train):].reset_index(drop=True)\n\n# Separate back into training and testing sets (for diagnostic printing)\nX = combined_df_filtered.iloc[:len(df_train)]\nX_test = combined_df_filtered.iloc[len(df_train):]\n\nprint(f\"Final training data shape after correlation: {X.shape}\")\nprint(f\"Final test data shape after correlation: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:17.992159Z","iopub.execute_input":"2025-11-08T09:01:17.992444Z","iopub.status.idle":"2025-11-08T09:01:18.859775Z","shell.execute_reply.started":"2025-11-08T09:01:17.992423Z","shell.execute_reply":"2025-11-08T09:01:18.859089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:21.563197Z","iopub.execute_input":"2025-11-08T09:01:21.563938Z","iopub.status.idle":"2025-11-08T09:01:21.568525Z","shell.execute_reply.started":"2025-11-08T09:01:21.563912Z","shell.execute_reply":"2025-11-08T09:01:21.567634Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation Matrix","metadata":{}},{"cell_type":"code","source":"# Correlation heatmap for numerical features\nnumerical_features = combined_df_filtered.select_dtypes(include=np.number)\n# id is not important for correlation matrix\nnumerical_features.drop(columns=['id'], errors='ignore', inplace=True)\nplt.figure(figsize=(12, 10))\nsns.heatmap(numerical_features.corr(), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix of AFTER Threshold and Target Encoding')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:31.107116Z","iopub.execute_input":"2025-11-08T09:01:31.107391Z","iopub.status.idle":"2025-11-08T09:01:31.922332Z","shell.execute_reply.started":"2025-11-08T09:01:31.107371Z","shell.execute_reply":"2025-11-08T09:01:31.921521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Model Parameters","metadata":{}},{"cell_type":"markdown","source":"- In previous versions, I had a Random Forest model, but it was underperforming so I will not use it anymore !! bad idea !! with my model ensemble strategies\n- because removing Random Forest reduced diversity and forced all ensemble methods to collapse to the mean of three similar models\n- I will restore it","metadata":{}},{"cell_type":"markdown","source":"# LETS TRY OPTUNA","metadata":{}},{"cell_type":"code","source":"# # XGBoost LGB and CAT parameters \n# # OPTUNA for later ??\n# # in new vesrion I made some changes to parameters to see if it will improve overal AUC \n# # Optuna takes too much time so I will manually change parameters and I will learn during process what they actually do \n# xgb_params = {\n#     'objective': 'binary:logistic',\n#     \"device\": \"cuda\",          # lets try GPU\n#     'eval_metric': 'auc',\n#     'n_estimators': 4000,      # more trees    2000\n#     'early_stopping_rounds': 100,\n#     'tree_method': 'hist',\n#     'n_jobs': -1,\n#     'random_state': 42,\n#     'learning_rate': 0.02,     #smaller LR\n#     'max_depth': 3,            # safer from overfitting     4\n#     'subsample': 0.75,         # stronger regularization   0.99\n#     'colsample_bytree': 0.55,  # more column randomness    0.68\n#     'gamma': 1,                # reduce overfitting\n#     'reg_lambda': 1.0,         # stronger L2            0.0036\n#     'reg_alpha': 0.1           # slightly stronger L1   0.021\n# }\n\n# # LightGBM parameters\n# lgb_params = {\n#     'objective': 'binary',\n#     'device': 'gpu',            # lets try GPU\n#     'metric': 'auc',\n#     'boosting_type': 'gbdt',\n#     'n_estimators': 4000,       # number of trees   2000\n#     'early_stopping_rounds': 100,\n#     'learning_rate': 0.015,     # small learning rate with high number tree is better 0.03    \n#     'num_leaves': 31,           # shallower tree → higher AUC stability   50\n#     'max_depth': -1,            # let leaves control complexity           6\n#     'subsample_freq': 1, \n#     'min_child_samples': 40,    # safer split constraint                  20\n#     'subsample': 0.75,          # 0.8\n#     'colsample_bytree': 0.65,    # 0.8\n#     'reg_alpha': 0.4,          # 0.05\n#     'reg_lambda': 0.6,         # 0.01\n#     'random_state': 42,\n#     'n_jobs': -1,\n#     'verbose': -1\n# }\n\n# # CatBoost parameters\n# catboost_params = {\n#     'task_type': 'GPU',        # lets try GPU\n#     'devices': '0',            # lets try GPU\n#     'iterations': 2500,        # 2000\n#     'learning_rate': 0.02,     # learnin rate too high  0.05\n#     'depth': 6,                # too low ?  5\n#     'l2_leaf_reg': 5,          # 3\n#     'loss_function': 'Logloss',\n#     'eval_metric': 'AUC',\n#     'random_seed': 42,\n#     'od_type': 'Iter',\n#     'od_wait': 100,\n#     'grow_policy': 'Lossguide',\n#     'verbose': False,\n#     'allow_writing_files': False,\n#     'verbose': False\n# }\n\n# # Random Forest parameters\n# rf_params = {\n#     'n_estimators': 500,\n#     'max_depth': 10,\n#     'min_samples_split': 20,\n#     'min_samples_leaf': 10,\n#     'max_features': 'sqrt',\n#     'random_state': 42,\n#     'n_jobs': -1,\n#     'class_weight': 'balanced_subsample'\n# }\n\n# print(\"Model parameters defined successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:38.413946Z","iopub.execute_input":"2025-11-08T09:01:38.41468Z","iopub.status.idle":"2025-11-08T09:01:38.418853Z","shell.execute_reply.started":"2025-11-08T09:01:38.414655Z","shell.execute_reply":"2025-11-08T09:01:38.418139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# XGBoost parameters \n# OPTUNA for later ??\nxgb_params = {\n    'device': 'cuda',          # lets try GPU\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'n_estimators': 2000,\n    'early_stopping_rounds': 100,\n    'tree_method': 'hist',\n    'n_jobs': -1,\n    'random_state': 42,\n    'learning_rate': 0.1,\n    'max_depth': 4,\n    'subsample': 0.99,\n    'colsample_bytree': 0.68,\n    'gamma': 8.9e-06,\n    'reg_lambda': 0.0036,\n    'reg_alpha': 0.021\n}\n\n# LightGBM parameters\nlgb_params = {\n    'device': 'gpu',            # lets try GPU\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 2000,\n    'early_stopping_rounds': 100,\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'max_depth': 5,\n    'min_child_samples': 20,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 0.01,\n    'reg_lambda': 0.01,\n    'random_state': 42,\n    'n_jobs': -1,\n    'verbose': -1\n}\n\n#lgb_params = {\n#    'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n#    'max_depth': 6, 'num_leaves': 50, 'learning_rate': 0.03,\n#    'colsample_bytree': 0.8, 'subsample': 0.8,\n#    'subsample_freq': 1, 'min_child_samples': 20, 'reg_alpha': 0.05,\n#    'reg_lambda': 0.1, 'random_state': 42,'n_estimators': 2000,\n#    'n_jobs': -1, \n#    'device': 'gpu',\n#    'verbose': -1,\n#}\n\n# CatBoost parameters\ncatboost_params = {\n    'task_type': 'GPU',        # lets try GPU\n    'devices': '0',            # lets try GPU\n    'metric_period': 5,        # lets try GPU it will not make a warning each loop  common performance warning, not an error. It's telling you that AUC can't be calculated on the GPU,\n    'iterations': 2000,\n    'learning_rate': 0.05,\n    'depth': 5,\n    'l2_leaf_reg': 3,\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'random_seed': 42,\n    'od_type': 'Iter',\n    'od_wait': 100,\n    'grow_policy': 'Lossguide',\n    'verbose': False,\n    'allow_writing_files': False\n}\n\n# Random Forest parameters\nrf_params = {\n    'n_estimators': 500,\n    'max_depth': 10,\n    'min_samples_split': 20,\n    'min_samples_leaf': 10,\n    'max_features': 'sqrt',\n    'random_state': 42,\n    'n_jobs': -1,\n    'class_weight': 'balanced_subsample'\n}\n\nprint(\"Model parameters defined successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:49.511978Z","iopub.execute_input":"2025-11-08T09:01:49.512648Z","iopub.status.idle":"2025-11-08T09:01:49.51956Z","shell.execute_reply.started":"2025-11-08T09:01:49.512622Z","shell.execute_reply":"2025-11-08T09:01:49.518767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up Optuna logging\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\ndef objective_xgboost(trial, X_train_full, y_train_full, categorical_features, n_folds=3):\n    \"\"\"Objective function for XGBoost optimization\"\"\"\n    \n    # Suggest hyperparameters\n    params = {\n        'objective': 'binary:logistic',\n        'device': 'cuda',\n        'eval_metric': 'auc',\n        'tree_method': 'hist',\n        'n_jobs': -1,\n        'random_state': 42,\n        \n        # Parameters to optimize\n        'n_estimators': trial.suggest_int('n_estimators', 100, 3000, step=100),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 5),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10, log=True),\n        'early_stopping_rounds': 50\n    }\n    \n    # Internal CV for Optuna\n    skf_inner = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    cv_scores = []\n    \n    for train_idx, val_idx in skf_inner.split(X_train_full, y_train_full):\n        X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n        y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n        \n        # Apply target encoding\n        target_encoder = ce.TargetEncoder(cols=categorical_features)\n        target_encoder.fit(X_train[categorical_features], y_train)\n        \n        X_train_encoded = X_train.copy()\n        X_val_encoded = X_val.copy()\n        X_train_encoded[categorical_features] = target_encoder.transform(X_train[categorical_features])\n        X_val_encoded[categorical_features] = target_encoder.transform(X_val[categorical_features])\n        \n        # Train model\n        model = xgb.XGBClassifier(**params)\n        model.fit(\n            X_train_encoded, y_train,\n            eval_set=[(X_val_encoded, y_val)],\n            verbose=False\n        )\n        \n        # Predict and calculate AUC\n        val_preds = model.predict_proba(X_val_encoded)[:, 1]\n        auc = roc_auc_score(y_val, val_preds)\n        cv_scores.append(auc)\n    \n    return np.mean(cv_scores)\n\ndef objective_lightgbm(trial, X_train_full, y_train_full, categorical_features, n_folds=3):\n    \"\"\"Objective function for LightGBM optimization\"\"\"\n    \n    params = {\n        'objective': 'binary',\n        'device': 'gpu',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'random_state': 42,\n        'n_jobs': -1,\n        'verbose': -1,\n        \n        # Parameters to optimize\n        'n_estimators': trial.suggest_int('n_estimators', 100, 3000, step=100),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10, log=True),\n        'early_stopping_rounds': 50\n    }\n    \n    # Internal CV for Optuna\n    skf_inner = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    cv_scores = []\n    \n    for train_idx, val_idx in skf_inner.split(X_train_full, y_train_full):\n        X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n        y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n        \n        # Apply target encoding\n        target_encoder = ce.TargetEncoder(cols=categorical_features)\n        target_encoder.fit(X_train[categorical_features], y_train)\n        \n        X_train_encoded = X_train.copy()\n        X_val_encoded = X_val.copy()\n        X_train_encoded[categorical_features] = target_encoder.transform(X_train[categorical_features])\n        X_val_encoded[categorical_features] = target_encoder.transform(X_val[categorical_features])\n        \n        # Train model\n        model = lgb.LGBMClassifier(**params)\n        model.fit(\n            X_train_encoded, y_train,\n            eval_set=[(X_val_encoded, y_val)],\n            callbacks=[lgb.log_evaluation(0)]\n        )\n        \n        val_preds = model.predict_proba(X_val_encoded)[:, 1]\n        auc = roc_auc_score(y_val, val_preds)\n        cv_scores.append(auc)\n    \n    return np.mean(cv_scores)\n\ndef objective_catboost(trial, X_train_full, y_train_full, categorical_features, n_folds=3):\n    \"\"\"Objective function for CatBoost optimization\"\"\"\n    \n    params = {\n        'task_type': 'GPU',\n        'devices': '0',\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'random_seed': 42,\n        'od_type': 'Iter',\n        'verbose': False,\n        'allow_writing_files': False,\n        'metric_period': 5,\n        \n        # Parameters to optimize\n        'iterations': trial.suggest_int('iterations', 100, 3000, step=100),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n        'border_count': trial.suggest_int('border_count', 32, 255),\n        'od_wait': trial.suggest_int('od_wait', 20, 100)\n    }\n    \n    # Add grow_policy conditionally\n    if params['depth'] > 6:\n        params['grow_policy'] = trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide'])\n    \n    # Internal CV for Optuna\n    skf_inner = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    cv_scores = []\n    \n    for train_idx, val_idx in skf_inner.split(X_train_full, y_train_full):\n        X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n        y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n        \n        # Apply target encoding\n        target_encoder = ce.TargetEncoder(cols=categorical_features)\n        target_encoder.fit(X_train[categorical_features], y_train)\n        \n        X_train_encoded = X_train.copy()\n        X_val_encoded = X_val.copy()\n        X_train_encoded[categorical_features] = target_encoder.transform(X_train[categorical_features])\n        X_val_encoded[categorical_features] = target_encoder.transform(X_val[categorical_features])\n        \n        # Train model\n        model = cb.CatBoostClassifier(**params)\n        model.fit(\n            X_train_encoded, y_train,\n            eval_set=(X_val_encoded, y_val),\n            use_best_model=True,\n            verbose=False\n        )\n        \n        val_preds = model.predict_proba(X_val_encoded)[:, 1]\n        auc = roc_auc_score(y_val, val_preds)\n        cv_scores.append(auc)\n    \n    return np.mean(cv_scores)\n\n# Function to run optimization and get best params\ndef optimize_model(model_name, X_train, y_train, categorical_features, n_trials=100):\n    \"\"\"Run Optuna optimization for a specific model\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Optimizing {model_name.upper()} with Optuna\")\n    print(f\"{'='*60}\")\n    \n    study = optuna.create_study(\n        direction='maximize',\n        sampler=TPESampler(seed=42)\n    )\n    \n    if model_name == 'xgboost':\n        objective = lambda trial: objective_xgboost(trial, X_train, y_train, categorical_features)\n    elif model_name == 'lightgbm':\n        objective = lambda trial: objective_lightgbm(trial, X_train, y_train, categorical_features)\n    elif model_name == 'catboost':\n        objective = lambda trial: objective_catboost(trial, X_train, y_train, categorical_features)\n    else:\n        raise ValueError(f\"Unknown model: {model_name}\")\n    \n    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n    \n    print(f\"\\nBest trial:\")\n    print(f\"  AUC: {study.best_value:.5f}\")\n    print(f\"  Params: {study.best_params}\")\n    \n    return study.best_params\n\n# MAIN TRAINING PIPELINE WITH OPTUNA\n# First, optimize hyperparameters using a subset of data (optional for speed)\nOPTIMIZE_PARAMS = True  # Set to False to use your manual parameters\nN_OPTUNA_TRIALS = 100   # Number of trials for each model (increase for better results)\n\nif OPTIMIZE_PARAMS:\n    # Use a subset for faster optimization (optional)\n    sample_size = min(100000, len(df_train_fe))\n    sample_idx = np.random.choice(df_train_fe.index, sample_size, replace=False)\n    X_sample = df_train_fe.loc[sample_idx]\n    y_sample = y.loc[sample_idx]\n    \n    # Optimize each model\n    best_params = {}\n    \n    # XGBoost\n    xgb_best = optimize_model('xgboost', X_sample, y_sample, categorical_features, N_OPTUNA_TRIALS)\n    xgb_params.update(xgb_best)\n    best_params['xgboost'] = xgb_best\n    \n    # LightGBM\n    lgb_best = optimize_model('lightgbm', X_sample, y_sample, categorical_features, N_OPTUNA_TRIALS)\n    lgb_params.update(lgb_best)\n    best_params['lightgbm'] = lgb_best\n    \n    # CatBoost\n    cb_best = optimize_model('catboost', X_sample, y_sample, categorical_features, N_OPTUNA_TRIALS)\n    catboost_params.update(cb_best)\n    best_params['catboost'] = cb_best\n    \n    # Save best parameters for future use\n    import json\n    with open('best_params.json', 'w') as f:\n        json.dump(best_params, f, indent=2)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"OPTIMIZATION COMPLETE - Best Parameters Saved\")\n    print(\"=\"*60)\n\n# Now run your original training loop with optimized parameters\n# (Your existing training code continues here...)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:31:33.298951Z","iopub.execute_input":"2025-11-07T21:31:33.299697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catboost_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:08:25.536941Z","iopub.execute_input":"2025-11-07T21:08:25.537602Z","iopub.status.idle":"2025-11-07T21:08:25.541914Z","shell.execute_reply.started":"2025-11-07T21:08:25.537579Z","shell.execute_reply":"2025-11-07T21:08:25.541337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:08:26.664176Z","iopub.execute_input":"2025-11-07T21:08:26.664402Z","iopub.status.idle":"2025-11-07T21:08:26.669077Z","shell.execute_reply.started":"2025-11-07T21:08:26.664386Z","shell.execute_reply":"2025-11-07T21:08:26.668377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:08:29.014861Z","iopub.execute_input":"2025-11-07T21:08:29.015177Z","iopub.status.idle":"2025-11-07T21:08:29.019884Z","shell.execute_reply.started":"2025-11-07T21:08:29.015157Z","shell.execute_reply":"2025-11-07T21:08:29.019158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Stratified K-Fold Cross Validation with Multiple Models","metadata":{}},{"cell_type":"code","source":"# Setup Stratifield K-Fold Cross-Validation\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# Initialize storage for out-of-fold predictions and test predictions\noof_predictions = {\n    'xgboost': np.zeros(X.shape[0]),\n    'lightgbm': np.zeros(X.shape[0]),\n    'catboost': np.zeros(X.shape[0]),\n    'random_forest': np.zeros(X.shape[0])\n}\n\ntest_predictions = {\n    'xgboost': np.zeros(X_test.shape[0]),\n    'lightgbm': np.zeros(X_test.shape[0]),\n    'catboost': np.zeros(X_test.shape[0]),\n    'random_forest': np.zeros(X_test.shape[0])\n}\n\n# Store models for each fold\nmodels = {\n    'xgboost': [],\n    'lightgbm': [],\n    'catboost': [],\n    'random_forest': []\n}\n\n# Store AUC scores for each model\nauc_scores = {\n    'xgboost': [],\n    'lightgbm': [],\n    'catboost': [],\n    'random_forest': []\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:01:58.007955Z","iopub.execute_input":"2025-11-08T09:01:58.008428Z","iopub.status.idle":"2025-11-08T09:01:58.0173Z","shell.execute_reply.started":"2025-11-08T09:01:58.008405Z","shell.execute_reply":"2025-11-08T09:01:58.01666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"\\n{'='*60}\")\n    print(f\"FOLD {fold+1}/{N_SPLITS}\")\n    print(f\"{'='*60}\")\n    \n    # Get fold data\n    X_train, X_val = df_train_fe.iloc[train_idx], df_train_fe.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Initialize target encoder for this fold\n    target_encoder = ce.TargetEncoder(cols=categorical_features)\n    \n    # Fit encoder on training data only\n    target_encoder.fit(X_train[categorical_features], y_train)\n    \n    # Transform train, validation, and test data using the same encoder\n    X_train_encoded = X_train.copy()\n    X_val_encoded = X_val.copy()\n    X_test_encoded = df_test_fe.copy()\n    \n    X_train_encoded[categorical_features] = target_encoder.transform(X_train[categorical_features])\n    X_val_encoded[categorical_features] = target_encoder.transform(X_val[categorical_features])\n    X_test_encoded[categorical_features] = target_encoder.transform(df_test_fe[categorical_features])\n    \n    # =====================\n    # XGBoost\n    # =====================\n    print(\"\\nTraining XGBoost...\")\n    xgb_model = xgb.XGBClassifier(**xgb_params)\n    xgb_model.fit(\n        X_train_encoded, y_train,\n        eval_set=[(X_val_encoded, y_val)],\n        verbose=False\n    )\n    val_preds = xgb_model.predict_proba(X_val_encoded)[:, 1]\n    oof_predictions['xgboost'][val_idx] = val_preds\n    test_predictions['xgboost'] += xgb_model.predict_proba(X_test_encoded)[:, 1] / N_SPLITS\n    auc = roc_auc_score(y_val, val_preds)\n    auc_scores['xgboost'].append(auc)\n    print(f\"XGBoost Fold {fold+1} AUC: {auc:.5f}\")\n    models['xgboost'].append(xgb_model)\n    \n    # =====================\n    # LightGBM\n    # =====================\n    print(\"\\nTraining LightGBM...\")\n    lgb_model = lgb.LGBMClassifier(**lgb_params)\n    lgb_model.fit(\n        X_train_encoded, y_train,\n        eval_set=[(X_val_encoded, y_val)],\n        callbacks=[lgb.log_evaluation(0)]\n    )\n    val_preds = lgb_model.predict_proba(X_val_encoded)[:, 1]\n    oof_predictions['lightgbm'][val_idx] = val_preds\n    test_predictions['lightgbm'] += lgb_model.predict_proba(X_test_encoded)[:, 1] / N_SPLITS\n    auc = roc_auc_score(y_val, val_preds)\n    auc_scores['lightgbm'].append(auc)\n    print(f\"LightGBM Fold {fold+1} AUC: {auc:.5f}\")\n    models['lightgbm'].append(lgb_model)\n    \n    # =====================\n    # CatBoost\n    # =====================\n    print(\"\\nTraining CatBoost...\")\n    cb_model = cb.CatBoostClassifier(**catboost_params)\n    cb_model.fit(\n        X_train_encoded, y_train,\n        eval_set=(X_val_encoded, y_val),\n        use_best_model=True,\n        verbose=False\n    )\n    val_preds = cb_model.predict_proba(X_val_encoded)[:, 1]\n    oof_predictions['catboost'][val_idx] = val_preds\n    test_predictions['catboost'] += cb_model.predict_proba(X_test_encoded)[:, 1] / N_SPLITS\n    auc = roc_auc_score(y_val, val_preds)\n    auc_scores['catboost'].append(auc)\n    print(f\"CatBoost Fold {fold+1} AUC: {auc:.5f}\")\n    models['catboost'].append(cb_model)\n    \n    # =====================\n    # Random Forest\n    # =====================\n    print(\"\\nTraining Random Forest...\")\n    rf_model = RandomForestClassifier(**rf_params)\n    rf_model.fit(X_train_encoded, y_train)\n    val_preds = rf_model.predict_proba(X_val_encoded)[:, 1]\n    oof_predictions['random_forest'][val_idx] = val_preds\n    test_predictions['random_forest'] += rf_model.predict_proba(X_test_encoded)[:, 1] / N_SPLITS\n    auc = roc_auc_score(y_val, val_preds)\n    auc_scores['random_forest'].append(auc)\n    print(f\"Random Forest Fold {fold+1} AUC: {auc:.5f}\")\n    models['random_forest'].append(rf_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T09:02:03.640482Z","iopub.execute_input":"2025-11-08T09:02:03.64077Z","execution_failed":"2025-11-08T09:07:49.256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print CV results for each model\nprint(\"\\n\" + \"=\"*60)\nprint(\"CROSS-VALIDATION RESULTS\")\nprint(\"=\"*60)\n\nfor model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest']:\n    mean_auc = np.mean(auc_scores[model_name])\n    std_auc = np.std(auc_scores[model_name])\n    print(f\"\\n{model_name.upper()}:\")\n    print(f\"  Average CV AUC: {mean_auc:.5f} (+/- {std_auc:.5f})\")\n    print(f\"  Fold scores: {[f'{score:.5f}' for score in auc_scores[model_name]]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.279498Z","iopub.status.idle":"2025-11-07T21:29:54.279818Z","shell.execute_reply.started":"2025-11-07T21:29:54.27965Z","shell.execute_reply":"2025-11-07T21:29:54.279666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Model Ensemble Strategies","metadata":{}},{"cell_type":"markdown","source":"## stacking meta-learner (I will use later )","metadata":{}},{"cell_type":"code","source":"# # Meta features from OOF predictions\n# p_xgb = oof_predictions['xgboost']\n# p_lgb = oof_predictions['lightgbm']\n# p_cb  = oof_predictions['catboost']\n\n# X_meta = np.column_stack([p_xgb, p_lgb, p_cb\n\n# from sklearn.linear_model import LogisticRegression\n\n# meta = LogisticRegression(max_iter=1000)\n# meta.fit(X_meta, y)\n\n# xgb_test_pred = test_predictions['xgboost']\n# lgb_test_pred = test_predictions['lightgbm']\n# cb_test_pred  = test_predictions['catboost']\n\n# X_meta_test = np.column_stack([\n#     xgb_test_pred,\n#     lgb_test_pred,\n#     cb_test_pred\n# ])\n\n# final_prob = meta.predict_proba(X_meta_test)[:, 1]\n\n\n\n# #calibration\n# from sklearn.isotonic import IsotonicRegression\n\n# iso = IsotonicRegression(out_of_bounds='clip')\n# iso.fit(meta.predict_proba(X_meta)[:,1], y)\n\n# final_prob = iso.predict(final_prob)\n\n\n# # ============================================================\n# # Final Submission for Stacking Meta-Model\n# # ============================================================\n\n# # Ensure predictions are within [0, 1] range\n# final_predictions = np.clip(final_prob, 0, 1)\n\n# # Create submission dataframe\n# submission_df = pd.DataFrame({\n#     'id': test_ids,\n#     'loan_paid_back': final_predictions\n# })\n\n# # Save the submission file\n# submission_df.to_csv(\"submission.csv\", index=False)\n# print(\"\\nSubmission file created: submission.csv\")\n\n# # Show preview + statistics\n# print(\"\\nSample of final submission:\")\n# print(submission_df.head(10))\n\n# print(\"\\nPrediction statistics:\")\n# print(submission_df['loan_paid_back'].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.280925Z","iopub.status.idle":"2025-11-07T21:29:54.281148Z","shell.execute_reply.started":"2025-11-07T21:29:54.281044Z","shell.execute_reply":"2025-11-07T21:29:54.281053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4 Strategies","metadata":{}},{"cell_type":"code","source":"# Strategy 1: Simple Average Ensemble\nprint(\"\\nEnsemble Strategy 1: Simple Average\")\nprint(\"=\"*40)\n\nensemble_oof_simple = np.mean([\n    oof_predictions['xgboost'],\n    oof_predictions['lightgbm'],\n    oof_predictions['catboost'],\n    oof_predictions['random_forest']\n], axis=0)\n\nensemble_test_simple = np.mean([\n    test_predictions['xgboost'],\n    test_predictions['lightgbm'],\n    test_predictions['catboost'],\n    test_predictions['random_forest']\n], axis=0)\n\nensemble_auc_simple = roc_auc_score(y, ensemble_oof_simple)\nprint(f\"Simple Average Ensemble CV AUC: {ensemble_auc_simple:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.28218Z","iopub.status.idle":"2025-11-07T21:29:54.282431Z","shell.execute_reply.started":"2025-11-07T21:29:54.282314Z","shell.execute_reply":"2025-11-07T21:29:54.282326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Strategy 2: Weighted Average Ensemble (based on CV performance)\nprint(\"\\nEnsemble Strategy 2: Weighted Average (based on CV performance)\")\nprint(\"=\"*40)\n\n# Calculate weights based on CV AUC scores\nmodel_weights = {}\ntotal_auc = 0\nfor model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest']:\n    mean_auc = np.mean(auc_scores[model_name])\n    model_weights[model_name] = mean_auc\n    total_auc += mean_auc\n\n# Normalize weights to sum to 1\nfor model_name in model_weights:\n    model_weights[model_name] /= total_auc\n    print(f\"{model_name} weight: {model_weights[model_name]:.4f}\")\n\n# Create weighted ensemble predictions\nensemble_oof_weighted = (\n    model_weights['xgboost'] * oof_predictions['xgboost'] +\n    model_weights['lightgbm'] * oof_predictions['lightgbm'] +\n    model_weights['catboost'] * oof_predictions['catboost'] +\n    model_weights['random_forest'] * oof_predictions['random_forest']\n)\n\nensemble_test_weighted = (\n    model_weights['xgboost'] * test_predictions['xgboost'] +\n    model_weights['lightgbm'] * test_predictions['lightgbm'] +\n    model_weights['catboost'] * test_predictions['catboost'] +\n    model_weights['random_forest'] * test_predictions['random_forest']\n)\n\nensemble_auc_weighted = roc_auc_score(y, ensemble_oof_weighted)\nprint(f\"\\nWeighted Average Ensemble CV AUC: {ensemble_auc_weighted:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.283226Z","iopub.status.idle":"2025-11-07T21:29:54.283587Z","shell.execute_reply.started":"2025-11-07T21:29:54.283392Z","shell.execute_reply":"2025-11-07T21:29:54.283413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Strategy 3: Rank Average Ensemble\nprint(\"\\nEnsemble Strategy 3: Rank Average\")\nprint(\"=\"*40)\n\nfrom scipy.stats import rankdata\n\n# Rank the predictions for each model\noof_ranks = {\n    model_name: rankdata(oof_predictions[model_name]) / len(oof_predictions[model_name])\n    for model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest']\n}\n\ntest_ranks = {\n    model_name: rankdata(test_predictions[model_name]) / len(test_predictions[model_name])\n    for model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest']\n}\n\n# Average the ranks\nensemble_oof_rank = np.mean(list(oof_ranks.values()), axis=0)\nensemble_test_rank = np.mean(list(test_ranks.values()), axis=0)\n\nensemble_auc_rank = roc_auc_score(y, ensemble_oof_rank)\nprint(f\"Rank Average Ensemble CV AUC: {ensemble_auc_rank:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.284616Z","iopub.status.idle":"2025-11-07T21:29:54.284925Z","shell.execute_reply.started":"2025-11-07T21:29:54.284745Z","shell.execute_reply":"2025-11-07T21:29:54.28476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Strategy 4: Optimized Weighted Ensemble using scipy.optimize\nprint(\"\\nEnsemble Strategy 4: Optimized Weighted Ensemble\")\nprint(\"=\"*40)\n\nfrom scipy.optimize import minimize\n\ndef ensemble_score(weights):\n    \"\"\"Calculate negative AUC score for optimization (minimize negative = maximize positive)\"\"\"\n    ensemble_pred = (\n        weights[0] * oof_predictions['xgboost'] +\n        weights[1] * oof_predictions['lightgbm'] +\n        weights[2] * oof_predictions['catboost'] +\n        weights[3] * oof_predictions['random_forest']\n    )\n    return -roc_auc_score(y, ensemble_pred)\n\n# Initial weights (equal)\ninit_weights = [0.25, 0.25, 0.25, 0.25]\n#init_weights = [1/3, 1/3, 1/3]\n\n# Constraints: weights sum to 1 and are between 0 and 1\nconstraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\nbounds = [(0, 1)] * 4\n\n# Optimize\nresult = minimize(ensemble_score, init_weights, method='SLSQP', \n                 bounds=bounds, constraints=constraints)\n\noptimal_weights = result.x\nprint(f\"Optimal weights:\")\nprint(f\"  XGBoost: {optimal_weights[0]:.4f}\")\nprint(f\"  LightGBM: {optimal_weights[1]:.4f}\")\nprint(f\"  CatBoost: {optimal_weights[2]:.4f}\")\nprint(f\"  Random Forest: {optimal_weights[3]:.4f}\")\n\n# Create optimized ensemble predictions\nensemble_oof_optimized = (\n    optimal_weights[0] * oof_predictions['xgboost'] +\n    optimal_weights[1] * oof_predictions['lightgbm'] +\n    optimal_weights[2] * oof_predictions['catboost'] +\n    optimal_weights[3] * oof_predictions['random_forest']\n)\n\nensemble_test_optimized = (\n    optimal_weights[0] * test_predictions['xgboost'] +\n    optimal_weights[1] * test_predictions['lightgbm'] +\n    optimal_weights[2] * test_predictions['catboost'] +\n    optimal_weights[3] * test_predictions['random_forest']\n)\n\nensemble_auc_optimized = roc_auc_score(y, ensemble_oof_optimized)\nprint(f\"\\nOptimized Ensemble CV AUC: {ensemble_auc_optimized:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.285916Z","iopub.status.idle":"2025-11-07T21:29:54.28614Z","shell.execute_reply.started":"2025-11-07T21:29:54.28602Z","shell.execute_reply":"2025-11-07T21:29:54.286029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Compare All Models and Ensemble Strategies","metadata":{}},{"cell_type":"code","source":"# Summary of all model performances\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL PERFORMANCE COMPARISON\")\nprint(\"=\"*60)\n\n# Individual models\nprint(\"\\nIndividual Models:\")\nprint(\"-\"*30)\nfor model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest']:\n    mean_auc = np.mean(auc_scores[model_name])\n    print(f\"{model_name.ljust(15)}: {mean_auc:.5f}\")\n\n# Ensemble strategies\nprint(\"\\nEnsemble Strategies:\")\nprint(\"-\"*30)\nprint(f\"Simple Average  : {ensemble_auc_simple:.5f}\")\nprint(f\"Weighted Average: {ensemble_auc_weighted:.5f}\")\nprint(f\"Rank Average    : {ensemble_auc_rank:.5f}\")\nprint(f\"Optimized       : {ensemble_auc_optimized:.5f}\")\n\n# Find best strategy\nbest_ensemble_scores = {\n    'Simple': ensemble_auc_simple,\n    'Weighted': ensemble_auc_weighted,\n    'Rank': ensemble_auc_rank,\n    'Optimized': ensemble_auc_optimized\n}\nbest_strategy = max(best_ensemble_scores, key=best_ensemble_scores.get)\nprint(f\"\\nBest Ensemble Strategy: {best_strategy} with AUC = {best_ensemble_scores[best_strategy]:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.287092Z","iopub.status.idle":"2025-11-07T21:29:54.28737Z","shell.execute_reply.started":"2025-11-07T21:29:54.287215Z","shell.execute_reply":"2025-11-07T21:29:54.287228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Feature Importance Analysis","metadata":{}},{"cell_type":"code","source":"# Get feature importance from tree-based models\nimport matplotlib.pyplot as plt\n\n# Average feature importance across folds for each model\nfeature_importance = pd.DataFrame()\nfeature_importance['feature'] = X.columns\n\n# XGBoost importance\nxgb_importance = np.mean([model.feature_importances_ for model in models['xgboost']], axis=0)\nfeature_importance['xgboost'] = xgb_importance\n\n# LightGBM importance\nlgb_importance = np.mean([model.feature_importances_ for model in models['lightgbm']], axis=0)\nfeature_importance['lightgbm'] = lgb_importance\n\n# CatBoost importance\ncb_importance = np.mean([model.get_feature_importance() for model in models['catboost']], axis=0)\nfeature_importance['catboost'] = cb_importance\n\n# Random Forest importance\nrf_importance = np.mean([model.feature_importances_ for model in models['random_forest']], axis=0)\nfeature_importance['random_forest'] = rf_importance\n\n# Average importance across all models\nfeature_importance['average'] = feature_importance[['xgboost', 'lightgbm', 'catboost', 'random_forest']].mean(axis=1)\n#feature_importance['average'] = feature_importance[['xgboost', 'lightgbm', 'catboost']].mean(axis=1)\n\n# Sort by average importance and display top 20\nfeature_importance_sorted = feature_importance.sort_values('average', ascending=False)\n\nprint(\"\\nTop 20 Most Important Features (Average across all models):\")\nprint(\"=\"*60)\nprint(feature_importance_sorted[['feature', 'average']].head(20).to_string(index=False))\n\n# Plot top 15 features\nplt.figure(figsize=(10, 8))\ntop_features = feature_importance_sorted.head(15)\nplt.barh(range(len(top_features)), top_features['average'], color='steelblue')\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Average Feature Importance')\nplt.title('Top 15 Most Important Features (Averaged Across All Models)')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.288794Z","iopub.status.idle":"2025-11-07T21:29:54.289073Z","shell.execute_reply.started":"2025-11-07T21:29:54.288952Z","shell.execute_reply":"2025-11-07T21:29:54.288965Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Create Final Submission Files","metadata":{}},{"cell_type":"code","source":"# Select the best ensemble strategy for final submission\nif best_strategy == 'Simple':\n    final_predictions = ensemble_test_simple\nelif best_strategy == 'Weighted':\n    final_predictions = ensemble_test_weighted\nelif best_strategy == 'Rank':\n    final_predictions = ensemble_test_rank\nelse:\n    final_predictions = ensemble_test_optimized\n\n# Ensure predictions are within [0, 1] range\nfinal_predictions = np.clip(final_predictions, 0, 1)\n\n# Create submission dataframe\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'loan_paid_back': final_predictions\n})\n\n# Save the best ensemble submission\n#submission_df.to_csv(f'submission_ensemble_{best_strategy.lower()}.csv', index=False)\nsubmission_df.to_csv(f'submission.csv', index=False)\nprint(f\"\\nSubmission file created: submission.csv\")\nprint(f\"Using {best_strategy} ensemble strategy with CV AUC: {best_ensemble_scores[best_strategy]:.5f}\")\n\n# Also save individual model predictions for comparison\nfor model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest']:\n    individual_submission = pd.DataFrame({\n        'id': test_ids,\n        'loan_paid_back': np.clip(test_predictions[model_name], 0, 1)\n    })\n    individual_submission.to_csv(f'submission_{model_name}.csv', index=False)\n    print(f\"Individual submission saved: submission_{model_name}.csv\")\n\n# Display sample of final submission\nprint(\"\\nSample of final submission:\")\nprint(submission_df.head(10))\nprint(f\"\\nPrediction statistics:\")\nprint(submission_df['loan_paid_back'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.290222Z","iopub.status.idle":"2025-11-07T21:29:54.290461Z","shell.execute_reply.started":"2025-11-07T21:29:54.29034Z","shell.execute_reply":"2025-11-07T21:29:54.290352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Model Performance Visualization","metadata":{}},{"cell_type":"code","source":"# Create comparison plots\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Model performance comparison\nax1 = axes[0, 0]\nmodel_names = list(auc_scores.keys()) + ['Simple\\nEnsemble', 'Weighted\\nEnsemble', 'Rank\\nEnsemble', 'Optimized\\nEnsemble']\nmodel_scores = [np.mean(auc_scores[m]) for m in auc_scores.keys()] + [ensemble_auc_simple, ensemble_auc_weighted, ensemble_auc_rank, ensemble_auc_optimized]\ncolors = ['skyblue'] * 3 + ['lightcoral'] * 4\nbars = ax1.bar(model_names, model_scores, color=colors)\nax1.set_ylabel('AUC Score')\nax1.set_title('Model Performance Comparison')\nax1.set_ylim([min(model_scores) * 0.98, max(model_scores) * 1.005])\nax1.tick_params(axis='x', rotation=45)\n\n# Add value labels on bars\nfor bar, score in zip(bars, model_scores):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n            f'{score:.5f}', ha='center', va='bottom', fontsize=8)\n\n# Plot 2: Cross-validation scores distribution\nax2 = axes[0, 1]\ncv_data = [auc_scores[m] for m in ['xgboost', 'lightgbm', 'catboost', 'random_forest']]\nbp = ax2.boxplot(cv_data, labels=['XGBoost', 'LightGBM', 'CatBoost', 'RF'])\nax2.set_ylabel('AUC Score')\nax2.set_title('Cross-Validation Score Distribution')\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Prediction distribution\nax3 = axes[1, 0]\nax3.hist(final_predictions, bins=50, edgecolor='black', alpha=0.7)\nax3.set_xlabel('Predicted Probability')\nax3.set_ylabel('Frequency')\nax3.set_title('Distribution of Final Test Predictions')\nax3.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='0.5 threshold')\nax3.legend()\n\nfig.delaxes(axes[1, 1])\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T21:29:54.291409Z","iopub.status.idle":"2025-11-07T21:29:54.291669Z","shell.execute_reply.started":"2025-11-07T21:29:54.291541Z","shell.execute_reply":"2025-11-07T21:29:54.29155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}