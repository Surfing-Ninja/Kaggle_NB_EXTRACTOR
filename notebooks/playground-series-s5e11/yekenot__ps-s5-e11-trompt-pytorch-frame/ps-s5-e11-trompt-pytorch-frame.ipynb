{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[PyTorch Frame](https://github.com/pyg-team/pytorch-frame) is a deep learning extension for PyTorch, designed for heterogeneous tabular data with different column types. [Trompt](https://arxiv.org/abs/2305.18446) - a novel tabular NN architecture inspired by\nprompt learning of language models.","metadata":{}},{"cell_type":"markdown","source":"## Setup Libraries","metadata":{}},{"cell_type":"code","source":"%%time\n!pip install -q pytorch_frame","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torch_frame\nfrom torch_frame import stype\nfrom torch_frame.data.loader import DataLoader\nfrom torch_frame.nn.models.trompt import Trompt\nfrom torch_frame.data.stats import compute_col_stats\nfrom torch_frame.data import DataFrameToTensorFrameConverter\n\nprint(\"PyTorch       version:\", torch.__version__)\nprint(\"PyTorch Frame version:\", torch_frame.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape :\", test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess Features","metadata":{}},{"cell_type":"code","source":"%%time\nTARGET = 'loan_paid_back'\nX = train.drop(['id', TARGET], axis=1)\ny = train[TARGET]; train_id = train.id\nX_test = test.drop(['id'], axis=1)\ndel train, test\nprint(\"X      shape:\", X.shape)\nprint(\"X_test shape:\", X_test.shape, \"\\n\")\n\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\nnum_cols = X.select_dtypes(exclude=['object']).columns.tolist()\nprint(\"len(cat_cols):\", len(cat_cols))\nprint(\"len(num_cols):\", len(num_cols), \"\\n\")\n\ncol_to_stype = {c: stype.numerical for c in num_cols}\ncol_to_stype.update({c: stype.categorical for c in cat_cols})\ncol_to_stype[TARGET] = stype.numerical\n\nscaler = MinMaxScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols]).astype(np.float32)\nX_test[num_cols] = scaler.transform(X_test[num_cols]).astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    FOLDS = 5\n    EPOCHS = 4\n    LR = 1e-3\n    WD = 1e-4\n    WARMUP = 0.2\n    BATCH = 512\n    CHANNELS = 64\n    PROMPTS = 8\n    LAYERS = 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train K-Fold","metadata":{}},{"cell_type":"code","source":"%%time\nskf = StratifiedKFold(n_splits=CFG.FOLDS, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test))\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n--- Fold {fold}/{CFG.FOLDS} ---\")\n\n    train_df = pd.concat([X.iloc[tr_idx].reset_index(drop=True),\n                          pd.Series(y[tr_idx], name=TARGET).reset_index(drop=True)], axis=1)\n    val_df   = pd.concat([X.iloc[val_idx].reset_index(drop=True),\n                          pd.Series(y[val_idx], name=TARGET).reset_index(drop=True)], axis=1)\n\n    col_stats = {c: compute_col_stats(train_df[c], stype=col_to_stype[c]) for c in train_df.columns if c in col_to_stype}\n\n    converter = DataFrameToTensorFrameConverter(col_stats=col_stats, target_col=TARGET, col_to_stype=col_to_stype)\n    tf_train = converter(train_df)\n    tf_val   = converter(val_df)\n    tf_test  = converter(X_test)\n\n    train_loader = DataLoader(tf_train, batch_size=CFG.BATCH, shuffle=True, pin_memory=True)\n    val_loader   = DataLoader(tf_val, batch_size=CFG.BATCH, shuffle=False, pin_memory=True)\n    test_loader  = DataLoader(tf_test, batch_size=CFG.BATCH, shuffle=False, pin_memory=True)\n\n    model = Trompt(\n        channels=CFG.CHANNELS,\n        out_channels=2,\n        num_prompts=CFG.PROMPTS,\n        num_layers=CFG.LAYERS,\n        col_stats=col_stats,\n        col_names_dict=converter.col_names_dict\n    ).to(device)\n\n    total_steps = len(train_loader) * CFG.EPOCHS\n    optimizer = optim.AdamW(model.parameters(), lr=CFG.LR, weight_decay=CFG.WD)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CFG.LR, total_steps=total_steps,\n                                                    pct_start=CFG.WARMUP, cycle_momentum=False)\n\n    for epoch in range(1, CFG.EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        total_samples = 0\n\n        for batch in train_loader:\n            batch = batch.to(device, non_blocking=True)\n            optimizer.zero_grad()\n\n            # Compute layer-wise supervised loss\n            out = model(batch)\n            batch_size, num_layers, num_classes = out.size()\n            # [batch_size * num_layers, num_classes]\n            pred = out.view(-1, num_classes)\n            # [batch_size * num_layers] (ints)\n            y_rep = batch.y.long().repeat_interleave(num_layers)\n            loss = F.cross_entropy(pred, y_rep)\n            \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            total_loss += loss.item() * batch_size\n            total_samples += batch_size\n        avg_loss = total_loss / (total_samples + 1e-12)\n\n        # Validation: average across layers, then compute positive-class prob\n        model.eval()\n        val_probs_parts = []\n        val_targets_parts = []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = batch.to(device, non_blocking=True)\n                preds = model(batch).mean(dim=1)            # Average layers\n                probs = torch.softmax(preds, dim=-1)[:, 1]  # Positive class prob\n                val_probs_parts.append(probs.cpu().numpy())\n                val_targets_parts.append(batch.y.cpu().numpy())\n\n        val_probs = np.concatenate(val_probs_parts)\n        val_targets = np.concatenate(val_targets_parts)\n        val_auc = roc_auc_score(val_targets, val_probs)\n        print(f\"    Epoch {epoch:02d} - train_loss: {avg_loss:.5f} val_auc: {val_auc:.5f}\")\n\n    # After training fold: produce OOF preds for this fold\n    model.eval()\n    fold_val_probs = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = batch.to(device, non_blocking=True)\n            preds = model(batch).mean(dim=1)\n            probs = torch.softmax(preds, dim=-1)[:, 1]\n            fold_val_probs.append(probs.cpu().numpy())\n    fold_val_probs = np.concatenate(fold_val_probs)\n    oof_preds[val_idx] = fold_val_probs\n\n    # Predict test set inside fold and average across folds\n    fold_test_preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch.to(device, non_blocking=True)\n            preds = model(batch).mean(dim=1)\n            probs = torch.softmax(preds, dim=-1)[:, 1]\n            fold_test_preds.append(probs.cpu().numpy())\n    fold_test_preds = np.concatenate(fold_test_preds)\n    test_preds += fold_test_preds / CFG.FOLDS\n\noof_df = pd.DataFrame({'id': train_id, 'oof_pred': oof_preds})\noof_df.to_csv('oof_preds.csv', index=False)\nprint(\"\\nFinal OOF AUC:\", np.round(roc_auc_score(y, oof_preds),5), \"\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-09T10:06:50.193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/playground-series-s5e11/sample_submission.csv\")\nsub[TARGET] = test_preds\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-09T10:06:50.193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}