{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I showcase an attempt to use a tabular NN model to solve this competition -> inspired by [FT-Transformer](https://arxiv.org/pdf/2106.11959)\n\n**CV: 0.92444**\n\nOnce again, thanks to https://github.com/bogoconic1/Qgentic-AI to iterate and develop the solution\n\nVersion 5: added SOTA **Grouped Query Attention**","metadata":{}},{"cell_type":"code","source":"%%writefile grouped_query_attention.py\n\n\"\"\"\nGrouped Query Attention (GQA) - Pure PyTorch Implementation\n\nGQA is a memory-efficient attention mechanism that reduces the number of key-value\nheads while maintaining multiple query heads. This balances the trade-off between\nMulti-Head Attention (MHA) and Multi-Query Attention (MQA).\n\nArchitecture:\n- MHA: num_query_heads == num_kv_heads (e.g., 8 Q heads, 8 KV heads)\n- GQA: num_query_heads > num_kv_heads (e.g., 8 Q heads, 2 KV heads)\n- MQA: num_kv_heads == 1 (e.g., 8 Q heads, 1 KV head)\n\nReference:\n- \"GQA: Training Generalized Multi-Query Transformer Models\" (Ainslie et al., 2023)\n- https://arxiv.org/abs/2305.13245\n\nUsage:\n    >>> attn = GroupedQueryAttention(embed_dim=512, num_heads=8, num_kv_heads=2)\n    >>> x = torch.randn(32, 10, 512)  # (batch, seq_len, embed_dim)\n    >>> out, attn_weights = attn(x, x, x)\n\"\"\"\n\nimport math\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GroupedQueryAttention(nn.Module):\n    \"\"\"\n    Grouped Query Attention (GQA) layer.\n\n    Args:\n        embed_dim: Total dimension of the model\n        num_heads: Number of query heads (must be divisible by num_kv_heads)\n        num_kv_heads: Number of key/value heads (default: same as num_heads for MHA)\n        dropout: Dropout probability (default: 0.0)\n        bias: Whether to use bias in projections (default: True)\n        batch_first: If True, input/output shape is (batch, seq, feature) (default: True)\n\n    Shape:\n        - Input: (batch, seq_len, embed_dim) if batch_first=True\n                 (seq_len, batch, embed_dim) if batch_first=False\n        - Output: Same shape as input\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        num_kv_heads: Optional[int] = None,\n        dropout: float = 0.0,\n        bias: bool = True,\n        batch_first: bool = True,\n    ):\n        super().__init__()\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\n        self.dropout = dropout\n        self.batch_first = batch_first\n\n        # Validate configuration\n        assert embed_dim % num_heads == 0, \\\n            f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n        assert num_heads % self.num_kv_heads == 0, \\\n            f\"num_heads ({num_heads}) must be divisible by num_kv_heads ({self.num_kv_heads})\"\n\n        self.head_dim = embed_dim // num_heads\n        self.num_groups = num_heads // self.num_kv_heads  # How many query heads share each KV head\n\n        # Query projection: full dimension (all query heads)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n        # Key/Value projections: reduced dimension (fewer KV heads)\n        kv_embed_dim = self.head_dim * self.num_kv_heads\n        self.k_proj = nn.Linear(embed_dim, kv_embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, kv_embed_dim, bias=bias)\n\n        # Output projection\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"Initialize parameters using Xavier uniform initialization.\"\"\"\n        nn.init.xavier_uniform_(self.q_proj.weight)\n        nn.init.xavier_uniform_(self.k_proj.weight)\n        nn.init.xavier_uniform_(self.v_proj.weight)\n        nn.init.xavier_uniform_(self.out_proj.weight)\n\n        if self.q_proj.bias is not None:\n            nn.init.constant_(self.q_proj.bias, 0.0)\n            nn.init.constant_(self.k_proj.bias, 0.0)\n            nn.init.constant_(self.v_proj.bias, 0.0)\n            nn.init.constant_(self.out_proj.bias, 0.0)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        key_padding_mask: Optional[torch.Tensor] = None,\n        need_weights: bool = False,\n        attn_mask: Optional[torch.Tensor] = None,\n        average_attn_weights: bool = True,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Forward pass of Grouped Query Attention.\n\n        Args:\n            query: Query tensor\n            key: Key tensor\n            value: Value tensor\n            key_padding_mask: Mask for padded keys (True = ignore)\n            need_weights: Return attention weights\n            attn_mask: Additive attention mask\n            average_attn_weights: Average attention weights across heads\n\n        Returns:\n            - attn_output: Attention output\n            - attn_weights: Attention weights (if need_weights=True)\n        \"\"\"\n\n        # Handle batch_first\n        if self.batch_first:\n            # Input: (batch, seq, embed_dim)\n            B, T_q, _ = query.shape\n            _, T_k, _ = key.shape\n        else:\n            # Input: (seq, batch, embed_dim)\n            T_q, B, _ = query.shape\n            T_k, _, _ = key.shape\n            # Convert to batch_first for processing\n            query = query.transpose(0, 1)\n            key = key.transpose(0, 1)\n            value = value.transpose(0, 1)\n\n        # Project Q, K, V\n        Q = self.q_proj(query)  # (B, T_q, embed_dim)\n        K = self.k_proj(key)    # (B, T_k, kv_embed_dim)\n        V = self.v_proj(value)  # (B, T_k, kv_embed_dim)\n\n        # Reshape for multi-head attention\n        # Q: (B, T_q, num_heads, head_dim)\n        Q = Q.view(B, T_q, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T_q, head_dim)\n\n        # K, V: (B, T_k, num_kv_heads, head_dim)\n        K = K.view(B, T_k, self.num_kv_heads, self.head_dim).transpose(1, 2)  # (B, num_kv_heads, T_k, head_dim)\n        V = V.view(B, T_k, self.num_kv_heads, self.head_dim).transpose(1, 2)  # (B, num_kv_heads, T_k, head_dim)\n\n        # Expand K and V to match number of query heads\n        # Each KV head is shared by (num_heads // num_kv_heads) query heads\n        if self.num_kv_heads != self.num_heads:\n            # Repeat each KV head `num_groups` times\n            # (B, num_kv_heads, T_k, head_dim) -> (B, num_heads, T_k, head_dim)\n            K = K.repeat_interleave(self.num_groups, dim=1)\n            V = V.repeat_interleave(self.num_groups, dim=1)\n\n        # Scaled dot-product attention\n        # Q: (B, num_heads, T_q, head_dim)\n        # K: (B, num_heads, T_k, head_dim)\n        # V: (B, num_heads, T_k, head_dim)\n\n        scale = 1.0 / math.sqrt(self.head_dim)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * scale  # (B, num_heads, T_q, T_k)\n\n        # Apply attention mask if provided\n        if attn_mask is not None:\n            attn_scores = attn_scores + attn_mask\n\n        # Apply key padding mask if provided\n        if key_padding_mask is not None:\n            # key_padding_mask: (B, T_k), True means ignore\n            # Reshape to (B, 1, 1, T_k) for broadcasting\n            attn_scores = attn_scores.masked_fill(\n                key_padding_mask.unsqueeze(1).unsqueeze(2),\n                float('-inf')\n            )\n\n        # Softmax and dropout\n        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_q, T_k)\n        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n\n        # Apply attention to values\n        attn_output = torch.matmul(attn_weights, V)  # (B, num_heads, T_q, head_dim)\n\n        # Reshape back to (B, T_q, embed_dim)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T_q, self.embed_dim)\n\n        # Output projection\n        attn_output = self.out_proj(attn_output)  # (B, T_q, embed_dim)\n\n        # Handle batch_first\n        if not self.batch_first:\n            attn_output = attn_output.transpose(0, 1)  # (T_q, B, embed_dim)\n\n        # Prepare attention weights for return\n        if need_weights:\n            if average_attn_weights:\n                attn_weights = attn_weights.mean(dim=1)  # (B, T_q, T_k)\n            return attn_output, attn_weights\n        else:\n            return attn_output, None\n\n\n# Convenience function for drop-in replacement\ndef MultiheadGQA(\n    embed_dim: int,\n    num_heads: int,\n    num_kv_heads: Optional[int] = None,\n    dropout: float = 0.0,\n    bias: bool = True,\n    batch_first: bool = True,\n) -> GroupedQueryAttention:\n    \"\"\"\n    Factory function to create a Grouped Query Attention layer.\n    Compatible with PyTorch's MultiheadAttention signature.\n\n    Args:\n        embed_dim: Total dimension of the model\n        num_heads: Number of query heads\n        num_kv_heads: Number of KV heads (default: num_heads for standard MHA)\n        dropout: Dropout probability\n        bias: Use bias in projections\n        batch_first: Input/output is (batch, seq, feature)\n\n    Returns:\n        GroupedQueryAttention module\n\n    Examples:\n        >>> # Standard MHA (8 Q heads, 8 KV heads)\n        >>> mha = MultiheadGQA(embed_dim=512, num_heads=8)\n\n        >>> # GQA (8 Q heads, 2 KV heads) - 4x memory reduction\n        >>> gqa = MultiheadGQA(embed_dim=512, num_heads=8, num_kv_heads=2)\n\n        >>> # MQA (8 Q heads, 1 KV head) - 8x memory reduction\n        >>> mqa = MultiheadGQA(embed_dim=512, num_heads=8, num_kv_heads=1)\n    \"\"\"\n    return GroupedQueryAttention(\n        embed_dim=embed_dim,\n        num_heads=num_heads,\n        num_kv_heads=num_kv_heads,\n        dropout=dropout,\n        bias=bias,\n        batch_first=batch_first,\n    )\n\n\nif __name__ == \"__main__\":\n    # Quick test\n    print(\"Testing Grouped Query Attention implementation...\\n\")\n\n    # Configuration\n    batch_size = 4\n    seq_len = 12\n    embed_dim = 128\n    num_heads = 8\n    num_kv_heads = 2\n\n    # Create input\n    x = torch.randn(batch_size, seq_len, embed_dim)\n\n    # Test different configurations\n    configs = [\n        (\"Standard MHA\", num_heads, num_heads),\n        (\"GQA (4:1 ratio)\", num_heads, 2),\n        (\"MQA (8:1 ratio)\", num_heads, 1),\n    ]\n\n    for name, n_heads, n_kv_heads in configs:\n        print(f\"{name}: {n_heads} Q heads, {n_kv_heads} KV heads\")\n        attn = MultiheadGQA(\n            embed_dim=embed_dim,\n            num_heads=n_heads,\n            num_kv_heads=n_kv_heads,\n            dropout=0.1,\n            batch_first=True,\n        )\n\n        # Forward pass\n        out, weights = attn(x, x, x, need_weights=True)\n\n        # Count parameters\n        n_params = sum(p.numel() for p in attn.parameters())\n\n        print(f\"  Input shape: {x.shape}\")\n        print(f\"  Output shape: {out.shape}\")\n        print(f\"  Attention weights shape: {weights.shape}\")\n        print(f\"  Parameters: {n_params:,}\")\n        print(f\"  Memory reduction vs MHA: {1 - (n_kv_heads / n_heads):.1%}\\n\")\n\n    print(\"✓ All tests passed!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# coding: utf-8\n# FT-Transformer single-file Kaggle script for \"task/playground-series-s5e11\"\n# v10: Added Grouped Query Attention (GQA) support for efficient inference\n# v9: Categorical NA fix + interest_rate quantized TE + dual representation (raw+TE) for all QD numerics\n#     5-fold Stratified CV with fold-averaged test predictions and optional multi-seed ensembling.\n#\n# Outputs:\n# - Logs: task/playground-series-s5e11/outputs/10_7/code_10_7_v9.txt\n# - Submission: task/playground-series-s5e11/outputs/10_7/submission_9.csv\n#\n# Task/metric: Binary classification; evaluation by ROC-AUC (per competition).\n# Loss: BCEWithLogitsLoss (stable, proper for probability estimation).\n#\n# GQA Configuration:\n# - USE_GQA=1 (default: 0)       → Enable Grouped Query Attention\n# - GQA_KV_HEADS=2 (default: 2)  → Number of KV heads for GQA\n\nimport os\nimport sys\nimport math\nimport time\nimport random\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import GQA implementation\ntry:\n    from grouped_query_attention import MultiheadGQA\n    GQA_AVAILABLE = True\nexcept ImportError:\n    GQA_AVAILABLE = False\n    print(\"WARNING: grouped_query_attention.py not found. GQA disabled.\")\n\n# ----------------------------- GQA Configuration -----------------------------\nUSE_GQA = int(os.environ.get(\"USE_GQA\", \"1\"))  # Default: disabled (use standard MHA)\nGQA_KV_HEADS = int(os.environ.get(\"GQA_KV_HEADS\", \"2\"))  # Default: 2 KV heads (4:1 ratio with 8 Q heads)\n\nif USE_GQA and not GQA_AVAILABLE:\n    print(\"ERROR: USE_GQA=1 but grouped_query_attention.py not found. Falling back to MHA.\")\n    USE_GQA = 0\n\n# ----------------------------- Paths & Logging -----------------------------\nBASE_DIR = Path(\"/kaggle/input/playground-series-s5e11\")\nOUTPUT_DIR = Path(\".\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nLOG_FILE = OUTPUT_DIR / \"code_10_7_v9.txt\"\nSUB_PATH = OUTPUT_DIR / \"submission_9.csv\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n    handlers=[\n        logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\"),\n        logging.StreamHandler(sys.stdout),\n    ],\n)\nprint(\"Purpose: Train FT-Transformer with 5-fold Stratified CV (multi-seed), leak-safe encoders, and dual TE+raw for QD numerics; emit submission CSV.\")\nprint(f\"Logs -> {LOG_FILE}\")\nprint(f\"Planned FULL-mode submission path -> {SUB_PATH}\")\nprint(f\"Attention mechanism: {'GQA' if USE_GQA else 'MHA'}\" + (f\" (Q heads: 8, KV heads: {GQA_KV_HEADS}, ratio: {8//GQA_KV_HEADS}:1)\" if USE_GQA else \"\"))\n\n# ----------------------------- Config -----------------------------\nDEBUG = True  # two-pass: DEBUG then FULL\nSEED = 42\n\n# Number of seeds (default 3 per user request; override via env N_SEEDS)\ntry:\n    N_SEEDS = int(os.environ.get(\"N_SEEDS\", \"1\"))\nexcept Exception:\n    N_SEEDS = 1\nSEEDS = [SEED + i for i in range(N_SEEDS)]\nprint(f\"Seed ensemble: N_SEEDS={N_SEEDS}, seeds={SEEDS}\")\n\n# FULL hyperparameters (24GB VRAM)\nHP_FULL = dict(\n    d_token=128,\n    n_blocks=4,\n    n_heads=8,\n    lr=8e-4,\n    weight_decay=1e-5,\n    betas=(0.9, 0.98),\n    eps=1e-8,\n    attn_dropout=0.10,\n    ffn_dropout=0.10,\n    residual_dropout=0.10,\n    d_ffn_factor=2.0,\n    max_epochs=25,\n    patience=3,\n    batch_size=4096,\n    grad_clip=1.0,\n    warmup_ratio=0.06,\n    min_lr=1e-5,\n)\n\n# DEBUG hyperparameters\nHP_DEBUG = dict(\n    d_token=64,\n    n_blocks=1,\n    n_heads=4,\n    lr=1e-3,\n    weight_decay=1e-5,\n    betas=(0.9, 0.98),\n    eps=1e-8,\n    attn_dropout=0.10,\n    ffn_dropout=0.10,\n    residual_dropout=0.10,\n    d_ffn_factor=2.0,\n    max_epochs=1,\n    patience=1,\n    batch_size=512,\n    grad_clip=1.0,\n    warmup_ratio=0.06,\n    min_lr=1e-5,\n)\n\nTARGET_COL = \"loan_paid_back\"\nID_COL = \"id\"\n\n# Quasi-discrete numerics and canonical categorical (if present)\nQD_NUMERICS_CANONICAL = [\"debt_to_income_ratio\", \"credit_score\", \"interest_rate\", \"annual_income\", \"loan_amount\"]\nCAT_CANONICAL = [\"gender\", \"marital_status\", \"education_level\", \"employment_status\", \"loan_purpose\", \"grade_subgrade\"]\n\n# ----------------------------- Reproducibility -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(SEED)\n\n# ----------------------------- Device & AMP -----------------------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nAMP = torch.cuda.is_available()\nprint(f\"Device check -> {DEVICE.type.upper()}. Mixed precision: {'ON' if AMP else 'OFF'}. Purpose: Use CUDA if available; proceed conservatively otherwise.\")\n\n# ----------------------------- Data Loading -----------------------------\ndef read_competition_data(base_dir: Path):\n    print(\"Purpose: Load train/test/sample CSV. Inputs: task/playground-series-s5e11/*.csv\")\n    train = pd.read_csv(base_dir / \"train.csv\")\n    test = pd.read_csv(base_dir / \"test.csv\")\n    sample = pd.read_csv(base_dir / \"sample_submission.csv\")\n    print(f\"Validation: train shape={train.shape}, test shape={test.shape}, sample shape={sample.shape}\")\n    return train, test, sample\n\ntrain_df, test_df, sample_df = read_competition_data(BASE_DIR)\nassert TARGET_COL in train_df.columns, f\"Target column '{TARGET_COL}' not found in train.csv\"\nassert ID_COL in train_df.columns and ID_COL in test_df.columns, \"ID column missing\"\n\ny_full = train_df[TARGET_COL].astype(int).values\nif not set(np.unique(y_full)).issubset({0, 1}):\n    print(\"Target not strictly {0,1}; binarizing at 0.5.\")\n    y_full = (train_df[TARGET_COL].astype(float).values >= 0.5).astype(int)\n\n# ----------------------------- Schema Inference (global preview only) -----------------------------\npresent_qd = [c for c in QD_NUMERICS_CANONICAL if c in train_df.columns]\npresent_cat = [c for c in CAT_CANONICAL if c in train_df.columns]\ndtype_cats = [c for c in train_df.columns if (train_df[c].dtype == \"object\" or str(train_df[c].dtype).startswith(\"category\")) and c not in [TARGET_COL, ID_COL]]\ncat_cols_global = list(dict.fromkeys(present_cat + dtype_cats))\nnumeric_candidates_global = [c for c in train_df.columns if (np.issubdtype(train_df[c].dtype, np.number)) and c not in [TARGET_COL, ID_COL]]\nprint(f\"Global schema preview: numeric≈{len(numeric_candidates_global)}, cats≈{len(cat_cols_global)}, present_qd={present_qd}\")\n\n# ----------------------------- Encoders & Transforms -----------------------------\nNA_CAT_TOKEN = \"__NA__\"\nUNK_CAT_TOKEN = \"__UNK__\"\nNA_STR_SET = {\"\", \"nan\", \"none\", \"null\", \"na\", \"n/a\"}\n\ndef canonicalize_cat_series(s: pd.Series) -> pd.Series:\n    # Robust NA handling: strip, lower, unify to NA token for empties and textual NAs\n    ser = s.copy()\n    mask_null = ser.isna()\n    ser = ser.astype(str).str.strip()\n    mask_text_na = ser.str.lower().isin(NA_STR_SET)\n    ser = ser.mask(mask_null | mask_text_na, NA_CAT_TOKEN)\n    return ser\n\ndef build_cat_vocabs(df: pd.DataFrame, cat_cols: List[str]) -> Dict[str, Dict[str, int]]:\n    vocabs: Dict[str, Dict[str, int]] = {}\n    for c in cat_cols:\n        vals = canonicalize_cat_series(df[c])\n        uniq = pd.unique(vals)\n        mapping = {UNK_CAT_TOKEN: 0}\n        for i, v in enumerate(sorted(map(str, uniq)), start=1):\n            mapping[v] = i\n        if NA_CAT_TOKEN not in mapping:\n            mapping[NA_CAT_TOKEN] = len(mapping)\n        vocabs[c] = mapping\n    return vocabs\n\ndef apply_cat_vocabs(df: pd.DataFrame, cat_cols: List[str], vocabs: Dict[str, Dict[str, int]]) -> np.ndarray:\n    mats = []\n    for c in cat_cols:\n        mapping = vocabs[c]\n        arr = canonicalize_cat_series(df[c]).values\n        idx = np.array([mapping.get(v, mapping[UNK_CAT_TOKEN]) for v in arr], dtype=np.int64)\n        mats.append(idx.reshape(-1, 1))\n    if len(mats) == 0:\n        return np.zeros((len(df), 0), dtype=np.int64)\n    return np.concatenate(mats, axis=1)\n\ndef quantize_interest_rate_series(s: pd.Series, step: float = 0.25) -> pd.Series:\n    # Convert to numeric, quantize to 'step' percent, format as string token; NA -> \"NaNLevel\"\n    x = pd.to_numeric(s, errors=\"coerce\")\n    q = (x / step).round() * step\n    return q.map(lambda v: f\"{v:.2f}\" if pd.notnull(v) else \"NaNLevel\")\n\ndef m_estimate_mapping(count: int, pos: int, prior: float, m: float = 5.0) -> float:\n    return (pos + m * prior) / (count + m)\n\ndef _agg_to_full_map(agg_df: pd.DataFrame, count_col: str = \"count\", pos_col: str = \"sum\") -> Dict[str, Tuple[int, int]]:\n    full_map: Dict[str, Tuple[int, int]] = {}\n    if count_col not in agg_df.columns and \"cnt\" in agg_df.columns:\n        count_col = \"cnt\"\n    if pos_col not in agg_df.columns and \"pos\" in agg_df.columns:\n        pos_col = \"pos\"\n    for key, row in agg_df.iterrows():\n        cnt = int(float(row[count_col])) if pd.notnull(row[count_col]) else 0\n        pos = int(float(row[pos_col])) if pd.notnull(row[pos_col]) else 0\n        full_map[str(key)] = (cnt, pos)\n    return full_map\n\ndef cross_fit_m_estimate_oof(\n    df: pd.DataFrame,\n    y: np.ndarray,\n    col: str,\n    n_splits: int = 5,\n    m: float = 5.0,\n    seed: int = 42,\n) -> Tuple[np.ndarray, Dict[str, Tuple[int, int]], float]:\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    prior = float(np.mean(y))\n    # full map for inference\n    tmp_all = pd.DataFrame({col: df[col].astype(str).fillna(\"NaNLevel\"), \"_y\": y})\n    agg_all = tmp_all.groupby(col)[\"_y\"].agg([\"count\", \"sum\"])\n    full_map = _agg_to_full_map(agg_all, \"count\", \"sum\")\n    # OOF\n    oof = np.zeros(len(df), dtype=np.float32)\n    for tr_idx, va_idx in skf.split(df, y):\n        tmp_tr = pd.DataFrame({col: df.iloc[tr_idx][col].astype(str).fillna(\"NaNLevel\").values, \"_y\": y[tr_idx]})\n        agg_tr = tmp_tr.groupby(col)[\"_y\"].agg([\"count\", \"sum\"])\n        tr_map = _agg_to_full_map(agg_tr, \"count\", \"sum\")\n        vals_va = df.iloc[va_idx][col].astype(str).fillna(\"NaNLevel\").values\n        enc = np.array(\n            [m_estimate_mapping(tr_map[v][0], tr_map[v][1], prior, m) if v in tr_map else prior for v in vals_va],\n            dtype=np.float32,\n        )\n        oof[va_idx] = enc\n    return oof, full_map, prior\n\ndef apply_m_estimate_map(df: pd.DataFrame, col: str, full_map: Dict[str, Tuple[int, int]], prior: float, m: float = 5.0) -> np.ndarray:\n    vals = df[col].astype(str).fillna(\"NaNLevel\").values\n    out = np.empty(len(vals), dtype=np.float32)\n    for i, v in enumerate(vals):\n        if v in full_map:\n            cnt, pos = full_map[v]\n            out[i] = m_estimate_mapping(cnt, pos, prior, m)\n        else:\n            out[i] = prior\n    return out\n\ndef standardize_train_valid_test(\n    Xtr: pd.DataFrame, Xva: pd.DataFrame, Xte: pd.DataFrame, cols: List[str]\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, Tuple[float, float]]]:\n    stats = {}\n    def z(x, mu, sd):\n        return (x - mu) / sd if sd > 0 else x - mu\n    Xtr_out, Xva_out, Xte_out = [], [], []\n    for c in cols:\n        mu = float(np.nanmean(Xtr[c].values))\n        sd = float(np.nanstd(Xtr[c].values))\n        sd = sd if sd > 1e-12 else 1.0\n        stats[c] = (mu, sd)\n        Xtr_out.append(z(Xtr[c].values, mu, sd).reshape(-1, 1))\n        Xva_out.append(z(Xva[c].values, mu, sd).reshape(-1, 1))\n        Xte_out.append(z(Xte[c].values, mu, sd).reshape(-1, 1))\n    if len(cols) == 0:\n        return np.zeros((len(Xtr), 0), np.float32), np.zeros((len(Xva), 0), np.float32), np.zeros((len(Xte), 0), np.float32), stats\n    return (\n        np.concatenate(Xtr_out, axis=1).astype(np.float32),\n        np.concatenate(Xva_out, axis=1).astype(np.float32),\n        np.concatenate(Xte_out, axis=1).astype(np.float32),\n        stats,\n    )\n\n# ----------------------------- Dataset -----------------------------\nclass TabDataset(Dataset):\n    def __init__(self, X_num: np.ndarray, X_cat: np.ndarray, y: Optional[np.ndarray] = None):\n        n = len(y) if y is not None else (X_num.shape[0] if X_num is not None else X_cat.shape[0])\n        self.X_num = (X_num.astype(np.float32) if X_num is not None else np.zeros((n, 0), dtype=np.float32))\n        self.X_cat = (X_cat.astype(np.int64) if X_cat is not None else np.zeros((n, 0), dtype=np.int64))\n        self.y = None if y is None else y.astype(np.float32).reshape(-1, 1)\n    def __len__(self): return self.X_num.shape[0]\n    def __getitem__(self, idx):\n        if self.y is None: return self.X_num[idx], self.X_cat[idx]\n        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n\n# ----------------------------- Minimal FT-Transformer -----------------------------\nclass FeatureTokenizer(nn.Module):\n    def __init__(self, n_num_features: int, cat_cardinalities: Optional[List[int]], d_token: int):\n        super().__init__()\n        self.n_num = int(n_num_features)\n        self.d_token = int(d_token)\n        self.has_cat = cat_cardinalities is not None and len(cat_cardinalities) > 0\n        if self.n_num > 0:\n            self.num_weight = nn.Parameter(torch.empty(self.n_num, self.d_token))\n            self.num_bias = nn.Parameter(torch.empty(self.n_num, self.d_token))\n            nn.init.kaiming_uniform_(self.num_weight, a=math.sqrt(5))\n            nn.init.uniform_(self.num_bias, -1e-3, 1e-3)\n        else:\n            self.register_parameter(\"num_weight\", None)\n            self.register_parameter(\"num_bias\", None)\n        if self.has_cat:\n            self.embeddings = nn.ModuleList([nn.Embedding(int(c), self.d_token) for c in cat_cardinalities])\n            for emb in self.embeddings:\n                nn.init.kaiming_uniform_(emb.weight, a=math.sqrt(5))\n        else:\n            self.embeddings = nn.ModuleList()\n\n    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor]) -> torch.Tensor:\n        tokens = []\n        if self.n_num > 0 and x_num is not None and x_num.numel() > 0:\n            x = x_num.unsqueeze(-1) * self.num_weight.unsqueeze(0) + self.num_bias.unsqueeze(0)\n            tokens.append(x)\n        if self.has_cat and x_cat is not None and x_cat.shape[1] > 0:\n            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n            if len(embs) > 0:\n                tokens.append(torch.stack(embs, dim=1))\n        if len(tokens) == 0:\n            B = x_num.shape[0] if x_num is not None else x_cat.shape[0]\n            return torch.zeros(B, 0, self.d_token, device=x_num.device if x_num is not None else x_cat.device)\n        return torch.cat(tokens, dim=1)\n\nclass ReGLU_FFN(nn.Module):\n    def __init__(self, d_in: int, d_hidden: int, dropout: float):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, 2 * d_hidden)\n        self.fc2 = nn.Linear(d_hidden, d_in)\n        self.dropout = nn.Dropout(dropout)\n        nn.init.kaiming_uniform_(self.fc1.weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.fc2.weight, a=math.sqrt(5))\n        nn.init.uniform_(self.fc1.bias, -1e-3, 1e-3)\n        nn.init.uniform_(self.fc2.bias, -1e-3, 1e-3)\n    def forward(self, x):\n        u, v = self.fc1(x).chunk(2, dim=-1)\n        x = F.relu(u) * v\n        x = self.dropout(self.fc2(x))\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, d_ffn: int, attn_dropout: float, ffn_dropout: float, residual_dropout: float, use_gqa: bool = False, gqa_kv_heads: int = 2):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n\n        # Use GQA if enabled, otherwise standard MHA\n        if use_gqa:\n            self.attn = MultiheadGQA(embed_dim=d_model, num_heads=n_heads, num_kv_heads=gqa_kv_heads, dropout=attn_dropout, batch_first=True)\n        else:\n            self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=attn_dropout, batch_first=True)\n\n        self.drop_res1 = nn.Dropout(residual_dropout)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.ffn = ReGLU_FFN(d_model, d_ffn, ffn_dropout)\n        self.drop_res2 = nn.Dropout(residual_dropout)\n    def forward(self, x):\n        x_attn = self.attn(self.ln1(x), self.ln1(x), self.ln1(x), need_weights=False)[0]\n        x = x + self.drop_res1(x_attn)\n        x_ffn = self.ffn(self.ln2(x))\n        x = x + self.drop_res2(x_ffn)\n        return x\n\nclass FTTransformer(nn.Module):\n    def __init__(self, n_num_features: int, cat_cardinalities: Optional[List[int]], d_token: int, n_blocks: int, n_heads: int, d_ffn: int,\n                 attn_dropout: float, ffn_dropout: float, residual_dropout: float, d_out: int = 1, use_gqa: bool = False, gqa_kv_heads: int = 2):\n        super().__init__()\n        assert d_token % n_heads == 0, f\"d_token must be divisible by n_heads; got {d_token} % {n_heads}\"\n        self.tokenizer = FeatureTokenizer(n_num_features, cat_cardinalities, d_token)\n        self.cls = nn.Parameter(torch.zeros(1, 1, d_token))\n        nn.init.uniform_(self.cls, -1e-3, 1e-3)\n        self.blocks = nn.Sequential(*[\n            TransformerBlock(d_token, n_heads, d_ffn, attn_dropout, ffn_dropout, residual_dropout, use_gqa, gqa_kv_heads)\n            for _ in range(n_blocks)\n        ])\n        self.head_norm = nn.LayerNorm(d_token)\n        self.head = nn.Linear(d_token, d_out)\n        nn.init.kaiming_uniform_(self.head.weight, a=math.sqrt(5))\n        nn.init.uniform_(self.head.bias, -1e-3, 1e-3)\n    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor]) -> torch.Tensor:\n        x_tokens = self.tokenizer(x_num, x_cat)  # (B, T, d)\n        B = x_tokens.shape[0]\n        cls = self.cls.expand(B, -1, -1)\n        x = torch.cat([cls, x_tokens], dim=1)\n        x = self.blocks(x)\n        x = self.head_norm(x[:, 0, :])\n        return self.head(x)\n\n# ----------------------------- Scheduler -----------------------------\ndef make_warmup_cosine(total_steps: int, warmup_ratio: float = 0.06, min_lr_ratio: float = 0.01):\n    warmup = max(1, int(total_steps * warmup_ratio))\n    def lr_mult(step: int):\n        if step < warmup:\n            return (step + 1) / warmup\n        progress = (step - warmup) / max(1, total_steps - warmup)\n        return min_lr_ratio + 0.5 * (1 - min_lr_ratio) * (1 + math.cos(math.pi * progress))\n    return lr_mult\n\n# ----------------------------- Training helpers -----------------------------\ndef train_one_fold(\n    tr_df: pd.DataFrame,\n    va_df: pd.DataFrame,\n    te_df: pd.DataFrame,\n    y_tr: np.ndarray,\n    y_va: np.ndarray,\n    hp: dict,\n    fold_idx: int,\n    seed: int,\n    debug_mode: bool,\n    is_full_mode_first_fold: bool,\n) -> Tuple[np.ndarray, np.ndarray, float, int, bool]:\n    # Per-fold roles\n    present_qd = [c for c in QD_NUMERICS_CANONICAL if c in tr_df.columns]\n    present_cat = [c for c in CAT_CANONICAL if c in tr_df.columns]\n    dtype_cats = [c for c in tr_df.columns if (tr_df[c].dtype == \"object\" or str(tr_df[c].dtype).startswith(\"category\")) and c not in [TARGET_COL, ID_COL]]\n    cat_cols = list(dict.fromkeys(present_cat + dtype_cats))\n\n    numeric_candidates = [c for c in tr_df.columns if (np.issubdtype(tr_df[c].dtype, np.number)) and c not in [TARGET_COL, ID_COL]]\n    # Dual representation: keep raw numerics for all QD (do NOT remove them)\n    num_cols = list(numeric_candidates)  # includes raw debt_to_income_ratio, credit_score, interest_rate if numeric\n    \n    # Columns to TE (cross-fitted): the two native QD numerics and the quantized interest rate (if present)\n    qd_te_cols: List[str] = []\n    for c in present_qd:\n        qd_te_cols.append(c)\n\n    # Fit categorical vocabs on train-fold only\n    vocabs = build_cat_vocabs(tr_df, cat_cols)\n    cat_cardinalities = [max(vocabs[c].values()) + 1 for c in cat_cols]\n    print(f\"[fold{fold_idx}|seed{seed}] Categorical: {len(cat_cols)} cols; first few cardinalities={cat_cardinalities[:6]}\")\n\n    # Cross-fitted TE on train-fold; apply to valid/test\n    te_maps = {}; te_prior = {}; m_value = 5.0\n    te_tr_feats = []; te_va_feats = []; te_te_feats = []\n    for c in qd_te_cols:\n        oof, full_map, prior = cross_fit_m_estimate_oof(tr_df, y_tr, c, n_splits=5, m=m_value, seed=seed)\n        te_maps[c] = full_map; te_prior[c] = prior\n        te_tr_feats.append(oof.reshape(-1, 1))\n        te_va_feats.append(apply_m_estimate_map(va_df, c, full_map, prior, m=m_value).reshape(-1, 1))\n        te_te_feats.append(apply_m_estimate_map(te_df, c, full_map, prior, m=m_value).reshape(-1, 1))\n    Xtr_te = np.concatenate(te_tr_feats, axis=1) if te_tr_feats else np.zeros((len(tr_df), 0), np.float32)\n    Xva_te = np.concatenate(te_va_feats, axis=1) if te_va_feats else np.zeros((len(va_df), 0), np.float32)\n    Xte_te = np.concatenate(te_te_feats, axis=1) if te_te_feats else np.zeros((len(te_df), 0), np.float32)\n    if len(qd_te_cols) > 0:\n        print(f\"[fold{fold_idx}|seed{seed}] TE m={m_value} on {qd_te_cols}; priors={[round(te_prior[c],4) for c in qd_te_cols]}\")\n\n    # Standardize numerics (includes raw QD numerics for dual representation)\n    Xtr_num, Xva_num, Xte_num, zstats = standardize_train_valid_test(tr_df, va_df, te_df, num_cols)\n    print(f\"[fold{fold_idx}|seed{seed}] Standardized numerics={len(num_cols)}; example stats={list(zstats.items())[:3]}\")\n\n    # Combine numeric features: raw standardized + TE for QD numerics\n    Xtr_num_all = np.concatenate([Xtr_num, Xtr_te], axis=1) if Xtr_te.shape[1] else Xtr_num\n    Xva_num_all = np.concatenate([Xva_num, Xva_te], axis=1) if Xva_te.shape[1] else Xva_num\n    Xte_num_all = np.concatenate([Xte_num, Xte_te], axis=1) if Xte_te.shape[1] else Xte_num\n\n    # Categorical indices (canonicalized NA handling)\n    Xtr_cat = apply_cat_vocabs(tr_df, cat_cols, vocabs)\n    Xva_cat = apply_cat_vocabs(va_df, cat_cols, vocabs)\n    Xte_cat = apply_cat_vocabs(te_df, cat_cols, vocabs)\n\n    n_num_features = Xtr_num_all.shape[1]\n    print(f\"[fold{fold_idx}|seed{seed}] Final tokens -> numeric={n_num_features}, categorical={len(cat_cols)}\")\n\n    # DataLoaders\n    class TabDS(torch.utils.data.Dataset):\n        def __init__(self, Xn, Xc, y=None):\n            self.Xn = torch.from_numpy(Xn).float()\n            self.Xc = torch.from_numpy(Xc).long() if Xc.shape[1] > 0 else torch.zeros((Xn.shape[0], 0), dtype=torch.long)\n            self.y = None if y is None else torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n        def __len__(self): return self.Xn.shape[0]\n        def __getitem__(self, i):\n            if self.y is None: return self.Xn[i], self.Xc[i]\n            return self.Xn[i], self.Xc[i], self.y[i]\n\n    dl_tr = DataLoader(TabDS(Xtr_num_all, Xtr_cat, y_tr), batch_size=hp[\"batch_size\"], shuffle=True, num_workers=2, pin_memory=True)\n    dl_va = DataLoader(TabDS(Xva_num_all, Xva_cat, y_va), batch_size=hp[\"batch_size\"], shuffle=False, num_workers=2, pin_memory=True)\n    dl_te = DataLoader(TabDS(Xte_num_all, Xte_cat, None), batch_size=hp[\"batch_size\"], shuffle=False, num_workers=2, pin_memory=True)\n\n    # Build model\n    model = FTTransformer(\n        n_num_features=n_num_features,\n        cat_cardinalities=cat_cardinalities,\n        d_token=hp[\"d_token\"],\n        n_blocks=hp[\"n_blocks\"],\n        n_heads=hp[\"n_heads\"],\n        d_ffn=int(hp[\"d_token\"] * hp[\"d_ffn_factor\"]),\n        attn_dropout=hp[\"attn_dropout\"],\n        ffn_dropout=hp[\"ffn_dropout\"],\n        residual_dropout=hp[\"residual_dropout\"],\n        d_out=1,\n        use_gqa=USE_GQA,\n        gqa_kv_heads=GQA_KV_HEADS,\n    ).to(DEVICE)\n\n    # Print model parameters\n    n_params = sum(p.numel() for p in model.parameters())\n    print(f\"[fold{fold_idx}|seed{seed}] Model parameters: {n_params:,}\")\n\n    # Optimizer, loss, scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=hp[\"lr\"], betas=hp[\"betas\"], eps=hp[\"eps\"], weight_decay=hp[\"weight_decay\"])\n    loss_fn = nn.BCEWithLogitsLoss()\n    total_steps = max(1, len(dl_tr) * hp[\"max_epochs\"])\n    lr_lambda = make_warmup_cosine(total_steps, warmup_ratio=hp[\"warmup_ratio\"], min_lr_ratio=hp[\"min_lr\"] / hp[\"lr\"])\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n\n    def eval_auc(dloader):\n        model.eval()\n        preds, ys = [], []\n        with torch.no_grad():\n            for xb_num, xb_cat, yb in dloader:\n                xb_num = xb_num.to(DEVICE, non_blocking=True)\n                xb_cat = xb_cat.to(DEVICE, non_blocking=True) if xb_cat.shape[1] > 0 else None\n                with torch.cuda.amp.autocast(enabled=AMP):\n                    logits = model(xb_num, xb_cat)\n                preds.append(torch.sigmoid(logits).detach().cpu().numpy().ravel())\n                ys.append(yb.cpu().numpy().ravel())\n        p = np.concatenate(preds); y = np.concatenate(ys)\n        try:\n            return roc_auc_score(y, p), p\n        except ValueError:\n            return float(\"nan\"), p\n\n    best_auc, best_epoch, best_state = -1.0, -1, None\n    epochs_no_improve = 0\n    nan_loss_flag = False\n    t0 = time.time()\n\n    for epoch in range(1, hp[\"max_epochs\"] + 1):\n        model.train()\n        epoch_loss, n_seen = 0.0, 0\n        for xb_num, xb_cat, yb in dl_tr:\n            xb_num = xb_num.to(DEVICE, non_blocking=True)\n            xb_cat = xb_cat.to(DEVICE, non_blocking=True) if xb_cat.shape[1] > 0 else None\n            yb = yb.to(DEVICE, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=AMP):\n                logits = model(xb_num, xb_cat)\n                loss = loss_fn(logits, yb)\n            if torch.isnan(loss):\n                nan_loss_flag = True\n            scaler.scale(loss).backward()\n            if hp[\"grad_clip\"] is not None:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), hp[\"grad_clip\"])\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            bs = yb.shape[0]\n            epoch_loss += loss.item() * bs\n            n_seen += bs\n\n        train_loss = epoch_loss / max(1, n_seen)\n        val_auc, _ = eval_auc(dl_va)\n        print(f\"[fold{fold_idx}|seed{seed}] Epoch {epoch}/{hp['max_epochs']} - train_loss={train_loss:.5f} | val_auc={val_auc:.6f} | lr={optimizer.param_groups[0]['lr']:.6f}\")\n\n        if val_auc > best_auc:\n            best_auc, best_epoch = val_auc, epoch\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= hp[\"patience\"]:\n                print(f\"[fold{fold_idx}|seed{seed}] Early stopping at epoch {epoch}. Best AUC={best_auc:.6f} @ epoch {best_epoch}\")\n                break\n\n        if (not debug_mode) and is_full_mode_first_fold and epoch == 1 and nan_loss_flag:\n            logging.warning(\"[FULL] NaN loss detected after 1st epoch on fold 0. Aborting remaining training and proceeding to inference.\")\n            break\n\n    t1 = time.time()\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    print(f\"[fold{fold_idx}|seed{seed}] Best val AUC={best_auc:.6f}; best_epoch={best_epoch}; fold_train_time_sec={(t1 - t0):.1f}\")\n\n    # Final valid preds\n    model.eval()\n    va_preds = []\n    with torch.no_grad():\n        for xb_num, xb_cat, yb in dl_va:\n            xb_num = xb_num.to(DEVICE, non_blocking=True)\n            xb_cat = xb_cat.to(DEVICE, non_blocking=True) if xb_cat.shape[1] > 0 else None\n            with torch.cuda.amp.autocast(enabled=AMP):\n                logits = model(xb_num, xb_cat)\n            va_preds.append(torch.sigmoid(logits).detach().cpu().numpy().ravel())\n    va_probs = np.concatenate(va_preds)\n\n    # Test preds\n    te_preds = []\n    with torch.no_grad():\n        for xb_num, xb_cat in dl_te:\n            xb_num = xb_num.to(DEVICE, non_blocking=True)\n            xb_cat = xb_cat.to(DEVICE, non_blocking=True) if xb_cat.shape[1] > 0 else None\n            with torch.cuda.amp.autocast(enabled=AMP):\n                logits = model(xb_num, xb_cat)\n            te_preds.append(torch.sigmoid(logits).detach().cpu().numpy().ravel())\n    te_probs = np.concatenate(te_preds)\n\n    abort_all = (not debug_mode) and is_full_mode_first_fold and nan_loss_flag\n    return va_probs, te_probs, best_auc, best_epoch, abort_all\n\n# ----------------------------- CV runner (DEBUG/FULL) -----------------------------\ndef run_one_mode(debug: bool):\n    mode = \"DEBUG\" if debug else \"FULL\"\n    print(f\"====================== Running mode: {mode} ======================\")\n    hp = HP_DEBUG if debug else HP_FULL\n\n    n = len(train_df)\n    global_oof_sum = np.zeros(n, dtype=np.float64)\n    global_test_sum = np.zeros(len(test_df), dtype=np.float64)\n    per_seed_oof_aucs = []\n\n    for si, seed in enumerate(SEEDS):\n        set_seed(seed)\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n        oof = np.zeros(n, dtype=np.float64)\n        test_sum_folds = np.zeros(len(test_df), dtype=np.float64)\n        fold_aucs = []\n        abort_all = False\n\n        for fold_idx, (tr_idx, va_idx) in enumerate(skf.split(train_df, y_full)):\n            tr_df = train_df.iloc[tr_idx].reset_index(drop=True).copy()\n            va_df = train_df.iloc[va_idx].reset_index(drop=True).copy()\n            y_tr = y_full[tr_idx]; y_va = y_full[va_idx]\n            print(f\"[seed{seed}] Fold {fold_idx}: train={len(tr_df)}, valid={len(va_df)}\")\n\n            # DEBUG sampling\n            if debug:\n                n_debug = min(1000, len(tr_df))\n                if n_debug <= 0.5 * len(tr_df):\n                    sss = StratifiedShuffleSplit(n_splits=1, test_size=len(tr_df) - n_debug, random_state=seed)\n                    keep_idx, _ = next(sss.split(np.zeros(len(y_tr)), y_tr))\n                    tr_df = tr_df.iloc[keep_idx].reset_index(drop=True)\n                    y_tr = y_tr[keep_idx]\n                    print(f\"[seed{seed}] Fold {fold_idx}: DEBUG sampling -> train size={len(tr_df)}\")\n                else:\n                    logging.warning(f\"[seed{seed}] Fold {fold_idx}: DEBUG sample would exceed 50% of fold; using full train fold.\")\n\n            va_probs, te_probs, best_auc, best_epoch, abort_all = train_one_fold(\n                tr_df=tr_df, va_df=va_df, te_df=test_df, y_tr=y_tr, y_va=y_va, hp=hp,\n                fold_idx=fold_idx, seed=seed, debug_mode=debug, is_full_mode_first_fold=(fold_idx == 0)\n            )\n\n            oof[va_idx] = va_probs\n            test_sum_folds += te_probs\n            fold_aucs.append(best_auc)\n            print(f\"[seed{seed}] Fold {fold_idx} complete: val_auc={best_auc:.6f}, best_epoch={best_epoch}\")\n\n            if abort_all:\n                logging.warning(f\"[seed{seed}] Aborting remaining folds due to NaN-loss guard (FULL mode, fold 0 epoch 1).\")\n                break\n\n        # Per-seed aggregation\n        seed_oof_auc = roc_auc_score(y_full[:len(oof)], oof)\n        per_seed_oof_aucs.append(seed_oof_auc)\n        global_oof_sum += oof\n        n_folds_executed = len(fold_aucs)\n        test_avg_folds = test_sum_folds / max(1, n_folds_executed)\n        global_test_sum += test_avg_folds\n\n        print(f\"[seed{seed}] OOF AUC={seed_oof_auc:.6f} over {n_folds_executed} folds; per-fold AUCs={['{:.6f}'.format(a) for a in fold_aucs]}\")\n\n    # Seed-averaged aggregates\n    oof_mean = global_oof_sum / len(SEEDS)\n    overall_oof_auc = roc_auc_score(y_full, oof_mean)\n    test_mean = global_test_sum / len(SEEDS)\n    print(f\"[{mode}] Overall OOF AUC (seed-averaged)={overall_oof_auc:.6f}; per-seed OOF AUCs={['{:.6f}'.format(a) for a in per_seed_oof_aucs]}\")\n\n    # Output handling\n    if debug:\n        print(\"[DEBUG] Skipping submission write as per guidelines.\")\n    else:\n        sub = pd.DataFrame({ID_COL: test_df[ID_COL].values, TARGET_COL: np.clip(test_mean, 1e-6, 1 - 1e-6)})\n        sub.to_csv(SUB_PATH, index=False)\n        print(f\"[FULL] Submission written: {SUB_PATH}\")\n        pct = np.percentile(test_mean, [0, 1, 5, 25, 50, 75, 95, 99, 100])\n        print(f\"[FULL] Prediction summary: min={pct[0]:.6f}, p1={pct[1]:.6f}, p5={pct[2]:.6f}, \"\n                     f\"p25={pct[3]:.6f}, median={pct[4]:.6f}, p75={pct[5]:.6f}, p95={pct[6]:.6f}, p99={pct[7]:.6f}, max={pct[8]:.6f}\")\n\n# ----------------------------- Execute -----------------------------\nprint(\"Purpose: Execute two passes: (1) DEBUG sanity-check (no submission), (2) FULL CV training + inference with N_SEEDS ensembling. Metric: ROC-AUC.\")\nhf_token = os.environ.get(\"HF_TOKEN\", \"\")\nif hf_token:\n    print(\"HF_TOKEN detected in environment (not used).\")\n\n# Pass 1: DEBUG\nrun_one_mode(debug=True)\n# Pass 2: FULL\nrun_one_mode(debug=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}