{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"ID_ITEM = \"id\"\nLABEL_ITEM = \"loan_paid_back\"\nIGNORE_ITEMS = [ID_ITEM, LABEL_ITEM]\nVARIANCE_BINS = 24\n\nDIM_MODEL = 1\nDIM_FEEDFORWARD = 1\nNUM_LAYERS = 0\n\nVAL_PER_STEP = 1024\nNUM_VAL_CYCLES = 32\nBATCH_SIZE = 512\nLEARNING_RATE = 1e-5\nWEIGHT_DECAY = 1e-2\nDROPOUT = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T14:58:39.485928Z","iopub.execute_input":"2025-11-09T14:58:39.486311Z","iopub.status.idle":"2025-11-09T14:58:39.492347Z","shell.execute_reply.started":"2025-11-09T14:58:39.486285Z","shell.execute_reply":"2025-11-09T14:58:39.491218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 数据预处理\nimport pandas as pd\nimport numpy as np\nfrom collections.abc import Callable\nfrom tqdm import tqdm\n\nContinuousFeatureType = int | float\nFeatureType = str | bool | ContinuousFeatureType\n\n\ndef create_feature_discretizer(feature: list[FeatureType]) -> tuple[Callable[[FeatureType], int], int]:\n    \"\"\"\n    创建特征离散化函数，将特征值映射到整数索引\n    \n    对于类别特征或低方差数值特征: 使用查表法直接映射\n    对于高方差数值特征: 基于 5%-95% 分位数进行分箱离散化\n    \n    Args:\n        feature: 特征值列表，支持字符串或数值类型\n        \n    Returns:\n        - 离散化函数: 输入特征值，返回整数索引\n        - 分箱数量: 离散化后的类别总数\n    \"\"\"\n    if isinstance(feature[0], str) or len(set(feature)) < VARIANCE_BINS:\n        # 类别特征或低基数数值特征，创建值到索引的映射\n        value_to_index = {value: idx for idx, value in enumerate(set(feature))}\n        return (lambda token, mapping=value_to_index: mapping[token]), len(value_to_index)\n\n    # 高基数数值特征，基于分位数进行分箱离散化\n    feature = np.array(feature)\n    lower_bound, upper_bound = np.percentile(feature, [5, 95])\n    value_range = upper_bound - lower_bound + 1e-8\n    return (lambda value, lower_bound=lower_bound, value_range=value_range: int(min(max((value - lower_bound) / value_range, 0), 1) * (VARIANCE_BINS - 1) + 0.5)), VARIANCE_BINS \n\n\ndef create_feature_standardizer(feature: list[ContinuousFeatureType]) -> tuple[Callable[[ContinuousFeatureType], float], Callable[[float], float]]:\n    \"\"\"\n    创建特征标准化函数，将数值特征转换为标准正态分布\n    \n    仅对高方差数值特征有效，类别特征返回 None\n    使用均值和标准差进行标准化: (value - mean) / std\n    对异常值比归一化更鲁棒\n    \n    Args:\n        feature: 特征值列表\n        \n    Returns:\n        标准化函数 - 将原始值转换为标准化值\n        反向标准化函数 - 将标准化值转换回原始尺度值\n    \"\"\"\n    # 高基数数值特征，创建基于均值和标准差的标准化函数\n    feature = np.array(feature)\n    mean = np.mean(feature)\n    std = np.std(feature)\n\n    # 避免除零错误\n    if std < 1e-8:\n        standardizer = lambda _: 0.0\n        inverser = lambda _, mean=mean: mean\n    else:\n        standardizer = lambda value, mean=mean, std=std: (value - mean) / std\n        inverser = lambda value, mean=mean, std=std: value * std + mean\n\n    return standardizer, inverser\n\n# 读取数据\nraw_data = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\n\n# 获取特征离散化函数和分箱数量\nfeature_discretizers_and_bins = [\n    (feature_name, create_feature_discretizer(list(feature)))\n    for feature_name, feature in raw_data.items()\n    if feature_name not in IGNORE_ITEMS\n]\nfeature_discretizers = {feature_type: discretizer for feature_type, (discretizer, bin_count) in feature_discretizers_and_bins}\nfeature_bin_counts = {feature_type: bin_count for feature_type, (discretizer, bin_count) in feature_discretizers_and_bins}\n\n# 获取特征标准化函数\nfeature_standardizers = {\n    feature_name: create_feature_standardizer(list(feature))[0]\n    for feature_name, feature in raw_data.items()\n    if feature_name not in IGNORE_ITEMS and not (isinstance(list(feature)[0], str) or len(set(feature)) < VARIANCE_BINS)\n}\n\n# 对特征列排序，先处理离散特征，再处理连续特征\nsorted_columns = sorted([\n    feature_name\n    for feature_name in raw_data\n    if feature_name not in IGNORE_ITEMS\n], key=lambda x: (x in feature_standardizers, x))\n\n# 获取标签标准化函数及其逆向函数\nlabel_standardizer, label_inverser = create_feature_standardizer(raw_data[LABEL_ITEM])\n\n# 转换数据集\ndataset = []\ntest_dataset = []\nfor source_data, target_dataset in [(raw_data, dataset), (test_data, test_dataset)]:\n    has_label = LABEL_ITEM in source_data\n    for row_idx in tqdm(range(len(source_data[sorted_columns[0]]))):\n        discretized_features = []  # 离散化特征值\n        standardized_features = []  # 标准化特征值\n        for feature_name in sorted_columns:\n            feature_value = source_data[feature_name][row_idx]\n\n            # 离散化处理（所有特征）\n            discretized_features.append(feature_discretizers[feature_name](feature_value))\n\n            # 标准化处理（仅连续特征）\n            if feature_name in feature_standardizers:\n                standardized_features.append(feature_standardizers[feature_name](feature_value))\n\n        item_id = source_data[ID_ITEM][row_idx]\n        label_value = label_standardizer(source_data[LABEL_ITEM][row_idx]) if has_label else float(\"nan\")\n        target_dataset.append((item_id, discretized_features, standardized_features, label_value))\n\n# 切割带标签的数据集，分为训练集和验证集\nsplit_point = int(len(dataset) * 0.9)\ntrain_dataset = dataset[:split_point]\nval_dataset = dataset[split_point:]\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-11-09T14:58:39.507772Z","iopub.execute_input":"2025-11-09T14:58:39.508068Z","iopub.status.idle":"2025-11-09T14:59:45.138622Z","shell.execute_reply.started":"2025-11-09T14:58:39.508049Z","shell.execute_reply":"2025-11-09T14:59:45.137467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 模型的定义\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, dim_model: int, dim_feedforward: int, dropout: float = 0.):\n        super().__init__()\n        self.linear1 = nn.Linear(dim_model, dim_feedforward * 2)\n        self.linear2 = nn.Linear(dim_feedforward, dim_model)\n        self.scale = nn.Parameter(torch.zeros(1))\n        self.norm = nn.BatchNorm1d(dim_model)\n        self.dropout = nn.Dropout(dropout)\n\n        # 初始化权重\n        for module in [self.linear1, self.linear2]:\n            nn.init.xavier_uniform_(module.weight)\n            nn.init.zeros_(module.bias)\n\n    def forward(self, x: torch.Tensor):\n        residual = x\n        x = self.norm(x)\n        gate, value = self.linear1(x).chunk(2, dim=-1)\n        x = self.linear2(self.dropout(value * F.silu(gate)))\n        return residual + x * self.scale\n\n\nclass FeatureInteractionPredictor(nn.Module):\n    def __init__(\n        self,\n        categorical_cardinalities: list[int],  # 离散特征的类别数量 [feature1_classes, feature2_classes, ...]\n        num_continuous: int,                   # 连续特征的数量\n        dim_model: int,                        # 模型隐藏层维度\n        dim_feedforward: int,                  # 前馈网络中间层维度\n        num_layers: int,                       # SwiGLU 层数\n        dropout: float = 0.                    # Dropout 比率\n    ):\n        super().__init__()\n        self.dim_model = dim_model\n\n        # 离散特征嵌入权重，控制每个特征嵌入的重要性\n        self.embedding_weights = nn.Parameter(torch.zeros(len(categorical_cardinalities)))\n\n        # 连续特征处理层\n        self.continuous_projection = nn.Linear(num_continuous, 1)\n\n        # 特征嵌入层，将离散特征映射到连续向量空间\n        self.feature_embeddings = nn.ModuleList(\n            nn.Embedding(num_categories, dim_model)\n            for num_categories in categorical_cardinalities\n        )\n\n        # SwiGLU 交互层，学习特征间的高阶交互\n        self.interaction_layers = nn.ModuleList(\n            SwiGLU(dim_model, dim_feedforward, dropout)\n            for _ in range(num_layers)\n        )\n\n        # 预测器，将隐藏表示映射到预测值\n        self.predictor = nn.Linear(dim_model, 1)\n\n        # 输出缩放因子，控制预测输出的尺度\n        self.output_scale = nn.Parameter(torch.zeros(1))\n\n        # 初始化权重\n        nn.init.zeros_(self.predictor.bias)\n        nn.init.zeros_(self.continuous_projection.bias)\n        for module in [self.predictor, self.continuous_projection, *self.feature_embeddings]:\n            nn.init.xavier_uniform_(module.weight)\n\n    def forward(self, categorical_features: torch.LongTensor, continuous_features: torch.Tensor) -> torch.Tensor:\n        # 加权求和所有特征的嵌入表示\n        x = torch.zeros(categorical_features.size(0), self.dim_model, device=categorical_features.device)\n        for feature_idx, feature_column in enumerate(categorical_features.T):  # 遍历每个特征列\n            embedding = self.feature_embeddings[feature_idx](feature_column)\n            weight = self.embedding_weights[feature_idx]\n            x = x + embedding * weight\n\n        # 通过交互层学习特征间复杂关系\n        for interaction_layer in self.interaction_layers:\n            x = interaction_layer(x)\n\n        # 预测并调整输出尺度\n        x = self.predictor(x).squeeze(-1) * self.output_scale\n\n        # 加入连续特征的线性贡献\n        x = x + self.continuous_projection(continuous_features).squeeze(-1)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T14:59:45.140096Z","iopub.execute_input":"2025-11-09T14:59:45.140367Z","iopub.status.idle":"2025-11-09T14:59:45.165866Z","shell.execute_reply.started":"2025-11-09T14:59:45.140348Z","shell.execute_reply":"2025-11-09T14:59:45.164671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 数据集和 collate_fn\nimport json\nfrom typing import Any\nimport pandas as pd\nfrom torch.utils.data import Dataset\n\n\n# 定义一个数据集\nclass FeatureInteractionDataset(Dataset):\n    def __init__(self, dataset: list[tuple[int, list[int], list[float], float]]):\n        super().__init__()\n        self.dataset = dataset\n\n    def __getitem__(self, idx: int) -> tuple[int, list[int], list[float], float]:\n        return self.dataset[idx]\n\n    def __len__(self) -> int:\n        return len(self.dataset)\n\n\ndef collate_fn(batch: list[tuple[int, list[int], list[float], float]]):\n    batch = [\n        torch.tensor(item)\n        for item in zip(*batch)\n    ]\n    return [\n        item.to(dtype=torch.float32) if item.is_floating_point() else item\n        for item in batch\n    ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T14:59:45.167137Z","iopub.execute_input":"2025-11-09T14:59:45.167557Z","iopub.status.idle":"2025-11-09T14:59:45.192448Z","shell.execute_reply.started":"2025-11-09T14:59:45.167526Z","shell.execute_reply":"2025-11-09T14:59:45.191311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 训练循环\nimport torch\nfrom tqdm import tqdm\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.amp import GradScaler, autocast\nfrom torch.utils.tensorboard import SummaryWriter\n\n# 获取设备\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# 创建模型、优化器、梯度缩放器\nmodel = FeatureInteractionPredictor(\n    [feature_bin_counts[feature_name] for feature_name in sorted_columns],\n    len(feature_standardizers),\n    DIM_MODEL, DIM_FEEDFORWARD, NUM_LAYERS, DROPOUT\n).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nscaler = GradScaler(device)\n\n# 创造训练、验证、测试加载器\ntrain_loader, val_loader, test_loader = [\n    DataLoader(FeatureInteractionDataset(dataset), batch_size=batch_size, collate_fn=collate_fn)\n    for dataset, batch_size in [\n        (train_dataset, BATCH_SIZE),\n        (val_dataset, BATCH_SIZE * 2),\n        (test_dataset, BATCH_SIZE * 2)\n    ]\n]\n\n# 训练循环\ncurrent_steps = 0\nprogress_bar = tqdm(desc=\"Train\", total=VAL_PER_STEP * NUM_VAL_CYCLES)\nwhile current_steps < VAL_PER_STEP * NUM_VAL_CYCLES:\n    for batch in train_loader:\n        batch = [x.to(device) for x in batch[1:]]\n        with autocast(device, dtype=torch.float16):\n            logits = model(*batch[:-1])\n            loss = F.binary_cross_entropy_with_logits(logits, batch[-1])\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        # 更新计数和进度条\n        current_steps += 1\n        progress_bar.update()","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-11-09T15:15:37.657217Z","iopub.execute_input":"2025-11-09T15:15:37.657568Z","execution_failed":"2025-11-09T15:15:38.486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 跑一遍结果\nfrom tqdm import tqdm\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nmodel.eval()\nwith open(\"/kaggle/working/submission.csv\", \"w\") as f:\n    f.write(\"id\")\n    with torch.inference_mode():\n        for batch in tqdm(test_loader):\n            data_indices = batch[0].tolist()\n            batch = [x.to(device) for x in batch[1:-1]]\n            outputs = F.sigmoid(model(*batch))\n            for data_idx, output in zip(data_indices, outputs.cpu().tolist()):\n                f.write(f\"{data_idx},{output}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:02:02.879149Z","iopub.execute_input":"2025-11-09T15:02:02.879586Z","iopub.status.idle":"2025-11-09T15:02:05.724067Z","shell.execute_reply.started":"2025-11-09T15:02:02.879562Z","shell.execute_reply":"2025-11-09T15:02:05.722749Z"}},"outputs":[],"execution_count":null}]}