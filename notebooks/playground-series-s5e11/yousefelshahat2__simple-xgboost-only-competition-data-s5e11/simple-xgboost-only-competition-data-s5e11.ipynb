{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":193.167856,"end_time":"2025-10-28T00:13:10.62552","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-28T00:09:57.457664","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade xgboost scikit-learn","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-05T18:10:33.954544Z","iopub.execute_input":"2025-11-05T18:10:33.954844Z","iopub.status.idle":"2025-11-05T18:10:37.431561Z","shell.execute_reply.started":"2025-11-05T18:10:33.954823Z","shell.execute_reply":"2025-11-05T18:10:37.430767Z"},"papermill":{"duration":13.691795,"end_time":"2025-10-28T00:10:15.020035","exception":false,"start_time":"2025-10-28T00:10:01.32824","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-05T18:10:37.433043Z","iopub.execute_input":"2025-11-05T18:10:37.433292Z","iopub.status.idle":"2025-11-05T18:10:37.437257Z","shell.execute_reply.started":"2025-11-05T18:10:37.433272Z","shell.execute_reply":"2025-11-05T18:10:37.436658Z"},"papermill":{"duration":3.157009,"end_time":"2025-10-28T00:10:18.182342","exception":false,"start_time":"2025-10-28T00:10:15.025333","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")\ntarget = df.columns.tolist()[-1]\nprint(df.shape)\ndf.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-05T18:10:37.438067Z","iopub.execute_input":"2025-11-05T18:10:37.438355Z"},"papermill":{"duration":1.355719,"end_time":"2025-10-28T00:10:19.542837","exception":false,"start_time":"2025-10-28T00:10:18.187118","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_frequency_features(df, df_test):\n    \"\"\"\n    Add frequency and binning features efficiently.\n\n    - For each categorical column, create <col>_freq = how often each value appears in train data.\n    - For numeric columns, split values into 5, 10, 15 quantile bins.\n    \"\"\"\n    # Pre-allocate DataFrames for new features to avoid fragmentation\n    freq_features_train = pd.DataFrame(index=df.index)\n    freq_features_test = pd.DataFrame(index=df_test.index)\n    bin_features_train = pd.DataFrame(index=df.index)\n    bin_features_test = pd.DataFrame(index=df_test.index)\n\n    for col in cols:\n        # --- Frequency encoding ---\n        freq = df[col].value_counts()\n        df[f\"{col}_freq\"] = df[col].map(freq)\n        freq_features_test[f\"{col}_freq\"] = df_test[col].map(freq).fillna(freq.mean())\n\n        # --- Quantile binning for numeric columns ---\n        if col in num:\n            for q in [5, 10, 15]:\n                try:\n                    train_bins, bins = pd.qcut(df[col], q=q, labels=False, retbins=True, duplicates=\"drop\")\n                    bin_features_train[f\"{col}_bin{q}\"] = train_bins\n                    bin_features_test[f\"{col}_bin{q}\"] = pd.cut(df_test[col], bins=bins, labels=False, include_lowest=True)\n                except Exception:\n                    bin_features_train[f\"{col}_bin{q}\"] = 0\n                    bin_features_test[f\"{col}_bin{q}\"] = 0\n\n    # Concatenate all new features at once\n    df = pd.concat([df, freq_features_train, bin_features_train], axis=1)\n    df_test = pd.concat([df_test, freq_features_test, bin_features_test], axis=1)\n\n    return df, df_test","metadata":{"papermill":{"duration":0.01394,"end_time":"2025-10-28T00:10:19.561684","exception":false,"start_time":"2025-10-28T00:10:19.547744","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I got the target Encoding and some other feature engineering parts from [安尾 晃貴](https://www.kaggle.com/code/sidakou/simple-xgboost-baseline-for-loan-payback)","metadata":{}},{"cell_type":"code","source":"def target_encoding(train, predict, n_splits=5):\n    \"\"\"\n    Add K-Fold target mean encoded features to train and predict datasets.\n    \"\"\"\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    mean_features_train = pd.DataFrame(index=train.index)\n    mean_features_test = pd.DataFrame(index=predict.index)\n\n    global_target_mean = train[target].mean()\n\n    for col in cols:\n        # --- K-Fold Target Mean Encoding ---\n        mean_encoded = np.zeros(len(train))\n        for tr_idx, val_idx in kf.split(train):\n            tr_fold = train.iloc[tr_idx]\n            val_fold = train.iloc[val_idx]\n            mean_map = tr_fold.groupby(col)[target].mean()\n            mean_encoded[val_idx] = val_fold[col].map(mean_map)\n\n        # Handle NaN in training folds\n        mean_encoded = np.where(np.isnan(mean_encoded), global_target_mean, mean_encoded)\n        mean_features_train[f'mean_{col}'] = mean_encoded\n\n        # --- Apply global mean mapping to prediction/test data ---\n        global_mean = train.groupby(col)[target].mean()\n        mean_features_test[f'mean_{col}'] = predict[col].map(global_mean)\n\n        # Handle NaN in test data\n        mean_features_test[f'mean_{col}'] = mean_features_test[f'mean_{col}'].fillna(global_target_mean)\n\n    # --- Concatenate new features at once to avoid fragmentation ---\n    train = pd.concat([train, mean_features_train], axis=1)\n    predict = pd.concat([predict, mean_features_test], axis=1)\n\n    # Defragment\n    train = train.copy()\n    predict = predict.copy()\n    return train, predict\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Processing","metadata":{"papermill":{"duration":0.004543,"end_time":"2025-10-28T00:10:19.570951","exception":false,"start_time":"2025-10-28T00:10:19.566408","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Specific feature engineering\ndf['subgrade'] = df['grade_subgrade'].str[1:].astype(int)\ndf_test['subgrade'] = df_test['grade_subgrade'].str[1:].astype(int)\n\ndf['grade'] = df['grade_subgrade'].str[0]\ndf_test['grade'] = df_test['grade_subgrade'].str[0]\n\n# Identify feature\ncols = df.drop(columns=[target,\"id\"]).columns.tolist()\n\n# Categorical features\ncat = [c for c in cols if df[c].dtype in [\"object\",\"category\"]]\n\n# Numerical features\nnum = [c for c in cols if df[c].dtype not in [\"object\",\"category\",\"bool\"]]\n\n# Creating new features based on the frequency of numerical features\ndf, df_test = target_encoding(df, df_test, 10)\ndf, df_test = create_frequency_features(df, df_test)\n\n# Preparing categorical features\ndf[cat], df_test[cat] = df[cat].astype(\"category\"), df_test[cat].astype(\"category\")\n\n# Dropping unnecessary columns\nremove = [\"education_level\",\"loan_purpose\", \"grade_subgrade\", \"interest_rate\", \"marital_status\",\n          \"gender\", \"employment_status_freq\", \"credit_score_bin5\",  \"loan_amount_bin5\",\n          \"credit_score_freq\",\"mean_subgrade\", \"subgrade_bin15\", \"subgrade_bin10\"\n          ,\"debt_to_income_ratio_bin5\"]\ndf, df_test = df.drop(columns = remove), df_test.drop(columns = remove)\n\n# Dropping ID and duplicates\ndf.drop(columns=\"id\", inplace=True)\ndf.drop_duplicates(inplace=True)","metadata":{"papermill":{"duration":2.074422,"end_time":"2025-10-28T00:10:21.649842","exception":false,"start_time":"2025-10-28T00:10:19.57542","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of columns {len(df.columns.tolist())}\")\nprint(df.columns.tolist())","metadata":{"papermill":{"duration":0.01109,"end_time":"2025-10-28T00:10:21.665921","exception":false,"start_time":"2025-10-28T00:10:21.654831","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()[lambda x: x>0] # Null values count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CV score of the model","metadata":{"papermill":{"duration":0.004914,"end_time":"2025-10-28T00:10:21.710234","exception":false,"start_time":"2025-10-28T00:10:21.70532","status":"completed"},"tags":[]}},{"cell_type":"code","source":"dtrain = xgb.DMatrix(\n    df.drop(columns=[target]),\n    label=df[target],\n    enable_categorical=True,\n)\n\nxgb_params = {\n    'tree_method': 'hist',\n    'device': 'cuda',\n    'eval_metric': 'auc',\n    'objective': 'binary:logistic',\n    'random_state': 42,\n    'scale_pos_weight':1,\n    'min_child_weight': 89,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'colsample_bylevel': 1,\n    'colsample_bynode': 1,\n    \"gamma\":0,\n    \"max_leaves\":4,\n    \"reg_alpha\":1.4,\n    \"reg_lambda\":5.9,\n    \"scale_pos_weight\":1,\n    \"eta\":0.3\n}\n\ncv_results = xgb.cv(\n    params=xgb_params,\n    dtrain=dtrain,\n    nfold=7,\n    num_boost_round=20000,\n    metrics='auc',\n    verbose_eval=False,\n    early_stopping_rounds=100,\n)\n\nprint(cv_results.tail())\n\n# Extract best boosting round\nbest_round = cv_results['test-auc-mean'].idxmax()\nbest_auc = cv_results['test-auc-mean'][best_round]\nprint(f\"Best round: {best_round}, Best CV AUC: {best_auc:.7f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# putting the n_estimator at the average early stopping point to avoid overfitting\nlast_round = len(cv_results) - 1\nxgb_params[\"n_estimators\"] = last_round + 100","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.011999,"end_time":"2025-10-28T00:12:40.934579","exception":false,"start_time":"2025-10-28T00:12:40.92258","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final training and submitting","metadata":{"papermill":{"duration":0.005473,"end_time":"2025-10-28T00:12:40.946353","exception":false,"start_time":"2025-10-28T00:12:40.94088","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Prepare training data\nX_train = df.drop(columns=target)\ny_train = df[target]\n\n# Train XGBoost model\nmodel = XGBClassifier(**xgb_params, enable_categorical=True)\nmodel.fit(X_train, y_train)\n\n# Predict on test set\npred = model.predict_proba(df_test.drop(columns = \"id\"))[:, 1]\n\n# Prepare submission\nsub = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    target: pred\n})\n\n# Save submission file\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"_kg_hide-input":false,"papermill":{"duration":28.836341,"end_time":"2025-10-28T00:13:09.788494","exception":false,"start_time":"2025-10-28T00:12:40.952153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}