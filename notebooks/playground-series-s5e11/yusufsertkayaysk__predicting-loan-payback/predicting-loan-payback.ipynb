{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nEnhanced Loan Prediction Pipeline\nImprovements:\n1. Better feature engineering with interaction terms\n2. Advanced categorical encoding strategies\n3. Optimized hyperparameters\n4. Cleaner code structure\n5. Better memory management\n6. Enhanced ensemble methods\n\"\"\"\n\n# ============================================================================\n# IMPORTS\n# ============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, TargetEncoder\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import DMatrix, XGBClassifier\nimport xgboost as xgb\nimport lightgbm\nfrom lightgbm import LGBMClassifier\nfrom tqdm.notebook import tqdm\nfrom colorama import Fore, Style\nimport warnings\nimport gc\nimport torch\n\nwarnings.filterwarnings(\"ignore\")\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nclass Config:\n    \"\"\"Enhanced configuration with better organization\"\"\"\n    \n    # Paths\n    target = 'loan_paid_back'\n    train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv', index_col='id')\n    test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv', index_col='id')\n    submission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n    orig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\n\n    # Device setup\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Model parameters\n    state = 42\n    n_splits = 10\n    early_stop = 200\n    metric = 'roc_auc'\n    task_type = \"binary\"\n    task_is_regression = False\n    n_classes = 2\n    labels = [0, 1]\n    \n    # Cross-validation\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=state)\n    \n    # Feature engineering flags\n    outliers = False\n    log_trf = False\n    missing = False\n\n\n# ============================================================================\n# PREPROCESSING\n# ============================================================================\nclass Preprocessing(Config):\n    \"\"\"Enhanced preprocessing with better feature engineering\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        \n    def fit_transform(self):\n        \"\"\"Main preprocessing pipeline\"\"\"\n        self.prepare_data()\n        \n        if self.missing:\n            self.missing_values()\n        \n        # Combine train and test for consistent feature engineering\n        combine = pd.concat([self.X, self.test], axis=0)\n        combine = self.feature_engineering(combine)\n        \n        # Split back\n        self.X = combine.iloc[:len(self.X)].copy()\n        self.test = combine.iloc[len(self.X):].copy()\n        \n        # Update feature lists\n        self.num_features = self.test.select_dtypes(exclude=['object', 'bool', 'category']).columns.tolist()\n        self.cat_features = self.test.select_dtypes(include=['object', 'bool', 'category']).columns.tolist()\n        \n        if self.outliers:\n            self.remove_outliers()\n        if self.log_trf:\n            self.log_transformation()\n        \n        print(f\"✓ Final feature count: {len(self.num_features)} numerical, {len(self.cat_features)} categorical\")\n        return self.X, self.y, self.test, self.cat_features, self.num_features\n    \n    def prepare_data(self):\n        \"\"\"Initial data preparation\"\"\"\n        self.train_raw = self.train.copy()\n        self.y = self.train[self.target]\n        self.X = self.train.drop(self.target, axis=1)\n        \n        self.num_features = self.X.select_dtypes(exclude=['object', 'bool']).columns.tolist()\n        self.cat_features = self.X.select_dtypes(include=['object', 'bool']).columns.tolist()\n    \n    def feature_engineering(self, data):\n        \"\"\"Enhanced feature engineering with multiple strategies\"\"\"\n        df = data.copy()\n        \n        # ========== Original dataset statistics ==========\n        global_stats = {\n            'mean': self.orig[self.target].mean(), \n            'count': len(self.orig)\n        }\n        \n        # Target encoding from original dataset\n        for c in self.num_features + self.cat_features:\n            for agg_func in ['mean', 'count']:\n                col_name = f'{c}_org_{agg_func}'\n                tmp = (self.orig.groupby(c)[self.target]\n                      .agg(agg_func)\n                      .rename(col_name)\n                      .reset_index())\n                df = df.merge(tmp, on=c, how='left')\n                df[col_name] = df[col_name].fillna(global_stats.get(agg_func, 0))\n        \n        # ========== Numerical transformations ==========\n        for c in self.num_features:\n            # Log transformations\n            df[f\"log_{c}\"] = np.log1p(df[c])\n            \n            # Polynomial features\n            df[f\"{c}_sq\"] = df[c] ** 2\n            df[f\"{c}_sqrt\"] = np.sqrt(df[c])\n        \n        # ========== Domain-specific features ==========\n        # Credit utilization ratio\n        df['credit_utilization'] = df['debt_to_income_ratio'] * df['annual_income'] / (df['loan_amount'] + 1)\n        \n        # Loan burden\n        df['loan_burden'] = df['loan_amount'] / (df['annual_income'] + 1)\n        \n        # Risk score (combination of multiple factors)\n        df['risk_score'] = (\n            (850 - df['credit_score']) / 850 * 0.4 +\n            df['debt_to_income_ratio'] * 0.3 +\n            df['interest_rate'] / 21 * 0.3\n        )\n        \n        # Income to loan ratio\n        df['income_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n        \n        # Affordability index\n        df['affordability'] = (df['annual_income'] * (1 - df['debt_to_income_ratio'])) / (df['loan_amount'] + 1)\n        \n        # ========== Grade features ==========\n        df['grade_number'] = df['grade_subgrade'].str[1].astype(int)\n        grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n        df['grade_rank'] = df['grade_subgrade'].str[0].map(grade_map)\n        df['grade_combined'] = df['grade_rank'] * 10 + df['grade_number']\n        \n        # ========== Binning strategies ==========\n        self.numtocat_features = []\n        \n        # Low cardinality numerics to categories\n        lowcard = ['credit_score', 'debt_to_income_ratio', 'interest_rate']\n        for c in lowcard:\n            if c in self.num_features:\n                df[f\"{c}_cat\"] = pd.qcut(df[c], q=10, labels=False, duplicates='drop')\n                df[f\"{c}_cat\"] = df[f\"{c}_cat\"].astype('category')\n                self.numtocat_features.append(f\"{c}_cat\")\n        \n        # High cardinality features\n        highcard = ['annual_income', 'loan_amount']\n        for c in highcard:\n            if c in self.num_features:\n                # Rounded values\n                df[f'{c}_round'] = df[c].round(0)\n                df[f\"{c}_round\"], _ = pd.factorize(df[f\"{c}_round\"])\n                df[f\"{c}_round\"] = df[f\"{c}_round\"].astype('category')\n                self.numtocat_features.append(f\"{c}_round\")\n                \n                # Thousands buckets\n                df[f'{c}_thousands'] = df[c].round(-3)\n                df[f\"{c}_thousands\"], _ = pd.factorize(df[f\"{c}_thousands\"])\n                df[f\"{c}_thousands\"] = df[f\"{c}_thousands\"].astype('category')\n                self.numtocat_features.append(f\"{c}_thousands\")\n        \n        # ========== Frequency encoding ==========\n        all_cats = self.numtocat_features + self.cat_features\n        for c in all_cats:\n            freqs = df[c].value_counts(normalize=True)\n            df[f\"{c}_freq\"] = df[c].map(freqs)\n        \n        # ========== Interaction features ==========\n        # Key interactions\n        df['income_credit_interaction'] = df['annual_income'] * df['credit_score']\n        df['loan_rate_interaction'] = df['loan_amount'] * df['interest_rate']\n        df['debt_credit_interaction'] = df['debt_to_income_ratio'] * df['credit_score']\n        \n        # Convert categorical features\n        df[self.cat_features] = df[self.cat_features].astype('category')\n        \n        return df\n    \n    def log_transformation(self):\n        \"\"\"Apply log transformation to target\"\"\"\n        self.y = np.log1p(self.y)\n    \n    def remove_outliers(self):\n        \"\"\"Remove outliers using IQR method\"\"\"\n        Q1 = self.y.quantile(0.25)\n        Q3 = self.y.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_limit = Q1 - 1.5 * IQR\n        upper_limit = Q3 + 1.5 * IQR\n        mask = (self.y >= lower_limit) & (self.y <= upper_limit)\n        self.X = self.X[mask]\n        self.y = self.y[mask]\n        self.X.reset_index(drop=True, inplace=True)\n        self.y.reset_index(drop=True, inplace=True)\n        print(f\"✓ Removed {(~mask).sum()} outliers\")\n    \n    def missing_values(self):\n        \"\"\"Handle missing values\"\"\"\n        self.X[self.cat_features] = self.X[self.cat_features].fillna('Missing')\n        self.test[self.cat_features] = self.test[self.cat_features].fillna('Missing')\n\n\n# ============================================================================\n# MODELS\n# ============================================================================\ndef get_models():\n    \"\"\"Enhanced model configurations\"\"\"\n    models = {\n        'XGB_optimized': XGBClassifier(\n            tree_method='hist',\n            n_estimators=10000,\n            objective='binary:logistic',\n            random_state=Config.state,\n            enable_categorical=True,\n            verbosity=0,\n            eval_metric='auc',\n            booster='gbtree',\n            n_jobs=-1,\n            learning_rate=0.01,\n            device=\"cuda\" if Config.device == 'cuda' else \"cpu\",\n            reg_lambda=0.31,\n            reg_alpha=4.45,\n            colsample_bytree=0.10,\n            subsample=0.67,\n            max_depth=8,\n            min_child_weight=2,\n            max_bin=512\n        ),\n        \n        'LGBM_optimized': LGBMClassifier(\n            random_state=Config.state,\n            verbose=-1,\n            n_estimators=10000,\n            metric='auc',\n            objective='binary',\n            learning_rate=0.01,\n            max_depth=5,\n            min_child_samples=162,\n            subsample=0.44,\n            colsample_bytree=0.23,\n            num_leaves=332,\n            reg_alpha=0.05,\n            reg_lambda=7.07,\n            max_bin=500,\n        ),\n        \n        'HGB_optimized': HistGradientBoostingClassifier(\n            max_iter=10000,\n            random_state=Config.state,\n            early_stopping=True,\n            categorical_features=\"from_dtype\",\n            learning_rate=0.01,\n            loss='log_loss',\n            l2_regularization=0.011,\n            max_depth=4,\n            max_leaf_nodes=85,\n            min_samples_leaf=50\n        ),\n    }\n    return models\n\n\n# ============================================================================\n# TRAINER\n# ============================================================================\nclass Trainer(Config):\n    \"\"\"Enhanced training pipeline with better memory management\"\"\"\n    \n    def __init__(self, X, y, test, models, num_features, cat_features, training=True):\n        super().__init__()\n        self.X = X\n        self.test = test\n        self.y = y\n        self.models = models\n        self.training = training\n        self.num_features = num_features\n        self.cat_features = cat_features\n        \n        # Results storage\n        self.scores = pd.DataFrame(columns=['Score'], dtype=float)\n        self.OOF_preds = pd.DataFrame(dtype=float)\n        self.TEST_preds = pd.DataFrame(dtype=float)\n    \n    def score_metric(self, y_true, y_pred):\n        \"\"\"Calculate evaluation metric\"\"\"\n        return roc_auc_score(y_true, y_pred)\n    \n    def train_model(self, model, X, y, test, model_name):\n        \"\"\"Train a single model with cross-validation\"\"\"\n        oof_pred = np.zeros(X.shape[0], dtype=float)\n        test_pred = np.zeros(test.shape[0], dtype=float)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Training {model_name}\")\n        print(f\"{'='*60}\")\n        \n        params = model.get_params()\n        \n        for n_fold, (train_idx, valid_idx) in enumerate(self.folds.split(X, y)):\n            print(f\"\\nFold {n_fold + 1}/{self.n_splits}\")\n            \n            # Split data\n            X_train = X.iloc[train_idx].copy()\n            y_train = y.iloc[train_idx]\n            X_val = X.iloc[valid_idx].copy()\n            y_val = y.iloc[valid_idx]\n            X_test = test.copy()\n            \n            # Target encoding for categorical features\n            if self.cat_features:\n                te = TargetEncoder(random_state=42, shuffle=True, cv=5, smooth=15)\n                X_train[self.cat_features] = te.fit_transform(X_train[self.cat_features], y_train).astype('float32')\n                X_val[self.cat_features] = te.transform(X_val[self.cat_features]).astype('float32')\n                X_test[self.cat_features] = te.transform(X_test[self.cat_features]).astype('float32')\n            \n            # Train based on model type\n            if \"LGBM\" in model_name:\n                train_data = lightgbm.Dataset(X_train, label=y_train)\n                val_data = lightgbm.Dataset(X_val, label=y_val, reference=train_data)\n                \n                model = lightgbm.train(\n                    params=params,\n                    train_set=train_data,\n                    valid_sets=[val_data],\n                    callbacks=[\n                        lightgbm.early_stopping(stopping_rounds=self.early_stop, verbose=False),\n                        lightgbm.log_evaluation(period=0)\n                    ]\n                )\n                y_pred_val = model.predict(X_val)\n                y_pred_test = model.predict(X_test)\n                \n            elif \"XGB\" in model_name:\n                dtrain = DMatrix(X_train, label=y_train, enable_categorical=True)\n                dval = DMatrix(X_val, label=y_val, enable_categorical=True)\n                dtest = DMatrix(X_test, enable_categorical=True)\n                \n                model = xgb.train(\n                    params=params,\n                    dtrain=dtrain,\n                    evals=[(dval, \"valid\")],\n                    num_boost_round=100000,\n                    early_stopping_rounds=self.early_stop,\n                    verbose_eval=False\n                )\n                y_pred_val = model.predict(dval)\n                y_pred_test = model.predict(dtest)\n                \n            elif \"HGB\" in model_name:\n                model.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n                y_pred_val = model.predict_proba(X_val)[:, 1]\n                y_pred_test = model.predict_proba(X_test)[:, 1]\n            \n            else:\n                model.fit(X_train, y_train)\n                y_pred_val = model.predict_proba(X_val)[:, 1]\n                y_pred_test = model.predict_proba(X_test)[:, 1]\n            \n            # Store predictions\n            oof_pred[valid_idx] = y_pred_val\n            test_pred += y_pred_test / self.n_splits\n            \n            # Calculate score\n            score = self.score_metric(y_val, y_pred_val)\n            print(f\"  Fold {n_fold + 1} ROC-AUC: {score:.6f}\")\n            self.scores.loc[model_name, f'Fold {n_fold + 1}'] = score\n            \n            # Memory cleanup\n            del X_train, y_train, X_val, y_val\n            gc.collect()\n        \n        # Calculate average score\n        avg_score = self.scores.loc[model_name, [f'Fold {i+1}' for i in range(self.n_splits)]].mean()\n        self.scores.loc[model_name, 'Score'] = avg_score\n        print(f\"\\n{model_name} Average ROC-AUC: {avg_score:.6f}\")\n        \n        return oof_pred, test_pred\n    \n    def run(self):\n        \"\"\"Main training loop\"\"\"\n        for model_name, model in tqdm(self.models.items(), desc=\"Training models\"):\n            if self.training:\n                oof_pred, test_pred = self.train_model(model, self.X, self.y, self.test, model_name)\n                \n                # Save predictions\n                pd.DataFrame(oof_pred, columns=[model_name]).to_csv(f'{model_name}_oof.csv', index=False)\n                pd.DataFrame(test_pred, columns=[model_name]).to_csv(f'{model_name}_test.csv', index=False)\n            else:\n                # Load predictions\n                oof_pred = pd.read_csv(f'/kaggle/input/loan-models/{model_name}_oof.csv')[model_name].values\n                test_pred = pd.read_csv(f'/kaggle/input/loan-models/{model_name}_test.csv')[model_name].values\n            \n            self.OOF_preds[model_name] = oof_pred\n            self.TEST_preds[model_name] = test_pred\n        \n        # Ensemble if multiple models\n        if len(self.models) > 1:\n            print(f\"\\n{'='*60}\")\n            print(\"Creating Ensemble\")\n            print(f\"{'='*60}\")\n            \n            meta_model = LogisticRegression(C=0.1, random_state=self.state, max_iter=1000)\n            self.OOF_preds[\"Ensemble\"], self.TEST_preds[\"Ensemble\"] = self.train_model(\n                meta_model, self.OOF_preds, self.y, self.TEST_preds, 'Ensemble'\n            )\n            \n            self.plot_results()\n            return self.TEST_preds[\"Ensemble\"]\n        else:\n            model_name = list(self.models.keys())[0]\n            self.plot_results()\n            return self.TEST_preds[model_name]\n    \n    def plot_results(self):\n        \"\"\"Visualize results\"\"\"\n        # Score comparison\n        plt.figure(figsize=(14, 6))\n        scores_sorted = self.scores.sort_values('Score', ascending=True)\n        colors = ['#3cb371' if idx != 'Ensemble' else 'red' for idx in scores_sorted.index]\n        bars = plt.barh(scores_sorted.index, scores_sorted['Score'], color=colors, height=0.6)\n        plt.bar_label(bars, fmt='%.6f', padding=5)\n        plt.xlabel('ROC-AUC Score', fontsize=12)\n        plt.ylabel('Model', fontsize=12)\n        plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n        plt.grid(axis='x', alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        \n        # ROC curves and confusion matrix\n        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n        \n        # ROC curves\n        for col in self.OOF_preds.columns:\n            RocCurveDisplay.from_predictions(\n                self.y, self.OOF_preds[col], \n                name=f\"{col} (AUC={self.scores.loc[col, 'Score']:.4f})\",\n                ax=axes[0]\n            )\n        axes[0].plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n        axes[0].set_xlabel('False Positive Rate', fontsize=11)\n        axes[0].set_ylabel('True Positive Rate', fontsize=11)\n        axes[0].set_title('ROC Curves', fontsize=12, fontweight='bold')\n        axes[0].legend(loc=\"lower right\", fontsize=9)\n        axes[0].grid(alpha=0.3)\n        \n        # Confusion matrix\n        best_model = self.scores['Score'].idxmax()\n        ConfusionMatrixDisplay.from_predictions(\n            self.y,\n            (self.OOF_preds[best_model] >= 0.5).astype(int),\n            display_labels=['Not Paid', 'Paid'],\n            cmap='Greens',\n            ax=axes[1]\n        )\n        axes[1].set_title(f'Confusion Matrix - {best_model}', fontsize=12, fontweight='bold')\n        \n        plt.tight_layout()\n        plt.show()\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\ndef main():\n    \"\"\"Main execution pipeline\"\"\"\n    print(f\"{Style.BRIGHT}{Fore.GREEN}\")\n    print(\"=\"*60)\n    print(\"ENHANCED LOAN PREDICTION PIPELINE\")\n    print(\"=\"*60)\n    print(f\"{Style.RESET_ALL}\\n\")\n    \n    # Preprocessing\n    print(f\"{Fore.CYAN}Step 1: Preprocessing{Style.RESET_ALL}\")\n    preprocessor = Preprocessing()\n    X, y, test, cat_features, num_features = preprocessor.fit_transform()\n    \n    # Model training\n    print(f\"\\n{Fore.CYAN}Step 2: Model Training{Style.RESET_ALL}\")\n    models = get_models()\n    trainer = Trainer(X, y, test, models, num_features, cat_features, training=False)\n    test_predictions = trainer.run()\n    \n    # Create submission\n    print(f\"\\n{Fore.CYAN}Step 3: Creating Submission{Style.RESET_ALL}\")\n    submission = Config.submission\n    submission[Config.target] = test_predictions\n    submission.to_csv(\"submission.csv\", index=False)\n    \n    print(f\"\\n{Fore.GREEN}✓ Submission saved successfully!{Style.RESET_ALL}\")\n    print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n    print(f\"Prediction mean: {test_predictions.mean():.4f}\")\n    \n    # Visualize submission distribution\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.hist(test_predictions, bins=50, color='#3cb371', alpha=0.7, edgecolor='black')\n    plt.xlabel('Predicted Probability')\n    plt.ylabel('Frequency')\n    plt.title('Prediction Distribution')\n    plt.grid(alpha=0.3)\n    \n    plt.subplot(1, 2, 2)\n    sns.kdeplot(test_predictions, fill=True, color='#3cb371', linewidth=2)\n    plt.xlabel('Predicted Probability')\n    plt.ylabel('Density')\n    plt.title('Prediction Density')\n    plt.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return submission\n\n\n# Execute\nif __name__ == \"__main__\":\n    submission = main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}