{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Lightweight Mask R-CNN for Scientific Image Forgery Detection","metadata":{}},{"cell_type":"markdown","source":"# üßæ Summary\n\nThis notebook implements a complete scientific image forgery detection pipeline using PyTorch Mask R-CNN with a MobileNetV3 backbone for lightweight, efficient training and inference.\n\nData Analysis & Visualization\n\nThe dataset consists of authentic, forged, and masked scientific images.\n\nFunctions analyze folder structure, image size distributions, and visualize samples alongside corresponding masks.\n\nDataset Preparation\n\nA custom ForgeryDataset class handles loading images and masks, aligning them, and generating bounding boxes for forged regions.\n\nIt supports both authentic and forged samples, automatically creating appropriate targets for Mask R-CNN training.\n\nModel Definition\n\nA lightweight Mask R-CNN model is built using MobileNetV3-Small as the backbone for efficiency.\n\nThe architecture is customized with appropriate anchor generators and ROI pooling layers.\n\nTraining Pipeline\n\nThe Trainer class manages the training and validation process using AdamW optimizer and StepLR scheduler.\n\nThe training loop computes loss per epoch and visualizes loss trends.\n\nData augmentations (flip, rotation, blur, brightness/contrast adjustment) are applied using Albumentations.\n\nVisualization and Evaluation\n\nSample batches are visualized to confirm correct image-mask alignment.\n\nTraining and validation loss curves demonstrate learning progression.","metadata":{}},{"cell_type":"code","source":"# Scientific Image Forgery Detection Pipeline\n# Complete implementation with Mask R-CNN\n\nimport os\nimport cv2\nimport json\nimport torch\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import MaskRCNN\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.transforms import functional as F_transforms\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Checking GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nclass ScientificForgeryPipeline:\n    def __init__(self, base_path):\n        self.base_path = base_path\n        self.device = device\n        self.model = None\n        self.paths = {}\n        \n    def analyze_data_structure(self):\n        \"\"\"Analyze the dataset structure and statistics\"\"\"\n        base_path = self.base_path\n        \n        # Checking train images\n        train_authentic_path = os.path.join(base_path, 'train_images/authentic')\n        train_forged_path = os.path.join(base_path, 'train_images/forged')\n        train_masks_path = os.path.join(base_path, 'train_masks')\n        test_images_path = os.path.join(base_path, 'test_images')\n        \n        print(\"üìä Dataset Analysis:\")\n        print(f\"‚úÖ Authentic images: {len(os.listdir(train_authentic_path))}\")\n        print(f\"‚ùå Forged images: {len(os.listdir(train_forged_path))}\")\n        print(f\"üé≠ Masks: {len(os.listdir(train_masks_path))}\")\n        print(f\"üß™ Test images: {len(os.listdir(test_images_path))}\")\n        \n        # Analyze mask format\n        mask_files = os.listdir(train_masks_path)[:3]\n        print(f\"\\nüîç Mask examples: {mask_files}\")\n        \n        if mask_files:\n            sample_mask = np.load(os.path.join(train_masks_path, mask_files[0]))\n            print(f\"üìê Mask shape: {sample_mask.shape}, dtype: {sample_mask.dtype}\")\n        \n        self.paths = {\n            'train_authentic': train_authentic_path,\n            'train_forged': train_forged_path,\n            'train_masks': train_masks_path,\n            'test_images': test_images_path\n        }\n        \n        return self.paths\n    \n    def get_unique_sizes(self, directory):\n        \"\"\"Get unique image sizes in directory\"\"\"\n        size_counts = defaultdict(int)\n        for root, _, files in os.walk(directory):\n            for file in files:\n                if file.lower().endswith(('.png', '.jpg', '.jpeg', 'JPG')):\n                    try:\n                        with Image.open(os.path.join(root, file)) as img:\n                            size = img.size\n                            size_counts[size] += 1\n                    except Exception as e:\n                        print(f\"Error {file}: {e}\")\n\n        return size_counts\n    \n    def visualize_size_distribution(self):\n        \"\"\"Visualize image size distribution\"\"\"\n        folders = [\n            self.paths['train_authentic'],\n            self.paths['train_forged'],\n            self.paths['test_images']\n        ]\n        \n        all_sizes = []\n        \n        for folder in folders:\n            print(f\"\\nüìÇ Folder: {folder}\")\n            sizes = self.get_unique_sizes(folder)\n\n            if not sizes:\n                print(\"No images or mistake in code\")\n                continue\n            \n            sorted_sizes = sorted(sizes.items(), key=lambda x: x[1], reverse=True)\n\n            print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n            print(\"‚îÇ  Width (px)  ‚îÇ Height (px) ‚îÇ Quantity ‚îÇ\")\n            print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n            for (w, h), count in sorted_sizes[:5]:  # Show top 5\n                print(f\"‚îÇ {w:<13} ‚îÇ {h:<13} ‚îÇ {count:<7} ‚îÇ\")\n            print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n            \n            # Collect for overall stats\n            for (w, h), count in sizes.items():\n                all_sizes.extend([(w, h)] * count)\n        \n        # Overall statistics\n        if all_sizes:\n            sizes_df = pd.DataFrame(all_sizes, columns=['width', 'height'])\n            print(\"\\nüìà Overall Image Size Statistics:\")\n            print(sizes_df.describe())\n\n            # Visualization\n            plt.figure(figsize=(12, 4))\n            plt.subplot(1, 2, 1)\n            plt.hist(sizes_df['width'], bins=20, alpha=0.7, color='blue')\n            plt.title('Width Distribution')\n            plt.xlabel('Width (px)')\n            plt.ylabel('Frequency')\n\n            plt.subplot(1, 2, 2)\n            plt.hist(sizes_df['height'], bins=20, alpha=0.7, color='red')\n            plt.title('Height Distribution')\n            plt.xlabel('Height (px)')\n            plt.ylabel('Frequency')\n\n            plt.tight_layout()\n            plt.show()\n    \n    def visualize_samples(self, num_samples=3):\n        \"\"\"Visualize sample images and masks\"\"\"\n        authentic_files = sorted(os.listdir(self.paths['train_authentic']))[:num_samples]\n        forged_files = sorted(os.listdir(self.paths['train_forged']))[:num_samples]\n        mask_files = sorted(os.listdir(self.paths['train_masks']))[:num_samples]\n        \n        fig, axes = plt.subplots(3, num_samples, figsize=(15, 10))\n        \n        # Authentic images\n        for i, file in enumerate(authentic_files):\n            img_path = os.path.join(self.paths['train_authentic'], file)\n            img = Image.open(img_path)\n            axes[0, i].imshow(img)\n            axes[0, i].set_title(f'Authentic: {file}')\n            axes[0, i].axis('off')\n            \n        # Forged images\n        for i, file in enumerate(forged_files):\n            img_path = os.path.join(self.paths['train_forged'], file)\n            img = Image.open(img_path)\n            axes[1, i].imshow(img)\n            axes[1, i].set_title(f'Forged: {file}')\n            axes[1, i].axis('off')\n            \n        # Masks\n        for i, file in enumerate(mask_files):\n            mask_path = os.path.join(self.paths['train_masks'], file)\n            mask = np.load(mask_path)\n            mask = np.squeeze(mask)\n            axes[2, i].imshow(mask, cmap='gray')\n            axes[2, i].set_title(f'Mask: {file}')\n            axes[2, i].axis('off')\n            \n        plt.tight_layout()\n        plt.show()\n\nclass ForgeryDataset(Dataset):\n    def __init__(self, authentic_path, forged_path, masks_path, transform=None, is_train=True):\n        self.transform = transform\n        self.is_train = is_train\n        \n        # Collect all data samples\n        self.samples = []\n        \n        # Authentic images\n        for file in os.listdir(authentic_path):\n            img_path = os.path.join(authentic_path, file)\n            base_name = file.split('.')[0]\n            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n            \n            self.samples.append({\n                'image_path': img_path,\n                'mask_path': mask_path,\n                'is_forged': False,\n                'image_id': base_name\n            })\n        \n        # Forged images\n        for file in os.listdir(forged_path):\n            img_path = os.path.join(forged_path, file)\n            base_name = file.split('.')[0]\n            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n            \n            self.samples.append({\n                'image_path': img_path,\n                'mask_path': mask_path,\n                'is_forged': True,\n                'image_id': base_name\n            })\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Load image\n        image = Image.open(sample['image_path']).convert('RGB')\n        image = np.array(image)\n        \n        # Load and process mask\n        if os.path.exists(sample['mask_path']):\n            mask = np.load(sample['mask_path'])\n            \n            # Handle multi-channel masks\n            if mask.ndim == 3:\n                if mask.shape[0] <= 10:  # channels first (C, H, W)\n                    mask = np.any(mask, axis=0)\n                elif mask.shape[-1] <= 10:  # channels last (H, W, C)\n                    mask = np.any(mask, axis=-1)\n                else:\n                    raise ValueError(f\"Ambiguous 3D mask shape: {mask.shape}\")\n            \n            mask = (mask > 0).astype(np.uint8)\n        else:\n            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n    \n        # Shape validation\n        assert image.shape[:2] == mask.shape, f\"Shape mismatch: img {image.shape}, mask {mask.shape}\"\n        \n        # Apply transformations\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask']\n        else:\n            image = F_transforms.to_tensor(image)\n            mask = torch.tensor(mask, dtype=torch.uint8)\n        \n        # Prepare targets for Mask R-CNN\n        if sample['is_forged'] and mask.sum() > 0:\n            boxes, labels, masks = self.mask_to_boxes(mask)\n            \n            target = {\n                'boxes': boxes,\n                'labels': labels,\n                'masks': masks,\n                'image_id': torch.tensor([idx]),\n                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n                'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n            }\n        else:\n            # For authentic images or images without masks\n            target = {\n                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n                'labels': torch.zeros(0, dtype=torch.int64),\n                'masks': torch.zeros((0, image.shape[1], image.shape[2]), dtype=torch.uint8),\n                'image_id': torch.tensor([idx]),\n                'area': torch.zeros(0, dtype=torch.float32),\n                'iscrowd': torch.zeros((0,), dtype=torch.int64)\n            }\n        \n        return image, target\n    \n    def mask_to_boxes(self, mask):\n        \"\"\"Convert segmentation mask to bounding boxes for Mask R-CNN\"\"\"\n        if isinstance(mask, torch.Tensor):\n            mask_np = mask.numpy()\n        else:\n            mask_np = mask\n        \n        # Find contours in the mask\n        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        boxes = []\n        masks = []\n        \n        for contour in contours:\n            if len(contour) > 0:\n                x, y, w, h = cv2.boundingRect(contour)\n                # Filter out very small regions\n                if w > 5 and h > 5:\n                    boxes.append([x, y, x + w, y + h])\n                    # Create binary mask for this contour\n                    contour_mask = np.zeros_like(mask_np)\n                    cv2.fillPoly(contour_mask, [contour], 1)\n                    masks.append(contour_mask)\n        \n        if boxes:\n            boxes = torch.tensor(boxes, dtype=torch.float32)\n            labels = torch.ones((len(boxes),), dtype=torch.int64)\n            masks = torch.tensor(np.array(masks), dtype=torch.uint8)\n        else:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros(0, dtype=torch.int64)\n            masks = torch.zeros((0, mask_np.shape[0], mask_np.shape[1]), dtype=torch.uint8)\n        \n        return boxes, labels, masks\n\nclass MaskRCNNModel:\n    def __init__(self, num_classes=2, device=device):\n        self.device = device\n        self.model = None\n        self.num_classes = num_classes\n        \n    def create_light_mask_rcnn(self):\n        \"\"\"Create lightweight Mask R-CNN with MobileNetV3 backbone\"\"\"\n        backbone = torchvision.models.mobilenet_v3_small(pretrained=True).features\n        backbone.out_channels = 576\n        \n        # Enhanced backbone\n        backbone = nn.Sequential(\n            backbone,\n            nn.Conv2d(576, 256, kernel_size=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        backbone.out_channels = 256\n        \n        # Anchor generator\n        anchor_generator = AnchorGenerator(\n            sizes=((16, 32, 64, 128),),\n            aspect_ratios=((0.5, 1.0, 2.0),)\n        )\n        \n        # ROI pools\n        roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n            featmap_names=['0'],\n            output_size=7,\n            sampling_ratio=2\n        )\n        \n        mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n            featmap_names=['0'],\n            output_size=14,\n            sampling_ratio=2\n        )\n        \n        model = MaskRCNN(\n            backbone,\n            num_classes=self.num_classes,\n            rpn_anchor_generator=anchor_generator,\n            box_roi_pool=roi_pooler,\n            mask_roi_pool=mask_roi_pooler,\n            min_size=256,\n            max_size=256,\n            rpn_pre_nms_top_n_train=1000,\n            rpn_pre_nms_top_n_test=1000,\n            rpn_post_nms_top_n_train=200,\n            rpn_post_nms_top_n_test=200,\n            box_detections_per_img=100\n        )\n        \n        return model\n    \n    def initialize_model(self):\n        \"\"\"Initialize the model\"\"\"\n        self.model = self.create_light_mask_rcnn()\n        self.model.to(self.device)\n        \n        print(f\"‚úÖ Model initialized with {sum(p.numel() for p in self.model.parameters()):,} parameters\")\n        return self.model\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, device, learning_rate=0.001):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)\n        \n        self.train_losses = []\n        self.val_losses = []\n        \n    def train_epoch(self):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        \n        for batch_idx, (images, targets) in enumerate(tqdm(self.train_loader, desc=\"üöÄ Training\")):\n            images = [img.to(self.device) for img in images]\n            targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n            \n            # Forward pass\n            loss_dict = self.model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            # Backward pass\n            self.optimizer.zero_grad()\n            losses.backward()\n            self.optimizer.step()\n            \n            total_loss += losses.item()\n        \n        avg_loss = total_loss / len(self.train_loader)\n        self.train_losses.append(avg_loss)\n        return avg_loss\n    \n    def validate_epoch(self):\n        \"\"\"Validate for one epoch\"\"\"\n        self.model.train()  # For validation, we use train mode because of Mask R-CNN specifics\n        total_loss = 0\n        \n        with torch.no_grad():\n            for batch_idx, (images, targets) in enumerate(tqdm(self.val_loader, desc=\"üß™ Validating\")):\n                images = [img.to(self.device) for img in images]\n                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n                \n                loss_dict = self.model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                total_loss += losses.item()\n        \n        avg_loss = total_loss / len(self.val_loader)\n        self.val_losses.append(avg_loss)\n        return avg_loss\n    \n    def train(self, epochs=10):\n        \"\"\"Full training loop\"\"\"\n        print(f\"üéØ Starting training for {epochs} epochs...\")\n        \n        for epoch in range(epochs):\n            print(f\"\\nüìç Epoch {epoch+1}/{epochs}\")\n            \n            # Train\n            train_loss = self.train_epoch()\n            \n            # Validate\n            val_loss = self.validate_epoch()\n            \n            # Step scheduler\n            self.scheduler.step()\n            \n            print(f\"üìä Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n            \n        self.plot_training_history()\n    \n    def plot_training_history(self):\n        \"\"\"Plot training and validation loss\"\"\"\n        plt.figure(figsize=(10, 6))\n        plt.plot(self.train_losses, label='Training Loss', marker='o')\n        plt.plot(self.val_losses, label='Validation Loss', marker='s')\n        plt.title('Training History')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\ndef create_transforms():\n    \"\"\"Create data transformations\"\"\"\n    train_transform = A.Compose([\n        A.Resize(256, 256),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.GaussianBlur(blur_limit=3, p=0.3),\n        A.RandomBrightnessContrast(p=0.3),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n\n    val_transform = A.Compose([\n        A.Resize(256, 256),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    return train_transform, val_transform\n\ndef visualize_batch_samples(dataloader, model=None, device=device):\n    \"\"\"Visualize batch samples with predictions if model provided\"\"\"\n    images, targets = next(iter(dataloader))\n    \n    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n    \n    for i in range(min(4, len(images))):\n        # Original image\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n        \n        axes[0, i].imshow(img)\n        axes[0, i].set_title(f'Image {i+1}')\n        axes[0, i].axis('off')\n        \n        # Ground truth mask\n        mask = torch.zeros_like(images[i][0])\n        for target_mask in targets[i]['masks']:\n            mask = torch.max(mask, target_mask.cpu())\n        \n        axes[1, i].imshow(mask, cmap='hot', alpha=0.7)\n        axes[1, i].set_title(f'Ground Truth Mask {i+1}')\n        axes[1, i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main execution\nif __name__ == \"__main__\":\n    # Initialize pipeline\n    base_path = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'\n    pipeline = ScientificForgeryPipeline(base_path)\n    \n    # Analyze data\n    paths = pipeline.analyze_data_structure()\n    pipeline.visualize_size_distribution()\n    pipeline.visualize_samples(num_samples=3)\n    \n    # Create datasets and dataloaders\n    train_transform, val_transform = create_transforms()\n    \n    full_dataset = ForgeryDataset(\n        paths['train_authentic'], \n        paths['train_forged'], \n        paths['train_masks'],\n        transform=train_transform\n    )\n    \n    # Split into train/val\n    train_size = int(0.8 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n    \n    # Apply val transform to validation set\n    val_dataset.dataset.transform = val_transform\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n    \n    print(f\"üì¶ Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n    \n    # Visualize samples\n    visualize_batch_samples(train_loader)\n    \n    # Initialize model\n    model_handler = MaskRCNNModel(device=device)\n    model = model_handler.initialize_model()\n    \n    # Train model\n    trainer = Trainer(model, train_loader, val_loader, device, learning_rate=0.001)\n    trainer.train(epochs=10)\n    \n    print(\"üéâ Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T15:48:37.0985Z","iopub.execute_input":"2025-10-24T15:48:37.098784Z","iopub.status.idle":"2025-10-24T16:40:49.261147Z","shell.execute_reply.started":"2025-10-24T15:48:37.098764Z","shell.execute_reply":"2025-10-24T16:40:49.260427Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß© Conclusion\n\nThis notebook successfully establishes an end-to-end framework for scientific image forgery detection.\nIt integrates data exploration, preprocessing, augmentation, model construction, and training in a cohesive pipeline.\nThe lightweight MobileNetV3 + Mask R-CNN architecture balances accuracy and computational efficiency, making it suitable for large-scale or resource-constrained environments.\n\nFuture improvements could include:\n\nFine-tuning hyperparameters and augmentations.\n\nIncorporating more advanced backbones (e.g., Swin Transformer).\n\nAdding metrics like IoU or F1-score for mask evaluation.\n\nOverall, this project provides a strong foundation for automated forgery localization and authenticity verification in scientific imagery.","metadata":{}}]}