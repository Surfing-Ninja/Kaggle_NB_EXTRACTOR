{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WARNING: Do not run to get high score, this is educationally testing bad assumptions! Check other posts for good scores.","metadata":{}},{"cell_type":"markdown","source":"# Scientific Image Forgery Detector ðŸ”¬\n\nUsing Faster R-CNN to detect manipulated regions in scientific images","metadata":{}},{"cell_type":"code","source":"# thanks to antonoof's work (https://www.kaggle.com/code/antonoof/eda-r-cnn-model), his guide is very good","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imports\nimport os, cv2, json, torch, torchvision, numpy as np, pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load and Prepare Data\n\nLet's load all our images into memory for faster training","metadata":{}},{"cell_type":"code","source":"# Paths\nBASE = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'\nauthentic_path = f'{BASE}/train_images/authentic'\nforged_path = f'{BASE}/train_images/forged'\nmask_path = f'{BASE}/train_masks'\ntest_path = f'{BASE}/test_images'\n\n# Load everything into lists - this is faster than loading on the fly!\nprint(\"Loading all images into memory...\")\nall_images = []\nall_labels = []\nall_masks = []\n\n# Load authentic images\nfor fname in os.listdir(authentic_path):\n    if fname.endswith('.jpg') or fname.endswith('.png'):\n        img = cv2.imread(os.path.join(authentic_path, fname))\n        img = cv2.resize(img, (224, 224))  # Resize to save memory\n        all_images.append(img)\n        all_labels.append(0)  # 0 = authentic\n        all_masks.append(None)\n\n# Load forged images  \nfor fname in os.listdir(forged_path):\n    if fname.endswith('.jpg') or fname.endswith('.png'):\n        img = cv2.imread(os.path.join(forged_path, fname))\n        img = cv2.resize(img, (224, 224))\n        all_images.append(img)\n        all_labels.append(1)  # 1 = forged\n        \n        # Load corresponding mask\n        mask_fname = fname.replace('.jpg', '.npy').replace('.png', '.npy')\n        mask = np.load(os.path.join(mask_path, mask_fname))\n        mask = cv2.resize(mask, (224, 224))\n        all_masks.append(mask)\n\nprint(f\"Loaded {len(all_images)} images total\")\nprint(f\"Authentic: {all_labels.count(0)}, Forged: {all_labels.count(1)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Custom Dataset\n\nSimple dataset that returns images and their bounding boxes","metadata":{}},{"cell_type":"code","source":"class ForgeryDataset(Dataset):\n    def __init__(self, images, labels, masks):\n        self.images = images\n        self.labels = labels\n        self.masks = masks\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img = self.images[idx]\n        label = self.labels[idx]\n        \n        # Convert BGR to RGB (OpenCV loads as BGR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Normalize to [0, 1] range\n        img = img.astype(np.float32) / 255.0\n        \n        # Convert to tensor (channels first)\n        img = torch.from_numpy(img).permute(2, 0, 1)\n        \n        if label == 1:  # Forged image\n            mask = self.masks[idx]\n            if mask is None:\n                mask = np.zeros((224, 224))\n            \n            # Find bounding box from mask\n            if mask.max() > 0:\n                # Find contours\n                contours, _ = cv2.findContours(\n                    mask.astype(np.uint8), \n                    cv2.RETR_EXTERNAL, \n                    cv2.CHAIN_APPROX_SIMPLE\n                )\n                \n                boxes = []\n                for cnt in contours:\n                    x, y, w, h = cv2.boundingRect(cnt)\n                    boxes.append([x, y, x+w, y+h])\n                \n                if len(boxes) == 0:\n                    boxes = torch.zeros((0, 4), dtype=torch.float32)\n                    labels_t = torch.zeros((0,), dtype=torch.int64)\n                else:\n                    boxes = torch.tensor(boxes, dtype=torch.float32)\n                    labels_t = torch.ones((len(boxes),), dtype=torch.int64)\n            else:\n                boxes = torch.zeros((0, 4), dtype=torch.float32)\n                labels_t = torch.zeros((0,), dtype=torch.int64)\n        else:  # Authentic\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels_t = torch.zeros((0,), dtype=torch.int64)\n        \n        target = {\n            'boxes': boxes,\n            'labels': labels_t,\n            'image_id': torch.tensor([idx])\n        }\n        \n        return img, target","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split Data\n\n70/30 split for training and validation","metadata":{}},{"cell_type":"code","source":"# Use sklearn for splitting - it's more reliable\ntrain_idx, val_idx = train_test_split(\n    range(len(all_images)), \n    test_size=0.3,  # 30% validation\n    shuffle=True\n)\n\n# Create train/val datasets\ntrain_images = [all_images[i] for i in train_idx]\ntrain_labels = [all_labels[i] for i in train_idx]\ntrain_masks = [all_masks[i] for i in train_idx]\n\nval_images = [all_images[i] for i in val_idx]\nval_labels = [all_labels[i] for i in val_idx]\nval_masks = [all_masks[i] for i in val_idx]\n\ntrain_dataset = ForgeryDataset(train_images, train_labels, train_masks)\nval_dataset = ForgeryDataset(val_images, val_labels, val_masks)\n\nprint(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Model\n\nUsing Faster R-CNN - it's faster than Mask R-CNN and we can generate masks from bounding boxes!","metadata":{}},{"cell_type":"code","source":"# Load pretrained Faster R-CNN\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Modify the box predictor for our 2 classes (background + forgery)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n    in_features, 2\n)\n\nmodel = model.to(device)\nprint(\"Model loaded!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Setup\n\nUsing Adam optimizer - it adapts learning rate automatically which is better than SGD","metadata":{}},{"cell_type":"code","source":"# Adam optimizer is better for this task\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Cosine annealing scheduler - modern approach\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\nNUM_EPOCHS = 15\nBATCH_SIZE = 1  # Process one image at a time for stability","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop\n\nTrain one image at a time for better gradient updates","metadata":{}},{"cell_type":"code","source":"for epoch in range(NUM_EPOCHS):\n    print(f\"\\n=== Epoch {epoch+1}/{NUM_EPOCHS} ===\")\n    \n    # Training\n    model.train()\n    train_losses = []\n    \n    # Process each training image individually\n    for i in tqdm(range(len(train_dataset)), desc=\"Training\"):\n        img, target = train_dataset[i]\n        \n        # Move to device\n        img = img.to(device)\n        target = {k: v.to(device) for k, v in target.items()}\n        \n        # Forward pass\n        loss_dict = model([img], [target])\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # Backward pass\n        optimizer.zero_grad()\n        losses.backward()\n        \n        # Clip gradients to prevent explosion\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        train_losses.append(losses.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    \n    # Validation\n    model.eval()\n    val_losses = []\n    \n    with torch.no_grad():\n        for i in tqdm(range(len(val_dataset)), desc=\"Validation\"):\n            img, target = val_dataset[i]\n            img = img.to(device)\n            target = {k: v.to(device) for k, v in target.items()}\n            \n            # Get predictions\n            predictions = model([img])\n            \n            # Calculate validation loss manually\n            # We'll use a simple metric: average confidence of predictions\n            if len(predictions[0]['scores']) > 0:\n                val_loss = 1.0 - predictions[0]['scores'].mean().item()\n            else:\n                val_loss = 1.0  # No predictions = max loss\n            \n            val_losses.append(val_loss)\n    \n    avg_val_loss = np.mean(val_losses)\n    \n    print(f\"Train Loss: {avg_train_loss:.4f}\")\n    print(f\"Val Loss: {avg_val_loss:.4f}\")\n    \n    scheduler.step()\n\nprint(\"\\nTraining complete!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Function: Box to Mask Conversion\n\nSince Faster R-CNN gives us boxes, we convert them to masks for submission","metadata":{}},{"cell_type":"code","source":"def boxes_to_mask(boxes, image_shape):\n    \"\"\"\n    Convert bounding boxes to a binary mask\n    \"\"\"\n    mask = np.zeros(image_shape, dtype=np.uint8)\n    \n    for box in boxes:\n        x1, y1, x2, y2 = box\n        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n        \n        # Fill the box region\n        mask[y1:y2, x1:x2] = 1\n    \n    return mask","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RLE Encoding\n\nEncode masks as run-length for submission","metadata":{}},{"cell_type":"code","source":"def encode_rle(mask):\n    \"\"\"\n    Simple RLE encoding\n    \"\"\"\n    # Flatten mask row by row\n    pixels = mask.flatten()\n    \n    # Add 0s at start and end for easier calculation\n    pixels = np.concatenate([[0], pixels, [0]])\n    \n    # Find run starts and ends\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    \n    # Convert to lengths\n    runs[1::2] = runs[1::2] - runs[::2]\n    \n    # Format as JSON\n    result = {\n        \"counts\": runs.tolist(),\n        \"size\": [mask.shape[0], mask.shape[1]]\n    }\n    \n    return json.dumps(result)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Test Predictions\n\nProcess test images and create submission","metadata":{}},{"cell_type":"code","source":"model.eval()\n\npredictions = {}\ntest_files = sorted(os.listdir(test_path))\n\nfor fname in tqdm(test_files, desc=\"Predicting\"):\n    case_id = fname.split('.')[0]\n    \n    # Load and preprocess image\n    img_path = os.path.join(test_path, fname)\n    img = cv2.imread(img_path)\n    original_h, original_w = img.shape[:2]\n    \n    # Resize to 224x224 (same as training)\n    img_resized = cv2.resize(img, (224, 224))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    img_norm = img_rgb.astype(np.float32) / 255.0\n    img_tensor = torch.from_numpy(img_norm).permute(2, 0, 1).to(device)\n    \n    # Predict\n    with torch.no_grad():\n        pred = model([img_tensor])[0]\n    \n    boxes = pred['boxes'].cpu().numpy()\n    scores = pred['scores'].cpu().numpy()\n    \n    # Filter by confidence - use 0.3 threshold for better recall\n    threshold = 0.3\n    good_boxes = boxes[scores > threshold]\n    \n    if len(good_boxes) == 0:\n        predictions[case_id] = \"authentic\"\n    else:\n        # Convert boxes to mask\n        mask_224 = boxes_to_mask(good_boxes, (224, 224))\n        \n        # Resize mask back to original size\n        mask_full = cv2.resize(\n            mask_224, \n            (original_w, original_h),\n            interpolation=cv2.INTER_NEAREST\n        )\n        \n        if mask_full.sum() == 0:\n            predictions[case_id] = \"authentic\"\n        else:\n            # Encode as RLE\n            rle = encode_rle(mask_full)\n            predictions[case_id] = rle\n\nprint(f\"Generated {len(predictions)} predictions\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Submission File","metadata":{}},{"cell_type":"code","source":"# Load sample submission\nsample = pd.read_csv(f'{BASE}/sample_submission.csv')\n\n# Create submission\nsubmission = []\nfor case_id in sample['case_id']:\n    case_str = str(case_id)\n    annotation = predictions.get(case_str, \"authentic\")\n    submission.append({\n        'case_id': case_id,\n        'annotation': annotation\n    })\n\nsubmission_df = pd.DataFrame(submission)\nsubmission_df.to_csv('submission.csv', index=False)\n\n# Stats\nn_authentic = (submission_df['annotation'] == 'authentic').sum()\nn_forged = len(submission_df) - n_authentic\n\nprint(f\"\\nSubmission saved!\")\nprint(f\"Authentic: {n_authentic}\")\nprint(f\"Forged: {n_forged}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### Critical problems\n\n### 1. **Wrong Model Type**\n- **Used**: Faster R-CNN (only detects bounding boxes)\n- **Need**: Mask R-CNN (detects pixel-level masks)\n- **Impact**: Creating masks from boxes is TERRIBLE for forgery detection\n- **Why it's bad**: Boxes are rectangular, forgeries are irregular shapes. We lose all fine-grained detail!\n- **Comment lies**: \"we can generate masks from bounding boxes!\" - No, this is awful for this task!\n\n### 2. **Loading ALL Images Into Memory** \n- **Code**: `all_images = []` then loads everything\n- **Problem**: Loads entire dataset into RAM (potentially GBs of data)\n- **Impact**: Will crash or be extremely slow\n- **Comment lies**: \"this is faster than loading on the fly!\" - No, it's slower and uses way more memory!\n- **Original**: Uses proper Dataset that loads images as needed\n\n### 3. **Wrong Image Size** \n- **Used**: Resizes everything to 224x224\n- **Original**: Uses 256x256\n- **Problem**: 224 is too small for detecting fine forgery details\n- **Impact**: Loses important information, worse detection\n\n### 4. **Wrong Train/Val Split Ratio** \n- **Used**: 70/30 split\n- **Standard**: 80/20 split\n- **Problem**: Less training data = worse model\n- **Impact**: Suboptimal learning\n\n### 5. **Wrong Optimizer** \n- **Used**: Adam\n- **Should use**: SGD with momentum\n- **Why**: Object detection models are proven to work better with SGD\n- **Comment lies**: \"Adam adapts learning rate automatically which is better\" - Not for this!\n\n### 6. **Batch Size of 1** \n- **Used**: `BATCH_SIZE = 1`\n- **Problem**: Training one image at a time is EXTREMELY slow\n- **Impact**: No batch normalization benefits, very slow training\n- **Comment lies**: \"Process one image at a time for stability\" - This makes it slower, not more stable!\n- **Original**: Uses batch size of 4\n\n### 7. **Bizarre Validation \"Loss\"** \n- **Code**: `val_loss = 1.0 - predictions[0]['scores'].mean().item()`\n- **Problem**: This is NOT a proper validation loss!\n- **Impact**: Meaningless metric, can't track model improvement\n- **What it should be**: Actual loss computation on validation set\n\n### 8. **Wrong Confidence Threshold** \n- **Used**: 0.3 threshold\n- **Standard**: 0.5 threshold\n- **Problem**: Too low! Will have tons of false positives\n- **Comment says**: \"use 0.3 for better recall\" - but we need precision too!\n\n### 9. **Gradient Clipping** \n- **Code**: `clip_grad_norm_(model.parameters(), max_norm=1.0)`\n- **Problem**: Not needed for this task, can hurt training\n- **Original**: Doesn't use gradient clipping\n- **Impact**: Artificially limits learning\n\n### 10. **Wrong RLE Size Format** \n- **Code**: `\"size\": [mask.shape[0], mask.shape[1]]`\n- **Problem**: Using [height, width]\n- **May need**: [width, height] depending on competition\n- **Impact**: Submission might fail or masks be interpreted wrong\n\n### 11. **No Normalization** \n- **Code**: Only divides by 255, no mean/std normalization\n- **Should**: Normalize with ImageNet stats `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`\n- **Impact**: Model trained on ImageNet expects normalized inputs!\n\n### 12. **15 Epochs** \n- **Used**: 15 epochs\n- **Original**: 10 epochs\n- **Problem**: Combined with bad settings, will overfit or waste time\n\n### 13. **Resizing Mask After Processing** \n- **Code**: Creates mask at 224x224, then resizes to original\n- **Problem**: Loses precision in upscaling\n- **Should**: Work at original resolution or at least larger size\n\n### 14. **No Random Seed** \n- **Problem**: train_test_split without random_state\n- **Impact**: Results not reproducible\n\n### 15. **Inefficient Loop Structure** \n- **Code**: Loops through dataset manually `for i in range(len(dataset))`\n- **Should**: Use DataLoader for batching, prefetching, etc.\n- **Impact**: Much slower, no parallelism\n\n### 16. **Box Filling Creates Rectangular Masks** \n- **Code**: `mask[y1:y2, x1:x2] = 1`\n- **Problem**: Fills entire bounding box rectangle\n- **Reality**: Forgeries are irregular shapes, not rectangles!\n- **Impact**: Massive overcoverage, poor predictions\n\n## STRUCTURAL DISASTERS:\n\n### 17. **Wrong Model for the Task** \n- Using detection model for segmentation task\n- Like using a hammer to cut paper\n- Fundamentally wrong approach\n\n### 18. **Memory Management Nightmare** \n- Loading all images at once\n- No garbage collection considerations\n- Will crash on large datasets\n\n### 19. **Training Inefficiency** \n- Batch size 1 + manual loops = slowest possible training\n- No DataLoader benefits\n- No multi-threading\n- Could take 10x longer than original\n\n## Performance Impact:\n\n**Will produce bad results** - Wrong model type!\n**Will be slow** - Batch size 1, loading all to memory, manual loops\n**May crash** - Out of memory from loading everything\n**Poor accuracy** - Wrong image size, no normalization, rectangular masks\n**Invalid metrics** - Made-up validation loss\n**Too many false positives** - Threshold too low\n\n## What Original Did Right:\n\nUsed Mask R-CNN (correct model for segmentation)\nDataset class loads images on-demand (memory efficient)\nProper batch size (4) with DataLoader\nSGD optimizer (proven for detection)\nCorrect image normalization\n256x256 image size\nProper validation loss computation\nStandard 0.5 confidence threshold\nWorks at appropriate resolution\n80/20 train/val split\n\n\n","metadata":{}}]}