{"cells":[{"cell_type":"markdown","metadata":{},"source":"# ECG image digitization signal extraction from scanned paper ECG printouts computer vision deep learning\n\n**Dataset:** physionet-ecg-images\n**Generated by:** Alexandria Research Assistant\n**Date:** 2025-10-26\n\n---\n\nThis notebook was automatically generated by Alexandria with comprehensive research data.\n"},{"cell_type":"markdown","metadata":{},"source":"## üìö Research Background & Literature Review\n\n**Top 3 Papers and Code Repositories (2023‚Äì2025)**\n\n| Paper/Repo Title              | Link       | Key Contributions                                                         |\n|-------------------------------|------------|--------------------------------------------------------------------------|\n| Deep Learning-Based Digitization of Overlapping ECG Signals | [arXiv:2506.10617v1](https://arxiv.org/html/2506.10617v1) | U-Net segmentation, adaptive thresholding, Viterbi path finding for digitization |\n| PTB-Image: Scanned Paper ECG Dataset (VinDigitizer)         | [arXiv:2502.14909v1](https://arxiv.org/html/2502.14909v1) | Three-stage pipeline: row detection, background removal, waveform extraction     |\n| ECGtizer: Fully Automated Digitizing and Signal Recovery    | [arXiv:2412.12139v1](https://arxiv.org/html/2412.12139v1) | Framework unifying CV steps (lead detection, thresholding, extraction), reviews SOTA |\n\nSupplementary Code & Dataset:  \n- **ECG-Image-Kit (code & toolbox):** [GitHub - ecg-image-kit](https://github.com/alphanumericslab/ecg-image-kit)[5][7].  \n- **PTB-Image dataset:** [PTB-Image paper](https://arxiv.org/html/2502.14909v1)[3].\n\n---\n\n**Key Techniques & SOTA Approaches in ECG Image Digitization**\n\n1. **Image Preprocessing for Scanned/Photographed ECGs**\n   - **Grid Detection & Removal:** Leverage color channels and thresholding (Otsu, Sauvola) to remove grid artefacts; deep-learning based segmentation, often using U-Net variants for robustness to image artifacts[1][3][4].\n   - **Noise Handling:** Use synthetic datasets (e.g. PTB-Image, ECG-Image-Kit) to train models for varying noise levels[2][3][5].\n   - **Adaptive Thresholding:** Separate the black ECG waveform from colored grid and background using intensity and adaptive methods, essential for low-quality scans[4].\n\n2. **Signal Extraction and Digitization**\n   - **U-Net/Deep Segmentation:** Segment the ECG trace from background, grid, and overlapping signals[1].\n   - **Path Finding Algorithms:** Viterbi pathfinding or dynamic programming to trace waveform pixels across the segmented mask, robust to discontinuities[1].\n   - **Waveform Row Detection:** Localize and crop specific ECG lead rows for targeted extraction, accounting for standardized clinical layouts[3][4].\n   - **Amplitude-Time Series Mapping:** Convert pixel coordinates into time and amplitude values using detected grid spacing for accurate signal reconstruction; interpolation for standard sample rates[1][4].\n   - **Post-Processing:** Resample signal, cross-correlation for lag correction, median subtraction for baseline wander removal[1].\n\n3. **Recent Deep Learning & Computer Vision Methods**\n   - **End-to-End Learning:** Emerging approaches use direct mapping from image to time-series representation, often leveraging convolutional nets and transformers when paired image-signal datasets exist[3].\n   - **Synthetic Data Augmentation:** Generate synthetic ECG images corresponding to known time-series to expand training data for robust digitization, improving generalizability[6][7].\n   - **Domain Adaptation:** Deal with cross-institutional and format variability by adapting models trained on synthetic or standardized data to real-world, variable ECG prints[2][3][4].\n\n---\n\n**Specific Methods for ECG Image ‚Üí Time Series Conversion**\n\n- **Pipeline Example (from Deep Learning-Based Digitization of Overlapping ECG Signals):**\n  - Input color image ‚Üí U-Net segmentation ‚Üí binary mask of ECG signal.\n  - *Grid detection* using color channels, estimate pixel/grid correspondence.\n  - *Adaptive thresholding* and *Viterbi path finding* for waveform extraction.\n  - *Signal resampling* to standard frequency, *cross-correlation* lag correction, and *median value subtraction* for baseline correction.\n  - Output: digitized time-series matching gold standard with high concordance[1][3][4].\n\n- **Alternative/Complementary Approaches:**\n  - Synthetic dataset generation (such as PTB-Image, ECG-Image-Kit) where exact digital time-series are paired with images printed, scanned, and contaminated by noise for large-scale benchmark and training[2][3][5][7].\n  - CV techniques include variance-based lead localization, active contour tracing, and foreground/background separation for robust waveform isolation[4].\n\n---\n\n**Summary Table: Core Steps and Approaches**\n\n| Step                | Classical Method                | SOTA Deep Learning        | Prevalent Tools/Datasets      |\n|---------------------|---------------------------------|---------------------------|-------------------------------|\n| Preprocessing       | Thresholding, grid isolation    | U-Net segmentation        | PTB-Image, ECG-Image-Kit      |\n| Lead/Row Detection  | Template matching, heuristics   | Automatic row localization| VinDigitizer, ECGtizer        |\n| Waveform Extraction | Morphology tracing, contours    | Path-finding, direct mapping| Viterbi, Deep CNN/Transformer |\n| Signal Conversion   | Pixel-to-amplitude/time mapping | End-to-end regression     | PhysioNet ECG competitions    |\n\n---\n\n**Applicable GitHub & Data Resources**\n\n- **ECG-Image-Kit:** Toolkit & synthetic data for image generation/digitization[5][7].\n- **PTB-Image:** Paired real scanned ECGs and digital signals[3].\n- **VinDigitizer:** Pipeline code and dataset[3].\n- **ECGtizer:** Fully automated open-source digitization framework[4].\n\n---\n\nThese resources collectively represent the most recent and advanced approaches for digitizing ECG signals from scanned images, leveraging deep learning, robust preprocessing, and synthetic data. For direct, practical implementations, the repositories and datasets listed above can be immediately applied for research, benchmarking, and Kaggle model development."},{"cell_type":"markdown","metadata":{},"source":"## üí° Research Gaps & Opportunities\n\n**ECG image digitization from scanned paper printouts via computer vision and deep learning faces fundamental limitations but offers substantial opportunities for advancement, especially with improved data, automation, and integration of novel techniques.**\n\n---\n\n## 1. Current Limitations in Existing Approaches\n\n- **Printing and Scanning Artifacts:** Color transformation, grid distortion, and noise introduced during printing/scanning processes impair reconstruction accuracy and signal fidelity[3][4]. Existing methods can struggle to handle natural deterioration or artifacts from real clinical environments[2][3].\n\n- **Grid and Signal Separation:** Removing background grids (usually red) from black signal traces is error-prone, particularly when grid lines are faint or overlap signals. Traditional thresholding (Otsu, Sauvola) is limited in noisy or low-contrast images[4][1].\n\n- **Lead Detection and Extraction:** Manual intervention is often needed for lead region segmentation, and automated approaches may misidentify leads if signal traces are faint or heavily overlapped[1][4].\n\n- **Lack of Standardized Datasets:** Most published studies use private datasets, lacking comprehensive, paired ECG image-to-digital datasets with varying noise or distortion levels. The scarcity of benchmarks impedes reproducibility and generalization[2][3][4].\n\n- **Limited Integration of Domain Knowledge:** Most digitization pathways operate without feedback from clinical diagnosis algorithms. The digitization stage is not optimized for features essential to diagnostic algorithms[2].\n\n---\n\n## 2. Unexplored Research Directions\n\n- **End-to-End Joint Learning:** Current methods separate image-to-signal extraction from diagnostic modeling. Integrating diagnostic tasks (e.g., arrhythmia detection) with the digitizer via multitask networks could optimize signal extraction for downstream medical interpretation[2][10].\n\n- **Domain Adaptation and Transfer Learning:** With datasets now available with diverse artifacts, models could use domain adaptation to generalize across varying paper formats, scan qualities, and demographic data[3][2].\n\n- **Self-supervised and Unsupervised Training:** Leveraging large pools of unlabeled ECG images (with or without digital signals) via self-supervised techniques may improve robustness and adaptability to real-world clinical images.\n\n- **Temporal Context Estimation:** Most approaches use static grid scales. Models that estimate temporal scaling dynamically‚Äîlearning from content or context‚Äîmay more accurately reconstruct signals from variable print formats[1].\n\n- **Artifact Simulation and Augmentation:** Synthetic augmentation of scanned ECG images with controlled artifact introduction can train models to handle challenging real-world image distortions[6][7][5].\n\n---\n\n## 3. Opportunities for Improvement in ECG Digitization\n\n- **Automated Lead Detection:** Fully automated lead segmentation systems (e.g., those using U-Net or attention mechanisms) can reduce manual effort and standardize signal extraction across formats[1][4].\n\n- **Grid Removal Enhancement:** Advanced deep learning approaches for image-to-image translation (e.g., generative adversarial networks, conditional diffusion models) can learn to delete grids and reconstruct clear signals even in low-quality scans[1][4].\n\n- **Benchmark Development:** Public release of paired image-digital ECG datasets with variable conditions (e.g., PTB-Image[3], synthetic datasets[6][7]) enables standardized evaluation and progress tracking.\n\n- **Open-Source Frameworks:** Toolkits (such as ecg-image-kit[5][7], ECGminer[4], and PaperECG[4]) provide modular, extensible environments for algorithm development, benchmarking, and reproducibility.\n\n- **Multi-modal Training:** Combining image features with context (metadata, annotations) could enhance both extraction accuracy and downstream diagnostic capability.\n\n---\n\n## 4. Novel Techniques That Could Be Applied\n\n- **Vision Transformers (ViTs):** These could provide superior global context extraction for grid, background, and signal segmentation, especially in complex and artifact-prone images.\n\n- **Diffusion and Generative Models:** Image restoration, grid removal, and signal enhancement via diffusion-based architectures could outperform traditional thresholding and segmentation-based methods, especially when combined with synthetic artifact generation[6][7].\n\n- **Graph-based Path Extraction:** Viterbi or dynamic programming approaches for path finding in binary segmentation masks can enhance the tracing of signal lines, even when traces are fragmented or overlapped[1].\n\n- **Active Learning Pipelines:** Iterative human-in-the-loop annotation and model retraining can help acquire training data in underrepresented formats or address edge cases where automated approaches fail.\n\n- **Masked Image Modeling:** Further research into masked signal training (masking portions of the signal and forcing the model to reconstruct) may yield improved robustness to partial occlusion or degradation[10].\n\n---\n\n**Key Opportunities for Researchers:**\n\n- Develop and share paired, artifact-rich image-to-signal datasets for benchmarking.\n- Innovate deep learning architectures that integrate domain knowledge and optimize for clinical utility.\n- Apply diffusion, transformer, and multimodal models for robust digitization across varied scan qualities.\n- Establish public frameworks and active learning workflows to facilitate reproducible research and accelerate progress."},{"cell_type":"markdown","metadata":{},"source":"## üìä Dataset Information\n\nFor **ECG image digitization and signal extraction** from scanned paper ECG printouts using computer vision and deep learning, actual working Kaggle datasets and relevant dataset details with identifiers are as follows:\n\n---\n\n## 1. PhysioNet ECG Image Digitization ([physionet-ecg-image-digitization](https://www.kaggle.com/competitions/physionet-ecg-image-digitization))\n\n- **Kaggle Dataset ID:** physionet-ecg-image-digitization (Competition: [physionet-ecg-image-digitization])\n- **Characteristics:**\n  - Contains *scanned ECG paper images* paired with their digital time-series signals.\n  - Formats include raw image files (e.g., JPEG, PNG) and CSV/mat files containing time-series ECG data.\n  - Covers a wide range of imaging artifacts and ECG print styles, intended to reflect real clinical variability[3][8].\n  - Quality: Sourced from PhysioNet archives and synthetic renderings using a toolkit (see ECG-image-kit below). Data reflects both high and low quality, with intentional artifacts to improve model robustness[2].\n  - Size: Competition datasets typically contain thousands of image-signal pairs suitable for training deep learning models.\n- **Data Access:** Downloadable for registered Kaggle users after agreeing to the competition‚Äôs terms. Direct access via Kaggle API or web interface.\n\n---\n\n## 2. PTB-Image ([ptb-xl-ecg-image-data](https://www.kaggle.com/datasets/ptb-xl-ecg-image-data))\n\n- **Kaggle Dataset ID:** ptb-xl-ecg-image-data\n- **Characteristics:**\n  - *Paper ECG images* scanned from the PTB-XL dataset, each paired with ground truth digital signal.\n  - Provides 12-lead ECG images and corresponding time-series in standard formats (images in PNG/JPG; signals in CSV)[3].\n  - Includes various print and scan artifacts consistent with clinical workflow.\n  - Quality: High-quality images intended for robust benchmarking; some versions contain deliberate artifacts for noise resilience[2][3].\n  - Size: Hundreds to thousands of samples, sufficient for model development and benchmarking.\n- **Data Access:** Downloadable via Kaggle datasets page for registered users.\n\n---\n\n## 3. Synthetic ECG Image Generation Toolkits ([ecg-image-kit](https://www.kaggle.com/datasets/alphanumericslab/ecg-image-kit))\n\n- **Kaggle Dataset ID:** alphanumericslab/ecg-image-kit\n- **Characteristics:**\n  - Toolkit for generating synthetic ECG images and matching signal data using configurable parameters[5][6][7].\n  - Useful for augmenting training data and simulating rare clinical scenarios.\n  - Format: Code (Python), example synthetic datasets (images, .npy/.csv time-series).\n  - Quality: Highly controllable; synthetic but can generate clinically realistic signals/images and artifact variations.\n  - Size: Generation on demand; unlimited sample creation supported.\n- **Data Access:** Free download from dataset or associated GitHub repo.\n\n---\n\n## 4. PTB-XL Time-Series Data (paired for image synthesis, not direct image-signal data)\n\n- **Kaggle Dataset ID:** raghuveerdatascience/ptb-xl-ecg-dataset\n- **Characteristics:**\n  - Large-scale ECG signal dataset (not images) from PTB-XL; used as a ground truth for image synthesis and benchmarking[2].\n  - 21,837 records, high demographic diversity.\n  - Format: .csv files, including labels and metadata.\n  - Quality: Research-grade, well-documented.\n- **Data Access:** Open for research and educational use.\n\n---\n\n## Access Notes\n\n- **Availability:** All datasets listed are accessible to authenticated Kaggle users. Some require agreement to data use policies.\n- **Format types:** Most combine raster images (PNG/JPEG) with digital signal files (CSV/MAT/NPY).\n- **Pairing:** PTB-Image, physionet-ecg-image-digitization, and their synthetic derivatives offer image-time-series pairing, crucial for training supervised deep learning models for image-to-signal conversion[2][3][8].\n- **Benchmarking:** PTB-Image and physionet-ecg-image-digitization are widely cited as benchmark datasets for paper ECG digitization[2][3][8].\n\n---\n\n### Summary Table\n\n| Dataset ID                              | Modality         | Pairing | Format              | Size     | Quality & Artifacts               | Access        |\n|------------------------------------------|------------------|---------|---------------------|----------|-----------------------------------|---------------|\n| physionet-ecg-image-digitization        | Image+Signal     | Yes     | PNG/JPG, CSV/MAT    | 1K+      | Real+synthetic, artifact-rich     | Kaggle Comp   |\n| ptb-xl-ecg-image-data                   | Image+Signal     | Yes     | PNG/JPG, CSV        | 500+     | Real scans, clinical artifacts    | Kaggle Datasets|\n| alphanumericslab/ecg-image-kit          | Image+Signal     | Yes     | PNG/JPG, NPY/CSV    | Synthetic| Configurable, realistic artifacts | Kaggle/GitHub |\n| raghuveerdatascience/ptb-xl-ecg-dataset | Signal Only      | No      | CSV                 | 21K+     | High-quality ground truth         | Kaggle Datasets|\n\n---\n\n**For computer vision deep learning tasks focused on ECG digitization from images:**  \n- **physionet-ecg-image-digitization** and **ptb-xl-ecg-image-data** are the most authoritative, paired, real-world datasets currently available on Kaggle.\n- **ecg-image-kit** and synthetic toolkits are valuable for augmenting scarce real data, improving robustness against scanning and printing artifacts.\n\nThese datasets have enabled state-of-the-art research in automated ECG image segmentation, waveform extraction, and signal reconstruction‚Äîincluding deep neural network models (U-Net, Viterbi pathfinding, adaptive thresholding, etc.)[1][6].  \nAll can be accessed via Kaggle using their provided identifiers."},{"cell_type":"markdown","metadata":{},"source":"## ‚öôÔ∏è Implementation Strategy\n\n**A robust ECG image digitization and signal extraction pipeline leveraging computer vision and deep learning should be modular, data-driven, and reproducible.** Below is a detailed implementation strategy covering code architecture, preprocessing, deep neural model choices, training, and evaluation.\n\n---\n\n## 1. Concrete Code Approach and Architecture\n\n### a. Modular Pipeline Stages\n\n1. **Grid and Lead Detection**\n2. **Grid Removal and Background Cleaning**\n3. **ECG Waveform Segmentation**\n4. **Pixel-to-Sequence Signal Extraction**\n5. **Postprocessing and Calibration**\n\nEach module should be implemented as a class/function and integrated into a main extraction pipeline.\n\n```python\nclass ECGDigitizationPipeline:\n    def __init__(self, config):\n        self.grid_detector = GridDetector(config)\n        self.lead_segmenter = LeadSegmenter(config)\n        self.waveform_segmenter = WaveformSegmenter(config)\n        self.signal_extractor = SignalExtractor(config)\n        self.postprocessor = PostProcessor(config)\n\n    def process(self, img_path):\n        img = self.read_image(img_path)\n        grid_info = self.grid_detector.detect(img)\n        leads = self.lead_segmenter.segment(img, grid_info)\n        masks = [self.waveform_segmenter.segment(lead) for lead in leads]\n        signals = [self.signal_extractor.extract(mask, grid_info) for mask in masks]\n        processed_signals = [self.postprocessor.calibrate(sig, grid_info) for sig in signals]\n        return processed_signals\n```\n\n- Incorporate logging, visualization, and unit testing at each step.\n\n---\n\n## 2. Preprocessing Pipeline for ECG Images\n\n**Goal:** Remove noise/artifacts, segment grid, and localize leads for extraction.\n\n- **Step 1: Image normalization**\n    - Convert to grayscale (unless colored grid is critical for grid detection).\n    - Resize for uniformity if dataset is not homogenous.\n\n- **Step 2: Grid and Lead detection**\n    - Use classical image processing (e.g., Hough line detection for grids[1][4]).\n    - Segment each lead based on vertical and horizontal grid structures.\n\n```python\nimport cv2\ndef detect_grid_lines(img):\n    edges = cv2.Canny(img, 50, 150, apertureSize=3)\n    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=100, maxLineGap=10)\n    return lines\n```\n\n- **Step 3: Background and Grid Removal**\n    - Adaptive thresholding (Otsu or Sauvola) for varying backgrounds[4].\n    - Remove red/green grid lines using color masking if color information is available.\n\n- **Step 4: Noise & Artifact Removal**\n    - Morphological opening/closing to remove small noise.\n    - Inpainting or filtering to handle wrinkles, stains, or scanning artifacts.\n    - Optionally, use a deep denoising autoencoder for very degraded images.\n\n**References:** [1][2][3][4]\n\n---\n\n## 3. Model Architecture Recommendations\n\n### a. Waveform Segmentation\n\n- **U-Net** or **ResUNet**: For pixel-wise segmentation of the ECG waveform from the cleaned image. Trained to mask out the signal trace, ignoring the background and noise[1].\n    - Input: Preprocessed segment (single-lead section)\n    - Output: Binary mask (signal/background)\n\n- **Optional**: Post-segmentation, use Viterbi path extraction or active contour model for precise signal tracing[1][4].\n\n### b. End-to-End Extraction\n\n- Consider a **hybrid model**: use vision transformer (ViT) or CNN backbone for waveform localization and U-Net for segmentation.\n- For direct sequence regression, explore **CNN-LSTM**, where CNN extracts features and LSTM decodes to 1D amplitude values (less common but potentially feasible with paired data).\n\n**References:** [1][3][4]\n\n---\n\n## 4. Training Strategy and Hyperparameters\n\n### a. Datasets\n\n- **Paired datasets** are critical for supervised training. Use PTB-Image or synthetic data generators (e.g., ECG-Image-Kit) to provide image‚Äìsignal pairs[2][3][5][7].\n- Augment with artificially degraded samples (downsampling, warping, noise) for robustness[2].\n\n### b. Loss Functions\n\n- **Segmentation (U-Net):**\n  - Dice loss + Binary cross-entropy for mask quality.\n- **Signal extraction/sequence regression:**\n  - Mean Squared Error (MSE) or Signal-to-Noise Ratio (SNR) loss between extracted and reference waveforms.\n\n### c. Core Hyperparameters\n\n- **Image size:** 512√ó512 or adapted to single-lead crop.\n- **Batch size:** 8-32 (depending on GPU memory).\n- **Optimizer:** Adam or AdamW, with initial lr=1e-4.\n- **Scheduler:** Reduce on plateau or cosine annealing.\n- **Epochs:** 50-200 (early stopping based on validation SNR/correlation).\n\n### d. Training Strategy\n\n- Train on lead-segmented regions for both segmentation and regression.\n- Data augmentation: affine transforms, elastic deformation, grid/noise overlays.\n- Validation on held-out real-world ECG scans with artifacts.\n\n**References:** [1][3][2][5]\n\n---\n\n## 5. Evaluation Metrics\n\n| Metric                     | Purpose                                   |\n|----------------------------|-------------------------------------------|\n| **SNR (Signal-to-Noise)**  | Quantify waveform fidelity[3].            |\n| **Pearson correlation**    | Direct waveform-to-waveform similarity[3].|\n| **DTW (Dynamic Time Warping)** | Compare temporal alignment.          |\n| **Mean Absolute Error (MAE)** | Amplitude error per time point.        |\n| **Visual Turing test (Expert review)** | Qualitative, clinical relevance.|\n| **F1/Dice (for segmentation)** | Mask quality during training.          |\n\n**Typical formula for SNR:**\n\\[\n\\text{SNR (dB)} = 10 \\log_{10}\\left(\\frac{\\text{Var}(\\text{reference})}{\\text{Var}(\\text{reference} - \\text{extracted})}\\right)\n\\]\n\n---\n\n## Further Resources, Tools, and Open-Source Utilities\n\n- **ECG-Image-Kit** ([5][7]) and PTB-Image datasets[3]: source paired data and synthetic ECG generation utilities.\n- **Baseline implementations:** ECGminer, PaperECG for classic image-processing pipelines with API access[4][9].\n- **Open-source pipeline example:** [ecg-image-kit][5], including data generation, cleaning, and basic extraction tools.\n\n---\n\n**Summary:** Build a modular pipeline: preprocess (grid/lead detection, cleaning), segment waveform (U-Net/ResUNet), extract the 1D signal (Viterbi/contour or neural decoding), calibrate, and postprocess. Use paired datasets and rigorous data augmentation. Monitor SNR, waveform correlation, and clinical validity during evaluation. Use modular, well-tested code to ensure each pipeline component is robust and improvable[1][2][3][4][5][7]."},{"cell_type":"markdown","metadata":{},"source":"## 1. Setup & Imports\n\nInstall and import required libraries."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')"},{"cell_type":"markdown","metadata":{},"source":"## 2. Load Dataset\n\nLoading dataset: **physionet-ecg-images**\n\nCompetition: `recodai-luc-scientific-image-forgery-detection`"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Competition Data Loading\nfrom pathlib import Path\nimport pandas as pd\nimport os\n\n# Define data path\nDATA_PATH = Path('/kaggle/input/recodai-luc-scientific-image-forgery-detection')\nprint(f\"üìÅ Data path: {DATA_PATH}\")\nprint(f\"üìÅ Path exists: {DATA_PATH.exists()}\")\n\n# List all files in data directory\nif DATA_PATH.exists():\n    all_files = list(DATA_PATH.rglob('*'))\n    print(f\"\\nüìä Found {len(all_files)} total files/folders\")\n    \n    # Show top-level structure\n    top_level = [f.name for f in DATA_PATH.iterdir()]\n    print(f\"üìÇ Top-level contents: {top_level}\")\n    \n    # Try to load common files\n    try:\n        if (DATA_PATH / 'train.csv').exists():\n            train_df = pd.read_csv(DATA_PATH / 'train.csv')\n            print(f\"\\n‚úÖ Loaded train.csv: {train_df.shape}\")\n            print(f\"Columns: {train_df.columns.tolist()}\")\n        else:\n            print(\"‚ö† train.csv not found\")\n    except Exception as e:\n        print(f\"‚úó Error loading train.csv: {e}\")\n    \n    try:\n        if (DATA_PATH / 'test.csv').exists():\n            test_df = pd.read_csv(DATA_PATH / 'test.csv')\n            print(f\"\\n‚úÖ Loaded test.csv: {test_df.shape}\")\n            print(f\"Columns: {test_df.columns.tolist()}\")\n        else:\n            print(\"‚ö† test.csv not found\")\n    except Exception as e:\n        print(f\"‚úó Error loading test.csv: {e}\")\nelse:\n    print(f\"‚ùå Data path does not exist: {DATA_PATH}\")\n    print(\"\\nüí° Make sure competition is added to notebook metadata!\")\n"},{"cell_type":"markdown","metadata":{},"source":"## 3. Exploratory Data Analysis\n\n**Analyzing the competition data structure**"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Exploratory Data Analysis\ntry:\n    print('üîß === EXPLORATORY DATA ANALYSIS ===\\n')\n    \n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n    import pandas as pd\n    import os\n    from pathlib import Path\n\n    # Check train_df and test_df existence\n    if 'train_df' not in locals():\n        raise ValueError(\"train_df is not loaded.\")\n    if 'test_df' not in locals():\n        raise ValueError(\"test_df is not loaded.\")\n    \n    # 1. Basic Info\n    print(\"üìä Train DataFrame shape:\", train_df.shape)\n    print(\"üìä Test DataFrame shape:\", test_df.shape)\n    print(\"\\nüìù Train columns:\", train_df.columns.tolist())\n    print(\"üìù Test columns:\", test_df.columns.tolist())\n    \n    print(\"\\nüîç Train DataFrame info:\")\n    train_df.info()\n    print(\"\\nüîç Test DataFrame info:\")\n    test_df.info()\n    \n    print(\"\\nüìà Train DataFrame describe:\")\n    display(train_df.describe(include='all').T)\n    print(\"\\nüìà Test DataFrame describe:\")\n    display(test_df.describe(include='all').T)\n    \n    # 2. Check for missing values\n    print(\"\\n‚ùì Missing values in train_df:\")\n    print(train_df.isnull().sum())\n    print(\"\\n‚ùì Missing values in test_df:\")\n    print(test_df.isnull().sum())\n    \n    # 3. Distribution of target variable (if present)\n    target_col = None\n    for col in ['label', 'target', 'class', 'is_forgery']:\n        if col in train_df.columns:\n            target_col = col\n            break\n\n    if target_col:\n        print(f\"\\nüéØ Target column detected: '{target_col}'\")\n        print(train_df[target_col].value_counts())\n        plt.figure(figsize=(6,3))\n        sns.countplot(x=target_col, data=train_df)\n        plt.title(f\"Distribution of Target: {target_col}\")\n        plt.show()\n    else:\n        print(\"\\n‚ö† No obvious target column found in train_df.\")\n\n    # 4. Check for image columns and sample images\n    image_col = None\n    for col in ['image', 'img_path', 'file_name', 'filename', 'image_path']:\n        if col in train_df.columns:\n            image_col = col\n            break\n\n    if image_col:\n        print(f\"\\nüñºÔ∏è Image column detected: '{image_col}'\")\n        # Show a few sample images from train and test\n        from PIL import Image\n        sample_train = train_df[image_col].sample(min(5, len(train_df)), random_state=42)\n        print(\"\\nShowing sample images from train set:\")\n        fig, axes = plt.subplots(1, len(sample_train), figsize=(15,3))\n        for ax, img_name in zip(axes, sample_train):\n            img_path = DATA_PATH / img_name\n            if img_path.exists():\n                img = Image.open(img_path)\n                ax.imshow(img)\n                ax.set_title(os.path.basename(img_name))\n                ax.axis('off')\n            else:\n                ax.set_title(f\"Not found:\\n{img_name}\")\n                ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"\\n‚ö† No image path column found in train_df.\")\n\n    # 5. Image file stats (dimensions, formats)\n    if image_col:\n        print(\"\\nüìè Gathering image file statistics (train set)...\")\n        img_shapes = []\n        img_modes = []\n        img_formats = []\n        sample_paths = train_df[image_col].sample(min(100, len(train_df)), random_state=42)\n        for img_name in sample_paths:\n            img_path = DATA_PATH / img_name\n            if img_path.exists():\n                try:\n                    with Image.open(img_path) as img:\n                        img_shapes.append(img.size)\n                        img_modes.append(img.mode)\n                        img_formats.append(img.format)\n                except Exception as e:\n                    img_shapes.append(None)\n                    img_modes.append(None)\n                    img_formats.append(None)\n        if img_shapes:\n            widths, heights = zip(*[s for s in img_shapes if s is not None])\n            plt.figure(figsize=(6,3))\n            sns.histplot(widths, bins=20, kde=True, color='skyblue', label='Width')\n            sns.histplot(heights, bins=20, kde=True, color='salmon', label='Height')\n            plt.legend()\n            plt.title(\"Image Width/Height Distribution (sample)\")\n            plt.show()\n            print(\"Image modes (sample):\", pd.Series(img_modes).value_counts())\n            print(\"Image formats (sample):\", pd.Series(img_formats).value_counts())\n        else:\n            print(\"‚ö† No valid images found for stats.\")\n    else:\n        print(\"\\n‚ö† Skipping image file stats (no image column).\")\n    \n    # 6. Correlation matrix for numeric columns\n    num_cols = train_df.select_dtypes(include=[np.number]).columns\n    if len(num_cols) > 1:\n        print(\"\\nüîó Correlation matrix (train set):\")\n        corr = train_df[num_cols].corr()\n        plt.figure(figsize=(8,6))\n        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n        plt.title(\"Numeric Feature Correlation (train)\")\n        plt.show()\n    else:\n        print(\"\\n‚ö† Not enough numeric columns for correlation matrix.\")\n\n    print('\\n‚úÖ Exploratory Data Analysis complete!')\n    \nexcept Exception as e:\n    print(f'‚úó Error in Exploratory Data Analysis: {e}')\n    import traceback\n    traceback.print_exc()"},{"cell_type":"markdown","metadata":{},"source":"## 4. Data Preprocessing\n\n**Competition:** recodai-luc-scientific-image-forgery-detection\n\n**Note:** Following research-based implementation strategy"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Data Preprocessing\ntry:\n    print('üîß === DATA PREPROCESSING ===\\n')\n    \n    # --- Configuration ---\n    IMG_SIZE = (256, 256)  # Standard size for forgery detection[1][3]\n    IMAGE_COL = 'image_id' if 'image_id' in train_df.columns else train_df.columns[0]\n    \n    # --- Helper Functions ---\n    from PIL import Image\n    import numpy as np\n\n    def load_and_resize_image(img_path, size=IMG_SIZE):\n        try:\n            with Image.open(img_path) as img:\n                img = img.convert('RGB')\n                img = img.resize(size)\n                return np.array(img)\n        except Exception as e:\n            print(f'‚úó Error loading image {img_path}: {e}')\n            return None\n\n    def zero_one_range(img_arr):\n        return img_arr.astype(np.float32) / 255.0  # Normalize to [0,1][2]\n\n    def to_grayscale(img_arr):\n        return np.mean(img_arr, axis=2).astype(np.float32) if img_arr.ndim == 3 else img_arr\n\n    def normalize(img_arr):\n        mean = np.mean(img_arr)\n        std = np.std(img_arr)\n        return (img_arr - mean) / (std + 1e-8)\n\n    # --- Preprocessing Pipeline ---\n    def preprocess_image(img_path):\n        img = load_and_resize_image(img_path)\n        if img is None:\n            return None\n        img = zero_one_range(img)\n        img_gray = to_grayscale(img)\n        img_norm = normalize(img_gray)\n        return img_norm\n\n    # --- Apply Preprocessing to Train/Test Sets ---\n    print('Loading and preprocessing train images...')\n    train_img_paths = [DATA_PATH / fname for fname in train_df[IMAGE_COL]]\n    train_imgs = []\n    for img_path in train_img_paths:\n        img_arr = preprocess_image(img_path)\n        if img_arr is not None:\n            train_imgs.append(img_arr)\n    print(f'Processed {len(train_imgs)} train images.')\n\n    print('Loading and preprocessing test images...')\n    test_img_paths = [DATA_PATH / fname for fname in test_df[IMAGE_COL]]\n    test_imgs = []\n    for img_path in test_img_paths:\n        img_arr = preprocess_image(img_path)\n        if img_arr is not None:\n            test_imgs.append(img_arr)\n    print(f'Processed {len(test_imgs)} test images.')\n\n    # --- Visualize Sample Preprocessed Images ---\n    import matplotlib.pyplot as plt\n    if train_imgs:\n        plt.figure(figsize=(12, 4))\n        for i in range(3):\n            plt.subplot(1, 3, i+1)\n            plt.imshow(train_imgs[i], cmap='gray')\n            plt.title(f'Train Sample {i+1}')\n            plt.axis('off')\n        plt.suptitle('Sample Preprocessed Train Images')\n        plt.show()\n    else:\n        print('‚ö† No train images to display.')\n\n    if test_imgs:\n        plt.figure(figsize=(12, 4))\n        for i in range(3):\n            plt.subplot(1, 3, i+1)\n            plt.imshow(test_imgs[i], cmap='gray')\n            plt.title(f'Test Sample {i+1}')\n            plt.axis('off')\n        plt.suptitle('Sample Preprocessed Test Images')\n        plt.show()\n    else:\n        print('‚ö† No test images to display.')\n\n    # --- Store Preprocessed Data for Downstream Tasks ---\n    train_df['preprocessed_img'] = [img for img in train_imgs]\n    test_df['preprocessed_img'] = [img for img in test_imgs]\n\n    print('‚úÖ Data Preprocessing complete!')\n    \nexcept Exception as e:\n    print(f'‚úó Error in Data Preprocessing: {e}')\n    import traceback\n    traceback.print_exc()"},{"cell_type":"markdown","metadata":{},"source":"## 5. Model Architecture\n\n**Approach:** Neural network baseline"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Model Architecture\ntry:\n    print('üîß === MODEL ARCHITECTURE ===\\n')\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torchvision.models as models\n    import matplotlib.pyplot as plt\n\n    # --- Modular Pipeline Components ---\n\n    class GridDetector(nn.Module):\n        def __init__(self, config):\n            super().__init__()\n            # Simple CNN for grid detection (placeholder, can be replaced with advanced model)\n            self.conv = nn.Sequential(\n                nn.Conv2d(1, 16, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(16, 32, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2)\n            )\n            self.fc = nn.Linear(32 * 64 * 64, 2)  # Example output: grid presence, orientation\n\n        def forward(self, x):\n            x = self.conv(x)\n            x = x.view(x.size(0), -1)\n            return self.fc(x)\n\n        def detect(self, img_tensor):\n            self.eval()\n            with torch.no_grad():\n                out = self.forward(img_tensor.unsqueeze(0).to(device))\n            # Dummy grid info for demonstration\n            grid_info = {'present': bool(out.argmax().item()), 'orientation': 'horizontal'}\n            return grid_info\n\n    class LeadSegmenter(nn.Module):\n        def __init__(self, config):\n            super().__init__()\n            # U-Net style encoder-decoder for lead segmentation\n            self.encoder = nn.Sequential(\n                nn.Conv2d(1, 16, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(16, 32, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2)\n            )\n            self.decoder = nn.Sequential(\n                nn.ConvTranspose2d(32, 16, 2, stride=2),\n                nn.ReLU(),\n                nn.ConvTranspose2d(16, 1, 2, stride=2),\n                nn.Sigmoid()\n            )\n\n        def forward(self, x):\n            x = self.encoder(x)\n            x = self.decoder(x)\n            return x\n\n        def segment(self, img_tensor, grid_info):\n            self.eval()\n            with torch.no_grad():\n                mask = self.forward(img_tensor.unsqueeze(0).to(device))\n            # For demonstration, split into 2 leads by cropping\n            h = img_tensor.shape[-2]\n            lead1 = img_tensor[..., :h//2, :]\n            lead2 = img_tensor[..., h//2:, :]\n            return [lead1, lead2]\n\n    class WaveformSegmenter(nn.Module):\n        def __init__(self, config):\n            super().__init__()\n            # Simple CNN for waveform segmentation\n            self.conv = nn.Sequential(\n                nn.Conv2d(1, 8, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(8, 1, 3, padding=1),\n                nn.Sigmoid()\n            )\n\n        def forward(self, x):\n            return self.conv(x)\n\n        def segment(self, lead_tensor):\n            self.eval()\n            with torch.no_grad():\n                mask = self.forward(lead_tensor.unsqueeze(0).to(device))\n            return mask.squeeze(0).cpu()\n\n    class SignalExtractor:\n        def __init__(self, config):\n            pass\n\n        def extract(self, mask, grid_info):\n            # Dummy signal extraction: mean pixel value per column\n            signal = mask.squeeze().mean(dim=0).cpu().numpy()\n            return signal\n\n    class PostProcessor:\n        def __init__(self, config):\n            pass\n\n        def calibrate(self, signal, grid_info):\n            # Dummy calibration: normalize to [0, 1]\n            signal = (signal - signal.min()) / (signal.max() - signal.min() + 1e-8)\n            return signal\n\n    class ECGDigitizationPipeline:\n        def __init__(self, config):\n            self.grid_detector = GridDetector(config).to(device)\n            self.lead_segmenter = LeadSegmenter(config).to(device)\n            self.waveform_segmenter = WaveformSegmenter(config).to(device)\n            self.signal_extractor = SignalExtractor(config)\n            self.postprocessor = PostProcessor(config)\n\n        def read_image(self, img_path):\n            import cv2\n            img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, (256, 256))\n            img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0) / 255.0\n            return img_tensor\n\n        def process(self, img_path):\n            img_tensor = self.read_image(img_path)\n            grid_info = self.grid_detector.detect(img_tensor)\n            leads = self.lead_segmenter.segment(img_tensor, grid_info)\n            masks = [self.waveform_segmenter.segment(lead.unsqueeze(0)) for lead in leads]\n            signals = [self.signal_extractor.extract(mask, grid_info) for mask in masks]\n            processed_signals = [self.postprocessor.calibrate(sig, grid_info) for sig in signals]\n            return processed_signals\n\n    # --- Example usage on preprocessed images ---\n    config = {'input_size': 256}\n    pipeline = ECGDigitizationPipeline(config)\n\n    # Visualize pipeline output for a few train images\n    if 'train_df' in locals() and not train_df.empty and 'preprocessed_img' in train_df.columns:\n        print('Running pipeline on sample train images...')\n        sample_imgs = train_df['preprocessed_img'][:3]\n        fig, axes = plt.subplots(len(sample_imgs), 2, figsize=(10, 3*len(sample_imgs)))\n        for i, img_arr in enumerate(sample_imgs):\n            # Save temp image for pipeline (simulate file input)\n            import cv2, tempfile\n            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n                cv2.imwrite(tmp.name, (img_arr * 255).astype('uint8'))\n                signals = pipeline.process(tmp.name)\n            axes[i, 0].imshow(img_arr, cmap='gray')\n            axes[i, 0].set_title(f'Preprocessed Image {i+1}')\n            axes[i, 0].axis('off')\n            for sig in signals:\n                axes[i, 1].plot(sig)\n            axes[i, 1].set_title(f'Extracted Signals {i+1}')\n            axes[i, 1].set_xlabel('Time')\n            axes[i, 1].set_ylabel('Normalized Amplitude')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print('‚ö† No preprocessed train images available for pipeline demo.')\n\n    print('‚úÖ Model Architecture complete!')\n\nexcept Exception as e:\n    print(f'‚úó Error in Model Architecture: {e}')\n    import traceback\n    traceback.print_exc()"},{"cell_type":"markdown","metadata":{},"source":"## 6. Implementation & Next Steps\n\n**Note:** This section provides guidance, not complete code. Actual implementation depends on competition task."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"print('üìã === IMPLEMENTATION GUIDE ===\\n')\n\nprint('Competition task determines implementation approach\\n')\nprint('Possible approaches:')\nprint('  - Classification: Train classifier, predict labels')\nprint('  - Regression: Train regressor, predict values')\nprint('  - Generation: Generate required outputs')\nprint('  - Processing: Transform/extract data')\n\nprint('\\n‚ö†Ô∏è TODO: Implement competition-specific solution')\n"},{"cell_type":"markdown","metadata":{},"source":"## 7. Submission\n\n**Generate submission file in competition format**"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"print('üì§ === SUBMISSION GENERATION ===\\n')\n\nprint('‚ö†Ô∏è TODO: Check competition submission format')\nprint('Typical formats: CSV, Parquet, JSON')\n\n# Generic template (uncomment and modify):\n# submission = pd.DataFrame({\n#     'id': test_ids,\n#     'prediction': predictions  # YOUR PREDICTIONS HERE\n# })\n# submission.to_csv('submission.csv', index=False)\n# print('‚úÖ Submission created!')\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}