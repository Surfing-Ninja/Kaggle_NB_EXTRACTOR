{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"collapsed_sections":["U4NcSAnx7wpY"],"gpuType":"L4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"1ce6b7e521eb40d582f705679598ff9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1eef1cd71c4f4dfd8cdb08bea9694d75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"461eee2eac8e41ed992f1e045ec1950f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4907c36bac5a41c496231076a3e8031c","placeholder":"â€‹","style":"IPY_MODEL_7fbc44070a264374801ea1fb81dbd43e","value":"Kaggle credentials successfully validated."}},"4769cfaab5904a928534bbcdb36c8ef0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_552026f5138e431383c1ce32b8f7dc18","placeholder":"â€‹","style":"IPY_MODEL_50a6fe11d90840fd8905e1241c6600f9","value":""}},"4907c36bac5a41c496231076a3e8031c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50a6fe11d90840fd8905e1241c6600f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"531d6b9c918c46c88acf30aa3fb393d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1eef1cd71c4f4dfd8cdb08bea9694d75","placeholder":"â€‹","style":"IPY_MODEL_919d17df28324bf8b655bd10c2626ccf","value":"\n<b>Thank You</b></center>"}},"5368d66c59d2494f803a012337342448":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"552026f5138e431383c1ce32b8f7dc18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7241cfbefd1f4a718e2dea9b2ef41472":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fbc44070a264374801ea1fb81dbd43e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"835f6009cf1a489dad644583e354529c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"88fbece6c0dd4baa9251037733237328":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_461eee2eac8e41ed992f1e045ec1950f"],"layout":"IPY_MODEL_b508137993f34ea7ade3be4830c723d0"}},"89ad6a52ba344146b5f432497c32fa86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f521d7881ab439d96017041db11e9a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89ad6a52ba344146b5f432497c32fa86","placeholder":"â€‹","style":"IPY_MODEL_1ce6b7e521eb40d582f705679598ff9e","value":"Connecting..."}},"8fb030a625984e25955580a553f9a2df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"919d17df28324bf8b655bd10c2626ccf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6bec01c34344f1998eb900920df5e61":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b508137993f34ea7ade3be4830c723d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"b5b8784f439e4efaa9dbbb3cc34be1f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbbec4eebfef4610a631bd8af2323cdb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"TextModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Username:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_8fb030a625984e25955580a553f9a2df","placeholder":"â€‹","style":"IPY_MODEL_5368d66c59d2494f803a012337342448","value":"frankmorales"}},"cceb35fb452a4f9f8bf6033d81db05a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_b5b8784f439e4efaa9dbbb3cc34be1f3","style":"IPY_MODEL_835f6009cf1a489dad644583e354529c","tooltip":""}},"fb7c37dbe7724c569d7ca2218e9dd305":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6bec01c34344f1998eb900920df5e61","placeholder":"â€‹","style":"IPY_MODEL_7241cfbefd1f4a718e2dea9b2ef41472","value":"<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ðŸ“„ Project Documentation: Scientific Image Forgery Detection (RLUC-SIFD)\n\nThis notebook implements a semantic segmentation model using a **U-Net architecture** combined with **Error Level Analysis (ELA)** features for detecting and segmenting copy-move forgeries in scientific images. The solution is specifically tailored for the Kaggle $\\text{Recod.ai/LUC}$ SIFD competition.\n\n### 1. Configuration & Paths\n\n| Parameter | Value | Description |\n| :--- | :--- | :--- |\n| $\\text{TARGET\\_SIZE}$ | $256$ | Standard resolution for image and mask resizing before model input. |\n| $\\text{DEVICE}$ | `'cuda'` / `'cpu'` | Execution device, prioritizing GPU. |\n| $\\text{BATCH\\_SIZE}$ | $8$ | Batch size for training and inference. |\n| $\\text{EPOCHS}$ | $3$ | Number of training epochs (reduced for quick iteration). |\n| $\\text{LEARNING\\_RATE}$ | $1e-4$ | Initial learning rate for the Adam optimizer. |\n| $\\text{FIXED\\_THRESHOLD}$ | $0.45$ | Probability threshold applied to U-Net output during inference. |\n| $\\text{MIN\\_FORGERY\\_AREA}$ | $32$ | Minimum pixel area for a predicted forgery segment to be kept. |\n| $\\text{TRAIN\\_ROOT}$ | `/kaggle/input/.../train_images` | Path to the training image directories. |\n| $\\text{MASK\\_ROOT}$ | `/kaggle/input/.../train_masks` | Path to the ground-truth binary mask files ($\\text{.npy}$). |\n| $\\text{MODEL\\_SAVE\\_PATH}$ | `/tmp/model_new_scratch.pth` | Location to save the best-performing model weights. |\n\n---\n\n### 2. Utility and Feature Engineering\n\n#### $2.1.$ `compute_ela(img_path, quality=95, scale=10)`\n\nThis function generates the **Error Level Analysis (ELA)** feature map. ELA highlights areas in an image that have a different compression history (often due to forgery, copy-move, or re-saving).\n\n* **Process:**\n    1.  Loads the image and resizes it to $\\text{TARGET\\_SIZE} \\times \\text{TARGET\\_SIZE}$.\n    2.  Compresses the resized image as a JPEG with a fixed quality ($\\text{quality}=95$).\n    3.  Calculates the **absolute difference (error)** between the original resized image and the re-compressed image.\n    4.  The mean of the absolute error across the RGB channels is taken, and this $2\\text{D}$ error map is scaled by **$10$** ($\\text{scale}=10$).\n* **Output:** A $2\\text{D}$ array ($\\text{float32}$) of shape $(\\text{TARGET\\_SIZE}, \\text{TARGET\\_SIZE})$.\n\n---\n\n### 3. Model Architecture\n\n#### $3.1.$ `UNet(in_channels=4, num_classes=1)`\n\nA standard **U-Net** architecture adapted for this specific task.\n\n* **Input Channels:** $\\text{in\\_channels}=4$. This accepts the combined $\\text{RGB}$ (3 channels) and $\\text{ELA}$ (1 channel) input tensor.\n* **Encoder:** Consists of $\\text{Conv-ReLU-Dropout-Conv-ReLU}$ blocks and $\\text{MaxPool2d}$ for downsampling.\n* **Decoder:** Uses $\\text{ConvTranspose2d}$ ($\\text{upconv}$) for upsampling, followed by concatenation ($\\text{torch.cat}$) with the corresponding feature maps from the encoder ($\\text{skip connections}$).\n* **Output Layer:** A final $\\text{Conv2d}$ followed by a **$\\text{Sigmoid}$ activation** function, producing a probability map for the forgery mask.\n\n---\n\n### 4. Loss Functions\n\n#### $4.1.$ `DiceLoss(smooth=1.0)`\n\nImplements the **Dice Loss**, a common metric for highly imbalanced segmentation tasks. It measures the overlap between the predicted mask and the ground truth.\n\n$$\n\\text{Dice Loss} = 1 - \\frac{2 \\cdot |\\text{Pred} \\cap \\text{Target}| + \\text{smooth}}{|\\text{Pred}| + |\\text{Target}| + \\text{smooth}}\n$$\n\n#### $4.2.$ `HybridLoss(dice_weight=0.5)`\n\nCombines **Dice Loss** and **Binary Cross-Entropy (BCE) Loss** to provide stable training. BCE is effective for pixel-wise classification, while Dice Loss optimizes for region overlap.\n\n$$\n\\text{Hybrid Loss} = \\text{dice\\_weight} \\cdot \\text{Dice Loss} + (1 - \\text{dice\\_weight}) \\cdot \\text{BCE Loss}\n$$\n\n* $\\text{dice\\_weight}$ is set to **$0.5$**, giving equal importance to both terms.\n\n---\n\n### 5. Data Handling\n\n#### $5.1.$ `ForgeryDataset(Dataset)`\n\nA custom PyTorch Dataset class that handles loading, feature generation, and preprocessing for a single sample.\n\n* **Steps:**\n    1.  Loads the image ($\\text{RGB}$) and the mask ($\\text{npy}$).\n    2.  Computes the **ELA feature map**.\n    3.  Resizes $\\text{RGB}$ image and $\\text{ELA}$ map to $\\text{TARGET\\_SIZE}$ ($\\text{256}$).\n    4.  Resizes the binary mask to $\\text{TARGET\\_SIZE}$ using $\\text{cv2.INTER\\_NEAREST}$ to preserve pixel values.\n    5.  **Concatenates** the normalized $\\text{RGB}$ image and the $\\text{ELA}$ feature (after expanding its dimensions) to create the 4-channel input tensor.\n    6.  The final input is an $\\text{L} \\times \\text{H} \\times \\text{W}$ tensor (i.e., $4 \\times 256 \\times 256$), and the target mask is $1 \\times 256 \\times 256$.\n\n---\n\n### 6. Inference and Submission\n\n#### $6.1.$ `rle_encode(mask)`\n\nImplements the standard **Run-Length Encoding (RLE)** function required by the competition. If no forgery is detected ($\\text{mask.sum()} = 0$), it returns `'authentic'`.\n\n#### $6.2.$ `run_inference_and_segment(unet_model, test_df)`\n\nHandles prediction and post-processing on the test data:\n\n1.  **Prediction:** Processes images in batches, combining $\\text{RGB}$ and $\\text{ELA}$ features for input.\n2.  **Thresholding:** Applies the $\\text{FIXED\\_THRESHOLD}$ ($\\mathbf{0.45}$) to the model's probability output to generate an initial binary mask.\n3.  **Post-Processing:** Uses $\\text{cv2.connectedComponentsWithStats}$ to identify connected segments. Segments with an area less than $\\text{MIN\\_FORGERY\\_AREA}$ ($\\mathbf{32}$) are discarded.\n4.  **Resizing:** The cleaned binary mask is resized back to the original image dimensions using $\\text{INTER\\_NEAREST}$.\n5.  **Submission Format:** The final mask is $\\text{RLE}$ encoded.\n6.  **CSV Output:** The results are written to $\\text{submission.csv}$, ensuring the $\\text{RLE}$ string is enclosed in brackets (`[RLE_STRING]`) as often required for Kaggle submissions.","metadata":{}},{"cell_type":"markdown","source":"## ðŸ’¾ EDA","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/authentic/*.png |wc -l","metadata":{"execution":{"iopub.status.busy":"2025-11-01T00:02:54.04066Z","iopub.execute_input":"2025-11-01T00:02:54.040904Z","iopub.status.idle":"2025-11-01T00:02:58.125929Z","shell.execute_reply.started":"2025-11-01T00:02:54.040885Z","shell.execute_reply":"2025-11-01T00:02:58.125249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/forged/*.png |wc -l ","metadata":{"execution":{"iopub.status.busy":"2025-11-01T00:03:33.171427Z","iopub.execute_input":"2025-11-01T00:03:33.171941Z","iopub.status.idle":"2025-11-01T00:03:38.260894Z","shell.execute_reply.started":"2025-11-01T00:03:33.171914Z","shell.execute_reply":"2025-11-01T00:03:38.26019Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks/*.npy |wc -l ","metadata":{"execution":{"iopub.status.busy":"2025-11-01T00:03:41.851009Z","iopub.execute_input":"2025-11-01T00:03:41.851736Z","iopub.status.idle":"2025-11-01T00:03:47.04507Z","shell.execute_reply.started":"2025-11-01T00:03:41.851705Z","shell.execute_reply":"2025-11-01T00:03:47.044417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## ðŸ“Š Exploratory Data Analysis (EDA)\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\n\n# --- CONFIGURATION (from the original notebook) ---\nTRAIN_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\"\nMASK_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\"\n\nprint(\"--- Starting Basic EDA ---\")\n\n# 1. Prepare Data List (same logic as in the notebook)\ndata_list = []\nfor root, _, files in os.walk(TRAIN_ROOT):\n    for f in files:\n        valid_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')\n        if f.lower().endswith(valid_extensions):\n            if 'forged' in root.lower():\n                case_id = os.path.splitext(f)[0]\n                img_path = os.path.join(root, f)\n                mask_path = os.path.join(MASK_ROOT, f\"{case_id}.npy\")\n                data_list.append({\n                    'case_id': case_id,\n                    'img_path': img_path,\n                    'mask_path': mask_path\n                })\nfull_df = pd.DataFrame(data_list)\n\n# Filter for images with existing masks\nif not full_df.empty:\n    full_df['mask_exists'] = full_df['mask_path'].apply(os.path.exists)\n    eda_df = full_df[full_df['mask_exists']].drop(columns=['mask_exists']).reset_index(drop=True)\nelse:\n    eda_df = pd.DataFrame(columns=['case_id', 'img_path', 'mask_path'])\n\n\n# 2. File and Case Counts\nprint(f\"\\nTotal potential images found in TRAIN_ROOT: {len(data_list)}\")\nprint(f\"Total valid image/mask pairs for training (Forged Cases): {len(eda_df)}\")\n\n# 3. Image and Mask Size Distribution\nif not eda_df.empty:\n    img_heights, img_widths = [], []\n    mask_pixels = [] # Count of non-zero pixels in the mask\n    \n    print(\"\\nAnalyzing image and mask dimensions/forgery area...\")\n    for index, row in tqdm(eda_df.iterrows(), total=len(eda_df)):\n        try:\n            # Read Image\n            img = cv2.imread(row['img_path'])\n            if img is not None and img.size > 0:\n                h, w = img.shape[:2]\n                img_heights.append(h)\n                img_widths.append(w)\n\n            # Read Mask (npy file)\n            mask = np.load(row['mask_path'])\n            # Assuming the forgery area is represented by non-zero pixels\n            mask_pixels.append(np.sum(mask > 0)) \n\n        except Exception as e:\n            # Handle files that can't be read (e.g., non-image .npy files not handled by cv2.imread)\n            pass\n\n    print(\"\\n--- Image Dimension Stats ---\")\n    print(f\"Unique Image Widths: {sorted(list(set(img_widths)))[:5]}{'...' if len(set(img_widths)) > 5 else ''}\")\n    print(f\"Unique Image Heights: {sorted(list(set(img_heights)))[:5]}{'...' if len(set(img_heights)) > 5 else ''}\")\n    print(f\"Mean Image Dimensions (H x W): {np.mean(img_heights):.0f} x {np.mean(img_widths):.0f}\")\n\n    # 4. Forgery Area Analysis\n    mask_pixels = np.array(mask_pixels)\n    forged_cases_with_area = np.sum(mask_pixels > 0)\n    total_forgery_area = np.sum(mask_pixels)\n    \n    print(\"\\n--- Forgery Area Stats ---\")\n    print(f\"Total cases with non-zero forgery area: {forged_cases_with_area} / {len(eda_df)}\")\n    print(f\"Mean Forgery Pixel Count per forged image: {np.mean(mask_pixels[mask_pixels > 0]):.0f} (pixels)\")\n    print(f\"Maximum Forgery Pixel Count: {np.max(mask_pixels)} (pixels)\")\n\n    # 5. Visualization: Forgery Area Distribution (First 100 cases for quick view)\n    plt.figure(figsize=(12, 5))\n    plt.bar(range(min(100, len(mask_pixels))), mask_pixels[:100])\n    plt.title('Forgery Pixel Count (First 100 Forged Samples)')\n    plt.xlabel('Sample Index')\n    plt.ylabel('Forged Pixel Count (Area)')\n    plt.show()\n\nelse:\n    print(\"ðŸ›‘ EDA Skipped: The dataframe is empty. Check TRAIN_ROOT and MASK_ROOT paths.\")\n\nprint(\"\\n--- EDA Complete ---\")","metadata":{"execution":{"iopub.status.busy":"2025-11-01T00:03:50.202392Z","iopub.execute_input":"2025-11-01T00:03:50.202757Z","iopub.status.idle":"2025-11-01T00:06:35.112621Z","shell.execute_reply.started":"2025-11-01T00:03:50.202731Z","shell.execute_reply":"2025-11-01T00:06:35.111763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸš€ Advanced Exploratory Data Analysis (Imbalance & Feature Check)","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport warnings\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore') # Suppress warnings\n\n# --- CONFIGURATION (from the original notebook) ---\nTARGET_SIZE = 256\nTRAIN_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\"\nMASK_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\"\n\n# Replicate compute_ela for feature analysis\ndef compute_ela(img_path, quality=95, scale=10):\n    # ... (omitted for brevity, assume the original function is available)\n    # The original notebook's ELA function is used here.\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    if img is None or img.size == 0:\n        return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_ela_{os.path.basename(img_path)}.jpg\" # Simplified temp_path\n    try:\n        # Use a consistent quality setting (95)\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality]) \n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale # Scale by 10 as in the notebook\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\n# Load the filtered DataFrame (assuming the prior EDA cell's 'eda_df' is available or recreate it)\ndata_list = []\nfor root, _, files in os.walk(TRAIN_ROOT):\n    for f in files:\n        valid_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')\n        if f.lower().endswith(valid_extensions) and 'forged' in root.lower():\n            case_id = os.path.splitext(f)[0]\n            mask_path = os.path.join(MASK_ROOT, f\"{case_id}.npy\")\n            if os.path.exists(mask_path):\n                data_list.append({'img_path': os.path.join(root, f), 'mask_path': mask_path})\neda_df = pd.DataFrame(data_list)\n\nprint(\"--- Starting Advanced EDA (Imbalance & Feature Check) ---\")\n\nif eda_df.empty:\n    print(\"ðŸ›‘ EDA Skipped: Data frame is empty.\")\nelse:\n    total_pixels = 0\n    forgery_pixels = 0\n    ela_values, rgb_means = [], []\n\n    # Process only the first 50 images to speed up ELA computation for EDA\n    for index, row in tqdm(eda_df.head(50).iterrows(), total=len(eda_df.head(50)), desc=\"Processing samples\"):\n        try:\n            # 1. Image and Mask Load\n            rgb_image = cv2.cvtColor(cv2.imread(row['img_path']), cv2.COLOR_BGR2RGB)\n            if rgb_image is None or rgb_image.size == 0: continue\n                \n            mask = np.load(row['mask_path'])\n            if mask.ndim > 2: mask = mask[:, :, 0]\n            \n            # 2. Imbalance Check (Use original sizes for best estimate)\n            h, w = rgb_image.shape[:2]\n            total_pixels += h * w\n            forgery_pixels += np.sum(mask > 0)\n            \n            # 3. ELA Feature Check (Use 256x256 resized data)\n            ela_feature = compute_ela(row['img_path'])\n            ela_values.extend(ela_feature.flatten())\n            \n            # RGB feature check (resize/normalize similar to training)\n            rgb_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE)) / 255.0\n            rgb_means.extend(rgb_resized.mean(axis=2).flatten())\n\n        except Exception as e:\n            # print(f\"Warning: Could not process {row['img_path']}: {e}\")\n            continue\n\n    # --- Analysis 1: Imbalance Ratio ---\n    if total_pixels > 0:\n        imbalance_ratio = (forgery_pixels / total_pixels) * 100\n        print(f\"\\n--- Imbalance Ratio (Forged Pixels) ---\")\n        print(f\"Total Pixels Sampled: {total_pixels:,}\")\n        print(f\"Forged Pixels Sampled: {forgery_pixels:,}\")\n        print(f\"Forgery Imbalance Ratio: **{imbalance_ratio:.2f}%** (Positive Class)\")\n    \n    # --- Analysis 2: ELA Feature Distribution vs. RGB ---\n    if ela_values:\n        ela_values = np.array(ela_values)\n        rgb_means = np.array(rgb_means)\n\n        print(f\"\\n--- ELA Feature Distribution (Scaled by 10) ---\")\n        print(f\"ELA Feature Mean: {np.mean(ela_values):.4f}\")\n        print(f\"ELA Feature Std Dev: {np.std(ela_values):.4f}\")\n        print(f\"RGB Mean (Normalized): {np.mean(rgb_means):.4f}\")\n\n        plt.figure(figsize=(12, 5))\n        plt.hist(ela_values, bins=50, alpha=0.6, label='ELA Feature (Scaled)', color='red')\n        plt.title('Distribution of ELA Feature Values')\n        plt.xlabel('ELA Value (0 to ~2550)')\n        plt.ylabel('Frequency')\n        plt.legend()\n        plt.show()\n        \n        # This histogram helps visualize if ELA is predominantly zero or clustered.\n\nprint(\"\\n--- Advanced EDA Complete ---\")","metadata":{"execution":{"iopub.status.busy":"2025-11-01T00:07:19.613372Z","iopub.execute_input":"2025-11-01T00:07:19.614036Z","iopub.status.idle":"2025-11-01T00:07:31.875987Z","shell.execute_reply.started":"2025-11-01T00:07:19.614014Z","shell.execute_reply":"2025-11-01T00:07:31.875117Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸš€ File image test","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport os\n\n# Define the file path\nTEST_IMAGE_PATH = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images/45.png\"\n\nprint(f\"Attempting to load image: {TEST_IMAGE_PATH}\")\n\nif not os.path.exists(TEST_IMAGE_PATH):\n    print(\"ðŸ›‘ ERROR: The file path was not found. Please ensure the Kaggle competition data is mounted correctly.\")\nelse:\n    # Load the image using OpenCV (loads as BGR)\n    img = cv2.imread(TEST_IMAGE_PATH)\n    \n    if img is None:\n        print(\"ðŸ›‘ ERROR: Could not read the image file.\")\n    else:\n        # Convert the image from BGR (OpenCV default) to RGB (Matplotlib default)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Plot the image\n        plt.figure(figsize=(10, 8))\n        plt.imshow(img_rgb)\n        plt.title(f\"Test Image 45 (Dimensions: {img.shape[0]}x{img.shape[1]})\")\n        plt.axis('off') # Hide axes for a cleaner image view\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-11-01T00:07:38.986331Z","iopub.execute_input":"2025-11-01T00:07:38.986606Z","iopub.status.idle":"2025-11-01T00:07:39.447492Z","shell.execute_reply.started":"2025-11-01T00:07:38.986587Z","shell.execute_reply":"2025-11-01T00:07:39.446693Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ’¾ Final Corrected Training Code","metadata":{"id":"CD3ZAtsN7VJw"}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport warnings\nfrom warnings import filterwarnings\n\n# Suppress the specific UserWarning from the LR scheduler\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.optim.lr_scheduler\")\nfilterwarnings('ignore')\n\n# --- CONFIGURATION ---\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\nEPOCHS = 1 \nLEARNING_RATE = 1e-4\n\n# --- PATHS (CORRECTED FOR KAGGLEHUB CACHE) ---\nTRAIN_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\"\nMASK_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\"\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\n\n# --- UTILITY FUNCTIONS ---\ndef compute_ela(img_path, quality=95, scale=10):\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3:\n                img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2:\n                img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception:\n            return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    if img is None or img.size == 0:\n        return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_ela_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE),\n                      interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\n# --- LOSS FUNCTIONS (FOCAL + DICE) ---\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1.0):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n    def forward(self, pred, target):\n        pred = pred.contiguous().view(-1)\n        target = target.contiguous().view(-1)\n        intersection = (pred * target).sum()\n        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n        return 1 - dice\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, inputs, targets):\n        inputs = inputs.contiguous().view(-1)\n        targets = targets.contiguous().view(-1)\n        BCE = F.binary_cross_entropy(inputs, targets, reduction='none')\n        BCE_EXP = torch.exp(-BCE)\n        Focal = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n        return Focal.mean()\n\nclass HybridFocalLoss(nn.Module):\n    def __init__(self, dice_weight=0.8):\n        super(HybridFocalLoss, self).__init__()\n        self.dice_loss = DiceLoss() \n        self.focal_loss = FocalLoss()\n        self.dice_weight = dice_weight\n\n    def forward(self, pred, target):\n        dice = self.dice_loss(pred, target)\n        focal = self.focal_loss(pred, target)\n        return self.dice_weight * dice + (1 - self.dice_weight) * focal\n\n\n# U-Net architecture (Unchanged)\nclass UNet(nn.Module):\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\nclass ForgeryDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = row['img_path']\n        mask_path = row['mask_path']\n\n        # --- Load Image ---\n        rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n\n        if rgb_image is None or rgb_image.size == 0:\n            try:\n                img_data = np.load(img_path)\n                if img_data.ndim == 3:\n                    rgb_image = img_data\n                elif img_data.ndim == 2:\n                    rgb_image = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)\n            except Exception as e:\n                raise RuntimeError(f\"Failed to load image from {img_path}: {e}\")\n\n        # --- Load Mask ---\n        try:\n            mask = np.load(mask_path)\n            if mask.ndim > 2:\n                mask = mask[:, :, 0]\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load mask from {mask_path}: {e}\")\n\n        ela_feature_2d = compute_ela(img_path)\n\n        # Resize all features\n        rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n        ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n\n        # Use INTER_NEAREST for binary mask resizing\n        mask_resized = cv2.resize(mask.astype(np.uint8), (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_NEAREST)\n\n        # Stack RGB (3) and ELA (1) for a 4-channel input\n        ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n        stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n\n        # Convert to PyTorch tensors and normalize\n        image = torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n        mask = torch.tensor(mask_resized / 255.0, dtype=torch.float32).unsqueeze(0)\n\n        return image, mask\n\ndef train_model(model, train_loader, val_loader, epochs=EPOCHS, save_path=MODEL_SAVE_PATH):\n    # Set the new loss function\n    criterion = HybridFocalLoss(dice_weight=0.8) \n    \n    # CRITICAL CHANGE: Switched to AdamW with Weight Decay to break flat minima\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2) \n\n    # Scheduler with patience=2 (unchanged)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\n    best_val_loss = float('inf')\n\n    model.to(DEVICE)\n    print(f\"Starting training on {DEVICE} for {epochs} epochs...\")\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss_sum = 0\n\n        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            train_loss_sum += loss.item()\n\n        avg_train_loss = train_loss_sum / len(train_loader)\n\n        # Validation Phase\n        model.eval()\n        val_loss_sum = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss_sum += loss.item()\n\n        avg_val_loss = val_loss_sum / len(val_loader)\n\n        # Scheduler Step\n        scheduler.step(avg_val_loss)\n\n        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), save_path)\n            print(f\"Model saved successfully to {save_path}. (New best Val Loss: {best_val_loss:.4f})\\n\")\n\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Current Learning Rate: {current_lr:.6f}\")\n\n\n# --- FUNCTIONS FOR THRESHOLD OPTIMIZATION (Unchanged) ---\ndef calculate_dice_coefficient(pred, target, smooth=1.0):\n    \"\"\"Calculates the Dice Similarity Coefficient (DSC) for post-processing tuning.\"\"\"\n    pred = pred.contiguous().view(-1)\n    target = target.contiguous().view(-1)\n    intersection = (pred * target).sum()\n    \n    # Dice Coefficient Formula: (2*TP) / (2*TP + FP + FN)\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    return dice.item()\n\ndef find_optimal_threshold(model, val_loader, device, threshold_range=np.arange(0.30, 0.75, 0.05)):\n    \"\"\"\n    Finds the threshold that maximizes the average Dice Coefficient on the validation set.\n    \"\"\"\n    model.to(device)\n    model.eval()\n    \n    best_dice = -1.0\n    best_threshold = 0.50\n    \n    print(\"\\n--- Finding Optimal Threshold on Validation Set ---\")\n    \n    # Iterate through each candidate threshold\n    for threshold in threshold_range:\n        total_dice = 0.0\n        \n        with torch.no_grad():\n            # TQDM wrapper for the val_loader\n            for inputs, targets in tqdm(val_loader, desc=f\"Evaluating T={threshold:.2f}\"):\n                inputs, targets = inputs.to(device), targets.to(device)\n                \n                # 1. Get raw probability output from the model\n                outputs = model(inputs)\n                \n                # 2. Apply current threshold to create a binary prediction mask\n                predicted_mask = (outputs > threshold).float()\n                \n                # 3. Calculate Dice Coefficient for the batch\n                batch_dice = calculate_dice_coefficient(predicted_mask, targets)\n                total_dice += batch_dice\n\n        avg_dice = total_dice / len(val_loader)\n        \n        if avg_dice > best_dice:\n            best_dice = avg_dice\n            best_threshold = threshold\n            \n        print(f\"Threshold: {threshold:.2f} | Avg. Val Dice: {avg_dice:.4f}\")\n\n    print(f\"\\nâœ… Optimal Threshold found: {best_threshold:.2f} with Dice: {best_dice:.4f}\")\n    return best_threshold\n\n# --- MAIN EXECUTION BLOCK ---\nif __name__ == '__main__':\n\n    print(\"Preparing training data paths...\\n\")\n\n    data_list = []\n\n    # Recursively walk through the TRAIN_ROOT to find all image files\n    for root, _, files in os.walk(TRAIN_ROOT):\n        for f in files:\n            valid_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')\n\n            if f.lower().endswith(valid_extensions):\n                # Only process files in the 'forged' subdirectory, as only they have masks\n                if 'forged' in root.lower():\n                    case_id = os.path.splitext(f)[0]\n                    img_path = os.path.join(root, f)\n\n                    # Use .npy for the mask extension\n                    mask_path = os.path.join(MASK_ROOT, f\"{case_id}.npy\")\n\n                    data_list.append({\n                        'case_id': case_id,\n                        'img_path': img_path,\n                        'mask_path': mask_path\n                    })\n\n    if not data_list:\n        full_df = pd.DataFrame(columns=['case_id', 'img_path', 'mask_path'])\n    else:\n        full_df = pd.DataFrame(data_list)\n\n    if not full_df.empty:\n        # Final check: Keep only images that have a corresponding mask file\n        full_df['mask_exists'] = full_df['mask_path'].apply(os.path.exists)\n        full_df = full_df[full_df['mask_exists']].drop(columns=['mask_exists']).reset_index(drop=True)\n\n    if full_df.empty:\n        print(\"ðŸ›‘ FATAL ERROR: No valid image/mask pairs found in the input paths. Cannot train. (Check file extensions/paths again)\")\n    else:\n        print(f\"âœ… Found {len(full_df)} valid forged samples for training.\")\n\n        # Split data\n        train_df, val_df = train_test_split(full_df, test_size=0.1, random_state=42)\n\n        # Create DataLoaders\n        train_dataset = ForgeryDataset(train_df)\n        val_dataset = ForgeryDataset(val_df)\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        # Instantiate model (4 input channels: 3 RGB + 1 ELA)\n        model = UNet(in_channels=4)\n\n        # START TRAINING\n        train_model(model, train_loader, val_loader)\n\n        print(\"\\nâœ… TRAINING COMPLETE.\")\n\n        # --- HYPERPARAMETER TUNING: FIND OPTIMAL THRESHOLD ---\n        try:\n            # Load the best saved model state before running the threshold search\n            model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n\n            # Run the optimization function\n            optimal_threshold = find_optimal_threshold(model, val_loader, DEVICE)\n\n            # Inform the user to update Cell 6\n            print(f\"\\nðŸ“¢ ACTION REQUIRED: Please use {optimal_threshold:.2f} as the FIXED_THRESHOLD in your final inference code (Cell 6).\")\n            \n        except FileNotFoundError:\n            print(f\"ðŸ›‘ ERROR: Trained model not found at {MODEL_SAVE_PATH}. Cannot perform threshold tuning.\")","metadata":{"id":"oeY6sOXG6-D6","outputId":"ae806cc5-1cdd-4419-a4e4-518d0109c8a6","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:39:59.016001Z","iopub.execute_input":"2025-11-01T03:39:59.016605Z","iopub.status.idle":"2025-11-01T03:48:29.316708Z","shell.execute_reply.started":"2025-11-01T03:39:59.016583Z","shell.execute_reply":"2025-11-01T03:48:29.315711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ’¾ Final Corrected Inference Code (Robust Settings)\n","metadata":{"id":"E-rnPI8N7PVM"}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/test_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:53:08.180339Z","iopub.execute_input":"2025-11-01T03:53:08.181059Z","iopub.status.idle":"2025-11-01T03:53:08.32203Z","shell.execute_reply.started":"2025-11-01T03:53:08.181034Z","shell.execute_reply":"2025-11-01T03:53:08.321189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -pr /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/forged/10.png /kaggle/working/test_images/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:53:11.52639Z","iopub.execute_input":"2025-11-01T03:53:11.52725Z","iopub.status.idle":"2025-11-01T03:53:11.670895Z","shell.execute_reply.started":"2025-11-01T03:53:11.527222Z","shell.execute_reply":"2025-11-01T03:53:11.670062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/authentic/10.png","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/test_images/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:57:14.093629Z","iopub.execute_input":"2025-11-01T03:57:14.094538Z","iopub.status.idle":"2025-11-01T03:57:14.239758Z","shell.execute_reply.started":"2025-11-01T03:57:14.094506Z","shell.execute_reply":"2025-11-01T03:57:14.238956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del TEST_IMAGE_ROOT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:00:52.634471Z","iopub.execute_input":"2025-11-01T04:00:52.635069Z","iopub.status.idle":"2025-11-01T04:00:52.658384Z","shell.execute_reply.started":"2025-11-01T04:00:52.635048Z","shell.execute_reply":"2025-11-01T04:00:52.657517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cat /kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:03:00.448232Z","iopub.execute_input":"2025-11-01T04:03:00.448892Z","iopub.status.idle":"2025-11-01T04:03:00.590865Z","shell.execute_reply.started":"2025-11-01T04:03:00.448866Z","shell.execute_reply":"2025-11-01T04:03:00.589938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nimport csv\nimport warnings\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore') # Suppress warnings\n\n# --- CONFIGURATION & PATHS (ROBUST SETTINGS) ---\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\n\n# MODIFIED PARAMETER: Using the optimal threshold calculated from Cell 5\nFIXED_THRESHOLD = 0.30 \n\n# ROBUST: Moderate filter to remove noise while keeping small artifacts\nMIN_FORGERY_AREA = 32\n\n# CORRECTED PATHS\n#TEST_IMAGE_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\nTEST_IMAGE_ROOT = \"/kaggle/working/test_images\"\nSAMPLE_SUBMISSION_FILE = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\n\n# Model path should point to the successfully trained model\nmodel_path = \"/tmp/model_new_scratch.pth\"\nOUTPUT_FILENAME = \"submission.csv\"\n\n# --- UTILITY FUNCTIONS (Unchanged) ---\n\ndef compute_ela(img_path, quality=95, scale=10):\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception:\n            return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    if img is None or img.size == 0:\n        return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE),\n                      interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\ndef create_test_df_robust(test_image_root, sample_submission_path):\n    master_df = pd.read_csv(sample_submission_path)\n    master_df['case_id'] = master_df['case_id'].astype(str)\n    present_files = {}\n    if os.path.exists(test_image_root):\n        for root, _, files in os.walk(test_image_root):\n            for f in files:\n                case_id = os.path.splitext(f)[0]\n                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')):\n                    present_files[case_id] = os.path.join(root, f)\n\n    master_df['img_path'] = master_df['case_id'].map(present_files)\n    master_df['img_path'] = master_df['img_path'].fillna('MISSING_FILE')\n    return master_df[['case_id', 'img_path']]\n\ndef rle_encode(mask):\n    if mask.sum() == 0: return \"authentic\"\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ', '.join(str(x) for x in runs)\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\n# --- INFERENCE FUNCTION WITH POST-PROCESSING ---\ndef run_inference_and_segment(unet_model, test_df):\n    results = []\n    unet_model.eval()\n    images_to_process = []\n\n    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n        case = row['case_id']\n        img_path = row['img_path']\n\n        if img_path == 'MISSING_FILE' or img_path == 'NOT_FOUND':\n            results.append({'case_id': case, 'annotation': 'authentic'})\n            continue\n\n        try:\n            rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            if rgb_image is None or rgb_image.size == 0:\n                img_data = np.load(img_path)\n                if img_data.ndim == 3: rgb_image = img_data\n                elif img_data.ndim == 2: rgb_image = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)\n\n            if rgb_image is None or rgb_image.size == 0: raise ValueError(f\"Invalid image data for {case}\")\n\n            original_shape = rgb_image.shape[:2]\n            ela_feature_2d = compute_ela(img_path)\n\n            rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n            stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n            images_to_process.append((case, original_shape, stacked_input))\n\n            # Process batch\n            if len(images_to_process) == BATCH_SIZE or index == len(test_df) - 1:\n                if images_to_process:\n                    batch_inputs = torch.stack([\n                        torch.tensor(img_data.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n                        for _, _, img_data in images_to_process\n                    ]).to(DEVICE)\n\n                    with torch.no_grad():\n                        outputs = unet_model(batch_inputs).detach().cpu().numpy()\n\n                    for i, output in enumerate(outputs):\n                        case_id_out, original_shape_out, _ = images_to_process[i]\n                        output_prob = output.squeeze()\n\n                        # --- LOG PROBABILITY HERE ---\n                        max_prob = np.max(output_prob)\n                        print(f\"|--- Case {case_id_out} Max Forgery Probability: {max_prob:.4f} ---|\")\n                        # ----------------------------\n\n                        # Apply Threshold (0.30)\n                        final_mask_resized = (output_prob > FIXED_THRESHOLD).astype(np.uint8)\n\n                        # Minimum Area Filtering (32)\n                        clean_mask_resized = np.zeros_like(final_mask_resized)\n\n                        # Find connected components\n                        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n                            final_mask_resized, 4, cv2.CV_32S\n                        )\n\n                        # Iterate through each component (label 0 is the background)\n                        for label in range(1, num_labels):\n                            area = stats[label, cv2.CC_STAT_AREA]\n                            if area >= MIN_FORGERY_AREA:\n                                # Keep segments that meet the minimum size requirement\n                                clean_mask_resized[labels == label] = 1\n\n                        # Resize the CLEANED mask back to the original size\n                        final_mask = cv2.resize(\n                            clean_mask_resized,\n                            (original_shape_out[1], original_shape_out[0]),\n                            interpolation=cv2.INTER_NEAREST\n                        )\n\n                        rle_annotation = rle_encode(final_mask)\n                        results.append({'case_id': case_id_out, 'annotation': rle_annotation})\n\n                    images_to_process = [] # Reset batch\n        except Exception as e:\n            print(f\"Error processing case {case}: {e}. Defaulting to authentic.\")\n            results.append({'case_id': case, 'annotation': 'authentic'})\n    return pd.DataFrame(results)\n\n# --- MAIN EXECUTION BLOCK ---\nif __name__ == \"__main__\":\n\n    print(f\"--- Starting inference on {DEVICE} at {pd.Timestamp.now()} ---\")\n\n    # 1. Load Model\n    model = None\n    try:\n        model = UNet(in_channels=4).to(DEVICE)\n        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n        model.eval() # Set model to evaluation mode\n        print(f\"Model loaded successfully from {model_path}\")\n    except Exception as e:\n        print(f\"Error loading model from {model_path}. Submitting 'authentic' for all cases. Error: {e}\")\n        model = None\n\n    # 2. Prepare Data\n    test_df = create_test_df_robust(TEST_IMAGE_ROOT, SAMPLE_SUBMISSION_FILE)\n    test_df['case_id'] = test_df['case_id'].astype(str)\n\n    # 3. Run Inference\n    if model:\n        results_df = run_inference_and_segment(model, test_df)\n        print(results_df)\n    else:\n        results_df = test_df[['case_id']].assign(annotation='authentic')\n\n    # 4. Finalize Submission DF\n    submission_df = test_df[['case_id']].copy().merge(results_df, on='case_id', how='left')\n    submission_df['annotation'] = submission_df['annotation'].fillna('authentic')\n    submission_df = submission_df[['case_id', 'annotation']].sort_values('case_id').reset_index(drop=True)\n\n    # 5. Write CSV with Correct RLE Formatting\n    with open(OUTPUT_FILENAME, \"w\", newline='') as f:\n        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(['case_id', 'annotation'])\n\n        for _, row in submission_df.iterrows():\n            case_id = str(row['case_id'])\n            annotation = row['annotation']\n\n            if annotation.lower() == 'authentic':\n                 writer.writerow([case_id, annotation])\n            else:\n                 # Create the full bracketed RLE string\n                 full_rle_string = f\"[{annotation}]\"\n                 writer.writerow([case_id, full_rle_string])\n\n    print(f\"\\nâœ… Created {OUTPUT_FILENAME} with {len(submission_df)} rows at {pd.Timestamp.now()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:05:58.629029Z","iopub.execute_input":"2025-11-01T04:05:58.629337Z","iopub.status.idle":"2025-11-01T04:05:58.713393Z","shell.execute_reply.started":"2025-11-01T04:05:58.629316Z","shell.execute_reply":"2025-11-01T04:05:58.712709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ’¾ submission","metadata":{"id":"U4NcSAnx7wpY"}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport time\nimport csv\nimport warnings\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore') # Suppress warnings\n\n# --- CONFIGURATION & PATHS (Corrected for case 10) ---\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\nFIXED_THRESHOLD = 0.30 # Optimal threshold from validation\nMIN_FORGERY_AREA = 32  # Minimum area filter\n\n# The model must be successfully saved during the training phase\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\n\n# Since you copied 10.png to this folder, we target it here.\nTEST_IMAGE_ROOT = \"/kaggle/working/test_images\"\n# Create a dummy submission file for case_id '10' only\nCUSTOM_SUB_PATH = \"/tmp/custom_sample_submission_10.csv\"\nOUTPUT_FILENAME = \"submission_case10.csv\"\n\n# --- UTILITY FUNCTIONS ---\ndef compute_ela(img_path, quality=95, scale=10):\n    \"\"\"Generates the Error Level Analysis (ELA) feature map.\"\"\"\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception:\n            return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    if img is None or img.size == 0:\n        return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE),\n                      interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\ndef create_test_df_robust(test_image_root, sample_submission_path):\n    \"\"\"Creates a DataFrame mapping case_ids to image paths.\"\"\"\n    master_df = pd.read_csv(sample_submission_path)\n    master_df['case_id'] = master_df['case_id'].astype(str)\n    present_files = {}\n    if os.path.exists(test_image_root):\n        for root, _, files in os.walk(test_image_root):\n            for f in files:\n                case_id = os.path.splitext(f)[0]\n                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')):\n                    present_files[case_id] = os.path.join(root, f)\n\n    master_df['img_path'] = master_df['case_id'].map(present_files)\n    master_df['img_path'] = master_df['img_path'].fillna('MISSING_FILE')\n    return master_df[['case_id', 'img_path']]\n\ndef rle_encode(mask):\n    \"\"\"Encodes a binary mask using Run-Length Encoding. Returns 'authentic' if empty.\"\"\"\n    if mask.sum() == 0: return \"authentic\"\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ', '.join(str(x) for x in runs)\n\n# --- U-NET ARCHITECTURE (copied from notebook) ---\nclass UNet(nn.Module):\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\n# --- INFERENCE FUNCTION WITH POST-PROCESSING ---\ndef run_inference_and_segment(unet_model, test_df, output_file):\n    results = []\n    unet_model.eval()\n    images_to_process = []\n    case_info = []\n\n    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n        case = row['case_id']\n        img_path = row['img_path']\n\n        if img_path == 'MISSING_FILE' or img_path == 'NOT_FOUND':\n            results.append({'case_id': case, 'annotation': 'authentic'})\n            continue\n\n        try:\n            rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            if rgb_image is None or rgb_image.size == 0:\n                # Add numpy file handling for robustness\n                img_data = np.load(img_path)\n                if img_data.ndim == 3: rgb_image = img_data\n                elif img_data.ndim == 2: rgb_image = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)\n\n            if rgb_image is None or rgb_image.size == 0: raise ValueError(f\"Invalid image data for {case}\")\n\n            original_shape = rgb_image.shape[:2]\n            ela_feature_2d = compute_ela(img_path)\n\n            # Preprocessing\n            rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n            stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n\n            images_to_process.append(\n                torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n            )\n            case_info.append((case, original_shape))\n\n            # Process batch\n            if len(images_to_process) == BATCH_SIZE or index == len(test_df) - 1:\n                if images_to_process:\n                    batch_inputs = torch.stack(images_to_process).to(DEVICE)\n\n                    with torch.no_grad():\n                        outputs = unet_model(batch_inputs).detach().cpu().numpy()\n\n                    for i, output in enumerate(outputs):\n                        case_id_out, original_shape_out = case_info[i]\n                        output_prob = output.squeeze()\n\n                        # Thresholding\n                        final_mask_resized = (output_prob > FIXED_THRESHOLD).astype(np.uint8)\n\n                        # Minimum Area Filtering (Post-processing)\n                        clean_mask_resized = np.zeros_like(final_mask_resized)\n                        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n                            final_mask_resized, 4, cv2.CV_32S\n                        )\n                        for label in range(1, num_labels):\n                            area = stats[label, cv2.CC_STAT_AREA]\n                            if area >= MIN_FORGERY_AREA:\n                                clean_mask_resized[labels == label] = 1\n\n                        # Resize back to original dimensions\n                        final_mask = cv2.resize(\n                            clean_mask_resized,\n                            (original_shape_out[1], original_shape_out[0]),\n                            interpolation=cv2.INTER_NEAREST\n                        )\n\n                        rle_annotation = rle_encode(final_mask)\n                        results.append({'case_id': case_id_out, 'annotation': rle_annotation})\n\n                    images_to_process = []\n                    case_info = []\n\n        except Exception as e:\n            print(f\"Error processing case {case}: {e}. Defaulting to authentic.\")\n            results.append({'case_id': case, 'annotation': 'authentic'})\n    \n    # Finalize Submission DF and write CSV\n    results_df = pd.DataFrame(results)\n    submission_df = test_df[['case_id']].copy().merge(results_df, on='case_id', how='left')\n    submission_df['annotation'] = submission_df['annotation'].fillna('authentic')\n    submission_df = submission_df[['case_id', 'annotation']].sort_values('case_id').reset_index(drop=True)\n\n    with open(output_file, \"w\", newline='') as f:\n        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(['case_id', 'annotation'])\n        for _, row in submission_df.iterrows():\n            case_id = str(row['case_id'])\n            annotation = row['annotation']\n            if annotation.lower() == 'authentic':\n                 writer.writerow([case_id, annotation])\n            else:\n                 full_rle_string = f\"[{annotation}]\"\n                 writer.writerow([case_id, full_rle_string])\n    \n    return submission_df\n\n# --- MAIN EXECUTION BLOCK ---\nif __name__ == \"__main__\":\n    \n    # 1. SETUP: Create Custom Test Submission for case_id '10'\n    custom_sub_df = pd.DataFrame({'case_id': ['10'], 'annotation': ['authentic']})\n    custom_sub_df.to_csv(CUSTOM_SUB_PATH, index=False)\n    \n    print(f\"--- Starting Inference for case 10 on {DEVICE} ---\")\n\n    # 2. Load Model\n    model = None\n    try:\n        model = UNet(in_channels=4).to(DEVICE)\n        # Attempt to load the best-performing weights\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n        model.eval()\n        print(f\"Model loaded successfully from {MODEL_SAVE_PATH}\")\n    except Exception as e:\n        print(f\"ðŸ›‘ ERROR: Model weights not found at {MODEL_SAVE_PATH}. Cannot run inference.\")\n        print(\"Defaulting to 'authentic' for all test cases.\")\n        model = None\n    \n    # 3. Prepare Data\n    test_df = create_test_df_robust(TEST_IMAGE_ROOT, CUSTOM_SUB_PATH)\n    \n    # 4. Run Inference\n    if model and not test_df.empty:\n        results_df = run_inference_and_segment(model, test_df, OUTPUT_FILENAME)\n        \n        print(f\"\\nâœ… Created {OUTPUT_FILENAME} with the prediction for case 10.\")\n        print(\"\\n--- Resulting Submission File Content ---\")\n        \n        # Print the contents of the generated CSV file\n        with open(OUTPUT_FILENAME, 'r') as f:\n            print(f.read())\n    else:\n        # If model failed to load, create a default 'authentic' submission for the case\n        submission_df = test_df[['case_id']].assign(annotation='authentic')\n        submission_df.to_csv(OUTPUT_FILENAME, index=False)\n        print(f\"ðŸ›‘ Failed to run model. Generated a default 'authentic' submission for case 10 in {OUTPUT_FILENAME}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:18:27.619355Z","iopub.execute_input":"2025-11-01T04:18:27.620005Z","iopub.status.idle":"2025-11-01T04:18:27.890765Z","shell.execute_reply.started":"2025-11-01T04:18:27.619981Z","shell.execute_reply":"2025-11-01T04:18:27.890083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport time\nimport csv\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore') # Suppress warnings\n\n# --- CONFIGURATION OVERRIDES FOR DEBUGGING ---\n# Setting the threshold extremely low and area filter to 1 to capture ANY prediction\nFIXED_THRESHOLD = 0.05 \nMIN_FORGERY_AREA = 1 \n\n# Paths and other parameters remain the same\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\nTEST_IMAGE_ROOT = \"/kaggle/working/test_images\"\nCUSTOM_SUB_PATH = \"/tmp/custom_sample_submission_10.csv\"\nOUTPUT_FILENAME_DEBUG = \"submission_10_DEBUG.csv\"\n\n# --- UTILITY & MODEL DEFINITIONS (Required to run, must be available in scope) ---\n# (rle_encode, compute_ela, UNet, create_test_df_robust, run_inference_and_segment functions \n# are assumed to be defined in previous notebook cells.)\n\n# --- MAIN EXECUTION BLOCK (Debug Run) ---\nif __name__ == \"__main__\":\n    \n    # SETUP: Create Custom Test Submission for case_id '10' (re-creates the file)\n    custom_sub_df = pd.DataFrame({'case_id': ['10'], 'annotation': ['authentic']})\n    custom_sub_df.to_csv(CUSTOM_SUB_PATH, index=False)\n    \n    print(f\"--- Starting DEBUG Inference for case 10 (Threshold={FIXED_THRESHOLD}, MinArea={MIN_FORGERY_AREA}) ---\")\n\n    # 1. Load Model\n    model = None\n    try:\n        model = UNet(in_channels=4).to(DEVICE)\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n        model.eval()\n    except Exception:\n        print(f\"ðŸ›‘ ERROR: Model weights not found. Cannot run debugging inference.\")\n        exit()\n    \n    # 2. Prepare Data\n    test_df = create_test_df_robust(TEST_IMAGE_ROOT, CUSTOM_SUB_PATH)\n    \n    # 3. Run Inference with Debug Settings\n    if model and not test_df.empty:\n        # Note: We use the existing run_inference_and_segment function which utilizes the global FIXED_THRESHOLD and MIN_FORGERY_AREA\n        results_df = run_inference_and_segment(model, test_df, OUTPUT_FILENAME_DEBUG)\n        \n        print(f\"\\nâœ… DEBUG output generated in {OUTPUT_FILENAME_DEBUG}.\")\n        print(\"\\n--- DEBUG Submission File Content ---\")\n        \n        # Print the contents of the generated CSV file\n        with open(OUTPUT_FILENAME_DEBUG, 'r') as f:\n            print(f.read())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:21:49.494252Z","iopub.execute_input":"2025-11-01T04:21:49.494866Z","iopub.status.idle":"2025-11-01T04:21:49.595504Z","shell.execute_reply.started":"2025-11-01T04:21:49.494845Z","shell.execute_reply":"2025-11-01T04:21:49.594732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2025-11-01T04:02:12.258112Z","iopub.execute_input":"2025-11-01T04:02:12.258378Z","iopub.status.idle":"2025-11-01T04:02:12.400406Z","shell.execute_reply.started":"2025-11-01T04:02:12.25836Z","shell.execute_reply":"2025-11-01T04:02:12.399651Z"},"id":"TIckAouP3Blt","outputId":"b03ece0f-6c0e-459d-b0c7-d7ef5d78ca47","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_and_print_rle(submission_df):\n    \"\"\"\n    Validates RLE output structure and prints debugging info.\n    Checks for: 1. Authentic/RLE count. 2. Even number of RLE elements.\n    \"\"\"\n    print(\"\\n--- RLE Output Validation Check ---\")\n\n    # Analyze the annotations\n    authentic_count = submission_df['annotation'].apply(lambda x: x == 'authentic').sum()\n    rle_rows = submission_df[submission_df['annotation'] != 'authentic']\n\n    print(f\"Total Submissions: {len(submission_df)}\")\n    print(f\"Authentic (No Forgery) Count: {authentic_count}\")\n    print(f\"RLE Annotated (Forged) Count: {len(rle_rows)}\")\n\n    # CRITICAL CHECK: RLE strings must always have an even number of elements (start, length, start, length...)\n    rle_check = rle_rows['annotation'].apply(lambda x: len(x.split(' ')) % 2 == 0)\n\n    if rle_check.all():\n        print(f\"âœ… RLE Structure: All {len(rle_rows)} RLE strings contain an even number of elements.\")\n    else:\n        # Prints a warning if any RLE string has an odd number of elements (a common error)\n        bad_rle_count = len(rle_rows) - rle_check.sum()\n        print(f\"ðŸ›‘ RLE ERROR: Found {bad_rle_count} RLE strings with an odd number of elements (Invalid pairing).\")","metadata":{"execution":{"iopub.status.busy":"2025-11-01T03:49:08.314063Z","iopub.execute_input":"2025-11-01T03:49:08.314416Z","iopub.status.idle":"2025-11-01T03:49:08.320429Z","shell.execute_reply.started":"2025-11-01T03:49:08.31439Z","shell.execute_reply":"2025-11-01T03:49:08.319642Z"},"id":"RTP7ZggN3Blt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.read_csv(\"submission.csv\")\nvalidate_and_print_rle(submission_df)","metadata":{"execution":{"iopub.status.busy":"2025-11-01T03:49:14.746246Z","iopub.execute_input":"2025-11-01T03:49:14.746546Z","iopub.status.idle":"2025-11-01T03:49:14.755929Z","shell.execute_reply.started":"2025-11-01T03:49:14.746526Z","shell.execute_reply":"2025-11-01T03:49:14.755255Z"},"id":"Ly35WP3X3Blt","outputId":"c13d8b6a-fb62-4c44-d91e-ad18f90b4f72","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm # Use tqdm.auto for notebook compatibility\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport warnings\nfrom warnings import filterwarnings\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.optim.lr_scheduler\")\nfilterwarnings('ignore')\n\n# --- CONFIGURATION (FINAL STABLE FIXES) ---\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\n# FINAL FIX 1: Set max epochs to 10\nEPOCHS = 10 \n# FINAL FIX 2: Reduced LR for stable convergence\nLEARNING_RATE = 1e-5 \n\n# --- PATHS (As per original notebook) ---\nTRAIN_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\"\nMASK_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\"\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\n\n# --- UTILITY FUNCTIONS (Unchanged) ---\ndef compute_ela(img_path, quality=95, scale=10):\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    if img is None or img.size == 0: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_ela_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE),\n                      interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\n\n# --- LOSS FUNCTIONS (FIXED) ---\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1.0):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n    def forward(self, pred, target):\n        pred = pred.contiguous().view(-1)\n        target = target.contiguous().view(-1)\n        intersection = (pred * target).sum()\n        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n        return 1 - dice\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, inputs, targets):\n        inputs = inputs.contiguous().view(-1)\n        targets = targets.contiguous().view(-1)\n        BCE = F.binary_cross_entropy(inputs, targets, reduction='none')\n        BCE_EXP = torch.exp(-BCE)\n        Focal = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n        return Focal.mean()\n\nclass HybridFocalLoss(nn.Module):\n    # FINAL FIX 3: Balance loss weights for stable convergence\n    def __init__(self, dice_weight=0.5): \n        super(HybridFocalLoss, self).__init__()\n        self.dice_loss = DiceLoss() \n        self.focal_loss = FocalLoss()\n        self.dice_weight = dice_weight\n\n    def forward(self, pred, target):\n        dice = self.dice_loss(pred, target)\n        focal = self.focal_loss(pred, target)\n        return self.dice_weight * dice + (1 - self.dice_weight) * focal\n\n\n# U-Net architecture (Unchanged)\nclass UNet(nn.Module):\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\nclass ForgeryDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = row['img_path']\n        mask_path = row['mask_path']\n\n        # --- Load Image (Robust) ---\n        rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        if rgb_image is None or rgb_image.size == 0:\n            try:\n                img_data = np.load(img_path)\n                if img_data.ndim == 3: rgb_image = img_data\n                elif img_data.ndim == 2: rgb_image = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)\n            except Exception as e:\n                raise RuntimeError(f\"Failed to load image from {img_path}: {e}\")\n\n        # --- Load Mask (Robust) ---\n        try:\n            mask = np.load(mask_path)\n            if mask.ndim > 2: mask = mask[:, :, 0]\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load mask from {mask_path}: {e}\")\n\n        ela_feature_2d = compute_ela(img_path)\n\n        # Resize all features\n        rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n        ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n        mask_resized = cv2.resize(mask.astype(np.uint8), (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_NEAREST)\n\n        # Stack RGB (3) and ELA (1)\n        ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n        stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n\n        # Convert to PyTorch tensors and normalize\n        image = torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n        mask = torch.tensor(mask_resized / 255.0, dtype=torch.float32).unsqueeze(0)\n\n        return image, mask\n\ndef train_model(model, train_loader, val_loader, epochs=EPOCHS, save_path=MODEL_SAVE_PATH):\n    criterion = HybridFocalLoss() \n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2) \n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\n    best_val_loss = float('inf')\n    \n    # --- EARLY STOPPING PARAMETERS ---\n    PATIENCE = 5                        # Stop if no improvement after 5 epochs\n    epochs_no_improve = 0\n    MIN_DELTA = 1e-5                    # Minimum improvement required to reset patience\n    # ---------------------------------\n\n    model.to(DEVICE)\n    print(f\"Starting stabilized training on {DEVICE} for max {epochs} epochs with LR={LEARNING_RATE}...\")\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss_sum = 0\n        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            train_loss_sum += loss.item()\n\n        avg_train_loss = train_loss_sum / len(train_loader)\n\n        # Validation Phase\n        model.eval()\n        val_loss_sum = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss_sum += loss.item()\n\n        avg_val_loss = val_loss_sum / len(val_loader)\n\n        # Scheduler Step\n        scheduler.step(avg_val_loss)\n\n        current_lr = optimizer.param_groups[0]['lr']\n\n        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Current LR: {current_lr:.6f}\")\n\n        # --- MODEL CHECKPOINTING & EARLY STOPPING LOGIC ---\n        if avg_val_loss < best_val_loss - MIN_DELTA:\n            # 1. New best loss found: Reset counter and save model\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), save_path)\n            print(f\"Model saved successfully to {save_path}. (New best Val Loss: {best_val_loss:.4f})\\n\")\n        else:\n            # 2. No significant improvement: Increment counter\n            epochs_no_improve += 1\n            print(f\"Validation loss did not significantly improve. Patience left: {PATIENCE - epochs_no_improve}\\n\")\n\n        # 3. Terminate if patience runs out\n        if epochs_no_improve >= PATIENCE:\n            print(f\"ðŸ›‘ EARLY STOPPING triggered after {epoch+1} epochs (patience={PATIENCE}). Stopping training.\")\n            break\n            \n    print(\"Training loop finished.\")\n\n\n# --- FUNCTIONS FOR THRESHOLD OPTIMIZATION (Unchanged) ---\ndef calculate_dice_coefficient(pred, target, smooth=1.0):\n    pred = pred.contiguous().view(-1)\n    target = target.contiguous().view(-1)\n    intersection = (pred * target).sum()\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    return dice.item()\n\ndef find_optimal_threshold(model, val_loader, device, threshold_range=np.arange(0.05, 0.75, 0.05)):\n    model.to(device)\n    model.eval()\n    \n    best_dice = -1.0\n    best_threshold = 0.50\n    \n    print(\"\\n--- Finding Optimal Threshold on Validation Set (Expanded Range) ---\")\n    \n    for threshold in threshold_range:\n        total_dice = 0.0\n        \n        with torch.no_grad():\n            for inputs, targets in tqdm(val_loader, desc=f\"Evaluating T={threshold:.2f}\"):\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                predicted_mask = (outputs > threshold).float()\n                batch_dice = calculate_dice_coefficient(predicted_mask, targets)\n                total_dice += batch_dice\n\n        avg_dice = total_dice / len(val_loader)\n        \n        if avg_dice > best_dice:\n            best_dice = avg_dice\n            best_threshold = threshold\n            \n        # print(f\"Threshold: {threshold:.2f} | Avg. Val Dice: {avg_dice:.4f}\") # Omitted for cleaner log\n\n    print(f\"\\nâœ… Optimal Threshold found: {best_threshold:.2f} with Dice: {best_dice:.4f}\")\n    return best_threshold\n\n\n# --- MAIN EXECUTION BLOCK (Unchanged) ---\nif __name__ == '__main__':\n\n    print(\"Preparing training data paths...\\n\")\n\n    data_list = []\n    for root, _, files in os.walk(TRAIN_ROOT):\n        for f in files:\n            valid_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')\n            if f.lower().endswith(valid_extensions):\n                if 'forged' in root.lower():\n                    case_id = os.path.splitext(f)[0]\n                    img_path = os.path.join(root, f)\n                    mask_path = os.path.join(MASK_ROOT, f\"{case_id}.npy\")\n                    data_list.append({\n                        'case_id': case_id,\n                        'img_path': img_path,\n                        'mask_path': mask_path\n                    })\n\n    if not data_list:\n        full_df = pd.DataFrame(columns=['case_id', 'img_path', 'mask_path'])\n    else:\n        full_df = pd.DataFrame(data_list)\n\n    if not full_df.empty:\n        full_df['mask_exists'] = full_df['mask_path'].apply(os.path.exists)\n        full_df = full_df[full_df['mask_exists']].drop(columns=['mask_exists']).reset_index(drop=True)\n\n    if full_df.empty:\n        print(\"ðŸ›‘ FATAL ERROR: No valid image/mask pairs found. Cannot train.\")\n    else:\n        print(f\"âœ… Found {len(full_df)} valid forged samples for stabilized training.\")\n\n        # Split data\n        train_df, val_df = train_test_split(full_df, test_size=0.1, random_state=42)\n\n        # Create DataLoaders\n        train_dataset = ForgeryDataset(train_df)\n        val_dataset = ForgeryDataset(val_df)\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        # Instantiate model\n        model = UNet(in_channels=4)\n\n        # START STABILIZED TRAINING\n        train_model(model, train_loader, val_loader)\n\n        print(\"\\nâœ… STABILIZED TRAINING COMPLETE.\")\n\n        # --- HYPERPARAMETER TUNING: FIND OPTIMAL THRESHOLD ---\n        try:\n            model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n            optimal_threshold = find_optimal_threshold(model, val_loader, DEVICE)\n            print(f\"\\nðŸ“¢ ACTION REQUIRED: Please use {optimal_threshold:.2f} as the FIXED_THRESHOLD in your final inference code.\")\n            \n        except FileNotFoundError:\n            print(f\"ðŸ›‘ ERROR: Trained model not found at {MODEL_SAVE_PATH}. Cannot perform threshold tuning.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T05:06:46.99806Z","iopub.execute_input":"2025-11-01T05:06:46.998654Z","iopub.status.idle":"2025-11-01T05:58:20.32644Z","shell.execute_reply.started":"2025-11-01T05:06:46.998626Z","shell.execute_reply":"2025-11-01T05:58:20.325649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## inference","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport time\nimport csv\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore') # Suppress warnings\n\n# --- CONFIGURATION & PATHS (Using final, stable settings) ---\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\n\n# CRITICAL: This value MUST be updated after the 15-epoch training run is complete.\n# Use the result from the 'Optimal Threshold found' message.\nFIXED_THRESHOLD = 0.45 # <<<--- UPDATE THIS AFTER TRAINING IS COMPLETE\n\nMIN_FORGERY_AREA = 32 # Retaining robust post-processing filter\n\n# Path to the actual Kaggle test set folder (for submission)\nTEST_IMAGE_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\nSAMPLE_SUBMISSION_FILE = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\nOUTPUT_FILENAME = \"submission.csv\"\n\n# --- UTILITY FUNCTIONS (Must be in scope) ---\ndef compute_ela(img_path, quality=95, scale=10):\n    # ... (function body as defined in original Cell 12) ...\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n    if img is None or img.size == 0: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\ndef create_test_df_robust(test_image_root, sample_submission_path):\n    master_df = pd.read_csv(sample_submission_path)\n    master_df['case_id'] = master_df['case_id'].astype(str)\n    present_files = {}\n    if os.path.exists(test_image_root):\n        for root, _, files in os.walk(test_image_root):\n            for f in files:\n                case_id = os.path.splitext(f)[0]\n                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')):\n                    present_files[case_id] = os.path.join(root, f)\n    master_df['img_path'] = master_df['case_id'].map(present_files)\n    master_df['img_path'] = master_df['img_path'].fillna('MISSING_FILE')\n    return master_df[['case_id', 'img_path']]\n\ndef rle_encode(mask):\n    if mask.sum() == 0: return \"authentic\"\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ', '.join(str(x) for x in runs)\n\n# --- U-NET ARCHITECTURE (Must be in scope) ---\nclass UNet(nn.Module):\n    # ... (UNet definition body from Cell 12) ...\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\n# --- INFERENCE FUNCTION WITH POST-PROCESSING ---\ndef run_inference_and_segment(unet_model, test_df):\n    results = []\n    unet_model.eval()\n    images_to_process = []\n    case_info = []\n\n    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n        case = row['case_id']\n        img_path = row['img_path']\n\n        if img_path == 'MISSING_FILE' or img_path == 'NOT_FOUND':\n            results.append({'case_id': case, 'annotation': 'authentic'})\n            continue\n\n        try:\n            rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            if rgb_image is None or rgb_image.size == 0:\n                img_data = np.load(img_path)\n                if img_data.ndim == 3: rgb_image = img_data\n                elif img_data.ndim == 2: rgb_image = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)\n\n            if rgb_image is None or rgb_image.size == 0: raise ValueError(f\"Invalid image data for {case}\")\n\n            original_shape = rgb_image.shape[:2]\n            ela_feature_2d = compute_ela(img_path)\n\n            rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n            stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n\n            images_to_process.append(\n                torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n            )\n            case_info.append((case, original_shape))\n\n            if len(images_to_process) == BATCH_SIZE or index == len(test_df) - 1:\n                if images_to_process:\n                    batch_inputs = torch.stack(images_to_process).to(DEVICE)\n\n                    with torch.no_grad():\n                        outputs = unet_model(batch_inputs).detach().cpu().numpy()\n\n                    for i, output in enumerate(outputs):\n                        case_id_out, original_shape_out = case_info[i]\n                        output_prob = output.squeeze()\n                        \n                        # Apply the newly found optimal FIXED_THRESHOLD\n                        final_mask_resized = (output_prob > FIXED_THRESHOLD).astype(np.uint8)\n\n                        # Minimum Area Filtering\n                        clean_mask_resized = np.zeros_like(final_mask_resized)\n                        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n                            final_mask_resized, 4, cv2.CV_32S\n                        )\n                        for label in range(1, num_labels):\n                            area = stats[label, cv2.CC_STAT_AREA]\n                            if area >= MIN_FORGERY_AREA:\n                                clean_mask_resized[labels == label] = 1\n\n                        # Resize back to original dimensions\n                        final_mask = cv2.resize(\n                            clean_mask_resized,\n                            (original_shape_out[1], original_shape_out[0]),\n                            interpolation=cv2.INTER_NEAREST\n                        )\n\n                        rle_annotation = rle_encode(final_mask)\n                        results.append({'case_id': case_id_out, 'annotation': rle_annotation})\n\n                    images_to_process = []\n                    case_info = []\n\n        except Exception as e:\n            print(f\"Error processing case {case}: {e}. Defaulting to authentic.\")\n            results.append({'case_id': case, 'annotation': 'authentic'})\n    \n    return pd.DataFrame(results)\n\n# --- MAIN EXECUTION BLOCK ---\nif __name__ == \"__main__\":\n\n    print(f\"--- Starting Final Inference on {DEVICE} with Threshold={FIXED_THRESHOLD} ---\")\n\n    # 1. Load Model (The new, stable model)\n    model = None\n    try:\n        model = UNet(in_channels=4).to(DEVICE)\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n        model.eval()\n        print(f\"Model loaded successfully from {MODEL_SAVE_PATH}\")\n    except Exception as e:\n        print(f\"ðŸ›‘ ERROR: Model weights not found at {MODEL_SAVE_PATH}. Cannot run inference.\")\n        print(\"Please wait for the training (Cell 12) to fully complete.\")\n        model = None\n\n    # 2. Prepare Data (Targets the full Kaggle test set)\n    test_df = create_test_df_robust(TEST_IMAGE_ROOT, SAMPLE_SUBMISSION_FILE)\n\n    # 3. Run Inference\n    if model:\n        results_df = run_inference_and_segment(model, test_df)\n    else:\n        results_df = test_df[['case_id']].assign(annotation='authentic')\n\n    # 4. Finalize Submission DF and write CSV\n    submission_df = test_df[['case_id']].copy().merge(results_df, on='case_id', how='left')\n    submission_df['annotation'] = submission_df['annotation'].fillna('authentic')\n    submission_df = submission_df[['case_id', 'annotation']].sort_values('case_id').reset_index(drop=True)\n\n    with open(OUTPUT_FILENAME, \"w\", newline='') as f:\n        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(['case_id', 'annotation'])\n        for _, row in submission_df.iterrows():\n            case_id = str(row['case_id'])\n            annotation = row['annotation']\n            if annotation.lower() == 'authentic':\n                 writer.writerow([case_id, annotation])\n            else:\n                 # Format the RLE string with brackets\n                 full_rle_string = f\"[{annotation}]\"\n                 writer.writerow([case_id, full_rle_string])\n\n    print(f\"\\nâœ… Created final submission file: {OUTPUT_FILENAME}\")\n    print(\"\\n--- Next Step ---\")\n    print(f\"Once training gives a new optimal threshold (e.g., 0.55), update the 'FIXED_THRESHOLD' variable and rerun this cell.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T06:02:28.464055Z","iopub.execute_input":"2025-11-01T06:02:28.464639Z","iopub.status.idle":"2025-11-01T06:02:28.617798Z","shell.execute_reply.started":"2025-11-01T06:02:28.464618Z","shell.execute_reply":"2025-11-01T06:02:28.617102Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## experiment test ","metadata":{}},{"cell_type":"code","source":"!cat submission.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T06:03:56.088713Z","iopub.execute_input":"2025-11-01T06:03:56.088992Z","iopub.status.idle":"2025-11-01T06:03:56.233592Z","shell.execute_reply.started":"2025-11-01T06:03:56.088972Z","shell.execute_reply":"2025-11-01T06:03:56.232664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport torch\nimport pandas as pd\nimport csv\nimport shutil\nimport cv2\nfrom tqdm.auto import tqdm\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- CONFIGURATION (FINAL EXPERIMENT SETTINGS) ---\n# NOTE: These settings MUST override the defaults for this specific test\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\nTEST_WORKING_ROOT = \"/kaggle/working/validation_test_final\"\nTARGET_SIZE = 256\nBATCH_SIZE = 8\n\n# CRITICAL FIX 1: Set threshold to the absolute minimum to capture any signal\nFIXED_THRESHOLD = 0.05 \n# CRITICAL FIX 2: Set minimum area to 1 to ensure NO pixels are filtered out\nMIN_FORGERY_AREA = 1 \n\n\nFIXED_THRESHOLD = 0.45\nMIN_FORGERY_AREA = 32\n\n# --- PATHS ---\nFORGED_PATH = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/forged/10.png\"\nAUTHENTIC_PATH = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/authentic/10.png\"\n\n\n# --- UTILITY & MODEL DEFINITIONS (Must be defined in the notebook) ---\n# Re-define necessary functions/classes to ensure they are available in this cell's scope.\n\ndef compute_ela(img_path, quality=95, scale=10):\n    img = cv2.imread(img_path)\n    # [Simplified loading logic for brevity, assuming full logic is in notebook]\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    if img is None or img.size == 0: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_ela_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\ndef rle_encode(mask):\n    \"\"\"Encodes a mask. Returns 'authentic' if empty.\"\"\"\n    if mask.sum() == 0: return \"authentic\"\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    # Final fix for submission: return space-separated string without external brackets\n    return ' '.join(str(x) for x in runs) \n\ndef create_test_df_robust(test_image_root, sample_submission_path):\n    master_df = pd.read_csv(sample_submission_path)\n    master_df['case_id'] = master_df['case_id'].astype(str)\n    present_files = {}\n    if os.path.exists(test_image_root):\n        for root, _, files in os.walk(test_image_root):\n            for f in files:\n                case_id = os.path.splitext(f)[0]\n                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')):\n                    present_files[case_id] = os.path.join(root, f)\n    master_df['img_path'] = master_df['case_id'].map(present_files)\n    master_df['img_path'] = master_df['img_path'].fillna('MISSING_FILE')\n    return master_df[['case_id', 'img_path']]\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        # Ensure block logic is correctly defined and indented\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\ndef run_inference_and_segment(unet_model, test_df):\n    \"\"\"Executes inference using the CURRENT GLOBAL FIXED_THRESHOLD and MIN_FORGERY_AREA.\"\"\"\n    # Note: Global variables FIXED_THRESHOLD, MIN_FORGERY_AREA, TARGET_SIZE, DEVICE are used directly.\n    \n    results = []\n    unet_model.eval()\n    images_to_process = []\n    case_info = []\n\n    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n        case = row['case_id']\n        img_path = row['img_path']\n        # ... [Image loading and ELA calculation remains the same] ...\n        \n        try:\n            rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            if rgb_image is None or rgb_image.size == 0:\n                img_data = np.load(img_path)\n                if img_data.ndim == 3: rgb_image = img_data\n                elif img_data.ndim == 2: rgb_image = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)\n            if rgb_image is None or rgb_image.size == 0: raise ValueError(f\"Invalid image data for {case}\")\n\n            original_shape = rgb_image.shape[:2]\n            ela_feature_2d = compute_ela(img_path)\n            \n            # Preprocessing\n            rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n            stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n\n            images_to_process.append(\n                torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n            )\n            case_info.append((case, original_shape))\n\n            if len(images_to_process) == BATCH_SIZE or index == len(test_df) - 1:\n                if images_to_process:\n                    batch_inputs = torch.stack(images_to_process).to(DEVICE)\n\n                    with torch.no_grad():\n                        outputs = unet_model(batch_inputs).detach().cpu().numpy()\n\n                    for i, output in enumerate(outputs):\n                        case_id_out, original_shape_out = case_info[i]\n                        output_prob = output.squeeze()\n                        \n                        final_mask_resized = (output_prob > FIXED_THRESHOLD).astype(np.uint8)\n\n                        # --- CRITICAL AREA FILTERING (Uses MIN_FORGERY_AREA) ---\n                        clean_mask_resized = np.zeros_like(final_mask_resized)\n                        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n                            final_mask_resized, 4, cv2.CV_32S\n                        )\n                        for label in range(1, num_labels):\n                            area = stats[label, cv2.CC_STAT_AREA]\n                            if area >= MIN_FORGERY_AREA: \n                                clean_mask_resized[labels == label] = 1\n\n                        final_mask = cv2.resize(\n                            clean_mask_resized,\n                            (original_shape_out[1], original_shape_out[0]),\n                            interpolation=cv2.INTER_NEAREST\n                        )\n                        # --- END CRITICAL AREA FILTERING ---\n\n                        rle_annotation = rle_encode(final_mask)\n                        results.append({'case_id': case_id_out, 'annotation': rle_annotation})\n\n                    images_to_process = []\n                    case_info = []\n\n        except Exception as e:\n            # print(f\"Error processing case {case}: {e}. Defaulting to authentic.\") # Suppressed for cleaner log\n            results.append({'case_id': case, 'annotation': 'authentic'})\n    \n    return pd.DataFrame(results)\n\n\n# --- MAIN EXPERIMENT EXECUTION ---\nif __name__ == \"__main__\":\n    \n    # 1. Setup Environment (Clean and copy files)\n    if os.path.exists(TEST_WORKING_ROOT):\n        shutil.rmtree(TEST_WORKING_ROOT)\n    os.makedirs(TEST_WORKING_ROOT)\n\n    shutil.copy(FORGED_PATH, os.path.join(TEST_WORKING_ROOT, \"F.png\"))\n    shutil.copy(AUTHENTIC_PATH, os.path.join(TEST_WORKING_ROOT, \"A.png\"))\n\n    # Create submission file mapping F -> F.png and A -> A.png\n    experiment_data = pd.DataFrame({\n        'case_id': ['F', 'A'],\n        'annotation': ['authentic', 'authentic']\n    })\n    EXPERIMENT_SUB_PATH = os.path.join(TEST_WORKING_ROOT, \"experiment_sub.csv\")\n    experiment_data.to_csv(EXPERIMENT_SUB_PATH, index=False)\n\n    print(f\"--- Starting FINAL Validation Experiment ---\")\n    print(f\"Testing: Threshold={FIXED_THRESHOLD} and Min Area={MIN_FORGERY_AREA} (Disabled Filter)\")\n\n    # 2. Load Model\n    try:\n        model = UNet(in_channels=4).to(DEVICE)\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n        model.eval()\n    except Exception as e:\n        print(f\"ðŸ›‘ ERROR: Model not loaded. Cannot run validation. Error: {e}\")\n        exit()\n\n    # 3. Run Inference\n    test_df = create_test_df_robust(TEST_WORKING_ROOT, EXPERIMENT_SUB_PATH)\n    results_df = run_inference_and_segment(model, test_df)\n    results_df = results_df.set_index('case_id')\n\n    # 4. Print Validation Results\n    print(\"\\n--- EXPERIMENT RESULTS ---\")\n    \n    forged_result = results_df.loc['F', 'annotation']\n    authentic_result = results_df.loc['A', 'annotation']\n    \n    print(f\"FORGED Case (F) Prediction: {forged_result}\")\n    print(f\"AUTHENTIC Case (A) Prediction: {authentic_result}\")\n\n    print(\"\\n--- LOGIC CHECK ---\")\n    \n    # Check 1: Positive Control (Forged)\n    if forged_result.lower() != 'authentic':\n        print(\"âœ… FORGED CHECK PASSED: Model now detects the known forgery (output RLE).\")\n    else:\n        print(\"ðŸ›‘ FORGED CHECK FAILED: Model still predicts 'authentic' even with disabled area filter.\")\n\n    # Check 2: Negative Control (Authentic)\n    if authentic_result.lower() == 'authentic':\n        print(\"âœ… AUTHENTIC CHECK PASSED: Model correctly ignores the authentic file.\")\n    else:\n        print(\"ðŸ›‘ AUTHENTIC CHECK FAILED: Model predicted forgery (RLE) for a known authentic file.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T06:21:40.680142Z","iopub.execute_input":"2025-11-01T06:21:40.680417Z","iopub.status.idle":"2025-11-01T06:21:40.840927Z","shell.execute_reply.started":"2025-11-01T06:21:40.680397Z","shell.execute_reply":"2025-11-01T06:21:40.840073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport torch\nimport cv2\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- CONFIGURATION (Uses Global Settings) ---\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\nTRAIN_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\"\nTARGET_SIZE = 256\nBATCH_SIZE = 8\n\n# Suspicion threshold: Files with max confidence below this are flagged.\nSUSPICION_THRESHOLD = 0.40 \nMIN_FORGERY_AREA = 1 # Set to 1 to check raw model confidence, disabling filtering\n\n# --- UTILITIES (Assumed to be defined in environment) ---\n# UNet, compute_ela, rle_encode are assumed to be accessible globals\n\n# --- MAIN EXECUTION: Integrated Confidence Check ---\nif __name__ == '__main__':\n    \n    # 1. Compile list of all 2751 forged files (Same logic as before)\n    forged_files = []\n    for root, _, files in os.walk(os.path.join(TRAIN_ROOT, 'forged')):\n        for f in files:\n            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n                forged_files.append({\n                    'case_id': os.path.splitext(f)[0],\n                    'img_path': os.path.join(root, f)\n                })\n    forged_df = pd.DataFrame(forged_files)\n\n    if forged_df.empty:\n        print(\"ðŸ›‘ No forged files found for checking.\")\n        exit()\n        \n    # 2. Load the Best Model\n    try:\n        model = UNet(in_channels=4).to(DEVICE)\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n        model.eval()\n        print(f\"Loaded model. Checking {len(forged_df)} forged files for confidence...\")\n    except Exception as e:\n        print(f\"ðŸ›‘ ERROR: Model not loaded. Cannot run confidence check. Error: {e}\")\n        exit()\n\n    # 3. RUN CONFIDENCE CHECK (MANUAL LOOP)\n    low_confidence_cases = []\n    \n    # Process the files one by one (to simplify input/output handling)\n    for index, row in tqdm(forged_df.iterrows(), total=len(forged_df), desc=\"Checking Confidence\"):\n        img_path = row['img_path']\n        case_id = row['case_id']\n        \n        try:\n            # --- Input Preparation ---\n            rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            ela_feature_2d = compute_ela(img_path)\n            rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n            stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n            \n            input_tensor = torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n            \n            # --- Prediction ---\n            with torch.no_grad():\n                output_prob = model(input_tensor).detach().cpu().numpy().squeeze()\n            \n            max_confidence = np.max(output_prob)\n            \n            # --- Outlier Flagging ---\n            if max_confidence < SUSPICION_THRESHOLD:\n                low_confidence_cases.append({\n                    'case_id': case_id,\n                    'max_confidence': max_confidence,\n                    'status': 'Low Confidence'\n                })\n            \n        except Exception as e:\n            low_confidence_cases.append({'case_id': case_id, 'max_confidence': -1.0, 'status': f\"Error: {e}\"})\n\n    # 4. Display Results\n    outlier_df = pd.DataFrame(low_confidence_cases).sort_values('max_confidence').reset_index(drop=True)\n    \n    print(f\"\\n--- Confidence Check Results (Max Confidence < {SUSPICION_THRESHOLD}) ---\")\n    print(f\"Total files checked: {len(forged_df)}\")\n    print(f\"Total potential outliers found: {len(outlier_df)}\")\n    \n    if not outlier_df.empty:\n        print(\"\\nTop 10 Suspicious Cases (Lowest Confidence, Potential Mislabels):\")\n        print(outlier_df.head(10).to_markdown(index=False))\n    else:\n        print(\"\\nâœ… All forged files had Max Confidence >= 0.40. Dataset appears highly consistent.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T06:33:41.255862Z","iopub.execute_input":"2025-11-01T06:33:41.256467Z","iopub.status.idle":"2025-11-01T06:37:32.377974Z","shell.execute_reply.started":"2025-11-01T06:33:41.256447Z","shell.execute_reply":"2025-11-01T06:37:32.377382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## new trainining","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm # Use tqdm.auto for notebook compatibility\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport warnings\nfrom warnings import filterwarnings\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.optim.lr_scheduler\")\nfilterwarnings('ignore')\n\n# --- CONFIGURATION (FINAL STABLE FIXES) ---\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\n# FINAL FIX 1: Set max epochs to 10\nEPOCHS = 10 \n# FINAL FIX 2: Reduced LR for stable convergence\nLEARNING_RATE = 1e-5 \n\n# --- PATHS (As per original notebook) ---\nTRAIN_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\"\nMASK_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\"\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\n\n# --- UTILITY FUNCTIONS (Unchanged) ---\ndef compute_ela(img_path, quality=95, scale=10):\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    if img is None or img.size == 0: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_ela_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE),\n                      interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\n\n# --- LOSS FUNCTIONS (FIXED) ---\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1.0):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n    def forward(self, pred, target):\n        pred = pred.contiguous().view(-1)\n        target = target.contiguous().view(-1)\n        intersection = (pred * target).sum()\n        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n        return 1 - dice\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, inputs, targets):\n        inputs = inputs.contiguous().view(-1)\n        targets = targets.contiguous().view(-1)\n        BCE = F.binary_cross_entropy(inputs, targets, reduction='none')\n        BCE_EXP = torch.exp(-BCE)\n        Focal = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n        return Focal.mean()\n\nclass HybridFocalLoss(nn.Module):\n    # FINAL FIX 3: Balance loss weights for stable convergence\n    def __init__(self, dice_weight=0.5): \n        super(HybridFocalLoss, self).__init__()\n        self.dice_loss = DiceLoss() \n        self.focal_loss = FocalLoss()\n        self.dice_weight = dice_weight\n\n    def forward(self, pred, target):\n        dice = self.dice_loss(pred, target)\n        focal = self.focal_loss(pred, target)\n        return self.dice_weight * dice + (1 - self.dice_weight) * focal\n\n\n# U-Net architecture (Unchanged)\nclass UNet(nn.Module):\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\nclass ForgeryDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = row['img_path']\n        mask_path = row['mask_path']\n\n        # --- Load Image (Robust) ---\n        rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        if rgb_image is None or rgb_image.size == 0:\n            try:\n                img_data = np.load(img_path)\n                if img_data.ndim == 3: rgb_image = img_data\n                elif img_data.ndim == 2: rgb_image = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)\n            except Exception as e:\n                raise RuntimeError(f\"Failed to load image from {img_path}: {e}\")\n\n        # --- Load Mask (Robust) ---\n        try:\n            mask = np.load(mask_path)\n            if mask.ndim > 2: mask = mask[:, :, 0]\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load mask from {mask_path}: {e}\")\n\n        ela_feature_2d = compute_ela(img_path)\n\n        # Resize all features\n        rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n        ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n        mask_resized = cv2.resize(mask.astype(np.uint8), (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_NEAREST)\n\n        # Stack RGB (3) and ELA (1)\n        ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n        stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n\n        # Convert to PyTorch tensors and normalize\n        image = torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n        mask = torch.tensor(mask_resized / 255.0, dtype=torch.float32).unsqueeze(0)\n\n        return image, mask\n\ndef train_model(model, train_loader, val_loader, epochs=EPOCHS, save_path=MODEL_SAVE_PATH):\n    # Fix 4: Set num_workers=0 to suppress multiprocessing AssertionErrors in notebook environments\n    # train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    # val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n    criterion = HybridFocalLoss() \n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2) \n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\n    best_val_loss = float('inf')\n    \n    # --- EARLY STOPPING PARAMETERS ---\n    PATIENCE = 5                        # Stop if no improvement after 5 epochs\n    epochs_no_improve = 0\n    MIN_DELTA = 1e-5                    # Minimum improvement required to reset patience (Fix for float comparison)\n    # ---------------------------------\n\n    model.to(DEVICE)\n    print(f\"Starting stabilized training on {DEVICE} for max {epochs} epochs with LR={LEARNING_RATE}...\")\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss_sum = 0\n        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            train_loss_sum += loss.item()\n\n        avg_train_loss = train_loss_sum / len(train_loader)\n\n        # Validation Phase\n        model.eval()\n        val_loss_sum = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss_sum += loss.item()\n\n        avg_val_loss = val_loss_sum / len(val_loader)\n\n        # Scheduler Step\n        scheduler.step(avg_val_loss)\n\n        current_lr = optimizer.param_groups[0]['lr']\n\n        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Current LR: {current_lr:.6f}\")\n\n        # --- MODEL CHECKPOINTING & EARLY STOPPING LOGIC ---\n        if avg_val_loss < best_val_loss - MIN_DELTA:\n            # 1. New best loss found: Reset counter and save model\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), save_path)\n            print(f\"Model saved successfully to {save_path}. (New best Val Loss: {best_val_loss:.4f})\\n\")\n        else:\n            # 2. No significant improvement: Increment counter\n            epochs_no_improve += 1\n            print(f\"Validation loss did not significantly improve. Patience left: {PATIENCE - epochs_no_improve}\\n\")\n\n        # 3. Terminate if patience runs out\n        if epochs_no_improve >= PATIENCE:\n            print(f\"ðŸ›‘ EARLY STOPPING triggered after {epoch+1} epochs (patience={PATIENCE}). Stopping training.\")\n            break\n            \n    print(\"Training loop finished.\")\n\n\n# --- FUNCTIONS FOR THRESHOLD OPTIMIZATION (Unchanged) ---\ndef calculate_dice_coefficient(pred, target, smooth=1.0):\n    pred = pred.contiguous().view(-1)\n    target = target.contiguous().view(-1)\n    intersection = (pred * target).sum()\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    return dice.item()\n\ndef find_optimal_threshold(model, val_loader, device, threshold_range=np.arange(0.05, 0.75, 0.05)):\n    model.to(device)\n    model.eval()\n    \n    best_dice = -1.0\n    best_threshold = 0.50\n    \n    print(\"\\n--- Finding Optimal Threshold on Validation Set (Expanded Range) ---\")\n    \n    for threshold in threshold_range:\n        total_dice = 0.0\n        \n        with torch.no_grad():\n            for inputs, targets in tqdm(val_loader, desc=f\"Evaluating T={threshold:.2f}\"):\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                predicted_mask = (outputs > threshold).float()\n                batch_dice = calculate_dice_coefficient(predicted_mask, targets)\n                total_dice += batch_dice\n\n        avg_dice = total_dice / len(val_loader)\n        \n        if avg_dice > best_dice:\n            best_dice = avg_dice\n            best_threshold = threshold\n            \n        # print(f\"Threshold: {threshold:.2f} | Avg. Val Dice: {avg_dice:.4f}\") # Omitted for cleaner log\n\n    print(f\"\\nâœ… Optimal Threshold found: {best_threshold:.2f} with Dice: {best_dice:.4f}\")\n    return best_threshold\n\n\n# --- MAIN EXECUTION BLOCK (Unchanged) ---\nif __name__ == '__main__':\n\n    print(\"Preparing training data paths...\\n\")\n\n    data_list = []\n    # Ensure num_workers=0 is used in DataLoader to prevent multiprocessing errors\n    NUM_WORKERS = 0\n\n    # Recursively walk through the TRAIN_ROOT to find all image files\n    for root, _, files in os.walk(TRAIN_ROOT):\n        for f in files:\n            valid_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')\n            if f.lower().endswith(valid_extensions):\n                if 'forged' in root.lower():\n                    case_id = os.path.splitext(f)[0]\n                    img_path = os.path.join(root, f)\n                    mask_path = os.path.join(MASK_ROOT, f\"{case_id}.npy\")\n                    data_list.append({\n                        'case_id': case_id,\n                        'img_path': img_path,\n                        'mask_path': mask_path\n                    })\n\n    if not data_list:\n        full_df = pd.DataFrame(columns=['case_id', 'img_path', 'mask_path'])\n    else:\n        full_df = pd.DataFrame(data_list)\n\n    if not full_df.empty:\n        full_df['mask_exists'] = full_df['mask_path'].apply(os.path.exists)\n        full_df = full_df[full_df['mask_exists']].drop(columns=['mask_exists']).reset_index(drop=True)\n\n    if full_df.empty:\n        print(\"ðŸ›‘ FATAL ERROR: No valid image/mask pairs found. Cannot train.\")\n    else:\n        print(f\"âœ… Found {len(full_df)} valid forged samples for stabilized training.\")\n\n        # Split data\n        train_df, val_df = train_test_split(full_df, test_size=0.1, random_state=42)\n\n        # Create DataLoaders\n        train_dataset = ForgeryDataset(train_df)\n        val_dataset = ForgeryDataset(val_df)\n        # Use num_workers=0 to prevent multiprocessing AssertionErrors\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n        # Instantiate model\n        model = UNet(in_channels=4)\n\n        # START STABILIZED TRAINING\n        train_model(model, train_loader, val_loader)\n\n        print(\"\\nâœ… STABILIZED TRAINING COMPLETE.\")\n\n        # --- HYPERPARAMETER TUNING: FIND OPTIMAL THRESHOLD ---\n        try:\n            model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n            optimal_threshold = find_optimal_threshold(model, val_loader, DEVICE)\n            print(f\"\\nðŸ“¢ ACTION REQUIRED: Please use {optimal_threshold:.2f} as the FIXED_THRESHOLD in your final inference code.\")\n            \n        except FileNotFoundError:\n            print(f\"ðŸ›‘ ERROR: Trained model not found at {MODEL_SAVE_PATH}. Cannot perform threshold tuning.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T07:47:18.892187Z","iopub.execute_input":"2025-11-01T07:47:18.892487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport time\nimport csv\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore') # Suppress warnings\n\n# --- CONFIGURATION & PATHS (Final Submission Settings) ---\nTARGET_SIZE = 256\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\n\n# CRITICAL: This MUST be manually set to the optimal threshold found\n# in your final training run's optimization step (likely ~0.45).\nFIXED_THRESHOLD = 0.45 \n\n# CRITICAL: Re-enable the robust area filter (no longer 1)\nMIN_FORGERY_AREA = 32\n\n# Paths target the final competition data\nTEST_IMAGE_ROOT = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\nSAMPLE_SUBMISSION_FILE = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\nMODEL_SAVE_PATH = \"/tmp/model_new_scratch.pth\"\nOUTPUT_FILENAME = \"submission.csv\"\n\n# --- UTILITY FUNCTIONS (Must be in scope) ---\n# NOTE: compute_ela, create_test_df_robust, rle_encode, and UNet definitions \n# are assumed to be accessible from previous notebook cells.\n\ndef compute_ela(img_path, quality=95, scale=10):\n    img = cv2.imread(img_path)\n    if img is None or img.size == 0:\n        try:\n            img_data = np.load(img_path)\n            if img_data.ndim == 3: img = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n            elif img_data.ndim == 2: img = cv2.cvtColor(img_data, cv2.COLOR_GRAY2BGR)\n        except Exception: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n    if img is None or img.size == 0: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n    img_resized = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n    temp_path = f\"/tmp/temp_{os.path.basename(img_path)}_{time.time()}.jpg\"\n    try:\n        cv2.imwrite(temp_path, img_resized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n        compressed_img = cv2.imread(temp_path)\n        if compressed_img is None: return np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.float32)\n        error = np.abs(img_resized.astype(np.float32) - compressed_img.astype(np.float32))\n        ela_feature_2d = np.mean(error, axis=2) * scale\n    finally:\n        if os.path.exists(temp_path): os.remove(temp_path)\n    return cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\ndef create_test_df_robust(test_image_root, sample_submission_path):\n    master_df = pd.read_csv(sample_submission_path)\n    master_df['case_id'] = master_df['case_id'].astype(str)\n    present_files = {}\n    if os.path.exists(test_image_root):\n        for root, _, files in os.walk(test_image_root):\n            for f in files:\n                case_id = os.path.splitext(f)[0]\n                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.npy')):\n                    present_files[case_id] = os.path.join(root, f)\n    master_df['img_path'] = master_df['case_id'].map(present_files)\n    master_df['img_path'] = master_df['img_path'].fillna('MISSING_FILE')\n    return master_df[['case_id', 'img_path']]\n\ndef rle_encode(mask):\n    if mask.sum() == 0: return \"authentic\"\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    # NOTE: Returns space-separated string (e.g., '1456 23 1455 25')\n    return ' '.join(str(x) for x in runs)\n\n# U-Net structure must be defined or accessible here\nclass UNet(nn.Module):\n    def __init__(self, in_channels=4, num_classes=1):\n        super().__init__()\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, 1, 1), nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU()\n            )\n        self.enc1 = block(in_channels, 64)\n        self.enc2 = block(64, 128)\n        self.bottleneck = block(128, 256)\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec2 = block(128 + 128, 128)\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec1 = block(64 + 64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, 1)\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = F.max_pool2d(e1, 2)\n        e2 = self.enc2(p1)\n        p2 = F.max_pool2d(e2, 2)\n        b = self.bottleneck(p2)\n        d2 = self.upconv2(b)\n        d2 = torch.cat((d2, e2), dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.upconv1(d2)\n        d1 = torch.cat((d1, e1), dim=1)\n        d1 = self.dec1(d1)\n        return torch.sigmoid(self.final_conv(d1))\n\n\n# --- INFERENCE FUNCTION WITH POST-PROCESSING ---\ndef run_inference_and_segment(unet_model, test_df):\n    global FIXED_THRESHOLD, MIN_FORGERY_AREA, TARGET_SIZE, DEVICE\n    results = []\n    unet_model.eval()\n    images_to_process = []\n    case_info = []\n\n    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n        case = row['case_id']\n        img_path = row['img_path']\n\n        if img_path == 'MISSING_FILE' or img_path == 'NOT_FOUND':\n            results.append({'case_id': case, 'annotation': 'authentic'})\n            continue\n\n        try:\n            # Load Image, ELA, and Preprocess (omitted for brevity, but assumes execution of full loading logic)\n            rgb_image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            if rgb_image is None or rgb_image.size == 0: raise ValueError(f\"Invalid image data for {case}\")\n\n            original_shape = rgb_image.shape[:2]\n            ela_feature_2d = compute_ela(img_path)\n            \n            # Preprocessing\n            rgb_image_resized = cv2.resize(rgb_image, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_resized = cv2.resize(ela_feature_2d, (TARGET_SIZE, TARGET_SIZE))\n            ela_feature_3d = np.expand_dims(ela_feature_resized, axis=-1)\n            stacked_input = np.concatenate([rgb_image_resized, ela_feature_3d], axis=-1)\n\n            images_to_process.append(\n                torch.tensor(stacked_input.transpose(2, 0, 1) / 255.0, dtype=torch.float32)\n            )\n            case_info.append((case, original_shape))\n\n            if len(images_to_process) == BATCH_SIZE or index == len(test_df) - 1:\n                if images_to_process:\n                    batch_inputs = torch.stack(images_to_process).to(DEVICE)\n\n                    with torch.no_grad():\n                        outputs = unet_model(batch_inputs).detach().cpu().numpy()\n\n                    for i, output in enumerate(outputs):\n                        case_id_out, original_shape_out = case_info[i]\n                        output_prob = output.squeeze()\n                        \n                        final_mask_resized = (output_prob > FIXED_THRESHOLD).astype(np.uint8)\n\n                        # --- CRITICAL: Minimum Area Filtering is now active ---\n                        clean_mask_resized = np.zeros_like(final_mask_resized)\n                        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n                            final_mask_resized, 4, cv2.CV_32S\n                        )\n                        for label in range(1, num_labels):\n                            area = stats[label, cv2.CC_STAT_AREA]\n                            if area >= MIN_FORGERY_AREA: # Uses 32\n                                clean_mask_resized[labels == label] = 1\n\n                        final_mask = cv2.resize(\n                            clean_mask_resized,\n                            (original_shape_out[1], original_shape_out[0]),\n                            interpolation=cv2.INTER_NEAREST\n                        )\n                        # --- END CRITICAL ---\n\n                        rle_annotation = rle_encode(final_mask)\n                        results.append({'case_id': case_id_out, 'annotation': rle_annotation})\n\n                    images_to_process = []\n                    case_info = []\n\n        except Exception as e:\n            results.append({'case_id': case, 'annotation': 'authentic'})\n    \n    return pd.DataFrame(results)\n\n# --- MAIN EXECUTION BLOCK ---\nif __name__ == \"__main__\":\n\n    print(f\"--- Starting Final Submission Inference on {DEVICE} with Threshold={FIXED_THRESHOLD} ---\")\n\n    # 1. Load Model (The stable model)\n    model = None\n    try:\n        model = UNet(in_channels=4).to(DEVICE)\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n        model.eval()\n        print(f\"Model loaded successfully from {MODEL_SAVE_PATH}\")\n    except Exception as e:\n        print(f\"ðŸ›‘ ERROR: Model weights not found at {MODEL_SAVE_PATH}. Cannot run inference.\")\n        model = None\n\n    # 2. Prepare Data (Targets the full Kaggle test set)\n    test_df = create_test_df_robust(TEST_IMAGE_ROOT, SAMPLE_SUBMISSION_FILE)\n\n    # 3. Run Inference\n    if model:\n        results_df = run_inference_and_segment(model, test_df)\n    else:\n        results_df = test_df[['case_id']].assign(annotation='authentic')\n\n    # 4. Finalize Submission DF and write CSV (Correct RLE Formatting)\n    submission_df = test_df[['case_id']].copy().merge(results_df, on='case_id', how='left')\n    submission_df['annotation'] = submission_df['annotation'].fillna('authentic')\n    submission_df = submission_df[['case_id', 'annotation']].sort_values('case_id').reset_index(drop=True)\n\n    with open(OUTPUT_FILENAME, \"w\", newline='') as f:\n        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(['case_id', 'annotation'])\n        for _, row in submission_df.iterrows():\n            case_id = str(row['case_id'])\n            annotation = row['annotation']\n            if annotation.lower() == 'authentic':\n                 writer.writerow([case_id, annotation])\n            else:\n                 # Ensure final RLE string is properly bracketed\n                 full_rle_string = f\"[{annotation}]\" \n                 writer.writerow([case_id, full_rle_string])\n\n    print(f\"\\nâœ… Created final submission file: {OUTPUT_FILENAME}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}