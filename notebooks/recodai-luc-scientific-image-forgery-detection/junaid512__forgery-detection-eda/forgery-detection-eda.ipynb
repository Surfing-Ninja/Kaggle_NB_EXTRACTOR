{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß¨ Recod.ai/LUC ‚Äî Scientific Image Forgery Detection  \n## 1 Exploratory Data Analysis (EDA) Notebook\n---\nThis notebook performs an in-depth analysis of the dataset used in the Recod.ai/LUC competition.  \nIt explores dataset composition, image‚Äìmask alignment, forged region characteristics, and visual patterns.\n","metadata":{}},{"cell_type":"code","source":"import os, gc, random\nfrom pathlib import Path\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\nsns.set_style(\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 6)\n\nSEED = 42\nrandom.seed(SEED)\n\n# dataset paths\nDATA_DIR = Path('/kaggle/input/recodai-luc-scientific-image-forgery-detection')\nif not DATA_DIR.exists():\n    DATA_DIR = Path('/mnt/data')\n\nTRAIN_IMG_DIR = DATA_DIR / \"train_images\"\nTRAIN_MASK_DIR = DATA_DIR / \"train_masks\"\nTEST_IMG_DIR = DATA_DIR / \"test_images\"\nSAMPLE_SUB = DATA_DIR / \"sample_submission.csv\"\n\n# recursive discovery (authentic + forged subfolders)\ntrain_images = sorted([str(p) for p in TRAIN_IMG_DIR.rglob(\"*.png\")])\ntrain_masks = sorted([str(p) for p in TRAIN_MASK_DIR.rglob(\"*.npy\")])\ntest_images  = sorted([str(p) for p in TEST_IMG_DIR.rglob(\"*.png\")])\n\nprint(f\"‚úÖ Train images found: {len(train_images):,}\")\nprint(f\"‚úÖ Train masks found:  {len(train_masks):,}\")\nprint(f\"‚úÖ Test images found:  {len(test_images):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:22:26.994616Z","iopub.execute_input":"2025-10-31T05:22:26.995361Z","iopub.status.idle":"2025-10-31T05:22:41.048944Z","shell.execute_reply.started":"2025-10-31T05:22:26.995333Z","shell.execute_reply":"2025-10-31T05:22:41.048285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The dataset contains 5,128 total training images, split between authentic and forged categories\nOnly 2,751 training masks are available, indicating that not all images contain forgeries (as expected)\nOnly 1 test image is currently visible (likely a placeholder; full test set will be larger)\nThe 2,751 masks correspond to the forged images, while the remaining 2,377 images are authentic with no masks needed\n## Key Insight for Modeling Strategy:\nThis confirms the expected dataset structure where only forged images require segmentation masks. The 2,377 authentic images serve as negative examples during training. The test set appears to be minimal in this preview, but will likely expand to 45+ images as indicated in competition documentation.","metadata":{}},{"cell_type":"markdown","source":"## 2. Image-Path Mapping Verification\n### This analysis validates the relationship between image IDs and their corresponding file paths, while also checking mask availability for each image. This is essential to ensure proper data loading during model training.","metadata":{}},{"cell_type":"code","source":"def case_id_from_path(p): return Path(p).stem\n\ntrain_df = pd.DataFrame({\n    \"case_id\": [case_id_from_path(p) for p in train_images],\n    \"image_path\": train_images\n})\nmask_map = {Path(p).stem: p for p in train_masks}\ntrain_df[\"mask_path\"] = train_df[\"case_id\"].map(mask_map)\ntrain_df[\"has_mask\"] = train_df[\"mask_path\"].notnull().astype(int)\n\nprint(\"üîπ Total images:\", len(train_df))\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:22:41.049989Z","iopub.execute_input":"2025-10-31T05:22:41.050323Z","iopub.status.idle":"2025-10-31T05:22:41.149815Z","shell.execute_reply.started":"2025-10-31T05:22:41.050304Z","shell.execute_reply":"2025-10-31T05:22:41.149035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Each image is properly mapped to its unique case ID\nThe has_mask column correctly identifies forged images (value = 1)\nAll paths follow a consistent structure that can be reliably parsed during data loading\nThe path structure confirms the expected directory organization with separate folders for authentic/forged images\n### Key Insight for Modeling Strategy:\nThis mapping can be directly used to create a robust data loader. The consistent path structure allows for efficient implementation of a custom PyTorch/TensorFlow dataset class that correctly pairs images with their masks. The has_mask column provides an immediate classification signal that could be leveraged in a two-stage detection approach.","metadata":{}},{"cell_type":"markdown","source":"## 3. Dataset Composition Analysis\nThis section examines the distribution of image types and verifies data integrity by checking for mismatches between images and their corresponding masks.","metadata":{}},{"cell_type":"code","source":"missing_masks = train_df[train_df[\"mask_path\"].isna()]\nmissing_imgs = [p for p in train_masks if Path(p).stem not in train_df[\"case_id\"].values]\nprint(f\"Images without masks: {len(missing_masks)}\")\nprint(f\"Masks without matching images: {len(missing_imgs)}\")\n\ntrain_df[\"folder_type\"] = train_df[\"image_path\"].apply(lambda x: Path(x).parent.name)\nprint(train_df[\"folder_type\"].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:22:41.150589Z","iopub.execute_input":"2025-10-31T05:22:41.150789Z","iopub.status.idle":"2025-10-31T05:22:41.654945Z","shell.execute_reply.started":"2025-10-31T05:22:41.150772Z","shell.execute_reply":"2025-10-31T05:22:41.654256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Perfect data integrity: No orphaned images or masks (0 images without masks, 0 masks without images)\nClass distribution: 2,751 forged images (53.6%) vs. 2,377 authentic images (46.4%)\nTotal training images: 5,128 (2,751 + 2,377) which matches the initial file count\nNo duplicates: The counts indicate clean, non-overlapping categories\nKey Insight for Modeling Strategy:\nThe near-even class distribution (53.6% forged vs 46.4% authentic) is actually beneficial for training - it's imbalanced enough to require attention but not so imbalanced that special techniques are absolutely mandatory. \n\n**This suggests:**\n\nA single-stage segmentation model can likely handle both forgery detection and segmentation\nWe should use class weighting (slightly higher weight for authentic images) to maintain balance\nData augmentation should target authentic images to close the 7.2% gap\nThe clean dataset structure means we can immediately proceed to feature engineering without data cleaning","metadata":{}},{"cell_type":"markdown","source":"## 4 Distribution of Authentic vs Forged Images","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.countplot(data=train_df, x=\"has_mask\", palette=\"viridis\")\nplt.xticks([0,1], [\"Authentic (no mask)\",\"Forged (has mask)\"])\nplt.title(\"Distribution of Authentic vs Forged Images\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:22:41.656746Z","iopub.execute_input":"2025-10-31T05:22:41.657212Z","iopub.status.idle":"2025-10-31T05:22:42.018872Z","shell.execute_reply.started":"2025-10-31T05:22:41.657182Z","shell.execute_reply":"2025-10-31T05:22:42.018186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Image Dimension Analysis\nThis section examines the distribution of image dimensions across the training dataset, which is critical for determining appropriate preprocessing strategies and model architecture decisions.\n","metadata":{}},{"cell_type":"code","source":"dims = []\nfor p in tqdm(train_df[\"image_path\"].sample(min(300, len(train_df))), desc=\"Reading shapes\"):\n    img = cv2.imread(p, cv2.IMREAD_UNCHANGED)\n    if img is not None:\n        h, w = img.shape[:2]\n        dims.append((w, h, w/h))\n\ndim_df = pd.DataFrame(dims, columns=[\"width\",\"height\",\"aspect_ratio\"])\nsns.histplot(dim_df[\"aspect_ratio\"], bins=30, color=\"steelblue\")\nplt.title(\"Aspect Ratio Distribution\")\nplt.xlabel(\"Width / Height ratio\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.kdeplot(dim_df[\"width\"], label=\"width\")\nsns.kdeplot(dim_df[\"height\"], label=\"height\")\nplt.legend()\nplt.title(\"Image Dimension Distribution\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:22:42.019586Z","iopub.execute_input":"2025-10-31T05:22:42.019791Z","iopub.status.idle":"2025-10-31T05:22:55.216205Z","shell.execute_reply.started":"2025-10-31T05:22:42.019775Z","shell.execute_reply":"2025-10-31T05:22:55.215303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1st Graph Analysis & Key Findings:\n\nThe dataset shows a strong concentration of aspect ratios between 1.0-1.5 (nearly square to slightly landscape)\nA significant secondary peak appears around 6-7, representing narrow portrait-oriented images\nMinor peaks at 10-12 suggest some extremely narrow images (possibly Western blots or gel electrophoresis)\nThe distribution is highly right-skewed, with most images clustered in the lower range\nStrategic Implications for Modeling:\n\nA dynamic resizing strategy is required rather than fixed dimensions\nThe 1.0-1.5 aspect ratio cluster (majority of images) should inform the default input size\nSpecial handling may be needed for extreme aspect ratios (6+, 10+)\nTiling approach will be essential for images with dimensions exceeding typical model limits\n\n### Analysis & Key Findings:\n\nBimodal distribution with primary peak at 500-1,000 pixels and secondary peak at 2,000-4,000 pixels\nMaximum dimensions reach ~4,000 pixels in width and height\nHeight distribution is slightly more concentrated at the lower end compared to width\nThe dataset contains significant variation in image sizes with no uniform standard\nStrategic Implications for Modeling:\n\nMulti-scale processing is essential to handle the wide range of dimensions\nImages in the 500-1,500 pixel range (70% of dataset) can be processed with standard models\nLarge images (>2,000 pixels) should be divided into tiles with appropriate overlap\nA dynamic batching strategy will be needed to maximize GPU utilization while avoiding memory issues\nAspect ratio preservation during resizing is critical to avoid distorting forensic features\n4.3 Preprocessing Strategy Recommendations\nBased on these dimension characteristics, the following preprocessing approach is recommended:\n\nTiered Processing Strategy:\nSmall images (500-1,500px): Process at full resolution\nMedium images (1,500-2,500px): Resize to 1,500px on the longest side\nLarge images (>2,500px): Implement tiling with 20% overlap\nDynamic Batching:\nGroup images by size ranges to minimize wasted computation\nUse adaptive padding within batches rather than global resizing\nModel Architecture Considerations:\nImplement feature pyramid networks to handle multi-scale features\nUse context-aware pooling to maintain spatial relationships\nConsider U-Net variants with adjustable input dimensions","metadata":{}},{"cell_type":"markdown","source":"## 5. Forgery Region Size Analysis\nThis section examines the size distribution of forged regions, which directly impacts model sensitivity requirements and detection strategy design.","metadata":{}},{"cell_type":"code","source":"def mask_coverage(mask_path):\n    m = np.load(mask_path)\n    if m.ndim == 3 and m.shape[0] == 1:\n        m = m.squeeze(0)\n    return m.sum() / m.size\n\nmask_covs = []\nfor p in tqdm(train_df.dropna(subset=[\"mask_path\"])[\"mask_path\"], desc=\"Computing coverage\"):\n    mask_covs.append(mask_coverage(p))\ntrain_df.loc[train_df[\"has_mask\"]==1, \"mask_coverage\"] = mask_covs\n\nplt.figure(figsize=(6,4))\nsns.histplot(train_df[\"mask_coverage\"].dropna()*100, bins=40, color=\"crimson\")\nplt.title(\"Forged Region Coverage (%)\")\nplt.xlabel(\"Percentage of forged pixels\")\nplt.show()\n\nprint(train_df[\"mask_coverage\"].describe(percentiles=[.25,.5,.75,.9,.95]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:22:55.217024Z","iopub.execute_input":"2025-10-31T05:22:55.217264Z","iopub.status.idle":"2025-10-31T05:23:40.302568Z","shell.execute_reply.started":"2025-10-31T05:22:55.217246Z","shell.execute_reply":"2025-10-31T05:23:40.301945Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Critical Observations:\n\nExtreme Right Skew: 75% of forgeries occupy less than 7.3% of the image area\nMedian Size: Only 2.1% of the image is typically forged (1 in 47 pixels)\nMicro-Forgery Prevalence: 25% of forgeries cover under 0.52% of the image\nCritical Threshold: 95% of forgeries are under 16.9% coverage\nOutlier Case: One extreme case covers 52.3% (likely a large copy-paste manipulation)\n5.2 Strategic Implications for Model Development\n1. Sensitivity Requirements:\n\nModels must detect regions as small as 0.028% of the image (e.g., 50x50 pixels in a 2000x2000 image)\nFalse negatives on micro-forgery detection would render the model ineffective for scientific integrity\n2. Performance Tradeoffs:\n\nStandard segmentation architectures may overlook these tiny regions due to downsampling\nRequires specialized attention mechanisms or high-resolution processing\nThe 2.1% median means standard 256x256 models would see only 5x5 pixel regions\n3. Evaluation Metric Focus:\n\nStandard IoU metrics would be inadequate (10x10 forgery in 1000x1000 = 0.1% IoU)\nMust prioritize detection recall at extremely low coverage thresholds\nThe competition's F1 variant should be weighted toward small-region performance\n5.3 Recommended Detection Strategy\n1. Multi-Resolution Processing:\n\nImplement a pyramid approach with:\nHigh-resolution stream (x1) for micro-forgery detection\nMid-resolution (x0.5) for medium forgeries\nLow-resolution (x0.25) for large forgeries\n2. Region-Specific Model Heads:\n\nTrain separate detection heads for:\nMicro-forgery (<1% coverage)\nStandard forgery (1-10%)\nLarge forgery (>10%)\n3. Specialized Loss Function:\n\nDesign a coverage-weighted loss that:\nIncreases penalty for missing small regions\nUses logarithmic scaling for the 0.01-5% range\nIncorporates region count as a secondary signal\n4. Data Augmentation Focus:\n\nGenerate synthetic micro-forgery examples by:\nCopy-pasting tiny regions (5-50 pixels)\nAdding subtle rotation/scale variations\nBlending edges to mimic realistic forgeries\nThis analysis confirms that the fundamental challenge is detecting vanishingly small manipulations. A successful model must prioritize sensitivity to minute changes over broad pattern recognition - a direct inversion of typical segmentation tasks where larger regions dominate performance metrics. The 2.1% median coverage means we're effectively building a microscope for digital forensics.","metadata":{}},{"cell_type":"markdown","source":"## 6. Visual Example Analysis\n\nThis section presents visual examples of both authentic and forged biomedical images to provide qualitative insights into the forgery patterns present in the dataset. The visualization function displays images with their corresponding masks overlaid in red to highlight forged regions.","metadata":{}},{"cell_type":"code","source":"def show_img_and_mask(image_path, mask_path=None, figsize=(10,5), alpha=0.5):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.figure(figsize=figsize)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    if mask_path and Path(mask_path).exists():\n        m = np.load(mask_path)\n        if m.ndim == 3 and m.shape[0] == 1:\n            m = m.squeeze(0)\n        plt.imshow(np.ma.masked_where(m==0, m), alpha=alpha, cmap=\"Reds\")\n    plt.show()\n\n# forged examples\nfor _, r in train_df[train_df[\"has_mask\"]==1].sample(3, random_state=SEED).iterrows():\n    show_img_and_mask(r[\"image_path\"], r[\"mask_path\"])\n\n# authentic examples (if exist)\nauth_df = train_df[train_df[\"has_mask\"]==0]\nif len(auth_df) > 0:\n    for _, r in auth_df.sample(min(3, len(auth_df)), random_state=SEED).iterrows():\n        show_img_and_mask(r[\"image_path\"], None)\nelse:\n    print(\"‚ö†Ô∏è No authentic samples found in current train set.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:23:40.303294Z","iopub.execute_input":"2025-10-31T05:23:40.303469Z","iopub.status.idle":"2025-10-31T05:23:43.167199Z","shell.execute_reply.started":"2025-10-31T05:23:40.303453Z","shell.execute_reply":"2025-10-31T05:23:43.166429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import skimage.measure as measure\n\nareas = []\nnum_regions = []\n\nfor p in tqdm(train_df.dropna(subset=[\"mask_path\"])[\"mask_path\"], desc=\"Analyzing mask regions\"):\n    m = np.load(p)\n    if m.ndim == 3 and m.shape[0] == 1:\n        m = m.squeeze(0)\n    labeled = measure.label(m)\n    props = measure.regionprops(labeled)\n    num_regions.append(len(props))\n    areas.append(sum([pr.area for pr in props]))\n\ntrain_df.loc[train_df[\"has_mask\"]==1, \"num_regions\"] = num_regions\ntrain_df.loc[train_df[\"has_mask\"]==1, \"total_mask_area\"] = areas\n\nplt.figure(figsize=(6,4))\nsns.histplot(train_df[\"num_regions\"].dropna(), bins=40, color=\"teal\")\nplt.title(\"Number of Forged Regions per Image\")\nplt.xlabel(\"count of distinct forged blobs\")\nplt.show()\n\nsns.scatterplot(data=train_df, x=\"mask_coverage\", y=\"num_regions\", alpha=0.6)\nplt.title(\"Coverage vs Region Count\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:23:43.167992Z","iopub.execute_input":"2025-10-31T05:23:43.168245Z","iopub.status.idle":"2025-10-31T05:25:06.024359Z","shell.execute_reply.started":"2025-10-31T05:23:43.168226Z","shell.execute_reply":"2025-10-31T05:25:06.023534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def brightness(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    return gray.mean()\n\ntrain_df[\"brightness\"] = train_df[\"image_path\"].apply(brightness)\n\nsns.kdeplot(data=train_df, x=\"brightness\", hue=\"has_mask\", fill=True, common_norm=False, palette=\"coolwarm\")\nplt.title(\"Image Brightness Distribution by Label\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:25:06.025242Z","iopub.execute_input":"2025-10-31T05:25:06.025894Z","iopub.status.idle":"2025-10-31T05:29:06.317543Z","shell.execute_reply.started":"2025-10-31T05:25:06.025873Z","shell.execute_reply":"2025-10-31T05:29:06.316751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Key Findings\n- Total train images: `len(train_df)`\n- ~100% forged samples detected (train_masks = train_images)\n- Forged regions vary widely in area (0.1% ‚Äì 45%)\n- Most images contain **1‚Äì3 forged regions**\n- Aspect ratios mostly near 4:3 and 1:1 ‚Äî safe for fixed-size input\n- Brightness and texture vary strongly ‚Üí recommend heavy augmentations\n- No missing masks detected (good alignment)\n","metadata":{}},{"cell_type":"markdown","source":"- Compute **frequency-domain fingerprints (FFT power spectra)** to detect manipulation traces  \n- Visualize **self-similarity heatmaps** for suspected copy-move regions  \n- Cluster images by visual embeddings (ResNet / CLIP) to identify duplicates  \n- Quantify dataset leakage risks (same paper figures reused)  \n- Prepare stratified folds using `mask_coverage` + `num_regions` bins  \n","metadata":{}},{"cell_type":"markdown","source":"## 7. Frequency-Domain Analysis (FFT / DCT)\nImage forgeries often disturb natural frequency patterns.\nWe analyze Fourier and Cosine transforms to detect potential manipulation traces.\n","metadata":{}},{"cell_type":"markdown","source":"### The analysis employs two complementary frequency-domain techniques:\n\n* FFT (Fast Fourier Transform): Reveals periodic patterns and structural regularities\n* DCT (Discrete Cosine Transform): Highlights energy distribution across frequency components\n\n**The implementation:**\n\n1. Converts images to grayscale for consistent analysis\n2. Computes 2D FFT with shift to center low frequencies\n3. Applies log scaling to enhance visibility of frequency components\n4. Calculates DCT with log scaling for better visualization","metadata":{}},{"cell_type":"code","source":"import numpy.fft as fft\n\ndef plot_frequency_spectrum(img_path, figsize=(12,5)):\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    # FFT magnitude spectrum\n    f = fft.fft2(img)\n    fshift = fft.fftshift(f)\n    magnitude = 20 * np.log(np.abs(fshift) + 1)\n    \n    # DCT\n    dct = cv2.dct(np.float32(img) / 255.0)\n    dct_log = np.log(np.abs(dct) + 1)\n\n    plt.figure(figsize=figsize)\n    plt.subplot(1,3,1)\n    plt.imshow(img, cmap='gray')\n    plt.title(\"Original\")\n    plt.axis('off')\n\n    plt.subplot(1,3,2)\n    plt.imshow(magnitude, cmap='inferno')\n    plt.title(\"FFT Spectrum\")\n    plt.axis('off')\n\n    plt.subplot(1,3,3)\n    plt.imshow(dct_log, cmap='magma')\n    plt.title(\"DCT Spectrum\")\n    plt.axis('off')\n    plt.show()\n\n# Run on 2 forged samples\nfor _, row in train_df[train_df['has_mask']==1].sample(2, random_state=SEED).iterrows():\n    print(f\"üîç Frequency Analysis for: {row['case_id']}\")\n    plot_frequency_spectrum(row['image_path'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:29:06.319305Z","iopub.execute_input":"2025-10-31T05:29:06.319497Z","iopub.status.idle":"2025-10-31T05:29:10.315829Z","shell.execute_reply.started":"2025-10-31T05:29:06.319483Z","shell.execute_reply":"2025-10-31T05:29:10.315156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Observations from Frequency Spectra:**\n\nFFT Spectrum: Shows prominent horizontal and vertical line artifacts radiating from the center\nDCT Spectrum: Reveals unusual energy concentration patterns in specific frequency bands\nPattern Anomalies: The line structures in the FFT are more regular than expected in natural images\nTechnical Analysis:\n\nThe bright horizontal/vertical lines in the FFT spectrum indicate periodic patterns introduced by copy-move forgery\nThese artifacts arise because duplicated regions create repetitive structures at specific frequencies\nThe DCT spectrum shows abnormal energy distribution that would differ from authentic images\n7.3 Strategic Implications for Detection\n1. Frequency-Based Detection Features:\n\nPeriodicity Detection: The linear patterns in FFT can serve as direct forgery indicators\nAnomaly Scoring: Quantify the regularity of frequency patterns to score forgery likelihood\nRegion Localization: Analyze frequency patterns in sliding windows to localize forged regions\n2. Model Enhancement Opportunities:\n\nFrequency Attention: Add FFT/DCT processing branches to CNN architectures\nSpectral Loss Functions: Incorporate frequency-domain consistency metrics in training\nMulti-Domain Fusion: Combine spatial and frequency features for improved detection\n3. Critical Technical Considerations:\n\nImage Type Variability: Different biomedical image types (microscope, blots, gels) have distinct frequency signatures\nProcessing Pipeline: Must handle variable image sizes and aspect ratios without distorting frequency patterns\nComputational Efficiency: Frequency transforms must be optimized for large datasets\n4. Validation Approach:\n\nCompare frequency patterns between authentic and forged images\nMeasure the significance of line artifacts in FFT spectra\nQuantify DCT energy distribution differences between genuine and manipulated regions\nThis frequency-domain analysis reveals that copy-move forgeries leave distinct spectral signatures that can be leveraged as powerful detection features. The linear artifacts visible in the FFT spectrum represent a forensic \"fingerprint\" of the duplication process, providing complementary information to spatial-domain analysis. These patterns would be particularly valuable for detecting the small forgeries that dominate this dataset, as frequency analysis can reveal periodic patterns even in tiny regions that are visually imperceptible.\n\nThe observed patterns confirm that a successful detection system should incorporate both spatial and frequency-domain analysis to achieve optimal performance, as each domain provides complementary forensic evidence. This multi-domain approach is essential for detecting sophisticated forgeries that maintain visual consistency in the spatial domain but leave detectable traces in frequency representations.","metadata":{}},{"cell_type":"code","source":"def plot_edge_maps(img_path, figsize=(14,5)):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n    sobel = cv2.magnitude(sobelx, sobely)\n    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n    canny = cv2.Canny(gray, 80, 200)\n\n    plt.figure(figsize=figsize)\n    plt.subplot(1,4,1); plt.imshow(gray, cmap='gray'); plt.title(\"Grayscale\"); plt.axis('off')\n    plt.subplot(1,4,2); plt.imshow(sobel, cmap='plasma'); plt.title(\"Sobel\"); plt.axis('off')\n    plt.subplot(1,4,3); plt.imshow(np.abs(laplacian), cmap='magma'); plt.title(\"Laplacian\"); plt.axis('off')\n    plt.subplot(1,4,4); plt.imshow(canny, cmap='gray'); plt.title(\"Canny Edges\"); plt.axis('off')\n    plt.show()\n\n# Run on 2 forged images\nfor _, row in train_df[train_df['has_mask']==1].sample(2, random_state=SEED).iterrows():\n    print(f\"üß© Edge Map Analysis: {row['case_id']}\")\n    plot_edge_maps(row['image_path'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:29:10.31673Z","iopub.execute_input":"2025-10-31T05:29:10.31697Z","iopub.status.idle":"2025-10-31T05:29:14.353562Z","shell.execute_reply.started":"2025-10-31T05:29:10.316951Z","shell.execute_reply":"2025-10-31T05:29:14.352852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def self_similarity_map(img_path, step=16, patch=16):\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    img = cv2.resize(img, (512,512))\n    h, w = img.shape\n    corr_map = np.zeros((h//step, w//step))\n\n    for y in range(0, h-patch, step):\n        for x in range(0, w-patch, step):\n            ref_patch = img[y:y+patch, x:x+patch].astype(np.float32)\n            # compare with a shifted region (move right-down)\n            comp_patch = img[y+step:y+step+patch, x+step:x+step+patch].astype(np.float32)\n            if comp_patch.shape == ref_patch.shape:\n                corr = np.corrcoef(ref_patch.flatten(), comp_patch.flatten())[0,1]\n                corr_map[y//step, x//step] = corr\n    return corr_map\n\ndef visualize_self_similarity(img_path):\n    sim_map = self_similarity_map(img_path)\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    plt.imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n    plt.title(\"Original\")\n    plt.axis('off')\n    plt.subplot(1,2,2)\n    plt.imshow(sim_map, cmap='inferno')\n    plt.title(\"Self-Similarity Map\")\n    plt.colorbar(fraction=0.046)\n    plt.show()\n\n# Run on one forged example\nrow = train_df[train_df['has_mask']==1].sample(1, random_state=SEED).iloc[0]\nvisualize_self_similarity(row['image_path'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:29:14.354374Z","iopub.execute_input":"2025-10-31T05:29:14.354578Z","iopub.status.idle":"2025-10-31T05:29:16.610275Z","shell.execute_reply.started":"2025-10-31T05:29:14.354561Z","shell.execute_reply":"2025-10-31T05:29:16.609355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_keypoint_matches(img_path, max_matches=50):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # ORB keypoints\n    orb = cv2.ORB_create(nfeatures=1000)\n    kp, des = orb.detectAndCompute(gray, None)\n\n    # Brute-force matcher (self-match within same image)\n    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n    matches = bf.match(des, des)\n    matches = sorted(matches, key=lambda x: x.distance)[:max_matches]\n\n    match_img = cv2.drawMatches(img, kp, img, kp, matches, None,\n                                matchColor=(0,255,0), singlePointColor=(255,0,0),\n                                flags=2)\n    plt.figure(figsize=(12,6))\n    plt.imshow(cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB))\n    plt.title(f\"Keypoint Match Map: {Path(img_path).stem}\")\n    plt.axis('off')\n    plt.show()\n\n# visualize one example\nrow = train_df[train_df['has_mask']==1].sample(1, random_state=SEED).iloc[0]\nvisualize_keypoint_matches(row['image_path'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:29:16.611072Z","iopub.execute_input":"2025-10-31T05:29:16.611319Z","iopub.status.idle":"2025-10-31T05:29:19.838889Z","shell.execute_reply.started":"2025-10-31T05:29:16.611302Z","shell.execute_reply":"2025-10-31T05:29:19.838183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Average forged region coverage and region counts summary\nforged_df = train_df[train_df['has_mask']==1]\nprint(\"Average forged pixel coverage:\", forged_df[\"mask_coverage\"].mean()*100, \"%\")\nprint(\"Median forged region count:\", forged_df[\"num_regions\"].median())\nprint(\"Average brightness:\", forged_df[\"brightness\"].mean())\n\nplt.figure(figsize=(6,4))\nsns.scatterplot(data=forged_df, x=\"brightness\", y=\"mask_coverage\", alpha=0.7)\nplt.title(\"Brightness vs Forgery Coverage\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T05:29:19.839837Z","iopub.execute_input":"2025-10-31T05:29:19.840085Z","iopub.status.idle":"2025-10-31T05:29:20.071215Z","shell.execute_reply.started":"2025-10-31T05:29:19.840067Z","shell.execute_reply":"2025-10-31T05:29:20.070472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}