{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"color:white;display:inline-block;border-radius:5px;background-color:#009688 ;font-family:Nexa;overflow:hidden\"><p style=\"padding:10px;color:white;overflow:hidden;font-size:85%;letter-spacing:0.5px;margin:0;border: 6px groove #ffd700;\"><b> </b>Imports Libraries</p></div>\n","metadata":{}},{"cell_type":"code","source":"# =======================================\n# PART 1: Configuration & Imports\n# =======================================\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom PIL import Image\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# Seed for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\n# Device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {DEVICE}\")\n\n# Paths & Hyperparameters\nIMG_SIZE = 256  # You can try 512 for better score if memory allows\nBATCH_SIZE = 8\nEPOCHS = 30  # Increase epochs for better learning\nLR = 1e-4\nDATA_PATH = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/\"\n\n# Threshold tuning for F1/Dice\nTHRESHOLD = 0.35\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:00:56.018393Z","iopub.execute_input":"2025-11-08T06:00:56.019342Z","iopub.status.idle":"2025-11-08T06:00:56.030725Z","shell.execute_reply.started":"2025-11-08T06:00:56.019313Z","shell.execute_reply":"2025-11-08T06:00:56.02991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================\n# PART 2: Dataset + Augmentation\n# =======================================\n!pip install albumentations --quiet\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# 1️⃣ Augmentation pipelines\ntrain_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n    A.GaussianBlur(blur_limit=(3,5), p=0.3),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\nmask_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    ToTensorV2()\n])\n\n# 2️⃣ Dataset class\nclass ForgeryDataset(Dataset):\n    def __init__(self, image_paths, mask_paths=None, is_authentic=None, transforms=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.is_authentic = is_authentic\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n        \n        if self.is_authentic[idx]:\n            mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n        else:\n            mask = np.array(np.load(self.mask_paths[idx]))\n            if mask.ndim == 3:\n                mask = mask[:, :, 0]\n            mask = (mask > 0).astype(np.uint8)\n        \n        if self.transforms:\n            augmented = self.transforms(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        \n        return img, mask\n\n# 3️⃣ Load file paths\nforged_images = sorted(glob(os.path.join(DATA_PATH, 'train_images/forged/*.png')))\ntrain_masks = sorted(glob(os.path.join(DATA_PATH, 'train_masks/*.npy')))\nauthentic_images = sorted(glob(os.path.join(DATA_PATH, 'train_images/authentic/*.png')))\n\nmatched_forged_images, matched_masks = [], []\nfor img_path in forged_images:\n    img_name = os.path.basename(img_path).replace('.png','')\n    mask_path = os.path.join(DATA_PATH, f'train_masks/{img_name}.npy')\n    if os.path.exists(mask_path):\n        matched_forged_images.append(img_path)\n        matched_masks.append(mask_path)\n\nall_images = matched_forged_images + authentic_images\nall_masks = matched_masks + [None]*len(authentic_images)\nall_is_authentic = [False]*len(matched_forged_images) + [True]*len(authentic_images)\n\n# 4️⃣ Train-validation split\nfrom sklearn.model_selection import train_test_split\n\ntrain_imgs, val_imgs, train_masks_split, val_masks_split, train_auth, val_auth = train_test_split(\n    all_images, all_masks, all_is_authentic, test_size=0.2, random_state=SEED, stratify=all_is_authentic\n)\n\n# 5️⃣ Create Datasets and Dataloaders\ntrain_dataset = ForgeryDataset(train_imgs, train_masks_split, train_auth, transforms=train_transform)\nval_dataset = ForgeryDataset(val_imgs, val_masks_split, val_auth, transforms=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"✅ Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:01:24.739606Z","iopub.execute_input":"2025-11-08T06:01:24.740238Z","iopub.status.idle":"2025-11-08T06:05:59.200365Z","shell.execute_reply.started":"2025-11-08T06:01:24.74019Z","shell.execute_reply":"2025-11-08T06:05:59.199529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================\n# PART 3: Model + Loss + Optimizer + Training Loop\n# =======================================\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\n# 1️⃣ UNet model (simplified)\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[64,128,256,512]):\n        super().__init__()\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(2,2)\n        \n        # Down part\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n        \n        # Up part\n        for feature in reversed(features):\n            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n            self.ups.append(DoubleConv(feature*2, feature))\n        \n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        skip_connections = []\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n        \n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n        \n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            if x.shape != skip_connection.shape:\n                x = F.interpolate(x, size=skip_connection.shape[2:])\n            x = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](x)\n        \n        return self.final_conv(x)\n\n# 2️⃣ Loss functions\nclass DiceBCELoss(nn.Module):\n    def __init__(self, smooth=1):\n        super().__init__()\n        self.smooth = smooth\n        self.bce = nn.BCEWithLogitsLoss()\n    \n    def forward(self, preds, targets):\n        preds = torch.sigmoid(preds)\n        intersection = (preds * targets).sum(dim=(2,3))\n        dice = (2 * intersection + self.smooth) / (preds.sum(dim=(2,3)) + targets.sum(dim=(2,3)) + self.smooth)\n        dice_loss = 1 - dice.mean()\n        bce_loss = self.bce(preds, targets)\n        return bce_loss + dice_loss\n\n# 3️⃣ Initialize model, optimizer, loss\nmodel = UNet(in_channels=3, out_channels=1).to(DEVICE)\ncriterion = DiceBCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\n# 4️⃣ Training function\nfrom tqdm import tqdm\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n    best_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n            imgs = imgs.to(DEVICE)\n            masks = masks.to(DEVICE)\n            \n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * imgs.size(0)\n        \n        train_loss /= len(train_loader.dataset)\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n                imgs = imgs.to(DEVICE)\n                masks = masks.to(DEVICE)\n                outputs = model(imgs)\n                loss = criterion(outputs, masks)\n                val_loss += loss.item() * imgs.size(0)\n        \n        val_loss /= len(val_loader.dataset)\n        \n        if scheduler:\n            scheduler.step(val_loss)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n        \n        # Save best model\n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"✅ Saved Best Model!\")\n    \n    return model\n\n# 5️⃣ Start training\n# trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:05:59.530706Z","iopub.execute_input":"2025-11-08T06:05:59.530925Z","iopub.status.idle":"2025-11-08T06:05:59.862131Z","shell.execute_reply.started":"2025-11-08T06:05:59.5309Z","shell.execute_reply":"2025-11-08T06:05:59.861539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n# Import CV2 (used by albumentations internally for image operations)\nimport cv2 \nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n# Import albumentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2 \nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\n# =======================================\n# Configuration Variables\n# =======================================\nIMG_SIZE = 256\nBATCH_SIZE = 16\nDATA_DIR = \"./data\" # Adjust this path\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Assume train_df is loaded here:\n# train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\")) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:07:51.479869Z","iopub.execute_input":"2025-11-08T06:07:51.480446Z","iopub.status.idle":"2025-11-08T06:07:51.487446Z","shell.execute_reply.started":"2025-11-08T06:07:51.480415Z","shell.execute_reply":"2025-11-08T06:07:51.486682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = A.Compose([\n    A.Resize(height=IMG_SIZE, width=IMG_SIZE, interpolation=cv2.INTER_NEAREST), # Resize first\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=15, p=0.5),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(), # Converts numpy array to torch tensor\n])\n\nval_transforms = A.Compose([\n    A.Resize(height=IMG_SIZE, width=IMG_SIZE, interpolation=cv2.INTER_NEAREST), # Resize first\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n# For masks that don't need normalization/color jitter, the main transforms suffice \n# because ToTensorV2 handles masks correctly (no normalization applied to them).\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:08:07.796548Z","iopub.execute_input":"2025-11-08T06:08:07.796829Z","iopub.status.idle":"2025-11-08T06:08:07.808039Z","shell.execute_reply.started":"2025-11-08T06:08:07.796808Z","shell.execute_reply":"2025-11-08T06:08:07.807461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ForgeryDataset(Dataset):\n    # Rename 'transform' to 'transforms' for clarity with albumentations naming\n    def __init__(self, df, img_dir, mask_dir, transforms=None): \n        self.df = df\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.transforms = transforms # Now holds the A.Compose object\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx][\"image_name\"]\n        img_path = os.path.join(self.img_dir, img_name)\n        mask_path = os.path.join(self.mask_dir, img_name.replace(\".jpg\", \".png\"))\n\n        # Open image and mask using PIL\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Check if mask exists, otherwise assume authentic (zero mask)\n        if os.path.exists(mask_path):\n            mask = Image.open(mask_path).convert(\"L\")\n        else:\n            # Create a zero mask matching the image size/mode if authentic\n            mask = Image.fromarray(np.zeros((image.height, image.width), dtype=np.uint8))\n        \n        # Convert PIL images to numpy arrays, which Albumentations expects\n        image = np.array(image)\n        mask = np.array(mask)\n        \n        if self.transforms:\n            # This is where the magic happens: albumentations syncs transforms\n            augmented = self.transforms(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            # Ensure mask tensor is float and has the correct dimensions (e.g. 1 channel)\n            if len(mask.shape) == 2:\n                mask = mask.unsqueeze(0).float()\n            mask = mask.float() / 255.0 # Normalize mask values to 0.0 or 1.0\n\n        return image, mask\n\n# Train-validation split (Requires 'train_df' to be defined)\ntrain_split, val_split = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Initialize datasets with the single 'transforms' argument\ntrain_dataset = ForgeryDataset(train_split, os.path.join(DATA_DIR, \"train_images\"),\n                               os.path.join(DATA_DIR, \"train_masks\"), train_transforms)\nval_dataset = ForgeryDataset(val_split, os.path.join(DATA_DIR, \"train_images\"),\n                             os.path.join(DATA_DIR, \"train_masks\"), val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:08:20.732744Z","iopub.execute_input":"2025-11-08T06:08:20.733026Z","iopub.status.idle":"2025-11-08T06:08:20.74654Z","shell.execute_reply.started":"2025-11-08T06:08:20.733007Z","shell.execute_reply":"2025-11-08T06:08:20.745591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this in a notebook cell to install the library\n!pip install segmentation-models-pytorch\n!pip install timm # segmentation_models_pytorch often needs timm as a dependency\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:14:00.472866Z","iopub.execute_input":"2025-11-08T06:14:00.473466Z","iopub.status.idle":"2025-11-08T06:18:58.326682Z","shell.execute_reply.started":"2025-11-08T06:14:00.473441Z","shell.execute_reply":"2025-11-08T06:18:58.325655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================\n# Training and Validation Functions\n# =======================================\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for imgs, masks in tqdm(loader, desc=\"Training\"):\n        imgs, masks = imgs.to(device), masks.to(device)\n\n        # Forward pass\n        preds = model(imgs)\n        loss = criterion(preds, masks)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * imgs.size(0)\n    \n    return running_loss / len(loader.dataset)\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for imgs, masks in tqdm(loader, desc=\"Validation\"):\n            imgs, masks = imgs.to(device), masks.to(device)\n            preds = model(imgs)\n            loss = criterion(preds, masks)\n            running_loss += loss.item() * imgs.size(0)\n            \n    return running_loss / len(loader.dataset)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:19:18.161589Z","iopub.execute_input":"2025-11-08T06:19:18.161849Z","iopub.status.idle":"2025-11-08T06:19:18.168349Z","shell.execute_reply.started":"2025-11-08T06:19:18.161831Z","shell.execute_reply":"2025-11-08T06:19:18.167511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = \"./data\" \nos.path.join(DATA_DIR, \"train_images\") # Result: './data/train_images'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:20:39.750106Z","iopub.execute_input":"2025-11-08T06:20:39.750425Z","iopub.status.idle":"2025-11-08T06:20:39.756959Z","shell.execute_reply.started":"2025-11-08T06:20:39.750405Z","shell.execute_reply":"2025-11-08T06:20:39.756374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Change the previous line where you defined DATA_DIR\nDATA_DIR = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n\n# Now the script will look here:\n# /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\n# /kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:21:07.219557Z","iopub.execute_input":"2025-11-08T06:21:07.220197Z","iopub.status.idle":"2025-11-08T06:21:07.223596Z","shell.execute_reply.started":"2025-11-08T06:21:07.220175Z","shell.execute_reply":"2025-11-08T06:21:07.22297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use a single dot to mean \"current directory\"\nDATA_DIR = \".\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:21:16.532162Z","iopub.execute_input":"2025-11-08T06:21:16.532827Z","iopub.status.idle":"2025-11-08T06:21:16.53609Z","shell.execute_reply.started":"2025-11-08T06:21:16.532804Z","shell.execute_reply":"2025-11-08T06:21:16.535378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfor root, dirs, files in os.walk(\"/\"):\n    if \"train_images\" in dirs:\n        print(f\"Found train_images folder at: {os.path.join(root, 'train_images')}\")\n        # Stop searching if found one\n        break \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:21:38.978892Z","iopub.execute_input":"2025-11-08T06:21:38.979514Z","iopub.status.idle":"2025-11-08T06:22:01.997682Z","shell.execute_reply.started":"2025-11-08T06:21:38.979488Z","shell.execute_reply":"2025-11-08T06:22:01.997007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================\n# Configuration Variables and Imports (Top of your script)\n# =======================================\n\n# Replace the previous DATA_DIR = \"./data\" with the actual path you found:\n\nDATA_DIR = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\" \n# OR\n# DATA_DIR = \"C:/Users/YourName/Desktop/KaggleForgeries\" \n# OR\n# DATA_DIR = \"/home/username/data/forgery_detection\"\n\n# ... (rest of your configuration code remains the same) ...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:22:25.156295Z","iopub.execute_input":"2025-11-08T06:22:25.156564Z","iopub.status.idle":"2025-11-08T06:22:25.160612Z","shell.execute_reply.started":"2025-11-08T06:22:25.156546Z","shell.execute_reply":"2025-11-08T06:22:25.159753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code from inside the ForgeryDataset class\ndef __getitem__(self, idx):\n    # ...\n    augmented = self.transforms(image=image, mask=mask) # Uses 'self.transforms'\n    # ...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:23:51.715621Z","iopub.execute_input":"2025-11-08T06:23:51.716176Z","iopub.status.idle":"2025-11-08T06:23:51.720202Z","shell.execute_reply.started":"2025-11-08T06:23:51.716157Z","shell.execute_reply":"2025-11-08T06:23:51.71954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The problematic line, if you copied it exactly:\ntest_transform = val_transforms # It expects val_transforms to be available globally\n\n# ...\n# augmented = self.transforms(image=input_image_np) # If this is in your submission code, it's wrong.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:24:26.867623Z","iopub.execute_input":"2025-11-08T06:24:26.867963Z","iopub.status.idle":"2025-11-08T06:24:26.871389Z","shell.execute_reply.started":"2025-11-08T06:24:26.86794Z","shell.execute_reply":"2025-11-08T06:24:26.870688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make sure these are imported at the top of your notebook:\n# import albumentations as A\n# import cv2\n# from albumentations.pytorch.transforms import ToTensorV2\n# import numpy as np\n# from PIL import Image\n\ndef create_submission(model, test_dir, submission_path, img_size):\n    model.eval()\n    test_images_names = os.listdir(test_dir)\n    submission_data = []\n\n    # Define the transformation needed for testing locally within the function\n    # so it doesn't need to reference 'self' or global 'val_transforms'\n    test_transform = A.Compose([\n        A.Resize(height=img_size, width=img_size, interpolation=cv2.INTER_NEAREST),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])\n\n    with torch.no_grad():\n        for img_name in tqdm(test_images_names, desc=\"Predicting Test Images\"):\n            img_path = os.path.join(test_dir, img_name)\n            image = Image.open(img_path).convert(\"RGB\")\n            original_shape = image.size # (width, height)\n            \n            # Convert to numpy for albumentations, then transform\n            input_image_np = np.array(image)\n            # Use the local variable 'test_transform'\n            augmented = test_transform(image=input_image_np) \n            input_image = augmented['image'].unsqueeze(0).to(DEVICE)\n            \n            # Get model output\n            output = model(input_image)\n            \n            # Post-process: sigmoid and threshold\n            mask_pred = torch.sigmoid(output).cpu().numpy().squeeze()\n            \n            # Resize the mask back to original image size using PIL\n            if mask_pred.ndim == 3 and mask_pred.shape == 1:\n                mask_pred = mask_pred.squeeze(0)\n                \n            mask_pred_resized = Image.fromarray((mask_pred * 255).astype(np.uint8)).resize(original_shape, Image.NEAREST)\n            mask_pred_resized_np = np.array(mask_pred_resized) > 127 # Binary mask (True/False)\n            \n            # RLE Encode (ensure rle_encode is defined globally)\n            rle_mask = rle_encode(mask_pred_resized_np)\n            \n            submission_data.append({\"case_id\": img_name.replace('.jpg', ''), \"annotation\": rle_mask})\n\n    submission_df = pd.DataFrame(submission_data)\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission file saved to {submission_path}\")\n\n# Run the submission function by passing IMG_SIZE as an argument\ncreate_submission(\n    model, \n    os.path.join(DATA_DIR, \"test_images\"), \n    \"submission.csv\",\n    IMG_SIZE # Pass the global IMG_SIZE variable\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:24:42.477938Z","iopub.execute_input":"2025-11-08T06:24:42.478777Z","iopub.status.idle":"2025-11-08T06:24:42.63297Z","shell.execute_reply.started":"2025-11-08T06:24:42.478751Z","shell.execute_reply":"2025-11-08T06:24:42.632289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SmallUNet().to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:24:48.189681Z","iopub.execute_input":"2025-11-08T06:24:48.190373Z","iopub.status.idle":"2025-11-08T06:24:48.204992Z","shell.execute_reply.started":"2025-11-08T06:24:48.190351Z","shell.execute_reply":"2025-11-08T06:24:48.204346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:24:49.201003Z","iopub.execute_input":"2025-11-08T06:24:49.201391Z","iopub.status.idle":"2025-11-08T06:24:49.206073Z","shell.execute_reply.started":"2025-11-08T06:24:49.201365Z","shell.execute_reply":"2025-11-08T06:24:49.205152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for imgs, masks in train_loader:\n            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:24:49.948035Z","iopub.execute_input":"2025-11-08T06:24:49.948417Z","iopub.status.idle":"2025-11-08T06:24:49.954394Z","shell.execute_reply.started":"2025-11-08T06:24:49.948385Z","shell.execute_reply":"2025-11-08T06:24:49.953488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================\n# Prediction and Submission\n# =======================================\n\ndef create_submission(model, test_dir, sample_sub_path, submission_path):\n    model.eval()\n    test_images_names = os.listdir(test_dir)\n    submission_data = []\n\n    # Use the same validation transform for test images (without augmentation)\n    test_transform = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    with torch.no_grad():\n        for img_name in tqdm(test_images_names):\n            img_path = os.path.join(test_dir, img_name)\n            image = Image.open(img_path).convert(\"RGB\")\n            original_shape = image.size # (width, height)\n            \n            input_image = test_transform(image).unsqueeze(0).to(DEVICE)\n            output = model(input_image)\n            \n            # Post-process the model output\n            # Assuming U-Net output (logits), apply sigmoid and threshold\n            mask_pred = torch.sigmoid(output).cpu().numpy().squeeze()\n            # Resize the mask back to original image size\n            mask_pred_resized = Image.fromarray((mask_pred * 255).astype(np.uint8)).resize(original_shape, Image.NEAREST)\n            mask_pred_resized_np = np.array(mask_pred_resized) > 127 # Binary mask\n            \n            rle_mask = rle_encode(mask_pred_resized_np)\n            \n            submission_data.append({\"case_id\": img_name, \"annotation\": rle_mask})\n\n    submission_df = pd.DataFrame(submission_data)\n    # The competition expects \"case_id\" without extension for submission, check sample_submission.csv format\n    submission_df['case_id'] = submission_df['case_id'].str.replace('.jpg', '', regex=False) \n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission file saved to {submission_path}\")\n\n# Example usage (needs a working 'model' variable):\ncreate_submission(model, os.path.join(DATA_DIR, \"test_images\"), \n                  os.path.join(DATA_DIR, \"sample_submission.csv\"), \n                  \"submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:24:52.581729Z","iopub.execute_input":"2025-11-08T06:24:52.581997Z","iopub.status.idle":"2025-11-08T06:24:52.671103Z","shell.execute_reply.started":"2025-11-08T06:24:52.581978Z","shell.execute_reply":"2025-11-08T06:24:52.670259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_PATH = \"./best_model.pth\" \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T06:26:55.035069Z","iopub.execute_input":"2025-11-08T06:26:55.035817Z","iopub.status.idle":"2025-11-08T06:26:55.040193Z","shell.execute_reply.started":"2025-11-08T06:26:55.035786Z","shell.execute_reply":"2025-11-08T06:26:55.039431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}