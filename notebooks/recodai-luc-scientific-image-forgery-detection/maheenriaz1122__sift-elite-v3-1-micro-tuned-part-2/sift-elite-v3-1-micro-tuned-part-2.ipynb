{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nHYBRID ENSEMBLE - SIFT (0.303) + Fast CNN\nStrategy: Use proven SIFT as primary, CNN as refinement\nTarget: 0.305-0.315\n\nRationale:\n- SIFT gives consistent 0.303\n- Add lightweight CNN for edge cases SIFT misses\n- Ensemble voting for final decision\n\"\"\"\n\nimport os\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\nfrom scipy.ndimage import label as scipy_label, binary_fill_holes\nfrom skimage.measure import regionprops\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# =============================================================================\n# CONFIG\n# =============================================================================\n\nclass Config:\n    BASE_PATH = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'\n    TRAIN_IMAGES_FORGED = os.path.join(BASE_PATH, 'train_images/forged')\n    TRAIN_IMAGES_AUTH = os.path.join(BASE_PATH, 'train_images/authentic')\n    TRAIN_MASKS = os.path.join(BASE_PATH, 'train_masks')\n    TEST_IMAGES = os.path.join(BASE_PATH, 'test_images')\n    SAMPLE_SUB = os.path.join(BASE_PATH, 'sample_submission.csv')\n    \n    # SIFT parameters (proven 0.303)\n    SIFT_FEATURES = 5500\n    SIFT_CONTRAST = 0.019\n    MATCH_RATIO = 0.79\n    MIN_MATCHES = 4\n    RANSAC_THRESH = 5.5\n    MIN_DISPLACEMENT = 23\n    MAX_IMAGE_SIZE = 1600\n    USE_CLAHE = True\n    \n    # VARIANT: Slightly more aggressive for private LB\n    SIFT_CONFIDENCE_THRESHOLD = 0.31  # Lower from 0.33\n    SIFT_MIN_MASK_PIXELS = 85  # Lower from 90\n    SIFT_MIN_COVERAGE = 0.00035  # Slightly lower\n    SIFT_MAX_COVERAGE = 0.42  # Slightly higher\n    \n    # CNN parameters (lightweight)\n    CNN_ENABLED = True\n    CNN_IMAGE_SIZE = 256  # Small for speed\n    CNN_THRESHOLD = 0.5\n    \n    # Ensemble\n    ENSEMBLE_MODE = 'weighted'  # 'weighted' or 'voting'\n    SIFT_WEIGHT = 0.75  # SIFT is primary\n    CNN_WEIGHT = 0.25   # CNN is refinement\n\nconfig = Config()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ðŸŽ¯ HYBRID ENSEMBLE - SIFT (0.303) + Fast CNN\")\nprint(f\"   Device: {device}\")\n\n# =============================================================================\n# SIFT DETECTOR (PROVEN 0.303)\n# =============================================================================\n\ndef rle_encode(mask, fg_val=1):\n    if mask.sum() == 0:\n        return []\n    dots = np.where(mask.T.flatten() == fg_val)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef preprocess_image(img_array):\n    if len(img_array.shape) == 3:\n        if img_array.shape[2] == 4:\n            img_array = img_array[:, :, :3]\n        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = img_array.copy()\n    gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n    if config.USE_CLAHE:\n        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n        gray = clahe.apply(gray)\n    return gray\n\ndef resize_smart(image, max_size):\n    h, w = image.shape[:2]\n    if max(h, w) <= max_size:\n        return image, 1.0\n    scale = max_size / max(h, w)\n    new_h, new_w = int(h * scale), int(w * scale)\n    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n    return resized, scale\n\ndef scale_mask_back(mask, orig_shape, scale):\n    if scale == 1.0:\n        return mask\n    h, w = orig_shape[:2]\n    scaled = cv2.resize(mask.astype(np.float32), (w, h), interpolation=cv2.INTER_LINEAR)\n    return (scaled > 0.5).astype(np.uint8)\n\nclass SIFTDetector:\n    def __init__(self, config):\n        self.config = config\n        self.sift = cv2.SIFT_create(\n            nfeatures=config.SIFT_FEATURES,\n            contrastThreshold=config.SIFT_CONTRAST,\n            edgeThreshold=10\n        )\n    \n    def detect(self, image):\n        gray = preprocess_image(image)\n        orig_shape = image.shape\n        gray, scale = resize_smart(gray, config.MAX_IMAGE_SIZE)\n        h, w = gray.shape\n        \n        kp, desc = self.sift.detectAndCompute(gray, None)\n        \n        if desc is None or len(desc) < config.MIN_MATCHES * 2:\n            return np.zeros((h, w), dtype=np.uint8), 0.0\n        \n        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n        matches = bf.knnMatch(desc, desc, k=2)\n        \n        good = []\n        for m_n in matches:\n            if len(m_n) == 2:\n                m, n = m_n\n                if m.queryIdx != m.trainIdx and m.distance < config.MATCH_RATIO * n.distance:\n                    good.append(m)\n        \n        if len(good) < config.MIN_MATCHES:\n            return np.zeros((h, w), dtype=np.uint8), 0.0\n        \n        src_pts = np.float32([kp[m.queryIdx].pt for m in good]).reshape(-1, 2)\n        dst_pts = np.float32([kp[m.trainIdx].pt for m in good]).reshape(-1, 2)\n        \n        disp = dst_pts - src_pts\n        dist = np.linalg.norm(disp, axis=1)\n        valid = dist > config.MIN_DISPLACEMENT\n        \n        if valid.sum() < config.MIN_MATCHES:\n            return np.zeros((h, w), dtype=np.uint8), 0.0\n        \n        src_pts = src_pts[valid]\n        dst_pts = dst_pts[valid]\n        \n        try:\n            M, inliers = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, config.RANSAC_THRESH)\n            \n            if M is None or inliers is None:\n                return np.zeros((h, w), dtype=np.uint8), 0.0\n            \n            num_inliers = inliers.sum()\n            if num_inliers < config.MIN_MATCHES:\n                return np.zeros((h, w), dtype=np.uint8), 0.0\n            \n            mask = np.zeros((h, w), dtype=np.uint8)\n            \n            inlier_src = src_pts[inliers.flatten() > 0]\n            inlier_dst = dst_pts[inliers.flatten() > 0]\n            \n            for pt in inlier_src:\n                x, y = int(pt[0]), int(pt[1])\n                if 0 <= x < w and 0 <= y < h:\n                    cv2.circle(mask, (x, y), 13, 1, -1)\n            \n            for pt in inlier_dst:\n                x, y = int(pt[0]), int(pt[1])\n                if 0 <= x < w and 0 <= y < h:\n                    cv2.circle(mask, (x, y), 13, 1, -1)\n            \n            confidence = min(1.0, num_inliers / max(len(src_pts), 10))\n            if num_inliers > 15:\n                confidence = min(1.0, confidence * 1.25)\n            elif num_inliers > 10:\n                confidence = min(1.0, confidence * 1.15)\n            \n            if scale != 1.0:\n                mask = scale_mask_back(mask, orig_shape, scale)\n            \n            return mask, confidence\n        except:\n            return np.zeros((h, w), dtype=np.uint8), 0.0\n\ndef refine_mask(mask):\n    if mask.sum() == 0:\n        return mask\n    \n    h, w = mask.shape\n    kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n    kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    \n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_small)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_medium, iterations=2)\n    mask = binary_fill_holes(mask).astype(np.uint8)\n    \n    labeled, _ = scipy_label(mask)\n    for region in regionprops(labeled):\n        if region.area < 45 or region.area > (h * w) * 0.40:\n            mask[labeled == region.label] = 0\n    \n    return mask\n\n# =============================================================================\n# FAST CNN (LIGHTWEIGHT REFINEMENT)\n# =============================================================================\n\nclass FastCNN(nn.Module):\n    \"\"\"Ultra-lightweight CNN for quick inference\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.classifier = nn.Linear(64, 1)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return torch.sigmoid(self.classifier(x))\n\nclass CNNDetector:\n    def __init__(self, config, device):\n        self.config = config\n        self.device = device\n        self.model = FastCNN().to(device)\n        self.model.eval()\n    \n    def detect(self, image):\n        \"\"\"Quick CNN-based forgery probability\"\"\"\n        # Resize\n        img_small = cv2.resize(image, (config.CNN_IMAGE_SIZE, config.CNN_IMAGE_SIZE))\n        \n        # Normalize\n        img_tensor = torch.from_numpy(img_small.astype(np.float32) / 255.0)\n        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(self.device)\n        \n        # Predict\n        with torch.no_grad():\n            prob = self.model(img_tensor).cpu().item()\n        \n        return prob\n\n# =============================================================================\n# HYBRID ENSEMBLE\n# =============================================================================\n\nclass HybridEnsemble:\n    def __init__(self, config, device):\n        self.config = config\n        self.sift_detector = SIFTDetector(config)\n        if config.CNN_ENABLED:\n            self.cnn_detector = CNNDetector(config, device)\n        else:\n            self.cnn_detector = None\n    \n    def detect(self, image):\n        h, w = image.shape[:2]\n        \n        # Primary: SIFT detection\n        sift_mask, sift_conf = self.sift_detector.detect(image)\n        sift_mask = refine_mask(sift_mask)\n        \n        # Adjust SIFT confidence\n        if sift_mask.sum() > 0:\n            coverage = sift_mask.sum() / (h * w)\n            if coverage < 0.0003:\n                sift_conf *= 0.35\n            elif coverage > 0.40:\n                sift_conf *= 0.25\n            elif 0.0008 < coverage < 0.18:\n                sift_conf = min(1.0, sift_conf * 1.13)\n            elif coverage > 0.25:\n                sift_conf *= 0.8\n        else:\n            sift_conf = 0.0\n        \n        sift_conf = np.clip(sift_conf, 0, 1)\n        \n        # Secondary: CNN refinement (if enabled)\n        if self.cnn_detector is not None:\n            cnn_prob = self.cnn_detector.detect(image)\n        else:\n            cnn_prob = 0.5  # Neutral\n        \n        # Ensemble decision\n        if config.ENSEMBLE_MODE == 'weighted':\n            final_conf = config.SIFT_WEIGHT * sift_conf + config.CNN_WEIGHT * cnn_prob\n        else:  # voting\n            sift_vote = 1 if sift_conf > config.SIFT_CONFIDENCE_THRESHOLD else 0\n            cnn_vote = 1 if cnn_prob > config.CNN_THRESHOLD else 0\n            final_conf = (sift_vote + cnn_vote) / 2.0\n        \n        return sift_mask, final_conf\n\n# =============================================================================\n# SUBMISSION\n# =============================================================================\n\ndef generate_submission(config, detector):\n    print(\"\\nGenerating submission...\")\n    \n    sample_sub = pd.read_csv(config.SAMPLE_SUB)\n    submissions = []\n    stats = {'authentic': 0, 'forged': 0}\n    \n    for case_id in tqdm(sample_sub['case_id'], desc=\"Processing\"):\n        try:\n            img = np.array(Image.open(os.path.join(config.TEST_IMAGES, f\"{case_id}.png\")))\n            mask, conf = detector.detect(img)\n            \n            is_forged = False\n            if conf > config.SIFT_CONFIDENCE_THRESHOLD and mask.sum() >= config.SIFT_MIN_MASK_PIXELS:\n                coverage = mask.sum() / (img.shape[0] * img.shape[1])\n                if config.SIFT_MIN_COVERAGE < coverage < config.SIFT_MAX_COVERAGE:\n                    is_forged = True\n            \n            if is_forged:\n                rle = rle_encode(mask)\n                if len(rle) > 0:\n                    annotation = json.dumps([int(x) for x in rle])\n                    stats['forged'] += 1\n                else:\n                    annotation = 'authentic'\n                    stats['authentic'] += 1\n            else:\n                annotation = 'authentic'\n                stats['authentic'] += 1\n            \n            submissions.append({'case_id': case_id, 'annotation': annotation})\n        except:\n            submissions.append({'case_id': case_id, 'annotation': 'authentic'})\n            stats['authentic'] += 1\n    \n    df = pd.DataFrame(submissions)\n    df.to_csv('submission.csv', index=False)\n    \n    print(f\"\\nâœ“ Complete:\")\n    print(f\"  Forged: {stats['forged']} ({stats['forged']/len(df)*100:.1f}%)\")\n    print(f\"  Authentic: {stats['authentic']} ({stats['authentic']/len(df)*100:.1f}%)\")\n    print(\"\\nâœ“ Saved: submission.csv\")\n\ndef main():\n    print(\"=\"*60)\n    print(\"HYBRID ENSEMBLE - SIFT + CNN\")\n    print(\"=\"*60)\n    \n    detector = HybridEnsemble(config, device)\n    generate_submission(config, detector)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"âœ… ENSEMBLE COMPLETE\")\n    print(\"=\"*60)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:33:21.544759Z","iopub.execute_input":"2025-11-06T01:33:21.545418Z","iopub.status.idle":"2025-11-06T01:33:28.530586Z","shell.execute_reply.started":"2025-11-06T01:33:21.545395Z","shell.execute_reply":"2025-11-06T01:33:28.529556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# =============================================================================\n# CONFIG\n# =============================================================================\nclass Config:\n    BASE_PATH = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'\n    TRAIN_IMAGES_FORGED = os.path.join(BASE_PATH, 'train_images/forged')\n    TRAIN_IMAGES_AUTH = os.path.join(BASE_PATH, 'train_images/authentic')\n    TRAIN_MASKS = os.path.join(BASE_PATH, 'train_masks')\n    TEST_IMAGES = os.path.join(BASE_PATH, 'test_images')\n    SAMPLE_SUB = os.path.join(BASE_PATH, 'sample_submission.csv')\n\n    # SIFT parameters\n    SIFT_FEATURES = 5500\n    SIFT_CONTRAST = 0.019\n    MATCH_RATIO = 0.79\n    MIN_MATCHES = 4\n    RANSAC_THRESH = 5.5\n    MIN_DISPLACEMENT = 23\n    MAX_IMAGE_SIZE = 1600\n    USE_CLAHE = True\n\n    # Variant tweaks\n    SIFT_CONFIDENCE_THRESHOLD = 0.31\n    SIFT_MIN_MASK_PIXELS = 85\n    SIFT_MIN_COVERAGE = 0.00035\n    SIFT_MAX_COVERAGE = 0.42\n\n    # CNN parameters\n    CNN_ENABLED = True\n    CNN_IMAGE_SIZE = 256\n    CNN_THRESHOLD = 0.5\n\n    # Ensemble\n    ENSEMBLE_MODE = 'weighted'\n    SIFT_WEIGHT = 0.75\n    CNN_WEIGHT = 0.25\n\nconfig = Config()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"ðŸŽ¯ HYBRID ENSEMBLE - SIFT + Fast CNN\")\nprint(f\" Device: {device}\")\n\n# =============================================================================\n# Dummy CNN Model (replace with your trained model)\n# =============================================================================\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(8,1)\n    def forward(self,x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = torch.sigmoid(self.fc(x))\n        return x\n\ncnn_model = SimpleCNN().to(device)\ncnn_model.eval()\n\n# =============================================================================\n# SIFT scoring\n# =============================================================================\ndef sift_score(img_path, ref_path):\n    img = cv2.imread(img_path, 0)\n    ref = cv2.imread(ref_path, 0)\n    if img is None or ref is None:\n        return 0.0\n    if config.USE_CLAHE:\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n        img = clahe.apply(img)\n        ref = clahe.apply(ref)\n    sift = cv2.SIFT_create(nfeatures=config.SIFT_FEATURES, contrastThreshold=config.SIFT_CONTRAST)\n    kp1, des1 = sift.detectAndCompute(img, None)\n    kp2, des2 = sift.detectAndCompute(ref, None)\n    if des1 is None or des2 is None:\n        return 0.0\n    bf = cv2.BFMatcher()\n    matches = bf.knnMatch(des1, des2, k=2)\n    good = []\n    for m,n in matches:\n        if m.distance < config.MATCH_RATIO * n.distance:\n            good.append(m)\n    score = len(good) / max(len(matches),1)\n    return float(score)\n\n# =============================================================================\n# CNN scoring\n# =============================================================================\ndef cnn_score(img_path):\n    img = Image.open(img_path).convert('RGB').resize((config.CNN_IMAGE_SIZE, config.CNN_IMAGE_SIZE))\n    img = np.array(img)/255.0\n    img = torch.tensor(img).permute(2,0,1).unsqueeze(0).float().to(device)\n    with torch.no_grad():\n        out = cnn_model(img).item()\n    return float(out)\n\n# =============================================================================\n# Generate Submission\n# =============================================================================\ndef generate_submission():\n    sub = pd.read_csv(config.SAMPLE_SUB)\n    preds = []\n    # Use first authentic image as reference\n    ref_img_name = os.listdir(config.TRAIN_IMAGES_AUTH)[0]\n    ref_path = os.path.join(config.TRAIN_IMAGES_AUTH, ref_img_name)\n\n    for img_name in tqdm(sub['case_id']):\n        img_name_str = str(img_name) + \".png\"  # Convert to string and add extension\n        test_path = os.path.join(config.TEST_IMAGES, img_name_str)\n        s_score = sift_score(test_path, ref_path)\n        c_score = cnn_score(test_path) if config.CNN_ENABLED else 0.0\n\n        # Ensemble\n        if config.ENSEMBLE_MODE == 'weighted':\n            final_score = config.SIFT_WEIGHT*s_score + config.CNN_WEIGHT*c_score\n        else: # voting\n            final_score = 1.0 if (s_score>0.5 or c_score>config.CNN_THRESHOLD) else 0.0\n        preds.append(final_score)\n\n    sub['annotation'] = preds\n    output_file = \"submission.csv\"\n    sub.to_csv(output_file, index=False)\n    print(f\"âœ… Submission saved to {output_file}\")\n\n# =============================================================================\n# MAIN\n# =============================================================================\nif __name__ == \"__main__\":\n    generate_submission()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:47:26.971601Z","iopub.execute_input":"2025-11-06T01:47:26.972315Z","iopub.status.idle":"2025-11-06T01:47:29.75874Z","shell.execute_reply.started":"2025-11-06T01:47:26.972294Z","shell.execute_reply":"2025-11-06T01:47:29.758067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}