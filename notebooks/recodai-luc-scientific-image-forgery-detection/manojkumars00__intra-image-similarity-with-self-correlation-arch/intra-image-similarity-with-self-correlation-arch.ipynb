{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# Summary\n## How forgery is being done in bio medical ?\n1. Copy & Move Forgey - Frequently used\n2. Splicing, Copying an entity from different image - Seems like dataset in this competition don't have this type of forged images.\n\n## What problem to solve ?\n1. In Copy & Move forgery the copied entity will be pasted in some other part of the same image with,\n2. * Rotation\n   * flipped vertically or horzintally\n   * No change in shape or size - As certain entity only can be in certain size.\n3. So we need to identify which entity is similar to which entity in a same image.","metadata":{"_uuid":"de462aa2-ea41-4ff2-9a87-55cb77e21042","_cell_guid":"9ce2773e-481d-48d8-b2f6-0ab78920f006","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Exisiting Research Papers References\n- https://pmc.ncbi.nlm.nih.gov/articles/PMC11111128/\n- https://arxiv.org/pdf/2311.13263","metadata":{"_uuid":"88bfe28f-0b83-49ad-ad3e-2188b87c3974","_cell_guid":"c84c3ef2-2f39-4438-b15c-ba9f666452e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Self Correlation Architecture\nWe are trying to build a model that can learn the intra image similarity to find the copy & moved entities in the image.\n\n*It is like giving the each pixel in image a mirror and asking each part to find its twin within the same image.*","metadata":{"_uuid":"ecf8ae42-fe96-4071-8c96-a5d2e54bbfd5","_cell_guid":"be46d8b1-58f7-4c70-bbea-00d07da589f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"_uuid":"e8eb125d-ef63-451d-b4cf-1fbb6e4f795f","_cell_guid":"bae468a0-09bb-4dd9-b0cb-10d5246fbacf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:39:12.885976Z","iopub.execute_input":"2025-11-10T17:39:12.886946Z","iopub.status.idle":"2025-11-10T17:39:33.435309Z","shell.execute_reply.started":"2025-11-10T17:39:12.886903Z","shell.execute_reply":"2025-11-10T17:39:33.434257Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing Deps","metadata":{"_uuid":"82d2bdcc-8c5d-464a-975d-885ba37721f2","_cell_guid":"148b9797-7568-4d35-8f51-8921359effc6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from transformers import SegformerImageProcessor, SegformerForImageClassification\nfrom collections import defaultdict\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch","metadata":{"_uuid":"f6683bfe-442f-401a-8127-e71078c83e5d","_cell_guid":"c922b5ff-ca71-4cc3-9b76-ffa7ab73250c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:39:33.437133Z","iopub.execute_input":"2025-11-10T17:39:33.437488Z","iopub.status.idle":"2025-11-10T17:40:04.92824Z","shell.execute_reply.started":"2025-11-10T17:39:33.43745Z","shell.execute_reply":"2025-11-10T17:40:04.927234Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Self Correlation Block","metadata":{"_uuid":"4248c1c4-1de4-4245-ba20-c1512d326751","_cell_guid":"6c93fda8-705d-445a-991b-d46346558c5a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class SelfCorrelation(nn.Module):\n    def __init__(self, topk: int = 8, chunk: int = 4096):\n        super().__init__()\n        self.topk = topk\n        self.chunk = chunk\n\n    def forward(self, x: torch.Tensor):\n        B, C, H, W = x.shape\n        N = H * W\n        x = F.normalize(x, p=2, dim=1)\n        q = x.view(B, C, N).permute(0, 2, 1)  # (B,N,C)\n        k = x.view(B, C, N)                   # (B,C,N)\n\n        # chunked top-k over columns to avoid materializing (B,N,N)\n        vals_chunks = []\n        for s in range(0, N, self.chunk):\n            e = min(s + self.chunk, N)\n            # sim_chunk: (B, (e-s), N)\n            sim_chunk = torch.matmul(q[:, s:e, :], k)\n            v, _ = torch.topk(sim_chunk, k=self.topk, dim=-1)\n            vals_chunks.append(v)  # [(B,chunk,K), ...]\n            del sim_chunk\n        vals = torch.cat(vals_chunks, dim=1)  # (B,N,K)\n        vals = vals.clamp_min(0)\n        vals = vals / (vals.norm(dim=-1, keepdim=True) + 1e-6)\n        corr_map = vals.transpose(1, 2).view(B, self.topk, H, W)\n        return corr_map\n","metadata":{"_uuid":"ff2a87bd-f08d-4619-a011-fe2ea88236c8","_cell_guid":"9bf32ef7-2bb7-4dc0-95fa-f4e2c3666c7e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:04.929519Z","iopub.execute_input":"2025-11-10T17:40:04.930342Z","iopub.status.idle":"2025-11-10T17:40:04.939018Z","shell.execute_reply.started":"2025-11-10T17:40:04.930307Z","shell.execute_reply":"2025-11-10T17:40:04.938119Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fusion of Multi Scale Self Correlation","metadata":{"_uuid":"7b8a38c7-2323-4e2b-8982-c888ec587a02","_cell_guid":"e8afa5dd-cc1b-4de8-9d8d-b23e50670b20","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MultiScaleCorrelationFusion(nn.Module):\n    def __init__(self, in_ch=8, out_ch=32):\n        super().__init__()\n        self.corr_1x1_conv1 = nn.Conv2d(in_ch, out_ch, (1, 1), 1, 0)\n        self.corr_1x1_conv2 = nn.Conv2d(in_ch, out_ch, (1, 1), 1, 0)\n        self.corr_1x1_conv3 = nn.Conv2d(in_ch, out_ch, (1, 1), 1, 0)\n        self.corr_1x1_conv4 = nn.Conv2d(in_ch, out_ch, (1, 1), 1, 0)\n        \n        self.conv1_3x3 = nn.Conv2d(out_ch, out_ch, (3, 3), 1, 1)\n        self.conv2_3x3 = nn.Conv2d(out_ch, out_ch, (3, 3), 1, 1)\n        self.conv3_3x3 = nn.Conv2d(out_ch, out_ch, (3, 3), 1, 1)\n\n        self.fuse_corr_conv = nn.Conv2d(out_ch*4, out_ch, (3, 3), 1, 1)\n\n        # PPM\n        self.ppm_convs = nn.ModuleList([\n            nn.Sequential(nn.AdaptiveAvgPool2d(b),\n                      nn.Conv2d(out_ch, out_ch, 1, 1, 0),\n                      nn.ReLU(inplace=True)\n            )\n            for b in [1, 2, 3, 6]\n        ])\n        self.ppm_fuse = nn.Conv2d(out_ch*5, out_ch, (1, 1), 1, 0)\n        self.out_act = nn.ReLU(inplace=True)\n\n\n    def _ppm(self, x):\n        B, C, H, W = x.shape\n        outs = [x]\n        for stage in self.ppm_convs:\n            y = stage(x)\n            y = F.interpolate(y, size=(H, W), mode='bilinear', align_corners=False)\n            outs.append(y)\n        x = torch.cat(outs, dim=1)\n        return self.ppm_fuse(x)\n\n    \n    def forward(self, C1, C2, C3, C4):\n        C1, C2, C3, C4 = F.gelu(self.corr_1x1_conv1(C1)), F.gelu(self.corr_1x1_conv2(C2)), F.gelu(self.corr_1x1_conv3(C3)), F.gelu(self.corr_1x1_conv4(C4))\n\n        C4 = self._ppm(C4)\n        c4 = F.interpolate(C4, size=C1.shape[-2:], mode='bilinear', align_corners=False)\n        C4_2x = F.interpolate(C4, size=C3.shape[-2:], mode='bilinear', align_corners=False)\n\n        C3_fuse = C3 + C4_2x\n        c3 = F.gelu(self.conv3_3x3(C3_fuse))\n        c3 = F.interpolate(c3, size=C1.shape[-2:], mode='bilinear', align_corners=False)\n        C3_2x = F.interpolate(C3_fuse, size=C2.shape[-2:], mode='bilinear', align_corners=False)\n\n        C2_fuse = C2 + C3_2x\n        c2 = F.gelu(self.conv2_3x3(C2_fuse))\n        c2 = F.interpolate(c2, size=C1.shape[-2:], mode='bilinear', align_corners=False)\n        C2_2x = F.interpolate(C2_fuse, size=C1.shape[-2:], mode='bilinear', align_corners=False)\n\n        C1_fuse = C1 + C2_2x\n        c1 = F.gelu(self.conv1_3x3(C1_fuse))\n\n        C_hat = torch.cat([c1, c2, c3, c4], dim=1)\n        C_hat = self.out_act(self.fuse_corr_conv(C_hat))\n        \n        return C_hat","metadata":{"_uuid":"e56025e7-3e35-4d01-9bc8-54118138a122","_cell_guid":"a0ba8b68-89d2-444c-ab1c-8c641281ac75","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:04.941011Z","iopub.execute_input":"2025-11-10T17:40:04.941292Z","iopub.status.idle":"2025-11-10T17:40:04.967697Z","shell.execute_reply.started":"2025-11-10T17:40:04.941259Z","shell.execute_reply":"2025-11-10T17:40:04.966773Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Single Cycle Fully Connected Block","metadata":{"_uuid":"37ea0315-1190-4c2f-bc9a-07f08ced8afb","_cell_guid":"197e4da7-b9dd-462c-aed7-2742d717e242","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class CycleFC(nn.Module):\n    \"\"\"\n    Cycle Fully-Connected (single branch).\n    - Shifts each channel by a small, channel-dependent offset (cycle/roll),\n      then applies a 1x1 conv across channels (W_mlp).\n    - Stepsize (SH, SW) determines the offset pattern; dilation scales the offsets.\n\n    Args:\n        in_ch:   input channels  (Cin)\n        out_ch:  output channels (Cout)\n        SH:      stepsize along height  (1 or 3 in the paper)\n        SW:      stepsize along width   (1 or 3 in the paper)\n        dilation: scale the spatial offset (e.g., 1, 6, 12, 18)\n        wrap:    True -> cyclic shift (torch.roll), False -> zero-padded shift\n    \"\"\"\n    def __init__(self, in_ch, out_ch, SH=3, SW=1, dilation=1, wrap=True):\n        super().__init__()\n        assert SH in (1, 3) and SW in (1, 3), \"Paper uses SH/SW ∈ {1,3}\"\n        self.in_ch = in_ch\n        self.out_ch = out_ch\n        self.SH, self.SW = SH, SW\n        self.dilation = dilation\n        self.wrap = wrap\n\n        # 1x1 conv = W_mlp (Cin -> Cout)\n        self.proj = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=True)\n\n        # Precompute channel groups that share the same (δm, δn)\n        # We use symmetric offsets: for SH=3 -> [-1, 0, 1]; for SH=1 -> [0].\n        m_offsets = [0] if SH == 1 else [-1, 0, 1]\n        n_offsets = [0] if SW == 1 else [-1, 0, 1]\n\n        # δm(c) = m_offsets[c % SH], δn(c) = n_offsets[(c // SH) % SW]\n        groups = defaultdict(list)\n        for c in range(in_ch):\n            dm = m_offsets[c % SH]\n            dn = n_offsets[(c // SH) % SW]\n            groups[(dm, dn)].append(c)\n\n        # Store grouped indices and their (dm, dn)\n        self.groups = []\n        for (dm, dn), idx_list in groups.items():\n            idx = torch.tensor(idx_list, dtype=torch.long)\n            self.groups.append((dm * dilation, dn * dilation, idx))\n\n    def _shift_zero_pad(self, x, sh, sw):\n        \"\"\"Shift x by (sh, sw) with zero padding (no wrap).\"\"\"\n        B, C, H, W = x.shape\n        pad_t = max(+sh, 0); pad_b = max(-sh, 0)\n        pad_l = max(+sw, 0); pad_r = max(-sw, 0)\n        x = F.pad(x, (pad_l, pad_r, pad_t, pad_b))\n        x = x[:, :, pad_t:pad_t+H, pad_l:pad_l+W]\n        return x\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, Cin, H, W)\n        returns: (B, Cout, H, W)\n        \"\"\"\n        B, C, H, W = x.shape\n        assert C == self.in_ch\n\n        # Build shifted tensor per channel (vectorized by groups)\n        x_shifted = torch.zeros_like(x)\n        for dm, dn, idx in self.groups:\n            xi = x[:, idx, :, :]\n            if self.wrap:\n                yi = torch.roll(xi, shifts=(dm, dn), dims=(2, 3))\n            else:\n                yi = self._shift_zero_pad(xi, dm, dn)\n            x_shifted[:, idx, :, :] = yi\n\n        # Channel mixing (W_mlp)\n        y = self.proj(x_shifted)   # (B, out_ch, H, W)\n        return y","metadata":{"_uuid":"fb4660b3-5c22-4479-a4ef-d169f3d123c6","_cell_guid":"11fd4dee-9d2b-4a59-94d7-bb8f75e44528","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:04.968785Z","iopub.execute_input":"2025-11-10T17:40:04.969854Z","iopub.status.idle":"2025-11-10T17:40:04.992857Z","shell.execute_reply.started":"2025-11-10T17:40:04.969817Z","shell.execute_reply":"2025-11-10T17:40:04.991811Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi Scale Cycle Fully Connected Block","metadata":{"_uuid":"a12796dd-c465-4936-b7d1-7e6ec88f85c5","_cell_guid":"8430603f-a3a7-465b-bf42-e7db5d45c686","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MultiScaleCycleFC(nn.Module):\n    \"\"\"\n    Multi-scale Cycle-FC block (Eq. 13):\n      Ce = Conv3x3( Cb + f_linear( sum_r beta_r * CycleFC_r(Cb) ) )\n    Uses 9 branches:\n      - 1x3 with dilations {1,6,12,18}\n      - 3x1 with dilations {1,6,12,18}\n      - 1x1 (no shift)\n    All branches map in_ch -> in_ch so they can be summed and residually added to Cb.\n    \"\"\"\n    def __init__(self, ch, dilations=(1, 6, 12, 18)):\n        super().__init__()\n        self.ch = ch\n\n        # 8 directional branches with different dilations\n        self.branches = nn.ModuleList()\n        for d in dilations:\n            self.branches.append(CycleFC(ch, ch, SH=1, SW=3, dilation=d, wrap=True))  # horizontal\n        for d in dilations:\n            self.branches.append(CycleFC(ch, ch, SH=3, SW=1, dilation=d, wrap=True))  # vertical\n\n        # 1×1 branch (no spatial shift); simple 1x1 conv\n        self.branch_1x1 = nn.Conv2d(ch, ch, kernel_size=1, bias=True)\n\n        # Learnable weights β_r for the 9 branches (8 + 1)\n        self.betas = nn.Parameter(torch.ones(9, dtype=torch.float32))\n\n        # f_linear: channelwise linear transform (1x1 conv)\n        self.f_linear = nn.Conv2d(ch, ch, kernel_size=1, bias=True)\n\n        # Final 3x3 conv + ReLU\n        self.post = nn.Conv2d(ch, ch, kernel_size=3, padding=1, bias=True)\n        self.out_act = nn.ReLU(inplace=True)\n\n    def forward(self, Cb):\n        \"\"\"\n        Cb: (B, ch, H, W)   # fused correlation feature from your HFI/FPN block\n        returns: Ce (B, ch, H, W)\n        \"\"\"\n        outs = []\n\n        # 8 directional branches\n        for i, layer in enumerate(self.branches):\n            yi = layer(Cb)                      # (B, ch, H, W)\n            outs.append(self.betas[i] * yi)\n\n        # 1x1 branch (index 8)\n        y9 = self.branch_1x1(Cb)               # (B, ch, H, W)\n        outs.append(self.betas[8] * y9)\n\n        # Sum of weighted branches, linear proj, residual add, 3x3 refine\n        Y = torch.stack(outs, dim=0).sum(dim=0)      # (B, ch, H, W)\n        Y = self.f_linear(Y)                         # (B, ch, H, W)\n        Ce = Cb + Y\n        Ce = F.relu(self.post(Ce), inplace=True)     # final correlation map reinforced by Cycle-FC\n        return Ce","metadata":{"_uuid":"5e4e14bb-bee1-41de-996c-d8f43a9500ba","_cell_guid":"d46189e9-36ea-4b59-841a-0b9b7a2a1d40","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:04.993892Z","iopub.execute_input":"2025-11-10T17:40:04.994215Z","iopub.status.idle":"2025-11-10T17:40:05.021064Z","shell.execute_reply.started":"2025-11-10T17:40:04.994185Z","shell.execute_reply":"2025-11-10T17:40:05.02006Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Mask Constructing Block","metadata":{"_uuid":"b7b3ce8c-342b-4e40-8057-3122438e64fd","_cell_guid":"19cacbe5-8ee8-4a2d-8c37-e50f74e45377","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MaskReconstruction(nn.Module):\n    \"\"\"\n    Paper Eq. (14)-(15): \n      M = Conv1x1(  up2( ReLU(Conv1x1( up2( ReLU(Conv1x1( up2( ReLU(Conv1x1(Ce)) ))) ))) )  )\n    - Default n_upsamples=3 -> overall ×8 (H/8 → H).\n    - Returns logits by default; apply softmax outside if you use CrossEntropyLoss.\n    \"\"\"\n    def __init__(self, in_ch: int, num_classes: int = 2, n_upsamples: int = 3, apply_softmax: bool = False):\n        super().__init__()\n        self.n_upsamples = n_upsamples\n        self.apply_softmax = apply_softmax\n\n        # Three 1×1 convs (each followed by ReLU in forward) as in the paper\n        self.pre_convs = nn.ModuleList([nn.Conv2d(in_ch, in_ch, kernel_size=1) \n                                        for _ in range(n_upsamples)])\n        # Final 1×1 \"segmentation\" conv to 2 classes\n        self.conv_seg = nn.Conv2d(in_ch, num_classes, kernel_size=1)\n\n    def forward(self, Ce: torch.Tensor):\n        \"\"\"\n        Args:\n            Ce: (B, C, Hc, Wc) — correlation feature (e.g., C_e from Cycle-FC).\n                 If Hc=H/8 and n_upsamples=3, output will be (B, 2, H, W).\n        Returns:\n            logits or softmax probs: (B, num_classes, Hout, Wout)\n        \"\"\"\n        x = Ce\n        for conv in self.pre_convs:\n            x = F.relu(conv(x), inplace=True)\n            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n\n        logits = self.conv_seg(x)\n        if self.apply_softmax:\n            return F.softmax(logits, dim=1)\n        return logits","metadata":{"_uuid":"fa470e6c-8c6a-47fa-9c35-ee5ff3ec318a","_cell_guid":"14f0bf60-878f-4151-a613-d42f426df85d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.021988Z","iopub.execute_input":"2025-11-10T17:40:05.022292Z","iopub.status.idle":"2025-11-10T17:40:05.045001Z","shell.execute_reply.started":"2025-11-10T17:40:05.022262Z","shell.execute_reply":"2025-11-10T17:40:05.044002Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Instance Segmentation Head","metadata":{"_uuid":"0f785d2e-088a-43c2-8349-c2492499459c","_cell_guid":"90b8e71f-3938-40a5-b0e8-c4b7045475a0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class PositionalEncoding2D(nn.Module):\n    \"\"\"\n    Additive 2D sine/cos positional encoding.\n    Output channel = d_model. Must be divisible by 4.\n    \"\"\"\n    def __init__(self, d_model: int):\n        super().__init__()\n        assert d_model % 4 == 0, \"d_model must be divisible by 4\"\n        self.d_model = d_model\n        self.num_feats = d_model // 4\n\n    def forward(self, B: int, H: int, W: int, device=None):\n        # normalized [0,1] coordinates\n        y = torch.linspace(0, 1, steps=H, device=device).unsqueeze(1).repeat(1, W)  # (H,W)\n        x = torch.linspace(0, 1, steps=W, device=device).unsqueeze(0).repeat(H, 1)  # (H,W)\n\n        dim_t = torch.arange(self.num_feats, device=device).float()\n        dim_t = 10000 ** (2 * (dim_t // 2) / self.num_feats)\n\n        pos_x = x[..., None] / dim_t  # (H,W,F)\n        pos_y = y[..., None] / dim_t\n\n        pos_x = torch.stack([pos_x.sin(), pos_x.cos()], dim=-1).flatten(-2)  # (H,W,2F)\n        pos_y = torch.stack([pos_y.sin(), pos_y.cos()], dim=-1).flatten(-2)  # (H,W,2F)\n\n        pos = torch.cat([pos_y, pos_x], dim=-1)          # (H,W,4F=d_model)\n        pos = pos.permute(2, 0, 1).unsqueeze(0)          # (1,d_model,H,W)\n        return pos.repeat(B, 1, 1, 1)                    # (B,d_model,H,W)","metadata":{"_uuid":"9f8efc09-a0a1-40bf-8474-ebe540c003e7","_cell_guid":"502f4173-7460-47f0-8d54-ad01f085d0b7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.045913Z","iopub.execute_input":"2025-11-10T17:40:05.046161Z","iopub.status.idle":"2025-11-10T17:40:05.062375Z","shell.execute_reply.started":"2025-11-10T17:40:05.04614Z","shell.execute_reply":"2025-11-10T17:40:05.061385Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FeatureTokenizer(nn.Module):\n    \"\"\"\n    Projects Ce (B,C,Hc,Wc) to transformer width H,\n    adds positional encodings, and flattens to tokens (B, N, H) where N=Hc*Wc.\n    \"\"\"\n    def __init__(self, in_ch: int, hidden_dim: int):\n        super().__init__()\n        self.input_proj = nn.Conv2d(in_ch, hidden_dim, kernel_size=1)\n        self.pos_enc    = PositionalEncoding2D(hidden_dim)\n\n    def forward(self, Ce: torch.Tensor):\n        # Ce: (B,C,Hc,Wc)\n        B, _, Hc, Wc = Ce.shape\n        device = Ce.device\n\n        src = self.input_proj(Ce)                         # (B,H,Hc,Wc)\n        pos = self.pos_enc(B, Hc, Wc, device=device)      # (B,H,Hc,Wc)\n\n        tokens = (src + pos).flatten(2).permute(0, 2, 1)  # (B,N,H), N=Hc*Wc\n        return tokens, src                                # tokens for decoder, src for pixel embeddings","metadata":{"_uuid":"ee94ff06-e228-4af3-9248-c41dd4ba829f","_cell_guid":"b501fd06-009d-4cfa-baf4-4b0d78068c5e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.063264Z","iopub.execute_input":"2025-11-10T17:40:05.063545Z","iopub.status.idle":"2025-11-10T17:40:05.086978Z","shell.execute_reply.started":"2025-11-10T17:40:05.063516Z","shell.execute_reply":"2025-11-10T17:40:05.086038Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LearnableQueries(nn.Module):\n    \"\"\"\n    Table of Q learnable query vectors, tiled per batch.\n    \"\"\"\n    def __init__(self, num_queries: int, hidden_dim: int):\n        super().__init__()\n        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n        self.num_queries = num_queries\n        self.hidden_dim  = hidden_dim\n\n    def forward(self, B: int, device=None):\n        # (B,Q,H)\n        q = self.query_embed.weight.unsqueeze(0).repeat(B, 1, 1)\n        if device is not None and q.device != device:\n            q = q.to(device)\n        return q","metadata":{"_uuid":"e56461d1-46f7-49c1-882c-0a68f8eaebd4","_cell_guid":"e17e4dbc-6f2d-426b-bbfc-4b07d68a9215","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.089681Z","iopub.execute_input":"2025-11-10T17:40:05.089962Z","iopub.status.idle":"2025-11-10T17:40:05.113158Z","shell.execute_reply.started":"2025-11-10T17:40:05.089943Z","shell.execute_reply":"2025-11-10T17:40:05.112235Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QueryDecoder(nn.Module):\n    \"\"\"\n    A small Transformer decoder: self-attn on queries + cross-attn to tokens.\n    \"\"\"\n    def __init__(self, hidden_dim: int, nheads: int = 8, num_layers: int = 3, ff_mult: int = 4, dropout: float = 0.1):\n        super().__init__()\n        layer = nn.TransformerDecoderLayer(\n            d_model=hidden_dim,\n            nhead=nheads,\n            dim_feedforward=hidden_dim * ff_mult,\n            dropout=dropout,\n            activation=\"relu\",\n            batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(layer, num_layers=num_layers)\n\n    def forward(self, queries: torch.Tensor, tokens: torch.Tensor):\n        \"\"\"\n        queries: (B,Q,H)\n        tokens:  (B,N,H)\n        returns: (B,Q,H)\n        \"\"\"\n        hs = self.decoder(tgt=queries, memory=tokens)  # (B,Q,H)\n        return hs","metadata":{"_uuid":"5f9328be-7d70-4664-9069-4c1914f69480","_cell_guid":"1346e7fc-740a-40c2-b535-5c2b52f09027","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.113953Z","iopub.execute_input":"2025-11-10T17:40:05.114202Z","iopub.status.idle":"2025-11-10T17:40:05.132814Z","shell.execute_reply.started":"2025-11-10T17:40:05.114182Z","shell.execute_reply":"2025-11-10T17:40:05.13164Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ClassAndMaskHeads(nn.Module):\n    \"\"\"\n    - Class head: per-query logits for 2 classes (forgery / no-object).\n    - Mask head: query weights + per-pixel embeddings -> mask logits.\n    - Optional multi-stage x2 upscaling on the pixel embeddings before dot-product.\n      This preserves detail much better than just upsampling mask logits.\n    \"\"\"\n    def __init__(self, hidden_dim: int, mask_embed_dim: int,\n                 upsample_stages: int = 0,                    # e.g., 2 for 128->256->512\n                 use_depthwise_sharpen: bool = False):\n        super().__init__()\n        # class head\n        self.class_head = nn.Linear(hidden_dim, 2)\n\n        # mask head\n        self.mask_embed_mlp = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim), \n            nn.GELU(),\n            nn.Linear(hidden_dim, mask_embed_dim)\n        )\n        self.pixel_decoder  = nn.Conv2d(hidden_dim, mask_embed_dim, kernel_size=1)\n\n        # optional feature upscaler (acts on pixel embeddings)\n        self.upsample_stages = nn.ModuleList()\n        self.use_depthwise_sharpen = use_depthwise_sharpen\n        for _ in range(upsample_stages):\n            stage = []\n            stage += [nn.Conv2d(mask_embed_dim, mask_embed_dim, kernel_size=1),\n                      nn.ReLU(inplace=True)]\n            if use_depthwise_sharpen:\n                # lightweight edge sharpening after upsample\n                stage += [nn.Conv2d(mask_embed_dim, mask_embed_dim, kernel_size=3, padding=1,\n                                    groups=mask_embed_dim, bias=True)]\n            self.upsample_stages.append(nn.Sequential(*stage))\n\n    def _refine_pixel_embeddings(self, pixel_emb: torch.Tensor) -> torch.Tensor:\n        x = pixel_emb\n        for stage in self.upsample_stages:\n            x = stage(x)\n            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        return x\n\n    def forward(self, hs: torch.Tensor, src: torch.Tensor,\n                final_size: tuple | None = None):\n        \"\"\"\n        hs:  (B,Q,H)      decoded queries\n        src: (B,H,Hc,Wc)  projected feature map (same H as hs)\n        final_size: optionally force an exact (H_out, W_out) after upscaling\n        returns:\n          class_logits: (B,Q,2)\n          mask_logits:  (B,Q,H*,W*) where H*,W* reflect upscaling\n        \"\"\"\n        B, Q, H = hs.shape\n        _, _, Hc, Wc = src.shape\n\n        # 1) class logits per query\n        class_logits = self.class_head(hs)                 # (B,Q,2)\n\n        # 2) query mask embeddings and pixel embeddings\n        mask_embed = self.mask_embed_mlp(hs)               # (B,Q,E)\n        pixel_emb  = self.pixel_decoder(src)               # (B,E,Hc,Wc)\n\n        # 3) (recommended) refine features then recompute masks\n        pixel_emb = self._refine_pixel_embeddings(pixel_emb)  # (B,E,H↑,W↑)\n        if final_size is not None:\n            pixel_emb = F.interpolate(pixel_emb, size=final_size,\n                                      mode='bilinear', align_corners=False)\n\n        mask_embed = F.normalize(mask_embed, dim=-1)\n        pixel_emb  = F.normalize(pixel_emb,  dim=1)\n\n        # 4) dot-product: per-query masks at high-res\n        mask_logits = torch.einsum('bqc,bchw->bqhw', mask_embed, pixel_emb)  # (B,Q,H↑,W↑)\n        return class_logits, mask_logits","metadata":{"_uuid":"fc52154d-af75-480a-9391-e4692dfd167c","_cell_guid":"686a9985-2600-40e8-af28-ded27a921277","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.133962Z","iopub.execute_input":"2025-11-10T17:40:05.13429Z","iopub.status.idle":"2025-11-10T17:40:05.160471Z","shell.execute_reply.started":"2025-11-10T17:40:05.134262Z","shell.execute_reply":"2025-11-10T17:40:05.159449Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QueryInstanceHead(nn.Module):\n    \"\"\"\n    Architecture-only: from Ce -> class logits & mask logits per query.\n    \"\"\"\n    def __init__(self, in_ch: int, num_queries: int = 50, hidden_dim: int = 256,\n                 num_decoder_layers: int = 3, mask_embed_dim: int = 256, nheads: int = 8):\n        super().__init__()\n        self.tokenizer   = FeatureTokenizer(in_ch, hidden_dim)\n        self.queries     = LearnableQueries(num_queries, hidden_dim)\n        self.decoder     = QueryDecoder(hidden_dim, nheads=nheads, num_layers=num_decoder_layers)\n        self.heads       = ClassAndMaskHeads(hidden_dim, mask_embed_dim, upsample_stages=2)\n\n    def forward(self, Ce: torch.Tensor):\n        \"\"\"\n        Ce: (B,C,Hc,Wc)\n        returns:\n          class_logits: (B,Q,2)\n          mask_logits:  (B,Q,Hc,Wc)\n        \"\"\"\n        tokens, src = self.tokenizer(Ce)                  # tokens: (B,N,H), src: (B,H,Hc,Wc)\n        q = self.queries(B=Ce.size(0), device=Ce.device)  # (B,Q,H)\n        hs = self.decoder(q, tokens)                      # (B,Q,H)\n        \n        class_logits, mask_logits = self.heads(hs, src)   # (B,Q,2), (B,Q,Hc,Wc)\n        return {\"class_logits\": class_logits, \"mask_logits\": mask_logits}","metadata":{"_uuid":"c48c27ad-4e59-48b4-9281-144174e0cfa3","_cell_guid":"3ab1d8ba-a48d-484a-8c56-80a34fb0bed8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.161623Z","iopub.execute_input":"2025-11-10T17:40:05.161902Z","iopub.status.idle":"2025-11-10T17:40:05.184067Z","shell.execute_reply.started":"2025-11-10T17:40:05.161855Z","shell.execute_reply":"2025-11-10T17:40:05.182747Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Full Model For CMFD","metadata":{"_uuid":"a90a93a3-f229-4ed1-81a2-22dcb0951744","_cell_guid":"898bedd0-9dff-4f3e-b8e8-14c929765bd8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Sematic Segmentation Model","metadata":{}},{"cell_type":"code","source":"class SematicCmfdModel(nn.Module):\n    def __init__(self, backbone_name=\"nvidia/mit-b2\", topK=8, apply_softmax=True):\n        super().__init__()\n        self.image_processor = SegformerImageProcessor.from_pretrained(backbone_name)\n        self.encoder_model = SegformerForImageClassification.from_pretrained(backbone_name)\n\n        self.selfCorr = SelfCorrelation(topk=topK)\n\n        self.multiScaleCorrFusion = MultiScaleCorrelationFusion()\n        self.multi_scale_fc_cycle = MultiScaleCycleFC(ch=topK*4, dilations=(1,6,12,18))\n        self.mask_construction = MaskReconstruction(in_ch=topK*4, num_classes=2, n_upsamples=2, apply_softmax=apply_softmax)\n        # self.instance_mask_head = QueryInstanceHead(in_ch=topK*4, num_queries=n_instances, hidden_dim=decoder_hidden_dim, num_decoder_layers=num_decoder_layers, mask_embed_dim=decoder_mask_embed_dim, nheads=decoder_nheads)\n    \n    \n    def forward(self, x):\n        outputs = self.encoder_model(x, output_hidden_states=True)\n\n        corr1 = self.selfCorr(outputs['hidden_states'][0])\n        corr2 = self.selfCorr(outputs['hidden_states'][1])\n        corr3 = self.selfCorr(outputs['hidden_states'][2])\n        corr4 = self.selfCorr(outputs['hidden_states'][3])\n\n        multi_scale_self_corr_fusion = self.multiScaleCorrFusion(corr1, corr2, corr3, corr4)\n\n        Ce = self.multi_scale_fc_cycle(multi_scale_self_corr_fusion)\n\n        out = self.mask_construction(Ce)\n        # out = self.instance_mask_head(Ce)\n\n        return out","metadata":{"_uuid":"f55c70ba-388a-4ece-a2ef-65a253643c8d","_cell_guid":"fec78640-c736-45dc-973f-eb8e595767ae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-10T17:40:05.185245Z","iopub.execute_input":"2025-11-10T17:40:05.185598Z","iopub.status.idle":"2025-11-10T17:40:05.205007Z","shell.execute_reply.started":"2025-11-10T17:40:05.185564Z","shell.execute_reply":"2025-11-10T17:40:05.203977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Instance Segmentation Model","metadata":{}},{"cell_type":"code","source":"class CmfdInstanceModel(nn.Module):\n    def __init__(self, sematic_encoder_model, backbone_name=\"nvidia/mit-b2\", topK=8, n_instances=7, decoder_hidden_dim=64, num_decoder_layers=3, decoder_mask_embed_dim=128, decoder_nheads=8):\n        super().__init__()\n        self.image_processor = SegformerImageProcessor.from_pretrained(backbone_name)\n        self.encoder_model = sematic_encoder_model\n        self.instance_mask_head = QueryInstanceHead(in_ch=topK*4, num_queries=n_instances, hidden_dim=decoder_hidden_dim, num_decoder_layers=num_decoder_layers, mask_embed_dim=decoder_mask_embed_dim, nheads=decoder_nheads)\n        \n        # Freeze the encoder parameters\n        for param in self.encoder_model.parameters():\n            param.requires_grad = False\n        print(\"Encoder model frozen.\")\n        \n        # Placeholder for intermediate output\n        self._features = None\n\n        # Register forward hook to capture the output of `multi_scale_fc_cycle`\n        target_layer = dict(self.encoder_model.named_modules()).get(\"multi_scale_fc_cycle\")\n        if target_layer is None:\n            raise ValueError(\"Layer 'multi_scale_fc_cycle' not found in encoder_model.\")\n        \n        def hook_fn(module, input, output):\n            self._features = output\n\n        target_layer.register_forward_hook(hook_fn)\n        print(\"Hook registered on layer 'multi_scale_fc_cycle'. Model ready for inference.\")\n\n    def forward(self, x):\n        with torch.no_grad():\n            _ = self.encoder_model(x)\n\n        if self._features is None:\n            raise RuntimeError(\"Hook did not capture features from 'multi_scale_fc_cycle'.\")\n\n        # pass features to the instance head\n        outputs = self.instance_mask_head(self._features)\n\n        # optionally return both for debugging; or just return mask_logits\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:40:05.206468Z","iopub.execute_input":"2025-11-10T17:40:05.207173Z","iopub.status.idle":"2025-11-10T17:40:05.229042Z","shell.execute_reply.started":"2025-11-10T17:40:05.207137Z","shell.execute_reply":"2025-11-10T17:40:05.22792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sample Input","metadata":{"_uuid":"23efc092-b27b-4ea9-8f03-18fc5f80bcce","_cell_guid":"4a5a0d70-ca77-4d37-bf53-5952232c2072","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"if __name__==\"__main__\":\n    from PIL import Image\n    import numpy as np\n    import requests\n\n    def count_parameters(model):\n        total = sum(p.numel() for p in model.parameters())\n        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        return total, trainable\n    \n        \n    backbone_name = \"nvidia/mit-b2\"\n    sematic_model = SematicCmfdModel(backbone_name=backbone_name, topK=8, apply_softmax=True)\n    instance_model = CmfdInstanceModel(sematic_model, backbone_name=backbone_name)\n    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n    image = Image.open(requests.get(url, stream=True).raw)\n    image = np.array(image)\n    image = sematic_model.image_processor(image)['pixel_values']\n    image = torch.tensor(image)\n    sematic_out = sematic_model(image)\n    instance_out = instance_model(image)\n\n    \n    print(\"\\nSematic Sample\")    \n    print(f\"Shape of the input images: {image.shape}\")\n    print(f\"Shape of the Mask: {sematic_out.shape}\")\n    total, trainable = count_parameters(sematic_model)\n    print(f\"Number of parameters in sematic model: {total}\")\n\n    \n    print(\"\\nSInstance Sample\")    \n    print(f\"Shape of the input images: {image.shape}\")\n    print(f\"Shape of the Mask Labels: {instance_out['class_logits'].shape}\")\n    print(f\"Shape of the Masks: {instance_out['mask_logits'].shape}\")\n    total, trainable = count_parameters(instance_model)\n    print(f\"Number of parameters in instance model: {total}\")    \n    ","metadata":{"_uuid":"18e939bf-4345-4018-96d4-57c4aea54ed6","_cell_guid":"06280ec0-9c80-42bc-9f25-b5d4c3ab2970","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-10T17:40:05.230121Z","iopub.execute_input":"2025-11-10T17:40:05.230462Z","iopub.status.idle":"2025-11-10T17:40:15.215787Z","shell.execute_reply.started":"2025-11-10T17:40:05.230429Z","shell.execute_reply":"2025-11-10T17:40:15.214769Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training\n- Notebook Link: https://www.kaggle.com/code/manojkumars00/intra-image-similarity-learning","metadata":{"_uuid":"1c59ea86-7a5a-408e-8842-fa1256fe0504","_cell_guid":"9c2660fe-e4d0-4f83-882a-15e7042aabda","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"---","metadata":{"_uuid":"bce4e8ac-e5c9-48cd-b69e-a7ea11178b0d","_cell_guid":"b9c6db43-8df0-4668-842f-5d68be47519e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}