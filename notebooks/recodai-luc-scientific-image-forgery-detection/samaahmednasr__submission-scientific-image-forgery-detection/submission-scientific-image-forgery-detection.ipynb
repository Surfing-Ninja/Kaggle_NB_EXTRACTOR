{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"},{"sourceId":621632,"sourceType":"modelInstanceVersion","modelInstanceId":464638,"modelId":480460}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Scientific Image Forgery ‚Äî **Submission Notebook (Inference Only)**\n\nThis notebook **loads a trained model** (TorchScript) and **generates `submission.csv`** for the competition.\n\n- No training here ‚Äî pure inference.  \n- Follows the competition rule:  \n  - Output `\"authentic\"` if no forged region is detected.  \n  - Otherwise, output **RLE-encoded instance masks** using the **official encoding** (semicolons between instances).\n","metadata":{}},{"cell_type":"markdown","source":"## 1) Environment Report","metadata":{}},{"cell_type":"code","source":"\nimport sys, platform, torch, numpy as np, pandas as pd\nprint(\"Python:\", sys.version.split()[0])\nprint(\"OS:\", platform.platform())\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:52:51.496017Z","iopub.execute_input":"2025-10-28T07:52:51.496703Z","iopub.status.idle":"2025-10-28T07:52:56.368845Z","shell.execute_reply.started":"2025-10-28T07:52:51.496668Z","shell.execute_reply":"2025-10-28T07:52:56.367991Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2) Paths & Parameters","metadata":{}},{"cell_type":"code","source":"\nfrom pathlib import Path\nimport os\n\n# =========================\n# EDIT THESE TWO DIRECTORIES\n# =========================\nCOMP_DIR   = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\nTEST_DIR   = f\"{COMP_DIR}/test_images\"\n\n# Model artifact directory (where you uploaded your .pt)\nMODEL_DIR  = \"/kaggle/input/scientific-image-forgery-detection/pytorch/default/7\" \nMODEL_FILE = \"model_deeplabv3_fold0_ts.pt\"               # TorchScript file saved earlier\n\n# Output\nOUT_DIR = \"/kaggle/working\"\nos.makedirs(OUT_DIR, exist_ok=True)\nSUB_PATH = f\"{OUT_DIR}/submission.csv\"\n\n# Inference params\nIMAGE_SIZE  = 512\nTHRESHOLD   = 0.5\nMIN_AREA    = 64\nUSE_TTA     = True\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"TEST_DIR :\", TEST_DIR)\nprint(\"MODEL_DIR:\", MODEL_DIR)\nprint(\"MODEL_FILE:\", MODEL_FILE)\nprint(\"Saving to:\", SUB_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:52:56.369992Z","iopub.execute_input":"2025-10-28T07:52:56.370311Z","iopub.status.idle":"2025-10-28T07:52:56.37705Z","shell.execute_reply.started":"2025-10-28T07:52:56.370295Z","shell.execute_reply":"2025-10-28T07:52:56.375445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3) Utilities ‚Äî RLE, Preprocessing, Post-processing","metadata":{}},{"cell_type":"code","source":"\nimport json\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport torch\n\n# --- Official RLE encode from the competition metric (user-provided) ---\ntry:\n    import numba\n    from numba import njit\nexcept Exception as e:\n    numba = None\n    def njit(*args, **kwargs):\n        def deco(f): return f\n        return deco\n\n@njit\ndef _rle_encode_jit(x: np.ndarray, fg_val: int = 1) -> list:\n    dots = np.where(x.T.flatten() == fg_val)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef rle_encode(masks: list[np.ndarray], fg_val: int = 1) -> str:\n    return ';'.join([json.dumps(_rle_encode_jit(m.astype(np.uint8), fg_val)) for m in masks])\n\n\ndef load_rgb(path: str) -> Image.Image:\n    return Image.open(path).convert(\"RGB\")\n\ndef preprocess_pil(img: Image.Image, size: int) -> torch.Tensor:\n    img_r = img.resize((size, size), resample=Image.BILINEAR)\n    x = torch.from_numpy(np.array(img_r, dtype=np.float32) / 255.0).permute(2,0,1).unsqueeze(0)\n    return x\n\ndef predict_prob(model, img: Image.Image, size: int, tta: bool = True) -> np.ndarray:\n    x = preprocess_pil(img, size).to(DEVICE)\n    with torch.no_grad():\n        logits = model(x)  # TorchScript logits [B,1,H,W]\n        prob = torch.sigmoid(logits)[0,0]\n        if tta:\n            xh = torch.flip(x, dims=[3])\n            ph = torch.sigmoid(model(xh))[0,0]; ph = torch.flip(ph, dims=[1])\n            xv = torch.flip(x, dims=[2])\n            pv = torch.sigmoid(model(xv))[0,0]; pv = torch.flip(pv, dims=[0])\n            prob = (prob + ph + pv) / 3.0\n        return prob.detach().cpu().numpy()\n\ndef prob_to_instances(prob: np.ndarray, thr: float = 0.5, min_area: int = 64) -> list[np.ndarray]:\n    mask = (prob > thr).astype(np.uint8)\n    if mask.sum() < min_area:\n        return []\n    num, lbl, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n    insts = []\n    for i in range(1, num):\n        area = int(stats[i, cv2.CC_STAT_AREA])\n        if area >= min_area:\n            inst = (lbl == i).astype(np.uint8)\n            insts.append(inst)\n    return insts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:52:56.377926Z","iopub.execute_input":"2025-10-28T07:52:56.378166Z","iopub.status.idle":"2025-10-28T07:52:57.965333Z","shell.execute_reply.started":"2025-10-28T07:52:56.378145Z","shell.execute_reply":"2025-10-28T07:52:57.964648Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4) Load TorchScript Model","metadata":{}},{"cell_type":"code","source":"\nimport torch, os\nts_path = str(Path(MODEL_DIR) / MODEL_FILE)\nassert os.path.exists(ts_path), f\"Model file not found: {ts_path}\"\nmodel = torch.jit.load(ts_path, map_location=DEVICE).eval()\ntry:\n    for p in model.parameters():\n        p.requires_grad_(False)\nexcept Exception:\n    pass\nprint(\"Loaded TorchScript model from:\", ts_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:52:57.966797Z","iopub.execute_input":"2025-10-28T07:52:57.967122Z","iopub.status.idle":"2025-10-28T07:53:00.112582Z","shell.execute_reply.started":"2025-10-28T07:52:57.9671Z","shell.execute_reply":"2025-10-28T07:53:00.111841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5) Run Inference on Test Set and Build `submission.csv`","metadata":{}},{"cell_type":"code","source":"# üß™ Inference + Submission (orig + H/V flip TTA merge) with low-confidence \"authentic\" gate\nimport os, glob, json\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom PIL import ImageOps\nimport cv2\n\n# --- Sanity checks for required globals from previous cells ---\nneeded_syms = [\n    \"load_rgb\", \"predict_prob\", \"prob_to_instances\", \"rle_encode\",\n    \"IMAGE_SIZE\", \"USE_TTA\", \"THRESHOLD\", \"MIN_AREA\",\n    \"TEST_DIR\", \"COMP_DIR\", \"model\"\n]\nfor _n in needed_syms:\n    assert _n in globals(), f\"Missing `{_n}`. Run the earlier setup/inference utility cells.\"\n\n# Output path (define if not set)\nif \"SUB_PATH\" not in globals():\n    OUT_DIR = \"/kaggle/working\"\n    os.makedirs(OUT_DIR, exist_ok=True)\n    SUB_PATH = str(Path(OUT_DIR) / \"submission.csv\")\n\n# ---- TTA + merge helpers ----\ndef prob_tta_merged(pil_img, size, tta):\n    \"\"\"Return merged prob map (resized grid, e.g., IMAGE_SIZE√óIMAGE_SIZE) using mean(orig, H, V).\"\"\"\n    # original\n    p_orig = predict_prob(model, pil_img, size=size, tta=tta)  # (S,S) float in [0,1]\n\n    # horizontal flip (mirror) -> unflip back\n    p_h = predict_prob(model, ImageOps.mirror(pil_img), size=size, tta=tta)\n    p_h = np.fliplr(p_h)\n\n    # vertical flip -> unflip back\n    p_v = predict_prob(model, ImageOps.flip(pil_img), size=size, tta=tta)\n    p_v = np.flipud(p_v)\n\n    # merge: mean (switch to np.maximum.reduce([...]) for higher recall if needed)\n    p_m = (p_orig + p_h + p_v) / 3.0\n    return np.clip(p_m, 0.0, 1.0), p_orig, p_h, p_v\n\n# ---- Low-confidence \"authentic\" gate (ONLY for deciding authentic vs forged) ----\n# If both \"max prob\" and \"coverage above a low viz threshold\" are tiny, force authentic.\nLOW_CONF_MAX_PROB   = 0.06       # if the whole map is below ~6% prob ‚Üí suspiciously low\nLOW_VIZ_THR         = 0.04       # visualize/coverage threshold for \"is there anything at all?\"\nLOW_CONF_MIN_PIXELS = 256        # at resized grid (IMAGE_SIZE√óIMAGE_SIZE)\n\ndef is_low_confidence(prob_resized):\n    \"\"\"Return True if probabilities look too weak/rare to trust.\"\"\"\n    if float(prob_resized.max()) >= LOW_CONF_MAX_PROB:\n        return False\n    # Otherwise also check coverage at a small threshold (avoid 1-2 speckles)\n    cover = int((prob_resized >= LOW_VIZ_THR).sum())\n    return cover < LOW_CONF_MIN_PIXELS\n\n# --- Collect test images ---\ntest_paths = sorted(glob.glob(str(Path(TEST_DIR) / \"*\")))\nprint(\"Test images:\", len(test_paths))\n\nrows = []\n\n# If there are test images, run inference\nif len(test_paths) > 0:\n    for p in test_paths:\n        case_id = Path(p).stem  # keep as string; we'll align types later\n        img = load_rgb(p)\n\n        # 1) Get merged probability at resized grid (consistent with your pipeline)\n        prob_resized, p_o, p_h, p_v = prob_tta_merged(img, size=IMAGE_SIZE, tta=USE_TTA)\n\n        # 2) Low-confidence gate: if too low ‚Üí authentic\n        if is_low_confidence(prob_resized):\n            annot = \"authentic\"\n        else:\n            # 3) Normal instance extraction at your standard threshold\n            instances = prob_to_instances(prob_resized, thr=THRESHOLD, min_area=MIN_AREA)\n            annot = \"authentic\" if len(instances) == 0 else rle_encode(instances)\n\n        rows.append({\"case_id\": case_id, \"annotation\": annot})\n\n# Build dataframe (may be empty if no test files found)\nsub = pd.DataFrame(rows, columns=[\"case_id\", \"annotation\"])\n\n# --- Align with sample_submission order & types (competition rule friendly) ---\nss_path = str(Path(COMP_DIR) / \"sample_submission.csv\")\nif os.path.exists(ss_path):\n    ss = pd.read_csv(ss_path)\n    # Make merge key the same dtype on both sides (strings)\n    ss[\"case_id\"] = ss[\"case_id\"].astype(str)\n    if not sub.empty:\n        sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n        sub = ss[[\"case_id\"]].merge(sub, on=\"case_id\", how=\"left\")\n        sub[\"annotation\"] = sub[\"annotation\"].fillna(\"authentic\")\n    else:\n        # No predictions gathered (e.g., no test images visible) -> default to authentic\n        sub = ss[[\"case_id\"]].copy()\n        sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n        sub[\"annotation\"] = \"authentic\"\nelse:\n    # No sample file -> ensure proper dtypes\n    if not sub.empty:\n        sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n    else:\n        # Nothing to write and no sample to align to\n        print(\"Warning: No test images and no sample_submission.csv found.\")\n        sub = pd.DataFrame(columns=[\"case_id\", \"annotation\"])\n\n# --- Save submission ---\nsub.to_csv(SUB_PATH, index=False)\nprint(\"‚úÖ Wrote submission:\", SUB_PATH)\ndisplay(sub.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:53:00.113306Z","iopub.execute_input":"2025-10-28T07:53:00.113626Z","iopub.status.idle":"2025-10-28T07:53:02.574669Z","shell.execute_reply.started":"2025-10-28T07:53:00.113608Z","shell.execute_reply":"2025-10-28T07:53:02.573834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================\n# Single-image visualization (orig + H/V flips) ‚Äî self-contained version\n# =====================\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import ImageOps\nfrom pathlib import Path\n\nif len(test_paths) == 1:\n    p = test_paths[0]\n\n    # --- Viz parameters ---\n    VIS_THR  = 0.02\n    CMAP_MAX = 0.05\n\n    # --- local helpers ---\n    def overlay_alpha(img_rgb, mask01, color=(255,0,0), alpha=0.35):\n        if mask01.shape[:2] != img_rgb.shape[:2]:\n            mask01 = cv2.resize(mask01, (img_rgb.shape[1], img_rgb.shape[0]), interpolation=cv2.INTER_NEAREST)\n        if img_rgb.ndim == 2:\n            base = cv2.cvtColor(img_rgb, cv2.COLOR_GRAY2BGR)\n        elif img_rgb.shape[2] == 4:\n            base = cv2.cvtColor(img_rgb, cv2.COLOR_RGBA2BGR)\n        else:\n            base = img_rgb.copy()\n        if mask01.sum() == 0:\n            return base\n        overlay = base.copy()\n        overlay[mask01 > 0] = (0.65*base[mask01 > 0] + 0.35*np.array(color)).astype(np.uint8)\n        return overlay\n\n    def draw_outline(img_rgb, mask01, color=(0,255,0), thickness=2):\n        if mask01.shape[:2] != img_rgb.shape[:2]:\n            mask01 = cv2.resize(mask01, (img_rgb.shape[1], img_rgb.shape[0]), interpolation=cv2.INTER_NEAREST)\n        if img_rgb.ndim == 2:\n            vis = cv2.cvtColor(img_rgb, cv2.COLOR_GRAY2BGR)\n        elif img_rgb.shape[2] == 4:\n            vis = cv2.cvtColor(img_rgb, cv2.COLOR_RGBA2BGR)\n        else:\n            vis = img_rgb.copy()\n        if mask01.sum() == 0:\n            return vis\n        cnts, _ = cv2.findContours(mask01.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cv2.drawContours(vis, cnts, -1, color, thickness, lineType=cv2.LINE_AA)\n        return vis\n\n    def to_orig_size(prob_resized, W, H):\n        return cv2.resize(prob_resized, (W, H), interpolation=cv2.INTER_LINEAR)\n\n    def bin_mask(prob_map, thr):\n        return (prob_map >= thr).astype(np.uint8)\n\n    # --- load image ---\n    img_pil = load_rgb(p)\n    img_np  = np.array(img_pil)\n    H, W    = img_np.shape[:2]\n\n    # --- compute TTA probabilities ---\n    prob_orig = predict_prob(model, img_pil, size=IMAGE_SIZE, tta=USE_TTA)\n    prob_h    = np.fliplr(predict_prob(model, ImageOps.mirror(img_pil), size=IMAGE_SIZE, tta=USE_TTA))\n    prob_v    = np.flipud(predict_prob(model, ImageOps.flip(img_pil), size=IMAGE_SIZE, tta=USE_TTA))\n    prob_m    = np.clip((prob_orig + prob_h + prob_v) / 3.0, 0.0, 1.0)\n\n    # --- resize to original ---\n    p_orig = to_orig_size(prob_orig, W, H)\n    p_h    = to_orig_size(prob_h, W, H)\n    p_v    = to_orig_size(prob_v, W, H)\n    p_m    = to_orig_size(prob_m, W, H)\n\n    # --- low-threshold masks ---\n    m_orig = bin_mask(p_orig, VIS_THR)\n    m_h    = bin_mask(p_h, VIS_THR)\n    m_v    = bin_mask(p_v, VIS_THR)\n    m_m    = bin_mask(p_m, VIS_THR)\n\n    # --- quick stats ---\n    def pstats(name, p):\n        vals = p.ravel()\n        print(f\"{name:>6} | min={vals.min():.4f} mean={vals.mean():.4f} max={vals.max():.4f} \"\n              f\"p99={np.quantile(vals,0.99):.4f} p99.5={np.quantile(vals,0.995):.4f}\")\n\n    print(f\"File: {Path(p).name} | {W}√ó{H} | VIS_THR={VIS_THR:.3f} | cmap vmax={CMAP_MAX:.3f}\")\n    pstats(\"orig\",  p_orig)\n    pstats(\"hflip\", p_h)\n    pstats(\"vflip\", p_v)\n    pstats(\"merge\", p_m)\n\n    # --- visuals ---\n    fig = plt.figure(figsize=(18, 12))\n    ax1 = plt.subplot(3,4,1); im1=ax1.imshow(p_orig,cmap='magma',vmin=0,vmax=CMAP_MAX); ax1.set_title('Prob (orig)'); ax1.axis('off'); plt.colorbar(im1,ax=ax1,fraction=0.046,pad=0.02)\n    ax2 = plt.subplot(3,4,2); im2=ax2.imshow(p_h,   cmap='magma',vmin=0,vmax=CMAP_MAX); ax2.set_title('Prob (H‚Üíunflip)'); ax2.axis('off'); plt.colorbar(im2,ax=ax2,fraction=0.046,pad=0.02)\n    ax3 = plt.subplot(3,4,3); im3=ax3.imshow(p_v,   cmap='magma',vmin=0,vmax=CMAP_MAX); ax3.set_title('Prob (V‚Üíunflip)'); ax3.axis('off'); plt.colorbar(im3,ax=ax3,fraction=0.046,pad=0.02)\n    ax4 = plt.subplot(3,4,4); im4=ax4.imshow(p_m,   cmap='magma',vmin=0,vmax=CMAP_MAX); ax4.set_title('Prob (merged)'); ax4.axis('off'); plt.colorbar(im4,ax=ax4,fraction=0.046,pad=0.02)\n\n    ax5 = plt.subplot(3,4,5); ax5.imshow(m_orig,cmap='gray'); ax5.set_title(f'Binary ‚â•{VIS_THR:.3f} (orig)'); ax5.axis('off')\n    ax6 = plt.subplot(3,4,6); ax6.imshow(m_h,   cmap='gray'); ax6.set_title(f'Binary ‚â•{VIS_THR:.3f} (H)'); ax6.axis('off')\n    ax7 = plt.subplot(3,4,7); ax7.imshow(m_v,   cmap='gray'); ax7.set_title(f'Binary ‚â•{VIS_THR:.3f} (V)'); ax7.axis('off')\n    ax8 = plt.subplot(3,4,8); ax8.imshow(m_m,   cmap='gray'); ax8.set_title(f'Binary ‚â•{VIS_THR:.3f} (merged)'); ax8.axis('off')\n\n    ov  = overlay_alpha(img_np, m_m, (255,0,0), 0.35)\n    out = draw_outline(img_np, m_m, (0,255,0), thickness=max(2, int(0.003*max(H,W))))\n\n    ax9  = plt.subplot(3,4,9);  ax9.imshow(ov);  ax9.set_title('Overlay (merged)'); ax9.axis('off')\n    ax10 = plt.subplot(3,4,10); ax10.imshow(out); ax10.set_title('Outline (merged)'); ax10.axis('off')\n\n    ax11 = plt.subplot(3,4,11)\n    if m_m.sum()>0:\n        ys,xs=np.where(m_m>0); y0,y1=max(0,ys.min()-20),min(H,ys.max()+20); x0,x1=max(0,xs.min()-20),min(W,xs.max()+20)\n        ax11.imshow(out[y0:y1,x0:x1]); ax11.set_title('Zoom (merged)'); ax11.axis('off')\n    else:\n        ax11.imshow(out); ax11.set_title('Zoom (merged) - empty'); ax11.axis('off')\n\n    ax12=plt.subplot(3,4,12)\n    vals=p_m.ravel(); ax12.hist(vals,bins=100,range=(0,CMAP_MAX))\n    ax12.axvline(VIS_THR,color='r',ls='--',label=f'VIS_THR={VIS_THR:.3f}')\n    ax12.set_title('Histogram (merged probs)'); ax12.legend()\n\n    plt.tight_layout(); plt.show()\nelse:\n    print(\"‚ö†Ô∏è This cell is for single-image visual only (found\", len(test_paths), \")\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:56:36.934022Z","iopub.execute_input":"2025-10-28T07:56:36.934328Z","iopub.status.idle":"2025-10-28T07:56:41.741702Z","shell.execute_reply.started":"2025-10-28T07:56:36.934307Z","shell.execute_reply":"2025-10-28T07:56:41.74091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Note\n\nHere, the forged cells are not detected, but we can still see that the model try to predicting it with a prob of 0.04 at the good position, so it's a great start!","metadata":{}}]}