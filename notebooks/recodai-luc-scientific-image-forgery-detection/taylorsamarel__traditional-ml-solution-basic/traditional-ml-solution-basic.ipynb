{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14174843,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport json\nimport cv2\nfrom tqdm import tqdm\nfrom scipy.fftpack import dct\nfrom scipy.ndimage import median_filter, gaussian_filter\nfrom scipy import stats\nfrom skimage.measure import label, regionprops\nfrom skimage.feature import local_binary_pattern, hog, graycomatrix, graycoprops\nfrom skimage.filters import sobel\nimport matplotlib.pyplot as plt\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_sample_weight\nimport joblib\nimport torch\n\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\nprint(\"=\"*80)\nprint(\"ACCURACY-OPTIMIZED FORGERY DETECTION WITH ENSEMBLE\")\nprint(\"=\"*80)\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nclass Config:\n    BASE_PATH = Path('/kaggle/input/recodai-luc-scientific-image-forgery-detection')\n    TRAIN_IMAGES_DIR = BASE_PATH / 'train_images'\n    TRAIN_MASKS_DIR = BASE_PATH / 'train_masks'\n    TEST_IMAGES_DIR = BASE_PATH / 'test_images'\n    SAMPLE_SUB_PATH = BASE_PATH / 'sample_submission.csv'\n    \n    # Feature extraction\n    PATCH_SIZE = 64\n    PATCHES_PER_IMAGE_AUTHENTIC = 25\n    PATCHES_PER_IMAGE_FORGED = 80\n    \n    # Training data\n    MAX_TRAIN_IMAGES = 800\n    MAX_TRAIN_SAMPLES = 60000\n    \n    # Training\n    USE_GPU = True\n    USE_ENSEMBLE = True\n    USE_AUGMENTATION = True\n    USE_CLASS_WEIGHTS = True  # Instead of SMOTE\n    \n    # XGBoost params\n    XGBOOST_PARAMS = {\n        'n_estimators': 400,\n        'max_depth': 10,\n        'learning_rate': 0.05,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'min_child_weight': 3,\n        'gamma': 0.1,\n        'reg_alpha': 0.5,\n        'reg_lambda': 2.0,\n    }\n    \n    # Random Forest params\n    RF_PARAMS = {\n        'n_estimators': 250,\n        'max_depth': 18,\n        'min_samples_split': 5,\n        'min_samples_leaf': 2,\n        'n_jobs': -1,\n        'class_weight': 'balanced'\n    }\n    \n    # Prediction\n    STRIDE = 28\n    FORGERY_THRESHOLD = 0.45\n    MIN_REGION_AREA = 120\n    \n    # Parallel processing\n    NUM_WORKERS = 6\n    \n    VISUALIZE_SAMPLES = True\n    MAX_VIZ_SAMPLES = 2\n\n# ============================================================================\n# DATA DISCOVERY\n# ============================================================================\n\ndef discover_data(config):\n    print(\"\\n\" + \"=\"*80)\n    print(\"DATA DISCOVERY\")\n    print(\"=\"*80)\n    \n    authentic_dir = config.TRAIN_IMAGES_DIR / 'authentic'\n    forged_dir = config.TRAIN_IMAGES_DIR / 'forged'\n    \n    authentic_images = sorted(list(authentic_dir.glob('*.png')))[:config.MAX_TRAIN_IMAGES] if authentic_dir.exists() else []\n    forged_images = sorted(list(forged_dir.glob('*.png')))[:config.MAX_TRAIN_IMAGES] if forged_dir.exists() else []\n    \n    print(f\"üìÅ Training - Authentic: {len(authentic_images)}, Forged: {len(forged_images)}\")\n    \n    mask_files = {}\n    if config.TRAIN_MASKS_DIR.exists():\n        for mask_path in config.TRAIN_MASKS_DIR.glob('*.npy'):\n            mask_files[mask_path.stem] = mask_path\n    \n    print(f\"üìÅ Masks: {len(mask_files)}\")\n    \n    test_images = sorted(list(config.TEST_IMAGES_DIR.glob('*.png')))\n    print(f\"üìÅ Test: {len(test_images)}\")\n    print(\"=\"*80)\n    \n    return authentic_images, forged_images, mask_files, test_images\n\n# ============================================================================\n# COMPREHENSIVE FEATURE EXTRACTION\n# ============================================================================\n\ndef extract_comprehensive_features(patch):\n    \"\"\"Extract comprehensive features\"\"\"\n    \n    if patch.shape[0] < 8 or patch.shape[1] < 8:\n        return None\n    \n    features = []\n    \n    if patch.ndim == 3:\n        gray = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = patch\n    \n    gray = gray.astype(np.float32)\n    \n    # 1. Color features\n    if patch.ndim == 3:\n        for channel in range(3):\n            ch = patch[:, :, channel].astype(np.float32)\n            features.extend([\n                np.mean(ch), np.std(ch), np.median(ch),\n                np.percentile(ch, 25), np.percentile(ch, 75),\n                np.min(ch), np.max(ch), np.var(ch)\n            ])\n        \n        r, g, b = patch[:, :, 0], patch[:, :, 1], patch[:, :, 2]\n        features.extend([\n            np.mean(r / (g + 1)), np.mean(g / (b + 1)), np.mean(b / (r + 1)),\n            np.std(r - g), np.std(g - b), np.std(b - r)\n        ])\n    \n    # 2. Grayscale statistics\n    features.extend([\n        np.mean(gray), np.std(gray), np.median(gray),\n        np.percentile(gray, 10), np.percentile(gray, 25),\n        np.percentile(gray, 75), np.percentile(gray, 90),\n        np.min(gray), np.max(gray), np.var(gray), np.ptp(gray)\n    ])\n    \n    # 3. DCT features\n    dct_result = dct(dct(gray.T, norm='ortho').T, norm='ortho')\n    dct_feat = dct_result[:8, :8].flatten()\n    features.extend(dct_feat)\n    \n    h, w = gray.shape\n    center_h, center_w = h // 2, w // 2\n    low_freq = np.sum(np.abs(dct_result[center_h-4:center_h+4, center_w-4:center_w+4]))\n    mid_freq = np.sum(np.abs(dct_result[center_h-8:center_h+8, center_w-8:center_w+8])) - low_freq\n    high_freq = np.sum(np.abs(dct_result)) - mid_freq - low_freq\n    total = low_freq + mid_freq + high_freq + 1e-10\n    features.extend([low_freq/total, mid_freq/total, high_freq/total])\n    \n    # 4. Gradient features\n    grad_x = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n    grad_y = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n    grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n    grad_dir = np.arctan2(grad_y, grad_x)\n    \n    features.extend([\n        np.mean(np.abs(grad_x)), np.std(np.abs(grad_x)), np.max(np.abs(grad_x)),\n        np.mean(np.abs(grad_y)), np.std(np.abs(grad_y)), np.max(np.abs(grad_y)),\n        np.mean(grad_mag), np.std(grad_mag), np.max(grad_mag),\n        np.percentile(grad_mag, 90), np.std(grad_dir)\n    ])\n    \n    # 5. Edge features\n    edges = cv2.Canny(gray.astype(np.uint8), 50, 150)\n    edge_density = np.sum(edges > 0) / edges.size\n    \n    h, w = edges.shape\n    h_third, w_third = max(1, h // 3), max(1, w // 3)\n    edge_regions = []\n    for i in range(3):\n        for j in range(3):\n            region = edges[i*h_third:(i+1)*h_third, j*w_third:(j+1)*w_third]\n            if region.size > 0:\n                edge_regions.append(np.sum(region > 0) / region.size)\n    \n    features.append(edge_density)\n    features.extend(edge_regions)\n    \n    # 6. LBP\n    try:\n        lbp = local_binary_pattern(gray, 8, 1, method='uniform')\n        lbp_hist, _ = np.histogram(lbp.ravel(), bins=10, range=(0, 10), density=True)\n        features.extend(lbp_hist)\n    except:\n        features.extend([0] * 10)\n    \n    # 7. GLCM\n    try:\n        gray_uint = gray.astype(np.uint8)\n        glcm = graycomatrix(gray_uint, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], \n                           levels=256, symmetric=True, normed=True)\n        \n        for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']:\n            features.extend(graycoprops(glcm, prop).ravel())\n    except:\n        features.extend([0] * 20)\n    \n    # 8. Noise analysis\n    denoised = median_filter(gray, size=3)\n    noise = gray - denoised\n    \n    features.extend([\n        np.std(noise), np.mean(np.abs(noise)),\n        np.percentile(np.abs(noise), 90),\n        np.percentile(np.abs(noise), 95),\n        np.max(np.abs(noise))\n    ])\n    \n    gaussian_denoised = gaussian_filter(gray, sigma=1.0)\n    high_freq_noise = gray - gaussian_denoised\n    features.extend([np.std(high_freq_noise), np.mean(np.abs(high_freq_noise))])\n    \n    # 9. Texture variance\n    h, w = gray.shape\n    h_half, w_half = h // 2, w // 2\n    \n    if h_half > 0 and w_half > 0:\n        subregions = [\n            gray[:h_half, :w_half], gray[:h_half, w_half:],\n            gray[h_half:, :w_half], gray[h_half:, w_half:]\n        ]\n        for region in subregions:\n            if region.size > 0:\n                features.extend([np.mean(region), np.std(region), np.var(region)])\n    \n    # 10. HOG\n    if gray.shape[0] >= 16 and gray.shape[1] >= 16:\n        try:\n            hog_features = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n                              cells_per_block=(2, 2), visualize=False, feature_vector=True)\n            features.extend(hog_features[:32])\n        except:\n            features.extend([0] * 32)\n    \n    # 11. Sobel edges\n    try:\n        sobel_edges = sobel(gray)\n        features.extend([\n            np.mean(sobel_edges), np.std(sobel_edges),\n            np.max(sobel_edges), np.percentile(sobel_edges, 90)\n        ])\n    except:\n        features.extend([0] * 4)\n    \n    # 12. Statistical moments\n    features.extend([stats.skew(gray.ravel()), stats.kurtosis(gray.ravel())])\n    \n    return np.array(features)\n\n# ============================================================================\n# DATA AUGMENTATION\n# ============================================================================\n\ndef augment_patch(patch):\n    \"\"\"Apply random augmentation\"\"\"\n    augmented = [patch]\n    augmented.append(cv2.flip(patch, 1))\n    augmented.append(cv2.flip(patch, 0))\n    augmented.append(cv2.rotate(patch, cv2.ROTATE_90_CLOCKWISE))\n    return augmented\n\n# ============================================================================\n# TRAINING DATA GENERATION\n# ============================================================================\n\ndef process_image_patches_advanced(img_path, mask_path, config, is_forged):\n    \"\"\"Process one image and extract patches\"\"\"\n    patches_data = []\n    \n    try:\n        img = cv2.imread(str(img_path))\n        if img is None:\n            return patches_data\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        h, w = img.shape[:2]\n        if h < config.PATCH_SIZE or w < config.PATCH_SIZE:\n            return patches_data\n        \n        if is_forged and mask_path:\n            mask = np.load(str(mask_path))\n            if mask.ndim > 2:\n                mask = mask[:, :, 0] if mask.shape[2] == 1 else mask.max(axis=2)\n            if mask.shape[:2] != (h, w):\n                mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n            mask = (mask > 0).astype(np.uint8)\n            \n            forged_coords = np.argwhere(mask > 0)\n            \n            if len(forged_coords) > 0:\n                n_samples = min(config.PATCHES_PER_IMAGE_FORGED, len(forged_coords))\n                indices = np.random.choice(len(forged_coords), n_samples, replace=False)\n                \n                for idx in indices:\n                    coord = forged_coords[idx]\n                    y_center, x_center = coord\n                    y = max(0, min(y_center - config.PATCH_SIZE // 2, h - config.PATCH_SIZE))\n                    x = max(0, min(x_center - config.PATCH_SIZE // 2, w - config.PATCH_SIZE))\n                    \n                    patch = img[y:y+config.PATCH_SIZE, x:x+config.PATCH_SIZE]\n                    if patch.shape[0] == config.PATCH_SIZE and patch.shape[1] == config.PATCH_SIZE:\n                        if config.USE_AUGMENTATION and np.random.rand() > 0.5:\n                            augmented_patches = augment_patch(patch)\n                            for aug_patch in augmented_patches:\n                                features = extract_comprehensive_features(aug_patch)\n                                if features is not None:\n                                    patches_data.append((features, 1))\n                        else:\n                            features = extract_comprehensive_features(patch)\n                            if features is not None:\n                                patches_data.append((features, 1))\n            \n            for _ in range(config.PATCHES_PER_IMAGE_AUTHENTIC // 2):\n                y = np.random.randint(0, h - config.PATCH_SIZE + 1)\n                x = np.random.randint(0, w - config.PATCH_SIZE + 1)\n                patch_mask = mask[y:y+config.PATCH_SIZE, x:x+config.PATCH_SIZE]\n                \n                if patch_mask.sum() / patch_mask.size < 0.05:\n                    patch = img[y:y+config.PATCH_SIZE, x:x+config.PATCH_SIZE]\n                    if patch.shape[0] == config.PATCH_SIZE and patch.shape[1] == config.PATCH_SIZE:\n                        features = extract_comprehensive_features(patch)\n                        if features is not None:\n                            patches_data.append((features, 0))\n        else:\n            for _ in range(config.PATCHES_PER_IMAGE_AUTHENTIC):\n                y = np.random.randint(0, h - config.PATCH_SIZE + 1)\n                x = np.random.randint(0, w - config.PATCH_SIZE + 1)\n                \n                patch = img[y:y+config.PATCH_SIZE, x:x+config.PATCH_SIZE]\n                if patch.shape[0] == config.PATCH_SIZE and patch.shape[1] == config.PATCH_SIZE:\n                    features = extract_comprehensive_features(patch)\n                    if features is not None:\n                        patches_data.append((features, 0))\n    \n    except Exception as e:\n        pass\n    \n    return patches_data\n\ndef generate_training_data_advanced(authentic_images, forged_images, mask_files, config):\n    \"\"\"Generate training data with parallel processing\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING TRAINING DATA\")\n    print(\"=\"*80)\n    \n    all_patches = []\n    \n    print(\"\\nüìä Processing authentic images...\")\n    with ThreadPoolExecutor(max_workers=config.NUM_WORKERS) as executor:\n        futures = [executor.submit(process_image_patches_advanced, img_path, None, config, False) \n                   for img_path in authentic_images]\n        \n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Authentic\"):\n            all_patches.extend(future.result())\n    \n    print(\"\\nüìä Processing forged images...\")\n    with ThreadPoolExecutor(max_workers=config.NUM_WORKERS) as executor:\n        futures = []\n        for img_path in forged_images:\n            mask_path = mask_files.get(img_path.stem)\n            futures.append(executor.submit(process_image_patches_advanced, img_path, mask_path, config, True))\n        \n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Forged\"):\n            all_patches.extend(future.result())\n    \n    if len(all_patches) == 0:\n        return np.array([]), np.array([])\n    \n    X_train = np.array([p[0] for p in all_patches])\n    y_train = np.array([p[1] for p in all_patches])\n    \n    # Balanced sampling\n    if len(X_train) > config.MAX_TRAIN_SAMPLES:\n        authentic_indices = np.where(y_train == 0)[0]\n        forged_indices = np.where(y_train == 1)[0]\n        \n        n_per_class = config.MAX_TRAIN_SAMPLES // 2\n        \n        if len(authentic_indices) > n_per_class:\n            authentic_indices = np.random.choice(authentic_indices, n_per_class, replace=False)\n        if len(forged_indices) > n_per_class:\n            forged_indices = np.random.choice(forged_indices, n_per_class, replace=False)\n        \n        indices = np.concatenate([authentic_indices, forged_indices])\n        np.random.shuffle(indices)\n        \n        X_train = X_train[indices]\n        y_train = y_train[indices]\n    \n    print(f\"\\n‚úì Training samples: {len(X_train)}\")\n    print(f\"  - Authentic: {np.sum(y_train == 0)} ({100*np.sum(y_train == 0)/len(y_train):.1f}%)\")\n    print(f\"  - Forged: {np.sum(y_train == 1)} ({100*np.sum(y_train == 1)/len(y_train):.1f}%)\")\n    print(f\"  - Features: {X_train.shape[1]}\")\n    \n    return X_train, y_train\n\n# ============================================================================\n# MODEL TRAINING\n# ============================================================================\n\ndef train_ensemble_model(X_train, y_train, config):\n    \"\"\"Train ensemble of models\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING ENSEMBLE\")\n    print(\"=\"*80)\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    \n    # Compute sample weights for class imbalance\n    sample_weights = None\n    if config.USE_CLASS_WEIGHTS:\n        sample_weights = compute_sample_weight('balanced', y_train)\n        print(f\"\\nüìä Using class weights for balance\")\n    \n    models = []\n    \n    # XGBoost with GPU\n    print(\"\\nüöÄ Training XGBoost (GPU)...\")\n    xgb_params = {\n        'tree_method': 'hist',\n        'device': 'cuda' if config.USE_GPU else 'cpu',\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'random_state': 42,\n        **config.XGBOOST_PARAMS\n    }\n    \n    xgb_model = xgb.XGBClassifier(**xgb_params)\n    if sample_weights is not None:\n        xgb_model.fit(X_train, y_train, sample_weight=sample_weights, verbose=50)\n    else:\n        xgb_model.fit(X_train, y_train, verbose=50)\n    models.append(('XGBoost', xgb_model))\n    \n    # Random Forest\n    if config.USE_ENSEMBLE:\n        print(\"\\nüå≤ Training Random Forest...\")\n        rf_model = RandomForestClassifier(**config.RF_PARAMS, random_state=42)\n        rf_model.fit(X_train, y_train)\n        models.append(('RandomForest', rf_model))\n    \n    # Evaluate\n    print(\"\\nüìä Model Performance:\")\n    for name, model in models:\n        score = model.score(X_train, y_train)\n        print(f\"   {name}: {score:.4f}\")\n    \n    return models, scaler\n\n# ============================================================================\n# PREDICTION\n# ============================================================================\n\ndef predict_image_ensemble(image, models, scaler, config):\n    \"\"\"Predict using ensemble\"\"\"\n    \n    h, w = image.shape[:2]\n    \n    if h < config.PATCH_SIZE or w < config.PATCH_SIZE:\n        return np.zeros((h, w), dtype=np.uint8), 0.0\n    \n    prediction_maps = []\n    \n    for model_name, model in models:\n        prediction_map = np.zeros((h, w), dtype=np.float32)\n        count_map = np.zeros((h, w), dtype=np.int32)\n        \n        patches = []\n        positions = []\n        \n        for y in range(0, h - config.PATCH_SIZE + 1, config.STRIDE):\n            for x in range(0, w - config.PATCH_SIZE + 1, config.STRIDE):\n                patch = image[y:y+config.PATCH_SIZE, x:x+config.PATCH_SIZE]\n                \n                if patch.shape[0] == config.PATCH_SIZE and patch.shape[1] == config.PATCH_SIZE:\n                    features = extract_comprehensive_features(patch)\n                    if features is not None:\n                        patches.append(features)\n                        positions.append((y, x))\n        \n        if len(patches) == 0:\n            continue\n        \n        X = scaler.transform(np.array(patches))\n        predictions = model.predict_proba(X)[:, 1]\n        \n        for (y, x), pred in zip(positions, predictions):\n            prediction_map[y:y+config.PATCH_SIZE, x:x+config.PATCH_SIZE] += pred\n            count_map[y:y+config.PATCH_SIZE, x:x+config.PATCH_SIZE] += 1\n        \n        mask = np.divide(prediction_map, count_map, where=count_map > 0)\n        prediction_maps.append(mask)\n    \n    if len(prediction_maps) > 0:\n        ensemble_mask = np.mean(prediction_maps, axis=0)\n    else:\n        return np.zeros((h, w), dtype=np.uint8), 0.0\n    \n    confidence = np.mean(ensemble_mask[ensemble_mask > 0]) if ensemble_mask.max() > 0 else 0\n    \n    binary_mask = (ensemble_mask > config.FORGERY_THRESHOLD).astype(np.uint8)\n    \n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel, iterations=3)\n    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel, iterations=2)\n    \n    labeled = label(binary_mask)\n    regions = regionprops(labeled)\n    \n    refined_mask = np.zeros_like(binary_mask)\n    for region in regions:\n        if region.area >= config.MIN_REGION_AREA:\n            if region.perimeter > 0:\n                compactness = 4 * np.pi * region.area / (region.perimeter ** 2)\n                if compactness > 0.05:\n                    coords = region.coords\n                    refined_mask[coords[:, 0], coords[:, 1]] = 1\n    \n    return refined_mask, confidence\n\n# ============================================================================\n# VISUALIZATION\n# ============================================================================\n\ndef visualize_detection(image, mask, case_id, confidence, save_path='detection.png'):\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    axes[0].imshow(image)\n    axes[0].set_title(f'Original ({case_id})')\n    axes[0].axis('off')\n    \n    axes[1].imshow(mask, cmap='hot')\n    axes[1].set_title('Mask')\n    axes[1].axis('off')\n    \n    overlay = image.copy()\n    if mask.max() > 0:\n        mask_colored = np.zeros_like(image)\n        mask_colored[:, :, 0] = mask * 255\n        overlay = cv2.addWeighted(overlay, 0.7, mask_colored, 0.3, 0)\n    \n    axes[2].imshow(overlay)\n    axes[2].set_title(f'Conf: {confidence:.3f}')\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n    plt.close()\n    print(f\"   üíæ Saved: {save_path}\")\n\n# ============================================================================\n# RLE ENCODING\n# ============================================================================\n\ndef rle_encode(mask):\n    dots = np.where(mask.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    config = Config()\n    \n    authentic_images, forged_images, mask_files, test_images = discover_data(config)\n    \n    X_train, y_train = generate_training_data_advanced(authentic_images, forged_images, mask_files, config)\n    \n    if len(X_train) == 0:\n        print(\"\\n‚ùå ERROR: No training data!\")\n        return None\n    \n    models, scaler = train_ensemble_model(X_train, y_train, config)\n    \n    joblib.dump(models, 'ensemble_models.pkl')\n    joblib.dump(scaler, 'feature_scaler.pkl')\n    print(\"\\n‚úì Models saved\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PREDICTING\")\n    print(\"=\"*80)\n    \n    sample_sub = pd.read_csv(config.SAMPLE_SUB_PATH)\n    results = []\n    viz_count = 0\n    detection_summary = {'authentic': 0, 'forgery': 0, 'confidences': []}\n    \n    for idx, row in tqdm(sample_sub.iterrows(), total=len(sample_sub), desc=\"üîç\"):\n        case_id = str(row['case_id'])\n        \n        test_img_path = None\n        for img_path in test_images:\n            if img_path.stem == case_id:\n                test_img_path = img_path\n                break\n        \n        if test_img_path and test_img_path.exists():\n            try:\n                img = cv2.imread(str(test_img_path))\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                \n                print(f\"\\nüì∑ {case_id}: {img.shape[1]}x{img.shape[0]}px\")\n                \n                mask, confidence = predict_image_ensemble(img, models, scaler, config)\n                detection_summary['confidences'].append(confidence)\n                \n                is_forgery = confidence >= 0.45 and mask.sum() >= config.MIN_REGION_AREA\n                \n                if not is_forgery:\n                    results.append({'case_id': int(case_id), 'annotation': 'authentic'})\n                    detection_summary['authentic'] += 1\n                    print(f\"   ‚úì AUTHENTIC ({confidence:.3f})\")\n                else:\n                    run_lengths = rle_encode(mask)\n                    if len(run_lengths) > 0:\n                        results.append({'case_id': int(case_id), 'annotation': json.dumps([int(x) for x in run_lengths])})\n                        detection_summary['forgery'] += 1\n                        print(f\"   ‚ö†Ô∏è  FORGERY ({confidence:.3f}), area: {mask.sum()}px\")\n                    else:\n                        results.append({'case_id': int(case_id), 'annotation': 'authentic'})\n                        detection_summary['authentic'] += 1\n                \n                if config.VISUALIZE_SAMPLES and viz_count < config.MAX_VIZ_SAMPLES:\n                    visualize_detection(img, mask, case_id, confidence, f'detection_{case_id}.png')\n                    viz_count += 1\n            \n            except Exception as e:\n                print(f\"   ‚ùå Error: {e}\")\n                results.append({'case_id': int(case_id), 'annotation': 'authentic'})\n                detection_summary['authentic'] += 1\n        else:\n            results.append({'case_id': int(case_id), 'annotation': 'authentic'})\n            detection_summary['authentic'] += 1\n    \n    submission_df = pd.DataFrame(results)\n    submission_df.to_csv('submission.csv', index=False)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPLETE\")\n    print(\"=\"*80)\n    print(f\"‚úì Total: {len(submission_df)}\")\n    print(f\"‚úì Authentic: {detection_summary['authentic']}, Forgeries: {detection_summary['forgery']}\")\n    print(f\"üíæ Saved: submission.csv\")\n    print(\"=\"*80)\n    \n    return submission_df\n\nif __name__ == \"__main__\":\n    submission = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}