{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":4594,"databundleVersionId":860645,"sourceType":"competition"},{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"},{"sourceId":8587,"databundleVersionId":868304,"sourceType":"competition"},{"sourceId":33109,"databundleVersionId":4193407,"sourceType":"competition"},{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"},{"sourceId":408,"sourceType":"datasetVersion","datasetId":180}],"dockerImageVersionId":30260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:white;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:black;\">Gradient Descent Implementation\n    \n</h2>\n</div>\n    \n</center>\n\nAn optimization approach called gradient descent is frequently used to train neural networks and machine learning models.  These models gain knowledge over time by using training data, and the cost function in gradient descent especially serves as a barometer by assessing the accuracy of each iteration of parameter changes. The model will keep changing its parameters to provide the minimal error until the function is close to or equal to zero. \n\n\n<div style=\"text-align: center; border-radius: 5px; padding: 10px;\">\n    <img src=\"https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png\" alt=\"Descriptive Image Text\" style=\"width: 90%; height: auto; border-radius: 5px;\">\n    <h3 style=\"color: white; font-size: 110%; letter-spacing: 0.5px;\">How does gradient descent work?</h3>\n</div>\n\n\nBefore diving into gradient descent, let’s review linear regression basics. You may remember the equation of a line,  $y = mx + b$ , where  $m$  is the slope and  b  the y-intercept. In linear regression, we fit a line to minimize errors between actual outputs and predictions  $\\hat{y}$ , often using mean squared error.\n\n> Gradient descent operates similarly, but in a convex cost function. Starting from an initial point, we calculate the derivative to determine slope steepness, using it to update the weights and bias. The steep slope at the start gradually flattens as we approach the curve’s lowest point, or convergence.\n\nJust as in finding the best-fit line, gradient descent aims to minimize the cost function—the gap between predicted and actual values. This process relies on two key factors: direction and learning rate, which guide partial derivative calculations through each iteration, leading closer to a local or global minimum.\n\n\n- **Learning Rate** (or alpha) defines the size of steps toward the minimum. Usually small, it adjusts based on the cost function’s behavior. Higher learning rates take larger steps but risk overshooting, while lower rates improve precision at the cost of longer, more computation-intensive iterations.\n\n- The **cost function** measures the error between actual and predicted values, guiding model adjustments to reduce error. The model iterates along the negative gradient, progressively minimizing this function until learning concludes. The term “loss function” often refers to a single example’s error, while “cost function” typically represents the average error across the dataset, providing a broader performance view.\n\n\n\n<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h3 style=\"padding: 10px;\n              color:white;\">Types of Gradient Descent\n    \n</h3>\n</div>\n    \n</center>\n\n\nThere exist three prominent variants of gradient descent learning algorithms: batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.\n\n- **Batch Gradient Descent**\n  In batch gradient descent, errors for all training data points are summed, and the model updates after evaluating the entire set in a training epoch. Though computationally efficient, this approach can be memory-intensive and slow with large datasets. Batch gradient descent produces a stable error gradient and convergence but may settle in a local minimum rather than the global one.\n\n- **Stochastic Gradient Descent**\n  In stochastic gradient descent (SGD), each training example is used to update the model parameters individually, requiring less memory. While more frequent updates provide detailed insights and speed, they can reduce computational efficiency compared to batch gradient descent. The resulting noisy gradients help escape local minima, potentially leading to a global minimum.\n\n- **Mini-Batch Gradient Descent**\n  Mini-batch gradient descent amalgamates elements of both batch and stochastic gradient descent. It segments the training dataset into small batches and updates the model using each of these batches. This approach harmonizes the computational efficiency of batch gradient descent with the velocity of stochastic gradient descent, striking a favorable balance between the two.\n\nThese gradient descent variations offer distinct trade-offs in terms of computational efficiency, convergence behavior, and their ability to navigate local and global minima during the optimization process.\n\n<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h3 style=\"padding: 10px;\n              color:white;\">Challenges with gradient descent.\n    \n</h3>\n</div>\n    \n</center>\n\n\nWhile gradient descent remains a prevalent optimization technique, it confronts several challenges that warrant consideration. Some of these challenges encompass:\n\n- **Navigating Local Minima and Saddle Points**\nIn convex problems, gradient descent effectively finds the global minimum, optimizing model performance. In nonconvex problems, however, multiple minima complicate this process. A near-zero cost function slope suggests halted learning, yet this may occur at local minima or saddle points rather than the global minimum. Local minima resemble the global minimum with ascending slopes on both sides, whereas saddle points have a negative gradient on one side, like a saddle’s shape. Introducing noisy gradients can help escape these local minima and saddle points, allowing gradient descent to navigate complex optimization landscapes.\n\nThis interplay between optimization challenges and the potential remedies, exemplified by noisy gradients, underscores the nuanced nature of gradient descent's optimization journey.\n\n- **Vanishing and Exploding Gradients**\nIn the realm of deeper neural networks, particularly recurrent neural networks, gradient descent and backpropagation can introduce two additional challenges during model training.\n\n    - **Vanishing Gradients**: This predicament arises when the gradient becomes exceedingly minute. As we backtrack during the backpropagation process, the gradient steadily diminishes, causing earlier layers in the network to acquire knowledge at a lethargic pace compared to later layers. This phenomenon translates into weight parameters being updated incrementally until they dwindle to insignificance—essentially approaching 0. Consequently, the algorithm enters a state of stagnation, ceasing to learn effectively.\n\n    - **Exploding Gradients**: Conversely, this scenario emerges when the gradient swells to an unwieldy magnitude, engendering model instability. The repercussions include the inflation of model weights to colossal proportions, often spiraling into the realm of NaN (Not-a-Number) representation. A potential resolution to this quandary involves harnessing a dimensionality reduction technique. Such an approach aids in curtailing model intricacies, ameliorating the risk of gradients spiraling out of control.\n\nThe dual challenge of vanishing and exploding gradients in neural networks underscores the intricate balance that must be struck during model optimization. Overcoming these challenges contributes to the robustness and effectiveness of the neural network's learning dynamics.\n\n\nSpeaking of Neural Network, I also have implemented a rudimentary level of Neural Network from Scratch which you can check out by clicking [here](https://bit.ly/gradientdescent1)","metadata":{"papermill":{"duration":0.021447,"end_time":"2022-09-28T16:43:37.897781","exception":false,"start_time":"2022-09-28T16:43:37.876334","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport random\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns',100)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 14\nplt.rcParams['figure.figsize'] = (22, 5)\nplt.rcParams['figure.dpi'] = 100\nimport sqlite3\nfrom sklearn.metrics import r2_score, roc_curve, auc\nfrom urllib.request import urlretrieve\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.355932,"end_time":"2022-09-28T16:43:40.271592","exception":false,"start_time":"2022-09-28T16:43:37.91566","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:04.53615Z","iopub.execute_input":"2024-11-09T03:52:04.53709Z","iopub.status.idle":"2024-11-09T03:52:04.544677Z","shell.execute_reply.started":"2024-11-09T03:52:04.537049Z","shell.execute_reply":"2024-11-09T03:52:04.543692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Gradient Descent for Regression\n    \n</h2>\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.017066,"end_time":"2022-09-28T16:43:40.306377","exception":false,"start_time":"2022-09-28T16:43:40.289311","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Loading the data","metadata":{}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":1.133104,"end_time":"2022-09-28T16:43:41.457035","exception":false,"start_time":"2022-09-28T16:43:40.323931","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"medical_charges_url = 'https://raw.githubusercontent.com/JovianML/opendatasets/master/data/medical-charges.csv'\n#storing the data into a dataframe\nmedical_df = pd.read_csv(medical_charges_url)\nmedical_df.head()","metadata":{"papermill":{"duration":0.05353,"end_time":"2022-09-28T16:43:41.528265","exception":false,"start_time":"2022-09-28T16:43:41.474735","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:04.628225Z","iopub.execute_input":"2024-11-09T03:52:04.628612Z","iopub.status.idle":"2024-11-09T03:52:04.850671Z","shell.execute_reply.started":"2024-11-09T03:52:04.62858Z","shell.execute_reply":"2024-11-09T03:52:04.849712Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checking the shape of the dataframe\nmedical_df.shape","metadata":{"papermill":{"duration":0.028238,"end_time":"2022-09-28T16:43:41.574894","exception":false,"start_time":"2022-09-28T16:43:41.546656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:04.852311Z","iopub.execute_input":"2024-11-09T03:52:04.852719Z","iopub.status.idle":"2024-11-09T03:52:04.859225Z","shell.execute_reply.started":"2024-11-09T03:52:04.852673Z","shell.execute_reply":"2024-11-09T03:52:04.858378Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"medical_df.describe()","metadata":{"papermill":{"duration":0.055692,"end_time":"2022-09-28T16:43:41.648622","exception":false,"start_time":"2022-09-28T16:43:41.59293","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:04.860827Z","iopub.execute_input":"2024-11-09T03:52:04.861146Z","iopub.status.idle":"2024-11-09T03:52:04.887151Z","shell.execute_reply.started":"2024-11-09T03:52:04.861119Z","shell.execute_reply":"2024-11-09T03:52:04.886296Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"medical_df.info()","metadata":{"papermill":{"duration":0.040719,"end_time":"2022-09-28T16:43:41.708509","exception":false,"start_time":"2022-09-28T16:43:41.66779","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:04.889713Z","iopub.execute_input":"2024-11-09T03:52:04.890376Z","iopub.status.idle":"2024-11-09T03:52:04.901401Z","shell.execute_reply.started":"2024-11-09T03:52:04.890318Z","shell.execute_reply":"2024-11-09T03:52:04.90047Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Exploratory Data Analysis\n    \n</h2>\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.017968,"end_time":"2022-09-28T16:43:41.745911","exception":false,"start_time":"2022-09-28T16:43:41.727943","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Creating a sqlite3 database to run some SQL Queries \nconn = sqlite3.connect('database.db' )\n# Write the DataFrame to a table in the database\nmedical_df.to_sql('data', conn, if_exists='replace', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:04.902743Z","iopub.execute_input":"2024-11-09T03:52:04.903071Z","iopub.status.idle":"2024-11-09T03:52:04.940281Z","shell.execute_reply.started":"2024-11-09T03:52:04.903044Z","shell.execute_reply":"2024-11-09T03:52:04.939399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(data=medical_df, x='sex', y='charges')\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.title('Distribution of Charges by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Charges')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:04.941457Z","iopub.execute_input":"2024-11-09T03:52:04.941724Z","iopub.status.idle":"2024-11-09T03:52:05.191466Z","shell.execute_reply.started":"2024-11-09T03:52:04.941699Z","shell.execute_reply":"2024-11-09T03:52:05.190496Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> From the plot above, we could see males are likely to spend more on insurance charges..\n\n</div>","metadata":{}},{"cell_type":"code","source":"sns.catplot(data=medical_df,x= 'sex',y='charges', kind='swarm', hue='smoker', aspect =2, height=8)\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.axhline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.annotate(f' Average Medical Charges ($)', (.4, medical_df['charges'].mean()+900), fontsize=14,color='red')\nplt.title('Insurance Charges Gender Wise', fontsize=20)\nplt.show()","metadata":{"papermill":{"duration":1.311081,"end_time":"2022-09-28T16:43:43.075209","exception":false,"start_time":"2022-09-28T16:43:41.764128","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:05.192959Z","iopub.execute_input":"2024-11-09T03:52:05.193381Z","iopub.status.idle":"2024-11-09T03:52:06.846792Z","shell.execute_reply.started":"2024-11-09T03:52:05.193322Z","shell.execute_reply":"2024-11-09T03:52:06.845859Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> From the plot above, we could see male smokers are likely to spend more on insurance charges..\n\n</div>","metadata":{"papermill":{"duration":0.020484,"end_time":"2022-09-28T16:43:43.11663","exception":false,"start_time":"2022-09-28T16:43:43.096146","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sns.catplot(data=medical_df,x= 'region',y='charges', kind='box', aspect =2, height=8)\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.axhline(medical_df['charges'].mean(), linestyle='--', lw=4, zorder=1, color='red')\nplt.title('Medical Charges Region Wise', fontsize=18)\nplt.xlabel('Region')\nplt.ylabel('Medical Charges ($)')\nplt.show()","metadata":{"papermill":{"duration":0.292607,"end_time":"2022-09-28T16:43:43.429136","exception":false,"start_time":"2022-09-28T16:43:43.136529","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:06.848311Z","iopub.execute_input":"2024-11-09T03:52:06.848958Z","iopub.status.idle":"2024-11-09T03:52:07.319117Z","shell.execute_reply.started":"2024-11-09T03:52:06.848917Z","shell.execute_reply":"2024-11-09T03:52:07.318155Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n\n\n<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> The range of insurance charges incurred by the people from the southeast region is the maximum as compared to the people in northeast and southeast.\n\n</div>","metadata":{"papermill":{"duration":0.020636,"end_time":"2022-09-28T16:43:43.470947","exception":false,"start_time":"2022-09-28T16:43:43.450311","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sns.barplot(data=medical_df,x= 'smoker',y='charges', hue='smoker', alpha=0.7, dodge=False)\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.axhline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='red')\nplt.title('Insurance Charges smoker Wise', fontsize=18)\nplt.ylabel('Medical Charges ($)')\nplt.show()","metadata":{"papermill":{"duration":0.283047,"end_time":"2022-09-28T16:43:43.774706","exception":false,"start_time":"2022-09-28T16:43:43.491659","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:07.320577Z","iopub.execute_input":"2024-11-09T03:52:07.320942Z","iopub.status.idle":"2024-11-09T03:52:07.613331Z","shell.execute_reply.started":"2024-11-09T03:52:07.320905Z","shell.execute_reply":"2024-11-09T03:52:07.612364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> Smokers end up paying more insurance charges than the ones who don't smoke.\n</div>","metadata":{"papermill":{"duration":0.020225,"end_time":"2022-09-28T16:43:43.815805","exception":false,"start_time":"2022-09-28T16:43:43.79558","status":"completed"},"tags":[]}},{"cell_type":"code","source":"query = '''\n\nselect sex, smoker, round(sum(charges),2) as MedicalCharges \nfrom data \ngroup by sex, smoker\n\n'''\n\ntemp = pd.read_sql(query, conn)\nfemale_df = temp[temp['sex']=='female']\nmale_df = temp[temp['sex']=='male']\n# Create the first subplot on the left\nplt.subplot(1, 2, 1)\nplt.bar(x=female_df['smoker'], height=female_df['MedicalCharges'], color='#97E7E1')\nplt.xlabel('Is Smoker?')\nplt.ylabel('MedicalCharges')\nplt.title('Distribution of Medical Charges, Females')\n\n# Turn off top and right spines for the first subplot\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Create the second subplot on the right\nplt.subplot(1, 2, 2)\nplt.bar(x=male_df['smoker'], height=male_df['MedicalCharges'], color='#6AD4DD')\nplt.xlabel('Is Smoker?')\nplt.ylabel('MedicalCharges')\nplt.title('Distribution of Medical Charges, Males')\n\n# Turn off top and right spines for the second subplot\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:07.618023Z","iopub.execute_input":"2024-11-09T03:52:07.618302Z","iopub.status.idle":"2024-11-09T03:52:07.96883Z","shell.execute_reply.started":"2024-11-09T03:52:07.618276Z","shell.execute_reply":"2024-11-09T03:52:07.967924Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> Females who don't smoke pay higher charges while males who smoke pay more charges\n</div>","metadata":{}},{"cell_type":"code","source":"query = '''\nSELECT \n    CASE\n        WHEN age < 30 THEN 'Young'\n        WHEN age < 60 THEN 'Adult'\n        ELSE 'Elderly'\n    END AS age_group,\n    ROUND(AVG(charges), 2) AS AvgMedicalCharges\nFROM \n    data\nwhere smoker='yes'\nGROUP BY \n    CASE\n        WHEN age < 30 THEN 'Young'\n        WHEN age < 60 THEN 'Adult'\n        ELSE 'Elderly'\n    END;\n\n'''\n\ntemp = pd.read_sql(query, conn)\ncustom_palette = ['#C9CCD5', '#E23E57', '#C9CCD5']\n\nsns.barplot(data=temp,x= 'age_group',y='AvgMedicalCharges', hue='age_group', dodge=False, palette=custom_palette)\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.axhline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.title('Smokers AvgMedicalCharges Age Bracket Wise', fontsize=18)\nplt.ylabel('Avg Medical Charges ($)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:07.969909Z","iopub.execute_input":"2024-11-09T03:52:07.970179Z","iopub.status.idle":"2024-11-09T03:52:08.324673Z","shell.execute_reply.started":"2024-11-09T03:52:07.970154Z","shell.execute_reply":"2024-11-09T03:52:08.323743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = '''\nSELECT \n    CASE\n        WHEN age < 30 THEN 'Young'\n        WHEN age < 60 THEN 'Adult'\n        ELSE 'Elderly'\n    END AS age_group,\n    ROUND(AVG(charges), 2) AS AvgMedicalCharges\nFROM \n    data\nwhere smoker='no'\nGROUP BY \n    CASE\n        WHEN age < 30 THEN 'Young'\n        WHEN age < 60 THEN 'Adult'\n        ELSE 'Elderly'\n    END;\n\n'''\n\ntemp = pd.read_sql(query, conn)\n\nsns.barplot(data=temp,x= 'age_group',y='AvgMedicalCharges', hue='age_group', dodge=False,palette=custom_palette )\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.axhline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.title('Non Smokers AvgMedicalCharges Age Bracket Wise', fontsize=18)\nplt.ylabel('Avg Medical Charges ($)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:08.326105Z","iopub.execute_input":"2024-11-09T03:52:08.326518Z","iopub.status.idle":"2024-11-09T03:52:08.601534Z","shell.execute_reply.started":"2024-11-09T03:52:08.326479Z","shell.execute_reply":"2024-11-09T03:52:08.600638Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> Adult and Young Folks who smoke spend more on insurance charges as compared to their counterparts who don't\n</div>","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nN, bins, patches = ax.hist(np.array(medical_df.charges), edgecolor='white', color='lightgray',linewidth=5, alpha=0.7)\nfor i in range(0,1):\n    patches[i].set_facecolor('orange')\n    plt.title('Medical Charges Histogram', fontsize=18)\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    plt.xlabel('Medical Charges ($)')\n    plt.ylabel('Count')\n    plt.axvline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='blue')\n    plt.annotate(f' Average Medical Charges ($)', (13500, 500), fontsize=14,color='red')\n    plt.show()","metadata":{"papermill":{"duration":0.225147,"end_time":"2022-09-28T16:43:44.061597","exception":false,"start_time":"2022-09-28T16:43:43.83645","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:08.602882Z","iopub.execute_input":"2024-11-09T03:52:08.603203Z","iopub.status.idle":"2024-11-09T03:52:08.82449Z","shell.execute_reply.started":"2024-11-09T03:52:08.603168Z","shell.execute_reply":"2024-11-09T03:52:08.823562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> Most of the people pay insurance charges within the range of 0 to 7k USD.\n</div>","metadata":{"papermill":{"duration":0.020765,"end_time":"2022-09-28T16:43:44.103575","exception":false,"start_time":"2022-09-28T16:43:44.08281","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sns.scatterplot(y=medical_df['charges'], x=medical_df['age'], hue=medical_df['smoker'], alpha=0.5)\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.axhline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.annotate(f'Average Medical Charges ($)', (45, 13900), fontsize=14,color='red')\nplt.title('Age Wise Medical Charges Distribution', fontsize=18)\nplt.ylabel('Medical Charges ($)')\nplt.show()","metadata":{"papermill":{"duration":0.46348,"end_time":"2022-09-28T16:43:44.587984","exception":false,"start_time":"2022-09-28T16:43:44.124504","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:08.825994Z","iopub.execute_input":"2024-11-09T03:52:08.826606Z","iopub.status.idle":"2024-11-09T03:52:09.226451Z","shell.execute_reply.started":"2024-11-09T03:52:08.826567Z","shell.execute_reply":"2024-11-09T03:52:09.225505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n\n\n\n<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> If someone who is in his later years of life and is a smoker, chances are they may end up spending a nice amount on insurance charges as opposed to the ones who are elderly and don't smoke.\n</div>","metadata":{"papermill":{"duration":0.022428,"end_time":"2022-09-28T16:43:44.633744","exception":false,"start_time":"2022-09-28T16:43:44.611316","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sns.scatterplot(y=medical_df['bmi'], x=medical_df['charges'], hue=medical_df['smoker'], alpha=0.5)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nxticks = plt.gca().get_xticks()\nxlabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in xticks]\nplt.gca().set_xticklabels(xlabels)\n\nplt.axvline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.annotate(f'Average Medical Charges ($)', (13400, 52), fontsize=14, color='Red')\n\nplt.title('BMI & Medical Charges relation', fontsize=18)\nplt.xlabel('Medical Charges ($)')\nplt.ylabel('BMI')\nplt.show()","metadata":{"papermill":{"duration":0.345217,"end_time":"2022-09-28T16:43:45.001728","exception":false,"start_time":"2022-09-28T16:43:44.656511","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:09.227744Z","iopub.execute_input":"2024-11-09T03:52:09.228112Z","iopub.status.idle":"2024-11-09T03:52:09.579374Z","shell.execute_reply.started":"2024-11-09T03:52:09.228077Z","shell.execute_reply":"2024-11-09T03:52:09.578394Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.relplot(data=medical_df, x='children', y='charges',  hue='smoker', aspect = 2, height=8)\nyticks = plt.gca().get_yticks()\nylabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in yticks]\nplt.gca().set_yticklabels(ylabels)\nplt.axhline(medical_df['charges'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.annotate(f'Average Medical Charges ($)', (4.05 , 13900), fontsize=12, color='red')\nplt.title('Children & Medical Charges relation', fontsize=18)\nplt.ylabel('Medical Charges ($)')\nplt.xlabel('Number of Children')\nplt.show()","metadata":{"papermill":{"duration":0.521673,"end_time":"2022-09-28T16:43:45.549569","exception":false,"start_time":"2022-09-28T16:43:45.027896","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:09.58068Z","iopub.execute_input":"2024-11-09T03:52:09.580984Z","iopub.status.idle":"2024-11-09T03:52:10.293284Z","shell.execute_reply.started":"2024-11-09T03:52:09.580956Z","shell.execute_reply":"2024-11-09T03:52:10.292329Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n\n<div class=\"alert alert-block alert-info\">  \n<b>INTERPRETATION :</b> People with 0, 1 or 3 children are likely to spend more on insurance charges..\n</div>","metadata":{"papermill":{"duration":0.026616,"end_time":"2022-09-28T16:43:45.60345","exception":false,"start_time":"2022-09-28T16:43:45.576834","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sns.boxplot(x=medical_df['charges'])\nxticks = plt.gca().get_xticks()\nxlabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in xticks]\nplt.gca().set_xticklabels(xlabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.axvline(medical_df['charges'].mean(), linestyle='--', lw=4, zorder=1, color='red')\nplt.annotate(f'Average Medical Charges ($)', (13800, 0.47), fontsize=15, color='red')\nplt.title('Outliers in the data', fontsize=18)\nplt.xlabel('Medical Charges ($)')\nplt.show()","metadata":{"papermill":{"duration":0.189495,"end_time":"2022-09-28T16:43:45.820002","exception":false,"start_time":"2022-09-28T16:43:45.630507","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:10.294746Z","iopub.execute_input":"2024-11-09T03:52:10.295575Z","iopub.status.idle":"2024-11-09T03:52:10.460836Z","shell.execute_reply.started":"2024-11-09T03:52:10.295534Z","shell.execute_reply":"2024-11-09T03:52:10.459684Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DATA PREPROCESSING\n\n### Outlier Detection And Removal","metadata":{"papermill":{"duration":0.02703,"end_time":"2022-09-28T16:43:45.874233","exception":false,"start_time":"2022-09-28T16:43:45.847203","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"What constitutes an outlier? In essence, it denotes a data point that substantially deviates from the rest within a dataset. Although this description appears straightforward, the determination of what qualifies as an outlier is notably subjective, contingent upon the specific study and the comprehensive scope of data being gathered.\n\n**Histograms emerge as a potent tool for unveiling outliers within univariate (single-variable) data**. By segmenting the value range into distinct groups, histograms visualize the frequency of data occurrences through bar charts. When organized sequentially, these data groups readily unveil outliers residing at the extreme ends of the histogram, either on the far left or far right.\n\n","metadata":{"papermill":{"duration":0.026735,"end_time":"2022-09-28T16:43:45.928205","exception":false,"start_time":"2022-09-28T16:43:45.90147","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ax = sns.histplot(medical_df['charges'], kde=True, color='lightgray')\nxticks = plt.gca().get_xticks()\nxlabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in xticks]\nplt.gca().set_xticklabels(xlabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False);\nax.lines[0].set_color('red')\nplt.axvline(medical_df['charges'].mean(), linestyle='--', lw=3, zorder=1, color='blue')\nplt.annotate(f'Average Medical Charges ($)', (13700, 200), fontsize=15, color='blue')\nplt.title('Detecting Outliers Using Histogram', fontsize=18)\nplt.xlabel('Medical Charges ($)')\nplt.show()","metadata":{"papermill":{"duration":0.374934,"end_time":"2022-09-28T16:43:46.330353","exception":false,"start_time":"2022-09-28T16:43:45.955419","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:10.462703Z","iopub.execute_input":"2024-11-09T03:52:10.463104Z","iopub.status.idle":"2024-11-09T03:52:10.754504Z","shell.execute_reply.started":"2024-11-09T03:52:10.463065Z","shell.execute_reply":"2024-11-09T03:52:10.75346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract the 'charges' column values from the medical_df DataFrame and sort them\ndata = sorted(medical_df['charges'].values)\n\n# Calculate the mean and standard deviation of the sorted 'charges' data\ndata_mean, data_std = np.mean(data), np.std(data)\n\n# Calculate the cutoff value, which is three times the standard deviation\ncut_off = data_std * 3\n\n# Calculate the lower and upper bounds for identifying potential outliers\nlower, upper = data_mean - cut_off, data_mean + cut_off\n\n# Print the calculated values with appropriate labels\nprint('Cut Off =', round(cut_off, 3))  # Display the calculated cutoff value\nprint('Lower =', round(lower, 3))      # Display the calculated lower bound for potential outliers\nprint('Upper =', round(upper, 3))      # Display the calculated upper bound for potential outliers","metadata":{"papermill":{"duration":0.041147,"end_time":"2022-09-28T16:43:46.399528","exception":false,"start_time":"2022-09-28T16:43:46.358381","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:10.755843Z","iopub.execute_input":"2024-11-09T03:52:10.756151Z","iopub.status.idle":"2024-11-09T03:52:10.765122Z","shell.execute_reply.started":"2024-11-09T03:52:10.756121Z","shell.execute_reply":"2024-11-09T03:52:10.763968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = sns.histplot(medical_df['charges'], kde=True, color='lightgray')\nxticks = plt.gca().get_xticks()\nxlabels = [f\"{round(i / 1000)}k\" if i != 0 else \"0\" for i in xticks]\nplt.gca().set_xticklabels(xlabels)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False);\nax.lines[0].set_color('red')\nplt.axvline(data_mean, linestyle='--', lw=2, zorder=1, color='orange')\nplt.annotate(f'Average', (data_mean+500, 175), fontsize=15, color='blue')\n\nplt.axvline(upper, linestyle='--', lw=2, zorder=1, color='orange')\nplt.annotate(f'Upper', (upper+500, 175), fontsize=15, color='blue')\n\nplt.axvline(cut_off, linestyle='--', lw=2, zorder=1, color='orange')\nplt.annotate(f'Cut Off', (cut_off+500, 175), fontsize=15, color='blue')\n\nplt.title('Detecting Outliers', fontsize=18)\nplt.xlabel('Medical Charges ($)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:10.766508Z","iopub.execute_input":"2024-11-09T03:52:10.766875Z","iopub.status.idle":"2024-11-09T03:52:11.072844Z","shell.execute_reply.started":"2024-11-09T03:52:10.766837Z","shell.execute_reply":"2024-11-09T03:52:11.071841Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"medical_df = medical_df[medical_df['charges'] < upper]\nmedical_df = medical_df[medical_df['charges'] > lower]\nprint('The shape of our dataframe after the Outlier Removal is', medical_df.shape)","metadata":{"papermill":{"duration":0.040567,"end_time":"2022-09-28T16:43:46.468151","exception":false,"start_time":"2022-09-28T16:43:46.427584","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.074031Z","iopub.execute_input":"2024-11-09T03:52:11.074316Z","iopub.status.idle":"2024-11-09T03:52:11.082087Z","shell.execute_reply.started":"2024-11-09T03:52:11.07429Z","shell.execute_reply":"2024-11-09T03:52:11.08115Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Making a copy of the original dataframe to ensure the original data stays intact.","metadata":{"papermill":{"duration":0.028384,"end_time":"2022-09-28T16:43:46.524799","exception":false,"start_time":"2022-09-28T16:43:46.496415","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = medical_df.copy()","metadata":{"papermill":{"duration":0.03958,"end_time":"2022-09-28T16:43:46.593301","exception":false,"start_time":"2022-09-28T16:43:46.553721","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.083492Z","iopub.execute_input":"2024-11-09T03:52:11.083962Z","iopub.status.idle":"2024-11-09T03:52:11.091604Z","shell.execute_reply.started":"2024-11-09T03:52:11.083925Z","shell.execute_reply":"2024-11-09T03:52:11.090673Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:white;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:black;\">OneHotEncoding and Scaling\n    \n</h2>\n</div>\n    \n</center>\n\n","metadata":{"papermill":{"duration":0.02805,"end_time":"2022-09-28T16:43:46.649778","exception":false,"start_time":"2022-09-28T16:43:46.621728","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### OneHotEncoding","metadata":{"papermill":{"duration":0.027882,"end_time":"2022-09-28T16:43:46.706206","exception":false,"start_time":"2022-09-28T16:43:46.678324","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"![](https://i.imgur.com/n8GuiOO.png)\n\nRead More on Encoding data <a href=\"https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\"> here</a>","metadata":{"papermill":{"duration":0.027763,"end_time":"2022-09-28T16:43:46.762287","exception":false,"start_time":"2022-09-28T16:43:46.734524","status":"completed"},"tags":[]}},{"cell_type":"code","source":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\ncat_cols = df.select_dtypes(include='object').columns\nencoder.fit(df[cat_cols])","metadata":{"papermill":{"duration":0.044552,"end_time":"2022-09-28T16:43:46.835093","exception":false,"start_time":"2022-09-28T16:43:46.790541","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.092997Z","iopub.execute_input":"2024-11-09T03:52:11.093292Z","iopub.status.idle":"2024-11-09T03:52:11.106439Z","shell.execute_reply.started":"2024-11-09T03:52:11.093265Z","shell.execute_reply":"2024-11-09T03:52:11.105533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"onehot =encoder.transform(df[cat_cols])\nencoder.categories_","metadata":{"papermill":{"duration":0.042658,"end_time":"2022-09-28T16:43:46.905974","exception":false,"start_time":"2022-09-28T16:43:46.863316","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.107731Z","iopub.execute_input":"2024-11-09T03:52:11.108119Z","iopub.status.idle":"2024-11-09T03:52:11.119263Z","shell.execute_reply.started":"2024-11-09T03:52:11.10808Z","shell.execute_reply":"2024-11-09T03:52:11.118368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_cols = [['female', 'male','smokerno', 'smokeryes', 'northeast', 'northwest', 'southeast', 'southwest']]\ndf[['female', 'male','smokerno', 'smokeryes', 'northeast', 'northwest', 'southeast', 'southwest']] = onehot\ndf.drop(cat_cols, axis=1, inplace=True )","metadata":{"papermill":{"duration":0.04372,"end_time":"2022-09-28T16:43:46.977925","exception":false,"start_time":"2022-09-28T16:43:46.934205","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.120322Z","iopub.execute_input":"2024-11-09T03:52:11.120634Z","iopub.status.idle":"2024-11-09T03:52:11.137318Z","shell.execute_reply.started":"2024-11-09T03:52:11.120608Z","shell.execute_reply":"2024-11-09T03:52:11.136361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"papermill":{"duration":0.053415,"end_time":"2022-09-28T16:43:47.059837","exception":false,"start_time":"2022-09-28T16:43:47.006422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.13854Z","iopub.execute_input":"2024-11-09T03:52:11.138987Z","iopub.status.idle":"2024-11-09T03:52:11.165098Z","shell.execute_reply.started":"2024-11-09T03:52:11.138959Z","shell.execute_reply":"2024-11-09T03:52:11.164214Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Initializing a scaler object\nscaler = MinMaxScaler()","metadata":{"papermill":{"duration":0.059631,"end_time":"2022-09-28T16:43:47.320704","exception":false,"start_time":"2022-09-28T16:43:47.261073","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.166456Z","iopub.execute_input":"2024-11-09T03:52:11.167086Z","iopub.status.idle":"2024-11-09T03:52:11.175199Z","shell.execute_reply.started":"2024-11-09T03:52:11.167046Z","shell.execute_reply":"2024-11-09T03:52:11.174221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract the 'charges' column values and store them in y\ny = df['charges'].values\n\n# Drop the 'charges' column from the DataFrame to create the feature matrix X\nX = df.drop('charges', axis=1).values","metadata":{"papermill":{"duration":0.040583,"end_time":"2022-09-28T16:43:47.44867","exception":false,"start_time":"2022-09-28T16:43:47.408087","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.182643Z","iopub.execute_input":"2024-11-09T03:52:11.1829Z","iopub.status.idle":"2024-11-09T03:52:11.188699Z","shell.execute_reply.started":"2024-11-09T03:52:11.182876Z","shell.execute_reply":"2024-11-09T03:52:11.187729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('The shape of independent variables data is',X.shape)\nprint('The shape of the target variable data is',y.shape)","metadata":{"papermill":{"duration":0.038743,"end_time":"2022-09-28T16:43:47.516329","exception":false,"start_time":"2022-09-28T16:43:47.477586","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.190018Z","iopub.execute_input":"2024-11-09T03:52:11.191226Z","iopub.status.idle":"2024-11-09T03:52:11.199458Z","shell.execute_reply.started":"2024-11-09T03:52:11.191194Z","shell.execute_reply":"2024-11-09T03:52:11.198553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Finding the derivative equation\n    \n</h2>\n</div>\n    \n</center>","metadata":{}},{"cell_type":"markdown","source":"$$\\large MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\n**Differentiating w.r.t to the weights**\n\nLet start calculating the derivate of the MSE function using calculus with respect to the weights and the biases both. Let's say that using Power Rule we will get\n\n$$\\frac{dy}{dw} =  \\frac{1}{n} \\sum_{i=1}^{n} \\space 2({y_{i} - \\hat{y}_{i}})$$\n\nNow we will have to differentiate the term $y_{i} - \\hat{y}_{i}$. $y_{i}$ are constant values and constant values when differentiating become zero. We know from earlier that $\\hat{y}_{i} = mx + b$ so we can substitute this value and we will get \n\n$$\\frac{d}{dw} (y_{i} - \\hat{y}_{i}) =  0 - (m * x + b)$$\n\n\nwhich can then be simplified as  $\\frac{d}{dw}  (y_{i} - \\hat{y}_{i}) =  0 - (1 * x + 0)$ because $m$ and $w$ are the same thing and $b$ here is just another constant value so \n\n$$\\frac{d}{dw}  (y_{i} - \\hat{y}_{i}) =  -x $$\n\nFinally our derivative with respect to the weights will be \n\n\n$$\\frac{dy}{dw} =  \\frac{-2}{n} \\sum_{i=1}^{n} \\space ({y_{i} - \\hat{y}_{i}}) \\space {x}$$\n\n**Differentiating w.r.t to the bias**\n\nFor this, the steps are almost identical with a slight change in how we will calculate derivative of $y_{i} - \\hat{y}_{i}$. First off let's apply the Power Rule which will get us:-\n\n$$\\frac{dy}{dw} =  \\frac{1}{n} \\sum_{i=1}^{n} \\space 2({y_{i} - \\hat{y}_{i}})$$\n\nNow we will have to differentiate the term $y_{i} - \\hat{y}_{i}$. $y_{i}$ are constant values and constant values when differentiating become zero. We know from earlier that $\\hat{y}_{i} = mx + b$ so we can substitute this value and we will get \n\n$$\\frac{d}{dw} (y_{i} - \\hat{y}_{i}) =  0 - (m * x + b)$$\n\nwhich can then be simplified as $\\frac{d}{dw}  (y_{i} - \\hat{y}_{i}) =  0 - (0 + 1)$ because whenever a third term comes into the equation, we zero it out and here $m$ and $x$ are the third terms since we are differentiating the cost with respect to $b$ so now\n\n$$\\frac{d}{dw} (y_{i} - \\hat{y}_{i}) =  -1 $$\n\nFinally our derivative with respect to the weights will be \n\n\n$$\\frac{dy}{dw} =  \\frac{-2}{n} \\sum_{i=1}^{n} \\space ({y_{i} - \\hat{y}_{i}})$$\n\n\n**Final derivative**\n\n$$\\frac{\\partial{y}}{\\partial{x}_{b}} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$\n\n\n$$\\frac{\\partial{y}}{\\partial{x}_{w}} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\space x$$\n\n\n**Parameter update rule** \n\n$$ W_{new} = W_{old} - \\eta \\space \\frac {dL}{dW}$$\n\n$$b_{new} = b_{old} - \\eta \\space \\frac {dL}{db}$$","metadata":{"papermill":{"duration":0.028461,"end_time":"2022-09-28T16:43:47.641234","exception":false,"start_time":"2022-09-28T16:43:47.612773","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Visualization of how gradient descent works\n    \n</h2>\n</div>\n    \n</center>\n\n![](https://baptiste-monpezat.github.io/6f5cc892ec345d96f64c881b62b0d910/gradient_descent_parameter_a.gif)","metadata":{}},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Building a model and training\n    \n</h2>\n</div>\n    \n</center>","metadata":{}},{"cell_type":"code","source":"# Function for calculating the EarlyStopping criteria by comparing consecutive losses\ndef EarlyStopping(loss):\n    for i in range(1, len(loss)):\n        yield (loss[i-1], loss[i])\n\n# Function to determine the number of batches based on batchsize and the size of the dataset\ndef batch_size(batchsize, X):\n    batches = round(X.shape[0] // batchsize)\n    return batches\n\n# Gradient Descent Function for linear regression\ndef regression_gradient_descent(X_train, y_train, m, b):\n    # Predictions using the linear equation y = mx + b\n    yhat = np.dot(X_train, m) + b\n\n    # Mean Squared Error (MSE) calculation\n    MSE = (np.sum((y_train - yhat)**2)) / N\n\n    # R-squared calculation to measure the goodness of fit\n    r_squared = r2_score(y_train, yhat)\n\n    # Calculate the gradient of the loss function with respect to intercept 'b'\n    loss_slope_b = -(2/N) * sum(y_train - yhat)\n\n    # Calculate the gradient of the loss function with respect to the slope 'm'\n    loss_slope_m = -(2/N) * (np.dot((y_train - yhat), X_train))\n\n    # Update the slope 'm' and intercept 'b' using the gradient descent update rule\n    m = m - (learning_rate * loss_slope_m)\n    b = b - (learning_rate * loss_slope_b)\n\n    return m, b, MSE, r_squared","metadata":{"papermill":{"duration":0.037756,"end_time":"2022-09-28T16:43:47.776526","exception":false,"start_time":"2022-09-28T16:43:47.73877","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.200508Z","iopub.execute_input":"2024-11-09T03:52:11.200812Z","iopub.status.idle":"2024-11-09T03:52:11.21112Z","shell.execute_reply.started":"2024-11-09T03:52:11.200785Z","shell.execute_reply":"2024-11-09T03:52:11.210114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's go over the code above \n\n**Code Explanation**\n\nThis code snippet comprises three essential functions: `EarlyStopping`, `batch_size`, and `regression_gradient_descent`.\n\n**EarlyStopping Function**\n\nThe `EarlyStopping` function is designed to facilitate early stopping during training by comparing consecutive loss values. It is implemented as a generator, which sequentially provides pairs of adjacent loss values. This functionality aids in monitoring the optimization process and determining when to halt training based on loss trends.\n\n**`batch_size` Function**\n\nThe `batch_size` function serves the purpose of determining the optimal number of batches in a dataset based on a specified batch size. Given the desired batch size and the total size of the dataset, this function calculates and returns the appropriate number of batches for efficient processing during training.\n\n**`regression_gradient_descent`**\n\nThe `regression_gradient_descent` function plays a central role in performing gradient descent optimization specifically tailored for linear regression tasks. Within this function, the following tasks are executed:\n\n1. **Prediction**: Utilizing the linear regression equation (y = mx + b), the function computes predicted values (yhat) based on the input features (X_train) and the slope (m) and intercept (b) coefficients.\n\n2. **Mean Squared Error (MSE) Calculation**: The function calculates the Mean Squared Error by evaluating the squared differences between the true target values (y_train) and the predicted values (yhat). The sum of these squared differences is divided by the number of samples (N) to yield the MSE.\n\n3. **R-squared Computation**: The R-squared metric is determined to assess the quality of the linear regression fit. It measures the proportion of the variance in the target variable explained by the model's predictions.\n\n4. **Gradient Calculation and Parameter Update**: The gradients of the loss function with respect to the intercept (b) and slope (m) are computed. These gradients guide the adjustment of the parameters to minimize the loss. The learning rate (learning_rate) determines the step size of the parameter updates.\n\nBy integrating these steps, the `regression_gradient_descent` function optimizes the linear regression model by iteratively adjusting the slope and intercept parameters to minimize the loss function and improve the model's predictive performance.","metadata":{}},{"cell_type":"code","source":"#Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:11.212312Z","iopub.execute_input":"2024-11-09T03:52:11.212728Z","iopub.status.idle":"2024-11-09T03:52:11.221268Z","shell.execute_reply.started":"2024-11-09T03:52:11.212691Z","shell.execute_reply":"2024-11-09T03:52:11.220385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:11.222445Z","iopub.execute_input":"2024-11-09T03:52:11.222722Z","iopub.status.idle":"2024-11-09T03:52:11.234175Z","shell.execute_reply.started":"2024-11-09T03:52:11.222696Z","shell.execute_reply":"2024-11-09T03:52:11.233257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Calculate the total number of data points\nN = X.shape[0]\n\n# Set the initial learning rate, decay rate, and other variables\nlearning_rate = 0.2\ndecay_rate = 0.01\nLR = []  # List to store learning rates\nValidationLoss = []  # List to store validation loss\nTrainingloss = []  # List to store training loss\nbatchsize = 30\n\n# Initialize the intercept and slope\nIntercept = []\nSlope = []\nm = np.ones(X.shape[1])  # Initializing some initial values for the slope\nb = 1  # Initializing some initial values for the intercept\nprint('The initial Value of w and b are', m, b)\n\n# Calculate the number of batches\nbatches = batch_size(batchsize, X)\n\nnum_epochs =  2000\n\n# Loop over multiple epochs for training\nfor i in range(num_epochs):\n    epoch = i\n    \n    # Loop over batches within the current epoch\n    for j in range(batches):\n        if i == 0:\n            # Initial epoch updates (no decay rate applied)\n            if j % batchsize == 0:\n                learning_rate = learning_rate  # Maintain initial learning rate\n                np.random.seed(0)\n                np.random.shuffle([X_train_scaled, y_train])\n                m, b, MSE, r_squared = regression_gradient_descent(X_train_scaled, y_train, m, b)\n                m_test, b_test, MSE_test, r_squared_test = regression_gradient_descent(X_test_scaled, y_test, m, b)\n            else:\n                m = m\n                b = b\n        else:\n            # Updates with decay rate applied\n            if j % batchsize == 0:\n                learning_rate = [(1 / (1 + decay_rate)) * learning_rate for j in range(batches)][0]\n                np.random.seed(0)\n                np.random.shuffle([X_train_scaled, y_train])\n                m, b, MSE, r_squared = regression_gradient_descent(X_train_scaled, y_train, m, b)\n                m_test, b_test, MSE_test, r_squared_test = regression_gradient_descent(X_test_scaled, y_test, m, b)\n            else:\n                m = m\n                b = b\n\n    # Store values for analysis and tracking\n    Intercept.append(b)\n    Slope.append(m)\n    Trainingloss.append(MSE)\n    ValidationLoss.append(MSE_test)\n    LR.append(learning_rate)\n    \n    # Print progress every 20 epochs\n    if i % 100 == 0:\n        print(f'Epoch: {i}/{num_epochs} [==============================] - Loss: {MSE:.2e} - val Loss: {MSE_test:.2e} - r-squared: {round(r_squared, 4)} - val_r-squared: {round(r_squared_test, 4)}')\n\n       \n    # Early Stopping Mechanism\n    for prev, curr in EarlyStopping(ValidationLoss):\n        if prev - curr < 1e-6:\n            print(f'\\n -- Early Stopping at Epoch : {i} val_loss : {np.around(MSE_test, 5)},  val r-squared : {np.around(r_squared_test, 5)} --')\n            break  # Inner Loop Break\n    else: \n        continue  # Executed if the inner loop did NOT break\n    break  # Executed if the inner loop DID break\n\n# Print the final values of slope 'm' and intercept 'b'\nprint('\\nThe final values of w and b are', m, b)","metadata":{"papermill":{"duration":0.38173,"end_time":"2022-09-28T16:43:48.18786","exception":false,"start_time":"2022-09-28T16:43:47.80613","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:11.235584Z","iopub.execute_input":"2024-11-09T03:52:11.235857Z","iopub.status.idle":"2024-11-09T03:52:17.506968Z","shell.execute_reply.started":"2024-11-09T03:52:11.235831Z","shell.execute_reply":"2024-11-09T03:52:17.505431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Code Explanation**\n\nThis code snippet trains a linear regression model using gradient descent and implements an early stopping mechanism for improved training efficiency. Here's a summary of its key functionalities:\n\n1. **Setup and Initialization:**\n   - A random seed is set to ensure reproducibility.\n   - Key parameters like learning rate, decay rate, and batch size are initialized.\n   - Lists are prepared to store training-related metrics.\n\n2. **Training Loop:**\n   - The code iterates over multiple epochs, representing passes through the entire dataset.\n   - Within each epoch, the code loops over data batches to perform updates.\n   - The learning rate can decay over epochs to fine-tune learning.\n\n3. **Gradient Descent Updates:**\n   - Gradient descent updates are performed to adjust the model's slope (m) and intercept (b).\n   - Training and validation Mean Squared Error (MSE) and R-squared metrics are calculated.\n\n4. **Results Tracking and Printing:**\n   - Parameters like intercept, slope, and loss values are stored for analysis.\n   - Progress is printed periodically, showing the current epoch, loss, and R-squared metrics.\n\n5. **Early Stopping Mechanism:**\n   - The code monitors validation loss trends using an early stopping approach.\n   - If the validation loss fails to decrease significantly, the training is halted early.","metadata":{}},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Making Predictions\n    \n</h2>\n</div>\n    \n</center>","metadata":{}},{"cell_type":"code","source":"# Get the coefficient (slope) and intercept values for the current epoch\ncoefficient = Slope[epoch]\nintercept = Intercept[epoch]\n\n# Predictions using the linear regression equation y = mx + b\ny_pred = np.dot(X_test_scaled, coefficient) + intercept\n\n# Create a DataFrame to compare predicted and actual values\ndf = pd.DataFrame(y_pred, y_test, columns=['y']).reset_index().rename(columns={'index': 'y', 'y': 'y_pred'})\n\n# Plotting the Figures\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 5))\nfig.subplots_adjust(hspace=.2, wspace=.3)\n\n# First Plot: Training Loss Plot\nax1.plot(Trainingloss, linestyle='--')\nax1.set_title(\"Training Loss Plot, Regression\")\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Training Loss')\n\n# Second Plot: Validation Loss Plot\nax2.plot(ValidationLoss, 'tab:orange', linestyle='dashed', markersize=5)\nax2.set_title(\"Validation Loss Plot, Regression\")\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Validation Loss')\n\n# Third Plot: Scatter Plot of Targets vs. Predictions\nax3 = sns.regplot(data=df, x=df['y'], y=df['y_pred'], color='lightgray', fit_reg=True)\nax3.lines[0].set_color('red')\nax3.set_title('Targets vs Prediction, Regression')\nax3.set_xlabel('Targets')\nax3.set_ylabel('Prediction')\n\n# Display the plots\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:17.513101Z","iopub.execute_input":"2024-11-09T03:52:17.517273Z","iopub.status.idle":"2024-11-09T03:52:18.32971Z","shell.execute_reply.started":"2024-11-09T03:52:17.517197Z","shell.execute_reply":"2024-11-09T03:52:18.328641Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n<center>\n<div class=\"alert alert-block alert-success\">  \nBy Implementing a Mini-Batch Gradient Descent we have been able to accomplish the same results in lesser epochs. This concept really helps when we have large amounts of data to work with..\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.031553,"end_time":"2022-09-28T16:43:48.908305","exception":false,"start_time":"2022-09-28T16:43:48.876752","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Gradient Descent for Classification\n    \n</h2>\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.032599,"end_time":"2022-09-28T16:43:50.71192","exception":false,"start_time":"2022-09-28T16:43:50.679321","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Logistic regression emerges as a powerful tool tailored for tackling classification quandaries, particularly those where the outcome is characterized by discrete, categorical values. This method finds its prime application in the realm of binary classification, an endeavor marked by the presence of two distinct possible outcomes.\n\nAs the nomenclature implies, binary classification scenarios encompass a dichotomous spectrum of choices. Within this framework, the sigmoid function, also known as the logistic function, takes center stage. It serves as a transformative conduit, orchestrating the metamorphosis of input values spanning a vast continuum into a confined interval.\n\nThe sigmoid function's role proves pivotal; it reconfigures the expansive gamut of input values into a compact range, enabling the classification process to operate within defined boundaries. In this way, logistic regression deftly navigates the nuances of binary classification, molding intricate input-output relationships into a comprehensible and actionable form.\n\n<center>\n<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2021/01/log-reg-sigmoid.png\">\n</center>\n","metadata":{"papermill":{"duration":0.031297,"end_time":"2022-09-28T16:43:50.775253","exception":false,"start_time":"2022-09-28T16:43:50.743956","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The Sigmoid Function formula encapsulates the probability associated with observing the output $y = 1$ from a Bernoulli random variable—an entity that assumes values of either 1 or 0. While linear regression employs the mean squared error (MSE) as its preferred cost function, logistic regression takes a distinct route.\n\nIn logistic regression, opting for the mean of squared discrepancies between actual and predicted outcomes as the cost function could lead to a convoluted, undulating solution landscape. This intricate terrain may host numerous local optima, contributing to heightened complexity during optimization processes.\n\n<center>\n<img src=\"https://miro.medium.com/max/992/1*eetskzKtJEDvzYnINgXpmQ.png\">\n</center>\n\n\nMathematically the sigmoid is defined by the formula:\n$$ y =  \\frac {1}{1+e^{-x}}$$","metadata":{"papermill":{"duration":0.031481,"end_time":"2022-09-28T16:43:50.838558","exception":false,"start_time":"2022-09-28T16:43:50.807077","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Loading the Data\ndata = load_breast_cancer()\ncancerdf = pd.DataFrame(data=data.data, columns=data.feature_names)\ncancerdf['target'] = data.target\ncols = list(cancerdf.columns)\ncancerdf.columns = [i.replace(\" \",\"\") for i in cols]\ncancerdf.head()","metadata":{"papermill":{"duration":0.080031,"end_time":"2022-09-28T16:43:50.950451","exception":false,"start_time":"2022-09-28T16:43:50.87042","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:18.331007Z","iopub.execute_input":"2024-11-09T03:52:18.331308Z","iopub.status.idle":"2024-11-09T03:52:18.390254Z","shell.execute_reply.started":"2024-11-09T03:52:18.33128Z","shell.execute_reply":"2024-11-09T03:52:18.389364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cancerdf.describe().T","metadata":{"papermill":{"duration":0.125402,"end_time":"2022-09-28T16:43:51.109748","exception":false,"start_time":"2022-09-28T16:43:50.984346","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:18.391588Z","iopub.execute_input":"2024-11-09T03:52:18.392334Z","iopub.status.idle":"2024-11-09T03:52:18.484098Z","shell.execute_reply.started":"2024-11-09T03:52:18.392293Z","shell.execute_reply":"2024-11-09T03:52:18.483132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cancerdf.info()","metadata":{"papermill":{"duration":0.053719,"end_time":"2022-09-28T16:43:51.19651","exception":false,"start_time":"2022-09-28T16:43:51.142791","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:52:18.48544Z","iopub.execute_input":"2024-11-09T03:52:18.485837Z","iopub.status.idle":"2024-11-09T03:52:18.499317Z","shell.execute_reply.started":"2024-11-09T03:52:18.485797Z","shell.execute_reply":"2024-11-09T03:52:18.498393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cancerdf.to_sql('cancer_data', conn, if_exists='replace', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T03:52:18.50059Z","iopub.execute_input":"2024-11-09T03:52:18.500903Z","iopub.status.idle":"2024-11-09T03:52:18.544719Z","shell.execute_reply.started":"2024-11-09T03:52:18.500875Z","shell.execute_reply":"2024-11-09T03:52:18.543981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Exploratory Data Analysis\n    \n</h2>\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.032922,"end_time":"2022-09-28T16:43:51.263656","exception":false,"start_time":"2022-09-28T16:43:51.230734","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sns.countplot(cancerdf['target'], hue= cancerdf['target'],dodge=False)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.title('Target Value Count', fontsize=20)\nplt.xlabel('Target Value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:18.545879Z","iopub.execute_input":"2024-11-09T03:52:18.546246Z","iopub.status.idle":"2024-11-09T03:52:18.741566Z","shell.execute_reply.started":"2024-11-09T03:52:18.5462Z","shell.execute_reply":"2024-11-09T03:52:18.740626Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Which features have the highest average values for malignant cases (target = 1) compared to benign cases (target = 0)?","metadata":{}},{"cell_type":"code","source":"query = '''\nSELECT \n    target,\n    AVG(meanradius) AS avg_mean_radius,\n    AVG(meantexture) AS avg_mean_texture,\n    AVG(meanperimeter) AS avg_mean_perimeter,\n    AVG(meanarea) AS avg_mean_area,\n    AVG(meansmoothness) AS avg_mean_smoothness\n    \nFROM \n    cancer_data\nGROUP BY \n    target;\n'''\ntemp = pd.read_sql(query, conn)\ntemp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T03:52:18.742932Z","iopub.execute_input":"2024-11-09T03:52:18.743323Z","iopub.status.idle":"2024-11-09T03:52:18.75838Z","shell.execute_reply.started":"2024-11-09T03:52:18.743282Z","shell.execute_reply":"2024-11-09T03:52:18.757356Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Benign tumors (target = 0) have higher average values for mean_radius, mean_texture, mean_perimeter, and mean_area compared to malignant tumors (target = 1), while the mean_smoothness is slightly higher for benign tumors, suggesting that benign tumors tend to be larger and smoother on average.","metadata":{}},{"cell_type":"markdown","source":"Do “worst” metrics have higher values than “mean” metrics for malignant cases?","metadata":{}},{"cell_type":"code","source":"query = '''\nSELECT \ntarget,\n    AVG(meanradius) AS avg_mean_radius,\n    AVG(worstradius) AS avg_worst_radius,\n    AVG(meanperimeter) AS avg_mean_perimeter,\n    AVG(worstperimeter) AS avg_worst_perimeter,\n    AVG(meanarea) AS avg_mean_area,\n    AVG(worstarea) AS avg_worst_area\n    \nFROM \n    cancer_data\nGROUP BY \n    target;\n'''\ntemp = pd.read_sql(query, conn)\ntemp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T03:52:18.759906Z","iopub.execute_input":"2024-11-09T03:52:18.760408Z","iopub.status.idle":"2024-11-09T03:52:18.775973Z","shell.execute_reply.started":"2024-11-09T03:52:18.760366Z","shell.execute_reply":"2024-11-09T03:52:18.774979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Benign tumors (target = 0) tend to have larger sizes (radius, perimeter, area) than malignant tumors (target = 1), with “worst” metrics highlighting more compact malignant cases, suggesting these features could help differentiate between the two.","metadata":{}},{"cell_type":"code","source":"ax = sns.histplot(cancerdf['meanarea'], kde=True, color='lightgray')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False);\nax.lines[0].set_color('red')\n\nplt.axvline(cancerdf['meanarea'].mean(), linestyle='--', lw=2, zorder=1, color='blue')\nplt.annotate(f'Average Cancer Mean Area', (680, 70), fontsize=15, color='blue')\n\nplt.title('Cancer Mean Area Distribution')\nplt.xlabel('Cancer Mean Area')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:41.80948Z","iopub.execute_input":"2024-11-09T03:52:41.809877Z","iopub.status.idle":"2024-11-09T03:52:42.167739Z","shell.execute_reply.started":"2024-11-09T03:52:41.809845Z","shell.execute_reply":"2024-11-09T03:52:42.166797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = sns.histplot(cancerdf['meantexture'], kde=True, color='lightgray')\n\nax.lines[0].set_color('crimson')\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.axvline(cancerdf['meantexture'].mean(), linestyle='--', lw=2, zorder=1, color='blue')\nplt.legend(['Histogram Line', 'Avg Cancer Mean Texture'])\nplt.title('Cancer Mean Texture Spread')\nplt.xlabel('Mean Texture')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:48.39112Z","iopub.execute_input":"2024-11-09T03:52:48.392087Z","iopub.status.idle":"2024-11-09T03:52:48.783794Z","shell.execute_reply.started":"2024-11-09T03:52:48.392046Z","shell.execute_reply":"2024-11-09T03:52:48.782762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(data=cancerdf['symmetryerror'], color = 'C0', alpha=0.3)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.axhline(cancerdf['symmetryerror'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.annotate(f'Cancer Average Symmetry Error', (450, 0.025), fontsize=15, color='red')\n\nplt.title('Symmetry Error Spread')\nplt.xlabel('Symmetry Error')\nplt.ylabel('')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:52:55.712212Z","iopub.execute_input":"2024-11-09T03:52:55.713262Z","iopub.status.idle":"2024-11-09T03:52:55.978742Z","shell.execute_reply.started":"2024-11-09T03:52:55.7132Z","shell.execute_reply":"2024-11-09T03:52:55.977802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(data=cancerdf['meanconcavepoints'], color = '#5642C5', alpha=0.5)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.axhline(cancerdf['meanconcavepoints'].mean(), linestyle='--', lw=2, zorder=1, color='black')\nplt.annotate(f'Cancer Average Mean Concave Points', (430, 0.054), fontsize=15, color='red')\n\nplt.title('Mean Concave Points Spread')\nplt.xlabel('Mean Concave Points')\nplt.ylabel('')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:53:11.216166Z","iopub.execute_input":"2024-11-09T03:53:11.216711Z","iopub.status.idle":"2024-11-09T03:53:11.539695Z","shell.execute_reply.started":"2024-11-09T03:53:11.216674Z","shell.execute_reply":"2024-11-09T03:53:11.538712Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Data Pre-Processing\n    \n</h2>\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.035988,"end_time":"2022-09-28T16:43:52.557123","exception":false,"start_time":"2022-09-28T16:43:52.521135","status":"completed"},"tags":[]}},{"cell_type":"code","source":"input_cols = list(cancerdf.columns[:-1])\ntarget_col =  cancerdf.columns[-1]","metadata":{"papermill":{"duration":0.047254,"end_time":"2022-09-28T16:43:52.640033","exception":false,"start_time":"2022-09-28T16:43:52.592779","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.433329Z","iopub.execute_input":"2024-11-09T03:53:24.433746Z","iopub.status.idle":"2024-11-09T03:53:24.439264Z","shell.execute_reply.started":"2024-11-09T03:53:24.433711Z","shell.execute_reply":"2024-11-09T03:53:24.438224Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs_df = cancerdf[list(input_cols)].copy()\ninputs_df.head()","metadata":{"papermill":{"duration":0.068998,"end_time":"2022-09-28T16:43:52.824946","exception":false,"start_time":"2022-09-28T16:43:52.755948","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.441555Z","iopub.execute_input":"2024-11-09T03:53:24.442011Z","iopub.status.idle":"2024-11-09T03:53:24.483411Z","shell.execute_reply.started":"2024-11-09T03:53:24.441974Z","shell.execute_reply":"2024-11-09T03:53:24.482363Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets = cancerdf[(target_col)]\ntargets.head()","metadata":{"papermill":{"duration":0.048052,"end_time":"2022-09-28T16:43:52.908881","exception":false,"start_time":"2022-09-28T16:43:52.860829","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.484851Z","iopub.execute_input":"2024-11-09T03:53:24.485254Z","iopub.status.idle":"2024-11-09T03:53:24.492772Z","shell.execute_reply.started":"2024-11-09T03:53:24.485215Z","shell.execute_reply":"2024-11-09T03:53:24.491779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#scaling the values\nscaler = MinMaxScaler()\nscaler.fit(inputs_df[input_cols])\ninputs_df[input_cols] = scaler.transform(inputs_df[input_cols])\ninputs_df[input_cols].head()","metadata":{"papermill":{"duration":0.076458,"end_time":"2022-09-28T16:43:53.0227","exception":false,"start_time":"2022-09-28T16:43:52.946242","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.494404Z","iopub.execute_input":"2024-11-09T03:53:24.494808Z","iopub.status.idle":"2024-11-09T03:53:24.537974Z","shell.execute_reply.started":"2024-11-09T03:53:24.494768Z","shell.execute_reply":"2024-11-09T03:53:24.536982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Dimensionality Reduction\n    \n</h2>\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.036541,"end_time":"2022-09-28T16:43:53.096258","exception":false,"start_time":"2022-09-28T16:43:53.059717","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Dimensionality refers to the count of input features, variables, or columns residing within a given dataset. The process of curtailing these features is aptly termed as dimensionality reduction. This technique is characterized by its mission: to transform a dataset replete with higher dimensions into a more streamlined counterpart, while retaining comparable information content.\n\nIn essence, dimensionality reduction endeavors to distill the essence of intricate data representations, all the while engendering a reduction in complexity. This methodological approach finds its vantage point within the domain of machine learning, where its role becomes especially pronounced in refining predictive models tailored for classification and regression quandaries.\n\nIn practical scenarios, datasets frequently encompass an abundance of input features, potentially culminating in a perplexing predictive modeling task. This complexity can impede visualizations and predictions, particularly for training datasets boasting an excessive array of features. To circumvent this predicament, dimensionality reduction techniques step onto the stage, serving as invaluable allies in the quest to unravel intricate patterns and enhance the efficacy of predictive models.\n\n<center>\n<img src=\"https://miro.medium.com/max/959/1*kK4aMPHQ89ssFEus6RT4Yw.jpeg\">\n</center>\n\nSome common feature extraction techniques are:\n\n1. Principal Component Analysis\n2. Linear Discriminant Analysis\n3. Kernel PCA\n4. Quadratic Discriminant Analysis","metadata":{"papermill":{"duration":0.036299,"end_time":"2022-09-28T16:43:53.169356","exception":false,"start_time":"2022-09-28T16:43:53.133057","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Fetching the Column Values \ncolumn_values = []\nfor i in range(len(inputs_df.columns)):\n    column_values.append(inputs_df.iloc[:,i].values)\n    \n#Making Covariance Matrix\ncovariance_matrix = np.cov(column_values)\n\n#Getting the EigenVectors and the EigenValues\neigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)","metadata":{"papermill":{"duration":0.053444,"end_time":"2022-09-28T16:43:53.260363","exception":false,"start_time":"2022-09-28T16:43:53.206919","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.540067Z","iopub.execute_input":"2024-11-09T03:53:24.540408Z","iopub.status.idle":"2024-11-09T03:53:24.549908Z","shell.execute_reply.started":"2024-11-09T03:53:24.540373Z","shell.execute_reply":"2024-11-09T03:53:24.54903Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"explained_variance = []\ncumulative_variance = 0\n\n# Calculate the percentage explained by each eigenvalue\nfor eigenvalue in eigen_values:\n    percentage = np.around((eigenvalue / np.sum(eigen_values)) * 100, 3)\n    cumulative_variance += percentage\n    \n    # Append cumulative variance if it's less than 92%\n    if cumulative_variance <= 92:\n        explained_variance.append(cumulative_variance)\n        \nsns.lineplot(x=explained_variance,  y=range(1,len(explained_variance)+1))\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.annotate(f'{explained_variance[1]}', (69, 2.12), fontsize=15, fontweight='bold', color='red')\nplt.annotate(f'{explained_variance[0]}', (52, 1.12), fontsize=15,  fontweight='bold', color='red')\nplt.annotate(f'{explained_variance[-1]}', (90.9, 5.7), fontsize=15, fontweight='bold', color='red')\n\nplt.title('Explained Variance Plot', fontsize=20)\nplt.xlabel('Explained Variance', fontsize=16)\nplt.ylabel('Principal Components',fontsize=16)\nplt.grid(True)\nplt.show()","metadata":{"papermill":{"duration":0.114088,"end_time":"2022-09-28T16:43:53.414924","exception":false,"start_time":"2022-09-28T16:43:53.300836","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.55122Z","iopub.execute_input":"2024-11-09T03:53:24.551561Z","iopub.status.idle":"2024-11-09T03:53:24.862761Z","shell.execute_reply.started":"2024-11-09T03:53:24.551527Z","shell.execute_reply":"2024-11-09T03:53:24.861701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Convert the original data using the new Prinicipal Components","metadata":{}},{"cell_type":"code","source":"pc = eigen_vectors[0:6]\ntransformed_df = np.dot(cancerdf.iloc[:,0:30],pc.T)\nnew_df = pd.DataFrame(transformed_df,columns=[\"PC\"+str(i+1) for i in range(len(pc))])\nnew_df['Target'] = cancerdf['target'].values\nnew_df.head()","metadata":{"papermill":{"duration":0.058743,"end_time":"2022-09-28T16:43:53.511631","exception":false,"start_time":"2022-09-28T16:43:53.452888","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.864233Z","iopub.execute_input":"2024-11-09T03:53:24.864573Z","iopub.status.idle":"2024-11-09T03:53:24.883003Z","shell.execute_reply.started":"2024-11-09T03:53:24.864543Z","shell.execute_reply":"2024-11-09T03:53:24.882005Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df['Target'] = new_df['Target'].astype('str')\nsns.scatterplot(data= new_df,\n                 x=new_df['PC1'],\n                 y=new_df['PC2'],\n               hue=new_df['Target'])\nplt.title('Top 2 PCs PCA Scatterplot')\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.axhline(0, linestyle='--', lw=3, zorder=1, color='lightgray')\nplt.axvline(0, linestyle='--', lw=3, zorder=1, color='indianred')\nplt.axvspan(0,-900, alpha=0.15, zorder=1, color='gray')\n\nplt.annotate(f'Quadrant 2', (-940, 23), fontsize=14,color='red')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.ylim(20)\nplt.legend(loc='upper right')\nplt.show()","metadata":{"papermill":{"duration":0.10866,"end_time":"2022-09-28T16:43:53.657303","exception":false,"start_time":"2022-09-28T16:43:53.548643","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:24.884602Z","iopub.execute_input":"2024-11-09T03:53:24.885423Z","iopub.status.idle":"2024-11-09T03:53:25.267165Z","shell.execute_reply.started":"2024-11-09T03:53:24.885381Z","shell.execute_reply":"2024-11-09T03:53:25.266119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(data= new_df,x=new_df['PC5'],y=new_df['PC6'],hue=new_df['Target'])\nplt.title('Bottom 2 PCs PCA Scatterplot')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False);\n\nplt.axhline(0, linestyle='--', lw=2, zorder=1, color='indianred')\nplt.axvline(0, linestyle='--', lw=2, zorder=1, color='indianred')\nplt.axvspan(0,220, alpha=0.15, zorder=1, color='gray')\n\nplt.annotate(f'Quadrant 1', (190, 5), fontsize=14,color='green')\nplt.annotate(f'Quadrant 2', (-45, 5), fontsize=14,color='green')\n\nplt.xlabel('PC5')\nplt.ylabel('PC6')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:53:25.269812Z","iopub.execute_input":"2024-11-09T03:53:25.270133Z","iopub.status.idle":"2024-11-09T03:53:25.682999Z","shell.execute_reply.started":"2024-11-09T03:53:25.270102Z","shell.execute_reply":"2024-11-09T03:53:25.681983Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Splitting the data\n    \n</h2>\n</div>\n    \n</center>","metadata":{"papermill":{"duration":0.037378,"end_time":"2022-09-28T16:43:53.732239","exception":false,"start_time":"2022-09-28T16:43:53.694861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#getting the inputs\nX = new_df.iloc[:,0:-1].values\nnew_df['Target'] = new_df['Target'].astype('int32')\n#getting the labels\ny = new_df.iloc[:,-1].values","metadata":{"papermill":{"duration":0.048027,"end_time":"2022-09-28T16:43:53.81777","exception":false,"start_time":"2022-09-28T16:43:53.769743","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:25.684327Z","iopub.execute_input":"2024-11-09T03:53:25.68477Z","iopub.status.idle":"2024-11-09T03:53:25.69512Z","shell.execute_reply.started":"2024-11-09T03:53:25.684726Z","shell.execute_reply":"2024-11-09T03:53:25.694304Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Finding the Derivative Equation\n    \n</h2>\n</div>\n    \n</center>","metadata":{}},{"cell_type":"markdown","source":"We will be using **Sigmoid function** here to predict the output. The formula for sigmoid is $ S({x}) = \\frac {1} { (1+e^{-x})}$ . \n\nWe first of all calculate the probabilities using `Log Loss` and then pass the value to the sigmoid function which classifies it into either 0 or 1. The formula for the log_loss is as follows:-\n\n$$ L = \\frac {-1}{n} \\space \\Sigma\\: [y_{i}\\space log{y_i} + (1-y_{i}) \\space log({1-y_i}) ]$$ \n\nThe formula is sometimes referred to as `Binary Cross Entropy` due to its connection with maximum likelihood estimation. Initially, we compute the maximum likelihood, but the multiplication of probabilities can lead to exceedingly small values, rendering them impractical. To counter this, we take the logarithm of the probabilities. However, this introduces a conundrum: logarithms of smaller numbers yield larger values, and vice versa.\n\n> To address this challenge, we employ the negative sign. Additionally, to encompass both potential outcomes, positive and negative, the formula comprises two terms. One for the positive outcome and the other for the negative outcome. This dual-term structure equips the formula to handle various scenarios, enabling the selection of the model that yields the lowest loss value. Since a closed form of this formula is absent, our reliance shifts to Gradient Descent. This optimization technique aids in identifying the parameter values that correspond to the minimum loss.\n\nIn essence, the `Binary Cross Entropy` formula encapsulates a nuanced interplay of probabilities, logarithms, and negative signs, ultimately steering us toward optimal model configurations through the iterative process of Gradient Descent.\n\n\n### Deriving the derivatives using Calculus\n\nThe formula for log loss for a binary classification model is:\n\n$$ \\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n\nHere $ \\hat{y}_i $ is the predicted probability of the $ i $-th sample belonging to class 1, and it is typically modeled using a logistic function in the case of logistic regression:\n\n$$ \\hat{y}_i = \\sigma(w \\cdot x_i + b) $$\n\nwhere $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the sigmoid function. The $\\sigma$ introduces the non-linearity in what otherwise would have been a linear function equivalent to the regression problem we solved earlier.\n\n### Differentiation with respect to weights ($ w $)\n\nTo differentiate the log loss with respect to the weights, we apply the chain rule. First, notice how $ \\hat{y}_{i} $ changes with $ z_{i} = w \\cdot x_{i} + b $. Below is the derivative of the $\\sigma$ function\n\n$$ \\frac{d}{dz_{i}} \\sigma(z_{i}) = \\sigma(z_{i})(1 - \\sigma(z_{i})) $$\n\n ---\n**Derivation of Sigmoid**\n\nLet's derive that as well. First off let's differentiate the term $ 1 + e^{-z}$ so to do that we will say $u = 1 + e^{-z}$ and differentiating this term we will get $\\frac{du}{dz} = 0 + e^{-z}$ which can then become $\\frac{du}{dz} = 0 + e^{-z}(-1)$ since we are differentiating and the same term becomes 1 when differentiating with respect to itself so we will get $\\frac{du}{dz} = -e^{-z}$ \n\n\nNow that we have the differentiation of the lower term we apply the quotient rule which is:\n\n\n$$\\frac{du}{dz} = \\frac{u \\cdot \\frac{dv}{dz} - v(z) \\cdot \\frac{du}{dz}}{u^2}$$\n\n\nSo now we are able to do this easily by substituting values $\\frac{du}{dz} = \\frac{(1 + e^{-z}) \\cdot 0 - 1 \\cdot e^{-z}}{(-e^{-z})^2}$ which will give us $\\frac{du}{dz} = \\frac{(0 + e^{-z})}{(-e^{-z})^2}$ so now we will get  $\\frac{du}{dz} = e^{-z} $ which is our sigmoid derivative\n\n---\n\nSince $ z_{i} = w \\cdot x_{i} + b $, the derivative of $ z_{i} $ with respect to $ w $ is $ x_{i}$ (assuming $ x_{i} $ is a vector and $ w $ are the corresponding weights). Applying the chain rule:\n\n$$ \\frac{\\partial \\text{Log Loss}}{\\partial w} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{y_i}{\\hat{y}_i} - \\frac{1 - y_i}{1 - \\hat{y}_i} \\right] \\frac{\\partial \\hat{y}_i}{\\partial w} $$\n\n$$\\frac{\\partial \\hat{y}_i}{\\partial w} = \\hat{y}_i(1 - \\hat{y}_i) x_i $$\n\n$$ \\frac{\\partial \\text{Log Loss}}{\\partial w} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i (1 - \\hat{y}_i) - (1 - y_i) \\hat{y}_i \\right] x_i $$\n\n$$\\frac{\\partial \\text{Log Loss}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) x_i $$\n\n### Differentiation with respect to bias ($ b $)\n\nThe differentiation with respect to the bias $ b $ follows a similar pattern, except that the derivative of $ z_{i} $ with respect to $ b $ is 1:\n\n$$ \\frac{\\partial \\text{Log Loss}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) $$\n\nThese gradients are used in gradient descent algorithms to update the weights and bias to minimize the Log Loss, thereby improving the classifier.\n\n**PARAM UPDATE RULE** \n\nThe Parameter Update rule then becomes the following. The addition sign is because of the negative sign in the final equation of the derivation $$ W = W + \\eta \\frac{1}{m}(y-\\hat{y})\\space X $$","metadata":{"papermill":{"duration":0.036941,"end_time":"2022-09-28T16:43:53.892115","exception":false,"start_time":"2022-09-28T16:43:53.855174","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Building the Model and Training\n    \n</h2>\n</div>\n    \n</center>","metadata":{}},{"cell_type":"code","source":"#initializing random Parameters\ndef initialize_betas(dim):\n    b = random.randint(0, 1)\n    w = np.random.rand(dim)\n    return b,w \n\n#Sigmoid Function\ndef sigmoid(b, w ,X_new):\n    Z = b + np.matmul(X_new,w)\n    return (1.0 / (1 + np.exp(-Z)))  \n\n#Cost Calculation\ndef cost( y, y_hat):\n        return - np.sum((np.dot(y.T,np.log(y_hat)))+ (np.dot((1-y).T,np.log(1-y_hat)))) / ( len(y))\n\n#Updating Parameters\ndef update_params (b, w , y , y_hat, X_new, alpha):\n    db = np.sum( y_hat - y)/ len(y)\n    b = b - (alpha * db)\n    dw = np.dot((y_hat - y), X_new)/ len(y)\n    w = w - (alpha * dw)\n    return b,w ","metadata":{"papermill":{"duration":0.050871,"end_time":"2022-09-28T16:43:54.055221","exception":false,"start_time":"2022-09-28T16:43:54.00435","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:25.696572Z","iopub.execute_input":"2024-11-09T03:53:25.696975Z","iopub.status.idle":"2024-11-09T03:53:25.708185Z","shell.execute_reply.started":"2024-11-09T03:53:25.696937Z","shell.execute_reply":"2024-11-09T03:53:25.707166Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's go ahead and use `scikit-learn`'s function to split the data into training and test set.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:53:25.710128Z","iopub.execute_input":"2024-11-09T03:53:25.711025Z","iopub.status.idle":"2024-11-09T03:53:25.720108Z","shell.execute_reply.started":"2024-11-09T03:53:25.710986Z","shell.execute_reply":"2024-11-09T03:53:25.719111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The loop that we will be going through ideally has the same idea of training however the functions used to update weights have been changed and the way we get prediction has also been changed since we are dealing with a different type of Machine learning problem, which is Classification, with the second dataset","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\ntrain_costs = []\nval_costs = []\nAccuracy = []\nVal_accuracy= []\nbatchsize = 10\nlearning_rate = 0.001\ndecay_rate =  0.01\n\nb,w = initialize_betas(X.shape[1])\n\nprint('The initial Value of w and b are', w, b)\nbatches = batch_size(10, X)\n\nfor i in range(2000):\n    epoch = i\n    \n    for j in range(batches):\n        if i==0:\n            \n            #Updating the params at certain intervals in an epoch\n            if j % batchsize==0:\n                learning_rate = learning_rate\n                y_hat = sigmoid(b, w , X_train)\n                current_cost = cost(y_train, y_hat)\n                prev_b = b\n                prev_w = w\n                b, w = update_params(prev_b, prev_w, y_train, y_hat, X_train, learning_rate)\n                y_pred = [1 if i>0.5 else 0 for i in y_hat]\n                accuracy = round(len(y_train[y_train==y_pred])/y_train.shape[0]*100,4)\n                np.random.seed(0)\n                np.random.shuffle([X_train, y_train])\n                \n                #Calculations on test data\n                y_hat_test = sigmoid(b, w , X_test)\n                current_cost_test = cost(y_test, y_hat_test)\n                prev_b = b\n                prev_w = w\n                b, w = update_params(prev_b, prev_w, y_test, y_hat_test, X_test, learning_rate)\n                y_pred_test = [1 if i>0.5 else 0 for i in y_hat_test]\n                accuracy_test = round(len(y_test[y_test==y_pred])/y_test.shape[0]*100,4)\n                np.random.seed(0)\n                np.random.shuffle([X_test, y_test])\n            else:\n                #Not training the Parameters if the above condition is not met\n                prev_w = prev_w\n                prev_b = prev_b\n                \n        else: #Decaying the Learning Rate if its not the first iteration\n            \n            #Updating the params at certain intervals in an epoch\n            if j % batchsize==0:\n                learning_rate = [(1/(1+decay_rate))* learning_rate for j in range(batches)][0]\n                y_hat = sigmoid(b, w , X_train)\n                current_cost = cost(y_train, y_hat)\n                prev_b = b\n                prev_w = w\n                b, w = update_params(prev_b, prev_w, y_train, y_hat, X_train, learning_rate)\n                y_pred = [1 if i>0.5 else 0 for i in y_hat]\n                accuracy = round(len(y_train[y_train==y_pred])/y_train.shape[0]*100,4)\n                np.random.seed(0)\n                np.random.shuffle([X_train, y_train])\n                \n                #Calculations on test data\n                y_hat_test = sigmoid(b, w , X_test)\n                current_cost_test = cost(y_test, y_hat_test)\n                prev_b = b\n                prev_w = w\n                b, w = update_params(prev_b, prev_w, y_test, y_hat_test, X_test, learning_rate)\n                y_pred_test = [1 if i>0.5 else 0 for i in y_hat_test]\n                accuracy_test = round(len(y_test[y_test==y_pred_test])/y_test.shape[0]*100,4)\n                np.random.seed(0)\n                np.random.shuffle([X_test, y_test])\n            else:\n                \n                #Not training the Parameters if the above condition is not met\n                prev_w = prev_w\n                prev_b = prev_b\n                \n    Accuracy.append(accuracy)\n    Val_accuracy.append(accuracy_test)\n    train_costs.append(current_cost)\n    val_costs.append(current_cost_test)\n    \n    if i % 100 == 0:\n        print('===> Epoch:',i,' Loss: ',round(current_cost_test,3) , '  Accuracy:  ', accuracy, ' Val Accuracy ',\n              accuracy_test, ' Learning Rate ', learning_rate)\n    \n    #Early Stopping Mechanism\n    for prev, curr  in EarlyStopping(train_costs):\n        if prev -curr<1e-6:\n            print('-- Early Stopping at Epoch', i, 'with Val Loss',np.around(current_cost_test,5),\n                  'and Val Accuracy', np.around(accuracy_test,4), '--')\n            break                  #Inner Loop Break\n    else:\n        continue                   # executed if the inner loop did NOT break\n    break                          # executed if the inner loop DID break\nprint('The final estimates of w and b are',w, b)","metadata":{"papermill":{"duration":0.870104,"end_time":"2022-09-28T16:43:54.962683","exception":false,"start_time":"2022-09-28T16:43:54.092579","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:25.721819Z","iopub.execute_input":"2024-11-09T03:53:25.722098Z","iopub.status.idle":"2024-11-09T03:53:26.259534Z","shell.execute_reply.started":"2024-11-09T03:53:25.722072Z","shell.execute_reply":"2024-11-09T03:53:26.258372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Logistic Regression Model Training \n\nThe model's parameters (weights 'w' and bias 'b') are updated iteratively using gradient descent while monitoring costs and accuracies. The training involves essential steps such as data preprocessing, updates, and early stopping.\n\n1. **Initialization and Setup:**\n   - Seeds the random number generator for reproducibility.\n   - Initializes empty lists `train_costs`, `val_costs`, `Accuracy`, and `Val_accuracy` to track training and validation metrics.\n   - Sets hyperparameters: `batchsize`, `learning_rate`, and `decay_rate`.\n\n2. **Initial Parameters and Batch Setup:**\n   - Initializes parameters 'b' and 'w' using the `initialize_betas` function.\n   - Defines the number of batches using the `batch_size` function.\n\n3. **Training Loop:**\n   - Iterates over multiple epochs (iterations over the entire dataset).\n   - Within each epoch, iterates over batches of data.\n\n4. **Parameter Updates:**\n   - For the first epoch (`i==0`), updates model parameters at intervals defined by `batchsize`:\n     - Computes predictions using the `sigmoid` function.\n     - Calculates the cost using the `cost` function.\n     - Updates parameters 'b' and 'w' using the `update_params` function.\n     - Computes training accuracy based on predictions and updates.\n     - Shuffles the training data.\n     - Repeats the above steps for test data.\n   - For subsequent epochs, updates parameters with a decaying learning rate.\n\n5. **Metric Tracking and Display:**\n   - Appends training and validation metrics such as accuracy and cost to respective lists.\n   - Periodically prints epoch progress, displaying loss, accuracy, and learning rate.\n\n6. **Early Stopping Mechanism:**\n   - Implements an early stopping mechanism based on training cost trends.\n   - Compares consecutive training costs, and if the decrease is small, triggers early stopping.\n","metadata":{}},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#999999;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Model Performance\n    \n</h2>\n</div>\n    \n</center>","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,4.5))\nfig.subplots_adjust(hspace = .2, wspace=.25)\n\n#Second Plot\nax1.plot(val_costs, 'tab:orange')\nax1.set_title(\"Validation Loss, Classification\")\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Validation Loss')\n\n#Third Plot\nax2.plot(range(epoch+1), np.array(Val_accuracy), 'tab:green' )\nax2.set_title(\"Validation Accuracy, Classification\")\nax2.set_ylabel('Validation Accuracy')\nax2.set_xlabel('Epoch')\n\n#Fourth Plot\nax3 = sns.countplot(y_test[y_test!=y_pred_test], hue=y_test[y_test!=y_pred_test])\nax3.set_title(\"Misclassified Labels, Classification\")\nax3.set_xlabel('Misclassified Labels')\nax3.set_ylabel('Count')\nax3.get_legend().remove()\nplt.show()","metadata":{"papermill":{"duration":0.487284,"end_time":"2022-09-28T16:43:55.489018","exception":false,"start_time":"2022-09-28T16:43:55.001734","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-09T03:53:26.260819Z","iopub.execute_input":"2024-11-09T03:53:26.261107Z","iopub.status.idle":"2024-11-09T03:53:26.733381Z","shell.execute_reply.started":"2024-11-09T03:53:26.261074Z","shell.execute_reply":"2024-11-09T03:53:26.732407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is another way to evaluate the model which is by calculating TPR and FPR:-\n\n1. True Positive Rate (TPR), also known as Sensitivity or Recall, measures a model's ability to correctly identify positive cases,\n2. False Positive Rate (FPR) quantifies how often the model incorrectly labels negative cases as positive.\n3. These metrics play a crucial role in evaluating binary classification models, particularly when dealing with imbalanced datasets.","metadata":{}},{"cell_type":"code","source":"def evaluate(y, y_preds):  \n    from sklearn.metrics import confusion_matrix\n    cf = confusion_matrix(y_test, y_preds)\n    TP = cf[0][0]\n    FP = cf[0][1]\n    FN = cf[1][0]\n    TN = cf[1][1]\n    TPR = TP/(TP+FN)\n    FPR = FP/(FP+TN)\n    return TPR, FPR","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:53:26.734894Z","iopub.execute_input":"2024-11-09T03:53:26.735628Z","iopub.status.idle":"2024-11-09T03:53:26.742827Z","shell.execute_reply.started":"2024-11-09T03:53:26.735585Z","shell.execute_reply":"2024-11-09T03:53:26.741681Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = evaluate(y_test, y_pred_test)\nprint(f\"True Positive Rate : {round(results[0],4)}, False Positive Rate :{round(results[1],4)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:53:26.744076Z","iopub.execute_input":"2024-11-09T03:53:26.744481Z","iopub.status.idle":"2024-11-09T03:53:26.757234Z","shell.execute_reply.started":"2024-11-09T03:53:26.74445Z","shell.execute_reply":"2024-11-09T03:53:26.756264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"An ideal case is a TPR equal to 1 and an FPR equal to 0.","metadata":{}},{"cell_type":"code","source":"y_preds_prob = sigmoid(b, w , X_test)\n\n##You can play with the threshold which we have picked here to be 0.5\ny_preds = (y_preds_prob > 0.5).astype(int)\n\nTPR, FPR = evaluate(y_test, y_preds)\n\n# Plot ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_preds_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve, Test Set')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False);\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T03:53:26.758436Z","iopub.execute_input":"2024-11-09T03:53:26.758733Z","iopub.status.idle":"2024-11-09T03:53:27.039525Z","shell.execute_reply.started":"2024-11-09T03:53:26.758706Z","shell.execute_reply":"2024-11-09T03:53:27.038543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Conclusion\n    \n</h2>\n</div>\n    \n</center>\n\n- Gradient descent stands as a fundamental optimization technique applicable to a wide array of machine learning algorithms. Its simplicity and versatility make it a cornerstone of optimization in machine learning.\n  \n- Notably, gradient descent's computational efficiency extends across varying dimensional spaces, ensuring its applicability to diverse problem domains. This efficiency allows it to traverse complex landscapes efficiently.\n\n- With meticulous data preprocessing and well-informed hyperparameter tuning, gradient descent is poised to converge towards global minima in fewer iterations. The right setup and fine-tuning amplify its capability to pinpoint optimal parameter values effectively.\n\nI appreciate your time and effort in reviewing the notebook! Your engagement and upvotes aids in refining and advancing our understanding of these intricate machine learning concepts.","metadata":{"papermill":{"duration":0.039078,"end_time":"2022-09-28T16:43:55.568045","exception":false,"start_time":"2022-09-28T16:43:55.528967","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<center>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<h2 style=\"padding: 10px;\n              color:white;\">Further Reading\n    \n</h2>\n</div>\n    \n</center>\n\n1. Logistic Regression Using Gradient Descent https://bit.ly/3UGv1JL\n2. Linear Regression Using Gradient Descent https://bit.ly/2OwHopq\n3. Curse Of Dimensionality https://bit.ly/3RerRtZ\n4. Gradient Descent - Deep Dive https://bit.ly/3xR8rog\n5. Neural Network From Scratch - https://bit.ly/gradientdescent1","metadata":{"papermill":{"duration":0.03854,"end_time":"2022-09-28T16:43:55.645175","exception":false,"start_time":"2022-09-28T16:43:55.606635","status":"completed"},"tags":[]}}]}