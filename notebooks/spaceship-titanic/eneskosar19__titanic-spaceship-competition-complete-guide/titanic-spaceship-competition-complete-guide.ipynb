{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.0 IMPORTS\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nplt.rcParams[\"figure.figsize\"] = (10,6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.0 READ DATASET","metadata":{}},{"cell_type":"code","source":"train_data0 = pd.read_csv(\"spaceship_train.csv\")\ntrain_data0['Transported'] = train_data0['Transported'].astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data0.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"spaceship_test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train_data = train_data0.copy() #to save the first version of my train_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train_data, test_data], axis=0)\ndf.head(3)\n\n#We will do EDA part","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summary(df):\n    print(f\"Dataset has {df.shape[1]} features and {df.shape[0]} examples.\")\n    summary = pd.DataFrame(index=df.columns)\n    summary[\"Unique\"] = df.nunique().values\n    summary[\"Missing\"] = df.isnull().sum().values\n    summary[\"Duplicated\"] = df.duplicated().sum()\n    summary[\"Types\"] = df.dtypes\n    return summary\n\nsummary(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # FEATURE ENGİNEERİNG","metadata":{}},{"cell_type":"markdown","source":"### Group size - PassengerId\nA unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n\n-So we will create a group_size feature  using PassengerID feature","metadata":{}},{"cell_type":"code","source":"group = df['PassengerId'].apply(lambda x: x.split('_')[0]).value_counts().to_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Group_size'] = df['PassengerId'].apply(lambda x: group[x.split('_')[0]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.set_index('PassengerId', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Group_size\"].head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HomePlanet\nThe planet the passenger departed from, typically their planet of permanent residence.","metadata":{}},{"cell_type":"code","source":"df['HomePlanet'].value_counts(normalize=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = df['HomePlanet'].value_counts()\ntmp ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating probability distribution for each planet\nv = tmp.index # ['Earth', 'Europa', 'Mars']\n\np = tmp.values \np = p/sum(p)\np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df['HomePlanet'].isna(), 'HomePlanet'] = np.random.choice(v, df['HomePlanet'].isna().sum(), p=p)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['HomePlanet'].isnull().sum().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#If we use methods like mod,median,nearest neighboor etc we may cause bias which will misslead the model.\n#In order to increase our model's generalization ability we randomly filled the null values\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CRYOSLEEP\nIn here we assume that nan values are 0 because if they are in cryosleep they should have been written as 1","metadata":{}},{"cell_type":"code","source":"df['CryoSleep'].fillna(df['CryoSleep'].median(), inplace=True)\ndf['CryoSleep'] = df['CryoSleep'].astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['CryoSleep'].isnull().sum().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cabin\nThe cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.","metadata":{}},{"cell_type":"code","source":"df['Cabin'].head(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#As you can see we have a lot of information in this feature, we will extract some features from cabin and also fill the missing values\ntmp = df['Cabin'].apply(lambda x: x.split('/') if type(x) != float else ['-1', '-1', '-1']).to_list()\ntmp = np.array(tmp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Cabin_deck'] = tmp[:, 0]\ndf['Cabin_num'] = tmp[:, 1]\ndf['Cabin_side'] = tmp[:, 2]\ndf.drop(columns='Cabin', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CABİN_DECK","metadata":{}},{"cell_type":"code","source":"df.loc[df['Cabin_deck']=='-1', 'Cabin_deck'] = np.random.choice(['F', 'G'], sum(df['Cabin_deck']=='-1'), \n                                                              p=[0.5, 0.5])\n#This maintains the original distribution of cabin_deck\n#We use F and G because they are the top 2 most used cabins again this maintains the original dist.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Cabin_deck'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Cabin_deck'].value_counts(normalize=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cabin_number","metadata":{}},{"cell_type":"code","source":"df['Cabin_num'].nunique()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We have lots of unique numbers, this can help our model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Cabin_num'] = df['Cabin_num'].astype(int)\ndf.loc[df['Cabin_num']=='-1', 'Cabin_num'] = int(df['Cabin_num'].mean())\n#Fill missing values with mean, because we don't have too much missing values and we do not want to misslead the model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cabin_side","metadata":{}},{"cell_type":"code","source":"#Same as cabin deck we want to avoid bias in order to get more higher test scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df['Cabin_side']=='-1', 'Cabin_side'] = np.random.choice(['S', 'P'], sum(df['Cabin_side']=='-1'), \n                                                              p=[0.5, 0.5])\ndf['Cabin_side'] = df['Cabin_side'].map({'S':0, 'P':1})\ndf['Cabin_side'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Destination\nto avoid introducing a bias towards a specific planet again we will handle the missing values by randomly assigning of three planets","metadata":{}},{"cell_type":"code","source":"df['Destination'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df['Destination'].isna(), 'Destination'] = np.random.choice(['TRAPPIST-1e', '55 Cancri e', 'PSO J318.5-22'], \n                                                                  sum(df['Destination'].isna()), \n                                                                  p=[0.5, 0.3, 0.2])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AGE","metadata":{}},{"cell_type":"code","source":"summary(df)[3:4]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We have 270 missing values, for age we want to maintain the  overall distribution of feature in order not to biased for a spesific age","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_age = df[\"Age\"].mean()\nstd_age = df[\"Age\"].std()\nis_null = df[\"Age\"].isnull().sum()\nrand_sample = np.random.uniform(mean_age - std_age, mean_age + std_age, size = is_null)\ndf.loc[df['Age'].isna(), 'Age'] = rand_sample","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VIP","metadata":{}},{"cell_type":"code","source":"df['VIP'].value_counts()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Same as cryosleep we assume that missing values are 0 because if they pay for vip they should have written as vip\ndf['VIP'].fillna(False, inplace=True)\ndf['VIP'] = df['VIP'].astype(int) #convert to 0-1 instead true false ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RoomService, FoodCourt, ShoppingMall, Spa, VRDeck","metadata":{}},{"cell_type":"code","source":"#For missing values in each column we will fill them with median\n\n#This strategy is more robust to outliers and skewed data than filling with the mean !!!","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nfor col in cols:\n    df[col].fillna(df[col].median(), inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's create a features of expends","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['total_spending'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] +\\\ndf['Spa'] + df['VRDeck']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols.append('total_spending')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_colors = [\n    (100/255, 108/255, 116/255),   # nevada\n    (228/255, 12/255, 33/255),     # red-ribbon\n    (68/255, 68/255, 76/255),      # abbey\n    (172/255, 28/255, 44/255),     # roof-terracotta \n]\ncustom_palette = sns.color_palette(custom_colors)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(len(cols),2, figsize=(12,14))\nfor i, col in enumerate(cols):\n    sns.histplot(data=df, x=col, ax=axes[i, 0], bins=20, color=custom_colors[0])\n    sns.histplot(data=np.log(df[[col]]), x=col, ax=axes[i, 1], color=custom_colors[1])\n    axes[i, 0].set_title('Normal Distribution')\n    axes[i, 1].set_title('Logarithmic Distribution')\nplt.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#When we use logarithmic transformation we must pay attention to values which are 0 and negative.Logarithm is not defined at these values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols:\n    df.loc[df[col]==0, col] = 0.367 # is approximately -1\n    df[col] = np.log(df[col])\n#value for log0 is not defined so we used 0.367","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NAME","metadata":{}},{"cell_type":"code","source":"df['Name'].fillna('Unkown Unkown', inplace=True) # fill missing values with unkown\ntmp = np.array(df['Name'].apply(lambda x: x if type(x)==float else x.split(' ')).to_list())\n#Split the names","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Name_first'] = tmp[:, 0]\ndf['Name_last'] = tmp[:, 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndf[\"Name_first\"] = label_encoder.fit_transform(df.loc[:, \"Name_first\"])\n\nlabel_encoder = LabelEncoder()\ndf[\"Name_last\"] = label_encoder.fit_transform(df.loc[:, \"Name_last\"])\n\n#This helps model to understand the uniqueness of a name and last name","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns=\"Name\", inplace=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#As you can see we completed missing values part","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features = ['HomePlanet', 'Destination', 'Cabin_deck']\ndf = pd.concat([df, pd.get_dummies(df[categorical_features],dtype=int)], axis=1)\ndf.drop(columns=categorical_features, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now split back the train-test data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df  = df[train_data.shape[0]:]\ntrain_df = df[:train_data.shape[0]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODELLING","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop('Transported', axis=1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_df['Transported'].astype(int)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.15, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lazypredict\nfrom lazypredict.Supervised import LazyClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\nmodels, predictions = clf.fit(X_train, X_test, y_train, y_test)\nprint(models)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipelines = {\n    'adaboost': make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=1234)),\n    'xgboost': make_pipeline(StandardScaler(), XGBClassifier(random_state=1234)),\n    'catboost' : make_pipeline(StandardScaler(), CatBoostClassifier(random_state=1234)),\n    'gradientboost': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=1234)),\n    'ligthgbm': make_pipeline(StandardScaler(), LGBMClassifier(random_state=1234)),\n    'randomforest': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=1234)),\n    'logistic': make_pipeline(StandardScaler(), LogisticRegression(random_state=1234)),\n    'knn': make_pipeline(StandardScaler(), KNeighborsClassifier())\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid = {\n    'adaboost': {\n        'adaboostclassifier__n_estimators': [50, 100, 150,],\n        'adaboostclassifier__learning_rate': [0.01, 0.05, 0.1,]\n    },\n    'xgboost': {\n        'xgbclassifier__n_estimators': [50, 100, 150,],\n        'xgbclassifier__learning_rate': [0.01, 0.05,],\n        'xgbclassifier__max_depth': [3, 4],\n        'xgbclassifier__gamma': [0.1, 0.2],\n        'xgbclassifier__subsample': [0.6, 0.8]\n    },\n    'catboost': {\n        'catboostclassifier__learning_rate': [0.01, 0.05, 0.1, 0.5],\n        'catboostclassifier__depth': [2,3,4], \n        'catboostclassifier__l2_leaf_reg': [1,2,3], \n    },\n    'gradientboost': {\n        'gradientboostingclassifier__n_estimators': [50, 100],\n        'gradientboostingclassifier__learning_rate': [0.01, 0.05, 0.1],\n        'gradientboostingclassifier__max_depth': [3, 4],\n        'gradientboostingclassifier__min_samples_split': [2, 5]\n    },\n    'ligthgbm': {\n        'lgbmclassifier__n_estimators': [50, 100, 150],\n        'lgbmclassifier__learning_rate': [0.01, 0.05, 0.1,],\n        'lgbmclassifier__max_depth': [3, 4],\n        'lgbmclassifier__min_child_samples': [5, 10, 20]\n    },\n    'randomforest': {\n        'randomforestclassifier__n_estimators': [100, 200],\n        'randomforestclassifier__max_depth': [None,4, 5,],\n        'randomforestclassifier__min_samples_split': [2, 5, 10]\n    },\n    'logistic': {\n        'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n        'logisticregression__penalty': ['l1', 'l2']\n    },\n    'knn': {\n        'kneighborsclassifier__n_neighbors': [3, 5, 7, 9],\n        'kneighborsclassifier__weights': ['uniform', 'distance']\n    }\n}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create a blank dictionary to hold the models \nfit_models = {}\n# Loop through all the algos \nfor algo, pipeline in pipelines.items():\n  print(f'Training the {algo} model.')\n  # Create new Grid Search CV Cclass \n  model = GridSearchCV(pipeline, grid[algo], n_jobs=-1, cv=5,scoring=\"accuracy\")\n  # Train the model \n  model.fit(X_train, y_train)\n  # Store results inside of the dictionary\n  fit_models[algo] = model ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.drop(\"Transported\",axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the performance of the model \nfor algo, model in fit_models.items(): \n  yhat = model.predict(X_test)\n  accuracy = accuracy_score(y_test, yhat)\n  precision = precision_score(y_test, yhat)\n  recall = recall_score(y_test, yhat)\n  print(f'Metrics for {algo}: accuracy- {accuracy}, recall- {recall}, precision- {precision}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FİNAL MODEL SELECTİON","metadata":{}},{"cell_type":"code","source":"yhat_test = fit_models['catboost'].predict(test_df)\n","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame([test_data['PassengerId'], yhat_test]).T\nsubmission.columns = ['PassengerID', 'Transported']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"Transported\"] = submission[\"Transported\"] == 1\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission_SpaceShip-catboost-v1.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### We get a score from catboost which was the highest but we need to look for better parameters. Catboost will take a long time for grid search\n# so i will try to get a better score with xgboost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n        'xgbclassifier__n_estimators': [200,300,400],\n        'xgbclassifier__learning_rate': [0.01,0.045,0.05,0.055,0.1,0,2],\n        'xgbclassifier__max_depth': [3,4],\n        'xgbclassifier__gamma': [0.05,0.1,0.15],\n        'xgbclassifier__subsample': [0.4,0.6,0.8]\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = GridSearchCV(pipelines[\"xgboost\"], param_grid=param_grid, n_jobs=-1, cv=5,scoring=\"accuracy\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.fit(X,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = XGBClassifier(\n    gamma=0.05,\n    learning_rate=0.05,\n    max_depth=3,\n    n_estimators=300,\n    subsample=0.6\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.fit(X,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I tried many values in Colab,TPU in another copy of the notebook, so these were the values which i get the best score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {'learning_rate': 0.019549356549743555,\n 'depth': 4,\n 'l2_leaf_reg': 6.238880563296214,\n 'border_count': 113,\n 'verbose' : False}\n\nfinal_model = CatBoostClassifier(**best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.fit(X,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat_test = final_model.predict(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame([test_data['PassengerId'], yhat_test]).T\nsubmission.columns = ['PassengerID', 'Transported']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"Transported\"] = submission[\"Transported\"] == 1\nsubmission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission_SpaceShip-catboost-v6.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{},"execution_count":null,"outputs":[]}]}