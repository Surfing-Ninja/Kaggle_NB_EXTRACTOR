{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n***\nWelcome aboard the Spaceship Titanic! In this notebook, we will explore the dataset of passengers who were transported to an alternate dimension when the Titanic spaceship collided. \n\n> Our goal is to predict which passengers would have survived this catastrophic event using machine learning techniques.\n\nWe will be using scikit-learn pipelines, one-hot and label encoders, quantile transformer, PCA for great 3D visualization, hyperopt for hyperparameter tuning and CatBoostClassifier.\n\n> With an accuracy score of **81.2%**, this notebook ranked in the **top 7%** of all participants.\n\nJoin me on this journey as we uncover insights and patterns that will help us understand what it takes to survive a spaceship collision.\n\n## We will go through\n\n- Creating easy to use **data processing pipelines** using scikit-learn\n- Applying standard scaler and **quantile transformer** to normalize the data and reduce outliers\n- Using PCA to reduce dimensionality and create **stunning 3D plots**\n- Optimizing the model hyperparameters using **hyperopt**\n- Evaluating the model performance using accuracy, F1 score and confusion matrix\n- Interpreting the model results using feature importance and **SHAP values**\n","metadata":{}},{"cell_type":"markdown","source":"# Libraries\n***","metadata":{}},{"cell_type":"code","source":"# Holy grail\nimport numpy as np\nimport pandas as pd\n\n# Scikit-learn\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer, OrdinalEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\n# Hyperparameter optimization library\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n# Machine learning model\nfrom catboost import CatBoostClassifier, Pool\n\n# Model evaluation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom catboost.utils import get_confusion_matrix\n\n# SHAP values\nimport shap\n\n# Random state\nRS = 2137","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data\n***","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/spaceship-titanic/train.csv', index_col='PassengerId')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data analysis\n***","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of missing values in each column\ntrain.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(16, 5))\nsns.histplot(data=train, x='Age', hue='Transported', ax=ax[0])\nsns.histplot(data=train, x='Age', hue='Transported', stat='percent', multiple='stack', ax=ax[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(16, 5))\nsns.countplot(data=train, x='HomePlanet', hue='Transported', ax=ax[0])\nsns.countplot(data=train, x='Destination', hue='Transported', ax=ax[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n\nsns.countplot(data=train, x='VIP', hue='Transported', ax=ax[0])\nsns.countplot(data=train, x='CryoSleep', hue='Transported', ax=ax[1])\n\nax[0].set_title('VIP status vs transportation')\nax[1].set_title('Cryosleep vs transportation')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(train['Transported'].value_counts(), shadow=True, explode=[.05,.05], autopct='%.1f%%')\nplt.title('Target distribution ', size=18)\nplt.legend(['False', 'True'], loc='best', fontsize=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n***","metadata":{}},{"cell_type":"markdown","source":"## Name\n**Create `Surname` column by splitting `Name`.**","metadata":{}},{"cell_type":"code","source":"train['Surname'] = train.loc[train['Name'].notnull(), 'Name'].apply(lambda x: x.split()[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Room number\n**First part of passenger's id can tell us his room number.** Room number will propably help us with missing cabins, surnames, destination and home planets.","metadata":{}},{"cell_type":"code","source":"train['RoomNumber'] = train.index.str.split('_').str[0].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigating columns corelated with room number\nLet's see which column's missing values can `RoomNumber` help us with. ","metadata":{}},{"cell_type":"code","source":"# Analysis of passengers in each cabin\ntrain[['Cabin', 'RoomNumber', 'Name', 'HomePlanet', 'Destination', 'VIP', 'CryoSleep']].value_counts().head(40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysis of passengers in each room\ntrain[['RoomNumber', 'Cabin', 'Name', 'HomePlanet', 'Destination', 'VIP', 'CryoSleep']].value_counts().head(40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"warning\" style=\"background-color: #DDE6ED; border-left: 6px solid #27374D; font-size: 100%; padding: 10px;\">\n    <h3 style=\"color: #27374D; font-size: 18px; margin-top: 0; margin-bottom: 10px;\">ðŸ”Ž  Observations:</h3>\n    <ol>\n        <li>Passengers in the same <i>Cabin</i> have the same <i>RoomNumber</i>.</li>\n        <li>Passengers with the same <i>RoomNumber</i> rarely don't have the same <i>Cabin</i> (see <i>RoomNumber</i> 6137 above).</li>\n        <li>Passengers in the same <i>Cabin</i>/RoomNumber most likely have the same <i>Surname</i>, <i>HomePlanet</i> and <i>Destination</i></li>\n        <li>There is no correlation between <i>Cabin</i>/<i>RoomNumber</i> and <i>VIP</i>, <i>CryoSleep</i></li>\n    </ol>\n</div>\n<br></br>\n<div style=\"border-radius: 10px; border: #27374D solid; padding: 15px; background-color: #ffffff00; font-size: 100%; text-align: left;\">\n    <b>Note :</b> For simplicity let's assume that one <i>RoomNumber</i> directly corresponds to one <i>Cabin</i> (it rarely isn't the case so it won't affect model performance by much). We have <i>RoomNumber</i> for every passenger so we will use it to fill missing values for <i>Cabin</i>, <i>Surname</i>, <i>HomePlanet</i> and <i>Destination</i>. Cabin would be a bit better because it has higher corelation with abovementioned columns but there are too many missing values in it.\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Filling missing values \nFirst let's fill missing values of `Cabin`, `Surname`, `HomePlanet` and `Destination` based on `RoomNumber`. We will assume that passengers in one room have the same cabin, destination, surname and home planet.","metadata":{}},{"cell_type":"code","source":"room_related_columns = ['Cabin', 'Surname', 'Destination', 'HomePlanet']\n\n# room_to_col is a pandas series that gives most common cabin, surname, destination or home planet for certain room number\n# Series indexes are room numbers and values are cabins, surnames, etc.\n# I will use this series to map passengers room number to missing values in columns\n# Note: note every room number is in series due to missing values in single person rooms\n# Reason for that is if only one person was in a room and it had one of the columns missing then his/hers room isn't\n# in room_to_col so it won't be used to map this missing value\n\nfor col in room_related_columns:\n    room_to_col = train[['RoomNumber', col]].dropna().groupby('RoomNumber')[col].apply(lambda x: x.mode()[0]) # without dropna mode for single person rooms with nan col would be empty and mode()[0] will raise an error\n    train[col] = train.apply(lambda row: room_to_col[row['RoomNumber']] if pd.isna(row[col]) and row['RoomNumber'] in room_to_col.index else row[col], axis=1)\n    train[col] = train.groupby('RoomNumber')[col].ffill().bfill()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Total spent\nLet's create `TotalSpent` column that sums up all the expenses\n\n**Total spent = Age + RoomService + FoodCourt + ShoppingMall + Spa + VRDeck**","metadata":{}},{"cell_type":"code","source":"expense_columns = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\ntrain['TotalSpent'] = train[expense_columns].sum(axis=1)\n\n# Assume that if passenger didn't spend any money in non-missing expense columns then he popably didn't in a missing columns (most often there is only 1 column missing so propability of that is high)\ntrain.loc[train['TotalSpent'] == 0, expense_columns] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cabin\n**Split `Cabin` collumn to `CabLetter`, `CabNumber`, `CabSide`**","metadata":{}},{"cell_type":"code","source":"train[['CabLetter', 'CabNumber', 'CabSide']] = train['Cabin'].str.split('/', expand=True)\n\n# After Cabin split, CabNumber is a object type column so we will convert it to int \ntrain['CabNumber'] = pd.to_numeric(train['CabNumber']).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cryosleep\n**If person was in cryosleep then they couldn't spent any money and if they spent money then they couldn't be in cryosleep.**\n\nLet's use that observation to fill in the missing values in those columns.","metadata":{}},{"cell_type":"code","source":"# If passenger is in cryosleep then set his/hers expenses to 0\ntrain.loc[train['CryoSleep']==True, ['TotalSpent'] + expense_columns] = 0\n# If passegner spend any money then they couln't be in cryosleep\ntrain.loc[train['CryoSleep'].isnull() & train['TotalSpent'] > 0, 'CryoSleep'] = False\n# If passegner didn't spend any money then they are propably in cryosleep\ntrain.loc[train['CryoSleep'].isnull() & train['TotalSpent'] == 0, 'CryoSleep'] = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop columns\nNow we can drop columns from which we have extracted necessary information","metadata":{}},{"cell_type":"code","source":"train.drop(columns=['Cabin', 'Name'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of missing values in each column\ntrain.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Missing values are now only in numerical columns**","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing\n***","metadata":{}},{"cell_type":"markdown","source":"## Split\n**Split `train` dataframe to `X` and `y` dataframes**","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns=['Transported'], inplace=False)\ny = train['Transported'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a pipeline\n### Numerical columns\n- `StandardImputer()` fills missing values with mean\n- `QuantileTransformer()` is used so the data follows normal distribution\n- `StandartScaler()` normalizes the data, mean = 0, std = 1\n\n### Categorical columns\n- `StandardImputer()` fills missing values with most frequent ones\n- `OneHotEncoder()` one-hot encodes the data\n- Categorical one-hot encoded columns are already binary so performing `QuantileTransformer()` and `StandartScaler()` won't change much\n\n**`Surname` column is categorical but label encoded (not binary) so transformers from numerical columns should be used on it**","metadata":{}},{"cell_type":"code","source":"numerical_pipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    QuantileTransformer(output_distribution='normal', random_state=RS),\n    StandardScaler()\n)\n\ncategorical_pipeline = make_pipeline(\n    SimpleImputer(strategy='most_frequent'),\n    OneHotEncoder(handle_unknown='ignore', drop='first')\n)\n\ncolumn_transformer = make_column_transformer(\n    # Numerical Columns\n    (\n        numerical_pipeline,\n        make_column_selector(dtype_include=['float64', 'int64'])\n    ),\n    # Categorical columns\n    (\n        categorical_pipeline,\n        ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'CabLetter', 'CabSide']\n    ),\n    (\n        make_pipeline(\n            SimpleImputer(strategy='most_frequent'),\n            OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value = -1)\n        ),\n        ['Surname']\n    ),\n    remainder='passthrough',\n    verbose_feature_names_out=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed = column_transformer.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_transformer.get_feature_names_out().tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed = pd.DataFrame(data=X_transformed, columns=column_transformer.get_feature_names_out().tolist())\nX_transformed.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After transformation:\n- Every numeric column has standard deviation = 1 and mean = 0\n- Categorical columns are one-hot encoded\n- Missing values were filled","metadata":{}},{"cell_type":"markdown","source":"# Visualization\n***\nUsing PCA, we can easily visualize the high-dimensional data in a lower-dimensional space and identify patterns and clusters that are not visible in the original data. In our case, we applied PCA to the Spaceship Titanic dataset and created **stunning 3D plots** that revealed **two main clusters** of passengers representing those who were more likely to survive and those who were less likely to survive the spaceship collision.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_transformed)\n\n# Create a dataframe with the PCA data and the target variable\ndf = pd.DataFrame({'x': X_pca[:, 0], 'y' : X_pca[:, 1], 'z' : X_pca[:, 2], 'Transported': y})\n\n# Create a 3D scatter plot with color-coded points\nfig = px.scatter_3d(df, x='x', y='y', z='z', color='Transported')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This code generates three plots that show the transport rate for each cabin letter, cabin side, and cabin number range\nfig, ax = plt.subplots(1, 3, figsize=(16, 5), width_ratios=[2, 1, 3])\n\n# Cabin letter and cabin side\ngrouped_by_letter = train.groupby(by='CabLetter')['Transported'].apply(lambda x: x.sum() / len(x)).sort_values(ascending=True)\ngrouped_by_type = train.groupby(by='CabSide')['Transported'].apply(lambda x: x.sum() / len(x)).sort_values(ascending=True)\n\ngrouped_by_letter.plot(kind='bar', title='Transport rate for each cabin letter', ylabel='Transport rate', xlabel='Cabin letter', ax=ax[0], cmap='plasma')\ngrouped_by_type.plot(kind='bar', title='Transport rate for cabin side', ylabel='Transport rate', xlabel='Cabin type', ax=ax[1], cmap='PiYG')\n\nax[0].tick_params(axis='x', labelrotation=0)\nax[1].tick_params(axis='x', labelrotation=0)\n\n# Cabin number range\nbins = pd.cut(train['CabNumber'], range(0, 1901, 100))\ntransported_per_bin = train.groupby(bins)['Transported'].apply(lambda x: x.sum() / len(x))\ntransported_per_bin.plot(kind='bar', title='Transport rate for each cabin number range', ylabel='Transport rate', xlabel='Cabin number range', ax=ax[2], cmap='viridis')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n***\nIn this almost final section we will:\n1. **Split the dataset** to training and validation sets\n2. **Find the best hyperparatemers** using Hyperopt\n3. **Fit the CatBoostClassifier** using those hyperparapeters\n4. Evaluate model using **accuracy score** and **confusion matrix**\n5. Investigate model using **SHAP** values\n\nI found `CatBoostClassifier` to work the best with our dataset. It has 2 percentage points higher accuracy than `XGBoostClassifier`.","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_transformed, y, train_size=0.8, random_state=RS)\n\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In space we define ranges of our hyperparameters\nspace = {\n    'num_trees': hp.quniform('num_trees', 64, 256, 10),\n    'learning_rate': hp.uniform('learning_rate', 0, 0.3),\n    'depth': hp.quniform('depth', 2, 8, 1),\n    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10),\n    'bagging_temperature': hp.uniform('bagging_temperature', 1, 10),\n    'seed': 0\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(space):\n    model = CatBoostClassifier(\n        num_trees = int(space['num_trees']), # quniform returns a float with .0 floating point and num_trees must be int\n        learning_rate = space['learning_rate'],\n        depth = int(space['depth']),\n        l2_leaf_reg = space['l2_leaf_reg'], \n        bagging_temperature = space['bagging_temperature']\n    )\n    \n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train), (X_val, y_val)],\n        verbose=False\n    )\n    \n    score = accuracy_score(y_val, model.predict(X_val))\n    return {'loss': 1-score, 'status': STATUS_OK, 'model': model}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trials = Trials()\n# I found around 500 evaluations to work the best\n# < 500 doesn't find the best parameters\n# > 1000 causes overfitting so the model doesn't generalize well\n# best_hyperparams = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals=500, trials = trials)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_hyperparams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CatBoostClassifier(\n    num_trees = 160,\n    learning_rate = 0.21253341884565896,\n    depth = 6,\n    l2_leaf_reg = 4.307588665726216, \n    bagging_temperature = 9.31465738460894\n)\n\nmodel.fit(X_train, y_train,\n          eval_set=[(X_val, y_val)],\n          verbose=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"markdown","source":"### Accuracy score\nThe accuracy score is the **ratio of correctly predicted observations to the total number of observations**. It measures how well the model can classify the data into the correct categories. The higher the accuracy score, the better the model.","metadata":{}},{"cell_type":"code","source":"print(\"Train accuarcy\", accuracy_score(y_train, model.predict(X_train)))\nprint(\"Validation accuracy\", accuracy_score(y_val, model.predict(X_val)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our model has a **high accuracy score** on both sets, which means it can correctly classify most of the data. However, there is a slight difference between the train and validation accuracy, which indicates **some overfitting** but difference isn't large enough to worry about it.","metadata":{}},{"cell_type":"markdown","source":"### Confusion matrix\nThe confusion matrix is a table that shows the number of **true positives, false positives, true negatives, and false negatives** for a binary classification problem. It helps us to understand how well the model can distinguish between the two classes and where it makes mistakes.","metadata":{}},{"cell_type":"code","source":"train_pool = Pool(X_val, y_val)\nconfusion_matrix = get_confusion_matrix(model, train_pool)\n\ndf_cm = pd.DataFrame(confusion_matrix, index=[\"True\", \"False\"], columns=[\"Predicted True\", \"Predicted False\"])\n\nax = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=\"Blues\", fmt='g')\n\nplt.xticks(rotation=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see our model doesn't have a strong preference for False Positives nor False Negatives but in our case it doesn't really matter.","metadata":{}},{"cell_type":"markdown","source":"### F1 score\nWe can calculate some metrics from the confusion matrix, such as precision, recall, and F1 score. F1 score is a harmonic mean of precision and recall. It measures how well the model can balance the trade-off between precision and recall.","metadata":{}},{"cell_type":"code","source":"precision = precision_score(y_val, model.predict(X_val))\nrecall = recall_score(y_val, model.predict(X_val))\nf1 = f1_score(y_val, model.predict(X_val))\n\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 score: {f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our model has a high precision, recall, and f1 score on the validation set, which means it can achieve a good balance between accuracy and sensitivity.","metadata":{}},{"cell_type":"markdown","source":"## SHAP values\n**SHAP (SHapley Additive exPlanations)** values are a way of explaining the predictions of a machine learning model. They measure how much each feature contributes to the prediction, either positively or negatively.","metadata":{}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_train)\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the most important features for our model are `CabNumber`, `CabSide`, `RoomNumber`, `VRDeck`, and `CabinLetter`. These features have the highest influence on the prediction, either increasing or decreasing the probability of being transported.","metadata":{}},{"cell_type":"markdown","source":"# Submission\n***\n**Now we can apply every transformation on a test dataset step by step like we did on a training set.**","metadata":{}},{"cell_type":"code","source":"# Load test dataset\nX_test = pd.read_csv('../input/spaceship-titanic/test.csv', index_col='PassengerId')\n\n# Surname\nX_test['Surname'] = X_test.loc[X_test['Name'].notnull(), 'Name'].apply(lambda x: x.split()[1])\n\n# Room number\nX_test['RoomNumber'] = X_test.index.str.split('_').str[0].astype(int)\n\n# Fill missing values related with room number\nfor col in room_related_columns:\n    room_to_col = X_test[['RoomNumber', col]].dropna().groupby('RoomNumber')[col].apply(lambda x: x.mode()[0])\n    X_test[col] = X_test.apply(lambda row: room_to_col[row['RoomNumber']] if pd.isna(row[col]) and row['RoomNumber'] in room_to_col.index else row[col], axis=1)\n    X_test[col] = X_test.groupby('RoomNumber')[col].ffill().bfill()\n    \n# Total spent\nX_test['TotalSpent'] = X_test[expense_columns].sum(axis=1)\nX_test.loc[X_test['TotalSpent'] == 0, expense_columns] = 0\n\n# Split Cabin\nX_test[['CabLetter', 'CabNumber', 'CabSide']] = X_test['Cabin'].str.split('/', expand=True)\nX_test['CabNumber'] = pd.to_numeric(X_test['CabNumber'])\n\n# Cryosleep\nX_test.loc[X_test['CryoSleep']==True, ['TotalSpent'] + expense_columns] = 0\nX_test.loc[X_test['CryoSleep'].isnull() & X_test['TotalSpent'] > 0, 'CryoSleep'] = False\nX_test.loc[X_test['CryoSleep'].isnull() & X_test['TotalSpent'] == 0, 'CryoSleep'] = True\n\n# Drop columns\nX_test.drop(columns=['Cabin', 'Name'], inplace=True)\n\n# Pipeline\nX_test = column_transformer.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\nsample_submission['Transported'] = model.predict(X_test).astype(bool)\nsample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank You! ðŸš€ðŸŒŸ\n\nThank you for joining me on this exciting journey aboard the Spaceship Titanic!\n\nI hope you had a blast exploring the dataset and building machine learning models that predict the survival outcome of each passenger. I hope you learned something new and had fun along the way.\n\nIf you have any questions or feedback, please feel free to reach out to me. Thank you again for your time and effort. Safe travels! ðŸ›¸ðŸ‘½","metadata":{}}]}