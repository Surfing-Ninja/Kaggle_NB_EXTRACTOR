{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ‚ù£Ô∏èAuthor's Note\nIf you like this notebook, consider sharing it to others or give feedback. I would highly appreciate any comments, suggestions and recommendations to improve the notebook.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"padding:20px;color:black;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#d9d9d9;overflow:hidden;font-weight:500\">\nPractice with Visualization & Analysis & XGB skills in DS</div>","metadata":{}},{"cell_type":"markdown","source":"# üìàOverview\n\n    \n**The tasks completed by this kernel are as follows:**\n\n1. **clean and manipulate the data** with python and figure out the distribution of each features.\n2. use matplotlib & seaborn to **visualization some features** which might be significant in build the model.\n3. use **statistic knowledge**(hypothesis test, correspondence analysis) to analysis some features\n4. use **XGB** Algorithm to **classificate** the dataset and analysis the result.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"padding:10px;background-color:orange;margin:0;color:black;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Environment Setting</h1>\n<br>","metadata":{"execution":{"iopub.status.busy":"2022-09-27T07:31:03.779821Z","iopub.execute_input":"2022-09-27T07:31:03.780262Z","iopub.status.idle":"2022-09-27T07:31:03.789118Z","shell.execute_reply.started":"2022-09-27T07:31:03.78023Z","shell.execute_reply":"2022-09-27T07:31:03.787015Z"}}},{"cell_type":"markdown","source":"# üõ†Ô∏è Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats\nimport json\nimport sklearn\nimport re\n\n#sklearn library\n# 1.model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# 2.preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# 3.metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# 4.model\nimport xgboost as xgb\nfrom xgboost import plot_tree\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:22.612243Z","iopub.execute_input":"2022-10-11T08:13:22.612615Z","iopub.status.idle":"2022-10-11T08:13:23.789319Z","shell.execute_reply.started":"2022-10-11T08:13:22.612581Z","shell.execute_reply":"2022-10-11T08:13:23.788206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üõ†Ô∏è Painting environment setting","metadata":{"execution":{"iopub.status.busy":"2022-09-27T07:23:26.405625Z","iopub.execute_input":"2022-09-27T07:23:26.406193Z","iopub.status.idle":"2022-09-27T07:23:26.431574Z","shell.execute_reply.started":"2022-09-27T07:23:26.406086Z","shell.execute_reply":"2022-09-27T07:23:26.430632Z"}}},{"cell_type":"code","source":"%matplotlib inline\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams['savefig.dpi'] = 300\nfont = {'family':'Helvetica, Ariel',\n        'weight':'normal',\n        'size':12}\nplt.rc('font', **font)\nsns.set(rc={\"figure.dpi\": 300, 'savefig.dpi': 300})\nsns.set_context('notebook')\nsns.set_style(\"ticks\")\nFIG_FONT = dict(family=\"Helvetica, Ariel\", weight=\"bold\", color=\"#7f7f7f\")\nsns.set_palette('Spectral')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:23.795347Z","iopub.execute_input":"2022-10-11T08:13:23.797716Z","iopub.status.idle":"2022-10-11T08:13:23.814309Z","shell.execute_reply.started":"2022-10-11T08:13:23.797677Z","shell.execute_reply":"2022-10-11T08:13:23.813013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"padding:10px;background-color:orange;margin:0;color:black;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">EDA</h1>\n<br>","metadata":{}},{"cell_type":"markdown","source":"<div id='imports'\n     style = \"display: fill;\n              border-radius: 10px;\n              background-color: #6867AC;\">\n    <h2 style = \"padding: 15px; \n                 color: White;\n                 text-align: left;\n                 font-family: Trebuchet MS;\">Load the dataset\n    </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"space_titanic= pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ndf=space_titanic.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:23.819726Z","iopub.execute_input":"2022-10-11T08:13:23.822069Z","iopub.status.idle":"2022-10-11T08:13:23.915772Z","shell.execute_reply.started":"2022-10-11T08:13:23.822029Z","shell.execute_reply":"2022-10-11T08:13:23.9147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìã Have a quick glance at the dataset\nuse and .describe() to see the structure of the dataset and use .info() to find the statistical descriptions of the continuous features","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:23.922014Z","iopub.execute_input":"2022-10-11T08:13:23.924333Z","iopub.status.idle":"2022-10-11T08:13:23.955465Z","shell.execute_reply.started":"2022-10-11T08:13:23.924293Z","shell.execute_reply":"2022-10-11T08:13:23.954481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:23.960078Z","iopub.execute_input":"2022-10-11T08:13:23.962406Z","iopub.status.idle":"2022-10-11T08:13:24.005635Z","shell.execute_reply.started":"2022-10-11T08:13:23.962366Z","shell.execute_reply":"2022-10-11T08:13:24.004684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align: justify; font-size:18px; color:k\">\nuse \"profile_report\" we can find the following characteristics in features:</h2>\n\n1. Percentage of missing value in each feature\n2. Distinct values in each categorized feature and frequency in each distinct value\n3. Histogram in each categorized and continuous feature\n4. Statistical describe, such as maximum, minimum, mean, etc, in each continuous feature\n5. Correlations Matrix between each two features in the dataset\n","metadata":{}},{"cell_type":"code","source":"import pandas_profiling\ndf.profile_report()","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:24.010283Z","iopub.execute_input":"2022-10-11T08:13:24.012899Z","iopub.status.idle":"2022-10-11T08:13:52.768871Z","shell.execute_reply.started":"2022-10-11T08:13:24.012857Z","shell.execute_reply":"2022-10-11T08:13:52.767511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id='imports'\n     style = \"display: fill;\n              border-radius: 10px;\n              background-color: #6867AC;\">\n    <h2 style = \"padding: 15px; \n                 color: White;\n                 text-align: left;\n                 font-family: Trebuchet MS;\">Data Process\n    </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# üìã Data processing\n### The purpose of the data cleaning is:\n1. Handle with missing values\n2. Remove some outliers in numerical features\n3. Standardized data format\n","metadata":{}},{"cell_type":"code","source":"df=space_titanic.copy()\n#1„ÄÅDelete unnecessary columns \ndf=df.drop(['Name','PassengerId'],axis=1)\n    \n#2„ÄÅHandling null values \ndf=df.dropna()\n    \n#3„ÄÅcreate y value by using \"Transported\", convert to int type Ê†áËÆ∞yÂÄºÔºåËΩ¨Êç¢ÊàêintÁ±ªÂûã\ntarget=df['Transported']\ndf=df.drop(['Transported'],axis=1)\ntarget = target.astype(int)\n\n#4„ÄÅProcessing category type data \ndf['Cabin_1']=df['Cabin'].apply(lambda x: re.findall('[\\w+]',x)[0] if pd.isnull(x) == False else float('nan'))\ndf['Cabin_2']=df['Cabin'].apply(lambda x: re.findall('[\\w]+',x)[1] if pd.isnull(x) == False else float('nan'))\ndf['Cabin_2']=pd.cut(df['Cabin_2'].astype('int'),5,labels=[0,1,2,3,4])\ndf['Cabin_3']=df['Cabin'].apply(lambda x: re.findall('[\\w]+',x)[2] if pd.isnull(x) == False else float('nan'))\ndf=df.drop(['Cabin'],axis=1)\n\n\n#5„ÄÅObtaining continuous variable \nnumaric_columns=list(df.select_dtypes(include=np.number).columns)\nprint(\"Numaric columns (\"+str(len(numaric_columns))+\") :\",\", \".join(numaric_columns))\n    \n#6„ÄÅObtaining discrete variables \ncat_columns=df.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns (\"+str(len(cat_columns))+\") :\",\", \".join(cat_columns))\n\n#7„ÄÅremove outliers\nfrom collections import Counter\ndef detect_outliers(df, n, features_list):\n    outlier_indices = [] \n    for feature in features_list: \n        Q1 = np.percentile(df[feature], 25)\n        Q3 = np.percentile(df[feature], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR \n        outlier_list_col = df[(df[feature] < Q1 - outlier_step) | (df[feature] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col) \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n) \n    return multiple_outliers\n\noutliers_to_drop = detect_outliers(df, 2, numaric_columns)\nprint(\"We will drop these {} indices: \".format(len(outliers_to_drop)), outliers_to_drop)\n\n\n\n#8„ÄÅCreate the training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(df, target, test_size = 0.2, \n                                                    random_state=100,stratify=target)\n    \n    \n#9„ÄÅUsing continuous variable training data \nX_train_n=X_train[numaric_columns].copy()\nX_test_n=X_test[numaric_columns].copy()\n    \n\n#10„ÄÅUsing discrete variable training data \nX_train_c=X_train[cat_columns]\nX_test_c=X_test[cat_columns]\n                               \n#11„ÄÅOrdinal label processing for discrete variables \nencoder=OrdinalEncoder()\nX_train_c = encoder.fit_transform(X_train_c)\nX_train_c=pd.DataFrame(X_train_c,index=X_train_n.index)\nX_test_c = encoder.transform(X_test_c)\nX_test_c=pd.DataFrame(X_test_c,index=X_test_n.index)\n    \n#12„ÄÅMerge together\nfor i,column in enumerate(X_train_c.columns):\n    X_train_n[\"cat_\"+str(i+1)]=X_train_c[column]\n    X_test_n[\"cat_\"+str(i+1)]=X_test_c[column]\n\nX_train = X_train_n\nX_test = X_test_n","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:52.770694Z","iopub.execute_input":"2022-10-11T08:13:52.771044Z","iopub.status.idle":"2022-10-11T08:13:53.157979Z","shell.execute_reply.started":"2022-10-11T08:13:52.771008Z","shell.execute_reply":"2022-10-11T08:13:53.157035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"padding:10px;background-color:orange;margin:0;color:black;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Visualization</h1>\n<br>","metadata":{}},{"cell_type":"markdown","source":"# üìã Feature Visualization","metadata":{"execution":{"iopub.status.busy":"2022-09-27T09:06:06.803466Z","iopub.execute_input":"2022-09-27T09:06:06.804277Z","iopub.status.idle":"2022-09-27T09:06:06.812449Z","shell.execute_reply.started":"2022-09-27T09:06:06.804202Z","shell.execute_reply":"2022-09-27T09:06:06.810759Z"}}},{"cell_type":"code","source":"import scipy.stats as stats\n\n#plot function\ndef univariate_double_plot(df=df, x=None, xlabel=None, explode=None,ylabel=None,palette=None,order=True,hue=None):\n    sns.set_palette(palette)\n    fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n    #count bar\n    if order == True:\n        feature_data = df[x].value_counts(ascending=True)\n        sns.countplot(data=df, x=x, ax=ax[0], order=feature_data.index,hue = hue)\n    \n    else:\n        feature_data = df[x].value_counts(sort=False).sort_index()\n        sns.countplot(data=df, x=x, ax=ax[0],order=feature_data.index,hue = hue)\n        \n    #pie chart\n    patches, texts, autotexts = ax[1].pie(feature_data.values,labels=feature_data.index,\n                                      autopct='%.0f%%',textprops={'size': 20})\n    for i in range(len(autotexts)):\n        autotexts[i].set_color('white')\n\n    #reduce non-data ink\n    sns.despine(bottom=True, left=True)\n    \n    #set label\n#     for i in range(len(feature_data.index)):\n#         ax[0].text(i,feature_data.iloc[i]*0.9,feature_data.iloc[i],ha='center',\n#                    fontsize=20,color='white')\n    for i in range(len(ax[0].containers)):\n        ax[0].bar_label(ax[0].containers[i], label_type='edge', size=12, padding=1, fontname=\"Helvetica, Ariel\", \n                        color=\"#7f7f7f\")\n        \n    ax[0].set_xlabel(xlabel=xlabel, size=12, fontdict=FIG_FONT)\n    #ax[0].set_xticklabels(feature_data.index,rotation=20,fontsize = 'small')\n    ax[0].set_ylabel(ylabel=ylabel)\n    ax[1].set_ylabel(ylabel=ylabel)\n    fig.text(0.5, 1, f'{xlabel} Distribution', size=16, fontdict=FIG_FONT, ha=\"center\", va=\"center\")\n    plt.show()\n    \ndef univariate_single_plot(df=df, x=None, xlabel=None, rotation=None,ylabel=None,palette=None):\n    sns.set_palette(palette)\n    fig, ax = plt.subplots(1, 1, figsize=(20, 7))\n    feature_data = df[x].value_counts(ascending=True)\n    sns.countplot(data=df, x=x, order=df[x].value_counts(ascending=True).index)\n    sns.despine(bottom=True, left=True)\n    plt.xlabel(xlabel=xlabel, size=14, fontdict=FIG_FONT)\n    plt.xticks(rotation=rotation)\n    plt.ylabel(ylabel=ylabel)\n#     if bar_label:\n#         ax.bar_label(ax.containers[0], label_type='edge', size=15, padding=1, fontname=\"Helvetica, Ariel\", \n#                         color=\"k\")\n    for i in range(len(feature_data.index)):\n        ax.text(i,feature_data.iloc[i]*0.9,feature_data.iloc[i],ha='center',\n                   fontsize=20,color='white')\n    plt.title(label=f'{xlabel} Distribution', size=18, fontdict=FIG_FONT)\n    plt.show()\n    \n    \ndef univariate_numerical_plot(df=df, x=None, xlabel=None,ylabel=None,palette=None,bins=20):\n    sns.set_palette(palette)\n    fig, ax = plt.subplots(1, 3, figsize=(20, 7))\n    #hist\n    sns.histplot(bins=bins,data=df, x=x, kde=True, ax=ax[0])\n    #box\n    sns.boxplot(data=df, y=x, ax=ax[1])\n    #prob\n    plt.sca(ax[2])\n    stats.probplot(df[x], dist = \"norm\", plot = plt)\n    plt.ylabel('Variable quantiles')\n    \n    sns.despine(bottom=True, left=True)\n    ax[0].set_xlabel(xlabel=xlabel, size=12, fontdict=FIG_FONT)\n    ax[0].set_title(f'The histogram of {x}')\n    ax[1].set_xlabel(xlabel=ylabel, size=12, fontdict=FIG_FONT)\n    ax[0].set_ylabel(ylabel=ylabel,size=12, fontdict=FIG_FONT)\n    ax[1].set_ylabel(ylabel=xlabel, size=12, fontdict=FIG_FONT)\n    ax[1].set_title(f'The boxplot of {x}')\n    fig.text(0.5, 1, f'{xlabel} Distribution', size=16, fontdict=FIG_FONT, ha=\"center\", va=\"center\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:53.159616Z","iopub.execute_input":"2022-10-11T08:13:53.160189Z","iopub.status.idle":"2022-10-11T08:13:53.180058Z","shell.execute_reply.started":"2022-10-11T08:13:53.160149Z","shell.execute_reply":"2022-10-11T08:13:53.179027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nfeature_data = univariate_numerical_plot(df,'Age')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:53.181246Z","iopub.execute_input":"2022-10-11T08:13:53.181532Z","iopub.status.idle":"2022-10-11T08:13:53.686264Z","shell.execute_reply.started":"2022-10-11T08:13:53.181507Z","shell.execute_reply":"2022-10-11T08:13:53.685352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x='HomePlanet'\nunivariate_double_plot(space_titanic,x,x,hue='Transported')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:53.690851Z","iopub.execute_input":"2022-10-11T08:13:53.692733Z","iopub.status.idle":"2022-10-11T08:13:54.014855Z","shell.execute_reply.started":"2022-10-11T08:13:53.6927Z","shell.execute_reply":"2022-10-11T08:13:54.013737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x='CryoSleep'\nunivariate_double_plot(space_titanic,x,x,hue='Transported')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:54.016605Z","iopub.execute_input":"2022-10-11T08:13:54.016957Z","iopub.status.idle":"2022-10-11T08:13:54.32028Z","shell.execute_reply.started":"2022-10-11T08:13:54.016918Z","shell.execute_reply":"2022-10-11T08:13:54.319356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x='Destination'\nunivariate_double_plot(space_titanic,x,x,hue='Transported')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:54.321946Z","iopub.execute_input":"2022-10-11T08:13:54.32262Z","iopub.status.idle":"2022-10-11T08:13:54.64328Z","shell.execute_reply.started":"2022-10-11T08:13:54.322581Z","shell.execute_reply":"2022-10-11T08:13:54.642346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x='VIP'\nunivariate_double_plot(space_titanic,x,x,hue='Transported')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:54.644736Z","iopub.execute_input":"2022-10-11T08:13:54.646692Z","iopub.status.idle":"2022-10-11T08:13:54.94442Z","shell.execute_reply.started":"2022-10-11T08:13:54.646651Z","shell.execute_reply":"2022-10-11T08:13:54.943342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"padding:10px;background-color:orange;margin:0;color:black;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Data Analysis</h1>\n<br>","metadata":{}},{"cell_type":"markdown","source":"# üìã Correspondence Analysis\n**Correspondence analysis between various features and Transported**","metadata":{}},{"cell_type":"code","source":"def Chi_square_test(data,use_method=1,alpha=0.05):\n    #1 Chi square independence test\n    #Use mode 1 or mode 2: select 2 for 1 degree of freedom and 1 for others\n    # use_method = 1 \n    # alpha=0.05 # Set quantile value\n\n    data=data.astype(int)\n    data_j=data.sum()/data.sum().sum()  #Calculate column marginal probability\n    data_i=data.sum(axis=1)             #Calculate row marginal probability\n\n    #Here, the degree of freedom of the matrix should be:(columns_numbers-1)*(row_numbers-1)\n    k=(len(data.columns)-1)*(len(data)-1)\n    #The corresponding rejection domain is:\n    rej_boundary=stats.chi2.ppf(1-alpha/2, k)\n\n    data_exp=np.dot(data_i.values.reshape(-1,1),data_j.values.reshape(1,-1)) \n    data_exp=pd.DataFrame(data_exp,index=data_i.index,columns=data_j.index) \n    data_obj=data.values.flatten()\n    data_exp=data_exp.values.flatten()\n\n    def calculate_chi_val(use_method):\n        if use_method == 1:\n            # method1: The matrix is compressed into one-dimensional calculation,\n            # which is applicable to the case where the degree of freedom is greater than 1\n            # while using stats.chisquare, the degree of freedom is len(f_obs)-1-ddof=(len(data.columns)-1)*(len(data)-1)Ôºåso the parameter 'ddof' should be \n            chi_val,p_val=stats.chisquare(f_obs=data_obj,f_exp=data_exp,ddof=len(data_obj)-1-(len(data.columns)-1)*(len(data)-1))\n            return chi_val,p_val\n\n        elif use_method == 2:\n            #method2ÔºöWhen the degree of freedom is 1, chi square test is performed on the frequency data, \n            # the probability will be low when the chi square distribution of continuous variables is used to calculate the probability\n            #use this formula: X_c^2 = sum(|O-E|-0.5)^2/E\n            chi_val=0\n            for i in range(len(data_obj)):\n                chi_val += (abs(data_obj[i]-data_exp[i])-0.5)**2/data_exp[i]\n            p_val=scipy.stats.chi2.sf(abs(chi_val),k)\n            return chi_val,p_val\n\n    chi_val,p_val = calculate_chi_val(use_method)\n    print('The Chi square value of the sample is:{:.3f}, The corresponding degrees of freedom is: {}, \\nwhen alpha={}, p-value is :{:.8f}, Reject domain boundary is: {:.3f}'\n          .format(chi_val,k,alpha,p_val,rej_boundary))\n\n    if chi_val>rej_boundary:\n        print(f'ConclusionÔºö alpha=0.05, refuse H0, accpet H1. There are essential differences between {data.index.values} in {data.index.name}')\n        print('-'*120)\n    else:\n        print(f'Conclusion: alpha=0.05, accpet H0, refuse H1. There is no essential difference between {data.index.values} in {data.index.name}')\n        print('-'*120)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:54.945917Z","iopub.execute_input":"2022-10-11T08:13:54.946516Z","iopub.status.idle":"2022-10-11T08:13:54.959127Z","shell.execute_reply.started":"2022-10-11T08:13:54.946477Z","shell.execute_reply":"2022-10-11T08:13:54.958055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories=['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\nfor category in categories:\n    feature=[category,'Transported']\n    data = space_titanic[feature]\n    data=data.pivot_table(index=feature[0],columns=feature[1],aggfunc=len,fill_value=0)\n    if 'No internet service' in data.index.values:\n        data.drop('No internet service',inplace=True)\n    if 'No phone service' in data.index.values:\n        data.drop('No phone service',inplace=True)\n    print(f'data matrix:\\n{data}\\n')\n    #print(f'The freedom of the matrix is {len(data.index.values)-1}\\n')\n    #print(data.index.values,data.index.name)\n    Chi_square_test(data,len(data.index.values)-1,0.01)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:54.960756Z","iopub.execute_input":"2022-10-11T08:13:54.961191Z","iopub.status.idle":"2022-10-11T08:13:55.009989Z","shell.execute_reply.started":"2022-10-11T08:13:54.961154Z","shell.execute_reply":"2022-10-11T08:13:55.008878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"padding:10px;background-color:orange;margin:0;color:black;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">XGB Classification</h1>\n<br>","metadata":{}},{"cell_type":"markdown","source":"# üìã XGBoost Classification\n### XGBoost Classification & Hyperparameter selection","metadata":{}},{"cell_type":"code","source":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:55.011583Z","iopub.execute_input":"2022-10-11T08:13:55.011991Z","iopub.status.idle":"2022-10-11T08:13:55.017685Z","shell.execute_reply.started":"2022-10-11T08:13:55.011954Z","shell.execute_reply":"2022-10-11T08:13:55.016673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_dmatrix =xgb.DMatrix(data=X_train, label=y_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:55.019242Z","iopub.execute_input":"2022-10-11T08:13:55.01987Z","iopub.status.idle":"2022-10-11T08:13:56.742716Z","shell.execute_reply.started":"2022-10-11T08:13:55.019827Z","shell.execute_reply":"2022-10-11T08:13:56.741696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5}\ncv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=10,params=PARAMETERS, as_pandas=True, seed=123 )\ncv_results","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:56.746995Z","iopub.execute_input":"2022-10-11T08:13:56.748821Z","iopub.status.idle":"2022-10-11T08:13:57.008105Z","shell.execute_reply.started":"2022-10-11T08:13:56.748786Z","shell.execute_reply":"2022-10-11T08:13:57.007362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_depth\nresults=[]\nfor value in range(3,10,1):\n    PARAMETERS['max_depth'] = value\n    cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=10,params=PARAMETERS, as_pandas=True, seed=123)\n    results.append((cv_results[\"train-auc-mean\"].tail().values[-1],cv_results[\"test-auc-mean\"].tail().values[-1]))\n            \ndata = list(zip(range(3,10,1), results))\nprint(pd.DataFrame(data,columns=['max_depth',\"auc(train,test)\"]))","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:57.009956Z","iopub.execute_input":"2022-10-11T08:13:57.012273Z","iopub.status.idle":"2022-10-11T08:13:58.744509Z","shell.execute_reply.started":"2022-10-11T08:13:57.012243Z","shell.execute_reply":"2022-10-11T08:13:58.743489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gamma\nresults=[]\nfor value in [0.1,0.2,0.5,1,1.5,2]:\n    PARAMETERS['gamma'] = value\n    cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=10,params=PARAMETERS, as_pandas=True, seed=123)\n    results.append((cv_results[\"train-auc-mean\"].tail().values[-1],cv_results[\"test-auc-mean\"].tail().values[-1]))\n            \ndata = list(zip([0.1,0.2,0.5,1,1.5,2], results))\nprint(pd.DataFrame(data,columns=['gamma',\"auc(train,test)\"]))","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:13:58.748228Z","iopub.execute_input":"2022-10-11T08:13:58.748541Z","iopub.status.idle":"2022-10-11T08:14:01.357531Z","shell.execute_reply.started":"2022-10-11T08:13:58.748512Z","shell.execute_reply":"2022-10-11T08:14:01.356493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# colsample_bytree\nresults=[]\nfor value in [.4,.5,.6,.7,.8,.9]:\n    PARAMETERS['colsample_bytree'] = value\n    cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=10,params=PARAMETERS, as_pandas=True, seed=123)\n    results.append((cv_results[\"train-auc-mean\"].tail().values[-1],cv_results[\"test-auc-mean\"].tail().values[-1]))\n            \ndata = list(zip([.4,.5,.6,.7,.8,.9], results))\nprint(pd.DataFrame(data,columns=['colsample_bytree',\"auc(train,test)\"]))","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:14:01.36088Z","iopub.execute_input":"2022-10-11T08:14:01.361258Z","iopub.status.idle":"2022-10-11T08:14:03.074667Z","shell.execute_reply.started":"2022-10-11T08:14:01.361228Z","shell.execute_reply":"2022-10-11T08:14:03.073657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = [0.1,0.3,0.6]\nmax_depth = [3,6,8]\n# min_child_weigh = list(range(0,5,2))\ngamma=[0.2,0.5,1]\nsubsample = [.9,1]\ncolsample_bytree = [.8,1]\n# scale_pos_weight = [.5,1,2]\n# reg_alpha = np.linspace(0.001,1,2).tolist()\nreg_lambda = np.linspace(0.001,1,2).tolist()\n# n_estimators = [50,200,400]\n\nparameters = { 'learning_rate': learning_rate,\n              'max_depth': max_depth,\n#              'min_child_weigh':min_child_weigh,\n             'gamma':gamma,\n             'subsample':subsample,\n             'colsample_bytree':colsample_bytree,\n#              'scale_pos_weight':scale_pos_weight,\n#              'reg_alpha':reg_alpha,\n             'reg_lambda':reg_lambda,\n#              'n_estimators':n_estimators\n             }\n\nmodel = xgb.XGBClassifier(tree_method=\"gpu_hist\")\nclf = GridSearchCV(model, parameters,cv=3,scoring='accuracy',verbose=1,n_jobs=-1)\nclf = clf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:14:03.078046Z","iopub.execute_input":"2022-10-11T08:14:03.08018Z","iopub.status.idle":"2022-10-11T08:15:43.433012Z","shell.execute_reply.started":"2022-10-11T08:14:03.080146Z","shell.execute_reply":"2022-10-11T08:15:43.431473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:15:43.434481Z","iopub.status.idle":"2022-10-11T08:15:43.435532Z","shell.execute_reply.started":"2022-10-11T08:15:43.435248Z","shell.execute_reply":"2022-10-11T08:15:43.435279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = clf.predict(X_train)\ny_test_pred = clf.predict(X_test)\nroc_auc_score(y_test,y_test_pred)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:15:43.437165Z","iopub.status.idle":"2022-10-11T08:15:43.437655Z","shell.execute_reply.started":"2022-10-11T08:15:43.437388Z","shell.execute_reply":"2022-10-11T08:15:43.437424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_test_pred, target_names=[\"0\",\"1\"]))","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:15:43.439075Z","iopub.status.idle":"2022-10-11T08:15:43.439977Z","shell.execute_reply.started":"2022-10-11T08:15:43.439714Z","shell.execute_reply":"2022-10-11T08:15:43.439738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(clf, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:15:43.441737Z","iopub.status.idle":"2022-10-11T08:15:43.442214Z","shell.execute_reply.started":"2022-10-11T08:15:43.441971Z","shell.execute_reply":"2022-10-11T08:15:43.441995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(y_train_pred,y_train,figsize=(5,5), cmap= 'YlGnBu',title='train data confusion matrix')\nskplt.metrics.plot_confusion_matrix(y_test_pred,y_test,figsize=(5,5), cmap= 'YlGnBu',title='test data confusion matrix')","metadata":{"execution":{"iopub.status.busy":"2022-10-11T08:15:43.443878Z","iopub.status.idle":"2022-10-11T08:15:43.444345Z","shell.execute_reply.started":"2022-10-11T08:15:43.4441Z","shell.execute_reply":"2022-10-11T08:15:43.444123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:20px;color:white;margin:20;font-size:270%;text-align:center;display:fill;border-radius:5px;background-color:#cc1100;overflow:hidden;font-weight:700\">Please <b>UPVOTE</b> if it helped!</div>","metadata":{}}]}