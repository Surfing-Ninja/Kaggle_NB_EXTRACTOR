{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30184,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# General imports:-\nfrom math import floor;\nimport numpy as np;\nimport pandas as pd;\nfrom scipy.stats import iqr, mode;\n\nfrom termcolor import colored;\nfrom warnings import filterwarnings;\nfrom gc import collect;\nfrom tqdm.notebook import tqdm;\nfrom IPython.display import clear_output;\n\nimport seaborn as sns;\nimport matplotlib.pyplot as plt;\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:39.068892Z","iopub.execute_input":"2022-07-09T09:11:39.069898Z","iopub.status.idle":"2022-07-09T09:11:40.364046Z","shell.execute_reply.started":"2022-07-09T09:11:39.069792Z","shell.execute_reply":"2022-07-09T09:11:40.362753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sklearn and other model imports:-\nfrom sklearn.pipeline import Pipeline;\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler;\nfrom sklearn_pandas import DataFrameMapper, gen_features;\nfrom sklearn.model_selection import GridSearchCV, train_test_split, KFold;\n\nimport optuna;\nfrom optuna import create_study, trial, Trial;\nfrom optuna.integration import LightGBMPruningCallback, XGBoostPruningCallback;\nfrom optuna.pruners import MedianPruner;\n\nfrom lightgbm import LGBMClassifier;\nfrom xgboost import XGBClassifier;\nfrom catboost import CatBoostClassifier, Pool, cv;\nfrom sklearn.metrics import auc, roc_auc_score, log_loss;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:40.366635Z","iopub.execute_input":"2022-07-09T09:11:40.366994Z","iopub.status.idle":"2022-07-09T09:11:43.236139Z","shell.execute_reply.started":"2022-07-09T09:11:40.366961Z","shell.execute_reply":"2022-07-09T09:11:43.2348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spaceship Titanic Classification:-","metadata":{}},{"cell_type":"code","source":"# Importing relevant data:-\nxytrain = pd.read_csv('../input/spaceship-titanic/train.csv', encoding= 'utf8');\nxtest = pd.read_csv('../input/spaceship-titanic/test.csv', encoding= 'utf8');\nsub_fl = pd.read_csv('../input/spaceship-titanic/sample_submission.csv', encoding= 'utf8');\n\n# Visualizing the data:-\nprint(colored(f\"\\nTrain data:-\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.head(5));\nprint(colored(f\"\\nTest data:-\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xtest.head(5));\nprint(colored(f\"\\nSample Submission:-\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(sub_fl.head(5));\n\nFtre_Lst = list(xytrain.drop('Transported', axis=1).columns);\nTarget = 'Transported';\n\nprint(colored(f\"\\nModel Features:-\\n\", color = 'blue', attrs= ['bold', 'dark']));\nprint(colored(f\"{Ftre_Lst}\", color = 'blue'));","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:43.237825Z","iopub.execute_input":"2022-07-09T09:11:43.238219Z","iopub.status.idle":"2022-07-09T09:11:43.415047Z","shell.execute_reply.started":"2022-07-09T09:11:43.238153Z","shell.execute_reply":"2022-07-09T09:11:43.413722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data visualization and pre-processing:-\n\nIn this section, we develop data visualizations and a strategy to fill nulls across the columns in the train-test sets","metadata":{}},{"cell_type":"code","source":"# Train-test information and description:-\nprint(colored(f\"\\nTrain set info\\n\", color=  'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.info());\n\nprint(colored(f\"\\nTest set info\\n\", color=  'blue', attrs= ['bold', 'dark']));\ndisplay(xtest.info());\n\nprint(colored(f\"\\nTrain set description\\n\", color=  'blue', attrs= ['bold', 'dark']));\ndisplay(xytrain.describe().transpose().style.format('{:,.2f}'));\n\nprint(colored(f\"\\nTest set description\\n\", color=  'blue', attrs= ['bold', 'dark']));\ndisplay(xtest.describe().transpose().style.format('{:,.2f}'));","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:43.418252Z","iopub.execute_input":"2022-07-09T09:11:43.419059Z","iopub.status.idle":"2022-07-09T09:11:43.598557Z","shell.execute_reply.started":"2022-07-09T09:11:43.419008Z","shell.execute_reply":"2022-07-09T09:11:43.597141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target column balance plot:-\nfig, ax = plt.subplots(1,1, figsize= (3,5));\nxytrain.Transported.value_counts(normalize= True).plot.bar(ax= ax, color = 'tab:blue');\nax.set_title(f\"\\nTarget column analysis\\n\", color = 'tab:blue', fontsize= 12);\nax.grid(visible= True, linestyle= '--', which = 'both', color = 'lightgrey');\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:43.600322Z","iopub.execute_input":"2022-07-09T09:11:43.600757Z","iopub.status.idle":"2022-07-09T09:11:43.833255Z","shell.execute_reply.started":"2022-07-09T09:11:43.600714Z","shell.execute_reply":"2022-07-09T09:11:43.832223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing numerical feature distributions:-\nNum_Ftre_Lst = list(xytrain.select_dtypes(include= np.number).columns);\n\nfig, ax = plt.subplots(3,2, figsize=(20,14));\nfor i, col in enumerate(Num_Ftre_Lst):\n    sns.histplot(x=xytrain[col], bins=50, kde= True, color = 'tab:blue', ax=ax[floor(i/2), i%2]);\n    ax[floor(i/2), i%2].grid(visible= True, color= 'lightgrey', linestyle= '--', which= 'both');\nplt.suptitle(f\"Numerical feature distributions\", color= 'tab:blue', fontsize=12);\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:43.834768Z","iopub.execute_input":"2022-07-09T09:11:43.836046Z","iopub.status.idle":"2022-07-09T09:11:45.572137Z","shell.execute_reply.started":"2022-07-09T09:11:43.835994Z","shell.execute_reply":"2022-07-09T09:11:45.571302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing object feature distributions:-\nfig, ax = plt.subplots(2,2, figsize=(10,10));\nfor i, col in enumerate(['HomePlanet', 'CryoSleep', 'Destination', 'VIP']):\n    sns.countplot(x=xytrain[col], color = 'tab:blue', ax=ax[floor(i/2), i%2]);\n    ax[floor(i/2), i%2].grid(visible= True, color= 'lightgrey', linestyle= '--', which= 'both');\nplt.suptitle(f\"Object feature distributions\", color= 'tab:blue', fontsize=12);\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:45.573039Z","iopub.execute_input":"2022-07-09T09:11:45.573354Z","iopub.status.idle":"2022-07-09T09:11:46.119107Z","shell.execute_reply.started":"2022-07-09T09:11:45.573324Z","shell.execute_reply":"2022-07-09T09:11:46.117164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Null check:-\n_ = pd.concat((xytrain[Ftre_Lst].isna().sum(axis=0)/ len(xytrain), \n               xtest.isna().sum(axis=0)/ len(xtest)), axis=1).\\\nrename({0:'Train', 1:'Test'}, axis=1);\n\nfig, ax = plt.subplots(1,1, figsize= (14,7));\n_.plot.bar(ax=ax);\nax.grid(visible= True, linestyle= '--', which = 'both', color = 'lightgrey');\nax.set_title(\"\\nNull records in the train-test data\\n\", color = 'tab:blue', fontsize= 12);\nplt.show();\n\nprint(colored(f\"\\nNull records in the train-test data\\n\", \n              color='blue', attrs= ['bold', 'dark']));\ndisplay(_.drop('PassengerId').style.format('{:.2%}').highlight_max(color= 'lightblue', axis=0).\\\n       highlight_min(color = 'lightyellow', axis=0));","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:46.121937Z","iopub.execute_input":"2022-07-09T09:11:46.122536Z","iopub.status.idle":"2022-07-09T09:11:46.474508Z","shell.execute_reply.started":"2022-07-09T09:11:46.122485Z","shell.execute_reply":"2022-07-09T09:11:46.473244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing categorical feature levels and associated target states:-\nfor col in xytrain.drop(['Name', 'PassengerId', 'Cabin'], axis=1).\\\nselect_dtypes(exclude= np.number).columns:\n    print(colored(f\"\\nAnalysis for {col}\\n\", color = 'blue', attrs= ['dark', 'bold']));\n    display(xytrain.groupby(col,dropna=False).\\\n    agg(Nb_Records=pd.NamedAgg('Transported', np.size),\n        Nb_Transported = pd.NamedAgg('Transported', np.sum)));","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:46.476545Z","iopub.execute_input":"2022-07-09T09:11:46.476976Z","iopub.status.idle":"2022-07-09T09:11:46.558776Z","shell.execute_reply.started":"2022-07-09T09:11:46.476933Z","shell.execute_reply":"2022-07-09T09:11:46.557658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing the cabin column further:-\n_cabin_prf = xytrain.Cabin.str.split('/', expand= True).add_prefix('Cabin').join(xytrain.Transported);\n_cabin_prf['CabinCtg'] = _cabin_prf['Cabin0']+_cabin_prf['Cabin2'];\n\nfig, ax = plt.subplots(1,1, figsize= (14,7));\n_cabin_prf.groupby('CabinCtg').agg(Nb_Passengers= pd.NamedAgg('Transported',np.size), \n                                   Nb_Transported= pd.NamedAgg('Transported',np.sum)\n                                  ).plot.bar(ax= ax);\nax.set_title(f\"\\nCabin category verus transported passengers\\n\", color= 'tab:blue', fontsize= 12);\nax.grid(visible= True, linestyle= '--', which = 'both', color = 'lightgrey');\nax.set_yticks(range(0,1600,100), fontsize= 8);\nax.set_xlabel(\"\\nCabin Category\");\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:46.56191Z","iopub.execute_input":"2022-07-09T09:11:46.562287Z","iopub.status.idle":"2022-07-09T09:11:47.108784Z","shell.execute_reply.started":"2022-07-09T09:11:46.562253Z","shell.execute_reply":"2022-07-09T09:11:47.107544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating cross-tab between cabin category and destination:-\n_ = _cabin_prf.join(xytrain.Destination).groupby(['Destination', 'CabinCtg']).\\\nagg({'Transported': [np.size, np.sum]}).reset_index().\\\npivot(index= 'CabinCtg', columns= 'Destination');\n\n_.columns = [j+'-'+k for i, j,k in _.columns.to_flat_index()];\n\nprint(colored(f\"\\nCross-tab between cabin category and destination\\n\", \n              color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.format(precision = 0));\n\ndel _;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:47.110468Z","iopub.execute_input":"2022-07-09T09:11:47.11102Z","iopub.status.idle":"2022-07-09T09:11:47.150774Z","shell.execute_reply.started":"2022-07-09T09:11:47.110987Z","shell.execute_reply":"2022-07-09T09:11:47.149527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating cross-tab between cabin category and embarkation:-\n_ = _cabin_prf.join(xytrain.HomePlanet).groupby(['HomePlanet', 'CabinCtg']).\\\nagg({'Transported': [np.size, np.sum]}).reset_index().\\\npivot(index= 'CabinCtg', columns= 'HomePlanet');\n\n_.columns = [j+'-'+k for i, j,k in _.columns.to_flat_index()];\n\nprint(colored(f\"\\nCross-tab between cabin category and HomePlanet\\n\", \n              color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.format(precision = 0));\n\ndel _;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:47.15278Z","iopub.execute_input":"2022-07-09T09:11:47.153289Z","iopub.status.idle":"2022-07-09T09:11:47.188664Z","shell.execute_reply.started":"2022-07-09T09:11:47.153239Z","shell.execute_reply":"2022-07-09T09:11:47.18784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating cross-tab between cabin category and VIP status:-\n_ = _cabin_prf.join(xytrain.VIP).groupby(['VIP', 'CabinCtg']).\\\nagg({'Transported': [np.size, np.sum]}).reset_index().\\\npivot(index= 'CabinCtg', columns= 'VIP');\n_.columns = [j+'-'+str(k) for i, j,k in _.columns.to_flat_index()];\n\nprint(colored(f\"\\nCross-tab between cabin category and VIP-status\\n\", \n              color = 'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.highlight_max(axis=0, color = 'lightblue').\\\n        highlight_min(axis=0, color = 'lightyellow').\\\n        format('{:,.0f}'));\n\ndel _;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:47.189971Z","iopub.execute_input":"2022-07-09T09:11:47.191072Z","iopub.status.idle":"2022-07-09T09:11:47.23571Z","shell.execute_reply.started":"2022-07-09T09:11:47.191037Z","shell.execute_reply":"2022-07-09T09:11:47.234467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing the correlation between the training features and the target:-\nfig, ax = plt.subplots(1,1,figsize= (14,8));\nsns.heatmap(data=xytrain.corr(), cmap = 'Spectral_r', ax=ax,linecolor= 'white', center=True,\n            linewidth = 2.0, annot= True, fmt= '.2%', cbar= None);\nax.set_title(f\"Correlation heatmap for train data\\n\", color = 'black', fontsize= 12);\nplt.yticks(rotation= 45, fontsize= 8);\nplt.xticks(rotation= 45, fontsize= 8);\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:47.237355Z","iopub.execute_input":"2022-07-09T09:11:47.237888Z","iopub.status.idle":"2022-07-09T09:11:47.599452Z","shell.execute_reply.started":"2022-07-09T09:11:47.237843Z","shell.execute_reply":"2022-07-09T09:11:47.598255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing the HomePlanet and destination interaction feature:-\n\n_ = pd.concat((xytrain['HomePlanet']+ ' - ' + xytrain['Destination'], xytrain.Transported), axis=1).\\\nrename({0:'Journey_Lbl'}, axis=1).groupby('Journey_Lbl').agg({'Transported': [np.size, np.sum]});\n_.columns = ['Nb_Passengers', 'Nb_Transported'];\n\nfig, ax = plt.subplots(1,1, figsize= (14,9));\n_.plot.bar(ax =ax);\nax.set_title(\"Interaction- journey details\\n\", fontsize= 12, color = 'black');\nax.grid(visible= True, linestyle= '--', which = 'both', color = 'lightgrey');\nax.set_xlabel('');\nax.set_yticks(range(0,3300,100));\nplt.xticks(rotation= 45, fontsize= 7);\nplt.show();\n\ndel fig, ax;\n\nprint(colored(f\"\\nInteraction- journey details\\n\", color=  'blue', attrs= ['bold', 'dark']));\ndisplay(_.style.highlight_max(axis=0, color = 'lightblue').\n        highlight_min(axis=0, color = 'lightyellow').\n        format('{:,.0f}'));\ndel _;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:11:56.368463Z","iopub.execute_input":"2022-07-09T09:11:56.36888Z","iopub.status.idle":"2022-07-09T09:11:56.787559Z","shell.execute_reply.started":"2022-07-09T09:11:56.368845Z","shell.execute_reply":"2022-07-09T09:11:56.786254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing customer spending by VIP-status and cryosleep:-\nfilterwarnings('ignore');\n_ = xytrain[['CryoSleep','VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']];\n_['Total_Spend'] = _.select_dtypes(include= np.number).sum(axis=1);\n\nprint(colored(f\"Cross-tab:- Total Spend versus Cryosleep and VIP status\\n\",\n      color = 'blue', attrs= ['dark', 'bold']));\ndisplay(_.groupby(['CryoSleep', 'VIP'])['Total_Spend'].describe().\\\n        style.highlight_max(axis=0, color= 'lightblue').\\\n        format('{:,.2f}'));\n\n# Analyzing transported passengers versus VIP-status and cryosleep:-\n_xtab = _[['VIP','CryoSleep']].join(xytrain.Transported).\\\ngroupby(['VIP','CryoSleep']).agg({'Transported': [np.size, np.sum]});\n_xtab.columns = ['Nb_Passengers', 'Nb_Transported'];\n_xtab['Rt_Transported'] = _xtab['Nb_Transported']/ _xtab['Nb_Passengers'];\n\nprint(colored(f\"\\nCross-tab:- Total transported passengers versus Cryosleep and VIP status\\n\",\n      color = 'blue', attrs= ['dark', 'bold']));\ndisplay(_xtab.style.format({'Nb_Passengers': '{:,.0f}','Nb_Transported': '{:,.0f}',\n                           'Rt_Transported': '{:.2%}'}));\n\ndel _xtab;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:02.270454Z","iopub.execute_input":"2022-07-09T09:12:02.270831Z","iopub.status.idle":"2022-07-09T09:12:02.349049Z","shell.execute_reply.started":"2022-07-09T09:12:02.2708Z","shell.execute_reply":"2022-07-09T09:12:02.347922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing total spending and target distribution by age:-\n_ = _[['Total_Spend', 'VIP', 'CryoSleep']].join(xytrain[['Age', 'Transported']]);\n_['Lifestage'] = np.select([_.Age < 13.0, _.Age<18.0], ['1.Child', '2.Teen'], '3.Adult');\n\nprint(colored(f\"Cross-tab:- Total transported passengers versus Cryosleep,VIP status and Lifestage\\n\",\n      color = 'blue', attrs= ['dark', 'bold']));\ndisplay(\n_.groupby(['Lifestage','VIP','CryoSleep']).\\\nagg({'Transported': [np.size, np.sum],'Total_Spend': [np.mean, np.median]}).\\\nstyle.format('{:,.0f}').highlight_max(axis=1, color= 'lightblue').\\\nhighlight_min(axis=1, color= 'lightyellow')\n);","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:05.530719Z","iopub.execute_input":"2022-07-09T09:12:05.531275Z","iopub.status.idle":"2022-07-09T09:12:05.609945Z","shell.execute_reply.started":"2022-07-09T09:12:05.531221Z","shell.execute_reply":"2022-07-09T09:12:05.608492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing Name and Passenger ID to elicit filial relations:-\n_ = pd.concat((xytrain['PassengerId'].str.split('_', expand= True).add_prefix('ID'),\n               xytrain['Name'].str.split(' ', expand= True).\\\n               rename({0:'FirstName', 1:'LastName'}, axis=1),\n              _[['Total_Spend']],\n              xytrain[['CryoSleep', 'Transported', 'VIP']]\n              ),\n              axis=1);\n_['CryoSleep'] = np.where(_.CryoSleep == True, 1,0);\n\n# Pooling spending details and passengers per family name:-\n_family_dtl = \\\n_.groupby(['VIP','LastName']).\\\nagg(Total_Spend = pd.NamedAgg('Total_Spend',np.sum),\n    Nb_Passengers = pd.NamedAgg('Transported', np.size),\n    Nb_Transported = pd.NamedAgg('Transported', np.sum),\n    Nb_CryoSleep = pd.NamedAgg('CryoSleep', np.sum)\n   ).\\\nreset_index().sort_values('Total_Spend', ascending= False);\n\n_family_dtl['Rt_Transport'] = _family_dtl['Nb_Transported']/ _family_dtl['Nb_Passengers'];\n\n# Plotting the top 20 spender families:-\nfig, ax= plt.subplots(1,1, figsize= (16,8));\nsns.barplot(data= _family_dtl.head(20),y= 'Total_Spend', x= 'LastName', \n            palette= 'Blues', ax=ax);\nax.set_title(\"Top 20 spender families\\n\", color= 'tab:blue', fontsize=12);\nax.grid(visible= True, which= 'both', linestyle= '--', color= 'lightgrey');\nax.set_yticks(range(0,53000,2000));\nax.set_xlabel(f\"\\nFamily Name\\n\", fontsize=12);\nax.set_ylabel(f\"Spending\\n\", fontsize=12);\nplt.xticks(rotation=45);\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:08.322197Z","iopub.execute_input":"2022-07-09T09:12:08.322576Z","iopub.status.idle":"2022-07-09T09:12:08.925223Z","shell.execute_reply.started":"2022-07-09T09:12:08.322544Z","shell.execute_reply":"2022-07-09T09:12:08.924243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Key notes and inferences:-\n\n1. People in cryosleep did not spend any money throughout the trip\n2. Transportation rate for cryosleep > others\n3. VIP passengers also have higher transportation rate than others\n4. Children have a higher propensity for cryosleep and transportation\n5. Children under 12 years of age did not spend any money at all (could be a policy)\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Data Transformation Pipeline:-\n\nIn this section, we develop a pipeline that elicits the data transformation process and returns the associated model train-test sets. Custom classes and functions are used to develop the pipeline.\n\nThis is divided into 4 functions as below-\n1. Add features- create new columns for name, cabinID components and treat nulls in float columns for amenities\n2. Treat nulls in age and cryosleep columns based on family name and spending details\n3. Create journey column based on null treated home planet and destination, based on family name\n4. Treat nulls in VIP and cabin columns based on family names\n\nThe pipeline is then developed using FunctionTransformer as all of these functions are stateless.\n\n**Pipeline implementation- key note:-**\n\nI stongly discourage the practice of appending the train-test data and generating a common feature processing. \nMy previous versions in the same competition have used the train-test datasets separately to develop and implement the piepline. This is the most common and accepted practice in the industry. One may consider this as an experiment to augment my score on the competition as a sole reason for this step. \n\nThe global variable 'traintest_pipeimpl_req' controls the implementation process. It is advisable to keep this as a 'Y'. Herewith, I have kept it as an 'N'","metadata":{}},{"cell_type":"code","source":"# Initializing a global variable for the traintest separate pipeline implementation:-\ntraintest_pipeimpl_req = 'N';","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:20.572159Z","iopub.execute_input":"2022-07-09T09:12:20.572577Z","iopub.status.idle":"2022-07-09T09:12:20.577832Z","shell.execute_reply.started":"2022-07-09T09:12:20.572543Z","shell.execute_reply":"2022-07-09T09:12:20.57662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def AddFeatures(X: pd.DataFrame, Ftre_Lst: list = Ftre_Lst):\n    \"This function adds the name split, total spending features and treats nulls in amenities\";\n    \n    global traintest_pipeimpl_req;    \n    if traintest_pipeimpl_req == 'N': Ftre_Lst = Ftre_Lst + ['Label'];\n    \n    df = pd.concat((X[Ftre_Lst].drop(['Name', 'Cabin'],axis=1),\n                X.Name.str.split(' ', expand= True).add_prefix('Name'),\n                X.Cabin.str.split('/', expand= True).add_prefix('Cabin')), axis=1);\n\n    df[['RoomService', 'FoodCourt','ShoppingMall','Spa','VRDeck']]=\\\n    df[['RoomService', 'FoodCourt','ShoppingMall','Spa','VRDeck']].fillna(0.0);\n    df['TotalSpend'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + \\\n    df['Spa'] + df['VRDeck'];\n    \n    df['VIP'] = df['VIP']*1.0;\n    df['VIP'] = df['VIP'].astype(np.float16);\n    \n    return df;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:24.939505Z","iopub.execute_input":"2022-07-09T09:12:24.939939Z","iopub.status.idle":"2022-07-09T09:12:24.950537Z","shell.execute_reply.started":"2022-07-09T09:12:24.939903Z","shell.execute_reply":"2022-07-09T09:12:24.949412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TrtNullAgeCrSlp(df:pd.DataFrame):\n    \"\"\"\n    This function fills nulls in age and cryosleep as below-\n    Cryosleep:-\n    1. For non-spenders, cryosleep=1 as cryosleep customers don't spend\n    2. If age <=12 and cryosleep is null, then cryosleep= 1\n    3. For spenders, cryosleep= 0\n    \n    Age:-\n    1. For spenders/ non-cryosleep, median age > 12 for family is considered (child cannot spend)\n    2. For all remaining nulls, overall median age is used\n    \n    Flag for Is_Child (Age <=12) is also created\n    \"\"\";\n    \n    # 1. Filling nulls in cryosleep based on spending and age details:-\n    df['CryoSleep'] = np.float16(df['CryoSleep']*1.0);\n    df.loc[(df.CryoSleep.isna()==True) & (df.TotalSpend == 0.0), ['CryoSleep']] = 1.0;\n    # Assuming child (age <=12) and null cryosleep = cryosleep\n    df.loc[(df.CryoSleep.isna()==True) & (df.Age <=12), ['CryoSleep']] = 1.0;\n    # Assuming no cryosleep for spenders:-\n    df.loc[(df.CryoSleep.isna()==True) & (df['TotalSpend'] > 0.0), ['CryoSleep']] = 0.0;\n    df['CryoSleep'] = df['CryoSleep'].astype(np.int8);\n    \n    # 2. Assuming average family age for spenders:-\n    df = df.merge(df.loc[df.Age >12,['Name1', 'Age']].dropna().groupby('Name1').\\\n                  agg(_Age= pd.NamedAgg('Age', np.median)),\n                  how= 'left', left_on= 'Name1', right_on='Name1', suffixes= ('',''));\n    df.loc[(df.Age.isna()==True) & ((df.TotalSpend > 0.0) | (df.CryoSleep==0)), ['Age']] = df._Age;\n    # Filling median age for remaining nulls:-\n    df['Age'] = df['Age'].fillna(df.Age.median());\n    \n    # 3. Creating flag for child:-\n    df['Is_Child'] = np.where(df.Age <= 12, 1,0);\n    df['Is_Child'] = df['Is_Child'].astype(np.int8);\n    \n    df = df.drop(['_Age'], axis=1);  \n    df['Age'] = df['Age'].astype(np.int8);\n    return df;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:28.605719Z","iopub.execute_input":"2022-07-09T09:12:28.606097Z","iopub.status.idle":"2022-07-09T09:12:28.620871Z","shell.execute_reply.started":"2022-07-09T09:12:28.606066Z","shell.execute_reply":"2022-07-09T09:12:28.619685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CreateJourney(df: pd.DataFrame):\n    \"\"\"\n    This function treats nulls in HomePlanet and Destination and combines them to form Journey\n    1. Home Planet:-\n    a. Based on family name, home planet nulls are filled (all family members have same home planet)\n    b. Remaining nulls are filled using the overall mode\n    \n    2. Destination:-\n    a. Based on family name, mode of destination is created and filled up for nulls\n    b. For all remaining nulls, overall mode is used\n    \n    3. Journey = HomePlanet - Destination is the interaction feature\n    \"\"\";\n    \n    # 1. Fostering null treatment for HomePlanet based on last name and overall mode:-\n    df = df.merge(df[['Name1', 'HomePlanet']].drop_duplicates().dropna(), \n                 how= 'left',left_on= 'Name1', right_on= 'Name1', suffixes= ('', '_'));\n    df['HomePlanet'] = df['HomePlanet'].fillna(df.HomePlanet_);\n    df['HomePlanet'] = df['HomePlanet'].fillna(df[['HomePlanet']].\\\n                                               apply(lambda x: x.mode()).values[0][0]);\n\n    # 2. Fostering null treatment for destination based on last name and overall mode:-\n    _ = df[['Name1', 'Destination']].groupby('Name1')['Destination'].\\\n    value_counts(ascending= False);\n    _.name = 'Nb_Destination';\n    _ = _.reset_index().groupby(['Name1']).head(1);\n\n    df = df.merge(_, how = 'left', left_on= 'Name1', right_on= 'Name1', suffixes= ('', '_'));\n    df['Destination'] = df['Destination'].fillna(df['Destination_']);\n    df['Destination'] = df['Destination'].fillna(df[['Destination']].\\\n                                                 apply(lambda x: x.mode()).values[0][0]);\n    del _;\n\n    # 3. Developing interaction column for journey:-\n    df['Journey'] = df['HomePlanet'] + ' - ' + df['Destination'];\n    df = df.drop(['HomePlanet','Destination','HomePlanet_','Destination_','Nb_Destination'], \n                 axis=1,errors = 'ignore');\n\n    return df;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:31.679031Z","iopub.execute_input":"2022-07-09T09:12:31.679445Z","iopub.status.idle":"2022-07-09T09:12:31.692963Z","shell.execute_reply.started":"2022-07-09T09:12:31.679412Z","shell.execute_reply":"2022-07-09T09:12:31.691945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TrtNullVIPCabin(df: pd.DataFrame):\n    \"\"\"\n    This function treats nulls in VIP and cabin columns using the last name.\n    We assume that members of the same family have the same cabin and VIP IDs\n    As an addition, it downcasts the float64 columns to conserve memory.\n    \"\"\";\n\n    # Assuming that members of the same family have the same VIP ID:-\n    df = df.merge(df[['VIP', 'Name1']].groupby('Name1')['VIP'].max(), \n                how = 'left', left_on= 'Name1', right_on= 'Name1', suffixes= ('','_'));\n    df['VIP'] = df['VIP'].fillna(df.VIP_);\n    df['VIP'] = df['VIP'].fillna(0.0);\n    df['VIP'] = df['VIP'].astype(np.int8);\n\n    # Assuming that members of the same family have the same cabin0/ cabin2 ID:-\n    _ = df[['Cabin0', 'Name1']].groupby('Name1')['Cabin0'].value_counts();\n    _.name = 'Nb_Records';\n    df = df.merge(_.reset_index().groupby('Name1').head(1).drop('Nb_Records', axis=1), \n                how= 'left', left_on= 'Name1', right_on= 'Name1', suffixes= ('','_'));\n    df['Cabin0'] = df['Cabin0'].fillna(df.Cabin0_);\n    del _;\n\n    _ = df[['Cabin2', 'Name1']].groupby('Name1')['Cabin2'].value_counts();\n    _.name = 'Nb_Records';\n    df = df.merge(_.reset_index().groupby('Name1').head(1).drop('Nb_Records', axis=1), \n                how= 'left', left_on= 'Name1', right_on= 'Name1', suffixes= ('','_'));\n    df['Cabin2'] = df['Cabin2'].fillna(df.Cabin2_);\n\n    df = df.drop(['Cabin0_', 'Cabin2_'], axis=1, errors= 'ignore');\n    del _;\n\n    # Considering remaining nulls with overall cabin mode based on VIP status:-\n    df = df.merge(df.groupby(['VIP']).agg({'Cabin0': lambda df: df.mode(), 'Cabin2': lambda y: y.mode()}),\n                how= 'left', left_on= 'VIP', right_index= True, suffixes= ('', '_'));\n    df['Cabin0'] = df['Cabin0'].fillna(df.Cabin0_);\n    df['Cabin2'] = df['Cabin2'].fillna(df.Cabin2_);\n    df = df.drop(['Cabin0_', 'Cabin2_', 'VIP_'], axis=1, errors= 'ignore');\n \n    # Downcasting columns to conserve memory:-    \n    df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TotalSpend']] = \\\n    df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TotalSpend']].astype(np.float16);  \n    \n    # Dropping extra columns after usage:-\n    df = df.drop(['Cabin1', 'Name0', 'Name1'], axis=1, errors= 'ignore');\n    return df;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:34.244751Z","iopub.execute_input":"2022-07-09T09:12:34.245628Z","iopub.status.idle":"2022-07-09T09:12:34.263903Z","shell.execute_reply.started":"2022-07-09T09:12:34.24558Z","shell.execute_reply":"2022-07-09T09:12:34.262507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Organizing the target columns:-\nytrain = np.where(xytrain[['Transported']] == True, 1,0).ravel();\n\n# Collating train data for the feature transformation:-\nXtrain = xytrain.drop('Transported', axis=1);\n\n# Developing the data transformer pipeline:-\nData_Xformer=\\\nPipeline(steps= \n         [('AddFeatures', FunctionTransformer(AddFeatures)),\n          ('TrtNullAgeCrSlp', FunctionTransformer(TrtNullAgeCrSlp)),\n          ('CreateJourney', FunctionTransformer(CreateJourney)),\n          ('TrtNullVIPCabin', FunctionTransformer(TrtNullVIPCabin)),\n          ('LblEncode', DataFrameMapper(input_df= True, df_out= True, drop_cols= ['PassengerId'],default=None,\n                                       features=gen_features(columns= [['Journey'], ['Cabin0'], ['Cabin2']],\n                                                             classes= [LabelEncoder])\n                                       ))\n         ], verbose= True);","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:36.673Z","iopub.execute_input":"2022-07-09T09:12:36.673646Z","iopub.status.idle":"2022-07-09T09:12:36.684644Z","shell.execute_reply.started":"2022-07-09T09:12:36.673607Z","shell.execute_reply":"2022-07-09T09:12:36.683677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if traintest_pipeimpl_req == 'Y':\n    # Implementing the pipeline on the training set and test set:-\n    Xtrain = Data_Xformer.fit_transform(Xtrain, ytrain);\n    Xtest = Data_Xformer.transform(xtest);\n\n    print(colored(f\"\\nTrain-Test pipeline implementation results\\n\", color= 'blue', attrs= ['bold', 'dark']));\n    print(colored(f\"{len(Xtrain), len(Xtest)}\", color = 'blue'));\n\nelif traintest_pipeimpl_req == 'N':\n    print(colored(f\"\\nSingle pipeline implementation results\\n\", color= 'blue', attrs= ['bold', 'dark']));\n    \n    # Creating a common data-set using the train and test data:-\n    X = pd.concat([Xtrain.assign(Label = 'Train'), xtest.assign(Label = 'Test')], axis=0, ignore_index=True);\n\n    # Implementing the pipeline on the common dataset:-\n    X = Data_Xformer.fit_transform(X, np.zeros(len(X)));\n\n    # Splitting the common dataset back to train-test components:-\n    Xtrain = X.loc[X.Label == 'Train'].drop(['Label'], axis=1);\n    Xtest = X.loc[X.Label == 'Test'].drop(['Label'], axis=1);\n    print(colored(f\"\\nLength(X)= {len(X)}, Length(Xtrain,Xtest)= {len(Xtrain), len(Xtest)}\\n\", \n                  color = 'blue'));\n    del X;\n\nprint(colored(f\"\\nTrain-set pipeline output columns\", color= 'blue', attrs= ['bold', 'dark']));\nprint(colored(f\"{list(Xtrain.columns)}\", color = 'blue'));\n\nprint(colored(f\"\\nTest-set pipeline output columns\", color= 'blue', attrs= ['bold', 'dark']));\nprint(colored(f\"{list(Xtest.columns)}\", color = 'blue'));\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:12:54.93948Z","iopub.execute_input":"2022-07-09T09:12:54.939861Z","iopub.status.idle":"2022-07-09T09:12:55.534388Z","shell.execute_reply.started":"2022-07-09T09:12:54.939831Z","shell.execute_reply":"2022-07-09T09:12:55.533226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting correlation plot after the pipeline:-\nfig, ax= plt.subplots(1,1, figsize= (16,10));\n\n_ = Xtrain.corr();\nsns.heatmap(_, cmap = 'icefire', annot= True, fmt= '.1%', mask= np.triu(np.ones_like(_)),\n            linecolor= 'white',linewidth= 1.50, ax=ax, center= True, cbar= None);\nax.set_title(\"\\nCorrelation heatmap after the pipeline implementation\\n\", fontsize= 12, \n             color= 'tab:blue', fontweight= 'bold');\nplt.yticks(rotation= 45, fontsize= 8);\nplt.xticks(rotation= 45, fontsize= 8);\nplt.tight_layout();\nplt.show();\n\ndel _;\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:00.361306Z","iopub.execute_input":"2022-07-09T09:13:00.361684Z","iopub.status.idle":"2022-07-09T09:13:01.257668Z","shell.execute_reply.started":"2022-07-09T09:13:00.361653Z","shell.execute_reply":"2022-07-09T09:13:01.256183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Training and Development:-\n\nThis section is divided into 2 parts- \n\n1. We find the best model parameters with optuna. This is much better than grid-search as we have a better search algorithm with overall better tuning. The below link elicits the process in a detailed manner- \nhttps://programming.vip/docs/lightgbm-optuna-super-parameter-automatic-tuning-tutorial-with-code-framework.html\n\n2. Once the best parameters are elicited, we use bootstrapping sampling and train a large number of classifiers with the best parameters to generate candidate estimations for the test-set. The submission file is a central tendency of all these predictions (mostly mean/ median)\n\n3. CatBoost classifer does not have a pre-defined callback with optuna. We, thereby, design a callback function and use a CV and parameter dictionary for the same. We have considered the catboost parameters from the below link- https://www.kaggle.com/code/igorshirokov/top-10-space-titanic\n","metadata":{}},{"cell_type":"code","source":"# Creating best parameters dictionary:-\nbest_params = {};","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:05.850078Z","iopub.execute_input":"2022-07-09T09:13:05.850563Z","iopub.status.idle":"2022-07-09T09:13:05.855928Z","shell.execute_reply.started":"2022-07-09T09:13:05.850522Z","shell.execute_reply":"2022-07-09T09:13:05.854897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimating the best model hyper-parameters using optuna:-\ndef CreateLGBMObjective(trial, X, y):\n    \"\"\"\n    This function develops the param-grid for LGBM and optimizes it with a train-test split\n    \"\"\";\n       \n    # Creating the parameter-grid:-    \n    param_grid = \\\n    {\n    \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [500]),\n    'learning_rate': trial.suggest_loguniform('learning_rate', 0.03, 0.10),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 4, 7, step = 1),\n    \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n    \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, log= True),\n    \"random_state\": 10\n    };\n    \n    # Developing the train-test split:-\n    Xtr, Xdev, Ytr, Ydev = train_test_split(X, y, test_size=0.2,random_state=42);\n\n    # Developing the LGBM classifer with the parameter tuning and pruning:-\n    model = LGBMClassifier(objective = \"binary\", **param_grid);\n    model.fit(Xtr, Ytr, eval_set=[(Xdev, Ydev)], eval_metric= \"auc\", verbose= 0,\n              early_stopping_rounds= 40, callbacks=[LightGBMPruningCallback(trial, \"auc\")]);\n\n    return log_loss(Ydev, model.predict_proba(Xdev)[:,1]);","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:09.729923Z","iopub.execute_input":"2022-07-09T09:13:09.731112Z","iopub.status.idle":"2022-07-09T09:13:09.74056Z","shell.execute_reply.started":"2022-07-09T09:13:09.731072Z","shell.execute_reply":"2022-07-09T09:13:09.739389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CreateXGBoostObjective(trial, X, y):\n    \"\"\"\n    This function develops the param-grid for Xgboost classfier and optimizes it with a train-test split\n    \"\"\";\n    \n    # Creating the parameter-grid:-  \n    param_grid = \\\n    {\n    \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [500]),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 4, 7), \n    \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.04, 0.12),\n    \"random_state\": 10\n    };\n    \n    # Developing the train-test split:-\n    Xtr, Xdev, Ytr, Ydev = train_test_split(X, y, test_size=0.2,random_state=42); \n    \n    # Developing the XgBoost classifer with the parameter tuning and pruning:-\n    model = XGBClassifier(**param_grid);\n    model.fit(Xtr, Ytr, eval_set=[(Xdev, Ydev)], eval_metric= \"auc\", verbose= 0,\n              early_stopping_rounds= 40, callbacks=[XGBoostPruningCallback(trial, \"validation_0-auc\")]);     \n    return log_loss(Ydev, model.predict_proba(Xdev)[:,1]);","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:23.845324Z","iopub.execute_input":"2022-07-09T09:13:23.845773Z","iopub.status.idle":"2022-07-09T09:13:23.855922Z","shell.execute_reply.started":"2022-07-09T09:13:23.845739Z","shell.execute_reply":"2022-07-09T09:13:23.854459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CreateCatBoostObjective(trial, X, y, verbose= False):\n    \"\"\"\n    This function develops the param-grid for CatBoost classfier and optimizes it with a CV split\n    \"\"\";   \n    \n    # Creating the parameter-grid:- \n    param_grid = \\\n    {\n    'objective': 'Logloss',\n    'loss_function': 'Logloss',\n    'depth': trial.suggest_int('depth', 4, 7),\n    'subsample': trial.suggest_discrete_uniform(\"subsample\", 0.1, 1.0, 0.1),\n    'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 0.9, log=True),\n    'random_strength': trial.suggest_float('random_strength', 0.4, 1),\n    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 6),\n    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 48, 2),\n    'iterations': 1250\n    };\n    \n    # Developing the train-test split:-\n    cv_data = cv(pool= Pool(X, y, cat_features= None),\n                 params=param_grid,early_stopping_rounds=125,logging_level='Silent',\n                   fold_count=4,stratified=True,partition_random_seed= 38);\n \n    # Creating the catboost cross-validation summary:-\n    best_value = cv_data['test-Logloss-mean'].min()   \n    if verbose == True:\n        best_iter = cv_data['test-Logloss-mean'].values.argmin();\n        print(colored(f'\\nBest validation Logloss score:', color= 'blue', attrs= ['dark','bold']));\n        print(colored(f\"{best_value:.3f}Â±{cv_data['test-Logloss-std'][best_iter]:.3f} on step {best_iter}\\n\",\n                      color= 'blue'));\n    else:\n        return best_value;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:27.419249Z","iopub.execute_input":"2022-07-09T09:13:27.419618Z","iopub.status.idle":"2022-07-09T09:13:27.433035Z","shell.execute_reply.started":"2022-07-09T09:13:27.419588Z","shell.execute_reply":"2022-07-09T09:13:27.431683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CreateCatBoostCallBack(study, frozen_trial):\n    \"\"\"\n    This function creates a logging callback for the catboost classifier.\n    The optuna module does not have a pre-defined callback for this algorithm.\n    \"\"\";\n    \n    previous_best_value = study.user_attrs.get(\"previous_best_value\", None);\n    if previous_best_value != study.best_value:\n        study.set_user_attr(\"previous_best_value\", study.best_value);","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:30.996822Z","iopub.execute_input":"2022-07-09T09:13:30.997464Z","iopub.status.idle":"2022-07-09T09:13:31.002702Z","shell.execute_reply.started":"2022-07-09T09:13:30.99742Z","shell.execute_reply":"2022-07-09T09:13:31.001796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def FindBestMdlParams(method:str, n_trials: np.int16):\n    \"\"\"\n    This function returns the best parameters using optuna parameter search for the chosen ensemble method\n    Inputs- \n    1. method- (string):- Ensemble method\n    2. n_trials- (int16):- number of trials to cater to best parameter selection\n    \"\"\"\n    \n    global best_params;\n    \n    filterwarnings('ignore');\n    if method == 'LGBM':\n        filterwarnings('ignore');\n        study = create_study(direction = \"maximize\", study_name= \"LGBM Parameters\", \n                             pruner = MedianPruner(n_warmup_steps= 10)); \n        func = lambda trial: CreateLGBMObjective(trial, Xtrain, ytrain);\n        study.optimize(func, n_trials= n_trials, show_progress_bar=True, gc_after_trial= True);\n\n    elif method == 'XgBoost':\n        study = create_study(direction = \"maximize\", study_name= \"XgBoost Parameters\",\n                            pruner= MedianPruner(n_warmup_steps= 10)); \n        func = lambda trial: CreateXGBoostObjective(trial, Xtrain, ytrain);\n        study.optimize(func, n_trials= n_trials, show_progress_bar=True, gc_after_trial= True);\n        \n    elif method == 'CatBoost':\n        study = create_study(study_name= \"CatBoost Parameters\",\n                             pruner= MedianPruner(n_warmup_steps=5),direction=\"minimize\");\n        func = lambda trial: CreateCatBoostObjective(trial, Xtrain, ytrain);\n        study.optimize(func, n_trials= n_trials,gc_after_trial=True,\n                       callbacks=[CreateCatBoostCallBack]);\n        \n    clear_output();\n    collect();\n    best_params[method] = study.best_params;","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:34.163358Z","iopub.execute_input":"2022-07-09T09:13:34.164044Z","iopub.status.idle":"2022-07-09T09:13:34.176664Z","shell.execute_reply.started":"2022-07-09T09:13:34.163984Z","shell.execute_reply":"2022-07-09T09:13:34.175844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for method in tqdm(['CatBoost']): \n    print('\\n');\n    FindBestMdlParams(method= method, n_trials= 1000);\n    print('\\n');\n\nclear_output();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:13:40.381136Z","iopub.execute_input":"2022-07-09T09:13:40.381567Z","iopub.status.idle":"2022-07-09T09:14:55.672507Z","shell.execute_reply.started":"2022-07-09T09:13:40.381535Z","shell.execute_reply":"2022-07-09T09:14:55.671155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we train ML models (tree based models) and develop the test set predictions using the pipeline output. We follow the below routine:-\n\n1. Sample the train data using a higher fraction (say- 90%) with different random states. Develop the model train, development sets thereby\n2. Use an ensemble approach on the sampled model train set and use the validation set for the scoring metric calculation\n3. Make a test set prediction using the model trained and so fitted. Predicting probabilities seems to be a better option than predicting labels\n4. Store the test set prediction in an output dataframe \n5. Once all models are executed, consider the probability prediction average across the n-model runs and use it as a final prediction\n6. Consider calibrating the model if needed. This is a last decision after the model development. Mostly, calibration is not necessary","metadata":{}},{"cell_type":"code","source":"# Creating the output dataframe for test predictions:-\nMdl_Pred_Prf = pd.DataFrame(data= None, index= xtest['PassengerId'], columns= None);\n\n# Defining the training function for the ensemble models:-\ndef TrainEnsembles(mdl_name: str, n_mdl_runs: np.int16 = 500, mdl_best_params= best_params, \n                   sample_frac:np.float16 = 0.95):\n    \"\"\"\n    This function trains the ensemble models based on the best parameters obtained from the grid-search.\n    Inputs- \n    1. mdl_name- (string):- Model ensemble method\n    2. n_mdl_runs- (int):- Number of candidates\n    3. mdl_best_params- (dict):- Best parameters (from grid search)\n    4. sample_frac- (float):- Sampling fraction for the training set\n    \"\"\";\n    \n    for i in tqdm(range(0,n_mdl_runs,1)):\n        filterwarnings('ignore');\n        print(colored(f\"\\nIteration{i}\", color= 'blue', attrs= ['bold', 'dark']));\n        xtr = Xtrain.sample(random_state = i, frac= sample_frac);\n        xdev = Xtrain.loc[~Xtrain.index.isin(xtr.index)];\n        ytr = ytrain[xtr.index];\n        ydev = ytrain[xdev.index];\n        \n        if mdl_name == 'LGBM':\n            mdl = LGBMClassifier(**mdl_best_params.get('LGBM'));\n        elif mdl_name == 'XgBoost':\n            mdl = XGBClassifier(**mdl_best_params.get('XgBoost'));\n        elif mdl_name == 'CatBoost':\n            mdl = CatBoostClassifier(**mdl_best_params.get('CatBoost'),eval_metric= 'AUC');\n        \n        if mdl_name != 'CatBoost':\n            mdl.fit(xtr, ytr, eval_set=[(xtr, ytr), (xdev, ydev)], eval_metric= ['auc'], \n                    early_stopping_rounds= 50, verbose= 300);\n        elif mdl_name == 'CatBoost':\n            mdl.fit(xtr,ytr,eval_set=[(xtr, ytr),(xdev, ydev)],early_stopping_rounds= 50,verbose= 300);\n            \n        Mdl_Pred_Prf[f\"{mdl_name}{i}\"] = mdl.predict_proba(Xtest)[:,1];\n\n        del xtr, xdev, ytr, ydev;\n        collect();\n    collect();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:14:59.702863Z","iopub.execute_input":"2022-07-09T09:14:59.703351Z","iopub.status.idle":"2022-07-09T09:14:59.868357Z","shell.execute_reply.started":"2022-07-09T09:14:59.703312Z","shell.execute_reply":"2022-07-09T09:14:59.866911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TrainEnsembles(mdl_name= 'LGBM', n_mdl_runs = 1000);\n# TrainEnsembles(mdl_name= 'XgBoost', n_mdl_runs = 1000);\nTrainEnsembles(mdl_name= 'CatBoost', n_mdl_runs = 3000);\n\nclear_output();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:15:09.397005Z","iopub.execute_input":"2022-07-09T09:15:09.397425Z","iopub.status.idle":"2022-07-09T09:15:16.769318Z","shell.execute_reply.started":"2022-07-09T09:15:09.397387Z","shell.execute_reply":"2022-07-09T09:15:16.76846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the model iterations in the working directory for any future use:-\nMdl_Pred_Prf.to_csv('Mdl_Pred_Prf.csv');\n\n# Preparing the submission file:-\nMdl_Pred_Prf['Transported'] = Mdl_Pred_Prf.median(axis=1);\nMdl_Pred_Prf['Transported'] = np.where(Mdl_Pred_Prf['Transported'] >=0.50, True,False)\n\nMdl_Pred_Prf.reset_index()[['PassengerId', 'Transported']].to_csv(\"submission.csv\", index= False);\n\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:15:32.236359Z","iopub.execute_input":"2022-07-09T09:15:32.236782Z","iopub.status.idle":"2022-07-09T09:15:32.442079Z","shell.execute_reply.started":"2022-07-09T09:15:32.236748Z","shell.execute_reply":"2022-07-09T09:15:32.441192Z"},"trusted":true},"execution_count":null,"outputs":[]}]}