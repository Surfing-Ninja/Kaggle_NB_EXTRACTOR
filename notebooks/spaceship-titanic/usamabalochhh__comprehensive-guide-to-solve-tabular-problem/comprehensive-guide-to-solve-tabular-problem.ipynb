{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n<h1><center><strong>Let's Move to Another Dimension ðŸš€</strong></center></h1>","metadata":{}},{"cell_type":"markdown","source":"#### We Use one of the basic Problem in Kaggle: **\"Space-ship Titanic Problem\"** ","metadata":{}},{"cell_type":"markdown","source":"<div>\n<img src=\"https://i.imgur.com/gfg8lWr.jpg?fb\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Import Libraries ðŸ“ƒ</strong></h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### If this helped in your learning, then please **UPVOTE** â€“ as they are the source of motivation!","metadata":{}},{"cell_type":"code","source":"# download this first then comment it.\n# ! pip install -q scikit-plot ","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:16.376659Z","iopub.execute_input":"2023-02-05T12:10:16.37708Z","iopub.status.idle":"2023-02-05T12:10:16.39657Z","shell.execute_reply.started":"2023-02-05T12:10:16.376993Z","shell.execute_reply":"2023-02-05T12:10:16.395737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as patches\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport scikitplot as skplt\n\nclass clr:\n    S = '\\033[1m' + '\\033[94m'\n    E = '\\033[0m'\n    \n    \nwarnings.filterwarnings('ignore')\nmy_colors = [\"#5EAFD9\", \"#449DD1\", \"#3977BB\", \n             \"#2D51A5\", \"#5C4C8F\", \"#8B4679\",\n             \"#C53D4C\", \"#E23836\", \"#FF4633\", \"#FF5746\"]\n\nprint(clr.S+\"Notebook Color Schemes:\"+clr.E)\nsns.palplot(sns.color_palette(my_colors))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:16.414386Z","iopub.execute_input":"2023-02-05T12:10:16.415572Z","iopub.status.idle":"2023-02-05T12:10:18.066628Z","shell.execute_reply.started":"2023-02-05T12:10:16.415509Z","shell.execute_reply":"2023-02-05T12:10:18.065603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>1. The Problem Statement ðŸ“ƒ</strong></h3>\n    <p>\n    In this kernel, I Explain full roadmap of analyzing the problem statement and then Use EDA to analyze the Data, we convert the raw data into insightful data.\n\n\n\nThe Dataset is from the competition named **Spaceship-Titanic** which is the tabular type problem and we have some categorical data and some numerical data in this **Spaceship-Titanic** Problem. \n\n### What we want to Achieve: \nPredict If the Person is Transported to an alternate dimension or Not.\n\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>2. Import Dataset ðŸ“ƒ</strong></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest_data = pd.read_csv('../input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.068412Z","iopub.execute_input":"2023-02-05T12:10:18.068941Z","iopub.status.idle":"2023-02-05T12:10:18.151909Z","shell.execute_reply.started":"2023-02-05T12:10:18.068906Z","shell.execute_reply":"2023-02-05T12:10:18.15092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>3. Exploratory Data Analysis ðŸ“Š</strong></h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Now I will explore the data and gain insight from the data.\n\n#### **View the Dimensions of the train and test dataset.**\n\n\nUsing function name **.shape** to find the rows and columns of the dataset, it returns the tuple (nrows,ncols) => so nrows shows the total data of the file and ncols shows the features of the file that we will use in future.\n\n\n**Let's dive into the Data to find Patterns** ðŸ˜Ž","metadata":{}},{"cell_type":"code","source":"# Printing the Shape of the Train and Tet data to find data's instances and features.\n\nprint(f'The Shpae of the Train data : {train_data.shape}')\nprint(f'The Shpae of the Test data : {test_data.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.15523Z","iopub.execute_input":"2023-02-05T12:10:18.155576Z","iopub.status.idle":"2023-02-05T12:10:18.160531Z","shell.execute_reply.started":"2023-02-05T12:10:18.155551Z","shell.execute_reply":"2023-02-05T12:10:18.159046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In Train data we have *8693* instances and *14* features.\n* In Test data we have *4277* instances and *13* features.","metadata":{}},{"cell_type":"markdown","source":"## Preview the dataset","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.163171Z","iopub.execute_input":"2023-02-05T12:10:18.163852Z","iopub.status.idle":"2023-02-05T12:10:18.197618Z","shell.execute_reply.started":"2023-02-05T12:10:18.163814Z","shell.execute_reply":"2023-02-05T12:10:18.195978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Function used to Print the useful of the csv file\n\ndef get_csv(df,name):\n    print(clr.S+f\" === {name} === \"+clr.E)\n    print(clr.S+\"Total Missing Values : \"+clr.E,df.isnull().sum().sum())\n    print(clr.S+\"Columns = \"+clr.E,list(df.columns),\"\\n\\n\")\n    \nget_csv(train_data,'Train')\nget_csv(test_data,'Test')","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.199165Z","iopub.execute_input":"2023-02-05T12:10:18.199636Z","iopub.status.idle":"2023-02-05T12:10:18.216579Z","shell.execute_reply.started":"2023-02-05T12:10:18.199595Z","shell.execute_reply":"2023-02-05T12:10:18.215225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>View summary of dataset ðŸ“ƒ</strong></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.218893Z","iopub.execute_input":"2023-02-05T12:10:18.219316Z","iopub.status.idle":"2023-02-05T12:10:18.248943Z","shell.execute_reply.started":"2023-02-05T12:10:18.219282Z","shell.execute_reply":"2023-02-05T12:10:18.247739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Findings:\n* we can see that dataset contain 7 character data and 6 numerical and 1 boolean\n* Transported is the Target Column\n* There are Some Missing Values in Some Columns\n*  HomePlanet, CryoSleep, Cabin, Destination, Age, VIP, RoomServices, FoodCourt, ShoppingMall, Spa,VRDeck and Name => These Columns have missing values","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>View Statistics of Data ðŸ‘€</strong></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"train_data.describe().T","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.250271Z","iopub.execute_input":"2023-02-05T12:10:18.25121Z","iopub.status.idle":"2023-02-05T12:10:18.287286Z","shell.execute_reply.started":"2023-02-05T12:10:18.251173Z","shell.execute_reply":"2023-02-05T12:10:18.286351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Important points to note:\n\n* The above command df.describe() helps us to view the statistical properties of numerical variables. It excludes character variables.\n* If we want to view the statistical properties of character variables, we should run the following command -\n\n         train_data.describe(include=['object'])\n\n* If we want to view the statistical properties of all the variables, we should run the following command -\n\n       train_data.describe(include='all')\n\n","metadata":{}},{"cell_type":"markdown","source":"### Key Points to Consider:\n\n* From Checking above Statistical Properties of the train data we concluded that: \n* Columns: Spa, RoomService, VRDeck, ShoppingMall and FoodCourt have outliers\n\n* Spa, RoomService, VRDeck, ShoppingMall and FoodCourt Columns are all about the expenses of Passengers. So, we add them up for every Passenger to Calculate the Total Expense.\n\n\n#### **Let's Explore More :)** ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,24))\nplt.subplot(4,3,1)\ntrain_data.dtypes.value_counts().plot(kind='pie',autopct='%.1f%%',colors=[my_colors[4],my_colors[5],my_colors[6]])\nplt.subplot(4,3,2)\nsns.countplot(data=train_data,x='Transported',palette = [my_colors[1],my_colors[2]])","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.288591Z","iopub.execute_input":"2023-02-05T12:10:18.288981Z","iopub.status.idle":"2023-02-05T12:10:18.543227Z","shell.execute_reply.started":"2023-02-05T12:10:18.288944Z","shell.execute_reply":"2023-02-05T12:10:18.542139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we see that Transported Column is balanced.\n* 50% of data are categorical and ~43% are Float and 7.1% are boolean","metadata":{}},{"cell_type":"markdown","source":"### Checking Nans in the Dataset:\n\n* Using isnull function we will detect the Nans in every column","metadata":{}},{"cell_type":"code","source":"print(clr.S+\" --- Count of Nan's in Every Column --- \"+clr.E)\ntrain_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.54433Z","iopub.execute_input":"2023-02-05T12:10:18.54556Z","iopub.status.idle":"2023-02-05T12:10:18.559148Z","shell.execute_reply.started":"2023-02-05T12:10:18.545524Z","shell.execute_reply":"2023-02-05T12:10:18.557806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.title('Count of Nan in Every Columns')\ntrain_data.isnull().sum().plot(kind='bar',color=my_colors[3])","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.563034Z","iopub.execute_input":"2023-02-05T12:10:18.564018Z","iopub.status.idle":"2023-02-05T12:10:18.777945Z","shell.execute_reply.started":"2023-02-05T12:10:18.563951Z","shell.execute_reply":"2023-02-05T12:10:18.776787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(clr.S+\"Percentage of Nan columns in Train data \"+clr.E)\nround(train_data.isnull().sum() / len(train_data) * 100,2)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.779495Z","iopub.execute_input":"2023-02-05T12:10:18.779799Z","iopub.status.idle":"2023-02-05T12:10:18.792959Z","shell.execute_reply.started":"2023-02-05T12:10:18.779771Z","shell.execute_reply":"2023-02-05T12:10:18.791428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpretation\n\n* we can see that we have slight Nans in most of the columns.\n* We do not drop the Column because we have low percentage of Nan in Columns.\n* We Will Use Imputation Techniques to Fill the Nan Columns. we will discuss it later :)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>First we Analyze Numerical Data</strong></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Using select_dtypes we include only numerical data\n\nnum_data = train_data.select_dtypes(exclude=['object']).copy()\n\nnum_data.head() # confirming that we selected numerical columns","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.794411Z","iopub.execute_input":"2023-02-05T12:10:18.795161Z","iopub.status.idle":"2023-02-05T12:10:18.81602Z","shell.execute_reply.started":"2023-02-05T12:10:18.795111Z","shell.execute_reply":"2023-02-05T12:10:18.814428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary of Numerical Columns:\n\n\n* Numericals Columns : *[Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck]*\n* All Columns are continuous datatype.","metadata":{}},{"cell_type":"markdown","source":"#### Checking Age Column:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,24))\ntemp = num_data['Age']\nx = pd.Series(temp,name='Age Variable')\nplt.subplot(4,3,1)\nax = sns.distplot(temp,bins=10,color=my_colors[0])\nax.set_title(\"Distribution of Age Variable\")\nplt.subplot(4,3,2)\nax = sns.kdeplot(x,shade=True,color=my_colors[8])\nax.set_title('Distribution of Age Variable')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:18.817512Z","iopub.execute_input":"2023-02-05T12:10:18.819759Z","iopub.status.idle":"2023-02-05T12:10:19.157248Z","shell.execute_reply.started":"2023-02-05T12:10:18.819709Z","shell.execute_reply":"2023-02-05T12:10:19.155556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n* We see that Age column is normalized and it is slighlty skewed but not too much so we consider it as Normalized Column. \n* If it is Normalized then, we can say that it does not have Outliers.\n* To Confirm Our Hypothesis that Age Column did not have outliers, we Use Boxplot to Confirm it","metadata":{}},{"cell_type":"markdown","source":"**Detect Outliers in** Age Column **with boxplot**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nax = sns.boxplot(num_data['Age'],color=my_colors[4])\nax.set_title('Visualizing Age Column to detect Outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:19.15959Z","iopub.execute_input":"2023-02-05T12:10:19.160089Z","iopub.status.idle":"2023-02-05T12:10:19.294492Z","shell.execute_reply.started":"2023-02-05T12:10:19.160046Z","shell.execute_reply":"2023-02-05T12:10:19.29365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see that Our Hypothesis is right because Age Column does not have any outliers.","metadata":{}},{"cell_type":"markdown","source":"#### Checking RoomService Column","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,24))\ntemp = num_data['RoomService']\nx = pd.Series(temp,name='RoomService Variable')\nplt.subplot(4,3,1)\nax = sns.distplot(temp,bins=10,color=my_colors[0])\nax.set_title(\"Distribution of RoomService Variable\")\nplt.subplot(4,3,2)\nax = sns.kdeplot(x,shade=True,color=my_colors[2])\nax.set_title('Distribution of RoomService Variable')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:19.297423Z","iopub.execute_input":"2023-02-05T12:10:19.297956Z","iopub.status.idle":"2023-02-05T12:10:19.670097Z","shell.execute_reply.started":"2023-02-05T12:10:19.29792Z","shell.execute_reply":"2023-02-05T12:10:19.668452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n* We see that RoomService column is Not normalized and it is positively skewed.\n* we can say that it have Outliers.\n* To Confirm Our Hypothesis that RoomService Column have outliers, we Use Boxplot to Confirm it","metadata":{}},{"cell_type":"markdown","source":"**Detect Outliers in** RoomService Column **with boxplot**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nax = sns.boxplot(num_data['RoomService'],color=my_colors[5])\nax.set_title('Visualizing RoomService Column to detect Outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:19.672031Z","iopub.execute_input":"2023-02-05T12:10:19.672488Z","iopub.status.idle":"2023-02-05T12:10:19.7934Z","shell.execute_reply.started":"2023-02-05T12:10:19.672453Z","shell.execute_reply":"2023-02-05T12:10:19.792492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our Hypothesis is right, RoomService Feature have Outliers. we will remove it in future after looking at other columns first.","metadata":{}},{"cell_type":"markdown","source":"#### Checking FoodCourt Column ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,24))\ntemp = num_data['FoodCourt']\nx = pd.Series(temp,name='FoodCourt Variable')\nplt.subplot(4,3,1)\nax = sns.distplot(temp,bins=10,color=my_colors[0])\nax.set_title(\"Distribution of FoodCourt Variable\")\nplt.subplot(4,3,2)\nax = sns.kdeplot(x,shade=True,color=my_colors[9])\nax.set_title('Distribution of FoodCourt Variable')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:19.79462Z","iopub.execute_input":"2023-02-05T12:10:19.794948Z","iopub.status.idle":"2023-02-05T12:10:20.162144Z","shell.execute_reply.started":"2023-02-05T12:10:19.794918Z","shell.execute_reply":"2023-02-05T12:10:20.159967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n* We see that FoodCourt column is Not normalized and it is positively skewed.\n* we can say that it have Outliers.\n* To Confirm Our Hypothesis that FoodCourt Column have outliers, we Use Boxplot to Confirm it","metadata":{}},{"cell_type":"markdown","source":"**Detect Outliers in** FoodCourt Column **with boxplot**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nax = sns.boxplot(num_data['FoodCourt'],color=my_colors[7])\nax.set_title('Visualizing FoodCourt Column to detect Outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:20.164589Z","iopub.execute_input":"2023-02-05T12:10:20.165045Z","iopub.status.idle":"2023-02-05T12:10:20.294824Z","shell.execute_reply.started":"2023-02-05T12:10:20.164993Z","shell.execute_reply":"2023-02-05T12:10:20.29403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our Hypothesis is right, FoodCourt Feature have Outliers.","metadata":{}},{"cell_type":"markdown","source":"#### Checking Spa Column\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,24))\ntemp = num_data['Spa']\nx = pd.Series(temp,name='Spa Variable')\nplt.subplot(4,3,1)\nax = sns.distplot(temp,bins=10,color=my_colors[0])\nax.set_title(\"Distribution of Spa Variable\")\nplt.subplot(4,3,2)\nax = sns.kdeplot(x,shade=True,color=my_colors[8])\nax.set_title('Distribution of Spa Variable')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:20.295991Z","iopub.execute_input":"2023-02-05T12:10:20.296553Z","iopub.status.idle":"2023-02-05T12:10:20.796071Z","shell.execute_reply.started":"2023-02-05T12:10:20.296518Z","shell.execute_reply":"2023-02-05T12:10:20.794861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n* We see that Spa column is Not normalized and it is positively skewed.\n* we can say that it have Outliers.\n* To Confirm Our Hypothesis that Spa Column have outliers, we Use Boxplot to Confirm it","metadata":{}},{"cell_type":"markdown","source":"**Detect Outliers in** Spa Column **with boxplot**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nax = sns.boxplot(num_data['Spa'],color=my_colors[8])\nax.set_title('Visualizing Spa Column to detect Outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:20.797493Z","iopub.execute_input":"2023-02-05T12:10:20.797797Z","iopub.status.idle":"2023-02-05T12:10:20.915552Z","shell.execute_reply.started":"2023-02-05T12:10:20.797769Z","shell.execute_reply":"2023-02-05T12:10:20.914169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our Hypothesis is right, Spa Feature have Outliers.","metadata":{}},{"cell_type":"markdown","source":"#### Checking ShoppingMall Column","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,24))\ntemp = num_data['ShoppingMall']\nx = pd.Series(temp,name='ShoppingMall Variable')\nplt.subplot(4,3,1)\nax = sns.distplot(temp,bins=10,color=my_colors[0])\nax.set_title(\"Distribution of ShoppingMall Variable\")\nplt.subplot(4,3,2)\nax = sns.kdeplot(x,shade=True,color=my_colors[8])\nax.set_title('Distribution of ShoppingMall Variable')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:20.917094Z","iopub.execute_input":"2023-02-05T12:10:20.917468Z","iopub.status.idle":"2023-02-05T12:10:21.274793Z","shell.execute_reply.started":"2023-02-05T12:10:20.917436Z","shell.execute_reply":"2023-02-05T12:10:21.273436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n* We see that ShoppingMall column is Not normalized and it is positively skewed.\n* we can say that it have Outliers.\n* To Confirm Our Hypothesis that ShoppingMall Column have outliers, we Use Boxplot to Confirm it","metadata":{}},{"cell_type":"markdown","source":"**Detect Outliers in** ShoppingMall Column **with boxplot**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nax = sns.boxplot(num_data['ShoppingMall'],color=my_colors[9])\nax.set_title('Visualizing ShoppingMall Column to detect Outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:21.276147Z","iopub.execute_input":"2023-02-05T12:10:21.276483Z","iopub.status.idle":"2023-02-05T12:10:21.389242Z","shell.execute_reply.started":"2023-02-05T12:10:21.276451Z","shell.execute_reply":"2023-02-05T12:10:21.388413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our Hypothesis is right, ShoppingMall Feature have Outliers.","metadata":{}},{"cell_type":"markdown","source":"#### Checking VRDeck Column","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,24))\ntemp = num_data['VRDeck']\nx = pd.Series(temp,name='VRDeck Variable')\nplt.subplot(4,3,1)\nax = sns.distplot(temp,bins=10,color=my_colors[0])\nax.set_title(\"Distribution of VRDeck Variable\")\nplt.subplot(4,3,2)\nax = sns.kdeplot(x,shade=True,color=my_colors[4])\nax.set_title('Distribution of VRDeck Variable')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:21.390615Z","iopub.execute_input":"2023-02-05T12:10:21.391123Z","iopub.status.idle":"2023-02-05T12:10:21.758602Z","shell.execute_reply.started":"2023-02-05T12:10:21.391088Z","shell.execute_reply":"2023-02-05T12:10:21.757073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Detect Outliers in** VRDeck Column **with boxplot**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nax = sns.boxplot(num_data['VRDeck'],color=my_colors[9])\nax.set_title('Visualizing VRDeck Column to detect Outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:21.766901Z","iopub.execute_input":"2023-02-05T12:10:21.767754Z","iopub.status.idle":"2023-02-05T12:10:21.889668Z","shell.execute_reply.started":"2023-02-05T12:10:21.767691Z","shell.execute_reply":"2023-02-05T12:10:21.888413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our Hypothesis is right, VRDeck Feature have Outliers.","metadata":{}},{"cell_type":"markdown","source":"### Interpretation:\n\n* Except Age column, Other are positively Skewed and they are not normalized. \n* RoomService, FoodCourt, ShoppingMall, VRDeck : These Columns have outliers as we detect it from boxplot ","metadata":{}},{"cell_type":"markdown","source":"## **Explore More** ðŸ‘€","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\ntemp = num_data['Age']\nx = pd.Series(temp,name='Age Variable')\nax = sns.histplot(temp,bins=10,color=my_colors[3])\nax.set_title('Distribution of the Age',fontsize=18)\nstyle = \"Simple, tail_width=1, head_width=12, head_length=14\"\nkw = dict(arrowstyle=style, color=my_colors[9])\narrow = patches.FancyArrowPatch((45, 1600), (24,1100),\n                             connectionstyle=\"arc3,rad=-.10\", **kw)\nplt.gca().add_patch(arrow)\n\nplt.text(x=40, y=1700, s=f\"Most of the People are from 18-30\", \n         color=\"black\", size=14)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:21.891075Z","iopub.execute_input":"2023-02-05T12:10:21.891463Z","iopub.status.idle":"2023-02-05T12:10:22.15028Z","shell.execute_reply.started":"2023-02-05T12:10:21.891422Z","shell.execute_reply":"2023-02-05T12:10:22.147531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we clearly see that most of the people are from age 18-30.\n* Now, we can find patterns from Column \"Age\", we can make it more useful by dividing to into standard age group.\n* We divide the Age Column into four Groups => Children, Youth, Adult and Senior.","metadata":{}},{"cell_type":"code","source":"num_data['child'] = num_data['Age'].apply(lambda x: 1 if x>=1 and x<=14 else 0)\nnum_data['youth'] = num_data['Age'].apply(lambda x: 1 if x>=15 and x<=24 else 0)\nnum_data['adult'] = num_data['Age'].apply(lambda x: 1 if x>=25 and x<=64 else 0)\nnum_data['senior'] = num_data['Age'].apply(lambda x: 1 if x>=65 else 0)\nnum_data['Transported'] = num_data['Transported'].astype(int)\n\nprint(clr.S+\" === Checking Age Group === \"+clr.E,'\\n')\nprint(f\"There are : {sum(num_data['child'])} Childrens in Spaceship\")\nprint(f\"There are : {sum(num_data['youth'])} Youths in Spaceship\")\nprint(f\"There are : {sum(num_data['adult'])} Adult in Spaceship\")\nprint(f\"There are : {sum(num_data['senior'])} Senior in Spaceship\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:22.151763Z","iopub.execute_input":"2023-02-05T12:10:22.152103Z","iopub.status.idle":"2023-02-05T12:10:22.192675Z","shell.execute_reply.started":"2023-02-05T12:10:22.152071Z","shell.execute_reply":"2023-02-05T12:10:22.190831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking the Age Column More Deeply ","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(4,1,figsize=(6,16))\nsns.countplot(x='Transported',hue='child',data=num_data,ax=ax[0],palette=[my_colors[6],my_colors[9]])\nax[0].set_title('Child Group with Respect to Transported')\nsns.countplot(x='Transported',hue='youth',data=num_data,ax=ax[1],palette=[my_colors[4],my_colors[3]])\nax[1].set_title('Youth Group with Respect to Transported')\nsns.countplot(x='Transported',hue='adult',data=num_data,ax=ax[2],palette=[my_colors[2],my_colors[0]])\nax[2].set_title('Adult Group with Respect to Transported')\nsns.countplot(x='Transported',hue='senior',data=num_data,ax=ax[3],palette=[my_colors[0],my_colors[3]])\nax[3].set_title('Senior Group with Respect to Transported')\nplt.tight_layout(pad=3.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:22.194017Z","iopub.execute_input":"2023-02-05T12:10:22.194399Z","iopub.status.idle":"2023-02-05T12:10:22.703858Z","shell.execute_reply.started":"2023-02-05T12:10:22.194349Z","shell.execute_reply":"2023-02-05T12:10:22.702378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Most of the People who Transported are youth and adult age group people.\n* Basically, we find out that People who had more potential then others in terms of strength, they transported successfully.","metadata":{}},{"cell_type":"markdown","source":"#### Other Columns:\n\n* we add up the remaining column because as we said previously that they can be count as the Expense of the Passengers into the Spaceship.\n* Let's dive into these columns and find out insights from it âœŒ","metadata":{}},{"cell_type":"code","source":"# We Summed up all the expense columns into one column and named it total expense.\n\nnum_data['Total_Expense'] = num_data['RoomService'] + num_data['Spa'] + num_data['ShoppingMall'] + num_data['VRDeck'] + num_data['FoodCourt']\n\n# Now check the patterns behind it.\n\nplt.figure(figsize=(10,4))\nx=num_data['Total_Expense']\nax = sns.distplot(x,kde=True,bins=10,color=my_colors[0])\nax.set_title('Total Expense Distribution')\nstyle = 'Simple, tail_width=1, head_width=12, head_length=14'\nkw = dict(arrowstyle=style,color=my_colors[5])\narrow = patches.FancyArrowPatch((30000,0.00020),(15000,0.00010),connectionstyle='arc3,rad=-.10',**kw)\nplt.gca().add_patch(arrow)\nplt.text(x=15200, y=0.00025, s=f\"Expense More then this Threshold can be Summed with Outliers\", \n         color=\"black\", size=10)\nplt.axvline(x=15000,linestyle='--',color='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:22.70544Z","iopub.execute_input":"2023-02-05T12:10:22.705817Z","iopub.status.idle":"2023-02-05T12:10:22.950052Z","shell.execute_reply.started":"2023-02-05T12:10:22.705786Z","shell.execute_reply":"2023-02-05T12:10:22.948525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Now we Analyze Categorical Data</strong></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"cat_data = train_data.select_dtypes(exclude=['float']).copy() # Extracted the object type data\ncat_data.head() # checking the data","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:22.951589Z","iopub.execute_input":"2023-02-05T12:10:22.951934Z","iopub.status.idle":"2023-02-05T12:10:22.973064Z","shell.execute_reply.started":"2023-02-05T12:10:22.951901Z","shell.execute_reply":"2023-02-05T12:10:22.971556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary of Categorical Columns:\n\n\n* Numericals Columns : [PassengerId,HomePlanet,CryoSleep,Cabin,VIP,Name]\n\n* All Columns are Categorical datatype, (Categorical means Object type).\n","metadata":{}},{"cell_type":"markdown","source":"#### Checking Nans in Categorical Columns","metadata":{}},{"cell_type":"code","source":"cat_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:22.974453Z","iopub.execute_input":"2023-02-05T12:10:22.974792Z","iopub.status.idle":"2023-02-05T12:10:22.988804Z","shell.execute_reply.started":"2023-02-05T12:10:22.974759Z","shell.execute_reply":"2023-02-05T12:10:22.987659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Except PassengerId Column, we have Nan Columns\n* Now we check Cardinality of Every Categorical Column, (Cardinality means => Frequency Distribution)\n\n","metadata":{}},{"cell_type":"code","source":"# Frequency Distribution of the categorical column\n\ncat_data.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:22.990162Z","iopub.execute_input":"2023-02-05T12:10:22.990565Z","iopub.status.idle":"2023-02-05T12:10:23.009446Z","shell.execute_reply.started":"2023-02-05T12:10:22.990528Z","shell.execute_reply":"2023-02-05T12:10:23.007841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n\n\n* There are Some Columns with High Frequency distribution and some are low.\n* PassengerId, Name and Cabin Columns have High Frequency Distribution.\n* HomePlanet, CryoSleep, Destination, VIP have low cardinality and low Frequency Distribution\n","metadata":{}},{"cell_type":"markdown","source":"# **Explore More** ðŸ‘€","metadata":{}},{"cell_type":"code","source":"for col in cat_data.columns:\n    print(cat_data[col].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.011021Z","iopub.execute_input":"2023-02-05T12:10:23.01151Z","iopub.status.idle":"2023-02-05T12:10:23.041422Z","shell.execute_reply.started":"2023-02-05T12:10:23.011461Z","shell.execute_reply":"2023-02-05T12:10:23.039887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we see the data and can say that Name can not be useful for us because there is randomness in this column, so we can not find any patterns from it in this problem.\n\n* ### Let's explore other columns individually","metadata":{}},{"cell_type":"code","source":"# Checking the Transported column\n\ncat_data['Transported'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.043532Z","iopub.execute_input":"2023-02-05T12:10:23.044031Z","iopub.status.idle":"2023-02-05T12:10:23.05296Z","shell.execute_reply.started":"2023-02-05T12:10:23.043984Z","shell.execute_reply":"2023-02-05T12:10:23.051441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are only two outputs => True if it is Transported or False if it is not Transported.","metadata":{}},{"cell_type":"code","source":"# Frequency distribution of True and False in Transported Column\nprint(clr.S+\"Percentage of True and False\" + clr.E)\nprint(round(cat_data['Transported'].value_counts()/len(cat_data['Transported']),3))","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.054584Z","iopub.execute_input":"2023-02-05T12:10:23.054923Z","iopub.status.idle":"2023-02-05T12:10:23.069273Z","shell.execute_reply.started":"2023-02-05T12:10:23.054892Z","shell.execute_reply":"2023-02-05T12:10:23.067802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the Frequency distribution of Transported Column","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(12,6))\nax[0] = cat_data['Transported'].value_counts().plot(kind='pie',\n                                                    autopct='%.1f%%',\n                                                    ax=ax[0],\n                                                    colors=[my_colors[0],my_colors[4]],\n                                                   shadow=True)\nax[0].set_title('Transported Percentage')\n\nax[1] = sns.countplot(x='Transported',data=cat_data,palette=[my_colors[9],my_colors[6]])\nax[1].set_title('Transported Counts')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.070861Z","iopub.execute_input":"2023-02-05T12:10:23.071234Z","iopub.status.idle":"2023-02-05T12:10:23.285909Z","shell.execute_reply.started":"2023-02-05T12:10:23.071201Z","shell.execute_reply":"2023-02-05T12:10:23.284746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we detect that the Transported Column is balanced\n\nNow check **Homeplanet** Column with respect to Transported","metadata":{}},{"cell_type":"code","source":"f , ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Transported',hue='HomePlanet',data=cat_data,palette='Set1')\nax.set_title('Frequencey distribution of HomePlanet with respect to Transported')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.287308Z","iopub.execute_input":"2023-02-05T12:10:23.287621Z","iopub.status.idle":"2023-02-05T12:10:23.455445Z","shell.execute_reply.started":"2023-02-05T12:10:23.287595Z","shell.execute_reply":"2023-02-05T12:10:23.454411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n\n* Most of the Passenger in Spaceship belongs to Earth Homeplanet.\n* Most of the Transported Passengers are from Homeplanet, then Europa and at last Mars.\n* As we can conclude that, if a Passenger are from Earth Transported most. the reason behind it can be that People from Earth are the most nearest to the place to get Transported.\n","metadata":{}},{"cell_type":"markdown","source":"Now check **CryoSleep** Column with respect to Transported","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Transported',hue='CryoSleep',data=cat_data,palette = 'Set2')\nax.set_title('Frequency distribution of CryoSleep Column with respect to Transported',fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.456811Z","iopub.execute_input":"2023-02-05T12:10:23.457139Z","iopub.status.idle":"2023-02-05T12:10:23.621574Z","shell.execute_reply.started":"2023-02-05T12:10:23.457112Z","shell.execute_reply":"2023-02-05T12:10:23.620566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n\n* This is an Important Column because it interprets that whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n* If a Person is in CryoSleep then, it get the most chances to be Transported.\n* we concluded that people who were confined in their cabins are most likely to be Transported. the reason can be that they can be the Special People among Passengers and that's why most of the Passenger from CryoSleep Transported.\n\n\nNow check **Destination** Column with respect to Transported","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Transported',hue='Destination',data=cat_data,palette = 'Set2')\nax.set_title('Frequency distribution of Destination Column with respect to Transported',fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.622952Z","iopub.execute_input":"2023-02-05T12:10:23.623249Z","iopub.status.idle":"2023-02-05T12:10:23.805708Z","shell.execute_reply.started":"2023-02-05T12:10:23.623222Z","shell.execute_reply":"2023-02-05T12:10:23.804639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n\n* TRAPPIST-1e is the Destination where most Passengers wants to debark there.\n* as we conclude from the visualization that, most of the Passengers from TRAPPIST-1e Destination have Successfully Transported.\n* There is a huge difference we see between TRAPPIST-1e Destination and Other two Destination. so, we can say that maybe the Spaceship were mostly nearer to the Destination \"TRAPPIST-1e\"\n\n\nNow check **VIP** Column with respect to Transported","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Transported',hue='VIP',data=cat_data,palette = 'Set1')\nax.set_title('Frequency distribution of VIP Column with respect to Transported',fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.806944Z","iopub.execute_input":"2023-02-05T12:10:23.807277Z","iopub.status.idle":"2023-02-05T12:10:23.991951Z","shell.execute_reply.started":"2023-02-05T12:10:23.807246Z","shell.execute_reply":"2023-02-05T12:10:23.990783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n\n* As we clearly see that, Those Passengers who does not paid for VIP Services in their voyage, they likley to be Transported.\n* The Reason can be that we have very low percentage of Passengers who does paid for VIP Services, that's why they have low Probability to be Transported instead of opposite Position Passengers.\n","metadata":{}},{"cell_type":"markdown","source":"**We Checked All Low Cardinality Categorical Columns (Features). Now we take a look into the High Cardinality Categorical Columns**\n\n* Main Point: we will not Explore Name Column, as I defined on above cells.","metadata":{}},{"cell_type":"markdown","source":"### **Explore the Cabin Column**\n\n\n* As I Read the Description of the Problem Statement and analyzes the column Cabin.\n* I find out that Cabin Column can be broken into three more columns => Deck,Num,Side.\n* These are the Columns which can be useful for Detecting wheather a Passenger can be Transported or not\n","metadata":{}},{"cell_type":"code","source":"cat_data['Cabin'] = cat_data['Cabin'].fillna(cat_data['Cabin'].mode()[0]) \n\n# Filling this because we have some Nans in Cabin\n# we can not split it until we remove all the Nan from this column. so I filled it to Split this column into three new columns as I mentioned below\n\ncat_data['Deck'] = cat_data['Cabin'].apply(lambda x: x.split('/')[0]) # Deck Column\ncat_data['num'] = cat_data['Cabin'].apply(lambda x: x.split('/')[1])  # Num Column\ncat_data['side'] = cat_data['Cabin'].apply(lambda x: x.split('/')[2]) # Side Column\n\nprint(cat_data['Deck'].value_counts())\nprint(cat_data['num'].value_counts())\nprint(cat_data['side'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:23.993276Z","iopub.execute_input":"2023-02-05T12:10:23.993658Z","iopub.status.idle":"2023-02-05T12:10:24.025866Z","shell.execute_reply.started":"2023-02-05T12:10:23.993628Z","shell.execute_reply":"2023-02-05T12:10:24.024531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n\n* we have Low Cardinality Columns are Deck and Side, but Num Column did not have low Cardinality.\n* We First Explore the Deck and Side Column with respect to Transported.\n","metadata":{}},{"cell_type":"markdown","source":"Now check **Deck** Column with respect to Transported","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Deck',data=cat_data,palette='Set1')\nax.set_title('Frequency distribution of Deck')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:24.027146Z","iopub.execute_input":"2023-02-05T12:10:24.028382Z","iopub.status.idle":"2023-02-05T12:10:24.196409Z","shell.execute_reply.started":"2023-02-05T12:10:24.028261Z","shell.execute_reply":"2023-02-05T12:10:24.195332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n* As we see that Most Passengers are from Deck F and G.\n* Deck T have the lowest Count of Passengers","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Transported',hue='Deck',data=cat_data,palette='Set1')\nax.set_title('Frequency distribution of Deck with respect to Transported')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:24.197775Z","iopub.execute_input":"2023-02-05T12:10:24.198086Z","iopub.status.idle":"2023-02-05T12:10:24.612349Z","shell.execute_reply.started":"2023-02-05T12:10:24.198059Z","shell.execute_reply":"2023-02-05T12:10:24.611035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n\n* As we concluded that Deck G have the Most Powerful or Impactful column on wheather a Passenger Transported or not.\n* Deck F also have impact on Transported but as we see on above figure that Deck G have less frequency of Passengers then Deck F but it shows most Transported Deck.\n* So, we can conclude that Deck G and Deck F Passengers are Most Likely to be Transported.\n* Deck F and G can be the Position where Most of the Passengers belongs to Earth because Passengers from Earth Mostly had chances to be Transported.\n","metadata":{}},{"cell_type":"markdown","source":"### Let's explore the Column Deck with HomePlanet with respect to Transported.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='HomePlanet',hue='Deck',data=cat_data,palette='Set1')\nax.set_title('Frequency distribution of Deck with respect to HomePlanet')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:24.613711Z","iopub.execute_input":"2023-02-05T12:10:24.614026Z","iopub.status.idle":"2023-02-05T12:10:24.855301Z","shell.execute_reply.started":"2023-02-05T12:10:24.614Z","shell.execute_reply":"2023-02-05T12:10:24.853701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As Our Hypothesis were Right: Most of the Passengers from HomePlanet Earth were belongs to the Deck G.\n* Passengers from Earth HomePlanet + Deck G have the Highest Probability to be Transported.\n* These features will help our Machine Learning Model to detect easily wheather a Passenger Transported or not.\n\n","metadata":{}},{"cell_type":"markdown","source":"Now check **side** Column with respect to Transported","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,4))\nax = sns.countplot(x='side',data=cat_data,palette='Set3')\nax.set_title('Frequency distribution of side')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:24.857193Z","iopub.execute_input":"2023-02-05T12:10:24.85764Z","iopub.status.idle":"2023-02-05T12:10:24.97274Z","shell.execute_reply.started":"2023-02-05T12:10:24.857602Z","shell.execute_reply":"2023-02-05T12:10:24.971888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This Column is Balanced and there is not any Big difference in frequency distribution of Data of Side Column.\n* Side column have two values : S and P => S is Starboard and P is Port","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Transported',hue='side',data=cat_data,palette='Set2')\nax.set_title('Frequency distribution of side with respect to Transported')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:24.973893Z","iopub.execute_input":"2023-02-05T12:10:24.974194Z","iopub.status.idle":"2023-02-05T12:10:25.131851Z","shell.execute_reply.started":"2023-02-05T12:10:24.974168Z","shell.execute_reply":"2023-02-05T12:10:25.130936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Passengers from Starboard have much more Probability to be Transported.\n* Now Our Hypothesis is : we can check that if Passengers are from Side S and Deck G are most likely to be Transported.","metadata":{}},{"cell_type":"markdown","source":"### Let's Explore Side Column with Deck Column to Find Pattern.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(x='Deck',hue='side',data=cat_data,palette='Set1')\nax.set_title('Frequency distribution of Side with respect of Deck')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.133135Z","iopub.execute_input":"2023-02-05T12:10:25.133502Z","iopub.status.idle":"2023-02-05T12:10:25.350255Z","shell.execute_reply.started":"2023-02-05T12:10:25.133467Z","shell.execute_reply":"2023-02-05T12:10:25.349266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As our Hypothesis is Right: Most of the Passengers from Side S are from Deck G and Deck G Passengers are From HomePlanet Earth, which shows the highest probability to get Transported.\n\nNow check **PassengerId** Column with respect to Transported","metadata":{}},{"cell_type":"code","source":"# PassengerId is the merge of two Data => group Id and the Passenger Id on that group.\n# Passenger Id on different groups can be same.\n\ncat_data['group_id'] = cat_data['PassengerId'].apply(lambda x: x.split('_')[0])\ncat_data['No_of_family_members'] = cat_data['group_id'].map(cat_data['group_id'].value_counts())\n\nprint(cat_data['No_of_family_members'])","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.351552Z","iopub.execute_input":"2023-02-05T12:10:25.352751Z","iopub.status.idle":"2023-02-05T12:10:25.370741Z","shell.execute_reply.started":"2023-02-05T12:10:25.352697Z","shell.execute_reply":"2023-02-05T12:10:25.369389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n* No_of_Famliy_members is the New Column we Created it from PassengerId Column.\n* We created this column by first understanding the Description of the Spaceship Titanic Problem.\n* As the Problem Statement said that, Passengers belongs to some groups and often groups have Family Members most probably.\n* So, we just calculated the group id from Passenger_Id for every Passenger and then just calculated the frequency of group id to count them up to Find No of Family Members for Every Passenger.\n","metadata":{}},{"cell_type":"markdown","source":"## More Exploration","metadata":{}},{"cell_type":"markdown","source":"# Encoding Categorical Data\n\n\n* We can not load our Machine Learning Model with Categorical Data because ML Models can not understand String Data\n* We must use technique to encode the Categorical Columns. there are many Techniques : LabelEncoding, One-hot-Encoding, TargetMeanEncoding and many more.\n* We will Encode the Categorical Columns on the Basis of the Frequency distribution (Cardinality)\n* We Use One-hot-Encoding for low Cardinality Columns and will use LabelEncoding for High Cardinality Columns.\n* One-hot-Encoding: it is a type of encoding in which we create new columns with the values of that categorical column and assign 1 for yes and 0 for no. Like Column Side have two values S and P. it will create two columns S and P and then assign 1 and 0 in S and also Same for P.\n* LabelEncoding: it is used when we have high cardinality column because if we have column which have 2000 different unique values in string, then we can not use one-hot-encoding beacause it will create 2000 columns which will be harder for us. so, we use LabelEncoding which encode on that column in which we are working on. it will assign numbers to every unique string in that particular column.\n","metadata":{}},{"cell_type":"code","source":"# we use one-hot-Encoding because we have low cardinality columns and high cardinality columns\n# Like Name and PassengerId are dropped because Name Column is not Useful for us now and PassengerId is divided and we find \n# insightful column named No of family member.\n# So, we Use One-Hot-Encoding by using pandas library. pd.get_dummies() => function used to create dummy columns which fullfill our plan\n\n\nle = LabelEncoder()\n\nprint(pd.get_dummies(cat_data['Deck']))\nprint(le.fit_transform(cat_data['side']))\nprint(le.fit_transform(cat_data['HomePlanet']))\nprint(le.fit_transform(cat_data['Destination']))","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.372264Z","iopub.execute_input":"2023-02-05T12:10:25.372993Z","iopub.status.idle":"2023-02-05T12:10:25.396381Z","shell.execute_reply.started":"2023-02-05T12:10:25.372963Z","shell.execute_reply":"2023-02-05T12:10:25.394802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Preprocessing</strong></h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"* In Preprocessing, we fill the Nans and empty columns with some imputation techniques.\n* To find any column containe Nan value, use Function .isnull().sum().\n* Let's find out the Columns with Nan First.","metadata":{}},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.39823Z","iopub.execute_input":"2023-02-05T12:10:25.398775Z","iopub.status.idle":"2023-02-05T12:10:25.414097Z","shell.execute_reply.started":"2023-02-05T12:10:25.398731Z","shell.execute_reply":"2023-02-05T12:10:25.412195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation\n\n* Columns with Nans: HomePlanet, CryoSleep, Cabin, Destination, Age, VIP, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, Name.\n* numerical columns: Age,FoodCourt,ShoppingMall,Spa,VRDeck\n* categorical columns: HomePlanet,CryoSleep,Destination,VIP,Name,Cabin\n* we drop the Name Column because it will not help our model to detect wheather a Passenger is likely to be Transported or not.\n* As we previously find out that numerical columns except Age have outliers. So, we can not impute or fill the Nans with mean because outliers have huge values or maybe some irrelevent values which will give us wrong mean. So, we fill the Nans with median it will be save for us.\n* we fill the Categorical Columns with Simple Imputation technique => we fill the Nan with mode of that column. it will fill the Column with Most occurring value in that Particular column.","metadata":{}},{"cell_type":"markdown","source":"# **Let's Impute** ðŸ“","metadata":{}},{"cell_type":"code","source":"def preprocess(df,name):\n    \n    df.drop('Name',axis=1,inplace=True) # drop the Name Column because it is not useful.\n    \n    # Numerical Column Imputation: \n    df['Age'] = df['Age'].fillna(df['Age'].mean()) # Age column does not have outliers so impute it with mean\n    df['RoomService'] = df['RoomService'].fillna(df['RoomService'].median())\n    df['Spa'] = df['Spa'].fillna(df['Spa'].median())\n    df['FoodCourt'] = df['FoodCourt'].fillna(df['FoodCourt'].median())\n    df['ShoppingMall'] = df['ShoppingMall'].fillna(df['ShoppingMall'].median())\n    df['VRDeck'] = df['VRDeck'].fillna(df['VRDeck'].median())\n    \n    \n    # Categorical Column Imputation:\n    \n    df['Cabin'] = df['Cabin'].fillna(df['Cabin'].mode()[0]) \n    df['HomePlanet'] = df['HomePlanet'].fillna(df['HomePlanet'].mode()[0])\n    df['CryoSleep'] = df['CryoSleep'].fillna(df['CryoSleep'].mode()[0])\n    df['VIP'] = df['VIP'].fillna(df['VIP'].mode()[0])\n    df['Destination'] = df['Destination'].fillna(df['Destination'].mode()[0])\n    \n    # Removing Outliers:\n    if name == 'Train':\n        \n        df.drop(df[df['RoomService'] > 10000].index,axis=0,inplace=True)\n        df.drop(df[df['VRDeck'] > 20000].index,axis=0,inplace=True)\n        df.drop(df[df['Spa'] > 20000].index,axis=0,inplace=True)\n        df.drop(df[df['FoodCourt'] > 20000].index,axis=0,inplace=True)\n        df.drop(df[df['ShoppingMall'] > 20000].index,axis=0,inplace=True)\n\n    return df\n\n\ntrain_data = preprocess(train_data,'Train') # calling the function to preprocess the Train data \ntest_data = preprocess(test_data,'Test') # Same for Test data","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.41569Z","iopub.execute_input":"2023-02-05T12:10:25.416107Z","iopub.status.idle":"2023-02-05T12:10:25.462616Z","shell.execute_reply.started":"2023-02-05T12:10:25.416069Z","shell.execute_reply":"2023-02-05T12:10:25.46098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's check the Nan.","metadata":{}},{"cell_type":"code","source":"print(clr.S+\"Missing Train Data \"+clr.E,train_data.isnull().sum().sum())\nprint(clr.S+\"Missing Test Data \"+clr.E,test_data.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.464458Z","iopub.execute_input":"2023-02-05T12:10:25.464881Z","iopub.status.idle":"2023-02-05T12:10:25.477859Z","shell.execute_reply.started":"2023-02-05T12:10:25.464843Z","shell.execute_reply":"2023-02-05T12:10:25.476646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Feature Engineering</strong></h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### Interpretation\n\n* we create new features from existing features in feature engineering.\n* Some Columns Like: PassengerId, Cabin and Age, we can create new features from these mentioned columns.\n* I explained new feature creation from these above mentioned featuress on above Cells.\n\n### Now Let's dirty our hand and do Some Feature Engineering\n","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    \n    df['Deck'] = df['Cabin'].apply(lambda x: x.split('/')[0]) # Deck Column\n    df['num'] = df['Cabin'].apply(lambda x: x.split('/')[1])  # Num Column\n    df['side'] = df['Cabin'].apply(lambda x: x.split('/')[2]) # Side Column\n\n    # Age column can be divided into four Age groups.\n    \n    df['child'] = df['Age'].apply(lambda x: 1 if x>=1 and x<=14 else 0)\n    df['youth'] = df['Age'].apply(lambda x: 1 if x>=15 and x<=24 else 0)\n    df['adult'] = df['Age'].apply(lambda x: 1 if x>=25 and x<=64 else 0)\n    df['senior'] = df['Age'].apply(lambda x: 1 if x>=65 else 0)\n    \n    # we calculated the Total Expense from column RoomService, Spa, ShoppingMall, VRDeck and FoodCourt\n    df['Total_Expense'] = df['RoomService'] + df['Spa'] + df['ShoppingMall'] + df['VRDeck'] + df['FoodCourt']\n    \n    # we excluded the group_id column from PassengerId and calculated the no of family members from group_id\n    \n    df['group_id'] = df['PassengerId'].apply(lambda x: x.split('_')[0])\n    df['No_of_family_members'] = df['group_id'].map(df['group_id'].value_counts())\n    \n    # Label Encoding:\n    \n    le = LabelEncoder()\n    df['CryoSleep'] = le.fit_transform(df['CryoSleep'])\n    df['VIP'] = le.fit_transform(df['VIP'])\n    df['HomePlanet'] = le.fit_transform(df['HomePlanet'])\n    df['Destination'] = le.fit_transform(df['Destination'])\n    df['side'] = le.fit_transform(df['side'])\n    df['num'] = le.fit_transform(df['num'])\n    \n    # One-Hot-Encoding:\n    \n    dff = pd.get_dummies(df['Deck'])\n    dff['PassengerId'] = df['PassengerId']\n    df = pd.merge(df,dff,on='PassengerId')\n    \n    #df['Deck'] = le.fit_transform(df['Deck'])\n    \n    \n    \n    return df\n    \ntrain_data = feature_engineering(train_data)\ntest_data = feature_engineering(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.479248Z","iopub.execute_input":"2023-02-05T12:10:25.47958Z","iopub.status.idle":"2023-02-05T12:10:25.603232Z","shell.execute_reply.started":"2023-02-05T12:10:25.479553Z","shell.execute_reply":"2023-02-05T12:10:25.601764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Feature Selection</strong></h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"* In Feature Selection, we select important features between these all features of table.\n* A great Notebook is based on feature selection\n[Comprehensive Guide on Feature Selection](https://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection)","metadata":{}},{"cell_type":"markdown","source":"#### Correlation Technique:\n\n\n* First we remove those features which have correlation of 0.8 or greater than 0.8. 0.8 is the threshold or we can say that limit. you can set this on the basis of your data and problem","metadata":{}},{"cell_type":"code","source":"def remove_feature(df,thresh):\n    corr_col = set()\n    correlation = df.corr()\n    for i in range(len(correlation.columns)):\n        for j in range(i):\n            if abs(correlation.iloc[i,j]) > thresh:\n                column = correlation.columns[i]\n                corr_col.add(column)\n    return corr_col\n\ncorrelated_features = remove_feature(train_data,0.8)\nprint(f'Highly correlated Features : {correlated_features}')","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.604553Z","iopub.execute_input":"2023-02-05T12:10:25.604979Z","iopub.status.idle":"2023-02-05T12:10:25.638432Z","shell.execute_reply.started":"2023-02-05T12:10:25.604939Z","shell.execute_reply":"2023-02-05T12:10:25.636856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we find out that side_S Feature have correlation greater than 0.8 which can reduce the accuracy of our Model.\n* we will check our Model by dropping this Column.","metadata":{}},{"cell_type":"markdown","source":"* We now drop columns like PassengerId, Cabin, HomePlanet, Destination, group_id, Deck, side because we created New features from these feature.","metadata":{}},{"cell_type":"code","source":"\ntrain_data.drop(['PassengerId','Cabin','group_id','Deck','Age'],axis=1,inplace=True)\ntest_data.drop(['PassengerId','Cabin','group_id','Deck','Age'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.640233Z","iopub.execute_input":"2023-02-05T12:10:25.640668Z","iopub.status.idle":"2023-02-05T12:10:25.654825Z","shell.execute_reply.started":"2023-02-05T12:10:25.640632Z","shell.execute_reply":"2023-02-05T12:10:25.653112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the data after some updation.\n\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.66288Z","iopub.execute_input":"2023-02-05T12:10:25.663315Z","iopub.status.idle":"2023-02-05T12:10:25.689155Z","shell.execute_reply.started":"2023-02-05T12:10:25.663278Z","shell.execute_reply":"2023-02-05T12:10:25.687929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Scaling</strong></h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### Interpretation\n\n* we have different columns with different variance. Some columns have high variance and some of them have low variance.\n* So there is a technique of scaling the columns(features), it standardize the columns and variance will be same for all columns. why we are doing this because if we have high variance then our ML Model have the chance to get over fitted. \n* Overfitting means that our ML Model shows good accuracy on Train data and show very low accuracy on Test data.\n* So, we scale it using sklearn.preprocessing function name: StandardScaler. ","metadata":{}},{"cell_type":"code","source":"def Scaling(df,name):\n    \n    if name == 'train':\n        t_data = df.copy().drop('Transported',axis=1)\n        ss = StandardScaler()\n        ss.fit(t_data)\n        dff = pd.DataFrame(ss.transform(t_data),index=t_data.index,columns=t_data.columns)\n        dff['Transported'] = df['Transported']\n    \n    else:\n        ss = StandardScaler()\n        ss.fit(df)\n        dff = pd.DataFrame(ss.transform(df),index=df.index,columns=df.columns)\n        \n    return dff\n\ntrain = Scaling(train_data,'train')\ntest = Scaling(test_data,'test')","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.690327Z","iopub.execute_input":"2023-02-05T12:10:25.690693Z","iopub.status.idle":"2023-02-05T12:10:25.713612Z","shell.execute_reply.started":"2023-02-05T12:10:25.690661Z","shell.execute_reply":"2023-02-05T12:10:25.712225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['Transported']\ntrain.drop('Transported',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.715175Z","iopub.execute_input":"2023-02-05T12:10:25.715524Z","iopub.status.idle":"2023-02-05T12:10:25.721535Z","shell.execute_reply.started":"2023-02-05T12:10:25.715491Z","shell.execute_reply":"2023-02-05T12:10:25.720622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that the Variance of all columns are Same Now.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Modeling</strong></h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### Interpretation\n\n* our data is ready to fit on the Machine Learning Model.\n* First, we need to split the data into train and validate. we split the Training data into 70-30%. 70% of the data used for training and 30% for validation.\n* we try to Use Different ML Models to check the accuracy score. so, we will further use cross_validation technique if we get less accuracy on our models.","metadata":{}},{"cell_type":"markdown","source":"#### **Splitting of Data**","metadata":{}},{"cell_type":"code","source":"# split the data into 70-30 portion.\n\nx_train,x_test,y_train,y_test = train_test_split(train,y,test_size=0.3)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.722947Z","iopub.execute_input":"2023-02-05T12:10:25.723241Z","iopub.status.idle":"2023-02-05T12:10:25.738795Z","shell.execute_reply.started":"2023-02-05T12:10:25.723215Z","shell.execute_reply":"2023-02-05T12:10:25.737045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(clr.S+\"Training Size = \"+clr.E, len(x_train))\nprint(clr.S+\"Validation Size = \"+clr.E, len(x_test))","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.74058Z","iopub.execute_input":"2023-02-05T12:10:25.741019Z","iopub.status.idle":"2023-02-05T12:10:25.749531Z","shell.execute_reply.started":"2023-02-05T12:10:25.740979Z","shell.execute_reply":"2023-02-05T12:10:25.748094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Fitting the Model**","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=500,max_depth=7)\nmodel.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:25.751163Z","iopub.execute_input":"2023-02-05T12:10:25.751513Z","iopub.status.idle":"2023-02-05T12:10:28.382054Z","shell.execute_reply.started":"2023-02-05T12:10:25.751485Z","shell.execute_reply":"2023-02-05T12:10:28.380948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Validation**","metadata":{}},{"cell_type":"code","source":"model.score(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:28.383532Z","iopub.execute_input":"2023-02-05T12:10:28.384048Z","iopub.status.idle":"2023-02-05T12:10:28.552947Z","shell.execute_reply.started":"2023-02-05T12:10:28.384016Z","shell.execute_reply":"2023-02-05T12:10:28.551885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Overfitting","metadata":{}},{"cell_type":"code","source":"print(model.score(x_train,y_train))\nprint(model.score(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:28.554863Z","iopub.execute_input":"2023-02-05T12:10:28.555272Z","iopub.status.idle":"2023-02-05T12:10:29.029976Z","shell.execute_reply.started":"2023-02-05T12:10:28.555236Z","shell.execute_reply":"2023-02-05T12:10:29.028232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we see that the difference between train data and validation data accuracy is very low and their accuracy are similar. \n* Our Model is not overfitted.","metadata":{}},{"cell_type":"markdown","source":"### **Confusion Matrix For this Model**","metadata":{}},{"cell_type":"markdown","source":"* This will show that how many prediction is right and how much is not.","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(x_test)\n\n\n\nskplt.metrics.plot_confusion_matrix(\n    y_test, \n    y_pred,\n    figsize=(10,6))","metadata":{"execution":{"iopub.status.busy":"2023-02-05T12:10:29.031215Z","iopub.execute_input":"2023-02-05T12:10:29.031549Z","iopub.status.idle":"2023-02-05T12:10:29.417201Z","shell.execute_reply.started":"2023-02-05T12:10:29.031519Z","shell.execute_reply":"2023-02-05T12:10:29.415712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **You can Improve the Score using different kind of Models and HyperParameter tuning.**\n\n#### **I Hope this Notebook Helps you a Lot. Comment your feedback :)**\n#### **Happy Ending** âœŒ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}