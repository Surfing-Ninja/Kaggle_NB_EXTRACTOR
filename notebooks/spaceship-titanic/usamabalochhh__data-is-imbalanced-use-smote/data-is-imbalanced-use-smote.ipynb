{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***BALANCING THE DATA USING SMOTE*** âœ”","metadata":{}},{"cell_type":"markdown","source":"***IF you Liked my notebook, Upvote it!!. Also comment down your feedback or any question you want to ask.***","metadata":{}},{"cell_type":"markdown","source":"***Wheather we use Deep Learning Model or Machine Learning Model or any powerful model, if we have imbalanced data, we can not get good accuracy or targeted accuracy we want, the accuracy can be good but if we want best accuracy then we must tackle this problem.*** ","metadata":{}},{"cell_type":"markdown","source":"# **EXPLANATION:**\n\n1. *The challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.*\n2. *For Example in this dataset we have Target Column Named 'Failure' which contain only two values 0 and 1*\n3. *When I apply Machine Learning Model it outputed with accuracy of 1.0 in train data and 0.50 on test. so this is the case of overfitting.*\n4. *Overfitting Means: Your Model performs best in the train data but worst in test data.*\n5. *Now we want to tackle this kind of problem. so when I use function value_counts() it gave me an output which clearly define that it is imbalanced.*\n6. *It shows me like 0: 20921 and 1: 5649*\n7. *We try to oversample the minority classes like in this example 0.*\n8. *This kind of Approach is only done by SMOTE (Synthetic Minority Oversampling Technique).*\n","metadata":{}},{"cell_type":"markdown","source":"#### First, we need to know **what is SMOTE and How it works ?**\n\n\nSMOTE is an oversampling technique that generates synthetic samples from the minority class. It is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier. The SMOTE samples are linear combinations of two similar samples from the minority class (x and xR) and are defined as \n\n### s=x+uâ‹…(xRâˆ’x),\n\n\nwith 0â€‰â‰¤â€‰uâ€‰â‰¤â€‰1; xR is randomly chosen among the 5 minority class nearest neighbors of x.\n\n#### **SMOTE does not change the expected value of the (SMOTE-augmented) minority class and it decreases its variability**\n\nSMOTE samples have the same expected value as the original minority class samples (E(Xj^SMOTE)=E(Xj)\n), but smaller variance (var(Xj^SMOTE)=2/3 var(Xj)).","metadata":{}},{"cell_type":"markdown","source":"# **AFTER APPLYING THIS APPROACH TO MY DATA, MY MODEL IMPROVED** ðŸ˜Ž\n","metadata":{}},{"cell_type":"markdown","source":"#### **Import Libraries:**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2022-10-16T08:36:08.467838Z","iopub.execute_input":"2022-10-16T08:36:08.468373Z","iopub.status.idle":"2022-10-16T08:36:09.575252Z","shell.execute_reply.started":"2022-10-16T08:36:08.468258Z","shell.execute_reply":"2022-10-16T08:36:09.574316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv('../input/tabular-playground-series-aug-2022/train.csv')\ntrain_csv.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:31.658786Z","iopub.execute_input":"2022-10-13T15:12:31.659818Z","iopub.status.idle":"2022-10-13T15:12:31.885125Z","shell.execute_reply.started":"2022-10-13T15:12:31.659776Z","shell.execute_reply":"2022-10-13T15:12:31.883909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in train_csv.columns:\n    if train_csv.loc[:,col].isnull().sum() > 0:\n        train_csv.loc[:,col].fillna(train_csv.loc[:,col].median(),inplace=True)\n\ntrain_data = train_csv.copy()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:36.665005Z","iopub.execute_input":"2022-10-13T15:12:36.665876Z","iopub.status.idle":"2022-10-13T15:12:36.714829Z","shell.execute_reply.started":"2022-10-13T15:12:36.665825Z","shell.execute_reply":"2022-10-13T15:12:36.713605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Do Read this :***","metadata":{}},{"cell_type":"markdown","source":"***Like I said on above Explanation that we detected that 0 have too many samples but 1 have too low. so this is imbalancing. if Our Model train on this then it learns the 0 output too much because it contain too many 0, when we try to predict the output which has actual value 0 and it will predict output 0 but when we try to predict the output which has actual value 1 but becasue of too many learning on 0 it will predict that output 0 which is incorrect.*** ","metadata":{}},{"cell_type":"code","source":"train_csv.failure.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:40.409954Z","iopub.execute_input":"2022-10-13T15:12:40.410422Z","iopub.status.idle":"2022-10-13T15:12:40.42567Z","shell.execute_reply.started":"2022-10-13T15:12:40.410385Z","shell.execute_reply":"2022-10-13T15:12:40.424192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor col in train_csv.columns:\n    if train_csv.loc[:,col].dtype != 'object':\n        \n        first_quartile = train_csv[col].quantile(0.25)\n        third_quartile = train_csv[col].quantile(0.75)\n        \n        IQR = third_quartile - first_quartile \n        \n        out = third_quartile + 3*IQR\n       \n        train_csv.drop(train_csv[train_csv[col] > out].index,axis=0,inplace=True)\n\n        \n\n\ntrain_csv[['loading','measurement_17']] = np.log(train_csv[['loading','measurement_17']])\ntrain = train_csv.drop(['id','product_code','attribute_0','attribute_1','attribute_2','attribute_3'],axis=1)\ninput = train.drop('failure',axis=1)\ntarget=train.failure","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:41.765776Z","iopub.execute_input":"2022-10-13T15:12:41.766198Z","iopub.status.idle":"2022-10-13T15:12:41.972579Z","shell.execute_reply.started":"2022-10-13T15:12:41.766163Z","shell.execute_reply":"2022-10-13T15:12:41.97155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(input,target,test_size=0.15,random_state=34)\nss = StandardScaler()\nx_train = ss.fit_transform(x_train)\nx_test = ss.fit_transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:44.420477Z","iopub.execute_input":"2022-10-13T15:12:44.420896Z","iopub.status.idle":"2022-10-13T15:12:44.455565Z","shell.execute_reply.started":"2022-10-13T15:12:44.42086Z","shell.execute_reply":"2022-10-13T15:12:44.454512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***As you see it outputs with 100% accuracy but on test.csv it gave 0.5 which is not good according to out train accuracy. so we will apply SMOTE to balance it.***","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0.3,grow_policy='depthwise',\n              importance_type=None,\n              learning_rate=0.005, max_bin=25, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=5,n_estimators=50,\n              verbosity=1)\n\nmodel.fit(x_train,y_train)\nscore = cross_val_score(model,input,target,cv=10)\nprint(score)\nprint(model.score(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:46.860822Z","iopub.execute_input":"2022-10-13T15:12:46.861253Z","iopub.status.idle":"2022-10-13T15:12:50.677548Z","shell.execute_reply.started":"2022-10-13T15:12:46.861216Z","shell.execute_reply":"2022-10-13T15:12:50.676406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Now We Apply SMOTE Method to Balance the data, we increase the sample size of minority data in this problem 1 to the sample size of 0***","metadata":{}},{"cell_type":"code","source":"X = train_data.drop(['id','failure','product_code','attribute_0','attribute_1'],axis=1)\nY = train_data['failure']\n\nsm = SMOTE(k_neighbors=143)\nX_new,Y_new = sm.fit_resample(X,Y)\nX_new['failure'] = Y_new","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:50.679161Z","iopub.execute_input":"2022-10-13T15:12:50.679487Z","iopub.status.idle":"2022-10-13T15:12:51.651246Z","shell.execute_reply.started":"2022-10-13T15:12:50.679458Z","shell.execute_reply":"2022-10-13T15:12:51.650193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:51.692098Z","iopub.execute_input":"2022-10-13T15:12:51.692509Z","iopub.status.idle":"2022-10-13T15:12:51.700477Z","shell.execute_reply.started":"2022-10-13T15:12:51.692476Z","shell.execute_reply":"2022-10-13T15:12:51.699231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Balanced Data we Get***  ðŸ˜Ž","metadata":{}},{"cell_type":"code","source":"X_new.failure.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:53.568249Z","iopub.execute_input":"2022-10-13T15:12:53.568681Z","iopub.status.idle":"2022-10-13T15:12:53.577961Z","shell.execute_reply.started":"2022-10-13T15:12:53.568644Z","shell.execute_reply":"2022-10-13T15:12:53.577101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Same PreProcessing***","metadata":{}},{"cell_type":"code","source":"\nfor col in X_new.columns:\n    if X_new.loc[:,col].dtype != 'object':\n        \n        first_quartile = X_new[col].quantile(0.25)\n        third_quartile = X_new[col].quantile(0.75)\n        \n        IQR = third_quartile - first_quartile \n        \n        out = third_quartile + 3*IQR\n       \n        X_new.drop(X_new[X_new[col] > out].index,axis=0,inplace=True)\n\n        \n\n\nX_new[['loading','measurement_17']] = np.log(X_new[['loading','measurement_17']])\nX_new.drop(['attribute_2','attribute_3'],axis=1,inplace=True)\ninput = X_new.drop('failure',axis=1)\ntarget=X_new.failure","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:55.336075Z","iopub.execute_input":"2022-10-13T15:12:55.336707Z","iopub.status.idle":"2022-10-13T15:12:55.558554Z","shell.execute_reply.started":"2022-10-13T15:12:55.336661Z","shell.execute_reply":"2022-10-13T15:12:55.55727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(input,target,test_size=0.15,random_state=34)\nss = StandardScaler()\nx_train = ss.fit_transform(x_train)\nx_test = ss.fit_transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:12:58.723246Z","iopub.execute_input":"2022-10-13T15:12:58.723809Z","iopub.status.idle":"2022-10-13T15:12:58.764907Z","shell.execute_reply.started":"2022-10-13T15:12:58.723755Z","shell.execute_reply":"2022-10-13T15:12:58.763509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Now as you see the accuracy improved and now we easily say that this is the original accuracy we get not the above one. it also improved on test.csv data, it improves from 0.50 -> 0.559*** ðŸ”¥","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0.3,grow_policy='depthwise',\n              importance_type=None,\n              learning_rate=0.005, max_bin=25, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=5,n_estimators=50,\n              verbosity=1)\n\nmodel.fit(x_train,y_train)\nscore = cross_val_score(model,input,target,cv=10)\nprint(score)\nprint(model.score(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-10-13T15:13:00.571674Z","iopub.execute_input":"2022-10-13T15:13:00.57275Z","iopub.status.idle":"2022-10-13T15:13:26.173936Z","shell.execute_reply.started":"2022-10-13T15:13:00.572704Z","shell.execute_reply":"2022-10-13T15:13:26.17297Z"},"trusted":true},"execution_count":null,"outputs":[]}]}