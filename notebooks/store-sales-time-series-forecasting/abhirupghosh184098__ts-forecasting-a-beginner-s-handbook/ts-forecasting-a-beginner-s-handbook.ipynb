{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition: [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting)","metadata":{}},{"cell_type":"markdown","source":"## [Short Description](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/description) (and Disclaimer)\n\nThis is a â€œgetting startedâ€ competition, where we use time-series forecasting to forecast store sales on data from CorporaciÃ³n Favorita, a large Ecuadorian-based grocery retailer. I used the [Time Series course on Kaggle](https://www.kaggle.com/learn/time-series) to help me get started, and a lot of the code in this notebook is from that course, which in turn, are inspired by winning solutions from past Kaggle time series forecasting competitions.\n\n## [Evaluation](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/evaluation)\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nThe RMSLE is calculated as:\n$[\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}]$\nwhere:\n\nð‘› is the total number of instances,  \nð‘¦Ì‚ ð‘– is the predicted value of the target for instance (i),  \nð‘¦ð‘– is the actual value of the target for instance (i), and,  \nlog is the natural logarithm.\n\n## [Data](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data)\n\n### Training data: train.csv\n\n* The training data, comprising time series of features **store_nbr**, **family**, and **onpromotion** as well as the target **sales**.\n* **store_nbr** identifies the store at which the products are sold.\n* **family** identifies the type of product sold.\n* **sales** gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n* **onpromotion** gives the total number of items in a product family that were being promoted at a store at a given date.\n\n\n### Test data: test.csv\n\n* The test data, having the same features as the training data. You will predict the target **sales** for the dates in this file. \n* The dates in the test data are for the 15 days after the last date in the training data.\n\n\n### Submission file: sample_submission.csv\nA sample submission file in the correct format.\n\n### Additional information\n#### 1. Store metadata: stores.csv\n  * Store metadata, including **city**, **state**, **type**, and **cluster**.\n  * **cluster** is a grouping of similar stores.\n\n#### 2. Daily oil price: oil.csv \nIncludes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)\n\n#### 3. Holidays and Events, with metadata: holidays_events.csv\n  * NOTE: Pay special attention to the **transferred** column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n  * Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).\n\n\n### Additional Notes\n* Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n* A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-06T16:12:09.016525Z","iopub.execute_input":"2022-12-06T16:12:09.016957Z","iopub.status.idle":"2022-12-06T16:12:09.049147Z","shell.execute_reply.started":"2022-12-06T16:12:09.016869Z","shell.execute_reply":"2022-12-06T16:12:09.048179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preliminaries","metadata":{}},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.time_series.ex4 import *\nimport datetime\n\n# Setup notebook\nfrom pathlib import Path\nfrom learntools.time_series.style import *  # plot style settings\nfrom learntools.time_series.utils import (seasonal_plot,\n                                          plot_periodogram,\n                                          make_lags,\n                                          make_leads,\n                                          plot_lags,\n                                          make_multistep_target,\n                                          plot_multistep)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_log_error\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder\n\n# Model 1 (trend)\nfrom pyearth import Earth\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n\n# Model 2\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:09.051011Z","iopub.execute_input":"2022-12-06T16:12:09.05233Z","iopub.status.idle":"2022-12-06T16:12:18.011278Z","shell.execute_reply.started":"2022-12-06T16:12:09.052291Z","shell.execute_reply":"2022-12-06T16:12:18.010546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Competition Data","metadata":{}},{"cell_type":"code","source":"comp_dir = Path('../input/store-sales-time-series-forecasting')\n\n# Training data: train.csv\n\n# For the first part of the analysis (time-dependence), we are going to\n# use a restricted training data, using information about the store number,\n# family, date and the sales; we will use the rest of the training data\n# as we expand the analysis.\nstore_sales = pd.read_csv(\n    comp_dir / 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\n# Holidays and Events, with metadata\nholidays_events = pd.read_csv(\n    comp_dir / \"holidays_events.csv\",\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nholidays_events = holidays_events.set_index('date').to_period('D')\n\n# Test data: test.csv\ndf_test = pd.read_csv(\n    comp_dir / 'test.csv',\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\ndf_test['date'] = df_test.date.dt.to_period('D')\ndf_test = df_test.set_index(['store_nbr', 'family', 'date']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:18.012762Z","iopub.execute_input":"2022-12-06T16:12:18.013144Z","iopub.status.idle":"2022-12-06T16:12:20.624062Z","shell.execute_reply.started":"2022-12-06T16:12:18.013109Z","shell.execute_reply":"2022-12-06T16:12:20.623264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training Data\", \"\\n\" + \"-\" * 13 + \"\\n\", store_sales)\nprint(\"\\n\")\nprint(\"Test Data\", \"\\n\" + \"-\" * 9 + \"\\n\", df_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.626411Z","iopub.execute_input":"2022-12-06T16:12:20.626992Z","iopub.status.idle":"2022-12-06T16:12:20.645488Z","shell.execute_reply.started":"2022-12-06T16:12:20.626954Z","shell.execute_reply":"2022-12-06T16:12:20.644579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"store_sales.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.646856Z","iopub.execute_input":"2022-12-06T16:12:20.647225Z","iopub.status.idle":"2022-12-06T16:12:20.663571Z","shell.execute_reply.started":"2022-12-06T16:12:20.647193Z","shell.execute_reply":"2022-12-06T16:12:20.662839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Indices","metadata":{}},{"cell_type":"markdown","source":"#### Date","metadata":{}},{"cell_type":"code","source":"print(\"Total duration of data:\")\nprint(f\"    Training data: {store_sales.index.get_level_values(2).min()} -- {store_sales.index.get_level_values(2).max()}\")\nprint(f\"    Test data: {df_test.index.get_level_values(2).min()} -- {df_test.index.get_level_values(2).max()}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.664936Z","iopub.execute_input":"2022-12-06T16:12:20.665554Z","iopub.status.idle":"2022-12-06T16:12:20.704741Z","shell.execute_reply.started":"2022-12-06T16:12:20.665521Z","shell.execute_reply":"2022-12-06T16:12:20.703631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This amounts to slightly more than 4.5 years (55 months) of training data, followed by the next 15 days to test our model on.","metadata":{}},{"cell_type":"markdown","source":"#### Family\n\nThe products in the training data belong to different families.","metadata":{}},{"cell_type":"code","source":"print(f\"Total number of families: {len(store_sales.index.unique(level=1))}\")\n\nprint(f\"First 5 families (in alphabetical order): {[x.capitalize() for x in store_sales.index.unique(level=1)[:5]]}\")\n\nprint(f\"Last 5 families (in alphabetical order): {[x.capitalize() for x in store_sales.index.unique(level=1)[-5:]]}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.706377Z","iopub.execute_input":"2022-12-06T16:12:20.706799Z","iopub.status.idle":"2022-12-06T16:12:20.746912Z","shell.execute_reply.started":"2022-12-06T16:12:20.706742Z","shell.execute_reply":"2022-12-06T16:12:20.745895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stores","metadata":{}},{"cell_type":"code","source":"print(f\"Total number of stores: {len(store_sales.index.unique(level=0))}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.748483Z","iopub.execute_input":"2022-12-06T16:12:20.748856Z","iopub.status.idle":"2022-12-06T16:12:20.766755Z","shell.execute_reply.started":"2022-12-06T16:12:20.748824Z","shell.execute_reply":"2022-12-06T16:12:20.765842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TL;DR: Data Summary\n\nThere are 1782 time series in this data, distributed between 54 stores and 33 families of products.","metadata":{}},{"cell_type":"markdown","source":"# Modelling time dependence","metadata":{}},{"cell_type":"markdown","source":"As we learnt in the [Time Series course on Kaggle](https://www.kaggle.com/learn/time-series), there are 2 essential components of a time-series:\n* **Time dependence** that leads to two key featues: [Trends](https://www.kaggle.com/code/ryanholbrook/trend) and [Seasonality](https://www.kaggle.com/code/ryanholbrook/seasonality)\n* **Serial depencdence** that leads to [cycles](https://www.kaggle.com/code/ryanholbrook/time-series-as-features), and can be explored through lag features.\n\nEach of these features are worth exploring one-by-one in detail. We are goinig to follow the lead of the tutorial in realising that a single machine learning algorithm might not be best-suited to capture all these independent components. While regression algorithms are better for detrending/deseasonalising, [serial dependence is best explored through **decision trees**](https://www.kaggle.com/code/ryanholbrook/time-series-as-features). Hence, getting the best of both worlds, we are going to explore [Hybrid models](https://www.kaggle.com/code/ryanholbrook/hybrid-models) that employ a combination of the above algorithms.","metadata":{}},{"cell_type":"markdown","source":"## Trends","metadata":{}},{"cell_type":"markdown","source":"As defined in [Time Series course on Kaggle](https://www.kaggle.com/code/ryanholbrook/trend) course, the **trend** component of a time series represents a persistent, long-term change in the mean of the series. The trend is the slowest-moving part of a series, the part representing the largest time scale of importance.","metadata":{}},{"cell_type":"markdown","source":"The training set has information that drills down to different levels of detail, eg, store number, product family, etc. At the highest-level, we start looking at trends in the time-dependence of average sales.","metadata":{}},{"cell_type":"code","source":"average_sales = store_sales.groupby('date').mean()['sales']\naverage_sales.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.770737Z","iopub.execute_input":"2022-12-06T16:12:20.771977Z","iopub.status.idle":"2022-12-06T16:12:20.852209Z","shell.execute_reply.started":"2022-12-06T16:12:20.77193Z","shell.execute_reply":"2022-12-06T16:12:20.850873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating a year-long moving average smoothens out short-term fluctuations in the series retaining only long-term changes. With this, we see a year-on-year growth in the average sales.","metadata":{}},{"cell_type":"code","source":"# moving average plot of average_sales estimating the trend\n\ntrend = average_sales.rolling(\n    window=365,\n    center=True,\n    min_periods=183,\n).mean()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.853582Z","iopub.execute_input":"2022-12-06T16:12:20.854581Z","iopub.status.idle":"2022-12-06T16:12:20.860451Z","shell.execute_reply.started":"2022-12-06T16:12:20.854541Z","shell.execute_reply":"2022-12-06T16:12:20.859403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cover graphic","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\naverage_sales.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ax=ax)\ntrend.plot(linewidth=3, label='trend', color='r', ax=ax)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.grid(False)\nax.legend();\nax.set_xlabel(\"\")\nplt.savefig('thumbnail_graphic.png', dpi=300)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:20.86181Z","iopub.execute_input":"2022-12-06T16:12:20.862136Z","iopub.status.idle":"2022-12-06T16:12:21.942318Z","shell.execute_reply.started":"2022-12-06T16:12:20.86211Z","shell.execute_reply":"2022-12-06T16:12:21.941584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First prediction: based on trend\n\nAt this point, we attempt to make our first prediction, just using the trend of first, the average sales, and then each family at a time. As we have already seen in the previous plot, the average sales show a year-on-year growth which can be approximated using a linear regression. The polynomial we fit to the sales is our choice, and we will use orders 1 and 3 to see their effects on the prediction. We use the `DeterministicProcess` function to create a feature set for a trend model.","metadata":{}},{"cell_type":"markdown","source":"#### Polynomial order: 1","metadata":{}},{"cell_type":"code","source":"# the target\n\ny = average_sales.copy()\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:21.94323Z","iopub.execute_input":"2022-12-06T16:12:21.943516Z","iopub.status.idle":"2022-12-06T16:12:21.95128Z","shell.execute_reply.started":"2022-12-06T16:12:21.94349Z","shell.execute_reply":"2022-12-06T16:12:21.950376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate `DeterministicProcess` with arguments\n# appropriate for a cubic trend model\ndp = DeterministicProcess(\n    index=y.index,  # dates from the training data\n    constant=True,       # dummy feature for the bias (y_intercept)\n    order=1,             # the time dummy (trend): linear trend\n    drop=True,           # drop terms if necessary to avoid collinearity\n)\n\n# YOUR CODE HERE: Create the feature set for the dates given in y.index\nX = dp.in_sample()\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:21.952657Z","iopub.execute_input":"2022-12-06T16:12:21.953209Z","iopub.status.idle":"2022-12-06T16:12:21.976447Z","shell.execute_reply.started":"2022-12-06T16:12:21.953177Z","shell.execute_reply":"2022-12-06T16:12:21.975184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As is standard for evaluating machine learning models, we split our training data into a training set and a validation set. We use the training set to train the model and evaluate its performance on the validation set. In this case, we choose our validation set to be the same size as the actual test set (15 days), and we choose it to be the last 15 days of the original training set. This is to resemble the test set which is 15 days from the end of the training data. \n\n**[Evaluation](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/evaluation):** We use the same metric for evaluating performance on the validation set as will be used for the actual test set, namely, the Root Mean Squared Logarithmic Error, calculated as:\n\n$[\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}]$\n\nwhere:\n\nð‘› is the total number of instances,  \nð‘¦Ì‚ ð‘– is the predicted value of the target for instance (i),  \nð‘¦ð‘– is the actual value of the target for instance (i), and,  \nlog is the natural logarithm.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train)\ny_fit = pd.Series(model.predict(X_train), index=X_train.index).clip(0.0)\ny_pred = pd.Series(model.predict(X_valid), index=X_valid.index).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\nax = y.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_fit.plot(ax=ax, label=\"Fitted\", color='C0')\nax = y_pred.plot(ax=ax, label=\"Forecast\", color='C3')\nax.legend();","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:21.977566Z","iopub.execute_input":"2022-12-06T16:12:21.978101Z","iopub.status.idle":"2022-12-06T16:12:22.286637Z","shell.execute_reply.started":"2022-12-06T16:12:21.978069Z","shell.execute_reply":"2022-12-06T16:12:22.285727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Polynomial order: 3","metadata":{}},{"cell_type":"code","source":"# Instantiate `DeterministicProcess` with arguments\n# appropriate for a cubic trend model\ndp = DeterministicProcess(\n    index=y.index,  # dates from the training data\n    constant=True,       # dummy feature for the bias (y_intercept)\n    order=3,             # the time dummy (trend): cubic trend\n    drop=True,           # drop terms if necessary to avoid collinearity\n)\n\n# YOUR CODE HERE: Create the feature set for the dates given in y.index\nX = dp.in_sample()\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:22.287975Z","iopub.execute_input":"2022-12-06T16:12:22.288217Z","iopub.status.idle":"2022-12-06T16:12:22.307954Z","shell.execute_reply.started":"2022-12-06T16:12:22.288194Z","shell.execute_reply":"2022-12-06T16:12:22.307002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train)\ny_fit = pd.Series(model.predict(X_train), index=X_train.index).clip(0.0)\ny_pred = pd.Series(model.predict(X_valid), index=X_valid.index).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\nax = y.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_fit.plot(ax=ax, label=\"Fitted\", color='C0')\nax = y_pred.plot(ax=ax, label=\"Forecast\", color='C3')\nax.legend();","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:22.30953Z","iopub.execute_input":"2022-12-06T16:12:22.31005Z","iopub.status.idle":"2022-12-06T16:12:22.621562Z","shell.execute_reply.started":"2022-12-06T16:12:22.31002Z","shell.execute_reply":"2022-12-06T16:12:22.620112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the cubic polynomial performs about the same on the training data, but generalises better to the validation set. We will retain the cubic trend for the remainder of this analyses.","metadata":{}},{"cell_type":"markdown","source":"#### Average sales vs Per Item sales\n\nIn practise, each family of items can have its own trends; and the resulting trend is expected to be their combination. We use this information to redefine our target sales in terms of the data **and family**, instead of averaging over the latter as previously. From there on, we follow the exact procedure with `DeterministicProcess` as we did for the `average_sales` in order to compute the resultant cubic trend of each family; and evaluate our performance on the validation set.","metadata":{}},{"cell_type":"code","source":"y = store_sales.unstack(['store_nbr', 'family'])  # the target\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:22.623115Z","iopub.execute_input":"2022-12-06T16:12:22.623767Z","iopub.status.idle":"2022-12-06T16:12:23.491273Z","shell.execute_reply.started":"2022-12-06T16:12:22.623733Z","shell.execute_reply":"2022-12-06T16:12:23.489708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate `DeterministicProcess` with arguments\n# appropriate for a cubic trend model\ndp = DeterministicProcess(\n    index=y.index,  # dates from the training data\n    constant=True,       # dummy feature for the bias (y_intercept)\n    order=3,             # the time dummy (trend): cubic trend\n    drop=True,           # drop terms if necessary to avoid collinearity\n)\n\n# YOUR CODE HERE: Create the feature set for the dates given in y.index\nX = dp.in_sample()\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:23.492821Z","iopub.execute_input":"2022-12-06T16:12:23.493198Z","iopub.status.idle":"2022-12-06T16:12:23.512125Z","shell.execute_reply.started":"2022-12-06T16:12:23.493155Z","shell.execute_reply":"2022-12-06T16:12:23.511126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y_train.columns).clip(0.0)\ny_pred = pd.DataFrame(model.predict(X_valid), index=X_valid.index, columns=y_valid.columns).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\ny_pred.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:23.513245Z","iopub.execute_input":"2022-12-06T16:12:23.513531Z","iopub.status.idle":"2022-12-06T16:12:23.833893Z","shell.execute_reply.started":"2022-12-06T16:12:23.513507Z","shell.execute_reply":"2022-12-06T16:12:23.831786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model seems to generalise well to our validation set. We will use this model to make our first submission and set a baseline for future revisions of our model.","metadata":{}},{"cell_type":"markdown","source":"### First submission: based on trends","metadata":{}},{"cell_type":"code","source":"# Create features for test set\nX_test = dp.out_of_sample(steps=16)\nX_test.index.name = 'date'\n\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:23.83545Z","iopub.execute_input":"2022-12-06T16:12:23.835813Z","iopub.status.idle":"2022-12-06T16:12:23.851686Z","shell.execute_reply.started":"2022-12-06T16:12:23.835764Z","shell.execute_reply":"2022-12-06T16:12:23.849648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_submit = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\ny_submit = y_submit.stack(['store_nbr', 'family'])\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission_trend.csv', index=False)\n\ny_submit.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:23.853492Z","iopub.execute_input":"2022-12-06T16:12:23.853834Z","iopub.status.idle":"2022-12-06T16:12:24.332757Z","shell.execute_reply.started":"2022-12-06T16:12:23.853803Z","shell.execute_reply":"2022-12-06T16:12:24.331338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Public Score: 0.63608](https://www.kaggle.com/code/abhirupghosh184098/store-sales-time-series-forecasting?scriptVersionId=111635179)\n\nWith the above submission, which only uses trends to make a prediction, we were able to get a RMSLE score of 0.63608. We will slowly improve on the model in iterations.","metadata":{}},{"cell_type":"markdown","source":"#### Only 2017 data\n\nOne of the reasons we might be doing so much better in the validation set than the training set using just information about trends is that the training set has > 50 months of data points, while the validation and test sets are just a fortnight. It might be worth checking out if trends in the more recent past might be better at predicting the trends of the test period, so as to not be biased by trends which are > 4 years old.","metadata":{}},{"cell_type":"code","source":"# target sales only in 2017\n\ny = store_sales.unstack(['store_nbr', 'family']).loc[\"2017\"]\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:24.334544Z","iopub.execute_input":"2022-12-06T16:12:24.334963Z","iopub.status.idle":"2022-12-06T16:12:25.215592Z","shell.execute_reply.started":"2022-12-06T16:12:24.334926Z","shell.execute_reply":"2022-12-06T16:12:25.214493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate `DeterministicProcess` with arguments\n# appropriate for a cubic trend model\ndp = DeterministicProcess(\n    index=y.index,  # dates from the training data\n    constant=True,       # dummy feature for the bias (y_intercept)\n    order=3,             # the time dummy (trend): cubic trend\n    drop=True,           # drop terms if necessary to avoid collinearity\n)\n\n# YOUR CODE HERE: Create the feature set for the dates given in y.index\nX = dp.in_sample()\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:25.217205Z","iopub.execute_input":"2022-12-06T16:12:25.217542Z","iopub.status.idle":"2022-12-06T16:12:25.23446Z","shell.execute_reply.started":"2022-12-06T16:12:25.217512Z","shell.execute_reply":"2022-12-06T16:12:25.233206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y_train.columns).clip(0.0)\ny_pred = pd.DataFrame(model.predict(X_valid), index=X_valid.index, columns=y_valid.columns).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\ny_pred.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:25.242092Z","iopub.execute_input":"2022-12-06T16:12:25.243194Z","iopub.status.idle":"2022-12-06T16:12:25.450886Z","shell.execute_reply.started":"2022-12-06T16:12:25.243152Z","shell.execute_reply":"2022-12-06T16:12:25.449252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suddenly, our training and validation scores are a lot closer to each other. This comes in the form of a betterment of the training score and a worsening of the validation score. But what it really indicates is that the model generalises a lot better now. \n\n**We will now use this model to make predictions for the test score.**","metadata":{}},{"cell_type":"code","source":"# Create features for test set\nX_test = dp.out_of_sample(steps=16)\nX_test.index.name = 'date'\n\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:25.452472Z","iopub.execute_input":"2022-12-06T16:12:25.452867Z","iopub.status.idle":"2022-12-06T16:12:25.468941Z","shell.execute_reply.started":"2022-12-06T16:12:25.45283Z","shell.execute_reply":"2022-12-06T16:12:25.467574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_submit = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\ny_submit = y_submit.stack(['store_nbr', 'family'])\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission_trend_2017.csv', index=False)\n\ny_submit.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:25.470261Z","iopub.execute_input":"2022-12-06T16:12:25.470687Z","iopub.status.idle":"2022-12-06T16:12:25.826265Z","shell.execute_reply.started":"2022-12-06T16:12:25.470645Z","shell.execute_reply":"2022-12-06T16:12:25.825097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Public Score: 0.75026](https://www.kaggle.com/code/abhirupghosh184098/store-sales-time-series-forecasting?scriptVersionId=111635179)\n\nRestricting our data to just the 2017 data actually worsens our public score to 0.75026. However, our training, validation and test sets still remain comparable (0.62841 / 0.56709 / 0.75026). Hence we will continue restricting ourselves to the 2017 data, and turn to the next feature of the time-series: **Seasonality**.","metadata":{}},{"cell_type":"markdown","source":"## Seasonality","metadata":{}},{"cell_type":"markdown","source":"While trends represent long-term changes in the mean of the series, [seasonality](https://www.kaggle.com/code/ryanholbrook/seasonality) represents **regular, periodic changes in the mean of the series** caused by weekly, monthly, seasonal or yearly patterns of social behaviour. In this context a season can mean a week, month, year or an actual 'season' (e.g., Vivaldi's \"The Four Seasons\"). However, depending on the number of observations in a season, we might use **two distinct features** to model seasonality:\n\n* [Seasonal indicators](https://www.kaggle.com/code/ryanholbrook/seasonality#Seasonal-Plots-and-Seasonal-Indicators): For a season with few observations (eg, a weekly season of daily observations) seasonal differences in the level of the time series (eg, difference between daily observations in a week) can be represented through binary features, or more specifically, one-hot-encoded categorical features. These features are called **seasonal indicators** and can be represented through seasonal plots.\n* [Fourier features](https://www.kaggle.com/code/ryanholbrook/seasonality#Fourier-Features-and-the-Periodogram): Seasonal indicators create a feature for every unit of the period of the season. Hence, they have the tendency to blow up for long seasons, e.g., daily observations over a year. For such cases, we use **Fourier features**, pairs of sine and cosine curves, one pair for each potential frequency in the season starting with the longest, to capture the overall shape of the seasonal curve with just a few features. We can choose these features using a **periodogram** which tells us the strength of the frequencies in a time series.","metadata":{}},{"cell_type":"code","source":"X = average_sales.loc['2017'].to_frame()\n\n# days within a week\nX[\"day\"] = X.index.dayofweek  # the x-axis (freq)\nX[\"week\"] = X.index.week  # the seasonal period (period)\n\n# days within a year\nX[\"dayofyear\"] = X.index.dayofyear\nX[\"year\"] = X.index.year\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\nseasonal_plot(X, y=\"sales\", period=\"week\", freq=\"day\", ax=ax0)\nseasonal_plot(X, y=\"sales\", period=\"year\", freq=\"dayofyear\", ax=ax1);","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:25.827825Z","iopub.execute_input":"2022-12-06T16:12:25.828207Z","iopub.status.idle":"2022-12-06T16:12:26.626204Z","shell.execute_reply.started":"2022-12-06T16:12:25.828169Z","shell.execute_reply":"2022-12-06T16:12:26.62484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_periodogram(average_sales.loc['2017']);","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:26.627886Z","iopub.execute_input":"2022-12-06T16:12:26.628576Z","iopub.status.idle":"2022-12-06T16:12:26.99881Z","shell.execute_reply.started":"2022-12-06T16:12:26.628536Z","shell.execute_reply":"2022-12-06T16:12:26.998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The periodogram tells us the strength of the frequencies in a time series. Specifically, the value on the y-axis of the graph is (a ** 2 + b ** 2) / 2, where a and b are the coefficients of the sine and cosine at that frequency (as in the Fourier Components plot above). From left to right, the periodogram shows appreciating Fourier contributions starting from a monthly frequency. This is followed by a significant biweekly contribution. These two make sense in view of the notes to the Store Sales dataset which mentions that wages in the public sector are paid out biweekly, on the 15th and last day of the month -- a possible origin for these seasons.\n\nAlready after these two features, we get down to weekly features which can be described through seasonal indicators.","metadata":{}},{"cell_type":"markdown","source":"### Creating seasonal features (also including linear trend)","metadata":{}},{"cell_type":"code","source":"y = store_sales.unstack(['store_nbr', 'family']).loc[\"2017\"]\n\n# Fourier features\nfourier = CalendarFourier(freq='M', order=4) ## 2 pairs of sine/cosine curves to model monthly/biweekly seasonality\ndp = DeterministicProcess(\n    index=y.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # cubic trend\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\n\nX = dp.in_sample()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:26.999935Z","iopub.execute_input":"2022-12-06T16:12:27.000846Z","iopub.status.idle":"2022-12-06T16:12:27.869583Z","shell.execute_reply.started":"2022-12-06T16:12:27.000811Z","shell.execute_reply":"2022-12-06T16:12:27.868918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y_train.columns).clip(0.0)\ny_pred = pd.DataFrame(model.predict(X_valid), index=X_valid.index, columns=y_valid.columns).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\ny_pred.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:27.870743Z","iopub.execute_input":"2022-12-06T16:12:27.87101Z","iopub.status.idle":"2022-12-06T16:12:28.097082Z","shell.execute_reply.started":"2022-12-06T16:12:27.870986Z","shell.execute_reply":"2022-12-06T16:12:28.096117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Second submission: based on (linear) trends + seasonality","metadata":{}},{"cell_type":"code","source":"# Create features for test set\nX_test = dp.out_of_sample(steps=16)\nX_test.index.name = 'date'\n\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:28.098519Z","iopub.execute_input":"2022-12-06T16:12:28.099744Z","iopub.status.idle":"2022-12-06T16:12:28.127966Z","shell.execute_reply.started":"2022-12-06T16:12:28.099656Z","shell.execute_reply":"2022-12-06T16:12:28.126618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_submit = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\ny_submit = y_submit.stack(['store_nbr', 'family'])\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission_trend_seasonality_2017.csv', index=False)\n\ny_submit.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:28.129571Z","iopub.execute_input":"2022-12-06T16:12:28.129978Z","iopub.status.idle":"2022-12-06T16:12:28.528871Z","shell.execute_reply.started":"2022-12-06T16:12:28.129948Z","shell.execute_reply":"2022-12-06T16:12:28.527219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Public Score: 0.58780](https://www.kaggle.com/code/abhirupghosh184098/store-sales-forecasting-a-comprehensive-guide?scriptVersionId=111669039)\n\nWe achieve our best score yet by:\n* restricting to 2017 data\n* assuming a linear trend\n* incorporating seasonality through:\n  * setting day-of-the-week seasonal indicators\n  * fourier features to represent monthly/bi-weekly behaviour (primarily influenced by wage-timings)\n  \nBefore we close the chapter on time-dependence and move on to serial dependence, there is one last piece of information we can add: **information about holidays**.","metadata":{}},{"cell_type":"markdown","source":"### Quick digression: Deseasonalising/Detrending","metadata":{}},{"cell_type":"markdown","source":"As a quick check of the effectiveness of our seasonality modelling, we can do a **residual analysis**. A residual is what is left behind when we subtract from the data, predictions of our model. If we have done a good job with our model, we are left with a residual that is effectively zero (sans some random fluctuations).\n\nWe have several stores and families of products that we detrended/deseasonalised individually. However, for an easy visual representation of the residuals, we once again consider the average sales, this time for just our training set.","metadata":{}},{"cell_type":"code","source":"y_train_avg = y_train.stack(['store_nbr', 'family']).groupby('date').mean()['sales']\ny_fit_avg = y_fit.stack(['store_nbr', 'family']).groupby('date').mean()['sales']\ny_deseason_avg = y_train_avg - y_fit_avg","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:28.531801Z","iopub.execute_input":"2022-12-06T16:12:28.532251Z","iopub.status.idle":"2022-12-06T16:12:28.69981Z","shell.execute_reply.started":"2022-12-06T16:12:28.532216Z","shell.execute_reply":"2022-12-06T16:12:28.698084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = y_deseason_avg.plot(**plot_params, alpha=0.5, ylim=[-300,300], title='Residuals')\nax.axhline(y=0, ls='dashed', lw=2)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:28.701482Z","iopub.execute_input":"2022-12-06T16:12:28.701926Z","iopub.status.idle":"2022-12-06T16:12:28.980464Z","shell.execute_reply.started":"2022-12-06T16:12:28.7019Z","shell.execute_reply":"2022-12-06T16:12:28.979174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also plot the periodogram of the deseasonalised time series. The previously present monthly/bi-weekly Fourier features should now be absent, if they had been correctly taken care of; and we indeed find that.","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7))\nax1 = plot_periodogram(y_train_avg, ax=ax1)\nax1.set_title(\"Product Sales Frequency Components\")\nax2 = plot_periodogram(y_deseason_avg, ax=ax2);\nax2.set_title(\"Deseasonalised\");","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:28.98233Z","iopub.execute_input":"2022-12-06T16:12:28.982714Z","iopub.status.idle":"2022-12-06T16:12:29.64396Z","shell.execute_reply.started":"2022-12-06T16:12:28.98268Z","shell.execute_reply":"2022-12-06T16:12:29.642814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Holidays","metadata":{}},{"cell_type":"markdown","source":"We had already read-in the \"Holidays and Events\" dataset at the very beginning. This is what the dataset looks like:","metadata":{}},{"cell_type":"code","source":"holidays_events.info()\nholidays_events.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:29.645209Z","iopub.execute_input":"2022-12-06T16:12:29.645478Z","iopub.status.idle":"2022-12-06T16:12:29.673827Z","shell.execute_reply.started":"2022-12-06T16:12:29.645455Z","shell.execute_reply":"2022-12-06T16:12:29.672122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 350 entries between 2012-03-02 and 2017-12-26; however, this dataset can be significantly reduced based on prior information and intuition. We can restrict ourselves to:\n* holidays in 2017 that fall within our training (2017-01-01 : 2017-07-31) + validation (2017-08-01 : 2017-08-15) + test set (2017-08-16 : 2017-08-31)\n* national and regional holidays, ignoring local holidays (assuming a local holiday will have minimum impact on the average national sales)\n\nThis reduces the holidays_events dataset to following 14 holidays.","metadata":{}},{"cell_type":"code","source":"# National and regional holidays in the training set\nholidays = (\n    holidays_events\n    .query(\"locale in ['National', 'Regional']\")\n    .loc['2017':'2017-08-15', ['description']] ## restricting ourselves to the dates in the training set\n    .assign(description=lambda x: x.description.cat.remove_unused_categories()) ## remove categories which are not used\n)\n\nholidays","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:29.675622Z","iopub.execute_input":"2022-12-06T16:12:29.676112Z","iopub.status.idle":"2022-12-06T16:12:29.699754Z","shell.execute_reply.started":"2022-12-06T16:12:29.676075Z","shell.execute_reply":"2022-12-06T16:12:29.697938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From a plot of the deseasonalized (training) average sales, it appears these holidays could have some predictive power.","metadata":{}},{"cell_type":"code","source":"ax = y_deseason_avg.plot(**plot_params)\nplt.plot_date(holidays.index[:-2], y_deseason_avg[holidays.index[:-2]], color='C3') # the [:-2] is to remove the last 2 dates, 2017-08-10 and 2017-08-11 because they are in the validation set, and not the training set.\nax.set_title('National and Regional Holidays');","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:29.70118Z","iopub.execute_input":"2022-12-06T16:12:29.701474Z","iopub.status.idle":"2022-12-06T16:12:29.972531Z","shell.execute_reply.started":"2022-12-06T16:12:29.70145Z","shell.execute_reply":"2022-12-06T16:12:29.971049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating holiday features as seasonal indicators","metadata":{}},{"cell_type":"markdown","source":"Holidays can be treated as [seasonal indicators](https://www.kaggle.com/code/ryanholbrook/seasonality) through a one-hot-encoded categorical feature.","metadata":{}},{"cell_type":"code","source":"X_holidays = pd.get_dummies(holidays)\nX_holidays.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:29.974011Z","iopub.execute_input":"2022-12-06T16:12:29.974337Z","iopub.status.idle":"2022-12-06T16:12:29.991703Z","shell.execute_reply.started":"2022-12-06T16:12:29.974313Z","shell.execute_reply":"2022-12-06T16:12:29.990379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create one feature set `X2` by combining the `X_holidays` feature set and the trend/seasonality `X` feature set.","metadata":{}},{"cell_type":"code","source":"X2 = X.join(X_holidays, on='date').fillna(0.0)\nX2.head().T","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:29.993318Z","iopub.execute_input":"2022-12-06T16:12:29.993743Z","iopub.status.idle":"2022-12-06T16:12:30.021523Z","shell.execute_reply.started":"2022-12-06T16:12:29.993707Z","shell.execute_reply":"2022-12-06T16:12:30.0205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X2, y, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y_train.columns).clip(0.0)\ny_pred = pd.DataFrame(model.predict(X_valid), index=X_valid.index, columns=y_valid.columns).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\ny_pred.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:30.022983Z","iopub.execute_input":"2022-12-06T16:12:30.023324Z","iopub.status.idle":"2022-12-06T16:12:30.195305Z","shell.execute_reply.started":"2022-12-06T16:12:30.023292Z","shell.execute_reply":"2022-12-06T16:12:30.194618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We find our training and validation scores to be better through the inclusion of information about holidays. That makes sense since people's purchasing patterns are influenced by an upcoming holiday, or the holiday itself.","metadata":{}},{"cell_type":"markdown","source":"### Third submission: based on (linear) trends + seasonality + holidays","metadata":{}},{"cell_type":"code","source":"# Create features for test set\nX_test = dp.out_of_sample(steps=16)\nX_test.index.name = 'date'\n\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:30.196341Z","iopub.execute_input":"2022-12-06T16:12:30.196723Z","iopub.status.idle":"2022-12-06T16:12:30.224175Z","shell.execute_reply.started":"2022-12-06T16:12:30.196699Z","shell.execute_reply":"2022-12-06T16:12:30.22236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Include information about holidays in the feature set\nX2_test = X_test.join(X_holidays, on='date').fillna(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:30.225859Z","iopub.execute_input":"2022-12-06T16:12:30.226174Z","iopub.status.idle":"2022-12-06T16:12:30.235232Z","shell.execute_reply.started":"2022-12-06T16:12:30.226145Z","shell.execute_reply":"2022-12-06T16:12:30.234063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This step might look redundant because there are no holidays in our test range. However, there are a couple of points:\n* absence of information can be information itself, i.e., knowing there are no holidays will lead to the algorithm reflect a pattern of behaviour distinctly different from holiday behaviour\n* from a technical point of view, our model expects a feature set with the same number of features on which it was originally trained.","metadata":{}},{"cell_type":"code","source":"y_submit = pd.DataFrame(model.predict(X2_test), index=X2_test.index, columns=y.columns)\ny_submit = y_submit.stack(['store_nbr', 'family'])\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission_trend_seasonality_holidays_2017.csv', index=False)\n\ny_submit.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:30.237209Z","iopub.execute_input":"2022-12-06T16:12:30.238075Z","iopub.status.idle":"2022-12-06T16:12:30.765385Z","shell.execute_reply.started":"2022-12-06T16:12:30.238044Z","shell.execute_reply":"2022-12-06T16:12:30.764491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Public Score: 0.58717](https://www.kaggle.com/code/abhirupghosh184098/store-sales-forecasting-a-comprehensive-guide?scriptVersionId=111942913)\n\nThis is a marginal improvement over our score for trends + seasonality; but it is an improvement nonetheless, showing including information about holidays does have an impact.\n\nWith this submission we have exhausted what we want to include about time dependence in our model (for the time being). It is time to move onto the next significant feature about a time series, i.e., lags and serial dependence. However, at this point, we will note some points that we can come back and explore further:\n\n* degree of the polynomial for the Linear Regression\n* other Regression algorithms like Ridge and Lasso\n* including information about local holidays\n* ... (we will continue expanding this list as and when we come across more points of interest)","metadata":{}},{"cell_type":"markdown","source":"## Short Recap: What have we done up till this point?\n\n* Restricted ourselves to 2017 data\n* Considered the last 15 days of the training set as our validation set (same length as trainig set)\n* Calculated trend based on a polynomial of order 1\n* Deseasoned using monthly and biweekly Fourier features, and seasonal indicators for days of the week\n* Used seasonal indicators for holidays","metadata":{}},{"cell_type":"markdown","source":"## Some bookkeeping\n\nWe save our progress on the time-dependence feature set as:","metadata":{}},{"cell_type":"code","source":"X_time = X2\nX_time_test = X2_test","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:30.766718Z","iopub.execute_input":"2022-12-06T16:12:30.767042Z","iopub.status.idle":"2022-12-06T16:12:30.77104Z","shell.execute_reply.started":"2022-12-06T16:12:30.767018Z","shell.execute_reply":"2022-12-06T16:12:30.770028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling serial dependence","metadata":{}},{"cell_type":"markdown","source":"[Serial dependence](https://www.kaggle.com/code/ryanholbrook/time-series-as-features) are behaviours in a time series that are _time-independent_, i.e., they have less to do with a particular date of occurance, but more to do with what happened in the recent past; thus lending a sense of irregularity to the events. An example of serial dependent properties are [cycles](https://www.kaggle.com/code/ryanholbrook/time-series-as-features#Cycles).\n\nSerial dependence can be of two types:\n* Linear: where past and present observations are linearly related. Such linear serial dependence can be explored through [lag series/plots](https://www.kaggle.com/code/ryanholbrook/time-series-as-features#Lagged-Series-and-Lag-Plots) where **the lag features are chosen by calculating (partial) autocorrelation**. They can also be anticipated through [leading indicators](https://www.kaggle.com/code/ryanholbrook/time-series-as-features#Example---Flu-Trends), like online trends or promotions. \n\n\n* Non-linear: where past and present observations can not be related by a simple linear relationship, hence **we can't calculate lag features through (partial) autocorrelations.** Non-linear relationships like these can either be transformed to be linear or else learned by an appropriate algorithm (XGBoost) using more complicated measures like [mutual information](https://www.kaggle.com/code/ryanholbrook/mutual-information/).","metadata":{}},{"cell_type":"markdown","source":"## Data Digression: 'onpromotion' and \"school and office supplies\"","metadata":{}},{"cell_type":"markdown","source":"At this point of time, we reload our datasets to include more information that would be needed to study serial dependence, mainly information about leading indicators, like the 'onpromotion' column. In exploring serial dependence of our store sales, we again take our lead from the [tutorial](https://www.kaggle.com/code/abhirupghosh184098/exercise-time-series-as-features/) which points to the sale of one particular family of products, \"school and office supplies\" as showing cyclic behaviour in 2017. Hence, we use this family as an example to demonstrate serial dependence modelling.","metadata":{}},{"cell_type":"code","source":"store_sales = pd.read_csv(\n    comp_dir / 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n        'onpromotion': 'uint32', ## NEW FEATURE: To be introduced later\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean()\n    .unstack('family')\n    .loc['2017']\n)\n\nfamily_sales.head()\n\nsupply_sales = family_sales.loc(axis=1)[:, 'SCHOOL AND OFFICE SUPPLIES']\ny_supply_sales = supply_sales.loc[:, 'sales'].squeeze()\ny_supply_sales","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:30.77216Z","iopub.execute_input":"2022-12-06T16:12:30.772539Z","iopub.status.idle":"2022-12-06T16:12:33.566311Z","shell.execute_reply.started":"2022-12-06T16:12:30.772506Z","shell.execute_reply":"2022-12-06T16:12:33.56515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cycles","metadata":{}},{"cell_type":"markdown","source":"Trend and seasonality will both create serial dependence that shows up in correlograms and lag plots. To isolate any purely cyclic behavior, we'll start with the deseasonalised 'SCHOOL AND OFFICE SUPPLIES' time series. \n\nAt the end of our exercise in time-dependence, we had the following series: \n* the original targets:`y_train` and `y_valid`\n* the corresponding predictions: `y_fit` and `y_pred`\n\nFrom their residuals, we can pick out the family: 'SCHOOL AND OFFICE SUPPLIES'","metadata":{}},{"cell_type":"code","source":"y_resid_train = y_train - y_fit \ny_resid_valid = y_valid - y_pred\n\ny_resid = pd.concat([y_resid_train, y_resid_valid])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:33.567395Z","iopub.execute_input":"2022-12-06T16:12:33.568068Z","iopub.status.idle":"2022-12-06T16:12:33.578182Z","shell.execute_reply.started":"2022-12-06T16:12:33.568038Z","shell.execute_reply":"2022-12-06T16:12:33.576875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_resid_supply_sales = y_resid.stack(['store_nbr', 'family']).groupby(['family', 'date']).mean() .unstack('family').loc(axis=1)[:, 'SCHOOL AND OFFICE SUPPLIES'].loc[:, 'sales'].squeeze().rename('sales_deseason')\ny_resid_supply_sales","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:33.579502Z","iopub.execute_input":"2022-12-06T16:12:33.579833Z","iopub.status.idle":"2022-12-06T16:12:33.692625Z","shell.execute_reply.started":"2022-12-06T16:12:33.579801Z","shell.execute_reply":"2022-12-06T16:12:33.691276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = y_resid_supply_sales.plot(label='deseasonalized')\ny_supply_sales.plot(ax=ax, label='raw data')\nax.set_title(\"Sales of School and Office Supplies (deseasonalized)\");\nax.legend()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:33.694099Z","iopub.execute_input":"2022-12-06T16:12:33.694436Z","iopub.status.idle":"2022-12-06T16:12:34.017503Z","shell.execute_reply.started":"2022-12-06T16:12:33.694407Z","shell.execute_reply":"2022-12-06T16:12:34.016607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This behaviour becomes clearer when we perform a moving average. ","metadata":{}},{"cell_type":"code","source":"y_supply_sales_ma = y_supply_sales.rolling(\n    window=7,       # 7-day window\n    center=True,      # puts the average at the center of the window\n).mean() \n\n\n# Plot\nax = y_supply_sales_ma.plot()\nax.set_title(\"Seven-Day Moving Average\");","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:34.018655Z","iopub.execute_input":"2022-12-06T16:12:34.01944Z","iopub.status.idle":"2022-12-06T16:12:34.303122Z","shell.execute_reply.started":"2022-12-06T16:12:34.019412Z","shell.execute_reply":"2022-12-06T16:12:34.302095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lag series/plots: Calculating (partial) autocorrelations","metadata":{}},{"cell_type":"code","source":"plot_pacf(y_resid_supply_sales, lags=8);\nplot_lags(y_resid_supply_sales, lags=8, nrows=2);","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:34.304405Z","iopub.execute_input":"2022-12-06T16:12:34.305621Z","iopub.status.idle":"2022-12-06T16:12:35.622312Z","shell.execute_reply.started":"2022-12-06T16:12:34.305572Z","shell.execute_reply":"2022-12-06T16:12:35.621046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the autocorrelation plots and the partial autocorrelation calculations, it seems the first lag series is important (and also perhaps the 8th).","metadata":{}},{"cell_type":"markdown","source":"### Creating lag features","metadata":{}},{"cell_type":"code","source":"# Make features from `y_resid_supply_sales`\nX_lags = make_lags(y_resid_supply_sales, lags=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:35.623593Z","iopub.execute_input":"2022-12-06T16:12:35.623934Z","iopub.status.idle":"2022-12-06T16:12:35.630237Z","shell.execute_reply.started":"2022-12-06T16:12:35.623906Z","shell.execute_reply":"2022-12-06T16:12:35.629104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_supply_sales = pd.concat([X_time, X_lags], axis=1).dropna()\nprint(f\"Total features in our combined feature set: {len(X_supply_sales.columns)}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:35.631616Z","iopub.execute_input":"2022-12-06T16:12:35.631942Z","iopub.status.idle":"2022-12-06T16:12:35.649078Z","shell.execute_reply.started":"2022-12-06T16:12:35.631911Z","shell.execute_reply":"2022-12-06T16:12:35.647193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_supply_sales, X_supply_sales = y_supply_sales.align(X_supply_sales, join='inner')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:35.650516Z","iopub.execute_input":"2022-12-06T16:12:35.650855Z","iopub.status.idle":"2022-12-06T16:12:35.661282Z","shell.execute_reply.started":"2022-12-06T16:12:35.650822Z","shell.execute_reply":"2022-12-06T16:12:35.65986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_supply_sales, X_valid_supply_sales, y_train_supply_sales, y_valid_supply_sales = train_test_split(X_supply_sales, y_supply_sales, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train_supply_sales, y_train_supply_sales)\ny_fit_supply_sales = pd.Series(model.predict(X_train_supply_sales), index=X_train_supply_sales.index).clip(0.0)\ny_pred_supply_sales = pd.Series(model.predict(X_valid_supply_sales), index=X_valid_supply_sales.index).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train_supply_sales, y_fit_supply_sales) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid_supply_sales, y_pred_supply_sales) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\nax = y_supply_sales.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_fit_supply_sales.plot(ax=ax, label=\"Fitted\", color='C0')\nax = y_pred_supply_sales.plot(ax=ax, label=\"Forecast\", color='C3')\nax.legend();","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:35.66493Z","iopub.execute_input":"2022-12-06T16:12:35.665265Z","iopub.status.idle":"2022-12-06T16:12:36.041264Z","shell.execute_reply.started":"2022-12-06T16:12:35.665235Z","shell.execute_reply":"2022-12-06T16:12:36.039876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Leading indicator: 'onpromotion'","metadata":{}},{"cell_type":"markdown","source":"A leading indicator provides \"advance notice\" of changes in the target. For our purpose, we can use the 'onpromotion' series in the training data. We load it in the same way as the 'store_sales' dataset [here](https://www.kaggle.com/code/abhirupghosh184098/store-sales-forecasting-a-comprehensive-guide#Preliminaries); and restrict ourselves to 2017 data for just the 'SCHOOL AND OFFICE SUPPLIES' family; and dividing between training and validation promotions.","metadata":{}},{"cell_type":"code","source":"y_supply_onpromotion = supply_sales.loc[:, 'onpromotion'].squeeze()\ny_supply_onpromotion","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:36.042651Z","iopub.execute_input":"2022-12-06T16:12:36.043746Z","iopub.status.idle":"2022-12-06T16:12:36.053298Z","shell.execute_reply.started":"2022-12-06T16:12:36.043714Z","shell.execute_reply":"2022-12-06T16:12:36.052468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to understand the impact of promotional campaigns on sales, we shift the onpromotion series forward and backward in relation to the sales time series (by 3 steps each). We find a higher correlation on leading series, as is expected intuitively. A promotion's effect on sales is only forward in time.","metadata":{}},{"cell_type":"code","source":"# Drop days without promotions\nplot_lags(x=y_supply_onpromotion.loc[y_supply_onpromotion > 1], y=y_resid_supply_sales.loc[y_supply_onpromotion > 1], lags=3, leads=3, nrows=1);","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:36.054536Z","iopub.execute_input":"2022-12-06T16:12:36.054837Z","iopub.status.idle":"2022-12-06T16:12:36.983438Z","shell.execute_reply.started":"2022-12-06T16:12:36.05481Z","shell.execute_reply":"2022-12-06T16:12:36.982829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating leading indicator (onpromotion) features","metadata":{}},{"cell_type":"code","source":"X_promo = pd.concat([\n    make_lags(y_supply_onpromotion, lags=1),\n    y_supply_onpromotion,\n    make_leads(y_supply_onpromotion, leads=1),\n], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:36.984469Z","iopub.execute_input":"2022-12-06T16:12:36.984967Z","iopub.status.idle":"2022-12-06T16:12:36.99442Z","shell.execute_reply.started":"2022-12-06T16:12:36.98494Z","shell.execute_reply":"2022-12-06T16:12:36.992679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We combine our time and serial (lags and leading indicators) dependence feature sets into one.","metadata":{}},{"cell_type":"code","source":"X_supply_sales = pd.concat([X_time, X_lags, X_promo], axis=1).dropna()\nprint(f\"Total features in our combined feature set: {len(X_supply_sales.columns)}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:36.995947Z","iopub.execute_input":"2022-12-06T16:12:36.996397Z","iopub.status.idle":"2022-12-06T16:12:37.011815Z","shell.execute_reply.started":"2022-12-06T16:12:36.996363Z","shell.execute_reply":"2022-12-06T16:12:37.009844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_supply_sales, X_supply_sales = y_supply_sales.align(X_supply_sales, join='inner')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:37.014085Z","iopub.execute_input":"2022-12-06T16:12:37.014518Z","iopub.status.idle":"2022-12-06T16:12:37.022127Z","shell.execute_reply.started":"2022-12-06T16:12:37.014477Z","shell.execute_reply":"2022-12-06T16:12:37.020764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_supply_sales, X_valid_supply_sales, y_train_supply_sales, y_valid_supply_sales = train_test_split(X_supply_sales, y_supply_sales, test_size=15, shuffle=False)\n\nmodel = LinearRegression(fit_intercept=False).fit(X_train_supply_sales, y_train_supply_sales)\ny_fit_supply_sales = pd.Series(model.predict(X_train_supply_sales), index=X_train_supply_sales.index).clip(0.0)\ny_pred_supply_sales = pd.Series(model.predict(X_valid_supply_sales), index=X_valid_supply_sales.index).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train_supply_sales, y_fit_supply_sales) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid_supply_sales, y_pred_supply_sales) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n\nax = y_supply_sales.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_fit_supply_sales.plot(ax=ax, label=\"Fitted\", color='C0')\nax = y_pred_supply_sales.plot(ax=ax, label=\"Forecast\", color='C3')\nax.legend();","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:37.025813Z","iopub.execute_input":"2022-12-06T16:12:37.026147Z","iopub.status.idle":"2022-12-06T16:12:37.4017Z","shell.execute_reply.started":"2022-12-06T16:12:37.026121Z","shell.execute_reply":"2022-12-06T16:12:37.400318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Short Recap: What have we done up till this point?\n* found cyclic behaviour in deseasonalised data\n* created lag features based on (partial) autocorrelation\n* created feature based on leading indicator (onpromotion) and its lags/leads\n* combined them with time-dependence feature set to create one feature set\n* used a linear algorithm to make train and validate\n\n## Caveats:\nThe above analysis focussed on **one family** and **average sales across all stores**. In reality, our submission would require us to have sales for each family at each store, across all families and stores. Hence, we need to take a generalised way to predicting the stores sales.\n\n## Next steps\n* Combine time and serial dependence (linear/non-linear) using hybrid models\n* Extend to all families of products\n* Include information about store numbers\n* Forecast lag features for test set","metadata":{}},{"cell_type":"markdown","source":"## Temporary digression\n\nAt this point we digress to do two things:\n* explore how we can build hybrid models to separately explore time and serial dependence\n\nFor this next assumption, we are temporarily going to restrict ourselves to leading indicators (not its leads/lags) while modelling serial dependence and not include lag features. We will come back to this in the last segment where we make forecasts for machine learning following the tutorial [here](https://www.kaggle.com/code/ryanholbrook/forecasting-with-machine-learning).","metadata":{}},{"cell_type":"markdown","source":"# Combining Time and Serial dependence: Hybrid Models","metadata":{}},{"cell_type":"markdown","source":"An accurate prediction must incorporate accurate time-dependent and time-independent (serial-dependent) modelling. It often makes sense to break the predictions into two separate steps:\n* Step 1: where we model out the time-dependence through incorporating trends, seasonality and seasonal indicators like holidays\n* Step 2: look for serial dependence in the residuals\n\nStep 2 itself can be based on linear modelling (restricted to the calculation of partial autocorrelation functions) or non-linear through algorithms like XGBoost or KNeighbors. A nice outline of combining time and serial dependence modelling using residuals is provided [here](https://www.kaggle.com/code/ryanholbrook/hybrid-models/tutorial#Hybrid-Forecasting-with-Residuals). The basic schema includes:\nâ€‹\n```\n# 1. Train and predict with first model\nmodel_1.fit(X_train_1, y_train)\ny_pred_1 = model_1.predict(X_train)\nâ€‹\n# 2. Train and predict with second model on residuals\nmodel_2.fit(X_train_2, y_train - y_pred_1)\ny_pred_2 = model_2.predict(X_train_2)\nâ€‹\n# 3. Add to get overall predictions\ny_pred = y_pred_1 + y_pred_2\n```\n\n**The second model `model_2` that we apply on the residuals would determine whether we incorporate non-linear effects (using an algorithm like XGBoost) or restrict ourselves to linear effects (w/ an algorithm like LinearRegression) as above. For the hybrid model below, we use KNeighbors.**","metadata":{}},{"cell_type":"markdown","source":"## Important assumptions in this section\n\nWe have so far been following a pattern of building up our model one step at a time, and in that regard, each section has build on from the previous one. However, for this section, we are going to break from tradition in the following ways:\n* **For our serial-dependence features, we will only assume leading indicators, i.e., the 'onpromotion' column, and not make lag features.** We will come back to this in the last segment where we make forecasts for machine learning following the tutorial [here](https://www.kaggle.com/code/ryanholbrook/forecasting-with-machine-learning).\n* We will directly use a non-linear algorithm, KNeighbors for modeeling the serial dependence\n* The implementation of the hybrid model in this section also differs from the tutorial in that we **don't average out over store numbers, but retain them**, since we will need them for the submissions.","metadata":{}},{"cell_type":"markdown","source":"## Time-dependence\n\nWe had spent some time exploring time-dependence earlier in this analysis and at the end of the section, stored our entire feature set as: 'X_time' and 'X_time_test'. We load them back here:","metadata":{}},{"cell_type":"code","source":"# X_1: Time-dependence features\n\nX_1 = X_time # training set time-features\nX_1_test = X_time_test # test set time-features\n\n# Splitting between training and validation sets\n\nX_1_train, X_1_valid, y_train, y_valid = train_test_split(X_1, y, test_size=15, shuffle=False)\n\n# Model 1: Ridge()\n\nmodel_1 = Ridge().fit(X_1_train, y_train)\n\ny_fit_1 = pd.DataFrame(model_1.predict(X_1_train), index=X_1_train.index, columns=y_train.columns).clip(0.0)\ny_pred_1 = pd.DataFrame(model_1.predict(X_1_valid), index=X_1_valid.index, columns=y_valid.columns).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit_1) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred_1) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:37.403237Z","iopub.execute_input":"2022-12-06T16:12:37.403527Z","iopub.status.idle":"2022-12-06T16:12:37.555559Z","shell.execute_reply.started":"2022-12-06T16:12:37.403502Z","shell.execute_reply":"2022-12-06T16:12:37.554819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NOTE**: Throughout this section, we use the letters 1 and 2 to refer to steps 1 (time-dependence) and 2 (serial-dependence) above.","metadata":{}},{"cell_type":"markdown","source":"## Serial Dependence","metadata":{}},{"cell_type":"code","source":"# X_2: Features for serial dependence\n# onpromotion feature as a boolean variable\nX_2 = store_sales.unstack(['store_nbr', 'family']).loc[\"2017\"].loc[:, 'onpromotion']\n\n# Label encoding for seasonality\nX_2[\"day\"] = X_2.index.day  # values are day of the month\n\n# Splitting between training and validation sets\nX_2_train, X_2_valid, y_train, y_valid = train_test_split(X_2, y, test_size=15, shuffle=False)\n\n# Model 2: KNeighborsRegressor\nmodel_2 = KNeighborsRegressor().fit(X_2_train, y_train - y_fit_1)\n\ny_fit_2 = pd.DataFrame(model_2.predict(X_2_train), index=X_2_train.index, columns=y_train.columns).clip(0.0)\ny_pred_2 = pd.DataFrame(model_2.predict(X_2_valid), index=X_2_valid.index, columns=y_valid.columns).clip(0.0)\n\ny_fit = y_fit_1 + y_fit_2\ny_pred = y_pred_1 + y_pred_2\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:37.558923Z","iopub.execute_input":"2022-12-06T16:12:37.561031Z","iopub.status.idle":"2022-12-06T16:12:38.615036Z","shell.execute_reply.started":"2022-12-06T16:12:37.560997Z","shell.execute_reply":"2022-12-06T16:12:38.614267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"families = y.columns[0:6]\naxs = y.loc(axis=1)[families].plot(\n    subplots=True, sharex=True, figsize=(11, 9), **plot_params, alpha=0.5,\n)\n_ = y_fit.loc(axis=1)[families].plot(subplots=True, sharex=True, color='C0', ax=axs)\n_ = y_pred.loc(axis=1)[families].plot(subplots=True, sharex=True, color='C3', ax=axs)\nfor ax, family in zip(axs, families):\n    ax.legend([])\n    ax.set_ylabel(family)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:38.61844Z","iopub.execute_input":"2022-12-06T16:12:38.621087Z","iopub.status.idle":"2022-12-06T16:12:40.357681Z","shell.execute_reply.started":"2022-12-06T16:12:38.621042Z","shell.execute_reply":"2022-12-06T16:12:40.356802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making a BoostedHybrid class","metadata":{}},{"cell_type":"code","source":"class BoostedHybrid:\n    \n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None  # store column names from fit method\n        \ndef fit(self, X_1, X_2, y):\n    # Train model_1\n    self.model_1.fit(X_1, y)\n\n    # Make predictions\n    y_fit = pd.DataFrame(\n        self.model_1.predict(X_1), \n        index=X_1.index, columns=y.columns,\n    )\n\n    # Compute residuals\n    y_resid = y - y_fit\n\n    # Train model_2 on residuals\n    self.model_2.fit(X_2, y_resid)\n\n    # Save column names for predict method\n    self.y_columns = y.columns\n    # Save data for question checking\n    self.y_fit = y_fit\n    self.y_resid = y_resid\n\n\n# Add method to class\nBoostedHybrid.fit = fit\n\ndef predict(self, X_1, X_2):\n    # Predict with model_1\n    y_pred = pd.DataFrame(\n        self.model_1.predict(X_1), \n        index=X_1.index, columns=self.y_columns,\n    )\n\n    # Add model_2 predictions to model_1 predictions\n    y_pred += self.model_2.predict(X_2)\n\n    return y_pred\n\n\n# Add method to class\nBoostedHybrid.predict = predict","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:40.359435Z","iopub.execute_input":"2022-12-06T16:12:40.359867Z","iopub.status.idle":"2022-12-06T16:12:40.369618Z","shell.execute_reply.started":"2022-12-06T16:12:40.35983Z","shell.execute_reply":"2022-12-06T16:12:40.368353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BoostedHybrid(\n    model_1=Ridge(fit_intercept=False),\n    model_2=KNeighborsRegressor(),\n)\n\nmodel.fit(X_1_train, X_2_train, y_train)\ny_fit = model.predict(X_1_train, X_2_train).clip(0.0)\ny_pred = model.predict(X_1_valid, X_2_valid).clip(0.0)\n\nrmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:40.370878Z","iopub.execute_input":"2022-12-06T16:12:40.37121Z","iopub.status.idle":"2022-12-06T16:12:40.657655Z","shell.execute_reply.started":"2022-12-06T16:12:40.371181Z","shell.execute_reply":"2022-12-06T16:12:40.656888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fourth Submission: BoostedHybrid using linear time-dependence + non-linear (leading-indicator-only) serial dependence","metadata":{}},{"cell_type":"code","source":"# X_1_test: Time-dependence features: test set\n\n# X_2_test: Serial-dependence features: test set\nX_2_test = df_test.unstack(['store_nbr', 'family']).loc[\"2017\"].loc[:, 'onpromotion']\n\n# Label encoding for seasonality\nX_2_test[\"day\"] = X_2_test.index.day  # values are day of the month\n\n# making submission predictions\ny_submit = model.predict(X_1_test, X_2_test).clip(0.0)\ny_submit = pd.DataFrame(y_submit.stack(['store_nbr', 'family']))#.rename('sales'))\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission_hybrid.csv', index=False)\ny_submit","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:40.658952Z","iopub.execute_input":"2022-12-06T16:12:40.659435Z","iopub.status.idle":"2022-12-06T16:12:41.129366Z","shell.execute_reply.started":"2022-12-06T16:12:40.659407Z","shell.execute_reply":"2022-12-06T16:12:41.127453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Public Score: 0.59726](https://www.kaggle.com/code/abhirupghosh184098/ts-forecasting-a-beginner-s-handbook?scriptVersionId=112684473)\n\nThe performance is actually marginally worse that using just time-dependence. This is not shocking. The only information we have added to time-dependence is promotion information and used it to model the entire residuals. However, the positive point is that we now have a hybrid model and we have extended it include store information as well in our predictions.\n\nWe will now move onto our last section where we will:\n* create a complete X_2 feature set for all families and store numbers using a complete list of lag/lead features (+ leading indicators) + categorical day of week\n* create a X_2_test features to be able to forecast using these lag features\n* make one last prediction","metadata":{}},{"cell_type":"markdown","source":"# Forecast using Machine Learning","metadata":{}},{"cell_type":"markdown","source":"This last section of the analysis aims at bringing together all the concepts we have accummulated so far (modelling time-dependence through trends, seasonality and indicators, and serial dependence through lag series and leading indicators; exploring linear and non-linear effects in the data through a multi-staged residuals analysis, and so on) in making a forecast for the Grocery-sales dataset. This section derives heavily from two pieces of work:\n\n* Lessons [5](https://www.kaggle.com/code/ryanholbrook/hybrid-models) and [6](https://www.kaggle.com/code/ryanholbrook/forecasting-with-machine-learning) from Ryan Holbrook's [time series course](https://www.kaggle.com/learn/time-series).\n* [FilterJoe](https://www.kaggle.com/filterjoe)'s [(unofficial) Time Series Bonus Lesson](https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial)\n\nAs [FilterJoe](https://www.kaggle.com/filterjoe) mentions at the very beginning of his notebook:\n\n> \"Implementing Time Series lessons 5 and 6 on the competition data set is hard, as many of you have discovered. So hard, that few of us have done it. A bonus lesson could be helpful. This is that bonus lesson.\"\n\nSo a special shoutout to him for this valuable resource.","metadata":{}},{"cell_type":"markdown","source":"Also, by this point, this notebook is quite long, and it would make sense to do some of things we have done before again, just so that we don't need to keep referring back to them from earlier sections.","metadata":{}},{"cell_type":"markdown","source":"## Store sales data (again)","metadata":{}},{"cell_type":"code","source":"store_sales = pd.read_csv(\n    comp_dir / 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n        'onpromotion': 'uint32', ## NEW FEATURE: To be introduced later\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:41.131066Z","iopub.execute_input":"2022-12-06T16:12:41.131441Z","iopub.status.idle":"2022-12-06T16:12:43.628977Z","shell.execute_reply.started":"2022-12-06T16:12:41.131409Z","shell.execute_reply":"2022-12-06T16:12:43.627005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training/Validation Range (again)","metadata":{}},{"cell_type":"markdown","source":"* Training range: 2017-01-01 -- 2017-07-31\n* Validation range: 2017-07-31 -- 2017-08-15\n* Test range: 2017-08-16 -- 2017-08-31","metadata":{}},{"cell_type":"code","source":"store_sales_train = store_sales.unstack(['store_nbr', 'family']).loc['2017':'2017-07-31']\nstore_sales_valid = store_sales.unstack(['store_nbr', 'family']).loc['2017-08-01':]","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:43.639403Z","iopub.execute_input":"2022-12-06T16:12:43.639756Z","iopub.status.idle":"2022-12-06T16:12:45.373161Z","shell.execute_reply.started":"2022-12-06T16:12:43.63973Z","shell.execute_reply":"2022-12-06T16:12:45.371928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target: sales","metadata":{}},{"cell_type":"code","source":"y_train = store_sales_train.loc[:, 'sales']\ny_val = store_sales_valid.loc[:, 'sales']","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.37468Z","iopub.execute_input":"2022-12-06T16:12:45.37513Z","iopub.status.idle":"2022-12-06T16:12:45.396029Z","shell.execute_reply.started":"2022-12-06T16:12:45.375093Z","shell.execute_reply":"2022-12-06T16:12:45.394163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hybrid model (again)","metadata":{}},{"cell_type":"code","source":"# Credit: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial\n\nclass BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None\n        self.stack_cols = None\n        self.y_resid = None\n\n    def fit1(self, X_1, y, stack_cols=None):\n        self.model_1.fit(X_1, y) # train model 1\n        y_fit = pd.DataFrame(\n            self.model_1.predict(X_1), # predict from model 1\n            index=X_1.index,\n            columns=y.columns,\n        )\n        self.y_resid = y - y_fit # residuals from model 1, which X2 may want to access to create lag (or other) features\n        self.y_resid = self.y_resid.stack(stack_cols).squeeze()  # wide to long\n        \n    def fit2(self, X_2, first_n_rows_to_ignore, stack_cols=None):\n        self.model_2.fit(X_2.iloc[first_n_rows_to_ignore*1782: , :], self.y_resid.iloc[first_n_rows_to_ignore*1782:]) # Train model_2\n        self.y_columns = y.columns # Save for predict method\n        self.stack_cols = stack_cols # Save for predict method\n\n    def predict(self, X_1, X_2, first_n_rows_to_ignore):\n        y_pred = pd.DataFrame(\n            self.model_1.predict(X_1.iloc[first_n_rows_to_ignore: , :]),\n            index=X_1.iloc[first_n_rows_to_ignore: , :].index,\n            columns=self.y_columns,\n        )\n        y_pred = y_pred.stack(self.stack_cols).squeeze()  # wide to long\n#         display(X_2.iloc[first_n_rows_to_ignore*1782: , :]) # uncomment when debugging\n        y_pred += self.model_2.predict(X_2.iloc[first_n_rows_to_ignore*1782: , :]) # Add model_2 predictions to model_1 predictions\n        return y_pred.unstack(self.stack_cols)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.397946Z","iopub.execute_input":"2022-12-06T16:12:45.398411Z","iopub.status.idle":"2022-12-06T16:12:45.410643Z","shell.execute_reply.started":"2022-12-06T16:12:45.398369Z","shell.execute_reply":"2022-12-06T16:12:45.409185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining our models","metadata":{}},{"cell_type":"code","source":"mod_1 = LinearRegression() # for time-dependence\nmod_2 = XGBRegressor() # for serial-dependence\n\nmodel = BoostedHybrid(model_1=mod_1, model_2=mod_2)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.41209Z","iopub.execute_input":"2022-12-06T16:12:45.412488Z","iopub.status.idle":"2022-12-06T16:12:45.427087Z","shell.execute_reply.started":"2022-12-06T16:12:45.412449Z","shell.execute_reply":"2022-12-06T16:12:45.425742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time-dependence (again)","metadata":{}},{"cell_type":"markdown","source":"### Creating X_1_train features","metadata":{}},{"cell_type":"code","source":"# Trends/Seasonality\n# Fourier features\nfourier = CalendarFourier(freq='M', order=4) ## 2 pairs of sine/cosine curves to model monthly/biweekly seasonality\ndp = DeterministicProcess(\n    index=y_train.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # cubic trend\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\n\nX_1_train = dp.in_sample()\n\n# Seasonal indicators: Holidays\nX_1_train = X_1_train.join(X_holidays, on='date').fillna(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.428666Z","iopub.execute_input":"2022-12-06T16:12:45.429075Z","iopub.status.idle":"2022-12-06T16:12:45.451393Z","shell.execute_reply.started":"2022-12-06T16:12:45.429042Z","shell.execute_reply":"2022-12-06T16:12:45.450033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training time-dependence with X_1_train features","metadata":{}},{"cell_type":"code","source":"model.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family'])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.453013Z","iopub.execute_input":"2022-12-06T16:12:45.45356Z","iopub.status.idle":"2022-12-06T16:12:45.571697Z","shell.execute_reply.started":"2022-12-06T16:12:45.453529Z","shell.execute_reply":"2022-12-06T16:12:45.570821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Complete X_2_train (serial dependence features)","metadata":{}},{"cell_type":"markdown","source":"### Creating features for leading indicator: 'onpromotion'","metadata":{}},{"cell_type":"markdown","source":"As we had noted earlier, the information already provided to us to anticipate and predict cycles is the **leading indicator 'onpromotion'**. Hence, we start with isolating that information.","metadata":{}},{"cell_type":"code","source":"on_promotions = store_sales_train.drop('sales', axis=1).stack(['store_nbr', 'family'])\non_promotions","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.572894Z","iopub.execute_input":"2022-12-06T16:12:45.573215Z","iopub.status.idle":"2022-12-06T16:12:45.694597Z","shell.execute_reply.started":"2022-12-06T16:12:45.573187Z","shell.execute_reply":"2022-12-06T16:12:45.693003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Source: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial\n\n# create feature set X2 for hybrid model 2, including helper functions\n\n# as lesson 5 suggests, X2 features can be anything,\n# allowing an algorithm weak on trends but strong on detecting relationships among variables\n# to further refine the modeling and forecasting\n\ndef encode_categoricals(df, columns):\n    le = LabelEncoder()  # from sklearn.preprocessing\n    for col in columns:\n        df[col] = le.fit_transform(df[col])\n    return df\n\ndef make_X2_lags(ts, lags, lead_time=1, name='y', stack_cols=None):\n    ts = ts.unstack(stack_cols)\n    df = pd.concat(\n        {\n            f'{name}_lag_{i}': ts.shift(i, freq=\"D\") # freq adds i extra day(s) to end: only one extra day is needed so rest will be dropped\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n    df = df.stack(stack_cols).reset_index()\n    df = encode_categoricals(df, stack_cols)\n    df = df.set_index('date').sort_values(by=stack_cols) # return sorted so can correctly compute rolling means (if desired)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.696522Z","iopub.execute_input":"2022-12-06T16:12:45.696927Z","iopub.status.idle":"2022-12-06T16:12:45.705995Z","shell.execute_reply.started":"2022-12-06T16:12:45.696894Z","shell.execute_reply":"2022-12-06T16:12:45.703931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Source: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial\n\n# promo_lag features\nshifted_promo_df = make_X2_lags(on_promotions.squeeze(), lags=2, name='promo', stack_cols=['store_nbr', 'family'])\nshifted_promo_df['promo_mean_rolling_7'] = shifted_promo_df['promo_lag_1'].rolling(window=7, center=False).mean()\nshifted_promo_df['promo_median_rolling_91'] = shifted_promo_df['promo_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\nshifted_promo_df['promo_median_rolling_162'] = shifted_promo_df['promo_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\n# for rolling window medians, backfilling seems reasonable as medians shouldn't change too much. Trying min_periods produced wacky (buggy?) results\nshifted_promo_df","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:45.709325Z","iopub.execute_input":"2022-12-06T16:12:45.709701Z","iopub.status.idle":"2022-12-06T16:12:46.443963Z","shell.execute_reply.started":"2022-12-06T16:12:45.709668Z","shell.execute_reply":"2022-12-06T16:12:46.442204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating features for lag series","metadata":{}},{"cell_type":"markdown","source":"For this, we need the residuals. This is because, when we explore serial dependence, we first factor out time-dependence and look for non-linear effects in the residuals.","metadata":{}},{"cell_type":"code","source":"y_resid_1 = model.y_resid\ny_resid_1","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:46.446595Z","iopub.execute_input":"2022-12-06T16:12:46.447102Z","iopub.status.idle":"2022-12-06T16:12:46.46393Z","shell.execute_reply.started":"2022-12-06T16:12:46.44706Z","shell.execute_reply":"2022-12-06T16:12:46.462719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Source: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial\n\n# y_lag features\nshifted_y_df = make_X2_lags(y_resid_1, lags=2, name='y_res', stack_cols=['store_nbr', 'family'])\nshifted_y_df['y_mean_rolling_7'] = shifted_y_df['y_res_lag_1'].rolling(window=7, center=False).mean()\nshifted_y_df['y_mean_rolling_14'] = shifted_y_df['y_res_lag_1'].rolling(window=14, center=False).mean()\nshifted_y_df['y_mean_rolling_28'] = shifted_y_df['y_res_lag_1'].rolling(window=28, center=False).mean()\nshifted_y_df['y_median_rolling_91'] = shifted_y_df['y_res_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\nshifted_y_df['y_median_rolling_162'] = shifted_y_df['y_res_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\nshifted_y_df","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:46.465405Z","iopub.execute_input":"2022-12-06T16:12:46.466374Z","iopub.status.idle":"2022-12-06T16:12:47.229715Z","shell.execute_reply.started":"2022-12-06T16:12:46.466323Z","shell.execute_reply":"2022-12-06T16:12:47.228822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Other Features","metadata":{}},{"cell_type":"code","source":"# Source: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial\n\nX_2_train = encode_categoricals(on_promotions.reset_index(['store_nbr', 'family']),columns=['store_nbr', 'family'])\nX_2_train[\"day_of_w\"] = X_2_train.index.dayofweek # does absolutely nothing alone\nX_2_train = encode_categoricals(X_2_train, [\"day_of_w\"])\nX_2_train['wage_day'] = (X_2_train.index.day == X_2_train.index.daysinmonth) | (X_2_train.index.day == 15) # is it bad to have this in both X1 AND X2?\nX_2_train['wage_day_lag_1'] = (X_2_train.index.day == 1) | (X_2_train.index.day == 16)\nX_2_train['promo_mean'] = X_2_train.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001\nX_2_train['promo_ratio'] = X_2_train.onpromotion / (X_2_train.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001)\nX_2_train","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:47.231211Z","iopub.execute_input":"2022-12-06T16:12:47.231518Z","iopub.status.idle":"2022-12-06T16:12:47.557137Z","shell.execute_reply.started":"2022-12-06T16:12:47.231485Z","shell.execute_reply":"2022-12-06T16:12:47.55618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combining the feature sets to form one serial-dependence feature set","metadata":{}},{"cell_type":"code","source":"# Source: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial\n\nX_2_train = X_2_train.merge(shifted_y_df, on=['date', 'store_nbr', 'family'], how='left')\nX_2_train = X_2_train.merge(shifted_promo_df, on=['date', 'store_nbr', 'family'], how='left') # merges work if they are last line before return\nX_2_train","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:47.558309Z","iopub.execute_input":"2022-12-06T16:12:47.558525Z","iopub.status.idle":"2022-12-06T16:12:47.801632Z","shell.execute_reply.started":"2022-12-06T16:12:47.558504Z","shell.execute_reply":"2022-12-06T16:12:47.799937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before moving on, we make a function to make the X_2 features, just like [here](https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial#Creating-X2-Features).","metadata":{}},{"cell_type":"code","source":"def make_X2_features(df, y_resid):\n    stack_columns = ['store_nbr', 'family']\n    \n    # promo_lag features\n    shifted_promo_df = make_X2_lags(df.squeeze(), lags=2, name='promo', stack_cols=['store_nbr', 'family'])\n    shifted_promo_df['promo_mean_rolling_7'] = shifted_promo_df['promo_lag_1'].rolling(window=7, center=False).mean()\n    shifted_promo_df['promo_median_rolling_91'] = shifted_promo_df['promo_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    shifted_promo_df['promo_median_rolling_162'] = shifted_promo_df['promo_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\n    # for rolling window medians, backfilling seems reasonable as medians shouldn't change too much. Trying min_periods produced wacky (buggy?) results\n    \n    # y_lag features\n    shifted_y_df = make_X2_lags(y_resid, lags=2, name='y_res', stack_cols=stack_columns)\n    shifted_y_df['y_mean_rolling_7'] = shifted_y_df['y_res_lag_1'].rolling(window=7, center=False).mean()\n    shifted_y_df['y_median_rolling_91'] = shifted_y_df['y_res_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    shifted_y_df['y_median_rolling_162'] = shifted_y_df['y_res_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\n    \n    # other features\n    df = df.reset_index(stack_columns)\n    X2 = encode_categoricals(df, stack_columns)\n    \n    X2[\"day_of_w\"] = X2.index.dayofweek # does absolutely nothing alone\n    X2 = encode_categoricals(df, ['day_of_w'])\n    \n    X2['wage_day'] = (X2.index.day == X2.index.daysinmonth) | (X2.index.day == 15) # is it bad to have this in both X1 AND X2?\n    X2['wage_day_lag_1'] = (X2.index.day == 1) | (X2.index.day == 16)\n    X2['promo_mean'] = X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001\n    X2['promo_ratio'] = X2.onpromotion / (X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001)\n\n    # combing into one feature set\n    X2 = X2.merge(shifted_y_df, on=['date', 'store_nbr', 'family'], how='left')\n    X2 = X2.merge(shifted_promo_df, on=['date', 'store_nbr', 'family'], how='left') # merges work if they are last line before return\n    return X2\n\nX_2_train = make_X2_features(store_sales_train\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model.y_resid)\n\nX_2_train","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:47.803048Z","iopub.execute_input":"2022-12-06T16:12:47.803378Z","iopub.status.idle":"2022-12-06T16:12:49.872035Z","shell.execute_reply.started":"2022-12-06T16:12:47.803349Z","shell.execute_reply":"2022-12-06T16:12:49.870885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling serial-dependence","metadata":{}},{"cell_type":"markdown","source":"Since we compute rolling (trailing) 7 day means based on lag_1, the first entry in time series is NaN. But since the next 6 are means based on <7 days, so the first 7 rows of the time series become NaNs, and hence need to be dropped. Consequently, we define a variable `max_lag = 7` to drop the first 7 rows.","metadata":{}},{"cell_type":"code","source":"max_lag = 7","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:49.873107Z","iopub.execute_input":"2022-12-06T16:12:49.873349Z","iopub.status.idle":"2022-12-06T16:12:49.879238Z","shell.execute_reply.started":"2022-12-06T16:12:49.873326Z","shell.execute_reply":"2022-12-06T16:12:49.877927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:12:49.881038Z","iopub.execute_input":"2022-12-06T16:12:49.881467Z","iopub.status.idle":"2022-12-06T16:13:15.054467Z","shell.execute_reply.started":"2022-12-06T16:12:49.881429Z","shell.execute_reply":"2022-12-06T16:13:15.053355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_fit = model.predict(X_1_train, X_2_train, max_lag).clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:13:15.055955Z","iopub.execute_input":"2022-12-06T16:13:15.056502Z","iopub.status.idle":"2022-12-06T16:13:15.900316Z","shell.execute_reply.started":"2022-12-06T16:13:15.056462Z","shell.execute_reply":"2022-12-06T16:13:15.899565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmsle_train = mean_squared_log_error(y_train.iloc[7:], y_fit) ** 0.5\nprint(f'Training RMSLE: {rmsle_train:.5f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:13:15.901376Z","iopub.execute_input":"2022-12-06T16:13:15.902433Z","iopub.status.idle":"2022-12-06T16:13:15.948952Z","shell.execute_reply.started":"2022-12-06T16:13:15.902397Z","shell.execute_reply":"2022-12-06T16:13:15.947046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing data for forecasting\n\nUnlike the previous submission steps, I have turned this one into a section of its own. This is because the data preparation for forecasting using serial dependence is distinctly different and more challenging than when we were using deterministic features like trends and seasonality to make predictions for the out-of-training-range test set dates.\n\nAs highlighted in the [tutorial](https://www.kaggle.com/code/ryanholbrook/forecasting-with-machine-learning), we could easily create forecasts for any time in the future by just generating our desired trend and seasonal features. However, when we added lag features, the nature of the problem changed. Lag features require that the lagged target value is known at the time being forecast. A lag 1 feature shifts the time series forward 1 step, which means you could forecast 1 step into the future but not 2 steps.","metadata":{}},{"cell_type":"markdown","source":"**Forecasting Strategy: Day-by-day Recursive w/ Fixed Past**\n\nAs the [tutorial](https://www.kaggle.com/code/ryanholbrook/forecasting-with-machine-learning#Multistep-Forecasting-Strategies) mentions, using the recursive strategy, we \n\n> train a single one-step model and use its forecasts to update the lag features for the next step. With the recursive method, we feed a model's 1-step forecast back in to that same model to use as a lag feature for the next forecasting step. We only need to train one model, but since errors will propagate from step to step, forecasts can be inaccurate for long horizons.\n\n\nIt can also be a slow process.","metadata":{}},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"# initialize y_pred_combined\n\ny_pred_combined = y_fit.copy()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:13:15.950799Z","iopub.execute_input":"2022-12-06T16:13:15.951889Z","iopub.status.idle":"2022-12-06T16:13:15.95792Z","shell.execute_reply.started":"2022-12-06T16:13:15.951832Z","shell.execute_reply":"2022-12-06T16:13:15.956241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recursive forecasting","metadata":{}},{"cell_type":"code","source":"# Credit: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial?scriptVersionId=80726243&cellId=36\n\nvalidation_days = len(y_val)\nval_start_day = datetime.datetime(2017, 8, 1)\nval_end_day = datetime.datetime(2017, 8, 15)\n\n\n# Create time-dependence features for validation set\n# loop through forecast, one day (\"step\") at a time\ndp_for_full_X1_val_date_range = dp.out_of_sample(steps=validation_days)\ndp_for_full_X1_val_date_range.index.name = 'date'\n\nfor step in range(validation_days):\n    \n    dp_steps_so_far = dp_for_full_X1_val_date_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step),:]\n    X_1_combined_dp_data = pd.concat([dp.in_sample(), dp_steps_so_far])\n    X_1_val = X_1_combined_dp_data.join(X_holidays, on='date').fillna(0.0)\n    \n    X_2_combined_data = pd.concat([store_sales_train,\n                                       store_sales_valid.loc[val_start_day:val_start_day+pd.Timedelta(days=step), :]])\n    \n    X_2_val = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    \n    y_pred_combined = pd.concat([y_pred_combined,\n                                     model.predict(X_1_val, X_2_val, max_lag).clip(0.0).iloc[-1:]\n                                    ])\n    y_plus_y_val = pd.concat([y_train, y_pred_combined.iloc[-(step+1):]]) # add newly predicted rows of y_pred_combined\n    model.fit1(X_1_val, y_plus_y_val, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to val date range will change slightly\n    model.fit2(X_2_val, max_lag, stack_cols=['store_nbr', 'family'])\n    \n    rmsle_valid = mean_squared_log_error(y_val.iloc[step:step+1], y_pred_combined.iloc[-1:]) ** 0.5\n    print(f'Validation RMSLE: {rmsle_valid:.5f}', \"for\", val_start_day+pd.Timedelta(days=step))\n    \ny_pred = y_pred_combined[val_start_day:val_end_day]\ndisplay(y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:13:15.960029Z","iopub.execute_input":"2022-12-06T16:13:15.961336Z","iopub.status.idle":"2022-12-06T16:20:31.336039Z","shell.execute_reply.started":"2022-12-06T16:13:15.96127Z","shell.execute_reply":"2022-12-06T16:20:31.334767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmsle_train = mean_squared_log_error(y_train.iloc[max_lag: , :].clip(0.0), y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_val.clip(0.0), y_pred) ** 0.5\nprint()\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n    \ny_predict = y_pred.stack(['store_nbr', 'family']).reset_index()\ny_target = y_val.stack(['store_nbr', 'family']).reset_index().copy()\ny_target.rename(columns={y_target.columns[3]:'sales'}, inplace=True)\ny_target['sales_pred'] = y_predict[0].clip(0.0) # Sales should be >= 0\ny_target['store_nbr'] = y_target['store_nbr'].astype(int)\n\nprint('\\nValidation RMSLE by family')\ndisplay(y_target.groupby('family').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])))\n\nprint('\\nValidation RMSLE by store')\ndisplay(y_target.sort_values(by=\"store_nbr\").groupby('store_nbr').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])))","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:20:31.337382Z","iopub.execute_input":"2022-12-06T16:20:31.337658Z","iopub.status.idle":"2022-12-06T16:20:31.508598Z","shell.execute_reply.started":"2022-12-06T16:20:31.337623Z","shell.execute_reply":"2022-12-06T16:20:31.50776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Set","metadata":{}},{"cell_type":"markdown","source":"Now, we repeat the entire procedure for the test set. The test set data looks like:","metadata":{}},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:20:31.510528Z","iopub.execute_input":"2022-12-06T16:20:31.511149Z","iopub.status.idle":"2022-12-06T16:20:31.523336Z","shell.execute_reply.started":"2022-12-06T16:20:31.511108Z","shell.execute_reply":"2022-12-06T16:20:31.521796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_sales_test = df_test.unstack(['store_nbr', 'family']).drop('id', axis=1)\nstore_sales_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:20:31.524998Z","iopub.execute_input":"2022-12-06T16:20:31.52542Z","iopub.status.idle":"2022-12-06T16:20:31.572029Z","shell.execute_reply.started":"2022-12-06T16:20:31.525382Z","shell.execute_reply":"2022-12-06T16:20:31.570643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Retraining over entire training and validation sets.** So we start again from the combined training store sales.","metadata":{}},{"cell_type":"code","source":"store_sales = pd.read_csv(\n    comp_dir / 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n        'onpromotion': 'uint32', ## NEW FEATURE: To be introduced later\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nstore_sales = store_sales.unstack(['store_nbr', 'family']).loc['2017']\ny = store_sales.loc[:, 'sales']","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:20:31.573834Z","iopub.execute_input":"2022-12-06T16:20:31.57414Z","iopub.status.idle":"2022-12-06T16:20:34.925608Z","shell.execute_reply.started":"2022-12-06T16:20:31.574113Z","shell.execute_reply":"2022-12-06T16:20:34.924095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model for testing\nmodel_for_test = BoostedHybrid(model_1=mod_1, model_2=mod_2)\n\n# Trends/Seasonality\n# Fourier features\nfourier = CalendarFourier(freq='M', order=4) ## 2 pairs of sine/cosine curves to model monthly/biweekly seasonality\ndp_test = DeterministicProcess(\n    index=y.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # cubic trend\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\n\nX_1_train = dp_test.in_sample()\n\n# Seasonal indicators: Holidays\nX_1_train = X_1_train.join(X_holidays, on='date').fillna(0.0)\n\n# fitting model 1 over entire training set sales: y\nmodel_for_test.fit1(X_1_train, y, stack_cols=['store_nbr', 'family'])\n\n# make X_2 features based on entire store_sales\n# preparing X2 for hybrid part 2: XGBoost\nX_2_train = make_X2_features(store_sales\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_test.y_resid)\n\n# fitting model 2 over residuals of entire training set sales\nmodel_for_test.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n\n# initializing with training set fit\ny_forecast_combined = model_for_test.predict(X_1_train, X_2_train, max_lag).clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:20:34.927847Z","iopub.execute_input":"2022-12-06T16:20:34.928856Z","iopub.status.idle":"2022-12-06T16:21:04.457566Z","shell.execute_reply.started":"2022-12-06T16:20:34.928803Z","shell.execute_reply":"2022-12-06T16:21:04.456032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_days = len(store_sales_test)\ntest_start_day = datetime.datetime(2017, 8, 16)\ntest_end_day = datetime.datetime(2017, 8, 31)\n\n\n# Create time-dependence features for test set\n# loop through forecast, one day (\"step\") at a time\ndp_for_full_X1_test_date_range = dp_test.out_of_sample(steps=test_days)\ndp_for_full_X1_test_date_range.index.name = 'date'\n\nfor step in range(test_days):\n    \n    dp_steps_so_far = dp_for_full_X1_test_date_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step),:]\n    X_1_combined_dp_data = pd.concat([dp_test.in_sample(), dp_steps_so_far])\n    X_1_test = X_1_combined_dp_data.join(X_holidays, on='date').fillna(0.0)\n    \n    X_2_combined_data = pd.concat([store_sales,\n                                       store_sales_test.loc[test_start_day:test_start_day+pd.Timedelta(days=step), :]])\n    \n    X_2_test = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    \n    y_forecast_combined = pd.concat([y_forecast_combined,\n                                     model_for_test.predict(X_1_test, X_2_test, max_lag).clip(0.0).iloc[-1:]\n                                    ])\n    y_plus_y_test = pd.concat([y, y_forecast_combined.iloc[-(step+1):]]) # add newly predicted rows of y_forecast_combined\n    model_for_test.fit1(X_1_test, y_plus_y_test, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to test date range will change slightly\n    model_for_test.fit2(X_2_test, max_lag, stack_cols=['store_nbr', 'family'])\n    \n    print(\"finished forecast for\", test_start_day+pd.Timedelta(days=step))","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:21:04.459358Z","iopub.execute_input":"2022-12-06T16:21:04.45975Z","iopub.status.idle":"2022-12-06T16:29:21.246695Z","shell.execute_reply.started":"2022-12-06T16:21:04.459716Z","shell.execute_reply":"2022-12-06T16:29:21.2458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_forecast = pd.DataFrame(y_forecast_combined[test_start_day:test_end_day].clip(0.0), columns=y.columns)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:29:21.301085Z","iopub.execute_input":"2022-12-06T16:29:21.301455Z","iopub.status.idle":"2022-12-06T16:29:21.314281Z","shell.execute_reply.started":"2022-12-06T16:29:21.301418Z","shell.execute_reply":"2022-12-06T16:29:21.313334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fifth submission: Hybrid time and (complete) serial dependence modelling + recursive forecasting","metadata":{}},{"cell_type":"code","source":"# making submission predictions\n\ny_submit = y_forecast.stack(['store_nbr', 'family'])\ny_submit = pd.DataFrame(y_submit, columns=['sales'])\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission_hybrid_recursive.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:29:21.31563Z","iopub.execute_input":"2022-12-06T16:29:21.316235Z","iopub.status.idle":"2022-12-06T16:29:21.835185Z","shell.execute_reply.started":"2022-12-06T16:29:21.316203Z","shell.execute_reply":"2022-12-06T16:29:21.834143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Our final model:\n* restricts our training data to 2017\n* models time-dependence through trends, monthly and bi-weekly Fourier features, day-of-the-week seasonal indicators and holiday features using a linear algorithm (LinearRegression)\n* models serial-dependence lagged series of residuals, lead/lag series of leading indicators (onpromotion) and other statistical features using a non-linear algorithm (XGBoost)\n* validates using the last 15 days of the full training set\n* forecasts for the test set dates using a recursive strategy \n\n## [Final Public Score: 0.50109](https://www.kaggle.com/code/abhirupghosh184098/ts-forecasting-a-beginner-s-handbook?scriptVersionId=113116759)","metadata":{}},{"cell_type":"markdown","source":"# The future\n\nThrough this notebook, I tried focussed on aspects of a time-series and how they influence forecasting. I did not delve deep into core machine learning concepts like data cleaning, data wrangling, feature engineering and evaluation of the algorithms themselves. As I go through the Kaggle landscape, I have tried tackling each issue separately, so that each solution remains a dominant resource on only one aspect of machine learning. Having said that, there are a few things one could do to improve on the analysis presented here:\n\n* include additional information provided in this competition itself, eg, oil prices, transactions data and store details\n* engineering features other than those directly related to aspects of a time series\n* focus on modelling by exploring the best choice of parameters (through hyper-parameter tuning) or evaluating the models themselves by seeing which model is best validated for our time-series forecasting\n* and finally, use forecasting strategies other than the recursive strategy we have used here.\n\nI hope you had fun reading this; and see you soon in another notebook. :)","metadata":{}}]}