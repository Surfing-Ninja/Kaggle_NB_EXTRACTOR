{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA)**\n\n**What is EDA?** Exploratory Data Analysis: this is unavoidable and one of the major step to fine-tune the given data set(s) in a different form of analysis to understand the insights of the key characteristics of various entities of the data set like column(s), row(s) by applying Pandas, NumPy, Statistical Methods, and Data visualization packages. \n\nI belive that **70 percent** of machine learning enginner time consuming is *EDA and Data Cleaning*, Thererore, in this Notebook, I will go through many steps that will reveal hidden secrets in this competation.\n\nSo, Why are we waiting?? Let's started","metadata":{}},{"cell_type":"markdown","source":"# **Table of Contents**\n\n* [1. IMPORTING PACKAGES & LOADING DATASETS](#section1)\n\n* [2. TRAIN DATASET ANALYSIS](#section2)\n    * [2.1 Dataset Preview](#section2.1)\n    * [2.2 Memory Usage Reduce](#section2.2)\n    * [2.3 Avg Sales By Day, Week, Month and Day Of Week](#section2.3)\n    * [2.4 Total Sales vs. On Promotion](#section2.4)\n    * [2.5 Avg Sales by Family Products](#section2.5)\n    * [2.6 Avg Sales by Store No.](#section2.6)\n    * [2.7 Determine Trend](#section2.7)\n    * [2.8 Determine Seasonality](#section2.8)\n    * [2.9 Lagged Series and Lag Plots](#section2.9)\n    \n    \n* [3. HOLIDAYS & EVENTS](#section3)\n\n    * [3.1 Holidays & Events Dataset Analysis](#section3.1)\n    * [3.2 Avg Sales on Holidays vs. Workdays](#section3.2)\n    \n    \n* [4. OIL PRICES](#section4)\n\n    * [4.1 Oil Dataset Preview ](#section4.1)\n    * [4.2 Avg Sales vs Oil Prices](#section4.2)\n    \n    \n* [5. STORES ANALYSIS ](#section5) \n\n* [6. TRANSACTIONS ](#section6) \n\n    * [6.1 Transactions Dataset Preview](#section6.1)\n    * [6.2 Total Sales vs Transactions](#section6.2)\n\n","metadata":{}},{"cell_type":"markdown","source":" # 1. IMPORTING PACKAGES & LOADING DATASETS <a class=\"anchor\"  id=\"section1\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom warnings import simplefilter\nsimplefilter('ignore')\n\n## Set Plot Parameters\nsns.set(color_codes=True)        \nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 5))\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)\nplot_params = dict(color=\"0.75\", style=\".-\", markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", legend=False)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-29T08:48:03.15804Z","iopub.execute_input":"2022-10-29T08:48:03.159427Z","iopub.status.idle":"2022-10-29T08:48:03.167749Z","shell.execute_reply.started":"2022-10-29T08:48:03.159387Z","shell.execute_reply":"2022-10-29T08:48:03.165988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path='../input/store-sales-time-series-forecasting/'  ## Path of datasets\n\n## Train & Test Datasets\ntrain=pd.read_csv(path+'train.csv',parse_dates=['date'])\ntest=pd.read_csv(path+'test.csv',parse_dates=['date'])\n\n## Supplementary Datasets\noil=pd.read_csv(path+'oil.csv',parse_dates=['date'])\nholidays_events=pd.read_csv(path+'holidays_events.csv',parse_dates=['date'])\nstores=pd.read_csv(path+'stores.csv')\ntransactions=pd.read_csv(path+'transactions.csv',parse_dates=['date'])","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:03.169972Z","iopub.execute_input":"2022-10-29T08:48:03.170951Z","iopub.status.idle":"2022-10-29T08:48:04.752606Z","shell.execute_reply.started":"2022-10-29T08:48:03.170913Z","shell.execute_reply":"2022-10-29T08:48:04.751886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. TRAIN DATASET ANALYSIS <a class=\"anchor\"  id=\"section2\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Dataset Preview <a class=\"anchor\"  id=\"section2.1\"></a>\n\nFirst of all, we need to interview with train dataset and get some knowledge from its structure, missing values, value counts...etc.\n\nThen I will meet with our target variable \"sales\" and get some statistics like (min,max,mean,median and skewness).","metadata":{}},{"cell_type":"code","source":"display(train.head())\ndisplay(train.info())\n\ndef info(dataset):\n    \n    \"\"\" This defination is to print most valuable information\n        about dataset columns.\n        Input: dataset\n        Output: dataset columns information\n    \"\"\"\n    for column in dataset.columns:\n        print('==========%s =========='%column)\n        print('Type is: ',dataset[column].dtype)\n        print(dataset[column].value_counts())\n        print('Number of unique values: ',dataset[column].nunique())\n        print('Number of null values: ',dataset[column].isna().sum())\n\ninfo(train)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-29T08:48:04.753971Z","iopub.execute_input":"2022-10-29T08:48:04.754692Z","iopub.status.idle":"2022-10-29T08:48:05.803799Z","shell.execute_reply.started":"2022-10-29T08:48:04.754655Z","shell.execute_reply":"2022-10-29T08:48:05.802925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Skewness of sales: ',train['sales'].skew())\ndisplay(train['sales'].describe()) ## get statistics from target variable\n\nsns.distplot(train['sales'],bins=6)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:05.805037Z","iopub.execute_input":"2022-10-29T08:48:05.805309Z","iopub.status.idle":"2022-10-29T08:48:19.999913Z","shell.execute_reply.started":"2022-10-29T08:48:05.805286Z","shell.execute_reply":"2022-10-29T08:48:19.998782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sales variable is highly positive skewness, the given distribution is shifted to the left and with its tail on the right side.","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Memory Usage Reduce <a class=\"anchor\"  id=\"section2.2\"></a>\n\nThe memory usage of the train dataset is pretty large, we can reduce it by some data type conversions like:\n\n* Convert dtype for [id] from \"int64\" to \"int32\"\n* Convert dtype for [store_nbr] from \"int64\" to \"int8\"\n* Convert dtype for [family] from \"object\" to \"category\"\n* Convert dtype for [sales] from \"float64\" to \"float32\"\n* Convert dtype for [onpromotion] from \"int64\" to \"int16\"","metadata":{}},{"cell_type":"code","source":"train['id']=train.id.astype('int32')\ntrain['store_nbr']=train.store_nbr.astype('int8')\ntrain['family']=train.family.astype('category')\ntrain['sales']=train.sales.astype('float32')\ntrain['onpromotion']=train.onpromotion.astype('int16')\ndisplay(train.info(verbose=False,memory_usage=True))\nprint('\\nIndeed, we succeed to reduce the memory usage from 137M to 57M (more than half) this will be reflected in speed processing especially for EDA tasks.')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:20.003215Z","iopub.execute_input":"2022-10-29T08:48:20.004038Z","iopub.status.idle":"2022-10-29T08:48:20.249264Z","shell.execute_reply.started":"2022-10-29T08:48:20.003999Z","shell.execute_reply":"2022-10-29T08:48:20.247838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Avg Sales By Day, Week, Month and Day Of Week <a class=\"anchor\"  id=\"section2.3\"></a>\n\nFirst, I just want to look at sales distribution at many time frames, without taking into considration any feature correlation.","metadata":{}},{"cell_type":"code","source":"train_eda = train.copy() ## Initializing a copy of train dataset for EDA purpose,\n##                          and to preserve the original train dataset\n\ntrain_eda=train_eda.set_index('date') ## Setting date to index for simple handling\n\ndaily_sales=train_eda.resample('D').sales.mean().to_frame()  ## Resample sales by day\nweekly_sales=train_eda.resample('W').sales.mean().to_frame()  ## Resample sales by week\nmonthly_sales=train_eda.resample('M').sales.mean().to_frame()  ## Resample sales by month\n\ndf=[daily_sales,weekly_sales,monthly_sales]\ntitles=['Daily Avg. Sales','Weekly Avg. Sales','Monthly Avg. Sales']\n\nfor i,j in zip(df,titles):\n    \n    sns.relplot(x=i.index,y=i.sales,kind='line',aspect=3,hue=i.index.year)\n    plt.xlabel('Date')\n    plt.ylabel('Avg. Sales')\n    plt.title(j)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:20.250533Z","iopub.execute_input":"2022-10-29T08:48:20.250872Z","iopub.status.idle":"2022-10-29T08:48:22.226359Z","shell.execute_reply.started":"2022-10-29T08:48:20.25084Z","shell.execute_reply":"2022-10-29T08:48:22.225042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Group by day of week\n\ntemp=train_eda.groupby(train_eda.index.day_of_week)['sales'].mean().to_frame()\n\ntemp.plot(**plot_params)\nplt.xlabel('Day of Week')\nplt.ylabel('Avg. Sales')\nplt.title('Avg. Sales by Day of Week')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:22.227677Z","iopub.execute_input":"2022-10-29T08:48:22.228353Z","iopub.status.idle":"2022-10-29T08:48:22.604006Z","shell.execute_reply.started":"2022-10-29T08:48:22.228314Z","shell.execute_reply":"2022-10-29T08:48:22.602672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:**\n\n1. Sales are increased at annual basis, that indicates a trend variable (will be investigated soon)\n\n2. There are peaks at daily / weekly basis at weekends, that indicates a seasonality variable (will be investigated soon)\n\n3. A zero sales at each start of a year can be shown from \"Daily Avg. Sales\", due to Favorita Supermarkets chain were closed at that days.\n\n4. Generally, sales on Thursdays are smaller than rest of days.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Total Sales vs. On Promotion <a class=\"anchor\"  id=\"section2.4\"></a>\n\n\n\n**onpromotion**: gives the total number of items in a product family that were being promoted at a store at a given date.\n\nI belive there is a correlation between the sales and onpromotion variables that deserve to investigate.","metadata":{}},{"cell_type":"code","source":"avg_sales=train_eda.groupby(['date'])['sales','onpromotion'].sum().reset_index() \n\nprint('The correlation between sales & onpromotion is: ',\n      np.round(avg_sales[['sales','onpromotion']].corr().iloc[0,1],4),'\\n')\n\nplt.figure(figsize=(10,5))\nsns.regplot(data=avg_sales,x='onpromotion',y='sales',ci=None,\n            scatter_kws={'color':'0.4'},line_kws={'color':'red','linewidth':3})\nplt.xlabel('Total Items on Promotion')\nplt.ylabel('Total Sales')\nplt.title('Total Sales vs. Total Items On-promotion')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:22.605512Z","iopub.execute_input":"2022-10-29T08:48:22.605848Z","iopub.status.idle":"2022-10-29T08:48:22.949234Z","shell.execute_reply.started":"2022-10-29T08:48:22.605819Z","shell.execute_reply":"2022-10-29T08:48:22.947643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is a positive correlation between onpromotion and sales units sold. Thus, when more items are on promotion, it's more likely to sell them.","metadata":{}},{"cell_type":"markdown","source":"## 2.5 Avg Sales by Family Products <a class=\"anchor\"  id=\"section2.5\"></a>\n\n\n\n**family** identifies the type of product sold.\n\nWhat are the best selling family products?? I guess that data scientists at Favorita supermarkets have done that analysis. Also me do like to perform this task :)","metadata":{}},{"cell_type":"code","source":"temp=train_eda.groupby('family')['sales'].mean().sort_values(ascending=False).to_frame()\n\nplt.figure(figsize=(16,10))\nsns.barplot(data=temp,x=temp.sales,y=temp.index,ci=None,order=list(temp.index))\nplt.xlabel('Avg. Sales')\nplt.ylabel('Family Product')\nplt.title('Avg. Sales by Family Product')\nplt.show()\n\nprint('The best family products sell are: ',list(temp.index[:5]))\nprint('\\nThe worst family products sell are: ',list(temp.index[-5:]))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:22.950904Z","iopub.execute_input":"2022-10-29T08:48:22.951261Z","iopub.status.idle":"2022-10-29T08:48:23.395593Z","shell.execute_reply.started":"2022-10-29T08:48:22.951233Z","shell.execute_reply":"2022-10-29T08:48:23.394701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.6 Avg Sales by Store No. <a class=\"anchor\"  id=\"section2.6\"></a>\n\n**store_nbr:** identifies the store at which the products are sold.\n\nThe same we did with family products, I will do here.","metadata":{}},{"cell_type":"code","source":"temp=train_eda.groupby('store_nbr')['sales'].mean().sort_values(ascending=False).to_frame()\n\nplt.figure(figsize=(14,8))\nsns.barplot(data=temp,x=temp.index,y=temp.sales,ci=None,order=list(temp.index))\nplt.xlabel('Store No.')\nplt.ylabel('Avg. Sales')\nplt.title('Avg. Sales by Store No.')\nplt.show()\n\nprint('The best stores No. sell are: ',list(temp.index[:5]))\nprint('\\nThe worst stores No. sell are: ',list(temp.index[-5:]))","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:23.396703Z","iopub.execute_input":"2022-10-29T08:48:23.396958Z","iopub.status.idle":"2022-10-29T08:48:24.579304Z","shell.execute_reply.started":"2022-10-29T08:48:23.396935Z","shell.execute_reply":"2022-10-29T08:48:24.578426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.7 Determine Trend <a class=\"anchor\"  id=\"section2.7\"></a>\n\nLet's make a moving average plot to see what kind of trend this series has. Since this series has daily observations, let's choose a window of 365 days to smooth over any short-term changes within the year.\n\nTo see what kind of trend a time series might have, we can use a moving average plot. To compute a moving average of a time series, we compute the average of the values within a sliding window of some defined width. Each point on the graph represents the average of all the values in the series that fall within the window on either side. The idea is to smooth out any short-term fluctuations in the series so that only long-term changes remain.","metadata":{}},{"cell_type":"code","source":"avg_sales=train_eda.groupby('date').sales.mean()\nmoving_avg=avg_sales.rolling(window=365,min_periods=183,center=True).mean()\n\nax=avg_sales.plot(**plot_params)\nax=moving_avg.plot(color='red',linewidth=3)\nplt.xlabel('Date')\nplt.ylabel('Avg. Sales')\nplt.title('Avg. Sales Trend Over Years')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:24.582219Z","iopub.execute_input":"2022-10-29T08:48:24.582475Z","iopub.status.idle":"2022-10-29T08:48:24.962288Z","shell.execute_reply.started":"2022-10-29T08:48:24.582451Z","shell.execute_reply":"2022-10-29T08:48:24.960983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What order polynomial trend might be appropriate for the this time series? Can you think of a non-polynomial curve that might work even better? \nThe upwards bend in the trend suggests an order 2 (quadratic) polynomial might be appropriate.","metadata":{}},{"cell_type":"markdown","source":"## 2.8 Determine Seasonality <a class=\"anchor\"  id=\"section2.8\"></a>\n\n\nWe say that a time series exhibits seasonality whenever there is a regular, periodic change in the mean of the series.\n\nJust like we used a moving average plot to discover the trend in a series, we can use a seasonal plot to discover seasonal patterns.","metadata":{}},{"cell_type":"code","source":"def seasonal_plot(X, y, period, freq):\n    \"\"\"\n    This defination is to plot seasonal fluctuation in a time series to discover seasonal patterns.\n    Inputs:\n            X: time series.\n            y: target variables \n            period: The period of time series\n            freq: The frequency to plot time seris for it\n    Output:\n        Time Series Seasonal Plot    \n    \"\"\"\n    _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(x=freq, y=y, hue=period, data=X, ci=False, ax=ax,\n                      palette=palette, legend=False)\n    \n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(name, xy=(1, y_), xytext=(6, 0), color=line.get_color(),size=14,\n                    xycoords=ax.get_yaxis_transform(), textcoords=\"offset points\",va=\"center\")\n        \n    return ax\n\n\ndef plot_periodogram(ts):\n    \"\"\"\n    This defination is to discover seasonality and plot the periodogram for a time series. \n    \"\"\"\n    \n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(ts, fs=fs, detrend='linear', window=\"boxcar\",scaling='spectrum')\n    \n    _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\"Annual (1)\", \"Semiannual (2)\", \"Quarterly (4)\", \"Bimonthly (6)\", \"Monthly (12)\",\n         \"Biweekly (26)\", \"Weekly (52)\", \"Semiweekly (104)\"], rotation=30)\n    \n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    \n    return ax","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-29T08:48:24.963709Z","iopub.execute_input":"2022-10-29T08:48:24.963982Z","iopub.status.idle":"2022-10-29T08:48:24.976731Z","shell.execute_reply.started":"2022-10-29T08:48:24.963956Z","shell.execute_reply":"2022-10-29T08:48:24.975179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_sales = train_eda.groupby('date')['sales'].mean().to_frame()\n\n# days within a week\navg_sales[\"day\"] = avg_sales.index.dayofweek  # the x-axis (freq)\navg_sales[\"week\"] = avg_sales.index.week  # the seasonal period (period)\n\n# days within a year\navg_sales[\"dayofyear\"] = avg_sales.index.dayofyear\navg_sales[\"year\"] = avg_sales.index.year\n\nseasonal_plot(avg_sales, y=avg_sales['sales'], period=\"week\", freq=\"day\")\nseasonal_plot(avg_sales, y=avg_sales['sales'], period=\"year\", freq=\"dayofyear\")","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:24.979172Z","iopub.execute_input":"2022-10-29T08:48:24.979556Z","iopub.status.idle":"2022-10-29T08:48:32.566129Z","shell.execute_reply.started":"2022-10-29T08:48:24.97952Z","shell.execute_reply":"2022-10-29T08:48:32.565123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look at the periodogram:","metadata":{}},{"cell_type":"code","source":"plot_periodogram(avg_sales.sales);","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:32.567648Z","iopub.execute_input":"2022-10-29T08:48:32.567903Z","iopub.status.idle":"2022-10-29T08:48:33.275108Z","shell.execute_reply.started":"2022-10-29T08:48:32.567874Z","shell.execute_reply":"2022-10-29T08:48:33.273883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both the seasonal plot and the periodogram suggest a strong weekly seasonality. From the periodogram, it appears there may be some Annual and Biweekly components as well. In fact, the notes to the Store Sales dataset say wages in the public sector are paid out biweekly, on the 15th and last day of the month -- a possible origin for these seasons.","metadata":{}},{"cell_type":"markdown","source":"## 2.9 Lagged Series and Lag Plots <a class=\"anchor\"  id=\"section2.9\"></a>\n\n\nTo investigate possible serial dependence (like cycles) in a time series, we need to create \"**lagged**\" copies of the series. Lagging a time series means to shift its values forward one or more time steps, or equivalently, to shift the times in its index backward one or more steps. In either case, the effect is that the observations in the lagged series will appear to have happened later in time.\n\nA **lag plot** of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot.\n\nWhen choosing lags to use as features, it generally won't be useful to include every lag with a large autocorrelation, The **partial autocorrelation** tells you the correlation of a lag accounting for all of the previous lags -- the amount of \"new\" correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use","metadata":{}},{"cell_type":"code","source":"def lagplot(x, lag=1, ax=None):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(lag)\n    y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(alpha=0.75, s=3)\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_, y=y_, scatter_kws=scatter_kws,\n                     line_kws=line_kws, lowess=True, ax=ax)\n    \n    at = AnchoredText(f\"{corr:.2f}\", prop=dict(size=\"large\"), frameon=True, loc=\"upper left\")\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    ax.set(title=f\"Lag {lag}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags(x, y=None, lags=6, nrows=1, **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    kwargs.setdefault('ncols', math.ceil(lags / nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2, nrows * 2 + 0.5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        if k + 1 <= lags:\n            ax = lagplot(x, lag=k + 1, ax=ax)\n            ax.set_title(f\"Lag {k + 1}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-29T08:48:33.277329Z","iopub.execute_input":"2022-10-29T08:48:33.277756Z","iopub.status.idle":"2022-10-29T08:48:33.291867Z","shell.execute_reply.started":"2022-10-29T08:48:33.277714Z","shell.execute_reply":"2022-10-29T08:48:33.290477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_pacf\n\n_ = plot_lags(avg_sales.sales, lags=12, nrows=2)\n_ = plot_pacf(avg_sales.sales, lags=12)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:33.293256Z","iopub.execute_input":"2022-10-29T08:48:33.29359Z","iopub.status.idle":"2022-10-29T08:48:37.505509Z","shell.execute_reply.started":"2022-10-29T08:48:33.293562Z","shell.execute_reply":"2022-10-29T08:48:37.50415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The lag plots indicate that the relationship of sales to its lags is somehow linear, while the partial autocorrelations suggest the dependence can be captured using lags 1, 3, 5, 6, 7, 8 and 9, So these lags will be used in training.","metadata":{}},{"cell_type":"markdown","source":" # 3. HOLIDAYS & EVENTS <a class=\"anchor\"  id=\"section3\"></a>\n\n\nAt trading world, every person belive that sales at holidays, weekends and events are larger than any work day..","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Holidays & Events Dataset Analysis <a class=\"anchor\"  id=\"section3.1\"></a>\n\nI belive that this supplementary dataset has a large correlation with train dataset, So Let's get some useful sale indicators from it.","metadata":{}},{"cell_type":"code","source":"display(holidays_events.info())\ninfo(holidays_events)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-29T08:48:37.506844Z","iopub.execute_input":"2022-10-29T08:48:37.507219Z","iopub.status.idle":"2022-10-29T08:48:37.534901Z","shell.execute_reply.started":"2022-10-29T08:48:37.507186Z","shell.execute_reply":"2022-10-29T08:48:37.533532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" \nAs described at metadata, \"Pay special attention to the transferred column.\nA holiday that is transferred officially falls on that calendar day, \nbut was moved to another date by the government.\" So I will get rid holidays that were transferred\nand holidays that are announced a Work day, in addition to dates that are duplicated.\n\"\"\"   \nholidays_events= holidays_events.loc[(holidays_events.transferred==False) & (holidays_events.type != 'Work Day')]\nholidays_events=holidays_events.drop_duplicates(subset='date')  ## Drop duplicated dates","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:37.536259Z","iopub.execute_input":"2022-10-29T08:48:37.536582Z","iopub.status.idle":"2022-10-29T08:48:37.54567Z","shell.execute_reply.started":"2022-10-29T08:48:37.536554Z","shell.execute_reply":"2022-10-29T08:48:37.544572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One important thing is to check if average sales are more at Holidays & Events than work days.\nIn next sections, I will investigate that, in addition I need to know which holidays their impact is significant on average sales, so we can use it as indecator for sales prediction model training. \n\nHolidays dataset has column distinguishes  between \"local\", \"National\", \"Regional\" holidays, So I will get insights from average sales during these three types of holidays. ","metadata":{}},{"cell_type":"code","source":"## National and Regional Holidays Only\n\nNRHolidays=holidays_events.loc[holidays_events['locale']!='Local',:]\nNRHolidays_avg_sales=avg_sales.reset_index().merge(NRHolidays,on='date',how='left')\nx_cor=NRHolidays_avg_sales.loc[NRHolidays_avg_sales['type'].notna(),'date'].values\ny_cor=NRHolidays_avg_sales.loc[NRHolidays_avg_sales['type'].notna(),'sales'].values\n_=avg_sales['sales'].plot(**plot_params)\n_=plt.plot_date(x_cor,y_cor,color='C3', label='National / Regional Holiday')\n_=plt.ylabel('Avg. Sales')\n_=plt.title('Avg. Sales At National and Regional Holidays Only')\n_=plt.legend()\n\nplt.show()\n\n## LocalHolidays Only\nLHolidays=holidays_events.loc[holidays_events['locale']=='Local',:]\nLHolidays_avg_sales=avg_sales.reset_index().merge(LHolidays,on='date',how='left')\nx_cor=LHolidays_avg_sales.loc[LHolidays_avg_sales['type'].notna(),'date'].values\ny_cor=LHolidays_avg_sales.loc[LHolidays_avg_sales['type'].notna(),'sales'].values\n_=avg_sales['sales'].plot(**plot_params)\n_=plt.plot_date(x_cor,y_cor,color='C3', label='Local Holiday')\n_=plt.ylabel('Avg. Sales')\n_=plt.title('Avg. Sales At Local Holidays')\n_=plt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:37.546886Z","iopub.execute_input":"2022-10-29T08:48:37.547333Z","iopub.status.idle":"2022-10-29T08:48:38.243297Z","shell.execute_reply.started":"2022-10-29T08:48:37.547292Z","shell.execute_reply":"2022-10-29T08:48:38.241698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, there is a differance between those three holiday types, Sales at Local holidays has no significant impact than another normal days, while sales at National/ Regional holidays are more significant for sales versus another days.\n\nI will get rid Local holidays.","metadata":{}},{"cell_type":"code","source":"holidays_events= holidays_events[holidays_events.locale!='Local']","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:38.245301Z","iopub.execute_input":"2022-10-29T08:48:38.24569Z","iopub.status.idle":"2022-10-29T08:48:38.252395Z","shell.execute_reply.started":"2022-10-29T08:48:38.245656Z","shell.execute_reply":"2022-10-29T08:48:38.250983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Avg Sales on Holidays  vs. Workdays <a class=\"anchor\"  id=\"section3.2\"></a>\n\nI will merge \"train\" & \"holidays_events\" datasets togather, creat new holiday indicator column at the new merged dataset that take two values:\n\n* 0 for workdays.\n* 1 for holiday, event, bridge, additional, weekend days.","metadata":{}},{"cell_type":"code","source":"## Merging and Mapping:\nholidays_events=holidays_events[['date','type']] ## Keep date & holiday type for merging\ntrain_eda=pd.merge(left=train_eda, right=holidays_events, on='date', how='left')\ntrain_eda.rename({'type':'is_holiday'},axis=1,inplace=True)\ntrain_eda['is_holiday']=train_eda.is_holiday.map({'Holiday':1,'Additional':1,'Event':1,\n                                                  'Bridge':1,'Transfer':1}).fillna(0).astype('int8')\n\n## Adding weekends to holiday as well\ntrain_eda.set_index('date',inplace=True)\ntrain_eda['day_of_week']=train_eda.index.dayofweek.astype('int8')\ntrain_eda.loc[(train_eda['day_of_week']==5) | (train_eda['day_of_week']==6), 'is_holiday']=1\n\n## Removing he first day of a year from holidays as mentioned before Favorita is closed these days.\ntrain_eda['day_of_year']=train_eda.index.dayofyear.astype('int16')\ntrain_eda.loc[train_eda['day_of_year']==1 ,'is_holiday']=0\n\ntrain_eda.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:38.253654Z","iopub.execute_input":"2022-10-29T08:48:38.25398Z","iopub.status.idle":"2022-10-29T08:48:38.971034Z","shell.execute_reply.started":"2022-10-29T08:48:38.253943Z","shell.execute_reply":"2022-10-29T08:48:38.969471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_sales_holiday=train_eda[train_eda.is_holiday==1].groupby('date')['sales'].mean() ## Grouping by holidays\navg_sales_workday=train_eda[train_eda.is_holiday==0].groupby('date')['sales'].mean() ## Grouping by workdays\n\n_=avg_sales_workday.plot(color='0.4',style='.', legend=True, label='Workday Avg. Sales')\n_=avg_sales_holiday.plot(color='red',style='.', legend=True, label='Holiday Avg. Sales')\nplt.xlabel('Date')\nplt.ylabel('Avg. Sales')\nplt.title('Avg. Sales on Holidays vs. Workdays')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:38.97287Z","iopub.execute_input":"2022-10-29T08:48:38.973248Z","iopub.status.idle":"2022-10-29T08:48:39.519912Z","shell.execute_reply.started":"2022-10-29T08:48:38.973216Z","shell.execute_reply":"2022-10-29T08:48:39.518476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:**\n\n* We succeed to prove the trading theory, from above chart we can obviously see the high difference between sales on holidays versus workdays. Except the zero sales at the first day of the years, and that is as mentioned before due to Favorita supermarkets chain was closed.\n\n* Holiday indicator will be used as a variable for training.","metadata":{}},{"cell_type":"markdown","source":" # 4. OIL PRICES <a class=\"anchor\"  id=\"section4\"></a>\n\nEcuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Oil Dataset Preview <a class=\"anchor\"  id=\"section4.1\"></a>\n\nDaily oil price. Includes values during both the train and test data timeframes.","metadata":{}},{"cell_type":"code","source":"display(oil.info())\ninfo(oil)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-29T08:48:39.521694Z","iopub.execute_input":"2022-10-29T08:48:39.522115Z","iopub.status.idle":"2022-10-29T08:48:39.54401Z","shell.execute_reply.started":"2022-10-29T08:48:39.522051Z","shell.execute_reply":"2022-10-29T08:48:39.542907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil.set_index('date',inplace=True)\noil.plot(**plot_params)\nplt.xlabel('Date')\nplt.ylabel('Oil Price')\nplt.title('Oil Prices Over Time')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:39.545569Z","iopub.execute_input":"2022-10-29T08:48:39.546232Z","iopub.status.idle":"2022-10-29T08:48:39.801945Z","shell.execute_reply.started":"2022-10-29T08:48:39.546202Z","shell.execute_reply":"2022-10-29T08:48:39.800954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Avg Sales vs Oil Prices <a class=\"anchor\"  id=\"section4.2\"></a>\n\nHigher oil prices tend to make production more expensive for businesses, just as they make it more expensive for households to do the things they normally do. It turns out that oil and gasoline prices are indeed very closely related.\n\nI belive that Favorita Supermarkets chain have impacted from daily oil prices on thier sales,so as what I did at the last section, I will merge oil dataset with train dataset to get insights.","metadata":{}},{"cell_type":"code","source":"## Merging and Filling nans:\ntrain_eda=pd.merge(left=train_eda,right=oil,left_index=True,right_index=True,how='left')\n\ntrain_eda.rename({'dcoilwtico':'oil_price'},axis=1,inplace=True)\n\ntrain_eda['oil_price']=train_eda['oil_price'].fillna(method='ffill').fillna(method='bfill').astype('float16')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:39.803135Z","iopub.execute_input":"2022-10-29T08:48:39.803394Z","iopub.status.idle":"2022-10-29T08:48:40.085118Z","shell.execute_reply.started":"2022-10-29T08:48:39.80337Z","shell.execute_reply":"2022-10-29T08:48:40.083743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_sales = train_eda.groupby(['date','oil_price'])['sales'].mean().reset_index() ## Grouping by oil_price\n\nprint('The correlation between average sales & oil price is: ',\n      np.round(avg_sales[['sales','oil_price']].corr().iloc[0,1],4),'\\n')\n\n# Plotting\nsns.regplot(data=avg_sales, x='oil_price', y='sales', scatter_kws={'color':'0.4'},\n           line_kws={'color':'red', 'linewidth':3})\nplt.xlabel('Oil Price')\nplt.ylabel('Avg. Sales')\nplt.title('Avg. Sales vs. Oil Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:40.086353Z","iopub.execute_input":"2022-10-29T08:48:40.086739Z","iopub.status.idle":"2022-10-29T08:48:40.56587Z","shell.execute_reply.started":"2022-10-29T08:48:40.086702Z","shell.execute_reply":"2022-10-29T08:48:40.564385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:**\n\n\n* At a consumer level, lower oil prices means more purchasing power for the customers. This explains why there's an increase in average sales since mid-2015.\n\n* The chart above clearly tells us that when there are lower oil prices the average units sold increases. Therefore, oil prices will be used as a variable for training.","metadata":{}},{"cell_type":"markdown","source":" # 5. STORES ANALYSIS <a class=\"anchor\"  id=\"section5\"></a>","metadata":{}},{"cell_type":"code","source":"display(stores.info())\ninfo(stores)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-29T08:48:40.567304Z","iopub.execute_input":"2022-10-29T08:48:40.5677Z","iopub.status.idle":"2022-10-29T08:48:40.59181Z","shell.execute_reply.started":"2022-10-29T08:48:40.567664Z","shell.execute_reply":"2022-10-29T08:48:40.590888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_sales=train_eda.groupby('store_nbr')['sales'].mean().reset_index()\n\nstores=stores.merge(avg_sales,on='store_nbr',how='left')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:40.592999Z","iopub.execute_input":"2022-10-29T08:48:40.593478Z","iopub.status.idle":"2022-10-29T08:48:40.649293Z","shell.execute_reply.started":"2022-10-29T08:48:40.593449Z","shell.execute_reply":"2022-10-29T08:48:40.648347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,16))\n\nplt.subplot(2,2,1) ## Avg. Sales by State Chart\nstate_grouped=stores.groupby('state')['sales'].mean().sort_values(ascending=False)\nsns.barplot(x=state_grouped.index, y=state_grouped.values,\n           ci=None, order=list(state_grouped.index))\nplt.xlabel('')\nplt.xticks(rotation=45)\nplt.ylabel('Avg. Sales')\nplt.title('Avg. Sales by State')\n\nplt.subplot(2,2,2)\ntype_grouped=stores.groupby('type')['sales'].mean().sort_values(ascending=False)\nplt.pie(type_grouped, labels=type_grouped.index, autopct=\"%1.1f%%\")\nplt.axis('equal')\nplt.title('Avg. Sales by Store Type')\n\nplt.subplot(2,2,3)\ncluster_grouped=stores.groupby('cluster')['sales'].mean().sort_values(ascending=False)\nplt.pie(cluster_grouped, labels=cluster_grouped.index, autopct=\"%1.1f%%\")\nplt.axis('equal')\nplt.title('Avg. Sales by Store Cluster')\n\nplt.subplot(2,2,4)\ncity_grouped=stores.groupby('city')['sales'].mean().sort_values(ascending=False)\nsns.barplot(x=city_grouped.index, y=city_grouped.values,\n           ci=None, order=list(city_grouped.index))\nplt.xlabel('')\nplt.xticks(rotation=45)\nplt.ylabel('Avg. Sales')\nplt.title('Avg. Sales by City')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:40.653112Z","iopub.execute_input":"2022-10-29T08:48:40.653405Z","iopub.status.idle":"2022-10-29T08:48:41.597329Z","shell.execute_reply.started":"2022-10-29T08:48:40.65338Z","shell.execute_reply":"2022-10-29T08:48:41.596061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:**\n\n1. The best States sells are ( Pichincha, Tungurahua, Loja ), while the worst are ( Cotopaxi, Manabi, Pastaza ).\n\n2. The best Cities sells are ( Quito, Cayambe, Ambato ), while the worst are ( Playas, Manta, Puyo ).\n\n3. The best Store Clusters sells are (5, 14, 8, 11), while the worst are (7, 3, 15, 16).\n\n4. The order of best Store Type is: A -> D -> B -> E -> C.\n\nAll of these variable have impact on Favorita Supermarkets sales, So thew will be used in trainig.","metadata":{}},{"cell_type":"code","source":"## Converting dtypes for stores variables to category for simplification.\nstores['type']=pd.Categorical(stores['type'], categories=type_grouped.index[::-1], ordered=True)\nstores['cluster']=pd.Categorical(stores['cluster'], categories=cluster_grouped.index[::-1], ordered=True)\nstores['state']=pd.Categorical(stores['state'], categories=state_grouped.index[::-1], ordered=True)\nstores['city']=pd.Categorical(stores['city'], categories=city_grouped.index[::-1], ordered=True)\nstores.drop('sales', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:41.59863Z","iopub.execute_input":"2022-10-29T08:48:41.598935Z","iopub.status.idle":"2022-10-29T08:48:41.610997Z","shell.execute_reply.started":"2022-10-29T08:48:41.598907Z","shell.execute_reply":"2022-10-29T08:48:41.609251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # 6. TRANSACTIONS <a class=\"anchor\"  id=\"section6\"></a>\n \nTransactions means how many people came to the store or how many invoices created in a day.\n\nThis feature is highly correlated with sales.","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Transactions Dataset Preview <a class=\"anchor\"  id=\"section6.1\"></a>\n\nSales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n\nThat's why, transactions will be one of the relevant features in the model. In the following sections, we will generate new features by using transactions.","metadata":{}},{"cell_type":"code","source":"display(transactions.info())\ninfo(transactions)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-29T08:48:41.612315Z","iopub.execute_input":"2022-10-29T08:48:41.612644Z","iopub.status.idle":"2022-10-29T08:48:41.652335Z","shell.execute_reply.started":"2022-10-29T08:48:41.612613Z","shell.execute_reply":"2022-10-29T08:48:41.651072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Total Sales vs Transactions <a class=\"anchor\"  id=\"section6.2\"></a>","metadata":{}},{"cell_type":"code","source":"avg_sales = train_eda.groupby(['date','store_nbr'])['sales'].sum().reset_index() ## Grouping by store_nbr\n\ntemp = pd.merge(avg_sales, transactions, how = \"left\") ## Merging \n\nprint('The correlation between total sales & transactions is: ',\n      np.round(temp[['sales','transactions']].corr().iloc[0,1],4),'\\n')\n\n# Plotting\nsns.regplot(data=temp,x='transactions',y='sales',ci=None,\n            scatter_kws={'color':'0.4'},line_kws={'color':'red','linewidth':3})\nplt.xlabel('Transactions')\nplt.ylabel('Total. Sales')\nplt.title('Total. Sales vs Transactions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:41.653961Z","iopub.execute_input":"2022-10-29T08:48:41.65455Z","iopub.status.idle":"2022-10-29T08:48:42.166711Z","shell.execute_reply.started":"2022-10-29T08:48:41.654512Z","shell.execute_reply":"2022-10-29T08:48:42.165462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's take a look at transactions by store total sales!**\n","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=temp,x='date',y='transactions',hue='store_nbr')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:48:42.168544Z","iopub.execute_input":"2022-10-29T08:48:42.168871Z","iopub.status.idle":"2022-10-29T08:48:45.459998Z","shell.execute_reply.started":"2022-10-29T08:48:42.168842Z","shell.execute_reply":"2022-10-29T08:48:45.458954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a stable pattern in Transaction. All months are similar except December from 2013 to 2017.\n\nStore sales had always increased at the end of the year.\n\nTransactions will be used in training as well.","metadata":{}},{"cell_type":"markdown","source":"# **NOTEBOOK WILL BE UPDATED AS SOON AS POSSIBLE!**\n\n**Don't forget give an upvote, if you liked it :)**","metadata":{}}]}