{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **INTRODUCTION**","metadata":{}},{"cell_type":"markdown","source":"This notebook is a supplement for my notebook at below URL that was for Exploratory Data Analysis of Favorita Supermarket chain Dataset.\n\nhttps://www.kaggle.com/code/adnanshikh/listen-to-secrets-in-your-data\n\nIn this notebook, I will try to demonstrate the machine learning models creation that can predict target sales for Favorita Supermarket chain with mean squared log error equal to: **0.564**\n\nI hope this notebook to be a referance for ML engineers, and it will be my plessure to have feedback from you.\n\nSo, Let's get strating..","metadata":{}},{"cell_type":"markdown","source":"# **IMPORTING PACKAGES**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n\nfrom warnings import simplefilter\nsimplefilter('ignore')\n\n## Set Plot Parameters\nsns.set(color_codes=True)        \nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 5))\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)\nplot_params = dict(color=\"0.75\", style=\".-\", markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", legend=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:46:14.278859Z","iopub.execute_input":"2022-11-09T12:46:14.279235Z","iopub.status.idle":"2022-11-09T12:46:14.2894Z","shell.execute_reply.started":"2022-11-09T12:46:14.279206Z","shell.execute_reply":"2022-11-09T12:46:14.288015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **IMPORTING DATASETS**","metadata":{}},{"cell_type":"code","source":"path='../input/store-sales-time-series-forecasting/'  ## Path of datasets\n\n## Train & Test Datasets\ntrain=pd.read_csv(path+'train.csv',parse_dates=['date'],\n                  dtype={'store_nbr':'int8', 'family':'category', 'sales':'float32',\n                        'onpromotion':'int16','id':'int32'} ,infer_datetime_format= True)\n\n\ntest=pd.read_csv(path+'test.csv',parse_dates=['date'],\n                 dtype={'store_nbr':'int8', 'family':'category','onpromotion':'int16','id':'int32'},\n                 infer_datetime_format= True)\n\n## Supplementary Datasets\noil=pd.read_csv(path+'oil.csv',parse_dates=['date'], infer_datetime_format= True)\nholidays_events=pd.read_csv(path+'holidays_events.csv',parse_dates=['date'], infer_datetime_format= True)\n\n## Train Dataset conversion from large to wide \n## Because of prediction is for only 16 days, I'll use data from 2017 only\nstore_sales = train.drop('id',axis=1).set_index(['store_nbr','family','date'])\n\nfamily_sales = store_sales.drop('onpromotion', axis=1).unstack(['store_nbr','family']).loc['2017']\nfamily_sales.index = family_sales.index.to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:46:14.304651Z","iopub.execute_input":"2022-11-09T12:46:14.30503Z","iopub.status.idle":"2022-11-09T12:46:17.200184Z","shell.execute_reply.started":"2022-11-09T12:46:14.305Z","shell.execute_reply":"2022-11-09T12:46:17.199287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CREATE SEASONAL FEATURES**\n\nWe saw from EDA noteboke that sales time series exhibits quadratic trend and both of weekly & annual seasonality.\n\nHere I selected only 2017 year for prediction of 16 days, so I will use DeterministicProcess and CalendarFourier to create:\n\n- Indicators for weekly seasons.\n\n- Linear order of trend.\n\n- Fourier features of order 4 for monthly seasons.","metadata":{}},{"cell_type":"code","source":"y = family_sales.loc(axis=1)['sales']\nfourier = CalendarFourier(freq='M', order=4) ## Fourier features of order 4 for monthly seasons\ndp =  DeterministicProcess(index = y.index, seasonal = True,  \n                          order = 1, additional_terms = [fourier], drop = True)\nX_train_1 = dp.in_sample()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:46:17.202039Z","iopub.execute_input":"2022-11-09T12:46:17.202453Z","iopub.status.idle":"2022-11-09T12:46:17.219082Z","shell.execute_reply.started":"2022-11-09T12:46:17.202421Z","shell.execute_reply":"2022-11-09T12:46:17.218122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **HOLIDAY EFFECT**\n\nAs found from my EDA notebook, there is a high correlation between the target varibale \"sales\" and holidays & weekends.\n\nFor remeber weekends and holidays are positively correlated with the sales.\n\nI will bring these variables to the model building, so high accurate sales prediction can be acheived. ","metadata":{}},{"cell_type":"code","source":"## Drop Transferred & Workday Holidays\nholidays_events= holidays_events.loc[(holidays_events.transferred==False) & (holidays_events.type != 'Work Day') & (holidays_events.locale != 'Local')]\n\n## Drop Duplicated Holiday Days:\nholidays_events.drop_duplicates('date', inplace=True)\nholidays_events = holidays_events[['date','type']] ## Keep date & holiday type for merging\n\n## Merging and Mapping:\nX_train_1.index = X_train_1.index.to_timestamp()\nX_train_1=X_train_1.reset_index().merge(holidays_events,on='date',how='left')\nX_train_1.rename({'type':'is_holiday'}, axis=1, inplace=True)\nX_train_1['is_holiday']=X_train_1.is_holiday.map({'Holiday':1, 'Transfer':1, 'Additional':1,\n                                                      'Bridge':1, 'Event':1})\nX_train_1['is_holiday']=X_train_1['is_holiday'].fillna(0).astype('int8')\n\n## Adding weekends to holiday as well\nX_train_1['day_of_week'] = X_train_1.date.dt.day_of_week.astype('int8')\nX_train_1.loc[(X_train_1['day_of_week']==5) | (X_train_1['day_of_week']==6), 'is_holiday'] = 1\nX_train_1.drop('day_of_week',axis=1, inplace=True)\n\n## Removing the first day of a year from holidays.\nX_train_1['start_of_year']= (X_train_1.date.dt.dayofyear ==1)\nX_train_1.loc[X_train_1['start_of_year']==True, 'is_holiday']=0\nX_train_1['start_of_year'] = X_train_1['start_of_year'].astype('int8')\n\nX_train_1 = X_train_1.set_index('date')\nX_train_1.index = X_train_1.index.to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:46:17.220274Z","iopub.execute_input":"2022-11-09T12:46:17.22121Z","iopub.status.idle":"2022-11-09T12:46:17.249482Z","shell.execute_reply.started":"2022-11-09T12:46:17.221173Z","shell.execute_reply":"2022-11-09T12:46:17.248034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **HYPRID MODELS**\n\nLinear regression excels at extrapolating trends, but can't learn interactions. CATBoost excels at learning interactions, but can't extrapolate trends. In the next codes, I'll create \"hybrid\" forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other.","metadata":{}},{"cell_type":"markdown","source":"# **COMPONENTS AND RESIDUALS**\n\nSo that we can design effective hybrids, we need a better understanding of how time series are constructed. Many time series can be closely described by an additive model of just three components (Seasons, trend, and cycles) plus some essentially unpredictable, entirely random error.\n\nseries = trend + seasons + cycles + error\n\nThe residuals of a model are the difference between the target the model was trained on and the predictions the model makes.\n\nAdd together all the components we learned and we get the complete model. This is essentially what linear regression would do if you trained it on a complete set of features modeling trend, seasons, and cycles.","metadata":{}},{"cell_type":"markdown","source":"# **HYBRID FORCASTING WITH RESIDUALS**\n\nIt's possible to use one algorithm for some of the components and another algorithm for the rest. This way we can always choose the best algorithm for each component. To do this, we use one algorithm to fit the original series and then the second algorithm to fit the residual series.\n\nIn detail, the process is this:\n\n1. Train and predict with first model\n\nmodel_1.fit(X_train_1, y_train)\n\ny_pred_1 = model_1.predict(X_train)\n\n2. Train and predict with second model on residuals\n\nmodel_2.fit(X_train_2, y_train - y_pred_1)\n\ny_pred_2 = model_2.predict(X_train_2)\n\n3. Add to get overall predictions\n\ny_pred = y_pred_1 + y_pred_2","metadata":{}},{"cell_type":"markdown","source":"# **FIT/PREDICT DEFINITIONS FOR HYBRID MODELS**","metadata":{}},{"cell_type":"code","source":"def fit(model_1, model_2, X_train_1, X_train_2, y):\n\n    \"\"\"\n    Defination for hyprid models fitting.\n    Inputs: \n    -   modeal_1: the 1st model.\n    -   model_2: the 2nd model.\n    -   X_train_1: training data for model_1.\n    -   X_train_2: trainng data fro model_2.\n    -   y: target of training data of model_1.\n    Output:\n    -   Hyprid models fitting.\n    \"\"\"\n    \n    model_1.fit(X_train_1,y)\n\n    # Make predictions with model_1\n    y_fit = pd.DataFrame(model_1.predict(X_train_1), index=X_train_1.index, columns=y.columns)\n\n    y_resid = y - y_fit # compute residuals\n    y_resid = y_resid.unstack().reset_index(['store_nbr','family','date']) # wide to long\n    y_resid['date'] = y_resid['date'].dt.to_timestamp()\n    y_resid = y_resid.set_index(['store_nbr','family','date'])\n    y_resid.rename({0:'residuals'},axis=1,inplace=True)\n    \n    # re-order the index of y_resid as the index of X_train_2\n    new_index=X_train_2.copy()\n    new_index['family']=le.inverse_transform(new_index['family']) # inverse transform label encoder for family\n    new_index=new_index.reset_index().set_index(['store_nbr','family','date'])\n    y_resid = y_resid.reindex(new_index.index)\n    \n    y_resid.reset_index(['store_nbr','family'], drop=True, inplace=True) # drop unneccessary varibales\n    \n    model_2.fit(X_train_2, y_resid)\n\ndef predict(model_1, model_2, X_train_1, X_train_2):\n    \n    \"\"\"\n    Defination for hyprid models prediction.\n    Inputs: \n    -   modeal_1: the 1st model (fiited).\n    -   model_2: the 2nd model (fiited).\n    -   X_train_1: training data for model_1.\n    -   X_train_2: trainng data fro model_2.\n    Output:\n    -   y_pred: final prediction of hyprid models.\n    \"\"\"\n    \n    global y \n    y_pred = pd.DataFrame(model_1.predict(X_train_1), index=X_train_1.index, columns=y.columns) # prediction of 1st model\n\n    y_pred = y_pred.unstack().reset_index(['store_nbr','family','date']) # wide to long\n    y_pred['date'] = y_pred['date'].dt.to_timestamp()\n    y_pred = y_pred.set_index(['store_nbr','family','date'])\n    y_pred.rename({0:'sales'},axis=1,inplace=True)\n    \n    # re-order the index of y_pred as the index of X_train_2\n    new_index=X_train_2.copy()\n    new_index['family']=le.inverse_transform(new_index['family']) # inverse transform label encoder for family\n    new_index=new_index.reset_index().set_index(['store_nbr','family','date'])\n    y_pred = y_pred.reindex(new_index.index)\n    \n    values = y_pred['sales'].values\n    sales = model_2.predict(X_train_2) + values\n    y_pred['sales'] = sales.clip(0.0)  ## clipping values < 0 to 0.0\n    \n    return y_pred ","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:46:17.252179Z","iopub.execute_input":"2022-11-09T12:46:17.252559Z","iopub.status.idle":"2022-11-09T12:46:17.265825Z","shell.execute_reply.started":"2022-11-09T12:46:17.252526Z","shell.execute_reply":"2022-11-09T12:46:17.264917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **HYPRID MODELS TRAINING/PREDICTION**","metadata":{}},{"cell_type":"code","source":"model_1 = LinearRegression() \n\nmodel_2 = CatBoostRegressor(silent= True) ## by default hyperparameter\n\n# X_train_2: Features for CatBoost\nX_train_2 = train.drop(['id','sales'],axis=1).set_index('date').loc['2017']\n\n# Label encoding for 'family'\nle = LabelEncoder()\nX_train_2['family'] = le.fit_transform(X_train_2['family'])\n\nX_train_2['start_of_year']= (X_train_2.index.dayofyear ==1).astype('int8')\n\n\nfit(model_1, model_2, X_train_1, X_train_2, y)\n\ny_pred= predict(model_1, model_2, X_train_1, X_train_2)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:46:17.267908Z","iopub.execute_input":"2022-11-09T12:46:17.268379Z","iopub.status.idle":"2022-11-09T12:46:52.886641Z","shell.execute_reply.started":"2022-11-09T12:46:17.268334Z","shell.execute_reply":"2022-11-09T12:46:52.885434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **HYPER MODELS EVALUATION**\n\nWe can look to the diagram of the true average sales vs. the predicted ones, as determine if our hyper models doing well or not.","metadata":{}},{"cell_type":"code","source":"ax=y.mean(axis=1).plot(**plot_params, label='Avg. Sales', title='True vs. Prediction')\nax=y_pred.groupby('date')['sales'].mean().plot(color='red', alpha=0.7, label='Prediction', legend=True)\nax=plt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:46:52.888081Z","iopub.execute_input":"2022-11-09T12:46:52.888445Z","iopub.status.idle":"2022-11-09T12:46:53.52693Z","shell.execute_reply.started":"2022-11-09T12:46:52.888411Z","shell.execute_reply":"2022-11-09T12:46:53.525807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TEST DATA PREPARATION**","metadata":{}},{"cell_type":"code","source":"X_test_1 = dp.out_of_sample(steps = 16) # 16 because we are predicting next 16 days\nX_test_1.index = X_test_1.index.rename('date').to_timestamp()\n\n## Merging and Mapping:\nX_test_1=X_test_1.reset_index().merge(holidays_events,on='date',how='left').set_index('date')\nX_test_1.rename({'type':'is_holiday'}, axis=1, inplace=True)\nX_test_1['is_holiday']=X_test_1.is_holiday.map({'Holiday':1, 'Transfer':1, 'Additional':1,\n                                                      'Bridge':1, 'Event':1})\nX_test_1['is_holiday']=X_test_1['is_holiday'].fillna(0).astype('int8')\n\n## Adding weekends to holiday as well\nX_test_1['day_of_week'] = X_test_1.index.day_of_week.astype('int8')\nX_test_1.loc[(X_test_1['day_of_week']==5) | (X_test_1['day_of_week']==6), 'is_holiday'] = 1\nX_test_1.drop('day_of_week',axis=1, inplace=True)\n\n## Removing the first day of a year from holidays as mentioned before Favorita is closed these days.\nX_test_1['start_of_year']= (X_test_1.index.dayofyear ==1)\nX_test_1.loc[X_test_1['start_of_year']==True, 'is_holiday']=0\nX_test_1['start_of_year'] = X_test_1['start_of_year'].astype('int8')\nX_test_1.index = X_test_1.index.to_period('D')\n\nX_test_2=test.set_index('date')\nX_test_2 = X_test_2.drop('id', axis = 1)\nX_test_2['start_of_year']= (X_test_2.index.dayofyear ==1).astype('int8')\nX_test_2['family'] = le.transform(test['family'])","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:47:44.656769Z","iopub.execute_input":"2022-11-09T12:47:44.657176Z","iopub.status.idle":"2022-11-09T12:47:44.698671Z","shell.execute_reply.started":"2022-11-09T12:47:44.657141Z","shell.execute_reply":"2022-11-09T12:47:44.697834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = predict(model_1, model_2, X_test_1, X_test_2)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:47:48.857854Z","iopub.execute_input":"2022-11-09T12:47:48.858282Z","iopub.status.idle":"2022-11-09T12:47:49.241765Z","shell.execute_reply.started":"2022-11-09T12:47:48.858233Z","shell.execute_reply":"2022-11-09T12:47:49.24085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SUBMISSION**","metadata":{}},{"cell_type":"code","source":"submission=pd.read_csv(path+'sample_submission.csv')\nsubmission['sales']=target.reset_index()['sales']\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:47:56.821381Z","iopub.execute_input":"2022-11-09T12:47:56.82183Z","iopub.status.idle":"2022-11-09T12:47:56.925525Z","shell.execute_reply.started":"2022-11-09T12:47:56.821797Z","shell.execute_reply":"2022-11-09T12:47:56.924186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T12:48:07.407912Z","iopub.execute_input":"2022-11-09T12:48:07.408352Z","iopub.status.idle":"2022-11-09T12:48:07.422317Z","shell.execute_reply.started":"2022-11-09T12:48:07.408316Z","shell.execute_reply":"2022-11-09T12:48:07.421398Z"},"trusted":true},"execution_count":null,"outputs":[]}]}