{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle getting started store sales\n\nJust for exercise purposes I did this entirely without looking at any of the discussions or other code in the contest. I'll take a look at it once I have something reasonable here","metadata":{}},{"cell_type":"code","source":"import utility as utl \n# My little collection of useful functions, updated frequently.\n# https://www.kaggle.com/code/beezus666/utility\n\nimport lightgbm as lgb\nfrom lightgbm.callback import early_stopping\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport optuna\nimport plotly.express as px\nfrom itertools import islice\nimport re\nfrom sklearn.metrics import mean_squared_log_error\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nIS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-04T14:13:06.239842Z","iopub.execute_input":"2023-10-04T14:13:06.240228Z","iopub.status.idle":"2023-10-04T14:13:11.03048Z","shell.execute_reply.started":"2023-10-04T14:13:06.240201Z","shell.execute_reply":"2023-10-04T14:13:11.028985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read in csvs and create DFs\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:        \n        print(os.path.join(dirname, filename))\n        no_ext = f'{os.path.splitext(filename)[0]}_df'\n        no_ext = no_ext.replace(\" \", \"_\")\n        no_ext = no_ext.replace(\"-\", \"_\")\n        globals()[no_ext] = pd.read_csv(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:11.03255Z","iopub.execute_input":"2023-10-04T14:13:11.032876Z","iopub.status.idle":"2023-10-04T14:13:14.366683Z","shell.execute_reply.started":"2023-10-04T14:13:11.03285Z","shell.execute_reply":"2023-10-04T14:13:14.365218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observations of DFs\nJust looking at the tables below, some quick observations:\n- Train and test cover the same stores\n- Train goes from Jan 2013 - Aug 15 2017\n- Test is from Aug 16 2017 - Aug 31 2017\n- Transactions are all transactions per store but only for the test data time period\n- Oil prices go all the way to the end of test time period though\n","metadata":{}},{"cell_type":"code","source":"dfs = %who_ls DataFrame\nfor df in dfs:     \n    utl.df_info(globals()[df], df)\n    utl.summary(globals()[df])","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:14.368058Z","iopub.execute_input":"2023-10-04T14:13:14.368503Z","iopub.status.idle":"2023-10-04T14:13:17.121743Z","shell.execute_reply.started":"2023-10-04T14:13:14.36846Z","shell.execute_reply":"2023-10-04T14:13:17.120726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transactions EDA\n\nBelow aggregates all transacitons, looking for trends. \n- Huge range around Christmas and NYE. Smaller upticks around other holidays, particularly in Early May.\n- Weekly trends shown for uptick on Saturday/Sunday and downtick for Thursday/Friday.\n\n\n## Notes from EDA\n- Yearly trends: \n    - Spikes on days leading up to Christmas and New Year's eve. \n    - Drop off on New year's day, almost zero sales on New Year's day.\n    - Smaller spikes in May 9th\n- Weekly trends:\n    - Saturday is biggest sales weekly, sunday 2nd\n    - Thurs/Friday lowest weekly sales\n- Overwhelming zeros\n    - Many zeros, try tweedie distribution?","metadata":{}},{"cell_type":"code","source":"# Ensure 'date' column is of datetime type\ntransactions_df['date'] = pd.to_datetime(transactions_df['date'])\n\n# Group by date and sum the transactions\ndaily_transactions = transactions_df.groupby('date').transactions.sum().reset_index()\n\n# Sort the dataframe by date in ascending order\ndaily_transactions = daily_transactions.sort_values(by='date', ascending=True)\n\n# Add day of the week column\ndaily_transactions['day_of_week'] = daily_transactions['date'].dt.day_name()\n\n# Create a custom hover column\ndaily_transactions['hover_text'] = daily_transactions['date'].dt.strftime('%Y-%m-%d') + \" (\" + daily_transactions['day_of_week'] + \"): \" + daily_transactions['transactions'].astype(str)\n\nfig = px.line(daily_transactions, x='date', y='transactions',\n             title='Transactions Trend Across All Stores by Date',\n             hover_name='hover_text')  # Use the custom hover column\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:17.125051Z","iopub.execute_input":"2023-10-04T14:13:17.125491Z","iopub.status.idle":"2023-10-04T14:13:19.248412Z","shell.execute_reply.started":"2023-10-04T14:13:17.12546Z","shell.execute_reply":"2023-10-04T14:13:19.247502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_train = train_df.copy()\n\n# Round the 'sales' column to the nearest integer\ncopy_train['sales'] = copy_train['sales'].round()\n\n# Filter the DataFrame based on sales \ncopy_train = copy_train[(copy_train['sales'] >= 0) & (copy_train['sales'] <= 10000)]\n\n# Create a histogram\nfig = px.histogram(copy_train, x='sales')\nfig.update_layout(\n    title='Sales aggregated and rounded',\n    xaxis_title='Number of Sales',\n    yaxis_title='Frequency'\n)\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:19.249298Z","iopub.execute_input":"2023-10-04T14:13:19.2499Z","iopub.status.idle":"2023-10-04T14:13:20.160061Z","shell.execute_reply.started":"2023-10-04T14:13:19.24987Z","shell.execute_reply":"2023-10-04T14:13:20.158199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"train_test_date = '2017-08-01' # we're training, so leave last 15 days for evaluation","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:20.161693Z","iopub.execute_input":"2023-10-04T14:13:20.162256Z","iopub.status.idle":"2023-10-04T14:13:20.169031Z","shell.execute_reply.started":"2023-10-04T14:13:20.162225Z","shell.execute_reply":"2023-10-04T14:13:20.167203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standard time series features \n\nLag and offsets are very standard time series features. The one tricky thing with this problem is that we have to predict 15 days out for the test set.\n\n### Feaures we're making:\n- Calendar time date explode features\n    - add datepart function takes dates and explodes it into different columns\n- target encode DOW\n- Target encode holidays\n","metadata":{}},{"cell_type":"code","source":"def process_df_pd(df, holidays_df, stores_df, cut_off_date, is_train = True):\n    # Create a new DataFrame with 'sales' and 'id' columns only\n    sales_df = df[['id', 'sales', 'date']].copy()    \n\n    # Explode out dates\n    df = utl.add_datepart(df, 'date', drop=False)\n    utl.df_info(df, 'Exploded dates added')\n    \n\n    # add national holidays    \n    no_transferred_national = holidays_df[(holidays_df['transferred'] == False) & (holidays_df['locale']=='National')]\n\n    no_transferred_national['date'] = pd.to_datetime(no_transferred_national['date'])\n    df['date'] = pd.to_datetime(df['date'])\n    df = pd.merge(df, no_transferred_national[['date', 'description']], on='date', how='left')\n    df.rename(columns={'description': 'national_holiday'}, inplace=True)\n\n    \n    def shift_sales(df, days):\n        # Filter data based on the cut_off_date_1 for calculating the offset sales\n        df_copy = df[df['date'] < cut_off_date].copy()\n        df_copy['date'] += pd.Timedelta(days=days)\n        df_copy.rename(columns={'sales': f'sales_{days}_days_ago'}, inplace=True)\n        return df.merge(df_copy[['store_nbr', 'family', 'date', f'sales_{days}_days_ago']], on=['store_nbr', 'family', 'date'], how='left')\n    \n    # lag features\n    for days in [7, 14, 28]:\n        df = shift_sales(df, days)\n        df[f'sales_{days}_days_ago'].fillna(method='ffill', inplace=True, limit=days)        \n        print(f'Adding {days} lags.')\n        if days == 7: utl.df_info(df, '7 days lags added') #take a look at it while processing\n\n    # Delete all values for sales starting on the cut_off_date in the original df\n    if is_train: df.loc[df['date'] >= cut_off_date, 'sales'] = np.nan\n    \n    # convert all object columns to categories\n    object_columns = utl.get_columns_by_type(df, 'object')\n    for col in object_columns: \n        df[col] = df[col].astype('category')   \n    utl.df_info(sales_df, 'sales and id only')\n    \n    return df, sales_df  # Return both the modified df and the new sales_df","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:20.171458Z","iopub.execute_input":"2023-10-04T14:13:20.172544Z","iopub.status.idle":"2023-10-04T14:13:20.200088Z","shell.execute_reply.started":"2023-10-04T14:13:20.172485Z","shell.execute_reply":"2023-10-04T14:13:20.198692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_test_date = pd.to_datetime(train_test_date)\nfeat_eng_df, sales_id_df = process_df_pd(train_df, holidays_events_df, stores_df, train_test_date)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:20.201811Z","iopub.execute_input":"2023-10-04T14:13:20.202858Z","iopub.status.idle":"2023-10-04T14:13:34.448849Z","shell.execute_reply.started":"2023-10-04T14:13:20.202822Z","shell.execute_reply":"2023-10-04T14:13:34.447236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dow_mean_encoding(df):\n    # Convert 'date' to datetime type\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Mask to exclude rows for mean calculation\n    mask = ~((df['date'].dt.month == 12) & (df['date'].dt.day >= 12) | \n             (df['date'].dt.month == 1) & (df['date'].dt.day <= 5))\n    filtered_df = df[mask]\n    \n    # Filter only rows from 2016 onwards\n    filtered_df = filtered_df[filtered_df['date'] >= '2016-01-01']\n    \n    # Aggregate the filtered dataframe and compute mean sales\n    aggregated_df = filtered_df.groupby(['store_nbr', 'family', 'Dayofweek'])['sales'].mean().reset_index()\n    aggregated_df.rename(columns={'sales': 'dow_mean_sales'}, inplace=True)\n    \n    # Merge the original DataFrame with the aggregated DataFrame\n    merged_df = pd.merge(df, aggregated_df, on=['store_nbr', 'family', 'Dayofweek'], how='left')\n\n    return merged_df, aggregated_df\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:34.4504Z","iopub.execute_input":"2023-10-04T14:13:34.451284Z","iopub.status.idle":"2023-10-04T14:13:34.459311Z","shell.execute_reply.started":"2023-10-04T14:13:34.451241Z","shell.execute_reply":"2023-10-04T14:13:34.458132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_eng_df, dow_encoded_df = dow_mean_encoding(feat_eng_df)\nutl.summary(feat_eng_df)\nutl.df_info(dow_encoded_df, \"DOW means\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:34.462785Z","iopub.execute_input":"2023-10-04T14:13:34.463932Z","iopub.status.idle":"2023-10-04T14:13:38.964871Z","shell.execute_reply.started":"2023-10-04T14:13:34.463888Z","shell.execute_reply":"2023-10-04T14:13:38.963913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_eng_df[(feat_eng_df['family'] == 'PRODUCE') & (feat_eng_df['store_nbr'] == 41)& (feat_eng_df['date'] >= '2017-07-01')].head(100)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:38.96622Z","iopub.execute_input":"2023-10-04T14:13:38.966848Z","iopub.status.idle":"2023-10-04T14:13:39.037679Z","shell.execute_reply.started":"2023-10-04T14:13:38.966816Z","shell.execute_reply":"2023-10-04T14:13:39.036458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Holiday target encodes","metadata":{}},{"cell_type":"code","source":"def holiday_mean_encoding(df):\n    # Step 1: Aggregate using mean\n    aggregated_df = df.groupby(['store_nbr', 'family', 'national_holiday'])['sales'].mean().reset_index()\n    aggregated_df.rename(columns={'sales': 'holiday_mean_sales'}, inplace=True)\n    \n    # Step 2: Merge the aggregated values with the original df\n    merged_df = pd.merge(df, aggregated_df, on=['store_nbr', 'family', 'national_holiday'], how='left')        \n    \n    # Step 3: Return both the merged DataFrame and the aggregated DataFrame\n    return merged_df, aggregated_df\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:39.039242Z","iopub.execute_input":"2023-10-04T14:13:39.039668Z","iopub.status.idle":"2023-10-04T14:13:39.047719Z","shell.execute_reply.started":"2023-10-04T14:13:39.039628Z","shell.execute_reply":"2023-10-04T14:13:39.046152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_eng_df, holiday_encoded_df = holiday_mean_encoding(feat_eng_df)\n\nutl.df_info(feat_eng_df, \"with holiday encode\")\nutl.df_info(holiday_encoded_df, \"holidays average\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:39.049006Z","iopub.execute_input":"2023-10-04T14:13:39.049484Z","iopub.status.idle":"2023-10-04T14:13:39.803697Z","shell.execute_reply.started":"2023-10-04T14:13:39.049443Z","shell.execute_reply":"2023-10-04T14:13:39.802335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_eng_df[(feat_eng_df['family'] == 'PRODUCE') & (feat_eng_df['store_nbr'] == 41)& (feat_eng_df['date'] >= '2016-07-01') & \n            pd.notna(feat_eng_df['holiday_mean_sales'])]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:39.805789Z","iopub.execute_input":"2023-10-04T14:13:39.806105Z","iopub.status.idle":"2023-10-04T14:13:39.866001Z","shell.execute_reply.started":"2023-10-04T14:13:39.806079Z","shell.execute_reply":"2023-10-04T14:13:39.864826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM model","metadata":{}},{"cell_type":"code","source":"feat_eng_df.tail()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:46.531315Z","iopub.execute_input":"2023-10-04T14:13:46.531725Z","iopub.status.idle":"2023-10-04T14:13:46.559636Z","shell.execute_reply.started":"2023-10-04T14:13:46.531695Z","shell.execute_reply":"2023-10-04T14:13:46.557963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n#    'objective': 'regression',\n    'objective': 'tweedie',\n    'metric': 'rmse',  # We will use RMSE for training, but we'll calculate RMSLE for evaluation later.\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'seed': 666\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:13:47.14354Z","iopub.execute_input":"2023-10-04T14:13:47.144211Z","iopub.status.idle":"2023-10-04T14:13:47.149393Z","shell.execute_reply.started":"2023-10-04T14:13:47.144178Z","shell.execute_reply":"2023-10-04T14:13:47.148412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dfs for train\nX_train = feat_eng_df[(feat_eng_df['date'] < train_test_date) & (feat_eng_df['date'] >= '2017-01-15')].drop(['sales', 'date', 'id'], axis = 1)\n#max_id = X_train['id'].max()\n#y_train = sales_id_df[sales_id_df['id'] <= max_id]['sales']\ny_train = feat_eng_df[(feat_eng_df['date'] < train_test_date) & (feat_eng_df['date'] >= '2017-01-15')]['sales']\n\n# Dfs for test\nX_test = feat_eng_df[(feat_eng_df['date'] >= train_test_date)].drop(['sales', 'date'], axis = 1)\nmin_id = X_test['id'].min()\nX_test.drop('id', axis=1, inplace = True)\ny_test = sales_id_df[sales_id_df['id'] >= min_id]['sales'] #need to use sales_id_df because deleted sales while targeting","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:16:43.649083Z","iopub.execute_input":"2023-10-04T14:16:43.649487Z","iopub.status.idle":"2023-10-04T14:16:43.840281Z","shell.execute_reply.started":"2023-10-04T14:16:43.649457Z","shell.execute_reply":"2023-10-04T14:16:43.839096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utl.df_info(X_test, 'X_test')","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:16:44.061465Z","iopub.execute_input":"2023-10-04T14:16:44.061906Z","iopub.status.idle":"2023-10-04T14:16:44.082623Z","shell.execute_reply.started":"2023-10-04T14:16:44.061876Z","shell.execute_reply":"2023-10-04T14:16:44.081663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:16:44.55233Z","iopub.execute_input":"2023-10-04T14:16:44.55302Z","iopub.status.idle":"2023-10-04T14:16:44.559285Z","shell.execute_reply.started":"2023-10-04T14:16:44.552987Z","shell.execute_reply":"2023-10-04T14:16:44.558137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlgb_train_data = lgb.Dataset(X_train, label=y_train)\nlgb_test_data = lgb.Dataset(X_test, label=y_test, reference=lgb_train_data)\n\nnum_round = 1000\ncallbacks = [early_stopping(stopping_rounds=50)]\n\n# should take about 40 seconds to train\nbst = lgb.train(params, lgb_train_data, num_round, valid_sets=[lgb_test_data],callbacks = callbacks)\n\ny_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\ny_pred[y_pred <= 0] = 0 #preds coming back negative...\n\n\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\nerror = rmsle(y_test, y_pred)\nrounded_error = round(error, 4)\nprint(f\"RMSLE: {rounded_error}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:16:46.980819Z","iopub.execute_input":"2023-10-04T14:16:46.98157Z","iopub.status.idle":"2023-10-04T14:17:13.522961Z","shell.execute_reply.started":"2023-10-04T14:16:46.981539Z","shell.execute_reply":"2023-10-04T14:17:13.522012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV Training log\n1. RMSLE: 1.8687 Base\n2. RMSLE: 1.8609 Removed Christmas from DOW mean sales encodes\n3. RMSLE: 0.9247 Tweedie distro - lots of 0's...\n4. RMSLE: 0.9202 removed id..\n5. RMSLE: 0.8519 Removed elapsed\n6. RMSLE: 0.9003 ~~Removed 'Is_month_end','Is_month_start', 'Is_quarter_end' ~~\n7. RMSLE: 0.8095 increased early stopping to 50. Besides improving the score... the model was unstable with early stopping = 10, the score changed (improved) when run a 2nd time.\n8. **RMSLE: 0.4884!! removed everything prior to 2016 for DOW encoding!!!**\n9. RMSLE: 0.5183 Added holiday encoding and score dropped to ~0.78, removed all training data prior to Jan 2017 and improved to new score","metadata":{}},{"cell_type":"code","source":"# Lets look at Feature importance of the model\nfeature_imp_gain = pd.DataFrame({\n    'Feature': bst.feature_name(),\n    'Importance (Gain)': bst.feature_importance(importance_type='gain')\n})\n\nfeature_imp_split = pd.DataFrame({\n    'Feature': bst.feature_name(),\n    'Importance (Split)': bst.feature_importance(importance_type='split')\n})\n\n# Sort the DataFrames by importance\nfeature_imp_gain = feature_imp_gain.sort_values(by='Importance (Gain)', ascending=False)\nfeature_imp_split = feature_imp_split.sort_values(by='Importance (Split)', ascending=False)\n\n# Create two separate plots\nfig_gain = px.bar(feature_imp_gain, \n             x='Importance (Gain)', \n             y='Feature', \n             orientation='h', \n             title='LightGBM Feature Importance (Gain)',\n             labels={'Importance (Gain)': 'Feature Importance', 'Feature': 'Feature Name'},\n             width=800, height=600)  \n\nfig_split = px.bar(feature_imp_split, \n             x='Importance (Split)', \n             y='Feature', \n             orientation='h', \n             title='LightGBM Feature Importance (Split)',\n             labels={'Importance (Split)': 'Feature Importance', 'Feature': 'Feature Name'},\n             width=800, height=600)  \n\nfig_gain.show()\nfig_split.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:18:00.247261Z","iopub.execute_input":"2023-10-04T14:18:00.247634Z","iopub.status.idle":"2023-10-04T14:18:00.356459Z","shell.execute_reply.started":"2023-10-04T14:18:00.247607Z","shell.execute_reply":"2023-10-04T14:18:00.355078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process test_df, predict and submit\n\n- Use the same defs made above for train\n    - Except for target encodes, use the resulting df\n- Predict\n    - Quick and dirty, just use model from CV\n    - When ready: \n        - reprocess train_df with whole data, retrain model\n        - reprocess train_df","metadata":{}},{"cell_type":"code","source":"holiday_encoded_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:18:22.778756Z","iopub.execute_input":"2023-10-04T14:18:22.779194Z","iopub.status.idle":"2023-10-04T14:18:22.796585Z","shell.execute_reply.started":"2023-10-04T14:18:22.779163Z","shell.execute_reply":"2023-10-04T14:18:22.795212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntest_df['date'] = pd.to_datetime(test_df['date'])\nmin_test_date = test_df['date'].min()\n\n# Concatenate train_df and test_df\n# Concatenating train and test because we want to get 7/14/28 day lags\nconcatenated_df = pd.concat([train_df, test_df], ignore_index=True)\n\n# Filter rows from train_df based on the date condition\n# date_condition = test_df['date'].min() - pd.DateOffset(days=56)\n# filtered_train_df = train_df[train_df['date'] >= date_condition]\n\n# Run the concatenated DataFrame through the process_df_pd function\nfeat_eng_test_df, sales_df_delete_me = process_df_pd(concatenated_df, holidays_events_df, stores_df, min_test_date, is_train = False)\n\n# remove all rows that weren't in training_df\nfeat_eng_test_df = feat_eng_test_df[(feat_eng_test_df['date'] >= min_test_date)]\n\n\n# get DOW and holiday encodes that were encoded during training\nfeat_eng_test_df = pd.merge(feat_eng_test_df, dow_encoded_df, on=['store_nbr', 'family', 'Dayofweek'], how='left')\nfeat_eng_test_df = pd.merge(feat_eng_test_df, holiday_encoded_df, on=['store_nbr', 'family', 'national_holiday'], how='left')\n\nutl.df_info(feat_eng_test_df, 'df for predictions')\n\n# The 'dow_mean_sales' column from dow_encoded_df has been added to feat_eng_test_df\n\n\n# # Step 4: Remove rows from train_df in the resulting DataFrame\n# final_df, sales_df_delete_me = processed_df[~processed_df.index.isin(filtered_train_df.index)]\n\n# Now, final_df contains the desired DataFrame with the specified operations applied.\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:18:23.000248Z","iopub.execute_input":"2023-10-04T14:18:23.0015Z","iopub.status.idle":"2023-10-04T14:18:37.370122Z","shell.execute_reply.started":"2023-10-04T14:18:23.00146Z","shell.execute_reply":"2023-10-04T14:18:37.368499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_eng_test_df[(feat_eng_test_df['family'] == 'PRODUCE') & (feat_eng_test_df['store_nbr'] == 9)].head(100)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:18:37.372133Z","iopub.execute_input":"2023-10-04T14:18:37.372557Z","iopub.status.idle":"2023-10-04T14:18:37.405748Z","shell.execute_reply.started":"2023-10-04T14:18:37.372527Z","shell.execute_reply":"2023-10-04T14:18:37.40458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dfs for test\nX_test_sub = feat_eng_test_df.drop(['sales', 'date', 'id'], axis = 1)\nX_test_ids = feat_eng_test_df['id']","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:18:37.40721Z","iopub.execute_input":"2023-10-04T14:18:37.4083Z","iopub.status.idle":"2023-10-04T14:18:37.419009Z","shell.execute_reply.started":"2023-10-04T14:18:37.408258Z","shell.execute_reply":"2023-10-04T14:18:37.417249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utl.df_info(X_test_sub, \"df for predict for submission\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:02:51.343767Z","iopub.execute_input":"2023-10-04T04:02:51.344798Z","iopub.status.idle":"2023-10-04T04:02:51.36271Z","shell.execute_reply.started":"2023-10-04T04:02:51.344765Z","shell.execute_reply":"2023-10-04T04:02:51.361571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnum_round = 1000\ncallbacks = [early_stopping(stopping_rounds=50)]\n\n\ny_pred_sub = bst.predict(X_test_sub, num_iteration=bst.best_iteration)\ny_pred_sub[y_pred_sub <= 0] = 0 #preds coming back negative...\n\ny_pred_sub[:20]","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:02:53.060525Z","iopub.execute_input":"2023-10-04T04:02:53.060899Z","iopub.status.idle":"2023-10-04T04:02:53.291077Z","shell.execute_reply.started":"2023-10-04T04:02:53.06087Z","shell.execute_reply":"2023-10-04T04:02:53.289953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'id': X_test_ids, 'sales': y_pred_sub}\n\n# Create a DataFrame\nsub_df = pd.DataFrame(data)\nsub_df.tail()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:02:57.009888Z","iopub.execute_input":"2023-10-04T04:02:57.01024Z","iopub.status.idle":"2023-10-04T04:02:57.023424Z","shell.execute_reply.started":"2023-10-04T04:02:57.010214Z","shell.execute_reply":"2023-10-04T04:02:57.022103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_df.tail()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:02:58.847045Z","iopub.execute_input":"2023-10-04T04:02:58.847422Z","iopub.status.idle":"2023-10-04T04:02:58.85709Z","shell.execute_reply.started":"2023-10-04T04:02:58.847394Z","shell.execute_reply":"2023-10-04T04:02:58.85581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:03:06.526004Z","iopub.execute_input":"2023-10-04T04:03:06.5264Z","iopub.status.idle":"2023-10-04T04:03:06.614791Z","shell.execute_reply.started":"2023-10-04T04:03:06.526372Z","shell.execute_reply":"2023-10-04T04:03:06.613464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Leaderboard log\n1. RMSLE: 0.51891... pretty encouraging that it's very close to my CV. Not a terrible score, didn't even put holidays in yet.\n2. RMSLE: 0.77593 added holiday encodes","metadata":{}}]}