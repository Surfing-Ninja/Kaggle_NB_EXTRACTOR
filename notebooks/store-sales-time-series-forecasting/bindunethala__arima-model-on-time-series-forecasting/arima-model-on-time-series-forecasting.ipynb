{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Introduction:**","metadata":{}},{"cell_type":"markdown","source":"Time series forecasting is a powerful technique for predicting future values of a target variable based on its past values. A time series dataset typically consists of a sequence of data points collected over time, with each data point representing a **measurement or observation** at a specific point in time.\n\nTime series forecasting is different from other types of prediction problems because it involves analyzing the temporal patterns and trends in the data. This means that the order of the data points matters, and that there may be **seasonality, trends,residuals, and other patterns** that need to be accounted for in order to make accurate predictions.\n\n**ML** models can be trained to automatically detect and account for variables that affect the target variable, such as promotions and special days, and can be used to generate accurate forecasts for a wide range of time series datasets.\n\n**AIMs**\n\nWe aim to develop a flexible and scalable framework that can be applied to a wide range of time series datasets, while accounting for variables that can affect the target variable, and examining the relationship between ensemble techniques and forecasting error.\n\nBy combining the strengths of multiple ML models and techniques, we aim to achieve better forecasting performance and gain deeper insights into the underlying patterns and trends in the data.","metadata":{}},{"cell_type":"markdown","source":"**METHODOLOGY:**","metadata":{}},{"cell_type":"markdown","source":"1. Data Preprocessing: The first step is to prepare the time series dataset for analysis.\n\n2. Stationarity Test: The next step is to test the time series for stationarity.\n\n3. Determine Order of Differencing: If the time series is found to be non-stationary, the next step is to determine the order of differencing required to achieve stationarity.\n\n4. Identify Order of AR and MA Terms: Once the time series is stationary, the next step is to identify the order of the autoregressive (AR) and moving average (MA) terms in the ARIMA model. This can be done by analyzing the autocorrelation and partial autocorrelation functions of the time series.\n\n5. Fit ARIMA Model: With the order of differencing, AR and MA terms identified, the next step is to fit the ARIMA model to the time series data.\n\n6. Model Diagnostics: After fitting the model, the next step is to evaluate its performance using diagnostic checks.\n\n7. Forecasting:The accuracy of the forecasts can be evaluated using measures such as mean absolute error (MAE) or mean squared error (MSE).\n\n8. Conclusion","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-05-05T16:43:17.293204Z","iopub.execute_input":"2023-05-05T16:43:17.293586Z","iopub.status.idle":"2023-05-05T16:43:17.304644Z","shell.execute_reply.started":"2023-05-05T16:43:17.293554Z","shell.execute_reply":"2023-05-05T16:43:17.303348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pmdarima","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-05-05T16:43:17.307117Z","iopub.execute_input":"2023-05-05T16:43:17.307684Z","iopub.status.idle":"2023-05-05T16:43:28.037303Z","shell.execute_reply.started":"2023-05-05T16:43:17.307646Z","shell.execute_reply":"2023-05-05T16:43:28.035772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pmdarima.arima import auto_arima\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:28.039245Z","iopub.execute_input":"2023-05-05T16:43:28.03972Z","iopub.status.idle":"2023-05-05T16:43:28.04756Z","shell.execute_reply.started":"2023-05-05T16:43:28.039678Z","shell.execute_reply":"2023-05-05T16:43:28.046412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\noil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\nstore = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\nholidays_events = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\ntest = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:28.050682Z","iopub.execute_input":"2023-05-05T16:43:28.051141Z","iopub.status.idle":"2023-05-05T16:43:29.988018Z","shell.execute_reply.started":"2023-05-05T16:43:28.051107Z","shell.execute_reply":"2023-05-05T16:43:29.986645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:29.991387Z","iopub.execute_input":"2023-05-05T16:43:29.991838Z","iopub.status.idle":"2023-05-05T16:43:30.005653Z","shell.execute_reply.started":"2023-05-05T16:43:29.991798Z","shell.execute_reply":"2023-05-05T16:43:30.004405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:30.007446Z","iopub.execute_input":"2023-05-05T16:43:30.007757Z","iopub.status.idle":"2023-05-05T16:43:30.02228Z","shell.execute_reply.started":"2023-05-05T16:43:30.007731Z","shell.execute_reply":"2023-05-05T16:43:30.021428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:30.023599Z","iopub.execute_input":"2023-05-05T16:43:30.024407Z","iopub.status.idle":"2023-05-05T16:43:30.040797Z","shell.execute_reply.started":"2023-05-05T16:43:30.024379Z","shell.execute_reply":"2023-05-05T16:43:30.039628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holidays_events.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:30.042202Z","iopub.execute_input":"2023-05-05T16:43:30.04254Z","iopub.status.idle":"2023-05-05T16:43:30.057543Z","shell.execute_reply.started":"2023-05-05T16:43:30.042503Z","shell.execute_reply":"2023-05-05T16:43:30.056413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":" We then merge the data into a single DataFrame:","metadata":{}},{"cell_type":"code","source":"train = train.merge(store, on ='store_nbr')\ntrain = train.merge(oil, on ='date', how='left')\nholidays_events = holidays_events.rename(columns={'type': 'holiday_type'})\ntrain = train.merge(holidays_events, on='date', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:30.062131Z","iopub.execute_input":"2023-05-05T16:43:30.062516Z","iopub.status.idle":"2023-05-05T16:43:33.411464Z","shell.execute_reply.started":"2023-05-05T16:43:30.062484Z","shell.execute_reply":"2023-05-05T16:43:33.410287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.fillna('', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:33.41291Z","iopub.execute_input":"2023-05-05T16:43:33.413874Z","iopub.status.idle":"2023-05-05T16:43:39.620381Z","shell.execute_reply.started":"2023-05-05T16:43:33.413832Z","shell.execute_reply":"2023-05-05T16:43:39.61928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:39.62155Z","iopub.execute_input":"2023-05-05T16:43:39.622299Z","iopub.status.idle":"2023-05-05T16:43:39.644557Z","shell.execute_reply.started":"2023-05-05T16:43:39.622267Z","shell.execute_reply":"2023-05-05T16:43:39.643422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis:","metadata":{}},{"cell_type":"markdown","source":"We will use store code 44, because it is the store that contains the highest number of sales. If we look at store size, we see that store 20 is in 9th place.","metadata":{}},{"cell_type":"markdown","source":"**Sales in holidays wise**","metadata":{}},{"cell_type":"markdown","source":"**calculate which store has maximum sales.**","metadata":{}},{"cell_type":"code","source":"num_stores = train['store_nbr'].nunique()\n# Print the result\nprint(\"Total number of stores:\", num_stores)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:39.645835Z","iopub.execute_input":"2023-05-05T16:43:39.646187Z","iopub.status.idle":"2023-05-05T16:43:39.671518Z","shell.execute_reply.started":"2023-05-05T16:43:39.646157Z","shell.execute_reply":"2023-05-05T16:43:39.670148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_products = train['family'].nunique()\n# Print the result\nprint(\"Total number of products:\", num_products)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:39.674324Z","iopub.execute_input":"2023-05-05T16:43:39.675223Z","iopub.status.idle":"2023-05-05T16:43:39.873183Z","shell.execute_reply.started":"2023-05-05T16:43:39.67518Z","shell.execute_reply":"2023-05-05T16:43:39.872048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnum_state = train['state'].nunique()\n# Print the result\nprint(\"Total number of states:\", num_state)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:39.874861Z","iopub.execute_input":"2023-05-05T16:43:39.875534Z","iopub.status.idle":"2023-05-05T16:43:40.057045Z","shell.execute_reply.started":"2023-05-05T16:43:39.875495Z","shell.execute_reply":"2023-05-05T16:43:40.055941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cities = train['city'].nunique()\n# Print the result\nprint(\"Total number of cities:\", num_cities)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:40.058669Z","iopub.execute_input":"2023-05-05T16:43:40.059417Z","iopub.status.idle":"2023-05-05T16:43:40.262897Z","shell.execute_reply.started":"2023-05-05T16:43:40.059379Z","shell.execute_reply":"2023-05-05T16:43:40.261668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# locale_name\nnum_locale_name = train['locale_name'].nunique()\n# Print the result\nprint(\"Total number of locale_name:\", num_locale_name)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:40.264598Z","iopub.execute_input":"2023-05-05T16:43:40.265356Z","iopub.status.idle":"2023-05-05T16:43:40.430849Z","shell.execute_reply.started":"2023-05-05T16:43:40.265307Z","shell.execute_reply":"2023-05-05T16:43:40.429679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from prettytable import PrettyTable\ntable = PrettyTable()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-05T16:43:40.432799Z","iopub.execute_input":"2023-05-05T16:43:40.433382Z","iopub.status.idle":"2023-05-05T16:43:40.439304Z","shell.execute_reply.started":"2023-05-05T16:43:40.433333Z","shell.execute_reply":"2023-05-05T16:43:40.438172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calculate the number of unique stores, products, states, cities, and locale_names\nnum_stores = train['store_nbr'].nunique()\nnum_products = train['family'].nunique()\nnum_state = train['state'].nunique()\nnum_cities = train['city'].nunique()\nnum_locale_name = train['locale_name'].nunique()\n\n# Create a pretty table to display the results\n\ntable.field_names = [\"Category\", \"Count\"]\ntable.add_row([\"Stores\", num_stores])\ntable.add_row([\"Products\", num_products])\ntable.add_row([\"States\", num_state])\ntable.add_row([\"Cities\", num_cities])\ntable.add_row([\"Locale Names\", num_locale_name])\n\n# Customize the table formatting\ntable.align[\"Category\"] = \"l\"\ntable.align[\"Count\"] = \"r\"\ntable.title = \"Summary of Train Data\"\ntable.title_style = \"bold magenta\"\ntable.header_style = \"upper\"\ntable.border_style = \"double\"\ntable.color_rows = [\"red\"]\ntable.color_border = \"blue\"\n\n# Print the table\nprint(table)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-05T16:43:40.440843Z","iopub.execute_input":"2023-05-05T16:43:40.441896Z","iopub.status.idle":"2023-05-05T16:43:41.144624Z","shell.execute_reply.started":"2023-05-05T16:43:40.441864Z","shell.execute_reply":"2023-05-05T16:43:41.143699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.groupby(['store_nbr'])['sales'].sum().sort_values(ascending=False).head()\n# calculate which store has maximum sales.\n\n# group the data by store_nbr and sum up the sales\nsales_by_store = train.groupby(\"store_nbr\")[\"sales\"].sum().sort_values(ascending=False)\n\n# create a new DataFrame from the result\nsales_table = pd.DataFrame({\n    \"store_nbr\": sales_by_store.index,\n    \"total_sales\": sales_by_store.values\n})\nprint(\"The top 5 stores sales\")\nsales_table.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:41.145723Z","iopub.execute_input":"2023-05-05T16:43:41.146455Z","iopub.status.idle":"2023-05-05T16:43:41.219018Z","shell.execute_reply.started":"2023-05-05T16:43:41.146423Z","shell.execute_reply":"2023-05-05T16:43:41.21777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Which product has highest sales and least sales**","metadata":{}},{"cell_type":"code","source":"# group the data by product family and sum up the sales\nsales_by_product = train.groupby(\"family\")[\"sales\"].sum().sort_values(ascending=False)\n\n\n# sort the table in decreasing order by sales\nprint(\"The total sales of the product is:\", sales_by_product)\n# get the name of the product with the most sales\nmost_sold_product = sales_by_product.index[0]\n\nprint(\"The product with the most sales is:\", most_sold_product)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:41.220285Z","iopub.execute_input":"2023-05-05T16:43:41.220596Z","iopub.status.idle":"2023-05-05T16:43:41.455186Z","shell.execute_reply.started":"2023-05-05T16:43:41.220569Z","shell.execute_reply":"2023-05-05T16:43:41.454142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the date column to datetime\ntrain[\"date\"] = pd.to_datetime(train[\"date\"])\n# extract the year from the date column\ntrain[\"year\"] = train[\"date\"].dt.year\n\n# group the data by year and product family and sum up the sales\nsales_by_year_and_product = train.groupby([\"year\", \"family\"])[\"sales\"].sum().reset_index()\n\n# sort the data by year and sales in descending order\nsales_by_year_and_product = sales_by_year_and_product.sort_values(by=[\"year\", \"sales\"], ascending=[True, False])\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# loop over the years and plot the top 10 products for each year\nfor year in sales_by_year_and_product[\"year\"].unique():\n    top_products = sales_by_year_and_product[sales_by_year_and_product[\"year\"] == year].head(10)\n    ax.bar(top_products[\"family\"], top_products[\"sales\"], label=str(year))\n\n# set the axis labels and legend\nax.set_xlabel(\"Product Family\")\nax.set_ylabel(\"Sales\")\nax.legend(title=\"Total Sales\", loc=\"upper right\", bbox_to_anchor=(1.2, 1))\n\n# rotate the x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# set the title and show the plot\nax.set_title(\"Top 10 Products\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:41.4565Z","iopub.execute_input":"2023-05-05T16:43:41.456897Z","iopub.status.idle":"2023-05-05T16:43:42.764123Z","shell.execute_reply.started":"2023-05-05T16:43:41.45686Z","shell.execute_reply":"2023-05-05T16:43:42.762757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a line chart\nplt.plot(train['date'],train['sales'])\n\n# Add title and axis labels\nplt.title('Sales over Time')\nplt.xlabel('Date')\nplt.ylabel('Sales')\n\n# Show the chart\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:42.765574Z","iopub.execute_input":"2023-05-05T16:43:42.765894Z","iopub.status.idle":"2023-05-05T16:43:44.257877Z","shell.execute_reply.started":"2023-05-05T16:43:42.765867Z","shell.execute_reply":"2023-05-05T16:43:44.256782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the 'date' column to a Pandas datetime object\noil['date'] = pd.to_datetime(oil['date'])\n\n# Create a line plot of the oil price over time\nplt.plot(oil['date'], oil['dcoilwtico'])\n\n# Set the plot title and axis labels\nplt.title('Oil Price Over Time')\nplt.xlabel('Date')\nplt.ylabel('Price (USD)')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:44.259229Z","iopub.execute_input":"2023-05-05T16:43:44.259539Z","iopub.status.idle":"2023-05-05T16:43:44.509182Z","shell.execute_reply.started":"2023-05-05T16:43:44.259513Z","shell.execute_reply":"2023-05-05T16:43:44.507956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" The resulting plot helps us visualize the trend and fluctuations in oil prices over time, which can provide valuable insights for analysis and decision-making.","metadata":{}},{"cell_type":"markdown","source":"We will filter all datasets with only top 5 store, since we want only top 5 stores","metadata":{}},{"cell_type":"code","source":"sales_by_store.describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:44.51503Z","iopub.execute_input":"2023-05-05T16:43:44.516111Z","iopub.status.idle":"2023-05-05T16:43:44.526371Z","shell.execute_reply.started":"2023-05-05T16:43:44.516076Z","shell.execute_reply":"2023-05-05T16:43:44.525337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['date'] = pd.to_datetime(train['date'])\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:44.527587Z","iopub.execute_input":"2023-05-05T16:43:44.528532Z","iopub.status.idle":"2023-05-05T16:43:44.620704Z","shell.execute_reply.started":"2023-05-05T16:43:44.528503Z","shell.execute_reply":"2023-05-05T16:43:44.619614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we haven't duplicate lines.","metadata":{}},{"cell_type":"code","source":"print(\"Rows: \", train.shape)\nprint(\"Remove duplicates rows:\", train.drop_duplicates().shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:44.622201Z","iopub.execute_input":"2023-05-05T16:43:44.62319Z","iopub.status.idle":"2023-05-05T16:43:50.925193Z","shell.execute_reply.started":"2023-05-05T16:43:44.623159Z","shell.execute_reply":"2023-05-05T16:43:50.924004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.groupby('date')['sales','onpromotion'].sum().reset_index()\nprint(train)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:50.926472Z","iopub.execute_input":"2023-05-05T16:43:50.927307Z","iopub.status.idle":"2023-05-05T16:43:51.041073Z","shell.execute_reply.started":"2023-05-05T16:43:50.927276Z","shell.execute_reply":"2023-05-05T16:43:51.040202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Stationarity testing and Determine Order of Differencing","metadata":{}},{"cell_type":"markdown","source":"**Testing the stationarity of a time series using the Augmented Dickey-Fuller (ADF) test from the statsmodels library:**","metadata":{}},{"cell_type":"code","source":"# define the time series as a pandas Series\nts = train['sales']","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:51.042068Z","iopub.execute_input":"2023-05-05T16:43:51.042392Z","iopub.status.idle":"2023-05-05T16:43:51.047543Z","shell.execute_reply.started":"2023-05-05T16:43:51.042365Z","shell.execute_reply":"2023-05-05T16:43:51.046379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\n# assuming your time series data is stored in a variable called 'ts'\nresult = adfuller(ts)\n\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:51.049269Z","iopub.execute_input":"2023-05-05T16:43:51.04966Z","iopub.status.idle":"2023-05-05T16:43:51.209794Z","shell.execute_reply.started":"2023-05-05T16:43:51.049625Z","shell.execute_reply":"2023-05-05T16:43:51.208219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Description**","metadata":{}},{"cell_type":"markdown","source":"The ADF statistic is a test statistic used in time series analysis to test for the presence of a unit root in a time series. A unit root implies that the series is non-stationary and its properties change over time, making it difficult to make reliable predictions.\n\nIn your case, the ADF statistic is -2.931646. This value indicates the strength of evidence against the null hypothesis of a unit root being present in the time series. The lower the ADF statistic, the stronger the evidence against the null hypothesis.\n\nThe p-value of 0.041790 is the probability of observing an ADF statistic as extreme as or more extreme than the observed value under the null hypothesis of a unit root. In other words, it tells you how likely it is that the ADF statistic is a result of random chance rather than a true indication of a non-stationary time series.\n\nIn this case, the p-value is less than 0.05, which means that we can reject the null hypothesis of a unit root with 95% confidence level, and conclude that the time series is stationary.\n\nThe critical values of the ADF test are provided for comparison with the observed ADF statistic. They indicate the threshold values that the ADF statistic needs to exceed in order to reject the null hypothesis of a unit root at different significance levels. In your case, the critical values at the 1%, 5%, and 10% levels are -3.434, -2.863, and -2.568, respectively. Since the observed ADF statistic is greater than the critical value at the 1% and 5% levels, we can reject the null hypothesis with 99% and 95% confidence, respectively.","metadata":{}},{"cell_type":"markdown","source":"<details>\n    <summary>\n        <font color = 'blue'><h1>About</h1></font>\n    </summary>\n    <p>\n\n1. **The Augmented Dickey-Fuller (ADF) Statistic:** This measures the test statistic of the ADF test. It is a negative number, and the more negative it is, the stronger the evidence for rejecting the null hypothesis (i.e., the more likely the time series is stationary).\n\n2. **p-value:** This measures the probability of observing the ADF statistic under the null hypothesis that the time series is non-stationary. The smaller the p-value, the stronger the evidence for rejecting the null hypothesis.\n\n3. **Critical Values:** These are pre-defined thresholds for the ADF statistic beyond which we can reject the null hypothesis with a certain level of confidence. There are different critical values for different levels of significance (e.g., 1%, 5%, 10%). If the ADF statistic is lower than the critical values, we can reject the null hypothesis and conclude that the time series is stationary.","metadata":{}},{"cell_type":"markdown","source":"# Determine Order of Differencing","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\n# decompose the time series\ndecomposition = seasonal_decompose(train['sales'], model='additive', period=12)\n\n# create a seasonal plot\nfig, axes = plt.subplots(nrows=4, ncols=1, figsize=(10,20))\nfig.subplots_adjust(hspace=0.5) # added space between subplots\n\ndecomposition.observed.plot(ax=axes[0], legend=False)\naxes[0].set_ylabel('Observed')\naxes[0].set_title('Observed Seasonal Plot')\n\ndecomposition.trend.plot(ax=axes[1], legend=False)\naxes[1].set_ylabel('Trend')\naxes[1].set_title('Trend Plot')\n\ndecomposition.seasonal.plot(ax=axes[2], legend=False)\naxes[2].set_ylabel('Seasonal')\naxes[2].set_title('Seasonal Plot')\n\ndecomposition.resid.plot(ax=axes[3], legend=False)\naxes[3].set_ylabel('Residual')\naxes[3].set_title('Residual Plot')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:51.216999Z","iopub.execute_input":"2023-05-05T16:43:51.221047Z","iopub.status.idle":"2023-05-05T16:43:52.196735Z","shell.execute_reply.started":"2023-05-05T16:43:51.220956Z","shell.execute_reply":"2023-05-05T16:43:52.195987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we have some seasonal pattern in the series and a high correlation between the present and past value and we don't have white noise in the series because we have this correlation. But in autocorrelation, if one value was correlated with the present, the next value is also the present. For this, we need to look at Partial Autocorrelations, because this way we eliminate the effects of past values for the next value.","metadata":{}},{"cell_type":"markdown","source":" # Identify Order of AR and MA Terms","metadata":{}},{"cell_type":"markdown","source":" To identify the order of AR and MA terms, you can use the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots","metadata":{}},{"cell_type":"markdown","source":"# Autocorrelation Function","metadata":{}},{"cell_type":"code","source":"plot_acf(train['sales'], alpha = 0.05);","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:52.197937Z","iopub.execute_input":"2023-05-05T16:43:52.198434Z","iopub.status.idle":"2023-05-05T16:43:52.660844Z","shell.execute_reply.started":"2023-05-05T16:43:52.198404Z","shell.execute_reply":"2023-05-05T16:43:52.659763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a correlation with 1 and 7 day sales in the past;","metadata":{}},{"cell_type":"markdown","source":"# Partial Autocorrelation function","metadata":{}},{"cell_type":"code","source":"plot_pacf(train['sales'], alpha = 0.05);","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:52.662343Z","iopub.execute_input":"2023-05-05T16:43:52.662641Z","iopub.status.idle":"2023-05-05T16:43:52.975893Z","shell.execute_reply.started":"2023-05-05T16:43:52.662614Z","shell.execute_reply":"2023-05-05T16:43:52.974841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we have a seasonal pattern on day 5,6 and 15, 16, that is, for every 5 days we have an increase in sales.","metadata":{}},{"cell_type":"markdown","source":"# Fit ARIMA Model","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\n\np = 1\nd = 1\nq = 1\n\ntrain_np = train['sales'].values.astype('float64')\nmodel = sm.tsa.ARIMA(train_np, order=(p, d, q))\n\n\n# Define the order of differencing, AR, and MA terms\nmodel_fit = model.fit()\n\n# Plot ACF and PACF\nplot_acf(train_np, lags=50)\nplot_pacf(train_np, lags=50)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:52.977612Z","iopub.execute_input":"2023-05-05T16:43:52.978602Z","iopub.status.idle":"2023-05-05T16:43:54.084916Z","shell.execute_reply.started":"2023-05-05T16:43:52.978568Z","shell.execute_reply":"2023-05-05T16:43:54.08376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, only dif_onpromotion has a low level of correlation with sales. We made a difference because sometimes correlation necessarily means that they are correlated.\n","metadata":{}},{"cell_type":"markdown","source":"# Model Diagnostics","metadata":{}},{"cell_type":"code","source":"train['date'] = pd.to_datetime(train['date'])\ntrain = train.set_index('date')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:54.086493Z","iopub.execute_input":"2023-05-05T16:43:54.086896Z","iopub.status.idle":"2023-05-05T16:43:54.096498Z","shell.execute_reply.started":"2023-05-05T16:43:54.086863Z","shell.execute_reply":"2023-05-05T16:43:54.095119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model diagnostics\nresiduals = pd.DataFrame(model_fit.resid, columns=['Residual'])\nresiduals.plot()\nplt.show()\n\nresiduals.plot(kind='kde')\nplt.show()\n\nprint(residuals.describe())","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:54.098071Z","iopub.execute_input":"2023-05-05T16:43:54.098621Z","iopub.status.idle":"2023-05-05T16:43:54.626224Z","shell.execute_reply.started":"2023-05-05T16:43:54.098591Z","shell.execute_reply":"2023-05-05T16:43:54.625457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the Model**","metadata":{}},{"cell_type":"code","source":"# define the order of differencing, AR, and MA terms\np = 1\nd = 1\nq = 1\n\n# extract the target variable as a numpy array\ntrain_np = train['sales'].values.astype('float64')\n\n# fit the ARIMA model\nmodel = sm.tsa.ARIMA(train_np, order=(p, d, q))\n\n# train the ARIMA model\nresults = model.fit()\n\n# print the summary of the trained model\nprint(results.summary())","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:54.627435Z","iopub.execute_input":"2023-05-05T16:43:54.627729Z","iopub.status.idle":"2023-05-05T16:43:54.87444Z","shell.execute_reply.started":"2023-05-05T16:43:54.627703Z","shell.execute_reply":"2023-05-05T16:43:54.873334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here **SARIMAX** Results means (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors)","metadata":{}},{"cell_type":"markdown","source":"# Forecasting ","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Assuming your training data is stored in a DataFrame called \"train\"\ny_train = train['sales']\nX_train = train['onpromotion']\n\n# Define and fit the SARIMAX model\nmodel = SARIMAX(y_train, exog=X_train, order=(1, 0, 1), seasonal_order=(1, 0, 1, 7))\nmodel_fit = model.fit()\n\n# Make predictions on the training data\ny_pred = model_fit.predict(start=train.index[0], end=train.index[-1], exog=X_train)\n\n# Calculate mean absolute error and mean squared error\nmae = mean_absolute_error(y_train, y_pred)\nmse = mean_squared_error(y_train, y_pred)\n\nprint('MAE:', mae)\nprint('MSE:', mse)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:54.876114Z","iopub.execute_input":"2023-05-05T16:43:54.876766Z","iopub.status.idle":"2023-05-05T16:43:59.557874Z","shell.execute_reply.started":"2023-05-05T16:43:54.876726Z","shell.execute_reply":"2023-05-05T16:43:59.556765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting the Sales for the next year","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Train ARIMA model on sales data\nmodel = SARIMAX(train['sales'], order=(2,1,2))\nmodel_fit = model.fit()\n\n# Predict sales for the next year\npredicted_sales = model_fit.forecast(steps=365)\n\n# Create a list of dates for the year 2018\ndates_2018 = pd.date_range(start='2018-01-01', end='2018-12-31')\n\n# Plot the predicted sales data for the year 2019\nplt.plot(dates_2018, predicted_sales, label='Predicted Sales')\n\n# Set the plot title and axis labels\nplt.title('Predicted Sales for 2018')\nplt.xlabel('Date')\nplt.ylabel('Sales')\n\n# Add a legend to the plot\nplt.legend()\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:43:59.559825Z","iopub.execute_input":"2023-05-05T16:43:59.560623Z","iopub.status.idle":"2023-05-05T16:44:01.63319Z","shell.execute_reply.started":"2023-05-05T16:43:59.560583Z","shell.execute_reply":"2023-05-05T16:44:01.631951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assuming your time series data is stored in a DataFrame called \"test_data\"\n# create an empty DataFrame for the submission file\nsubmission = pd.DataFrame()\n\n# add the required columns to the submission DataFrame\nsubmission['id'] = test.index\nsubmission['sales'] = np.zeros(len(test))\n\n# save the submission file as a CSV file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:44:01.634523Z","iopub.execute_input":"2023-05-05T16:44:01.63488Z","iopub.status.idle":"2023-05-05T16:44:01.684803Z","shell.execute_reply.started":"2023-05-05T16:44:01.634817Z","shell.execute_reply":"2023-05-05T16:44:01.683447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<details>\n    <summary>\n        <font color = 'green'><h1>CONCLUSION</h1></font>\n    </summary>\n    <p>\n\nFrom the above ARIMA Model we can see \n\n1) The output suggests that the model converged after 30 iterations. The final function value (F) is 13.5623.\n\n2) The machine precision used in the optimization is 2.220D-16.\n\n3) The mean absolute error (MAE) is 77501.8416, and the mean squared error (MSE) is 18170030523.2759.\n\n4) The plot shows the comparison between actual and predicted sales using an ARIMA model.It is evident that the model has captured the trend and        seasonality in the data, as the predicted sales follow a similar pattern as the actual sales. \n\n5) The overall performance of the model appears to be good, indicating that it can be used for forecasting sales in the future with a reasonable degree of accuracy.\n        \n    </p>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}