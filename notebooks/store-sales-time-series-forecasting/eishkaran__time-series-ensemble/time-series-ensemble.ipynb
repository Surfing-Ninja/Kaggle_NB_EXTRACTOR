{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Time Series Forecasting ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-12T13:19:07.301792Z","iopub.execute_input":"2023-06-12T13:19:07.303069Z","iopub.status.idle":"2023-06-12T13:19:07.339767Z","shell.execute_reply.started":"2023-06-12T13:19:07.30303Z","shell.execute_reply":"2023-06-12T13:19:07.338192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our dataset, there are 54 stores for 33 product families.\nWe need to predict the sales for each of these product families from each store. So 33 * 54 * 16 = 28,512 values to predict.","metadata":{}},{"cell_type":"markdown","source":"**Train.csv**\nMain file: train.csv. It contains some features and the label to predict sales, the number of sales per day:","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\ndisplay(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:07.63437Z","iopub.execute_input":"2023-06-12T13:19:07.634806Z","iopub.status.idle":"2023-06-12T13:19:10.235571Z","shell.execute_reply.started":"2023-06-12T13:19:07.634775Z","shell.execute_reply":"2023-06-12T13:19:10.234596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Holidays_events.csv**\nThe holidays_events.csv groups the national holidays. This information is independent of the store but can have an impact on sales.","metadata":{}},{"cell_type":"code","source":"df_holidays_events = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\ndisplay(df_holidays_events.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:10.237832Z","iopub.execute_input":"2023-06-12T13:19:10.238225Z","iopub.status.idle":"2023-06-12T13:19:10.255731Z","shell.execute_reply.started":"2023-06-12T13:19:10.238193Z","shell.execute_reply":"2023-06-12T13:19:10.253811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oil.csv**\nThen a CSV file gathers the daily oil price from January 01, 2013 to August 31, 2017:","metadata":{}},{"cell_type":"code","source":"df_oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\ndisplay(df_oil.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:10.257523Z","iopub.execute_input":"2023-06-12T13:19:10.258015Z","iopub.status.idle":"2023-06-12T13:19:10.277089Z","shell.execute_reply.started":"2023-06-12T13:19:10.257982Z","shell.execute_reply":"2023-06-12T13:19:10.275788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Store.csv**\n\nThe store.csv file gathers information about the stores. There is one store per line so 54 lines:","metadata":{}},{"cell_type":"code","source":"df_stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\ndisplay(df_stores.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:10.280299Z","iopub.execute_input":"2023-06-12T13:19:10.280681Z","iopub.status.idle":"2023-06-12T13:19:10.299001Z","shell.execute_reply.started":"2023-06-12T13:19:10.280633Z","shell.execute_reply":"2023-06-12T13:19:10.297226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transactions.csv**\n\nThe transactions.csv file groups the daily transactions by stores:","metadata":{}},{"cell_type":"code","source":"df_transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\ndisplay(df_transactions.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:10.300312Z","iopub.execute_input":"2023-06-12T13:19:10.300707Z","iopub.status.idle":"2023-06-12T13:19:10.34794Z","shell.execute_reply.started":"2023-06-12T13:19:10.300673Z","shell.execute_reply":"2023-06-12T13:19:10.347006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test.csv**\n\nFinally, the test.csv that will allow us to predict the sale column. The file starts on August 16, 2017 and ends on August 31, 2017. We also have the sample_submission.csv to fill in with the number of sales per day and per family.","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\ndf_sample_submission = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv')\ndisplay(df_test.head())\ndisplay(df_sample_submission.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:10.349174Z","iopub.execute_input":"2023-06-12T13:19:10.350732Z","iopub.status.idle":"2023-06-12T13:19:10.407997Z","shell.execute_reply.started":"2023-06-12T13:19:10.350614Z","shell.execute_reply":"2023-06-12T13:19:10.406819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing ","metadata":{}},{"cell_type":"code","source":"family_list = df_train['family'].unique()\nstore_list = df_stores['store_nbr'].unique()\ndisplay(family_list)\ndisplay(store_list)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:10.40989Z","iopub.execute_input":"2023-06-12T13:19:10.410349Z","iopub.status.idle":"2023-06-12T13:19:10.601769Z","shell.execute_reply.started":"2023-06-12T13:19:10.410308Z","shell.execute_reply":"2023-06-12T13:19:10.600771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We assemble the df_train and df_stores datasets. By grouping this data in a single dataset, it will allow us to access the information more easily. In addition to that, we sort the sales of the DataFrame by date, by family and by stores:","metadata":{}},{"cell_type":"code","source":"train_merged = pd.merge(df_train, df_stores, on ='store_nbr')\ntrain_merged = train_merged.sort_values([\"store_nbr\",\"family\",\"date\"])\ntrain_merged = train_merged.astype({\"store_nbr\":'str', \"family\":'str', \"city\":'str',\n                          \"state\":'str', \"type\":'str', \"cluster\":'str'})\n\ndisplay(train_merged.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:10.602904Z","iopub.execute_input":"2023-06-12T13:19:10.603326Z","iopub.status.idle":"2023-06-12T13:19:14.604968Z","shell.execute_reply.started":"2023-06-12T13:19:10.603299Z","shell.execute_reply":"2023-06-12T13:19:14.60371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A time series being the number of sales made per day for a family of a store, this sorting will allow us to extract them more easily.","metadata":{}},{"cell_type":"code","source":"df_test_dropped = df_test.drop(['onpromotion'], axis=1)\ndf_test_sorted = df_test_dropped.sort_values(by=['store_nbr','family'])\ndisplay(df_test_sorted.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:14.6065Z","iopub.execute_input":"2023-06-12T13:19:14.606822Z","iopub.status.idle":"2023-06-12T13:19:14.627283Z","shell.execute_reply.started":"2023-06-12T13:19:14.606799Z","shell.execute_reply":"2023-06-12T13:19:14.626363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Main Time Series**\n\nWe will use dart library to for time series processing ","metadata":{}},{"cell_type":"code","source":"!pip install darts==0.23.1 &> /dev/null","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:14.630986Z","iopub.execute_input":"2023-06-12T13:19:14.631259Z","iopub.status.idle":"2023-06-12T13:19:53.776437Z","shell.execute_reply.started":"2023-06-12T13:19:14.631238Z","shell.execute_reply":"2023-06-12T13:19:53.775607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like Pandas with its DataFrame, the Darts library offers us its class allowing to manipulate time series : the TimeSeries.\n\nWe will use these class to extract our time series.","metadata":{}},{"cell_type":"markdown","source":"**Strategy**\n\nWe have two approaches to this problem \n* The most obvious one is to train a Machine Learning model on our dataset. On the 1782 time series.This is obvious because it allows us to use a maximum of data to train our model. It will then be able to generalize its knowledge to each of the product families. With such a strategy, our model will have a good global prediction.\n* A less obvious strategy, but quite logical, is to train a Machine Learning model for each time series.Indeed, by assigning a model to each series, we ensure that each model is specialized in its task and therefore performs well in its prediction, and this for each product family of each store.\n**Problems**\n* The problem with the first method is that the model, having only a general knowledge of our data, will not have an optimal prediction on each specific time series.\n\n* The problem with the second method is that the model will be specialized on each time series, but will lack data to perfect its training.","metadata":{}},{"cell_type":"markdown","source":"**Our Strategy **\n\nOur strategy is to position ourselves between these two methods.","metadata":{}},{"cell_type":"code","source":"import darts\nfrom darts import TimeSeries\nfamily_TS_dict = {}\nfor family in family_list:\n  df_family = train_merged.loc[train_merged['family'] == family]\n\n  list_of_TS_family = TimeSeries.from_group_dataframe(\n                                df_family,\n                                time_col=\"date\",\n                                group_cols=[\"store_nbr\",\"family\"],\n                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"],\n                                value_cols=\"sales\",\n                                fill_missing_dates=True,\n                                freq='D')\n  for ts in list_of_TS_family:\n            ts = ts.astype(np.float32)\n\n  list_of_TS_family = sorted(list_of_TS_family, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n  family_TS_dict[family] = list_of_TS_family","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:19:53.777609Z","iopub.execute_input":"2023-06-12T13:19:53.777914Z","iopub.status.idle":"2023-06-12T13:20:20.128843Z","shell.execute_reply.started":"2023-06-12T13:19:53.777888Z","shell.execute_reply":"2023-06-12T13:20:20.12689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to indicate that the values of the TimeSeries must be interpreted in float32 and that the time series must be sorted by stores.","metadata":{}},{"cell_type":"code","source":"display(family_TS_dict['AUTOMOTIVE'][0])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:20:20.130288Z","iopub.execute_input":"2023-06-12T13:20:20.130668Z","iopub.status.idle":"2023-06-12T13:20:20.162278Z","shell.execute_reply.started":"2023-06-12T13:20:20.13062Z","shell.execute_reply":"2023-06-12T13:20:20.161066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We retrieve all the values indicated above: the number of sales, the date of each sale in Coordinates > date, and the dependent covariates in Attributes > static_covariates.\nWe can also see that the length of the time series is 1688. Originally it was 1684 but we added the values of the four December 25s that are missing from the dataset.\nApplying a normalization to our TimeSeries.","metadata":{}},{"cell_type":"markdown","source":"**Normalising time Series**","metadata":{}},{"cell_type":"markdown","source":"We can easily normalize a TimeSeries with the Scaler function of darts","metadata":{}},{"cell_type":"code","source":"from darts.dataprocessing import Pipeline\nfrom darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\nimport sklearn\nfamily_pipeline_dict = {}\nfamily_TS_transformed_dict = {}\nfor key in family_TS_dict:\n  train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n  static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\")\n  log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n  train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n\n  train_pipeline = Pipeline([train_filler,\n                             static_cov_transformer,\n                             log_transformer,\n                             train_scaler])\n  training_transformed = train_pipeline.fit_transform(family_TS_dict[key])\n  family_pipeline_dict[key] = train_pipeline\n  family_TS_transformed_dict[key] = training_transformed","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:20:20.163927Z","iopub.execute_input":"2023-06-12T13:20:20.16438Z","iopub.status.idle":"2023-06-12T13:20:55.372189Z","shell.execute_reply.started":"2023-06-12T13:20:20.164343Z","shell.execute_reply":"2023-06-12T13:20:55.371403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(family_TS_transformed_dict['AUTOMOTIVE'][0])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:20:55.373189Z","iopub.execute_input":"2023-06-12T13:20:55.373461Z","iopub.status.idle":"2023-06-12T13:20:55.410871Z","shell.execute_reply.started":"2023-06-12T13:20:55.373438Z","shell.execute_reply":"2023-06-12T13:20:55.409453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the sales have been normalized and that the static_covariates have been one hot encoded.\nWe now have our main time series that will allow us to train our model.","metadata":{}},{"cell_type":"markdown","source":"**Covariates**\nA covariate is a variable that helps to predict a target variable.\nThis covariate can be dependent on the target variable. For example, the type of store, type, where the sales are made. But it can also be independent. For example, the price of oil on the day of the sale of a product.\n\n**Date**\nThe first covariate we are interested in is the date.","metadata":{}},{"cell_type":"code","source":"from darts.utils.timeseries_generation import datetime_attribute_timeseries\nfull_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\nyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"year\")\nmonth = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"month\")\nday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"day\")\ndayofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofyear\")\nweekday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofweek\")\nweekofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"weekofyear\")\ntimesteps = TimeSeries.from_times_and_values(times=full_time_period,\n                                             values=np.arange(len(full_time_period)),\n                                             columns=[\"linear_increase\"])\n\ntime_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\ntime_cov = time_cov.astype(np.float32)\ndisplay(print(time_cov.components.values))\ndisplay(time_cov[100])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:20:55.412178Z","iopub.execute_input":"2023-06-12T13:20:55.412503Z","iopub.status.idle":"2023-06-12T13:20:55.484006Z","shell.execute_reply.started":"2023-06-12T13:20:55.412475Z","shell.execute_reply":"2023-06-12T13:20:55.482707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\ntime_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\ntime_cov_scaler.fit(time_cov_train)\ntime_cov_transformed = time_cov_scaler.transform(time_cov)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:20:55.485944Z","iopub.execute_input":"2023-06-12T13:20:55.486368Z","iopub.status.idle":"2023-06-12T13:20:55.516346Z","shell.execute_reply.started":"2023-06-12T13:20:55.486335Z","shell.execute_reply":"2023-06-12T13:20:55.515232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also see that a split is made between the dates before August 15, 2017 and after (dates that will be used in the prediction).","metadata":{}},{"cell_type":"markdown","source":"**Oil**\nThe price of oil is a future covariate because it is known in advance.\nCalculating the moving average allows us to remove the momentary fluctuations of a value and thus to accentuate the long-term trends.\nThe moving average is used in trading, but more generally in Time Series Analysis.","metadata":{}},{"cell_type":"code","source":"from darts.models import MovingAverage\noil = TimeSeries.from_dataframe(df_oil, \n                                time_col = 'date', \n                                value_cols = ['dcoilwtico'],\n                                freq = 'D')\n\noil = oil.astype(np.float32)\noil_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\noil_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\noil_pipeline = Pipeline([oil_filler, oil_scaler])\noil_transformed = oil_pipeline.fit_transform(oil)\noil_moving_average_7 = MovingAverage(window=7)\noil_moving_average_28 = MovingAverage(window=28)\noil_moving_averages = []\nma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\nma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\nma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\nma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\noil_moving_averages = ma_7.stack(ma_28)\ndisplay(oil_moving_averages[100])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:20:55.517541Z","iopub.execute_input":"2023-06-12T13:20:55.51787Z","iopub.status.idle":"2023-06-12T13:21:20.316514Z","shell.execute_reply.started":"2023-06-12T13:20:55.517846Z","shell.execute_reply":"2023-06-12T13:21:20.315578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Holidays**","metadata":{}},{"cell_type":"code","source":"def holiday_list(df_stores):\n    listofseries = []\n    for i in range(0,len(df_stores)):\n            df_holiday_dummies = pd.DataFrame(columns=['date'])\n            df_holiday_dummies[\"date\"] = df_holidays_events[\"date\"]\n            df_holiday_dummies[\"national_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & (df_holidays_events[\"locale\"] == \"National\")), 1, 0)\n            df_holiday_dummies[\"earthquake_relief\"] = np.where(df_holidays_events['description'].str.contains('Terremoto Manabi'), 1, 0)\n            df_holiday_dummies[\"christmas\"] = np.where(df_holidays_events['description'].str.contains('Navidad'), 1, 0)\n            df_holiday_dummies[\"football_event\"] = np.where(df_holidays_events['description'].str.contains('futbol'), 1, 0)\n            df_holiday_dummies[\"national_event\"] = np.where(((df_holidays_events[\"type\"] == \"Event\") & (df_holidays_events[\"locale\"] == \"National\") & (~df_holidays_events['description'].str.contains('Terremoto Manabi')) & (~df_holidays_events['description'].str.contains('futbol'))), 1, 0)\n            df_holiday_dummies[\"work_day\"] = np.where((df_holidays_events[\"type\"] == \"Work Day\"), 1, 0)\n            df_holiday_dummies[\"local_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & ((df_holidays_events[\"locale_name\"] == df_stores['state'][i]) | (df_holidays_events[\"locale_name\"] == df_stores['city'][i]))), 1, 0)     \n            listofseries.append(df_holiday_dummies)\n    return listofseries","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:21:20.317881Z","iopub.execute_input":"2023-06-12T13:21:20.318258Z","iopub.status.idle":"2023-06-12T13:21:20.328341Z","shell.execute_reply.started":"2023-06-12T13:21:20.318214Z","shell.execute_reply":"2023-06-12T13:21:20.326829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have a function to remove the days equal to 0 and the duplicates:","metadata":{}},{"cell_type":"code","source":" def remove_0_and_duplicates(holiday_list):\n    listofseries = []\n    for i in range(0,len(holiday_list)):\n            df_holiday_per_store = list_of_holidays_per_store[i].set_index('date')\n            df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store==0).all(axis=1)]\n            df_holiday_per_store = df_holiday_per_store.groupby('date').agg({'national_holiday':'max', 'earthquake_relief':'max', \n                                   'christmas':'max', 'football_event':'max', \n                                   'national_event':'max', 'work_day':'max', \n                                   'local_holiday':'max'}).reset_index()\n            listofseries.append(df_holiday_per_store)\n    return listofseries","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:21:20.330172Z","iopub.execute_input":"2023-06-12T13:21:20.330573Z","iopub.status.idle":"2023-06-12T13:21:20.359279Z","shell.execute_reply.started":"2023-06-12T13:21:20.330541Z","shell.execute_reply":"2023-06-12T13:21:20.357729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally a function that allows us to have the holidays associated to each of the 54 stores :","metadata":{}},{"cell_type":"code","source":"def holiday_TS_list_54(holiday_list):\n    listofseries = []\n    for i in range(0,54):\n            holidays_TS = TimeSeries.from_dataframe(list_of_holidays_per_store[i], \n                                        time_col = 'date',\n                                        fill_missing_dates=True,\n                                        fillna_value=0,\n                                        freq='D')\n            \n            holidays_TS = holidays_TS.slice(pd.Timestamp('20130101'),pd.Timestamp('20170831'))\n            holidays_TS = holidays_TS.astype(np.float32)\n            listofseries.append(holidays_TS)\n    return listofseries","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:21:20.360854Z","iopub.execute_input":"2023-06-12T13:21:20.36138Z","iopub.status.idle":"2023-06-12T13:21:20.378606Z","shell.execute_reply.started":"2023-06-12T13:21:20.361347Z","shell.execute_reply":"2023-06-12T13:21:20.377722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Applying These Functions**","metadata":{}},{"cell_type":"code","source":"list_of_holidays_per_store = holiday_list(df_stores)\nlist_of_holidays_per_store = remove_0_and_duplicates(list_of_holidays_per_store)   \nlist_of_holidays_store = holiday_TS_list_54(list_of_holidays_per_store)\nholidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\nholidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\nholidays_pipeline = Pipeline([holidays_filler, holidays_scaler])\nholidays_transformed = holidays_pipeline.fit_transform(list_of_holidays_store)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:21:20.380455Z","iopub.execute_input":"2023-06-12T13:21:20.380858Z","iopub.status.idle":"2023-06-12T13:21:21.856803Z","shell.execute_reply.started":"2023-06-12T13:21:20.380829Z","shell.execute_reply":"2023-06-12T13:21:21.856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(len(holidays_transformed))\ndisplay(holidays_transformed[0].components.values)\ndisplay(holidays_transformed[0][100])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:21:21.857708Z","iopub.execute_input":"2023-06-12T13:21:21.858036Z","iopub.status.idle":"2023-06-12T13:21:21.877822Z","shell.execute_reply.started":"2023-06-12T13:21:21.858011Z","shell.execute_reply":"2023-06-12T13:21:21.876392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Promotion**\n\nThe last future covariate to process is the onpromotion column.\n\n","metadata":{}},{"cell_type":"code","source":"df_promotion = pd.concat([df_train, df_test], axis=0)\ndf_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\ndf_promotion.tail()\nfamily_promotion_dict = {}\nfor family in family_list:\n  df_family = df_promotion.loc[df_promotion['family'] == family]\n  list_of_TS_promo = TimeSeries.from_group_dataframe(\n                                df_family,\n                                time_col=\"date\",\n                                group_cols=[\"store_nbr\",\"family\"],\n                                value_cols=\"onpromotion\",\n                                fill_missing_dates=True,\n                                freq='D')\n  for ts in list_of_TS_promo:\n    ts = ts.astype(np.float32)\n  family_promotion_dict[family] = list_of_TS_promo\ndisplay(family_promotion_dict['AUTOMOTIVE'][0])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:21:21.879273Z","iopub.execute_input":"2023-06-12T13:21:21.879687Z","iopub.status.idle":"2023-06-12T13:21:48.75492Z","shell.execute_reply.started":"2023-06-12T13:21:21.879633Z","shell.execute_reply":"2023-06-12T13:21:48.753654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s go further by calculating also the moving average in 7 and 28, like for the oil price:","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\npromotion_transformed_dict = {}\nfor key in tqdm(family_promotion_dict):\n  promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n  promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n  promo_pipeline = Pipeline([promo_filler,\n                             promo_scaler])\n  promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n  promo_moving_average_7 = MovingAverage(window=7)\n  promo_moving_average_28 = MovingAverage(window=28)\n  promotion_covs = []\n  for ts in promotion_transformed:\n    ma_7 = promo_moving_average_7.filter(ts)\n    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n    ma_7 = ma_7.astype(np.float32)\n    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n    ma_28 = promo_moving_average_28.filter(ts)\n    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n    ma_28 = ma_28.astype(np.float32)\n    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n    promo_and_mas = ts.stack(ma_7).stack(ma_28)\n    promotion_covs.append(promo_and_mas)\n  promotion_transformed_dict[key] = promotion_covs","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:21:48.756215Z","iopub.execute_input":"2023-06-12T13:21:48.756521Z","iopub.status.idle":"2023-06-12T13:22:47.168374Z","shell.execute_reply.started":"2023-06-12T13:21:48.756497Z","shell.execute_reply":"2023-06-12T13:22:47.166692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtain a normalized time series with 3 columns.","metadata":{}},{"cell_type":"code","source":"display(promotion_transformed_dict['AUTOMOTIVE'][0].components.values)\ndisplay(promotion_transformed_dict['AUTOMOTIVE'][0][1])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:47.170136Z","iopub.execute_input":"2023-06-12T13:22:47.17054Z","iopub.status.idle":"2023-06-12T13:22:47.192953Z","shell.execute_reply.started":"2023-06-12T13:22:47.170502Z","shell.execute_reply":"2023-06-12T13:22:47.191079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Grouping the covariates**\n\nTo finish with the future covariates, we are going to gather them in the same TimeSeries.\n\nWe start with the time series of the dates, the oil price and the moving averages of the oil price that we group in the variable general_covariates :","metadata":{}},{"cell_type":"code","source":"general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:47.195021Z","iopub.execute_input":"2023-06-12T13:22:47.195417Z","iopub.status.idle":"2023-06-12T13:22:47.207601Z","shell.execute_reply.started":"2023-06-12T13:22:47.195385Z","shell.execute_reply":"2023-06-12T13:22:47.205738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then for each store, we gather the TimeSeries of the holidays with the general_covariates :","metadata":{}},{"cell_type":"code","source":"store_covariates_future = []\nfor store in range(0,len(store_list)):\n  stacked_covariates = holidays_transformed[store].stack(general_covariates)  \n  store_covariates_future.append(stacked_covariates)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:47.209516Z","iopub.execute_input":"2023-06-12T13:22:47.209942Z","iopub.status.idle":"2023-06-12T13:22:47.319294Z","shell.execute_reply.started":"2023-06-12T13:22:47.209908Z","shell.execute_reply":"2023-06-12T13:22:47.317444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, for each family, we combine the previously created covariates with the promotion covariates:","metadata":{}},{"cell_type":"code","source":"future_covariates_dict = {}\nfor key in tqdm(promotion_transformed_dict):\n  promotion_family = promotion_transformed_dict[key]\n  covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n  future_covariates_dict[key] = covariates_future","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:47.324775Z","iopub.execute_input":"2023-06-12T13:22:47.325188Z","iopub.status.idle":"2023-06-12T13:22:51.670679Z","shell.execute_reply.started":"2023-06-12T13:22:47.325156Z","shell.execute_reply":"2023-06-12T13:22:51.669086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Different Columns of each family of each store","metadata":{}},{"cell_type":"code","source":"display(future_covariates_dict['AUTOMOTIVE'][0].components)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:51.672124Z","iopub.execute_input":"2023-06-12T13:22:51.672508Z","iopub.status.idle":"2023-06-12T13:22:51.681574Z","shell.execute_reply.started":"2023-06-12T13:22:51.672476Z","shell.execute_reply":"2023-06-12T13:22:51.679693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transactions – Past Covariates**\n\n\nBefore launching the training of the model, let’s extract the past covariates: the transactions.","metadata":{}},{"cell_type":"code","source":"df_transactions.sort_values([\"store_nbr\",\"date\"], inplace=True)\nTS_transactions_list = TimeSeries.from_group_dataframe(\n                                df_transactions,\n                                time_col=\"date\",\n                                group_cols=[\"store_nbr\"],\n                                value_cols=\"transactions\",\n                                fill_missing_dates=True,\n                                freq='D')\ntransactions_list = []\nfor ts in TS_transactions_list:\n            series = TimeSeries.from_series(ts.pd_series())\n            series = series.astype(np.float32)\n            transactions_list.append(series)\ntransactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\nfrom datetime import datetime, timedelta\ntransactions_list_full = []\nfor ts in transactions_list:\n  if ts.start_time() > pd.Timestamp('20130101'):\n    end_time = (ts.start_time() - timedelta(days=1))\n    delta = end_time - pd.Timestamp('20130101')\n    zero_series = TimeSeries.from_times_and_values(\n                              times=pd.date_range(start=pd.Timestamp('20130101'), \n                              end=end_time, freq=\"D\"),\n                              values=np.zeros(delta.days+1))\n    ts = zero_series.append(ts)\n    ts = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\")\n    transactions_list_full.append(ts)\ntransactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\ntransactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\ntransactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\ntransactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:51.684114Z","iopub.execute_input":"2023-06-12T13:22:51.68467Z","iopub.status.idle":"2023-06-12T13:22:53.148573Z","shell.execute_reply.started":"2023-06-12T13:22:51.684607Z","shell.execute_reply":"2023-06-12T13:22:53.147865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(transactions_transformed[0])","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:53.149381Z","iopub.execute_input":"2023-06-12T13:22:53.149594Z","iopub.status.idle":"2023-06-12T13:22:53.163154Z","shell.execute_reply.started":"2023-06-12T13:22:53.149574Z","shell.execute_reply":"2023-06-12T13:22:53.161442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's our first timeseries data for first store","metadata":{}},{"cell_type":"markdown","source":"# Machine Learning Model","metadata":{}},{"cell_type":"markdown","source":"**Training**\n\nHere are the important hyperparameters:\n\n* lags – the number of past values on which we base our predictions\n\n\n* lags_future_covariates – the number of future covariate values on which we base our \npredictions. If we give a tuple, the left value represents the number of covariates in the past and the right value represents the number of covariates in the future\n\n\n* lags_past_covariates – the number of past covariate values on which we base our predictions\n","metadata":{}},{"cell_type":"code","source":"from darts.models import LightGBMModel\nLGBM_Models_Submission = {}\nfor family in tqdm(family_list):\n  sales_family = family_TS_transformed_dict[family]\n  training_data = [ts for ts in sales_family] \n  TCN_covariates = future_covariates_dict[family]\n  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n  LGBM_Model_Submission = LightGBMModel(lags = 63,\n                                        lags_future_covariates = (14,1),\n                                        lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22],\n                                        output_chunk_length=1,\n                                        random_state=2022,\n                                        gpu_use_dp= \"false\",\n                                        )\n  LGBM_Model_Submission.fit(series=train_sliced, \n                        future_covariates=TCN_covariates,\n                        past_covariates=transactions_transformed)\n  LGBM_Models_Submission[family] = LGBM_Model_Submission","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:53.165215Z","iopub.execute_input":"2023-06-12T13:22:53.165863Z","iopub.status.idle":"2023-06-12T13:32:11.428233Z","shell.execute_reply.started":"2023-06-12T13:22:53.165829Z","shell.execute_reply":"2023-06-12T13:32:11.427341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above code, we only use lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22], why? Because during the 16th prediction (the one of August 31, 2017), the values of the past covariates from -1 to -15 are not known.\n\nAfter training, we obtain 33 Machine Learning models stored in LGBM_Models_Submission","metadata":{}},{"cell_type":"markdown","source":"# Predict\n\nWe can now perform the predictions:","metadata":{}},{"cell_type":"code","source":"LGBM_Forecasts_Families_Submission = {}\nfor family in tqdm(family_list):\n  sales_family = family_TS_transformed_dict[family]\n  training_data = [ts for ts in sales_family]\n  LGBM_covariates = future_covariates_dict[family]\n  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n  forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n                                         series=train_sliced,\n                                         future_covariates=LGBM_covariates,\n                                         past_covariates=transactions_transformed)\n  LGBM_Forecasts_Families_Submission[family] = forecast_LGBM","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:32:11.42977Z","iopub.execute_input":"2023-06-12T13:32:11.430588Z","iopub.status.idle":"2023-06-12T13:32:39.715858Z","shell.execute_reply.started":"2023-06-12T13:32:11.430557Z","shell.execute_reply":"2023-06-12T13:32:39.71447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Previously, we normalized our data with the Scaler function. So the predicted data are also normalized.\nTo de-normalize them we use the inverse_transform function on each TimeSeries:","metadata":{}},{"cell_type":"code","source":"LGBM_Forecasts_Families_back_Submission = {}\nfor family in tqdm(family_list):\n  LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:32:39.717403Z","iopub.execute_input":"2023-06-12T13:32:39.717778Z","iopub.status.idle":"2023-06-12T13:33:01.777283Z","shell.execute_reply.started":"2023-06-12T13:32:39.717719Z","shell.execute_reply":"2023-06-12T13:33:01.775869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the code that allows to go from the predicted time series cluster to the prediction DataFrame:","metadata":{}},{"cell_type":"code","source":"for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n  for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n    if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n        LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\nlistofseries = []\nfor store in tqdm(range(0,54)):\n  for family in family_list:\n      oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n      oneforecast.columns = ['fcast']\n      listofseries.append(oneforecast)\ndf_forecasts = pd.concat(listofseries) \ndf_forecasts.reset_index(drop=True, inplace=True)\ndf_forecasts[df_forecasts < 0] = 0\nforecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\nforecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\nforecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\nforecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\nforecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\nsubmission_kaggle = forecasts_kaggle_sorted","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:33:01.779192Z","iopub.execute_input":"2023-06-12T13:33:01.780293Z","iopub.status.idle":"2023-06-12T13:33:03.515054Z","shell.execute_reply.started":"2023-06-12T13:33:01.780238Z","shell.execute_reply":"2023-06-12T13:33:03.513232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Displaying the Predictions","metadata":{}},{"cell_type":"code","source":"submission_kaggle.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:33:03.516712Z","iopub.execute_input":"2023-06-12T13:33:03.51723Z","iopub.status.idle":"2023-06-12T13:33:03.531899Z","shell.execute_reply.started":"2023-06-12T13:33:03.517187Z","shell.execute_reply":"2023-06-12T13:33:03.530164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling ","metadata":{}},{"cell_type":"code","source":"model_params = [\n    {\"lags\" : 7, \"lags_future_covariates\" : (16,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]},\n    {\"lags\" : 365, \"lags_future_covariates\" : (14,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]},\n    {\"lags\" : 730, \"lags_future_covariates\" : (14,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]}\n]","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:33:03.534176Z","iopub.execute_input":"2023-06-12T13:33:03.534576Z","iopub.status.idle":"2023-06-12T13:33:03.544859Z","shell.execute_reply.started":"2023-06-12T13:33:03.534542Z","shell.execute_reply":"2023-06-12T13:33:03.543246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For each of these parameters, we will train 33 models, run the predictions and fill the final DataFrame. The 3 DataFrames obtained will be stored in the submission_kaggle_list :","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error as msle, mean_squared_error as mse\nfrom lightgbm import early_stopping\nsubmission_kaggle_list = []\nfor params in model_params:\n  LGBM_Models_Submission = {}\n  for family in tqdm(family_list):\n    sales_family = family_TS_transformed_dict[family]\n    training_data = [ts for ts in sales_family] \n    TCN_covariates = future_covariates_dict[family]\n    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n    LGBM_Model_Submission = LightGBMModel(lags = params[\"lags\"],\n                                          lags_future_covariates = params[\"lags_future_covariates\"],\n                                          lags_past_covariates = params[\"lags_past_covariates\"],\n                                          output_chunk_length=1,\n                                          random_state=2022,\n                                          gpu_use_dp= \"false\")\n    LGBM_Model_Submission.fit(series=train_sliced, \n                          future_covariates=TCN_covariates,\n                          past_covariates=transactions_transformed)\n    LGBM_Models_Submission[family] = LGBM_Model_Submission\n  LGBM_Forecasts_Families_Submission = {}\n  for family in tqdm(family_list):\n    sales_family = family_TS_transformed_dict[family]\n    training_data = [ts for ts in sales_family]\n    LGBM_covariates = future_covariates_dict[family]\n    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n    forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n                                          series=train_sliced,\n                                          future_covariates=LGBM_covariates,\n                                          past_covariates=transactions_transformed)\n    LGBM_Forecasts_Families_Submission[family] = forecast_LGBM\n  LGBM_Forecasts_Families_back_Submission = {}\n  for family in tqdm(family_list):\n    LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)\n  for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n    for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n      if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n          LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n  listofseries = []\n  for store in tqdm(range(0,54)):\n    for family in family_list:\n        oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n        oneforecast.columns = ['fcast']\n        listofseries.append(oneforecast)\n  df_forecasts = pd.concat(listofseries) \n  df_forecasts.reset_index(drop=True, inplace=True)\n  df_forecasts[df_forecasts < 0] = 0\n  forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n  forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n  forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n  forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n  forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n  submission_kaggle_list.append(forecasts_kaggle_sorted)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:33:03.546601Z","iopub.execute_input":"2023-06-12T13:33:03.547018Z","iopub.status.idle":"2023-06-12T14:36:34.652751Z","shell.execute_reply.started":"2023-06-12T13:33:03.546983Z","shell.execute_reply":"2023-06-12T14:36:34.651083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We end up with four prediction DataFrames that we will sum and average (this is the so-called ensemble method):","metadata":{}},{"cell_type":"code","source":"df_sample_submission['sales'] = (submission_kaggle[['sales']]+submission_kaggle_list[0][['sales']]+submission_kaggle_list[1][['sales']]+submission_kaggle_list[2][['sales']])/4","metadata":{"execution":{"iopub.status.busy":"2023-06-12T14:36:34.654535Z","iopub.execute_input":"2023-06-12T14:36:34.654916Z","iopub.status.idle":"2023-06-12T14:36:34.665216Z","shell.execute_reply.started":"2023-06-12T14:36:34.654881Z","shell.execute_reply":"2023-06-12T14:36:34.664191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**End Submission File**","metadata":{}},{"cell_type":"code","source":"df_sample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T14:36:34.666345Z","iopub.execute_input":"2023-06-12T14:36:34.668041Z","iopub.status.idle":"2023-06-12T14:36:34.741183Z","shell.execute_reply.started":"2023-06-12T14:36:34.668017Z","shell.execute_reply":"2023-06-12T14:36:34.740313Z"},"trusted":true},"execution_count":null,"outputs":[]}]}