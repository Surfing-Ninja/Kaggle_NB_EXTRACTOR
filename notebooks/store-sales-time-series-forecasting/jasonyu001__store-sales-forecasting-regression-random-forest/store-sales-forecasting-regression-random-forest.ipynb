{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Executive Summary\n\nThrough participation in Kaggle's competition, we applied time series forecasting to forecast store sales on data from Corporaci√≥n Favorita, a large Ecuadorian-based grocery retailer. Our goal was to build a model that could accurately predict the unit sales of items sold at different stores. An accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction.\n\nAmong the six available data files, we analyzed three of them, which are train, test, and stores. Although we did not study the effects of daily oil price or the holiday events in this project, we anticipate to spend additional time outside of this class to dive deeper for our learning and growth.\n\nIn our analysis, we explored two different models for time sereies forecasting: linear regression and random forest. Through preparing the data for linear regression, we came across some interesting insights, including sale increases on weekends, sale increases around the time the public sector is paid wages, and sale increases during November and December. We also noticed many massive decreases in sales throughout 2014 and 2015. Both of these increases and decreases could be due to store promotions, holidays, oil prices, or world events, which we were not able to investigate in our analysis.\n\nFor linear regression, we removed product families that were not being sold at that particular store, clustered the stores, and grouped the lower selling products into one product family. We investigated and removed outliers, and assessed the seasonality of the training data.\n\nFor random forest, we processed the categorical variables and removed the outliers.\n\nLinear regression proved to be inefficient and displayed exponential qualities, while random forest proved to be an easier model to implement. However, without the data processing for linear regression, we would not have been able to discover the insights that we did, as random forest is a black-box model.\n\nWe also included some final thoughts in regards to feature importance, hyperparameter optimization, and further findings on residual plots.\n\nIn conclusion, random forest was the better model for forcasting stores sales.","metadata":{}},{"cell_type":"code","source":"#This Python 3 environment comes with many helpful analytics libraries installed\n#It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n#For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Input data files are available in the read-only \"../input/\" directory\n#For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n#You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-11T21:10:10.730597Z","iopub.execute_input":"2024-03-11T21:10:10.731188Z","iopub.status.idle":"2024-03-11T21:10:11.293464Z","shell.execute_reply.started":"2024-03-11T21:10:10.731129Z","shell.execute_reply":"2024-03-11T21:10:11.292089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import Packages and Datasets","metadata":{}},{"cell_type":"code","source":"#Import packages\n#BASE\n# ------------------------------------------------------\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport warnings\n\n#Machine Learning\n# ------------------------------------------------------\nimport statsmodels.api as sm\nimport sklearn\n\n#Data Visualization\n# ------------------------------------------------------\n#import altair as alt\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nwarnings.filterwarnings('ignore')\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:11.295873Z","iopub.execute_input":"2024-03-11T21:10:11.296417Z","iopub.status.idle":"2024-03-11T21:10:14.834827Z","shell.execute_reply.started":"2024-03-11T21:10:11.296382Z","shell.execute_reply":"2024-03-11T21:10:14.833281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import datasets\ntrain = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\",parse_dates=['date'])\ntest = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\",parse_dates=['date'])\nstores = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/stores.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:14.836427Z","iopub.execute_input":"2024-03-11T21:10:14.836871Z","iopub.status.idle":"2024-03-11T21:10:20.055206Z","shell.execute_reply.started":"2024-03-11T21:10:14.836833Z","shell.execute_reply":"2024-03-11T21:10:20.053991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will check the data types for each column in the following data sets. In order to perform time series forecasting using the packages we have imported, we must ensure that the dates have been parsed as dates. Later on we will convert the \"object\" data types to categories. We will also have a look at our datasets to see if we need to do any cleaning or manipulation.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:20.057677Z","iopub.execute_input":"2024-03-11T21:10:20.058053Z","iopub.status.idle":"2024-03-11T21:10:20.082381Z","shell.execute_reply.started":"2024-03-11T21:10:20.058023Z","shell.execute_reply":"2024-03-11T21:10:20.080803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:20.083725Z","iopub.execute_input":"2024-03-11T21:10:20.084086Z","iopub.status.idle":"2024-03-11T21:10:20.105878Z","shell.execute_reply.started":"2024-03-11T21:10:20.084056Z","shell.execute_reply":"2024-03-11T21:10:20.104491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stores.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:20.107745Z","iopub.execute_input":"2024-03-11T21:10:20.108589Z","iopub.status.idle":"2024-03-11T21:10:20.122194Z","shell.execute_reply.started":"2024-03-11T21:10:20.108539Z","shell.execute_reply":"2024-03-11T21:10:20.120716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Processing","metadata":{}},{"cell_type":"markdown","source":"From train.csv and transactions.csv:\n\n* date - The date when the data was recorded.\n* store_nbr - Identifies the store at which the products are sold.\n* family - Identifies the type of product sold.\n* sales - Gives the total sales for a product family at a particular store at a given date. Fractional values are possible.\n* onpromotion - Gives the total number of items in a product family that were being promoted at a store at a given date.\n* transactions - The total number of transactions that occurred in a store on a given date.","metadata":{}},{"cell_type":"markdown","source":"## Remove Sales for Stores who do not Sell a Particular Family of Products\nAfter a quick look at our training dataset, we can see that there are a lot of zeroes. It is possible that some stores do not sell certain products as they are not the right store for that product. In that case, we will remove those values as when forecasted, they should not be having any sales.","metadata":{}},{"cell_type":"code","source":"zeros = train.groupby(['id', 'store_nbr', 'family']).sales.sum().reset_index().sort_values(['family','store_nbr'])\nzeros = zeros[zeros.sales == 0]\nzeros","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:20.123764Z","iopub.execute_input":"2024-03-11T21:10:20.124143Z","iopub.status.idle":"2024-03-11T21:10:22.644748Z","shell.execute_reply.started":"2024-03-11T21:10:20.124113Z","shell.execute_reply":"2024-03-11T21:10:22.643303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#full outer joining the tables and removing the rows where they match to get rid of the zeros\njoin = train.merge(zeros[zeros.sales == 0].drop(\"sales\",axis = 1), how='outer', indicator=True)\ntrain1 = join[~(join._merge == 'both')].drop(['id', '_merge'], axis = 1).reset_index()\ntrain1 = train1.drop(['index', 'onpromotion'], axis=1)\ntrain1","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:22.646336Z","iopub.execute_input":"2024-03-11T21:10:22.646705Z","iopub.status.idle":"2024-03-11T21:10:26.628964Z","shell.execute_reply.started":"2024-03-11T21:10:22.646676Z","shell.execute_reply":"2024-03-11T21:10:26.627673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cluster Stores\nThe training data set is what we will use to create our models. There are 54 stores, each with 33 product families. As this is a large dataset with over three million rows, grouping the stores into clusters will greatly reduce the computation of our following models. Fortunately, the stores are already clustered for us in the 'stores' dataset into seventeen clusters. We have also chosen to ignore the 'on promotion' variable for simplicity and time restraints.","metadata":{}},{"cell_type":"code","source":"def group_clusters (df) :\n    #left join train and stores on store number\n    jointr = df.merge(stores, on='store_nbr', how='left', indicator=False)\n\n    #replacing all store numbers with their cluster and grouping them by cluster\n    grouped = jointr.groupby(['date', 'cluster', 'family']).sum('sales').reset_index()\n\n    #removing columns id, store_nbr, and type as they are aggregated values with no significance\n    grouped = grouped.drop(['store_nbr'], axis=1)\n\n    return grouped","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:26.630965Z","iopub.execute_input":"2024-03-11T21:10:26.631951Z","iopub.status.idle":"2024-03-11T21:10:26.641076Z","shell.execute_reply.started":"2024-03-11T21:10:26.63192Z","shell.execute_reply":"2024-03-11T21:10:26.639327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped = group_clusters (train1)\ngrouped","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:26.64793Z","iopub.execute_input":"2024-03-11T21:10:26.648345Z","iopub.status.idle":"2024-03-11T21:10:27.494753Z","shell.execute_reply.started":"2024-03-11T21:10:26.648317Z","shell.execute_reply":"2024-03-11T21:10:27.493964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grouping Product Families\nWe will next explore the families to help us better understand which families contribute the most to the total sales of the stores, so that we can potentially group them.","metadata":{}},{"cell_type":"code","source":"#group by 'family', then sort by sales, and create a column for aggregate sales percent\ntemp = grouped.groupby('family').sum('sales').reset_index().sort_values(by='sales', ascending=False)\n\n#aggregated sales\ntemp = temp[['family','sales']]\ntemp['percent']=(temp['sales']/temp['sales'].sum())\ntemp['percent'] = temp['percent'].apply(lambda x: f'{x:.0%}')\ntemp['cumulative']=(temp['sales']/temp['sales'].sum()).cumsum()\ntemp['cumulative'] = temp['cumulative'].apply(lambda x: f'{x:.0%}')\ntemp.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:27.495933Z","iopub.execute_input":"2024-03-11T21:10:27.496708Z","iopub.status.idle":"2024-03-11T21:10:27.598203Z","shell.execute_reply.started":"2024-03-11T21:10:27.496678Z","shell.execute_reply":"2024-03-11T21:10:27.597243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above shows the families with the top five contributions to sales over all stores and dates: GROCERY I, BEVERAGES, PRODUCE, CLEANING, and DAIRY. GROCERY I contributes the most to the total sales at 32%. The top five product families account for 79% of total sales. The graph below shows a better picture of the cumulative percentage of sales for the product families.","metadata":{}},{"cell_type":"code","source":"#plot ranked category sales \nfig1 = px.bar(temp, x=\"family\",y=\"sales\",title = \"Sales\",text=\"cumulative\")\nfig1.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:27.599565Z","iopub.execute_input":"2024-03-11T21:10:27.599921Z","iopub.status.idle":"2024-03-11T21:10:29.32583Z","shell.execute_reply.started":"2024-03-11T21:10:27.599894Z","shell.execute_reply":"2024-03-11T21:10:29.325072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We decided to group the remaining product families into a family called \"OTHERS\" to reduce the number of family categories. This will make it easier for us to do regression with dummies later.","metadata":{}},{"cell_type":"code","source":"#list of the top 5 families\ntop5 = ['GROCERY I','BEVERAGES','PRODUCE','CLEANING','DAIRY']\n\n#removing the top 5 families so we can get the list of remaining families\ntmp = grouped[~grouped['family'].isin(top5)]\n\n#the list of families that we want to group into 'OTHERS'\ntmp['family'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:29.327129Z","iopub.execute_input":"2024-03-11T21:10:29.327983Z","iopub.status.idle":"2024-03-11T21:10:29.481661Z","shell.execute_reply.started":"2024-03-11T21:10:29.327953Z","shell.execute_reply":"2024-03-11T21:10:29.480408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replace the above list with 'OTHERS'\ntrainc = grouped.copy()\ntrainc['family'] = grouped['family'].replace(['AUTOMOTIVE', 'BABY CARE', 'BEAUTY', 'BOOKS', 'BREAD/BAKERY',\n       'CELEBRATION', 'DELI', 'EGGS', 'FROZEN FOODS', 'GROCERY II',\n       'HARDWARE', 'HOME AND KITCHEN I', 'HOME AND KITCHEN II',\n       'HOME APPLIANCES', 'HOME CARE', 'LADIESWEAR', 'LAWN AND GARDEN',\n       'LINGERIE', 'LIQUOR,WINE,BEER', 'MAGAZINES', 'MEATS',\n       'PERSONAL CARE', 'PET SUPPLIES', 'PLAYERS AND ELECTRONICS',\n       'POULTRY', 'PREPARED FOODS', 'SCHOOL AND OFFICE SUPPLIES',\n       'SEAFOOD'],'OTHERS')\n\nnewtrain = trainc.groupby(['date', 'cluster', 'family']).sum('sales').reset_index()\nnewtrain","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:29.483361Z","iopub.execute_input":"2024-03-11T21:10:29.484715Z","iopub.status.idle":"2024-03-11T21:10:31.034766Z","shell.execute_reply.started":"2024-03-11T21:10:29.484672Z","shell.execute_reply":"2024-03-11T21:10:31.03336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In summary, we have now removed the product families which never have sales, clustered the stores into seventeen clusters, and grouped the lower selling product families into \"OTHERS\". In doing so, we have greatly reduced the number of records in our dataset, which will improve computation and ease in our regression later.","metadata":{}},{"cell_type":"markdown","source":"# 3. Descriptive Analytics","metadata":{}},{"cell_type":"code","source":"newtrain.groupby('family').describe()['sales'].applymap(lambda x: f\"{x:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:31.036307Z","iopub.execute_input":"2024-03-11T21:10:31.037775Z","iopub.status.idle":"2024-03-11T21:10:31.149841Z","shell.execute_reply.started":"2024-03-11T21:10:31.037738Z","shell.execute_reply":"2024-03-11T21:10:31.148354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above shows a brief description of the product families. We will plot our grouped product families to determine the shape of our data and if there are any outliers.","metadata":{}},{"cell_type":"code","source":"# Create a subplot grid with 3 rows and 2 columns\nfig, axes = plt.subplots(3, 2, figsize=(12, 12))\n\n# Flatten axes for easier indexing\naxes = axes.ravel()\n\n# Unique family categories\nfamilies = newtrain['family'].unique()\n\n# Plot sales boxplots for different family categories\nfor i, family in enumerate(families):\n    filtered_data = newtrain[newtrain['family'] == family]\n    sns.boxplot(data=filtered_data, x='sales', ax=axes[i])\n    axes[i].set_xlabel('Sales')\n    axes[i].set_title(f'Boxplot of Sales for {family} Family')\n\n# Hide extra subplots\nfor j in range(len(families), len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()  # Automatically adjust subplot layout to prevent overlap\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:31.152159Z","iopub.execute_input":"2024-03-11T21:10:31.152666Z","iopub.status.idle":"2024-03-11T21:10:32.408621Z","shell.execute_reply.started":"2024-03-11T21:10:31.152622Z","shell.execute_reply":"2024-03-11T21:10:32.407208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the boxplots shown above, we can conclude that they are highly skewed. Although it is intuitive to think that grocery stores can have some high volume sales occasionally, for simplicity, we have decided to follow the boxplot as our reasoning for removing outliers.\n\nWe also note that many of the product families have zero sales. While this could be due to store closure, we cannot ignore the possibility of human errors. However, the existence of these zero sales could be pulling down the averages. As such, we will not completely remove all outliers outside of the boxplots.\n\n## Remove Outliers","metadata":{}},{"cell_type":"code","source":"#function for removing outliers\ndef remove_outliers (df) :\n    # Calculate the first quartile (Q1)\n    q1 = df.groupby('family')['sales'].transform('quantile', 0.25)\n\n    # Calculate the third quartile (Q3)\n    q3 = df.groupby('family')['sales'].transform('quantile', 0.75)\n\n    # Calculate the Interquartile Range (IQR)\n    IQR = q3 - q1\n\n    # Define the lower and upper bounds for outliers\n    lbound = q1 - 1.5 * IQR\n    ubound = q3 + 1.5 * IQR\n\n    # Filter the dataset to remove outliers\n    no_outliers = df[~((df['sales'] < lbound) | (df['sales'] > ubound))]\n    \n    return no_outliers","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:32.410491Z","iopub.execute_input":"2024-03-11T21:10:32.410874Z","iopub.status.idle":"2024-03-11T21:10:32.419513Z","shell.execute_reply.started":"2024-03-11T21:10:32.410838Z","shell.execute_reply":"2024-03-11T21:10:32.417865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_outliers = remove_outliers (newtrain)\nno_outliers","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:32.421676Z","iopub.execute_input":"2024-03-11T21:10:32.422155Z","iopub.status.idle":"2024-03-11T21:10:32.527373Z","shell.execute_reply.started":"2024-03-11T21:10:32.422117Z","shell.execute_reply":"2024-03-11T21:10:32.526034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot new boxplots for no_outliers\n\n# Create a subplot grid with 3 rows and 2 columns\nfig, axes = plt.subplots(3, 2, figsize=(12, 12))\n\n# Flatten axes for easier indexing\naxes = axes.ravel()\n\n# Unique family categories\nfamilies = no_outliers['family'].unique()\n\n# Plot sales boxplots for different family categories\nfor i, f in enumerate(families):\n    filtered = no_outliers[no_outliers['family'] == f]\n    sns.boxplot(data=filtered, x='sales', ax=axes[i])\n    axes[i].set_xlabel('Sales')\n    axes[i].set_title(f'Boxplot of Sales for {f} Family')\n\n# Hide extra subplots\nfor j in range(len(families), len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()  # Automatically adjust subplot layout to prevent overlap\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:32.529029Z","iopub.execute_input":"2024-03-11T21:10:32.530571Z","iopub.status.idle":"2024-03-11T21:10:33.68787Z","shell.execute_reply.started":"2024-03-11T21:10:32.530526Z","shell.execute_reply":"2024-03-11T21:10:33.686555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a dataset with reduced outliers.","metadata":{}},{"cell_type":"markdown","source":"# 4. Exploratory Data Analysis\nNow we can plot graphs for the top five product families and \"OTHERS\" to determine what kind of regression we should use.","metadata":{}},{"cell_type":"code","source":"wtrain = no_outliers.set_index(\"date\").groupby(\"family\").resample(\"W\").sales.sum().reset_index()\ntop6 = ['GROCERY I','BEVERAGES','PRODUCE','CLEANING','DAIRY', 'OTHERS']\ncond = wtrain['family'].isin(top6)\npx.line(wtrain[cond], x = \"date\", y = \"sales\", color = \"family\", title = \"Weekly Total Sales by Family\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:33.690261Z","iopub.execute_input":"2024-03-11T21:10:33.69068Z","iopub.status.idle":"2024-03-11T21:10:34.20568Z","shell.execute_reply.started":"2024-03-11T21:10:33.690643Z","shell.execute_reply":"2024-03-11T21:10:34.204501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows clear signs of seasonality by weeks and months, as well as a growing trend in all product families. Interestingly, we can can see two major dips in sales in 2014, as well as in 2015 and August 2017. These dips may be due to major world events, holidays, or oil prices, and although we have been provided with this data, we have chosen to ignore them for now. Perhaps they can be further investigated in the future.\n\nHowever, produce sales are exhibiting a strange pattern where it appears that it is not making any sales. This will be further investigated in the following graphs.","metadata":{}},{"cell_type":"code","source":"#sns.set(rc={\"figure.figsize\":(17, 10)}) \ndtrain = no_outliers.set_index(\"date\").groupby(\"family\").resample(\"D\").sales.sum().reset_index()\ngfig = sns.lineplot(dtrain[dtrain['family']=='GROCERY I'],x='date',y='sales').set_title('GROCERY I SALES')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:34.207185Z","iopub.execute_input":"2024-03-11T21:10:34.207633Z","iopub.status.idle":"2024-03-11T21:10:34.638595Z","shell.execute_reply.started":"2024-03-11T21:10:34.207604Z","shell.execute_reply":"2024-03-11T21:10:34.637507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows grocery I sales over all stores and dates. It clearly depicts daily seasonality and a consistent growing trend. The sales appear to be zero at the start of every year, and this may be related to a holiday or events at the beginning of the year, but we will not be investigating that in this report.","metadata":{}},{"cell_type":"code","source":"bfig = sns.lineplot(dtrain[dtrain['family']=='BEVERAGES'],x='date',y='sales').set_title('BEVERAGES SALES')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:34.640153Z","iopub.execute_input":"2024-03-11T21:10:34.640487Z","iopub.status.idle":"2024-03-11T21:10:35.048163Z","shell.execute_reply.started":"2024-03-11T21:10:34.640458Z","shell.execute_reply":"2024-03-11T21:10:35.046586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beverage sales also show a clear seasonality, but the seasonality increases along with the trend in 2014 and 2015, and appears to be stable starting in mid-2015. This could be something that we would need to investigate later. As with grocery I sales, there appears to be no sales at the start of the year.","metadata":{}},{"cell_type":"code","source":"pfig = sns.lineplot(dtrain[dtrain['family']=='PRODUCE'],x='date',y='sales').set_title('PRODUCE SALES')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:35.05053Z","iopub.execute_input":"2024-03-11T21:10:35.051349Z","iopub.status.idle":"2024-03-11T21:10:35.505654Z","shell.execute_reply.started":"2024-03-11T21:10:35.051302Z","shell.execute_reply":"2024-03-11T21:10:35.504489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Produce sales exhibit the strangest behaviour. The sales appear to very low in 2013, having multiple spikes in sales, and then decreasing again. The sales continue to be consistent with seasonality and trend starting in mid-2015.","metadata":{}},{"cell_type":"code","source":"ptrain = dtrain[(dtrain['date'].dt.year == 2013) & (dtrain['family'] == 'PRODUCE')]\npfig2013 = sns.lineplot(ptrain,x='date',y='sales').set_title('PRODUCE SALES in 2013')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:35.507636Z","iopub.execute_input":"2024-03-11T21:10:35.508301Z","iopub.status.idle":"2024-03-11T21:10:35.894852Z","shell.execute_reply.started":"2024-03-11T21:10:35.508254Z","shell.execute_reply":"2024-03-11T21:10:35.893294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the graph above, we take a closer look at the produce sales in 2013. Although the sales are low, it is still showing a clear sign of seasonality.","metadata":{}},{"cell_type":"code","source":"cfig = sns.lineplot(dtrain[dtrain['family']=='CLEANING'],x='date',y='sales').set_title('CLEANING SALES')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:35.896515Z","iopub.execute_input":"2024-03-11T21:10:35.897015Z","iopub.status.idle":"2024-03-11T21:10:36.290907Z","shell.execute_reply.started":"2024-03-11T21:10:35.896963Z","shell.execute_reply":"2024-03-11T21:10:36.289615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfig = sns.lineplot(dtrain[dtrain['family']=='DAIRY'],x='date',y='sales').set_title('DAIRY SALES')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:36.292508Z","iopub.execute_input":"2024-03-11T21:10:36.292986Z","iopub.status.idle":"2024-03-11T21:10:36.669495Z","shell.execute_reply.started":"2024-03-11T21:10:36.292927Z","shell.execute_reply":"2024-03-11T21:10:36.66833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ofig = sns.lineplot(dtrain[dtrain['family']=='OTHERS'],x='date',y='sales').set_title('OTHERS SALES')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:36.670841Z","iopub.execute_input":"2024-03-11T21:10:36.671255Z","iopub.status.idle":"2024-03-11T21:10:37.100194Z","shell.execute_reply.started":"2024-03-11T21:10:36.671217Z","shell.execute_reply":"2024-03-11T21:10:37.098884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sales for families cleaning, dairy, and others show clear daily seasonality, and a consistent trend.","metadata":{}},{"cell_type":"markdown","source":"## The following two figures simply illustrate the total sales or average for each category.","metadata":{}},{"cell_type":"code","source":"# plot a bar chart\nsales_by_family = newtrain.groupby('family')['sales'].sum().reset_index().sort_values(by='sales', ascending=False)\nsns.barplot(data=sales_by_family, x='family', y='sales').set_title(\"Sum Sales by Category\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:37.11127Z","iopub.execute_input":"2024-03-11T21:10:37.111705Z","iopub.status.idle":"2024-03-11T21:10:37.408146Z","shell.execute_reply.started":"2024-03-11T21:10:37.111671Z","shell.execute_reply":"2024-03-11T21:10:37.406839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot a bar chart\nsales_by_family2 = newtrain.groupby('family')['sales'].mean().reset_index().sort_values(by='sales', ascending=False)\nsns.barplot(data=sales_by_family2, x='family', y='sales').set_title(\"Average Sales by Category\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:37.410133Z","iopub.execute_input":"2024-03-11T21:10:37.410444Z","iopub.status.idle":"2024-03-11T21:10:37.685225Z","shell.execute_reply.started":"2024-03-11T21:10:37.410415Z","shell.execute_reply":"2024-03-11T21:10:37.683828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are some seasonalities, we could do some engineering and see if the 'days' in our data means anything","metadata":{}},{"cell_type":"markdown","source":"# 5. Seasonality Analysis\n\nSince the above graphs show clear signs of seasonality, we will do some feature engineering to see how the months and days impact sales.","metadata":{}},{"cell_type":"code","source":"# add numbers for the months, days, and days of the week (Monday is 0)\nno_outliers['month'] = no_outliers['date'].dt.month.astype('category')\nno_outliers['day_of_month'] = no_outliers['date'].dt.day.astype('category')\nno_outliers['day_of_week'] = no_outliers['date'].dt.dayofweek.astype('category')\n\nno_outliers","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:37.686769Z","iopub.execute_input":"2024-03-11T21:10:37.687361Z","iopub.status.idle":"2024-03-11T21:10:37.734017Z","shell.execute_reply.started":"2024-03-11T21:10:37.687323Z","shell.execute_reply":"2024-03-11T21:10:37.73271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace the categories with numbers, clean train (ct)\nct = no_outliers.copy()\nct['family'] = ct['family'].replace({'GROCERY I': 1, 'BEVERAGES': 2, 'PRODUCE': 3, 'CLEANING': 4, 'DAIRY': 5, 'OTHERS': 6}).astype('category')\nct","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:37.736068Z","iopub.execute_input":"2024-03-11T21:10:37.736556Z","iopub.status.idle":"2024-03-11T21:10:37.923673Z","shell.execute_reply.started":"2024-03-11T21:10:37.736514Z","shell.execute_reply":"2024-03-11T21:10:37.922009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot based on day of week\npx.line((ct.groupby('day_of_week')['sales'].mean().reset_index()),x='day_of_week',y='sales',title='Average Sales by Day of the Week (Monday is 0)')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:37.925691Z","iopub.execute_input":"2024-03-11T21:10:37.926261Z","iopub.status.idle":"2024-03-11T21:10:38.03232Z","shell.execute_reply.started":"2024-03-11T21:10:37.926221Z","shell.execute_reply":"2024-03-11T21:10:38.030893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows how the sales change during the week. It is clear that most sales occur on the weekends, with the lowest sales being on Thursdays. This is indicative of weekly seasonality and we will take this into account when creating dummies later.","metadata":{}},{"cell_type":"code","source":"# plot based on day\npx.line((ct.groupby('day_of_month')['sales'].mean().reset_index()),x='day_of_month',y='sales',title='Average Sales by Day of the Month')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.034146Z","iopub.execute_input":"2024-03-11T21:10:38.035441Z","iopub.status.idle":"2024-03-11T21:10:38.121369Z","shell.execute_reply.started":"2024-03-11T21:10:38.03533Z","shell.execute_reply":"2024-03-11T21:10:38.119271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows how the sales change during the days of the month. The source data states that wages in the public sector are paid every two weeks on the 15th and last day of the month. This could be the reason why the average sales are higher in the days following payday. This is indicative of daily seasonality and we will also take this into account when creating our dummies.","metadata":{}},{"cell_type":"code","source":"# plot based on month\npx.line((ct.groupby('month')['sales'].mean().reset_index()),x='month',y='sales',title='Average Sales by Month')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.122909Z","iopub.execute_input":"2024-03-11T21:10:38.123375Z","iopub.status.idle":"2024-03-11T21:10:38.208481Z","shell.execute_reply.started":"2024-03-11T21:10:38.123336Z","shell.execute_reply":"2024-03-11T21:10:38.20722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows that sales are generally higher in November and December, indicative of monthly seasonality. We have decided to leave the months as is.\n\nAs the data is exhibiting daily, weekly, and monthly seasonality, we should include these variables into our model. However, including all seven days of the week, 28-31 days of the month, and all twelve months of the year would lead to too many predictor variables, in addition to the six product families that have already been reduced.\n\nTo reduce the number of variables, we have decided to focus only on if the day of the week is a weekend, and if the day of the month is four days within paycheck.\n\nAs for the clusters, a regression with dummies should be run for each one.\n\n## Creating Dummies","metadata":{}},{"cell_type":"code","source":"#data frames where we are trying to reduce variables\n\n#make a new data frame with a binary variable if the day is within 4 days of pay day\n#1 if the day is within 4 days of pay day, 0 if not\nfourdays = pd.DataFrame({\n    'day_of_month': range(1, 32),\n    '4d_within_pay': [1 if x in [15, 16, 17, 18, 30, 31, 1, 2, 3] \n                     else 0 for x in range(1, 32)]\n})\n\n#make a new data frame with a binary variable if the day is a weekday\n#1 if it's a weekend, 0 if not\nwkends = pd.DataFrame({\n    'day_of_week': range(7),\n    'weekend': [0 if x in [0, 1, 2, 3, 4] \n                else 1 for x in range(7)]\n})","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.21087Z","iopub.execute_input":"2024-03-11T21:10:38.211291Z","iopub.status.idle":"2024-03-11T21:10:38.221161Z","shell.execute_reply.started":"2024-03-11T21:10:38.211258Z","shell.execute_reply":"2024-03-11T21:10:38.219618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merge the tables to turn day into a dummy variable based on the above conditions\njoin1 = ct.merge(fourdays, on='day_of_month', how='left', indicator=False)\n\n#merge the tables to turn day of the week into a dummy variable based on the above conditions\njoin2 = join1.merge(wkends, on='day_of_week', how='left', indicator=False)\n\n#training with dummies (td), removing the day of the month and week\ntd = join2.drop(['day_of_month', 'day_of_week'], axis=1)\ntd","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.22268Z","iopub.execute_input":"2024-03-11T21:10:38.223108Z","iopub.status.idle":"2024-03-11T21:10:38.342723Z","shell.execute_reply.started":"2024-03-11T21:10:38.223074Z","shell.execute_reply":"2024-03-11T21:10:38.341491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can make dummies for each product family and each month, and our model is finally ready for regression.","metadata":{}},{"cell_type":"code","source":"#dummies for family\nfdum = pd.get_dummies(td, columns=['family'])\n\n#dummies for months\nmdum = pd.get_dummies(fdum, columns = ['month'])\n\n#since dummies are booleans, cycle through all columns, check if bool, then change to int\nfor col in mdum:\n    if mdum[col].dtype == bool:\n         mdum[col] = mdum[col].astype(int)\nmdum.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.344382Z","iopub.execute_input":"2024-03-11T21:10:38.34472Z","iopub.status.idle":"2024-03-11T21:10:38.386917Z","shell.execute_reply.started":"2024-03-11T21:10:38.344693Z","shell.execute_reply":"2024-03-11T21:10:38.385852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mdum.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.388213Z","iopub.execute_input":"2024-03-11T21:10:38.388528Z","iopub.status.idle":"2024-03-11T21:10:38.412192Z","shell.execute_reply.started":"2024-03-11T21:10:38.388499Z","shell.execute_reply":"2024-03-11T21:10:38.411313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Model 1: Linear Regression","metadata":{}},{"cell_type":"markdown","source":"We will now attempt to perform linear regression on the following dataset with dummies. As we are treating each cluster separately, we will run linear regression on each cluster.","metadata":{}},{"cell_type":"code","source":"train_m1 = mdum.copy()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.413495Z","iopub.execute_input":"2024-03-11T21:10:38.414584Z","iopub.status.idle":"2024-03-11T21:10:38.447994Z","shell.execute_reply.started":"2024-03-11T21:10:38.414553Z","shell.execute_reply":"2024-03-11T21:10:38.446573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function for running linear regression\ndef lin_reg (df, c):\n    y = df['sales']\n    \n    #exclude family_6 and month_12 \n    x = df[[\n     '4d_within_pay',\n     'weekend',\n     'family_1',\n     'family_2',\n     'family_3',\n     'family_4',\n     'family_5',\n     'month_1',\n     'month_2',\n     'month_3',\n     'month_4',\n     'month_5',\n     'month_6',\n     'month_7',\n     'month_8',\n     'month_9',\n     'month_10',\n     'month_11',\n    ]]\n    \n    x = sm.add_constant(x)\n    \n    model = sm.OLS(y, x).fit()\n    \n    #plotting the residual and normal probability plots side by side\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    sns.residplot(x=model.fittedvalues, y=model.resid, lowess=True, line_kws={\"color\": \"red\"}, ax=axes[0])\n    axes[0].set_xlabel('Predicted Sales')\n    axes[0].set_ylabel('Residuals')\n    axes[0].set_title(f'Residual Plot for Cluster {c}')\n\n    sm.qqplot(model.resid, line='s', ax=axes[1])\n    axes[1].set_xlabel('Predicted Sales')\n    axes[1].set_ylabel('Actual Sales')\n    axes[1].set_title(f'Normal Probability Plot for Cluster {c}')\n    \n    plt.tight_layout\n    plt.show\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.449639Z","iopub.execute_input":"2024-03-11T21:10:38.450053Z","iopub.status.idle":"2024-03-11T21:10:38.463907Z","shell.execute_reply.started":"2024-03-11T21:10:38.44994Z","shell.execute_reply":"2024-03-11T21:10:38.462311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cycling through the clusters and running linear regression on each one\nfor c in range(1,18):\n    result = lin_reg(train_m1[train_m1['cluster'] == c], c)\n    print(\"\\nRegression Results for Cluster \" + str(c) + \"\\n\")\n    print(result.summary())","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:38.466047Z","iopub.execute_input":"2024-03-11T21:10:38.46655Z","iopub.status.idle":"2024-03-11T21:10:55.443983Z","shell.execute_reply.started":"2024-03-11T21:10:38.466512Z","shell.execute_reply":"2024-03-11T21:10:55.442573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion of Model 1\n\nThe purpose of model 1 was to see if there was a linear relationship in sales over time. As this is the most basic method of regression, this would help us determine what our next steps should be. If the model performed well, we could then use linear regression to forecast the sales. However, this model proved to be poor and we can see this during the data cleaning process, adjusted R-squared, and residual and normal probability plots.\n\nFirst, data processing took a considerable amount of time. As stated above, there are 54 stores and 33 product families, which already create too many different variables in our data, forcing us to cluster and group. There is also seasonality to deal with, and for us to do this with multiple regression, we had to create dummies, which again creates way too many variables. In order not to violate parsimony, having too many variables is not a good idea, and the amount of processing that needed to be done just to prepare the dataset for linear regression was inefficient. As the seasonality appeared to be additive, we could have explored an exponential smoothing method such as Holt-Winters, but we have chosen not to pursue it here.\n\nAs our model included many predictor variables, we will look at the adjusted R-squared values, which were poor for each cluster run with linear regression. Although some clusters output an adjusted R-squared value above 0.7, which is considered to be good, majority presented with R-squared values around 0.5. This means that about 50% of the time, our model does not capture the variations in number of sales. However, the p-values for the predictor variables in all runs were less than 0.05, indicating that they are significant.\n\nA first look at the residual plot for cluster 1 shows that the points are randomized, and the normal probability plot is almost a straight line. These are representations of a good model. However, as we continue to look through the residual plots, we notice that there does appear to be a pattern among them. Although we are not able to define this pattern based on our knowledge of different types of regression, the pattern does prove that the linear regression model we have used is not a good fit. Some of the normal probability plots are also exhibiting exponential growth, most notably, the plots for clusters 12, 15, and 16. In this case, we could try using an exponential trend model by taking the logarithm of sales, but due to time restraints and the fact that we know the regression model is not a good fit here, we have chosen not to explore the exponential trend at this time.\n\nWe have concluded that model 1 is not a good model for forecasting the stores sales in this dataset due to the amount of variables, adjusted R-squared, and residual plots. However, we do note that this model was created after we omitted other variables such as promotion, holidays, and oil prices from our analysis. Had we chosen to include those, we may have received better results, but at this time we will not be using it to make any predictions, and will instead move on to another technique which could potentially give us better results.","metadata":{}},{"cell_type":"markdown","source":"# 7. Model 2: Random Forest\n\nOur second approach to creating a model for forecasting store sales in Random Forest. In summary, Random Forest is a supervised learning and ensemble algorithm that is an extension of decision trees. It can be used both on classification and regression models with high accuracy, and is able to handle large datasets efficiently. However, it is black-box model as we cannot really interpret the individual decision trees within the random forest.\n\nRandom Forest is able to handle many stores, product families, and seasonality within our data, so we will not need to worry about grouping, creating dummies, and taking the logarithm of sales. As such, we can go back and look at our unprocessed data. Thankfully, there is a package for Random Forest that we can import without having to go into the specifics of breaking down our decision trees.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n#from sklearn.linear_model import LinearRegression\n#from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:55.445693Z","iopub.execute_input":"2024-03-11T21:10:55.446132Z","iopub.status.idle":"2024-03-11T21:10:55.855135Z","shell.execute_reply.started":"2024-03-11T21:10:55.446095Z","shell.execute_reply":"2024-03-11T21:10:55.85366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:55.857092Z","iopub.execute_input":"2024-03-11T21:10:55.857553Z","iopub.status.idle":"2024-03-11T21:10:55.876614Z","shell.execute_reply.started":"2024-03-11T21:10:55.857472Z","shell.execute_reply":"2024-03-11T21:10:55.875033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use original train dataset and drop all the zero sales value\ntrain_rf = join[~(join._merge == 'both')].drop(['id', '_merge'], axis = 1).reset_index()\ntrain_rf = train_rf.drop(['index'], axis=1)\ntrain_rf ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:55.87795Z","iopub.execute_input":"2024-03-11T21:10:55.879222Z","iopub.status.idle":"2024-03-11T21:10:56.252298Z","shell.execute_reply.started":"2024-03-11T21:10:55.879182Z","shell.execute_reply":"2024-03-11T21:10:56.250744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rotrain = remove_outliers(train_rf)\nrotrain['codetime'] = (rotrain['date'] - rotrain['date'].min()).dt.days\nrotrain['month'] = rotrain['date'].dt.month\nrotrain['day'] = rotrain['date'].dt.day\nrotrain['day_of_week'] = rotrain['date'].dt.dayofweek\ntrain2 = rotrain.copy()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:56.254605Z","iopub.execute_input":"2024-03-11T21:10:56.255263Z","iopub.status.idle":"2024-03-11T21:10:57.778142Z","shell.execute_reply.started":"2024-03-11T21:10:56.255227Z","shell.execute_reply":"2024-03-11T21:10:57.776489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing the Categorical Variables and Removing Outliers","metadata":{}},{"cell_type":"code","source":"#use train 2\ntrain2","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:57.780522Z","iopub.execute_input":"2024-03-11T21:10:57.781018Z","iopub.status.idle":"2024-03-11T21:10:57.801855Z","shell.execute_reply.started":"2024-03-11T21:10:57.780982Z","shell.execute_reply":"2024-03-11T21:10:57.800382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In model 1, we had grouped the product families together before removing the outliers, but now we will remove the outliers while keep all the families as they are.\n\nThe random forest package requires all categorical variables to be changed into numerical variables, so we will add dummies for each family.","metadata":{}},{"cell_type":"code","source":"train2.info()\ntrain2_dummy = pd.get_dummies(train2, columns=['family'])\ntrain2_dummy","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:57.804186Z","iopub.execute_input":"2024-03-11T21:10:57.805176Z","iopub.status.idle":"2024-03-11T21:10:58.740828Z","shell.execute_reply.started":"2024-03-11T21:10:57.805135Z","shell.execute_reply":"2024-03-11T21:10:58.739675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model training with RandomForest\nXrf = train2_dummy.drop(['sales','date'], axis=1)\nyrf = train2_dummy['sales']\n\n# splitting data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(Xrf, yrf, test_size=0.3)\n\n# creating the RandomForest model (hyperparameter temporarily set to: n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\nrfmodel = RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n\n# training the model\nrfresult = rfmodel.fit(X_train, y_train) ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:10:58.742607Z","iopub.execute_input":"2024-03-11T21:10:58.742964Z","iopub.status.idle":"2024-03-11T21:13:12.057669Z","shell.execute_reply.started":"2024-03-11T21:10:58.74292Z","shell.execute_reply":"2024-03-11T21:13:12.056421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the random forest model for the test part of dataset train2_dummy\ny_pred = rfmodel.predict(X_test)\n\nprint('The model score is: ',rfmodel.score(X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:13:12.059524Z","iopub.execute_input":"2024-03-11T21:13:12.06024Z","iopub.status.idle":"2024-03-11T21:13:13.930253Z","shell.execute_reply.started":"2024-03-11T21:13:12.060195Z","shell.execute_reply":"2024-03-11T21:13:13.928775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSE\n#y_pred = rfmodel.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mse**.5\nprint(mse)\nprint(rmse)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:13:13.932034Z","iopub.execute_input":"2024-03-11T21:13:13.932464Z","iopub.status.idle":"2024-03-11T21:13:13.943675Z","shell.execute_reply.started":"2024-03-11T21:13:13.932423Z","shell.execute_reply":"2024-03-11T21:13:13.942263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSLE for train2_dummy\n\nlog_actual = np.log1p(y_test)\nlog_pred = np.log1p(y_pred)\n\nrmsle = np.sqrt(np.mean((log_pred - log_actual) ** 2))\nrmsle\nprint(\"RMSLE:\", rmsle)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:13:13.945207Z","iopub.execute_input":"2024-03-11T21:13:13.945587Z","iopub.status.idle":"2024-03-11T21:13:13.98327Z","shell.execute_reply.started":"2024-03-11T21:13:13.945559Z","shell.execute_reply":"2024-03-11T21:13:13.981709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"Mean Absolute Error:\", mae)\nprint(\"R-squared:\", r2)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:13:13.985009Z","iopub.execute_input":"2024-03-11T21:13:13.986372Z","iopub.status.idle":"2024-03-11T21:13:14.005509Z","shell.execute_reply.started":"2024-03-11T21:13:13.98634Z","shell.execute_reply":"2024-03-11T21:13:14.004479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#functions for calculating performance measures\ndef adj_rsquared (r2, n, p) :\n    adj_r2 = 1 - ((1-r2) * (n-1) / (n-p-1))\n    return adj_r2\n\nadj_r2 = adj_rsquared(r2_score(y_test, y_pred), len(y_test), len(X_test.columns))\nprint(\"Adj R-squared:\", adj_r2)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:13:14.006522Z","iopub.execute_input":"2024-03-11T21:13:14.006884Z","iopub.status.idle":"2024-03-11T21:13:14.020997Z","shell.execute_reply.started":"2024-03-11T21:13:14.006851Z","shell.execute_reply":"2024-03-11T21:13:14.019268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion of Model 2\nModel 2 performed much better than model 1 and we will evaluate the data processing, performance measures, and residual plots to see why it performed better.\n\nUpon looking at the adjuested R-squared for model 2, it is much higher than the adjusted R-squared values we received from model 1 at 0.9. This means that 90% of the variance in sales can be explained by store number and product family. Although we were not able to see any p-values or coefficients, we were able to calculate RMSE which can be compared to other models should we decide to create them in the future.\n\nThe residual plot for random forest shows a definite pattern, although we are not able to determine what it is. These patterns may stem from other predictor variables such as promotion, holidays, and oil sales which we were not able to include in this analysis.\n\nWe were not able to investigate further into the trends exhibited in our random forest model, but we will conclude that model 2 performed better than model at forecasting stores sales.","metadata":{}},{"cell_type":"markdown","source":"# 8. Other Features of Random Forest","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Optimization","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:16:31.470523Z","iopub.execute_input":"2024-03-11T21:16:31.470947Z","iopub.status.idle":"2024-03-11T21:16:31.479308Z","shell.execute_reply.started":"2024-03-11T21:16:31.470918Z","shell.execute_reply":"2024-03-11T21:16:31.477884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set options for the hyperparameters\nparam_grid = { \n    'n_estimators': [25, 50, 80,100,120,150], \n    'max_features': ['sqrt', 'log2', None], \n    'max_depth': [5,7,10,12,15,20,None], \n    'min_samples_leaf': [1, 2, 4],\n    'min_samples_split': [2, 5, 7]\n} ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:16:31.481864Z","iopub.execute_input":"2024-03-11T21:16:31.482571Z","iopub.status.idle":"2024-03-11T21:16:31.502327Z","shell.execute_reply.started":"2024-03-11T21:16:31.482529Z","shell.execute_reply":"2024-03-11T21:16:31.500871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GridSearchCV takes way too long to run, could be hours\n\"\"\"grid_search = GridSearchCV(RandomForestRegressor(), \n                           param_grid=param_grid) \ngrid_search.fit(X_train, y_train) \nprint(grid_search.best_estimator_) \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:16:31.503736Z","iopub.execute_input":"2024-03-11T21:16:31.504723Z","iopub.status.idle":"2024-03-11T21:16:31.519825Z","shell.execute_reply.started":"2024-03-11T21:16:31.504693Z","shell.execute_reply":"2024-03-11T21:16:31.518622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so we use RandomizedSearchCV\nrand_search = RandomizedSearchCV(RandomForestRegressor(), \n                           param_distributions=param_grid,verbose=2,n_jobs=-1) ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:16:31.523659Z","iopub.execute_input":"2024-03-11T21:16:31.524448Z","iopub.status.idle":"2024-03-11T21:16:31.532044Z","shell.execute_reply.started":"2024-03-11T21:16:31.524415Z","shell.execute_reply":"2024-03-11T21:16:31.53073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run for the optimal paramaters, but it's still taking too long to run (6600+s), so we are just putting the result in next cell: \n#rand_search.fit(X_train, y_train) \n#print(rand_search.best_params_) ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T21:16:31.533542Z","iopub.execute_input":"2024-03-11T21:16:31.535038Z","iopub.status.idle":"2024-03-11T23:07:55.359177Z","shell.execute_reply.started":"2024-03-11T21:16:31.534999Z","shell.execute_reply":"2024-03-11T23:07:55.357131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# {'n_estimators': 120, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 12}","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:07:55.362935Z","iopub.execute_input":"2024-03-11T23:07:55.363508Z","iopub.status.idle":"2024-03-11T23:07:55.372419Z","shell.execute_reply.started":"2024-03-11T23:07:55.363442Z","shell.execute_reply":"2024-03-11T23:07:55.370753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the score for train and test sets, result is 0.94951 and 0.94528\n#print(rand_search.score(X_train,y_train))\n#print(rand_search.score(X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:07:55.37437Z","iopub.execute_input":"2024-03-11T23:07:55.375935Z","iopub.status.idle":"2024-03-11T23:08:12.794786Z","shell.execute_reply.started":"2024-03-11T23:07:55.375878Z","shell.execute_reply":"2024-03-11T23:08:12.793565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use new hyperparameters to fit the model\nrfmodel2 = RandomForestRegressor(n_estimators=120, max_depth=12, n_jobs=-1,min_samples_split=2, min_samples_leaf=1)\n\n# re-training the model\nrfresult2 = rfmodel2.fit(X_train, y_train) ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:34:41.190092Z","iopub.execute_input":"2024-03-11T23:34:41.190671Z","iopub.status.idle":"2024-03-11T23:40:48.461037Z","shell.execute_reply.started":"2024-03-11T23:34:41.190635Z","shell.execute_reply":"2024-03-11T23:40:48.45945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred2 = rfmodel2.predict(X_test)\n\nprint('The model score is: ',rfmodel2.score(X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:41:53.352767Z","iopub.execute_input":"2024-03-11T23:41:53.353925Z","iopub.status.idle":"2024-03-11T23:41:58.46507Z","shell.execute_reply.started":"2024-03-11T23:41:53.353888Z","shell.execute_reply":"2024-03-11T23:41:58.463891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSLE for train2_dummy\n\nlog_actual = np.log1p(y_test)\nlog_pred2 = np.log1p(y_pred2)\n\nrmsle2 = np.sqrt(np.mean((log_pred2 - log_actual) ** 2))\nprint(\"RMSLE:\", rmsle2)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:41:58.467769Z","iopub.execute_input":"2024-03-11T23:41:58.468647Z","iopub.status.idle":"2024-03-11T23:41:58.505644Z","shell.execute_reply.started":"2024-03-11T23:41:58.468606Z","shell.execute_reply":"2024-03-11T23:41:58.50413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score after the tuning is 0.945, higher than the score before 0.916. And the RMSLE now is 1.17 comparing to 1.45","metadata":{}},{"cell_type":"markdown","source":"Reference: https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/, https://www.youtube.com/watch?v=SctFnD_puQI","metadata":{}},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"# global feature importance\nglobal_importances = pd.Series(rfmodel2.feature_importances_, index=X_train.columns)\n# global_importances\nglobal_importances.sort_values(ascending=True, inplace=True)\nplt.figure(figsize=(10, 12)) \nglobal_importances.plot.barh()\n\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Global Feature Importance - Built-in Method\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:45:10.635715Z","iopub.execute_input":"2024-03-11T23:45:10.636282Z","iopub.status.idle":"2024-03-11T23:45:11.516776Z","shell.execute_reply.started":"2024-03-11T23:45:10.636223Z","shell.execute_reply":"2024-03-11T23:45:11.51532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So GROCERY I matters the most, followed by store number as the second, then BEVERAGES... However, it is important to note in random forest, there could be a cardinality bias. This bias is a common problem in Random Forest models, where the model tends to overestimate the importance of features with a high number of unique values. How to get feature importance in random forests: https://forecastegy.com/posts/feature-importance-in-random-forests/","metadata":{}},{"cell_type":"markdown","source":"## Residual Plot","metadata":{}},{"cell_type":"code","source":"y_pred_se =pd.Series(y_pred2)\ny_pred_se","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:45:34.655223Z","iopub.execute_input":"2024-03-11T23:45:34.655641Z","iopub.status.idle":"2024-03-11T23:45:34.666403Z","shell.execute_reply.started":"2024-03-11T23:45:34.655608Z","shell.execute_reply":"2024-03-11T23:45:34.665087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residuals = rfmodel2.predict(Xrf) - yrf\n\nresiduals_df = pd.DataFrame({'date': train2_dummy['date'], 'residuals': residuals})\n\nresiduals_mean_by_date = residuals_df.groupby('date')['residuals'].mean()\n\nsns.scatterplot(x=train2_dummy['date'], y=residuals)\n\nsns.scatterplot(data=residuals_mean_by_date, x=residuals_mean_by_date.index, y=residuals_mean_by_date.values, color='r', linestyle='--', label='Residuals Mean')\nsns.set(rc={\"figure.figsize\":(28, 10)}) \nplt.ylabel('Residual Value')\nplt.title('Residual Plot')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:45:34.669318Z","iopub.execute_input":"2024-03-11T23:45:34.67027Z","iopub.status.idle":"2024-03-11T23:46:03.520534Z","shell.execute_reply.started":"2024-03-11T23:45:34.670238Z","shell.execute_reply":"2024-03-11T23:46:03.519343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" residuals_df","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:46:03.522551Z","iopub.execute_input":"2024-03-11T23:46:03.523271Z","iopub.status.idle":"2024-03-11T23:46:03.537832Z","shell.execute_reply.started":"2024-03-11T23:46:03.523233Z","shell.execute_reply":"2024-03-11T23:46:03.536489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_array_with_df(array, df,column_name='predict'):\n    array_series = pd.Series(array, index=df.index)    \n    array_series.name = column_name\n    # merge df and series\n    merged_df = pd.concat([df, array_series], axis=1)\n    \n    return merged_df\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:46:03.539453Z","iopub.execute_input":"2024-03-11T23:46:03.540691Z","iopub.status.idle":"2024-03-11T23:46:03.551997Z","shell.execute_reply.started":"2024-03-11T23:46:03.540609Z","shell.execute_reply":"2024-03-11T23:46:03.550599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine predict sales with test dataset using our created function merge_array_with_df\ntrain2_dummy_pred = merge_array_with_df(y_pred2, y_test)\ntrain2_dummy_pred","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:46:03.555384Z","iopub.execute_input":"2024-03-11T23:46:03.555774Z","iopub.status.idle":"2024-03-11T23:46:03.584984Z","shell.execute_reply.started":"2024-03-11T23:46:03.55574Z","shell.execute_reply":"2024-03-11T23:46:03.583761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2_dummy_pred['residual'] = train2_dummy_pred['sales'] - train2_dummy_pred['predict']\nsns.scatterplot(train2_dummy_pred,x='predict',y='residual')\nsns.set(rc={\"figure.figsize\":(10, 10)}) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:46:03.586841Z","iopub.execute_input":"2024-03-11T23:46:03.587984Z","iopub.status.idle":"2024-03-11T23:46:05.363262Z","shell.execute_reply.started":"2024-03-11T23:46:03.587939Z","shell.execute_reply":"2024-03-11T23:46:05.361917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Prediction Sales & Plots","metadata":{}},{"cell_type":"code","source":"# make predictions using dataset: test\ntest","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:46:05.364785Z","iopub.execute_input":"2024-03-11T23:46:05.365178Z","iopub.status.idle":"2024-03-11T23:46:05.381176Z","shell.execute_reply.started":"2024-03-11T23:46:05.365147Z","shell.execute_reply":"2024-03-11T23:46:05.37964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a function to process the dataset into our desired form \ndef process_dataframe(df):    \n    df_processed = df[['date', 'store_nbr', 'family',  'onpromotion']]\n    df_processed['codetime'] = (df_processed['date'] - df_processed['date'].min()).dt.days\n    df_processed['month'] = df_processed['date'].dt.month\n    df_processed['day'] = df_processed['date'].dt.day\n    df_processed['day_of_week'] = df_processed['date'].dt.dayofweek\n    df_processed_dummy = pd.get_dummies(df_processed, columns=['family'])\n    df_processed_dummy = df_processed_dummy.drop(columns=['date']) \n    return df_processed_dummy","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:46:05.383127Z","iopub.execute_input":"2024-03-11T23:46:05.383956Z","iopub.status.idle":"2024-03-11T23:46:05.392574Z","shell.execute_reply.started":"2024-03-11T23:46:05.383915Z","shell.execute_reply":"2024-03-11T23:46:05.391686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_df = process_dataframe(test)\nprocessed_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:46:05.393655Z","iopub.execute_input":"2024-03-11T23:46:05.394686Z","iopub.status.idle":"2024-03-11T23:46:05.443355Z","shell.execute_reply.started":"2024-03-11T23:46:05.394653Z","shell.execute_reply":"2024-03-11T23:46:05.442121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the random forest model for the test part of dataset train2_dummy\ny_test_pred = rfmodel2.predict(processed_df)\ny_test_pred","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:48:48.95815Z","iopub.execute_input":"2024-03-11T23:48:48.95865Z","iopub.status.idle":"2024-03-11T23:48:49.093481Z","shell.execute_reply.started":"2024-03-11T23:48:48.958617Z","shell.execute_reply":"2024-03-11T23:48:49.092186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine predict sales with test dataset using our created function merge_array_with_df\ntestset_pred = merge_array_with_df(y_test_pred, test)\ntestset_pred\n# 0 is the column name for prediction sales","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:48:49.095927Z","iopub.execute_input":"2024-03-11T23:48:49.096842Z","iopub.status.idle":"2024-03-11T23:48:49.119482Z","shell.execute_reply.started":"2024-03-11T23:48:49.096774Z","shell.execute_reply":"2024-03-11T23:48:49.118108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(testset_pred,x='date',y='predict').set_title('Average Forecast Sales for All Stores')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:48:49.121263Z","iopub.execute_input":"2024-03-11T23:48:49.121632Z","iopub.status.idle":"2024-03-11T23:48:50.525536Z","shell.execute_reply.started":"2024-03-11T23:48:49.121601Z","shell.execute_reply":"2024-03-11T23:48:50.524439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top 5 Categories Forecast Sales\nmean_by_family = testset_pred.groupby('family')['predict'].sum()\n\n# select only the top 5 categories in 'family'\ntop_5_families = mean_by_family.nlargest(5).index\ntop_5_data = testset_pred[testset_pred['family'].isin(top_5_families)]\n\nsns.barplot(data=top_5_data, x='date', y='predict', hue='family').set_title('Top 5 Categories Forecast Sales')\nsns.set(rc={\"figure.figsize\":(35, 10)}) \nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:48:50.528021Z","iopub.execute_input":"2024-03-11T23:48:50.529252Z","iopub.status.idle":"2024-03-11T23:48:54.239147Z","shell.execute_reply.started":"2024-03-11T23:48:50.529214Z","shell.execute_reply":"2024-03-11T23:48:54.237844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top 5 Stores Forecast Sales\nmean_by_store = testset_pred.groupby('store_nbr')['predict'].sum()\n\n# select only the top 5 in 'store_nbr'\ntop_5_stores = mean_by_store.nlargest(5).index\ntop_5_store_data = testset_pred[testset_pred['store_nbr'].isin(top_5_stores)]\n#top_5_store_data['store_nbr'] = top_5_store_data['store_nbr'].astype('category')\n\nsns.barplot(data=top_5_store_data, x='date', y='predict', hue='store_nbr').set_title('Top 5 Stores Forecast Sales')\nsns.set(rc={\"figure.figsize\":(28, 10)}) \nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T23:48:54.241218Z","iopub.execute_input":"2024-03-11T23:48:54.242438Z","iopub.status.idle":"2024-03-11T23:48:57.621138Z","shell.execute_reply.started":"2024-03-11T23:48:54.24239Z","shell.execute_reply":"2024-03-11T23:48:57.620324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nhttps://www.kaggle.com/code/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide\n\nhttps://www.geeksforgeeks.org/random-forest-regression-in-python/","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}