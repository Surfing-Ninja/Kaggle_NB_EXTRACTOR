{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":30132,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-03T15:16:34.535356Z","iopub.execute_input":"2023-12-03T15:16:34.535724Z","iopub.status.idle":"2023-12-03T15:16:34.545661Z","shell.execute_reply.started":"2023-12-03T15:16:34.535682Z","shell.execute_reply":"2023-12-03T15:16:34.545066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://bit.ly/3ZLyF82 -O CSS.css -q\n    \nfrom IPython.core.display import HTML\nwith open('./CSS.css', 'r') as file:\n    custom_css = file.read()\n\nHTML(custom_css)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-03T15:16:34.603531Z","iopub.execute_input":"2023-12-03T15:16:34.603845Z","iopub.status.idle":"2023-12-03T15:16:35.820063Z","shell.execute_reply.started":"2023-12-03T15:16:34.603805Z","shell.execute_reply":"2023-12-03T15:16:35.819062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nfrom IPython.core.display import display, HTML, Javascript\n\nhtml_contents =\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\">\n        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Raleway\">\n        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Roboto\">\n        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Verdana\">\n        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Open Sans\">\n        <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n        <style>\n        .title-section{\n            font-family: \"Roboto\", Verdana, sans-serif;\n            font-weight: bold;\n            color: \"#6A8CAF\";\n            letter-spacing: 6px;\n        }\n        hr { border: 1px solid #E58F65 !important;\n             color: #E58F65 !important;\n             background: #E58F65 !important;\n           }\n        body {\n            font-family: \"Verdana\", sans-serif;\n            }        \n        </style>\n    </head>    \n</html>\n\"\"\"\n\nHTML(html_contents)\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-03T15:16:35.82241Z","iopub.execute_input":"2023-12-03T15:16:35.822727Z","iopub.status.idle":"2023-12-03T15:16:35.837572Z","shell.execute_reply.started":"2023-12-03T15:16:35.822688Z","shell.execute_reply":"2023-12-03T15:16:35.836482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Introduction\"></a>\n\n\n<h1 style='background:#6A8CAF; border:1;'>\n    <center>\n        Store Sales EDA, Prediction with TS - Analysis\n    </center>\n</h1>\n\n\n- This notebook provides an exploratory data analysis and time series prediction for store sales. \n- The notebook covers various aspects of the data, including data visualization, statistical analysis, and machine learning models for sales prediction. \n- It can be a valuable resource for understanding the factors influencing store sales and for implementing time series forecasting techniques.\n- The notebook covers several key aspects of the data and analysis, including:\n\n>>  1. **Data Visualization**: The notebook begins by exploring the data through various visualizations, such as histograms, box plots, and heatmaps, to gain insights into the distribution of the data and identify any patterns or trends.\n\n>>  2. **Statistical Analysis**: The analysis proceeds with descriptive statistics, such as mean, median, and standard deviation, to summarize the main characteristics of the data.\n\n>>  3. **Time Series Forecasting**: The notebook then focuses on time series forecasting, which is a statistical technique used to predict future values based on past values. The DataFrameGroup library is used to create a time series object, and various forecasting algorithms, such as Moving Averages, Autoregressive Integrated Moving Averages (ARIMA), and Exponential Smoothing State Space Models (ETS), are applied to the data.\n\n>>  4. **Model Evaluation**: The performance of the forecasting models is evaluated using standard metrics, such as Mean Absolute Error (MAE) and Mean Squared Error (MSE), to determine which model performs the best.\n\n>>  5. **Machine Learning Models**: The notebook also explores the use of machine learning models, such as K-Nearest Neighbors (KNN) and eXtreme Gradient Boosting (XGB), for sales prediction. These models are trained on the processed data and evaluated using the same metrics as the time series models.\n\n- Overall, the Jupyter notebook \"Store Sales EDA, Prediction with TS\" aims to be a resource for understanding the factors influencing store sales and implementing time series forecasting and machine learning techniques to predict sales.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"timeseriestools\"></a>\n<h1 style='background:#B2FF33; border:0;'><center>Time Series Intro</center></h1>\n\n### Intro\n###### Source: Wikipedia\n\nIn mathematics, a time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n\nA Time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\n\nTime series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called \"time series analysis\", which refers in particular to relationships between different points in time within a single series. Interrupted time series analysis is used to detect changes in the evolution of a time series from before to after some intervention which may affect the underlying variable.\n\n![Exampl2-1](https://upload.wikimedia.org/wikipedia/commons/7/77/Random-data-plus-trend-r2.png)| ![Example-2](https://upload.wikimedia.org/wikipedia/commons/0/05/Tuberculosis_incidence_US_1953-2009.png) | \n\n\n### Tools\n\nTools for investigating time-series data include:\n\n- Consideration of the *autocorrelation|autocorrelation function-and the *Spectral density|spectral density function-(also *cross-correlation function*s and cross-spectral density functions)\n- *Scaled correlation|Scaled-cross- and auto-correlation functions to remove contributions of slow components\n- Performing a *Fourier transform-to investigate the series in the *frequency domain*\n- Use of a *digital filter|filter-to remove unwanted *noise (physics)|noise*\n- *Principal component analysis-(or *empirical orthogonal function-analysis)\n- *Singular spectrum analysis*\n- \"Structural\" models:\n- General *State Space Model*s\n- Unobserved Components Models\n- *Machine Learning*\n> - *Artificial neural network*s\n> - *Support vector machine*\n> - *Fuzzy logic*\n> - *Gaussian process*\n> - *Hidden Markov model*\n- *Queueing theory-analysis\n- *Control chart*\n> - *Shewhart individuals control chart*\n> - *CUSUM-chart\n> - *EWMA chart*\n- *Detrended fluctuation analysis*\n- *Nonlinear mixed-effects model|Nonlinear mixed-effects modeling*\n- *Dynamic time warping*\n- *Cross-correlation*\n- *Dynamic Bayesian network*\n- *Time-frequency representation|Time-frequency analysis techniques:*\n> - *Fast Fourier transform*\n> - *Continuous wavelet transform*\n> - *Short-time Fourier transform*\n> - *Chirplet transform*\n> - *Fractional Fourier transform*\n- *Chaos theory|Chaotic analysis*\n> - *Correlation dimension*\n> - *Recurrence plot*s\n> - *Recurrence quantification analysis*\n> - *Lyapunov exponent*s\n> - *Entropy encoding*\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"timeseriestools\"></a>\n<h1 style='background:#B2FF33; border:0;'><center>Time Series Tools</center></h1>\n\n### I evaluated the following Time Series Python Libraries for this exercise. Some of them had issues in a Kaggle Environment\n\n| Library   | Description | \n| -------- | ---------- | \n|<a href='https://facebook.github.io/prophet/'><img src=\"https://miro.medium.com/max/546/1*pdXzwREgeCV0Mh1hONNzpw.png\" width=\"300\" height=\"200\"></a>|  Prophet is an excellent library developed by Facebookâ€™s data science team - [Link](https://facebook.github.io/prophet/)     |\n|<a href='https://github.com/uber/orbit'><img src=\"https://raw.githubusercontent.com/uber/orbit/dev/docs/img/orbit-banner.png\" width=\"300\" height=\"200\"></a>| Orbit is a Python framework created by Uber for Bayesian time series forecasting and inference - [Link](https://github.com/uber/orbit)     |\n|<a href='https://github.com/unit8co/darts'><img src=\"https://miro.medium.com/max/1314/1*ZJf8iWBoBAFYESK7plslgQ.png\" width=\"300\" height=\"200\"></a>| Darts is a scikit-learn friendly Python package for forecasting time series - [Link](https://github.com/unit8co/darts)     |\n|<a href='https://github.com/blue-yonder/tsfresh '><img src=\"https://miro.medium.com/max/966/1*O3uPA0cvopTm5X5fgAgczA.png\" width=\"300\" height=\"200\"></a>|tsfresh is a friendly Python package for forecasting time series - [Link](https://github.com/blue-yonder/tsfresh )     |\n|<a href='https://github.com/winedarksea/AutoTS'><img src=\"https://miro.medium.com/max/790/1*YKC9uODo3wUJQXx0Ll1Zsw.png\" width=\"300\" height=\"200\"></a>|As the name suggests it is a Python library for automated time series analysis. - [Link](https://github.com/winedarksea/AutoTS )     |\n|<a href='https://hcrystalball.readthedocs.io/en/latest/'><img src=\"https://raw.githubusercontent.com/heidelbergcement/hcrystalball/master/docs/_static/hcrystal_ball_logo_black.svg\" width=\"300\" height=\"200\"></a>|H CrystalBall is A library that unifies the API for most commonly used libraries and modelling techniques for time-series forecasting in the Python ecosystem. - [Link](https://hcrystalball.readthedocs.io/en/latest/)  |\n|<a href='https://www.statsmodels.org/stable/tsa.html?highlight=tsa#module-statsmodels.tsa'><img src=\"https://www.statsmodels.org/stable/_images/statsmodels-logo-v2-horizontal.svg\" width=\"300\" height=\"200\"></a>|statsmodels.tsa contains model classes and functions that are useful for time series analysis. Basic models include univariate autoregressive models (AR), vector autoregressive models (VAR) and univariate autoregressive moving average models (ARMA). - [Link](https://www.statsmodels.org/stable/tsa.html?highlight=tsa#module-statsmodels.tsa)  |\n|<a href='https://pyflux.readthedocs.io/en/latest/'><img src=\"https://techleerimages.s3.ap-south-1.amazonaws.com/2047.png\" width=\"300\" height=\"200\"></a>|Pyflux is an open-source time-series library built for python.PyFlux is a library for time series analysis and prediction. Users can choose from a flexible range of modelling and inference options, and use the output for forecasting and retrospection. - [Link](https://pyflux.readthedocs.io/en/latest/)  |\n|<a href='https://github.com/alteryx/featuretools'><img src=\"https://camo.githubusercontent.com/b76fee14056cf14312d32dd362cacf3d313a38dc9e21c1f614735d3cbe5d5ce2/68747470733a2f2f7777772e66656174757265746f6f6c732e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031372f31322f466561747572654c6162732d4c6f676f2d54616e676572696e652d3830302e706e67\" width=\"300\" height=\"200\"></a>|Featuretools is a python library for automated feature engineering. See the documentation for more information. - [Link](https://github.com/alteryx/featuretools)  |\n|<a href='https://github.com/arrow-py/arrow'><img src=\"https://arrow.apache.org/docs/_static/arrow.png\" width=\"300\" height=\"200\"></a>|Arrow is a Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps - [Link](https://github.com/arrow-py/arrow)  |\n|<a href='https://github.com/cesium-ml/cesium'><img src=\"https://cesium-ml.org/blog/theme/img/cesium-blue-light.png\" width=\"300\" height=\"200\"></a>|cesium: Open-Source Platform for Time Series Inference.Cesium is an open source library that allows users to extract features from raw time series data, build machine learning models from these features, as well as generate predictions for new data. - [Link] (https://github.com/cesium-ml/cesium)  |\n|<a href='https://github.com/TimeSynth/TimeSynth'>TimeSynth</a>|TimeSynth is an open source library for generating synthetic time series for model testing. The library can generate regular and irregular time series.  - [Link](https://github.com/TimeSynth/TimeSynth)  |\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"notebook-contents\"></a>\n<h1 style='background:#B2FF33; border:0;'><center>Notebook contents</center></h1>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"table-of-contents\"></a>\n<h1 style='background:#B2FF33; border:0;'><center>Table of Contents</center></h1>\n\n## [1. Introduction](#1)\n### [1.1 Loading of Libraries](#1.1)\n### [1.2 Data Loading](#1.2)\n## [2. Data Exploration](#2)\n### [2.1 Number of Rows and columns](#2.1)\n## [3. Features Analysis](#3)\n## [4. Use of Prophet for TimeSeries](#4)\n## [Conclusion](#999)","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"1\"></a>\n# 1. Introduction\nIntroduction of the problem, loading of libraries and data ","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"1.1\"></a>\n\n## 1.1 Loading of Libraries\n### Load all the libraries to be used","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# loading packages\n# basic + dates \nimport numpy as np\nimport pandas as pd\nfrom pandas import datetime\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns # advanced vizs\n%matplotlib inline\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode(connected = True)\n\n# statistics\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n# time series analysis\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# prophet by Facebook\nfrom fbprophet import Prophet","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-03T15:16:35.83973Z","iopub.execute_input":"2023-12-03T15:16:35.840555Z","iopub.status.idle":"2023-12-03T15:16:39.634635Z","shell.execute_reply.started":"2023-12-03T15:16:35.840517Z","shell.execute_reply":"2023-12-03T15:16:39.633707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n[back to top](#table-of-contents)\n<a id=\"1.2\"></a>\n## 1.2 Loading of Datasets\n### Let us load the datasets","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/store-sales-time-series-forecasting/'\noil_data = pd.read_csv(path+'oil.csv')\ntrain = pd.read_csv(path+'train.csv', parse_dates = True, low_memory = False, index_col = 'date')\ntest = pd.read_csv(path+'test.csv')\nsubmission_sample = pd.read_csv(path+'sample_submission.csv')\nholidays_data = pd.read_csv(path+'holidays_events.csv')\nstore_data =  pd.read_csv(path+'stores.csv')\ntransaction_data = pd.read_csv(path+'transactions.csv')\n\n\n# time series as indexes\ntrain.index","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:39.636267Z","iopub.execute_input":"2023-12-03T15:16:39.636601Z","iopub.status.idle":"2023-12-03T15:16:44.199781Z","shell.execute_reply.started":"2023-12-03T15:16:39.636554Z","shell.execute_reply":"2023-12-03T15:16:44.198884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2\"></a>\n# 2. Data Exploration\nLet us explore the data ","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.1\"></a>\n## 2.1 Number of Roww and Columns. \n### Let us load the datasets. Check the shape and info about the datasets","metadata":{}},{"cell_type":"code","source":"print('Number of train samples: ', train.shape)\nprint('Number of test samples: ', test.shape)\nprint('Number of store data: ', store_data.shape)\nprint('Number of Holiday data: ', holidays_data.shape)\nprint('Number of Oil Price data: ', oil_data.shape)\nprint('Number of features: ', len(train.columns))\nprint(train.info())\nprint(train.columns)\nprint(train.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:44.202709Z","iopub.execute_input":"2023-12-03T15:16:44.203101Z","iopub.status.idle":"2023-12-03T15:16:44.230478Z","shell.execute_reply.started":"2023-12-03T15:16:44.203056Z","shell.execute_reply":"2023-12-03T15:16:44.229668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.2\"></a>\n## 2.2 Further exploration","metadata":{}},{"cell_type":"markdown","source":"## Short description of Key Fields\n* date: Date of entry\n* store_nbr: Store Number\n* sales: the turnover for any given day (target variable).\n* family: Product Family\n* store_type: Type of Store\n* cluster: Store Cluster\n* city: City\n* state: State the store is located in\n* onpromotion: indicates whether a store is running a promo on that day.\n* holiday_type & locale : If it was a holiday and type of holiday\n* transactions: Number of transactions\n* sales: sales volume","metadata":{"execution":{"iopub.status.busy":"2021-10-07T08:26:29.914574Z","iopub.execute_input":"2021-10-07T08:26:29.914878Z","iopub.status.idle":"2021-10-07T08:26:30.080512Z","shell.execute_reply.started":"2021-10-07T08:26:29.914849Z","shell.execute_reply":"2021-10-07T08:26:30.079322Z"}}},{"cell_type":"code","source":"train.info()\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:44.231666Z","iopub.execute_input":"2023-12-03T15:16:44.232364Z","iopub.status.idle":"2023-12-03T15:16:44.596011Z","shell.execute_reply.started":"2023-12-03T15:16:44.232329Z","shell.execute_reply":"2023-12-03T15:16:44.595413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate Year, Month, Day and Week of the Year Categorical fields","metadata":{}},{"cell_type":"code","source":"# data extraction\ntrain['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Day'] = train.index.day\ntrain['WeekOfYear'] = train.index.weekofyear\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:44.597263Z","iopub.execute_input":"2023-12-03T15:16:44.59774Z","iopub.status.idle":"2023-12-03T15:16:47.060121Z","shell.execute_reply.started":"2023-12-03T15:16:44.597687Z","shell.execute_reply":"2023-12-03T15:16:47.059077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge the Training data with Store Data for an expanded view","metadata":{}},{"cell_type":"code","source":"train_store=train.merge(store_data, on = 'store_nbr', how='left')\ntrain_store.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:47.061544Z","iopub.execute_input":"2023-12-03T15:16:47.061825Z","iopub.status.idle":"2023-12-03T15:16:47.802587Z","shell.execute_reply.started":"2023-12-03T15:16:47.061792Z","shell.execute_reply":"2023-12-03T15:16:47.801469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for Unique Values","metadata":{}},{"cell_type":"code","source":"ts_unique = train_store.nunique()\nfor index, value in ts_unique.items():\n    print(f\"Index : {index}, Value : {value}\")\n    if value < 50:\n        print('*'*50)\n        print('keys')\n        print('*'*50)\n        print(train_store[index].unique())\n    else:\n        print('*'*50)\n        print('skipping', index)\n        print('More than 50 unique elements')\n        print('*'*50)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:47.804061Z","iopub.execute_input":"2023-12-03T15:16:47.804328Z","iopub.status.idle":"2023-12-03T15:16:50.476397Z","shell.execute_reply.started":"2023-12-03T15:16:47.804296Z","shell.execute_reply":"2023-12-03T15:16:50.475285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3\"></a>\n# 3. Features Analysis\n\n### Let us see some graphs","metadata":{}},{"cell_type":"markdown","source":"#### Family , Cluster, Type, State, City and Date elements with less than 50 unique values","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\ndef random_color():\n    return \"#{}{}{}{}{}{}\".format(*(random.choice(\"0123456789abcdef\") for _ in range(6)))\n\n# data\nts_sales_type = train_store.groupby('type').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)\nts_sales_family = train_store.groupby('family').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)[:20]\nts_sales_cluster = train_store.groupby('cluster').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False) \nts_sales_state = train_store.groupby('state').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)\nts_sales_city = train_store.groupby('city').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)\n\n\nts_sales_type['color'] = [random_color() for _ in range(len(ts_sales_type))]\nts_sales_type['color'][3:] = [random_color() for _ in range(len(ts_sales_type[3:]))]\nts_sales_family['color'] = [random_color() for _ in range(len(ts_sales_family))]\nts_sales_cluster['color'] = [random_color() for _ in range(len(ts_sales_cluster))]\nts_sales_state['color'] = [random_color() for _ in range(len(ts_sales_state))]\nts_sales_city['color'] = [random_color() for _ in range(len(ts_sales_city))]\n\nfig = make_subplots(rows=5, cols=2, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                           [{\"colspan\": 2}, None],\n                           [{\"type\": \"pie\"},{\"type\": \"bar\"} ],\n                           [{\"colspan\": 2}, None],\n                           [{\"colspan\": 2}, None]],\n                    column_widths=[0.5, 0.5], vertical_spacing=0, horizontal_spacing=0.02,\n                    subplot_titles=(\"Average Product Sales\", \n                                    \"Highest Average Sales in Store Types\",\n                                    \"\\n\",\n                                    \"\\n\"\n                                    \"Average Sales per State\",\n                                    \"Average Sales per Cluster\",\n                                    \"\\n\",\n                                    \"\\n\"\n                                    \"Average Sales per City\",\n                                   ))\nfig.add_trace(go.Bar(x=ts_sales_family['sales'], \n                     y=ts_sales_family['family'],\n                     marker=dict(color= ts_sales_family['color']),\n                     name='Family', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=ts_sales_type['sales'], labels=ts_sales_type['type'], name='Store Type',\n                     marker=dict(colors= ts_sales_family['color']), hole=0.4+(0.1 * random.randrange(-2, 2)),\n                     hoverinfo='label+percent+value', textinfo='label'),row=1, col=2)\nfig.add_trace(go.Bar(x=ts_sales_cluster['sales'], \n                     y=ts_sales_cluster['cluster'],\n                     marker=dict(color= ts_sales_cluster['color']),\n                     name='Cluster', orientation='h'), \n                     row=3, col=2)\nfig.add_trace(go.Pie(values=ts_sales_state['sales'], labels=ts_sales_state['state'], name='State',\n                     marker=dict(colors= ts_sales_state['color']), hole=0.4+(0.1 * random.randrange(-2, 2)),\n                     hoverinfo='label+percent+value', textinfo='label'),row=3, col=1)\nfig.add_trace(go.Bar(x=ts_sales_city['city'], y=ts_sales_city['sales'], \n                     marker=dict(color= ts_sales_city['color']), name='City'), \n                     row=5, col=1)\nfig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(visible=False, row=3, col=2)\nfig.update_xaxes(tickmode = 'array', tickvals=ts_sales_cluster.cluster, ticktext=[i for i in range(1,17)], row=3, col=2)\nfig.update_yaxes(visible=False, row=5, col=1)\nplt_themes = [\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]\nfig.update_layout(height=1800, bargap=0.3,\n                  margin=dict(b=10,r=20,l=20), \n                  xaxis=dict(tickmode='linear'),\n                  title_text=\"Average Sales Analysis\",\n                  title_x=0.5,\n                  template=plt_themes[random.randrange(0, 6)],\n                  title_font=dict(size=25, color=random_color(), family=\"Roboto, sans-serif\"),\n                  font=dict(color=random_color()),\n                  hoverlabel=dict(bgcolor=random_color(), font_size=15, font_family=\"Roboto, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:50.478401Z","iopub.execute_input":"2023-12-03T15:16:50.478732Z","iopub.status.idle":"2023-12-03T15:16:52.930953Z","shell.execute_reply.started":"2023-12-03T15:16:50.478688Z","shell.execute_reply":"2023-12-03T15:16:52.930058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let us explore some ECDF and statistical graphs","metadata":{}},{"cell_type":"code","source":"sns.set(style = \"ticks\")# to format into seaborn \nc = '#386B7F' # basic color for plots\nplt.figure(figsize = (12, 8))\n\nplt.subplot(211)\ncdf = ECDF(train['sales'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c)\nplt.xlabel('Sales')\nplt.ylabel('ECDF')\n\n# plot second ECDF  \nplt.subplot(212)\ncdf = ECDF(train['onpromotion'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c)\nplt.xlabel('Promotions')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:52.93215Z","iopub.execute_input":"2023-12-03T15:16:52.932422Z","iopub.status.idle":"2023-12-03T15:16:54.701437Z","shell.execute_reply.started":"2023-12-03T15:16:52.93239Z","shell.execute_reply":"2023-12-03T15:16:54.700802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let us check the content for the store_data including sum of null values","metadata":{}},{"cell_type":"code","source":"store_data.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-03T15:16:54.702636Z","iopub.execute_input":"2023-12-03T15:16:54.703499Z","iopub.status.idle":"2023-12-03T15:16:54.716367Z","shell.execute_reply.started":"2023-12-03T15:16:54.703363Z","shell.execute_reply":"2023-12-03T15:16:54.715479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_data.isnull().sum()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-03T15:16:54.717637Z","iopub.execute_input":"2023-12-03T15:16:54.717878Z","iopub.status.idle":"2023-12-03T15:16:54.730608Z","shell.execute_reply.started":"2023-12-03T15:16:54.717851Z","shell.execute_reply":"2023-12-03T15:16:54.729601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let us ejoin store data with stores by number","metadata":{}},{"cell_type":"code","source":"print(\"Joining train set with an additional store information.\")\n\n# by specifying inner join we make sure that only those observations \n# that are present in both train and store sets are merged together\ntrain_store = pd.merge(train, store_data, how = 'inner', on = 'store_nbr')\n\nprint(\"In total: \", train_store.shape)\ntrain_store.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:54.734717Z","iopub.execute_input":"2023-12-03T15:16:54.735017Z","iopub.status.idle":"2023-12-03T15:16:55.442165Z","shell.execute_reply.started":"2023-12-03T15:16:54.734975Z","shell.execute_reply":"2023-12-03T15:16:55.441258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot a Barplot on Sales and Family columns","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,12))\nsns.barplot(x=\"sales\", y=\"family\", data=train_store.sort_values(by=['sales','family'], ascending=False),\n            label=\"Sales\", color=\"b\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:16:55.443579Z","iopub.execute_input":"2023-12-03T15:16:55.444356Z","iopub.status.idle":"2023-12-03T15:17:23.045925Z","shell.execute_reply.started":"2023-12-03T15:16:55.444284Z","shell.execute_reply":"2023-12-03T15:17:23.044991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot a Seaborn Relplot ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(24,18))\nsns.relplot(x=\"onpromotion\", y=\"sales\", hue=\"city\", size=\"sales\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=train_store)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:17:23.047469Z","iopub.execute_input":"2023-12-03T15:17:23.0484Z","iopub.status.idle":"2023-12-03T15:21:23.207016Z","shell.execute_reply.started":"2023-12-03T15:17:23.048354Z","shell.execute_reply":"2023-12-03T15:21:23.206131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some EDA on Holidays","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(20,20))\nplt.subplot(211)\nplt.title('Counts of Type Of Holidays')\nsns.countplot(x=holidays_data.type, hue=holidays_data.locale)\nplt.legend(loc='upper right')\nplt.subplot(212)\nplt.title('Counts of type of holiday')\nsns.countplot(x=holidays_data.locale)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:23.208456Z","iopub.execute_input":"2023-12-03T15:21:23.208718Z","iopub.status.idle":"2023-12-03T15:21:23.815081Z","shell.execute_reply.started":"2023-12-03T15:21:23.20869Z","shell.execute_reply":"2023-12-03T15:21:23.814156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot a relplot on City , Sales and Family with a hue on State","metadata":{}},{"cell_type":"code","source":"g = sns.relplot(\n    data=train_store,\n    x=\"city\", y=\"sales\", col=\"family\", hue=\"state\",\n    kind=\"line\", palette=\"crest\", linewidth=4, zorder=5,\n    col_wrap=3, height=2, aspect=1.5, legend=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:23.816422Z","iopub.execute_input":"2023-12-03T15:21:23.816683Z","iopub.status.idle":"2023-12-03T15:22:53.760839Z","shell.execute_reply.started":"2023-12-03T15:21:23.816653Z","shell.execute_reply":"2023-12-03T15:22:53.760172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some Store Data EDA","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,3,figsize=(21,20))\nplt.subplot(311)\nsns.countplot(x=store_data.type, order = store_data.type.value_counts().index)\nplt.subplot(312)\nsns.countplot(y=store_data.cluster, order = store_data.cluster.value_counts().index)\nplt.subplot(313)\nsns.countplot(y=store_data.city, order = store_data.city.value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:53.762422Z","iopub.execute_input":"2023-12-03T15:22:53.762715Z","iopub.status.idle":"2023-12-03T15:22:54.890043Z","shell.execute_reply.started":"2023-12-03T15:22:53.762675Z","shell.execute_reply":"2023-12-03T15:22:54.889143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot a factorplot on yearly sales by cluster and type of store","metadata":{}},{"cell_type":"code","source":"sns.factorplot(data = train_store, x =\"Year\", y = \"sales\", \n               col = 'type', # per store type in cols\n               hue = 'Month',\n               row = \"cluster\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:54.891182Z","iopub.execute_input":"2023-12-03T15:22:54.891413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot a Category Plot on Type and Cluster - Use Strip Plot","metadata":{}},{"cell_type":"code","source":"sns.catplot(x = 'type', y='cluster',data=store_data, kind='strip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot a Category Plot on Type and Cluster - Use Swarm  Plot","metadata":{}},{"cell_type":"code","source":"sns.catplot(x = 'type', y='cluster',data=store_data, kind='swarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let us group by state the store city, type of store and sales summary is ","metadata":{}},{"cell_type":"code","source":"\ntrain_store.groupby('state')['city','type','sales'].sum()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let us group by city the store by state, type of store and sales summary ","metadata":{}},{"cell_type":"code","source":"train_store.groupby('city')['state','type','sales'].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Barchart on Store Sales by Store and Sales Value","metadata":{}},{"cell_type":"code","source":"from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\nx=train_store.groupby('store_nbr')['city','type','sales'].sum()\nx=x.sort_values(by='sales',ascending=False)\nx=x.iloc[0:55].reset_index()\n\nplt.figure(figsize=(20,15))\nax= sns.barplot( x.store_nbr ,x.sales, alpha=0.8,  palette=\"Spectral\")\n# annotation here\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" % round((p.get_height()/1000000),2), (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n\n\nplt.ticklabel_format(style='plain', axis='y')\nplt.title(\"Sales per Store in Millions\", fontsize=20)\nplt.ylabel('Total Sales', fontsize=12)\nplt.xlabel('Store', fontsize=12)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Barchart on Store Sales by City and Sales Value","metadata":{}},{"cell_type":"code","source":"from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\nx=train_store.groupby('city')['store_nbr','type','sales'].sum()\nx=x.sort_values(by='sales',ascending=False)\nx=x.iloc[0:55].reset_index()\n\nplt.figure(figsize=(20,15))\nax= sns.barplot( x.city ,x.sales, alpha=0.8,  palette=\"Spectral\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n# annotation here\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" % round((p.get_height()/1000000),2), (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n\n\nplt.ticklabel_format(style='plain', axis='y')\nplt.title(\"Sales per City in Millions\", fontsize=20)\nplt.ylabel('Total Sales', fontsize=12)\nplt.xlabel('City', fontsize=12)\nloc, labels = plt.xticks()\nax.set_xticklabels(labels, rotation=60)\nsns.despine(left=True, bottom=True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Barchart on Store Sales by State and Sales Value","metadata":{}},{"cell_type":"code","source":"from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\nsns.set(font_scale=0.8)\nx=train_store.groupby('state')['store_nbr','type','sales'].sum()\nx=x.sort_values(by='sales',ascending=False)\nx=x.iloc[0:55].reset_index()\n\nplt.figure(figsize=(20,15))\nax= sns.barplot( x.sales ,x.state, alpha=0.8,  palette=\"Spectral\")\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" % round((p.get_width()/1000000),2), (p.get_x() + p.get_width(), p.get_y() + 1.2),\n                xytext=(5, 15), textcoords='offset points')\n\nsns.set(font_scale=0.8)\nplt.title(\"Sales per State\", fontsize=20)\nplt.ylabel('Total Sales', fontsize=12)\nplt.xlabel('State', fontsize=12)\nloc, labels = plt.xticks()\nax.set_xticklabels(labels, rotation=60)\nsns.despine(left=True, bottom=True)\nsns.set_style('whitegrid')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Submission Sample Data","metadata":{}},{"cell_type":"code","source":"# importing data\ntrain = pd.read_csv(path+'train.csv', parse_dates = True, low_memory = False, index_col = 'date')\ntest = pd.read_csv(path+'test.csv')\nsubmission_sample = pd.read_csv(path+'sample_submission.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let us check the training dataset value","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reset the index values and take care of date Vs datetime fields","metadata":{}},{"cell_type":"code","source":"train=train.reset_index()\n# sales for the store number 1 (StoreType C)\nsales = train[train.store_nbr == 44].loc[:, ['date', 'sales']]\n\nsales = sales.sort_index(ascending = False)\n\n# to datetime64\nsales['date'] = pd.DatetimeIndex(sales['date'])\nsales.dtypes\n\n# from the prophet documentation every variables should have specific names\nsales = sales.rename(columns = {'date': 'ds',\n                                'sales': 'y'})\nsales.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot daily sales ","metadata":{}},{"cell_type":"code","source":"\n# plot daily sales\nax = sales.set_index('ds').plot(figsize = (12, 4), color = c)\nax.set_ylabel('Daily Sales')\nax.set_xlabel('Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for holiday types and locale","metadata":{}},{"cell_type":"code","source":"holidays_data.groupby(['type','locale']).head(100)\nprint(holidays_data.type.unique())\nprint(holidays_data.locale.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for holiday types and and group based on national, state and general holiday types","metadata":{}},{"cell_type":"code","source":"# create holidays dataframe\nstate_dates = holidays_data[((holidays_data.locale == 'National') |\n                             (holidays_data.locale == 'Regional') &\n                             (holidays_data.type == 'Holiday'))].loc[:, 'date'].values\n\nstate = pd.DataFrame({'holiday': 'state_holiday',\n                      'ds': pd.to_datetime(state_dates)})\nholidays = state      \nholidays.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set parameters for Facebook Prophet model and use it to fit the data","metadata":{}},{"cell_type":"code","source":"# set the uncertainty interval to 95% (the Prophet default is 80%)\nmy_model = Prophet(interval_width = 0.95, \n                   holidays = holidays)\nmy_model.fit(sales)\n\n# dataframe that extends into future 52 weeks \nfuture_dates = my_model.make_future_dataframe(periods = 52*7)\n\nprint(\"First week to forecast.\")\nfuture_dates.tail(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict the values based on the fitted model","metadata":{}},{"cell_type":"code","source":"# predictions\nforecast = my_model.predict(future_dates)\n\n# preditions for last week\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Forecast future values","metadata":{}},{"cell_type":"code","source":"fc = forecast[['ds', 'yhat']].rename(columns = {'Date': 'ds', 'Forecast': 'yhat'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the Prophet Forecast model","metadata":{}},{"cell_type":"code","source":"# visualizing predicions\nmy_model.plot(forecast);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the Prophet Forecast model components","metadata":{}},{"cell_type":"code","source":"my_model.plot_components(forecast);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying FastAI Tabular Libraries\n\n###### Thanks to the inspiration https://www.kaggle.com/alibaba19/fastai-training-inference-pipeline/notebook ","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import *\nfrom sklearn.metrics import mean_squared_log_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build a randomsplitter for the test and training datasets","metadata":{}},{"cell_type":"code","source":"splits = RandomSplitter(valid_pct=0.2)(range_of(train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build a Tabular Pandas model","metadata":{}},{"cell_type":"code","source":"tab_pd = TabularPandas(train, \n                       procs = [Categorify, FillMissing, Normalize],\n                       cat_names=['store_nbr', 'family'],\n                       cont_names =['onpromotion'],\n                       y_names = 'sales',\n                       splits = splits)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load teh dataframe","metadata":{}},{"cell_type":"code","source":"dls = tab_pd.dataloaders(bs=1024)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run the Show_batch function","metadata":{}},{"cell_type":"code","source":"dls.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict the sales max value","metadata":{}},{"cell_type":"code","source":"train.sales.max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle(inp, targ):\n    inp = inp.detach().cpu().numpy()\n    targ = targ.detach().cpu().numpy()\n    return np.sqrt(mean_squared_log_error(targ, inp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the tabular learner model based on the data shared","metadata":{}},{"cell_type":"code","source":"learner = tabular_learner(dls, metrics=rmsle, y_range=(0, 125000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit the model","metadata":{}},{"cell_type":"code","source":"learner.fit_one_cycle(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(path+'test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict the values","metadata":{}},{"cell_type":"code","source":"dl = learner.dls.test_dl(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = learner.get_preds(dl=dl)\npreds[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit the predictions","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"submission_sample = pd.read_csv(path+'sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_sample.sales = preds[0].detach().cpu().numpy()\nsubmission_sample.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_sample.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1> Conclusion</h1>\n<hr>\n<h2> Summary </h2>\n<p> I hope you liked the outcome </p>\n<hr>\n<img src=\"https://cdn.pixabay.com/photo/2020/04/22/11/59/thank-you-5077738_1280.jpg\" alt=\"Thank You\" width=\"500\" height=\"600\">>","metadata":{}}]}