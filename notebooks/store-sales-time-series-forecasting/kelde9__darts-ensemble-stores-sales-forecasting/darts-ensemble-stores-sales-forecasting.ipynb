{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**In this article, I’ll guide you through the code that allowed me to reach the first position in the [Store Sales – Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting).**\n\nBy exploring this code, you will be able to copy it and reach the top of the ranking... or even improve it to beat me!\n\nSo why am I sharing this code with you if I’m going to lose my ranking?\n\nFirst of all, because I like open source and the idea that everyone can access the information.\n\nSecond, because this competition is in the *Getting Started* category anyway. This implies that the ranking is renewed every 3 months. But above all, that there is no deadline to participate in this challenge.\n\nThe progress in AI being what it is, it is obvious that my record will be beaten in the months or years to come. So, instead of constraining this progress, why not encourage it by sharing my code?\n\nAnyway, that being said, for the sake of history (and honestly for my ego) I have recorded the ranking of April 04, 2023:\n\n<img src=\"https://inside-machinelearning.com/wp-content/uploads/2023/04/1st_place_Kaggle_Store_Sales-1.jpeg\">","metadata":{}},{"cell_type":"markdown","source":"To reach the first place I inspired myself from [Ferdinand Berr’s code](https://www.kaggle.com/code/ferdinandberr/darts-forecasting-deep-learning-global-models#4.1.-N-HiTS), adding some changes to it.\n\nFirst I modified the hyperparameters of the model he uses. Then I added a basic improvement technique that seems to have been neglected in this competition: [the Ensemble Method](https://inside-machinelearning.com/en/ensemble-methods/).\n\nThe idea is to combine the predictions of several models and to average them to obtain an optimal result. I detail the method [in this article](https://inside-machinelearning.com/en/ensemble-methods/).\n\n**The objective of this competition is to predict the number of sales that different stores located in Ecuador will generate.**\n\nTo make these predictions, we’ll have to rely on the past sales of the stores.\n\nThis type of variable is called a time series.\n\n> A **time series** is a sequence of data measured at regular intervals in time.\nIn our case, the data was recorded every day.\n\nIn the following I will detail the code I used. I have removed the extra information that was not helpful to the understanding to keep only the essential. You’ll see that the code is more airy than in the solution I used as a basis.\n\n**This tutorial is aimed at people with an intermediate level in AI, even if it can also be approached by beginners.**\n\nWe will combine several data and use a little known but powerful Machine Learning library for our task: [darts](https://unit8co.github.io/darts/).\n\nDarts allows to manipulate and predict the values of a time series easily.\n\nWithout further introduction, let’s get to work!","metadata":{}},{"cell_type":"markdown","source":"# **Data**\n\nFirst of all I propose to explore the dataset.\n\nOur goal is to predict future sales of stores located in Ecuador, for the dates August 16, 2017 to August 31, 2017 (16 days).\n\n**In our dataset, there are 54 stores for 33 product families.**\n\nWe need to predict the sales for each of these product families from each store. So `33 * 54 * 16 = 28,512` values to predict.\n\nLet's load the dataset with this line of code:","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-01T12:50:21.229189Z","iopub.execute_input":"2023-05-01T12:50:21.229609Z","iopub.status.idle":"2023-05-01T12:50:21.266352Z","shell.execute_reply.started":"2023-05-01T12:50:21.229572Z","shell.execute_reply":"2023-05-01T12:50:21.264975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To help us make the predictions, no less than 8 CSV files are provided.\n\n**Let’s display them to better understand our job.**","metadata":{}},{"cell_type":"markdown","source":"## **train.csv**\n\nFirst of all the main file: *train.csv*. It contains some features and the label to predict sales, the number of sales per day:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\ndisplay(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:21.268068Z","iopub.execute_input":"2023-05-01T12:50:21.268893Z","iopub.status.idle":"2023-05-01T12:50:25.123776Z","shell.execute_reply.started":"2023-05-01T12:50:21.268855Z","shell.execute_reply":"2023-05-01T12:50:25.122103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are the columns of the DataFrame:\n\n- `id` – the index of the row\n- `date` – the current date\n- `store_nbr` – the store\n- `family` – the product family\n- `sales` – number of sales in this family\n- `onpromotion` – the number of products on promotion in this family","metadata":{}},{"cell_type":"markdown","source":"## **holidays_events.csv**\n\nThe *holidays_events.csv* groups the national holidays. This information is independent of the store but can have an impact on sales.\n\nFor example, on a holiday, there might be more people in the city and therefore more customers in the stores. Or conversely, more people might go on vacation and therefore there would be fewer customers in the stores.","metadata":{}},{"cell_type":"code","source":"df_holidays_events = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\ndisplay(df_holidays_events.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:25.127269Z","iopub.execute_input":"2023-05-01T12:50:25.127735Z","iopub.status.idle":"2023-05-01T12:50:25.155566Z","shell.execute_reply.started":"2023-05-01T12:50:25.127688Z","shell.execute_reply":"2023-05-01T12:50:25.154125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are the columns in the DataFrame:\n\n- `date` – the date of the holiday\n- `type` – the type of holiday (`Holiday`, `Event`, `Transfer` (see `transferred` column), `Additional`, `Bridge`, `Work Day`)\n- `locale` – the scope of the event (Local, Regional, National)\n- `locale_name` – the city where the event takes place\n- `description` – name of the event\n- `transferred` – whether the event has been transferred (moved to another day) or not","metadata":{}},{"cell_type":"markdown","source":"## **oil.csv**\n\nThen a CSV file gathers the daily oil price from January 01, 2013 to August 31, 2017:","metadata":{}},{"cell_type":"code","source":"df_oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\ndisplay(df_oil.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:25.159031Z","iopub.execute_input":"2023-05-01T12:50:25.159866Z","iopub.status.idle":"2023-05-01T12:50:25.178363Z","shell.execute_reply.started":"2023-05-01T12:50:25.159812Z","shell.execute_reply":"2023-05-01T12:50:25.176926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **store.csv**\n\nThe *store.csv* file gathers information about the stores. There is one store per line so 54 lines:","metadata":{}},{"cell_type":"code","source":"df_stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\ndisplay(df_stores.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:25.180439Z","iopub.execute_input":"2023-05-01T12:50:25.181232Z","iopub.status.idle":"2023-05-01T12:50:25.201397Z","shell.execute_reply.started":"2023-05-01T12:50:25.181179Z","shell.execute_reply":"2023-05-01T12:50:25.200461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DataFrame columns:\n\n- `store_nbr` – the store\n- `city` – the city where the store is located\n- `state` – the state where the store is located\n- `type` – the type of the store\n- `cluster` – the number of similar stores in the vicinity","metadata":{}},{"cell_type":"markdown","source":"## **transactions.csv**\n\nThe *transactions.csv* file groups the daily transactions by stores:","metadata":{}},{"cell_type":"code","source":"df_transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\ndisplay(df_transactions.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:25.204982Z","iopub.execute_input":"2023-05-01T12:50:25.205866Z","iopub.status.idle":"2023-05-01T12:50:25.282113Z","shell.execute_reply.started":"2023-05-01T12:50:25.205808Z","shell.execute_reply":"2023-05-01T12:50:25.280753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: a transaction is a receipt created after a customer’s purchase","metadata":{}},{"cell_type":"markdown","source":"## **test.csv**\n\nFinally, we have the *test.csv* that will allow us to predict the `sale` column. The file starts on August 16, 2017 and ends on August 31, 2017. We also have the *sample_submission.csv* to fill in with the number of sales per day and per family:","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\ndf_sample_submission = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv')\ndisplay(df_test.head())\ndisplay(df_sample_submission.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:25.286258Z","iopub.execute_input":"2023-05-01T12:50:25.286646Z","iopub.status.idle":"2023-05-01T12:50:25.355214Z","shell.execute_reply.started":"2023-05-01T12:50:25.286608Z","shell.execute_reply":"2023-05-01T12:50:25.354289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The *test.csv* contains 5 columns:\n\n- `id` – the index of the row (which will be used to fill in the *sample_submission.csv* file)\ndate – the current date\n- `store_nbr` – the store\n- `family` – the product family\n- `sales` – the number of sales in this family\n- `onpromotion` – the number of products on promotion in this family\n\nNow that we know our dataset better, we can move on to the preprocessing step which will allow us to format our data to train our Machine Learning model.","metadata":{}},{"cell_type":"markdown","source":"# **Preprocessing**\n\nTo start the preprocessing, let's group the name of each product family and the number of each store:","metadata":{}},{"cell_type":"code","source":"family_list = df_train['family'].unique()\nstore_list = df_stores['store_nbr'].unique()\ndisplay(family_list)\ndisplay(store_list)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:25.357071Z","iopub.execute_input":"2023-05-01T12:50:25.358265Z","iopub.status.idle":"2023-05-01T12:50:25.615819Z","shell.execute_reply.started":"2023-05-01T12:50:25.358213Z","shell.execute_reply":"2023-05-01T12:50:25.614014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we assemble the `df_train` and `df_stores` datasets. By grouping this data in a single dataset, it will allow us to access the information more easily. In addition to that, we sort the sales of the DataFrame by date, by family and by stores:","metadata":{}},{"cell_type":"code","source":"train_merged = pd.merge(df_train, df_stores, on ='store_nbr')\ntrain_merged = train_merged.sort_values([\"store_nbr\",\"family\",\"date\"])\ntrain_merged = train_merged.astype({\"store_nbr\":'str', \"family\":'str', \"city\":'str',\n                          \"state\":'str', \"type\":'str', \"cluster\":'str'})\n\ndisplay(train_merged.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:25.61774Z","iopub.execute_input":"2023-05-01T12:50:25.618738Z","iopub.status.idle":"2023-05-01T12:50:32.482711Z","shell.execute_reply.started":"2023-05-01T12:50:25.618696Z","shell.execute_reply":"2023-05-01T12:50:32.481481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A time series being the number of sales made per day for a family of a store, this sorting will allow us to extract them more easily.\n\nSame thing for the test DataFrame, we sort the sales by date, by family and by stores:","metadata":{}},{"cell_type":"code","source":"df_test_dropped = df_test.drop(['onpromotion'], axis=1)\ndf_test_sorted = df_test_dropped.sort_values(by=['store_nbr','family'])\n\ndisplay(df_test_sorted.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:32.487068Z","iopub.execute_input":"2023-05-01T12:50:32.487677Z","iopub.status.idle":"2023-05-01T12:50:32.509631Z","shell.execute_reply.started":"2023-05-01T12:50:32.487628Z","shell.execute_reply":"2023-05-01T12:50:32.508392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to concretely create time series!","metadata":{}},{"cell_type":"markdown","source":"## **Main Time Series**\n\nAs I mentioned earlier, we are going to use a specific library for time series processing in Python: [Darts](https://unit8co.github.io/darts/).\n\nDarts allows us to easily manipulate time series.\n\nI invite you to install the darts library","metadata":{}},{"cell_type":"code","source":"!pip install darts==0.23.1 &> /dev/null","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:50:32.511089Z","iopub.execute_input":"2023-05-01T12:50:32.511613Z","iopub.status.idle":"2023-05-01T12:51:16.606369Z","shell.execute_reply.started":"2023-05-01T12:50:32.511529Z","shell.execute_reply":"2023-05-01T12:51:16.60452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like Pandas with its DataFrame, the Darts library offers us its class allowing to manipulate time series : the `TimeSeries`.\n\nWe will use these class to extract our time series.\n\nBut before that, we have to discuss our strategy.","metadata":{}},{"cell_type":"markdown","source":"### **Strategy**\n\n> Reminder: Our objective is to predict for each family in each store the number of future sales. There are 33 families for 54 stores.\n\n**From there, several routes can be taken.**\n\nThe most obvious one is to train a Machine Learning model on our dataset. On the 1782 time series.\n\nThis is obvious because it allows us to use a maximum of data to train our model. It will then be able to generalize its knowledge to each of the product families. With such a strategy, our model will have a good global prediction.\n\nA less obvious strategy, but quite logical, is to train a Machine Learning model for each time series.\n\nIndeed, by assigning a model to each series, we ensure that each model is specialized in its task and therefore performs well in its prediction, and this for each product family of each store.\n\nThe problem with the first method is that the model, having only a general knowledge of our data, will not have an optimal prediction on each specific time series.\n\nThe problem with the second method is that the model will be specialized on each time series, but will lack data to perfect its training.\n\n**We will therefore not take any of the strategies described above.**\n\nOur strategy is to position ourselves between these two methods.\n\nAfter several tests and analyses of our data (which I will not detail here), we understand that the sales by families seem to be correlated across stores.\n\n**Hence we'll train a Machine Learning model by product family.**\n\nWe will have 33 models, each trained on 54 time series.\n\nThis is a good compromise, because it allows us to have a lot of data to train a model. But also to obtain, at the end of the training, a model specialized in its task (because trained on a single product family).\n\nNow that you know the strategy, let’s implement it!","metadata":{}},{"cell_type":"markdown","source":"### **sales**\n\n#### **Extract the time series**\n\nFor each product family, we will gather all the time series concerning it.\n\nSo we will have 33 sub-datasets. These datasets will be contained in the `family_TS_dict` dictionary.\n\nIn the following lines of code, we extract the `TimeSeries` of the 54 stores for each family.\n\nThese TimeSeries will group the sales by family, the date of each sale, but also the dependent covariates (indicated with `group_cols` and `static_cols`) of these sales: `store_nbr`, `family`, `city`, `state`, `type`, `cluster` :","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport darts\nfrom darts import TimeSeries\n\nfamily_TS_dict = {}\n\nfor family in family_list:\n  df_family = train_merged.loc[train_merged['family'] == family]\n\n  list_of_TS_family = TimeSeries.from_group_dataframe(\n                                df_family,\n                                time_col=\"date\",\n                                group_cols=[\"store_nbr\",\"family\"],\n                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"],\n                                value_cols=\"sales\",\n                                fill_missing_dates=True,\n                                freq='D')\n  for ts in list_of_TS_family:\n            ts = ts.astype(np.float32)\n\n  list_of_TS_family = sorted(list_of_TS_family, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n  family_TS_dict[family] = list_of_TS_family","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:51:16.608588Z","iopub.execute_input":"2023-05-01T12:51:16.608958Z","iopub.status.idle":"2023-05-01T12:51:52.891175Z","shell.execute_reply.started":"2023-05-01T12:51:16.608922Z","shell.execute_reply":"2023-05-01T12:51:52.889794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also see that we indicate `fill_missing_dates=True` because in the dataset, the sales of each December 25th are missing.\n\nWe also indicate `freq='D'`, to indicate that the interval for the values of the time series is in days (D for day).\n\nFinally, we indicate that the values of the `TimeSeries` must be interpreted in `float32` and that the time series must be sorted by stores.\n\nWe can display the first time series of the first family:","metadata":{}},{"cell_type":"code","source":"display(family_TS_dict['AUTOMOTIVE'][0])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:51:52.893045Z","iopub.execute_input":"2023-05-01T12:51:52.893598Z","iopub.status.idle":"2023-05-01T12:51:52.930767Z","shell.execute_reply.started":"2023-05-01T12:51:52.893544Z","shell.execute_reply":"2023-05-01T12:51:52.92918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We retrieve all the values indicated above: the number of sales, the date of each sale in `Coordinates > date`, and the dependent covariates in `Attributes > static_covariates`.\n\nYou can also see that the length of the time series is 1688. Originally it was 1684 but we added the values of the four December 25s that are missing from the dataset.\n\nThen we apply a normalization to our `TimeSeries`.","metadata":{}},{"cell_type":"markdown","source":"#### **Normalizing time series**\n\nNormalization is a technique used to improve the performance of a Machine Learning model by facilitating its training. I let you refer to [our article on the subject](https://inside-machinelearning.com/en/normalize-your-data/) if you want to know more.\n\nWe can easily normalize a `TimeSeries` with the `Scaler` function of darts.\n\nMoreover, we will further optimize the training of the model by one hot encoding our covariates. We implement the one hot encoding via the `StaticCovariatesTransformer` function.","metadata":{}},{"cell_type":"code","source":"from darts.dataprocessing import Pipeline\nfrom darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\nimport sklearn\n\nfamily_pipeline_dict = {}\nfamily_TS_transformed_dict = {}\n\nfor key in family_TS_dict:\n  train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n  static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\")\n  log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n  train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n\n  train_pipeline = Pipeline([train_filler,\n                             static_cov_transformer,\n                             log_transformer,\n                             train_scaler])\n     \n  training_transformed = train_pipeline.fit_transform(family_TS_dict[key])\n  family_pipeline_dict[key] = train_pipeline\n  family_TS_transformed_dict[key] = training_transformed","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:51:52.933063Z","iopub.execute_input":"2023-05-01T12:51:52.93358Z","iopub.status.idle":"2023-05-01T12:53:05.273606Z","shell.execute_reply.started":"2023-05-01T12:51:52.933527Z","shell.execute_reply":"2023-05-01T12:53:05.272503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can display the first transformed `TimeSeries` of the first family:","metadata":{}},{"cell_type":"code","source":"display(family_TS_transformed_dict['AUTOMOTIVE'][0])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:05.275403Z","iopub.execute_input":"2023-05-01T12:53:05.275897Z","iopub.status.idle":"2023-05-01T12:53:05.322965Z","shell.execute_reply.started":"2023-05-01T12:53:05.275839Z","shell.execute_reply":"2023-05-01T12:53:05.321593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that the sales have been normalized and that the `static_covariates` have been one hot encoded.\n\nWe now have our main time series that will allow us to train our model.\n\nWhy not expand our dataset with other covariates?","metadata":{}},{"cell_type":"markdown","source":"## **Covariates**\n\n> **A covariate** is a variable that helps to predict a target variable.\n\nThis covariate can be dependent on the target variable. For example, the type of store, `type`, where the sales are made. But it can also be independent. For example, the price of oil on the day of the sale of a product.\n\nThis covariate can be known in advance, for example in our dataset we have the price of oil from January 1, 2013 to August 31, 2017. In this case, we talk about a **future covariate**.\n\nThere are also **past covariates**. These are covariates that are not known in advance. For example in our dataset, the transactions are known for the dates January 1, 2013 to August 15, 2017.","metadata":{}},{"cell_type":"markdown","source":"### **Date**\n\nThe first covariate we are interested in is the date.\n\nThe date is a future covariate because we know the date of the coming days.\n\nIt has, in many cases, an impact on the traffic of a store. For example, we can expect that on Saturday there will be more customers in the store than on Monday.\n\nBut it can also be expected that during the summer vacations the store will be less busy than in normal times.\n\n**Hence every little detail counts.**\n\nIn order not to miss anything, we will extract as much information as possible from this date. Here, 7 columns :\n\n- `year` – year\n- `month` – month\n- `day` – day\n- `dayofyear` – day of the year (for example February 1 is the 32nd day of the year)\n- `weekday` – day of the week (there are 7 days in a week)\n- `weekofyear` – week of the year (there are 52 weeks in a year)\n- `linear_increase` – the index of the interval","metadata":{}},{"cell_type":"code","source":"from darts.utils.timeseries_generation import datetime_attribute_timeseries\n\nfull_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\n\n\nyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"year\")\nmonth = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"month\")\nday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"day\")\ndayofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofyear\")\nweekday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofweek\")\nweekofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"weekofyear\")\ntimesteps = TimeSeries.from_times_and_values(times=full_time_period,\n                                             values=np.arange(len(full_time_period)),\n                                             columns=[\"linear_increase\"])\n\ntime_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\ntime_cov = time_cov.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:05.324467Z","iopub.execute_input":"2023-05-01T12:53:05.325199Z","iopub.status.idle":"2023-05-01T12:53:05.391563Z","shell.execute_reply.started":"2023-05-01T12:53:05.325159Z","shell.execute_reply":"2023-05-01T12:53:05.390018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is what it gives us for the date at index `100`:","metadata":{}},{"cell_type":"code","source":"display(print(time_cov.components.values))\ndisplay(time_cov[100])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:05.393526Z","iopub.execute_input":"2023-05-01T12:53:05.393887Z","iopub.status.idle":"2023-05-01T12:53:05.414036Z","shell.execute_reply.started":"2023-05-01T12:53:05.393851Z","shell.execute_reply":"2023-05-01T12:53:05.412727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And of course, we will normalize this data:","metadata":{}},{"cell_type":"code","source":"time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\ntime_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\ntime_cov_scaler.fit(time_cov_train)\ntime_cov_transformed = time_cov_scaler.transform(time_cov)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:05.415462Z","iopub.execute_input":"2023-05-01T12:53:05.41581Z","iopub.status.idle":"2023-05-01T12:53:05.455864Z","shell.execute_reply.started":"2023-05-01T12:53:05.415775Z","shell.execute_reply":"2023-05-01T12:53:05.454474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also see that a split is made between the dates before August 15, 2017 and after (dates that will be used in the prediction).","metadata":{}},{"cell_type":"markdown","source":"### **Oil**\n\nAs said before, the price of oil is a future covariate because it is known in advance.\n\nHere, we will not simply extract the daily oil price but we will calculate the moving average.\n\n> The **moving average in X**, is an average of the current value and the X-1 previous values of a time series.\n\nFor example the moving average in 7 is the average of `(t + t-1 + … + t-6) / 7`. It is calculated at each `t`, that’s why it is called “moving”.\n\n**Calculating the moving average allows us to remove the momentary fluctuations of a value and thus to accentuate the long-term trends.**\n\nThe moving average is used in trading, but more generally in Time Series Analysis.\n\nIn the following code, we calculate the moving average in 7 and 28 of the oil price. And of course, we apply a normalization :","metadata":{}},{"cell_type":"code","source":"from darts.models import MovingAverage\n# Oil Price\n\noil = TimeSeries.from_dataframe(df_oil, \n                                time_col = 'date', \n                                value_cols = ['dcoilwtico'],\n                                freq = 'D')\n\noil = oil.astype(np.float32)\n\n# Transform\noil_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\noil_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\noil_pipeline = Pipeline([oil_filler, oil_scaler])\noil_transformed = oil_pipeline.fit_transform(oil)\n\n# Moving Averages for Oil Price\noil_moving_average_7 = MovingAverage(window=7)\noil_moving_average_28 = MovingAverage(window=28)\n\noil_moving_averages = []\n\nma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\nma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\nma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\nma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\noil_moving_averages = ma_7.stack(ma_28)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:05.457464Z","iopub.execute_input":"2023-05-01T12:53:05.458319Z","iopub.status.idle":"2023-05-01T12:53:18.545203Z","shell.execute_reply.started":"2023-05-01T12:53:05.458268Z","shell.execute_reply":"2023-05-01T12:53:18.54412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the result obtained at index `100`:","metadata":{}},{"cell_type":"code","source":"display(oil_moving_averages[100])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:18.54688Z","iopub.execute_input":"2023-05-01T12:53:18.547265Z","iopub.status.idle":"2023-05-01T12:53:18.566423Z","shell.execute_reply.started":"2023-05-01T12:53:18.547227Z","shell.execute_reply":"2023-05-01T12:53:18.565061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Holidays**\n\nLet’s now focus on the holidays.\n\nHere, Ferdinand Berr has implemented functions to detail these holidays. In particular, he adds information about whether the holiday is Christmas day, whether it is a soccer game day, etc:","metadata":{}},{"cell_type":"code","source":"def holiday_list(df_stores):\n\n    listofseries = []\n    \n    for i in range(0,len(df_stores)):\n            \n            df_holiday_dummies = pd.DataFrame(columns=['date'])\n            df_holiday_dummies[\"date\"] = df_holidays_events[\"date\"]\n            \n            df_holiday_dummies[\"national_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & (df_holidays_events[\"locale\"] == \"National\")), 1, 0)\n\n            df_holiday_dummies[\"earthquake_relief\"] = np.where(df_holidays_events['description'].str.contains('Terremoto Manabi'), 1, 0)\n\n            df_holiday_dummies[\"christmas\"] = np.where(df_holidays_events['description'].str.contains('Navidad'), 1, 0)\n\n            df_holiday_dummies[\"football_event\"] = np.where(df_holidays_events['description'].str.contains('futbol'), 1, 0)\n\n            df_holiday_dummies[\"national_event\"] = np.where(((df_holidays_events[\"type\"] == \"Event\") & (df_holidays_events[\"locale\"] == \"National\") & (~df_holidays_events['description'].str.contains('Terremoto Manabi')) & (~df_holidays_events['description'].str.contains('futbol'))), 1, 0)\n\n            df_holiday_dummies[\"work_day\"] = np.where((df_holidays_events[\"type\"] == \"Work Day\"), 1, 0)\n\n            df_holiday_dummies[\"local_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & ((df_holidays_events[\"locale_name\"] == df_stores['state'][i]) | (df_holidays_events[\"locale_name\"] == df_stores['city'][i]))), 1, 0)\n                     \n            listofseries.append(df_holiday_dummies)\n\n    return listofseries","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:18.568393Z","iopub.execute_input":"2023-05-01T12:53:18.569742Z","iopub.status.idle":"2023-05-01T12:53:18.585037Z","shell.execute_reply.started":"2023-05-01T12:53:18.569685Z","shell.execute_reply":"2023-05-01T12:53:18.583663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we have a function to remove the days equal to 0 and the duplicates:","metadata":{}},{"cell_type":"code","source":"def remove_0_and_duplicates(holiday_list):\n\n    listofseries = []\n    \n    for i in range(0,len(holiday_list)):\n            \n            df_holiday_per_store = list_of_holidays_per_store[i].set_index('date')\n\n            df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store==0).all(axis=1)]\n            \n            df_holiday_per_store = df_holiday_per_store.groupby('date').agg({'national_holiday':'max', 'earthquake_relief':'max', \n                                   'christmas':'max', 'football_event':'max', \n                                   'national_event':'max', 'work_day':'max', \n                                   'local_holiday':'max'}).reset_index()\n\n            listofseries.append(df_holiday_per_store)\n\n    return listofseries","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:18.586993Z","iopub.execute_input":"2023-05-01T12:53:18.587656Z","iopub.status.idle":"2023-05-01T12:53:18.602253Z","shell.execute_reply.started":"2023-05-01T12:53:18.5876Z","shell.execute_reply":"2023-05-01T12:53:18.600775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally a function that allows us to have the holidays associated to each of the 54 stores :","metadata":{}},{"cell_type":"code","source":"def holiday_TS_list_54(holiday_list):\n\n    listofseries = []\n    \n    for i in range(0,54):\n            \n            holidays_TS = TimeSeries.from_dataframe(list_of_holidays_per_store[i], \n                                        time_col = 'date',\n                                        fill_missing_dates=True,\n                                        fillna_value=0,\n                                        freq='D')\n            \n            holidays_TS = holidays_TS.slice(pd.Timestamp('20130101'),pd.Timestamp('20170831'))\n            holidays_TS = holidays_TS.astype(np.float32)\n            listofseries.append(holidays_TS)\n\n    return listofseries","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:18.604189Z","iopub.execute_input":"2023-05-01T12:53:18.605418Z","iopub.status.idle":"2023-05-01T12:53:18.619903Z","shell.execute_reply.started":"2023-05-01T12:53:18.60533Z","shell.execute_reply":"2023-05-01T12:53:18.618527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we just need to apply these functions:","metadata":{}},{"cell_type":"code","source":"list_of_holidays_per_store = holiday_list(df_stores)\nlist_of_holidays_per_store = remove_0_and_duplicates(list_of_holidays_per_store)   \nlist_of_holidays_store = holiday_TS_list_54(list_of_holidays_per_store)\n\nholidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\nholidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n\nholidays_pipeline = Pipeline([holidays_filler, holidays_scaler])\nholidays_transformed = holidays_pipeline.fit_transform(list_of_holidays_store)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:18.621872Z","iopub.execute_input":"2023-05-01T12:53:18.622848Z","iopub.status.idle":"2023-05-01T12:53:20.919485Z","shell.execute_reply.started":"2023-05-01T12:53:18.622796Z","shell.execute_reply":"2023-05-01T12:53:20.918482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get 54 `TimeSeries` with 7 columns:\n\n- `national_holiday`\n- `earthquake_relief`\n- `christmas`\n- `football_event`\n- `national_event`\n- `work_day`\n- `local_holiday`\n\nHere is the `TimeSeries` index `100` for the first store:","metadata":{}},{"cell_type":"code","source":"display(len(holidays_transformed))\ndisplay(holidays_transformed[0].components.values)\ndisplay(holidays_transformed[0][100])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:20.920576Z","iopub.execute_input":"2023-05-01T12:53:20.920916Z","iopub.status.idle":"2023-05-01T12:53:20.946684Z","shell.execute_reply.started":"2023-05-01T12:53:20.920884Z","shell.execute_reply":"2023-05-01T12:53:20.945336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Promotion**\n\nThe last future covariate to process is the `onpromotion` column.\n\nIt gives us the number of items on promotion in a product family.\n\nHere the code is similar to the one used for the `sales` column. It allows to extract for each family, the time series of the 54 stores:","metadata":{}},{"cell_type":"code","source":"df_promotion = pd.concat([df_train, df_test], axis=0)\ndf_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\ndf_promotion.tail()\n\nfamily_promotion_dict = {}\n\nfor family in family_list:\n  df_family = df_promotion.loc[df_promotion['family'] == family]\n\n  list_of_TS_promo = TimeSeries.from_group_dataframe(\n                                df_family,\n                                time_col=\"date\",\n                                group_cols=[\"store_nbr\",\"family\"],\n                                value_cols=\"onpromotion\",\n                                fill_missing_dates=True,\n                                freq='D')\n  \n  for ts in list_of_TS_promo:\n    ts = ts.astype(np.float32)\n\n  family_promotion_dict[family] = list_of_TS_promo","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:20.948152Z","iopub.execute_input":"2023-05-01T12:53:20.948869Z","iopub.status.idle":"2023-05-01T12:53:56.853249Z","shell.execute_reply.started":"2023-05-01T12:53:20.948818Z","shell.execute_reply":"2023-05-01T12:53:56.851821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can display the first `TimeSeries` of the first family :","metadata":{}},{"cell_type":"code","source":"display(family_promotion_dict['AUTOMOTIVE'][0])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:56.854751Z","iopub.execute_input":"2023-05-01T12:53:56.85675Z","iopub.status.idle":"2023-05-01T12:53:56.87915Z","shell.execute_reply.started":"2023-05-01T12:53:56.856708Z","shell.execute_reply":"2023-05-01T12:53:56.877887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s go further by calculating also the moving average in 7 and 28, like for the oil price:","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\npromotion_transformed_dict = {}\n\nfor key in tqdm(family_promotion_dict):\n  promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n  promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n\n  promo_pipeline = Pipeline([promo_filler,\n                             promo_scaler])\n  \n  promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n  \n  # Moving Averages for Promotion Family Dictionaries\n  promo_moving_average_7 = MovingAverage(window=7)\n  promo_moving_average_28 = MovingAverage(window=28)\n\n  promotion_covs = []\n\n  for ts in promotion_transformed:\n    ma_7 = promo_moving_average_7.filter(ts)\n    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n    ma_7 = ma_7.astype(np.float32)\n    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n    ma_28 = promo_moving_average_28.filter(ts)\n    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n    ma_28 = ma_28.astype(np.float32)\n    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n    promo_and_mas = ts.stack(ma_7).stack(ma_28)\n    promotion_covs.append(promo_and_mas)\n\n  promotion_transformed_dict[key] = promotion_covs","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:53:56.887012Z","iopub.execute_input":"2023-05-01T12:53:56.88814Z","iopub.status.idle":"2023-05-01T12:55:29.607307Z","shell.execute_reply.started":"2023-05-01T12:53:56.888098Z","shell.execute_reply":"2023-05-01T12:55:29.605965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtain a normalized time series with 3 columns.\n\nWe can display the index `1` of the first `TimeSeries` of the first family:","metadata":{}},{"cell_type":"code","source":"display(promotion_transformed_dict['AUTOMOTIVE'][0].components.values)\ndisplay(promotion_transformed_dict['AUTOMOTIVE'][0][1])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:29.608935Z","iopub.execute_input":"2023-05-01T12:55:29.609259Z","iopub.status.idle":"2023-05-01T12:55:29.635055Z","shell.execute_reply.started":"2023-05-01T12:55:29.609227Z","shell.execute_reply":"2023-05-01T12:55:29.633598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Grouping the covariates**\n\nTo finish with the future covariates, we are going to gather them in the same `TimeSeries`.\n\nWe start with the time series of the dates, the oil price and the moving averages of the oil price that we group in the variable `general_covariates` :","metadata":{}},{"cell_type":"code","source":"general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:29.637235Z","iopub.execute_input":"2023-05-01T12:55:29.638212Z","iopub.status.idle":"2023-05-01T12:55:29.652488Z","shell.execute_reply.started":"2023-05-01T12:55:29.63816Z","shell.execute_reply":"2023-05-01T12:55:29.650775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then for each store, we gather the `TimeSeries` of the holidays with the `general_covariates` :","metadata":{}},{"cell_type":"code","source":"store_covariates_future = []\n\nfor store in range(0,len(store_list)):\n  stacked_covariates = holidays_transformed[store].stack(general_covariates)  \n  store_covariates_future.append(stacked_covariates)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:29.654238Z","iopub.execute_input":"2023-05-01T12:55:29.654818Z","iopub.status.idle":"2023-05-01T12:55:29.783741Z","shell.execute_reply.started":"2023-05-01T12:55:29.654778Z","shell.execute_reply":"2023-05-01T12:55:29.782469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, for each family, we combine the previously created covariates with the promotion covariates:","metadata":{}},{"cell_type":"code","source":"future_covariates_dict = {}\n\nfor key in tqdm(promotion_transformed_dict):\n\n  promotion_family = promotion_transformed_dict[key]\n  covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n\n  future_covariates_dict[key] = covariates_future","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:29.785438Z","iopub.execute_input":"2023-05-01T12:55:29.786277Z","iopub.status.idle":"2023-05-01T12:55:38.651862Z","shell.execute_reply.started":"2023-05-01T12:55:29.786227Z","shell.execute_reply":"2023-05-01T12:55:38.650215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are the different columns obtained for each `TimeSeries` of each family of each store:","metadata":{}},{"cell_type":"code","source":"display(future_covariates_dict['AUTOMOTIVE'][0].components)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:38.654648Z","iopub.execute_input":"2023-05-01T12:55:38.655117Z","iopub.status.idle":"2023-05-01T12:55:38.664495Z","shell.execute_reply.started":"2023-05-01T12:55:38.655068Z","shell.execute_reply":"2023-05-01T12:55:38.663094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Transactions – Past Covariates**\n\nBefore launching the training of the model, let’s extract the past covariates: the transactions.\n\nAs you might already have understand, after having taken the transactions for each store, we will normalize them:","metadata":{}},{"cell_type":"code","source":"df_transactions.sort_values([\"store_nbr\",\"date\"], inplace=True)\n\nTS_transactions_list = TimeSeries.from_group_dataframe(\n                                df_transactions,\n                                time_col=\"date\",\n                                group_cols=[\"store_nbr\"],\n                                value_cols=\"transactions\",\n                                fill_missing_dates=True,\n                                freq='D')\n\ntransactions_list = []\n\nfor ts in TS_transactions_list:\n            series = TimeSeries.from_series(ts.pd_series())\n            series = series.astype(np.float32)\n            transactions_list.append(series)\n\ntransactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\n\nfrom datetime import datetime, timedelta\n\ntransactions_list_full = []\n\nfor ts in transactions_list:\n  if ts.start_time() > pd.Timestamp('20130101'):\n    end_time = (ts.start_time() - timedelta(days=1))\n    delta = end_time - pd.Timestamp('20130101')\n    zero_series = TimeSeries.from_times_and_values(\n                              times=pd.date_range(start=pd.Timestamp('20130101'), \n                              end=end_time, freq=\"D\"),\n                              values=np.zeros(delta.days+1))\n    ts = zero_series.append(ts)\n    ts = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\")\n    transactions_list_full.append(ts)\n\ntransactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\ntransactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n\ntransactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\ntransactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:38.665713Z","iopub.execute_input":"2023-05-01T12:55:38.666033Z","iopub.status.idle":"2023-05-01T12:55:40.696595Z","shell.execute_reply.started":"2023-05-01T12:55:38.666001Z","shell.execute_reply":"2023-05-01T12:55:40.695389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the `TimeSeries` for the first store:","metadata":{}},{"cell_type":"code","source":"display(transactions_transformed[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:40.698248Z","iopub.execute_input":"2023-05-01T12:55:40.698605Z","iopub.status.idle":"2023-05-01T12:55:40.714731Z","shell.execute_reply.started":"2023-05-01T12:55:40.69857Z","shell.execute_reply":"2023-05-01T12:55:40.713423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are finally ready to create our Machine Learning model.","metadata":{}},{"cell_type":"markdown","source":"# **Machine Learning Model**\n\nNow, we will train a first Machine Learning model with the darts library to confirm that our data is consistent and that the predictions obtained are convincing.\n\nThen we will use ensemble methods to improve our final result.","metadata":{}},{"cell_type":"markdown","source":"## **Single model**\n\nThe Darts library offers us various Machine Learning models to use on `TimeSeries`.\n\nIn Ferdinand Berr’s solution, we can see that he uses different models:\n\n- `NHiTSModel` – score : 0.43265\n- `RNNModel` (with LSTM layers) – score : 0.55443\n- `TFTModel` – score : 0.43226\n- `ExponentialSmoothing` – score : 0.37411\n\nThese scores are obtained on validation data, artificially generated from the training data.\n\nPersonally, I decided to use the LightGBMModel, an implementation of the eponymous library model on which you will find [an article here](https://inside-machinelearning.com/en/lightgbm-guide/).\n\nWhy use this model ? Not after hours of practice and experimentation, but simply by using it and seeing that, alone, it gives me better results than the `ExponentialSmoothing`.\n\nAs explained in the Strategy section, we will train a Machine Learning model for each product family.\n\nSo for each family, we have to take the corresponding `TimeSeries` and send them to our Machine Learning model.\n\nFirst, we prepare the data:\n\n- `TCN_covariates` represents the future covariates associated with the target product family\n- `train_sliced` represents the number of sales associated with the target product family. The `slice_intersect` function that you can see used simply ensures that the components span the same time interval. In the case of different time intervals an error message will appear if we try to combine them.\n- `transactions_transformed`, the past covariates do not need to be indexed on the target family because there is only one global `TimeSeries` per store\n\nNext, we initialize hyperparameters for our model.\n\n**This is the key to model results.**\n\nBy modifying these hyperparameters you can improve the performance of the Machine Learning model.","metadata":{}},{"cell_type":"markdown","source":"### **Training**\n\nHere are the important hyperparameters:\n\n- `lags` – the number of past values on which we base our predictions\n- `lags_future_covariates` – the number of future covariate values on which we base our predictions. If we give a tuple, the left value represents the number of covariates in the past and the right value represents the number of covariates in the future\n- `lags_past_covariates` – the number of past covariate values on which we base our predictions\n\nFor these three hyperparameters, if a list is passed, we take the indexes associated with the numbers of this list. For example if we pass: `[-3, -4, -5]`, we take the indexes `t-3`, `t-4`, `t-5`. But if we pass an integer for example 10, we take the 10 previous values (or the 10 future values depending on the case).\n\nThe hyperparameters `output_chunk_length` controls the number of predicted values in the future, `random_state` ensures the reproducibility of the results and `gpu_use_dp` indicates if we want to use a GPU.\n\nAfter that we launch the training. And at the end, we save the trained model in a dictionary.","metadata":{}},{"cell_type":"code","source":"from darts.models import LightGBMModel\n\nLGBM_Models_Submission = {}\n\ndisplay(\"Training...\")\n\nfor family in tqdm(family_list):\n\n  sales_family = family_TS_transformed_dict[family]\n  training_data = [ts for ts in sales_family] \n  TCN_covariates = future_covariates_dict[family]\n  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n\n  LGBM_Model_Submission = LightGBMModel(lags = 63,\n                                        lags_future_covariates = (14,1),\n                                        lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22],\n                                        output_chunk_length=1,\n                                        random_state=2022,\n                                        gpu_use_dp= \"false\",\n                                        )\n     \n  LGBM_Model_Submission.fit(series=train_sliced, \n                        future_covariates=TCN_covariates,\n                        past_covariates=transactions_transformed)\n\n  LGBM_Models_Submission[family] = LGBM_Model_Submission","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:55:40.716869Z","iopub.execute_input":"2023-05-01T12:55:40.717326Z","iopub.status.idle":"2023-05-01T13:05:41.218002Z","shell.execute_reply.started":"2023-05-01T12:55:40.717288Z","shell.execute_reply":"2023-05-01T13:05:41.216727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above code, we only use `lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22]`, why? Because during the 16th prediction (the one of August 31, 2017), the values of the past covariates from -1 to -15 are not known.\n\nAfter training, we obtain 33 Machine Learning models stored in `LGBM_Models_Submission`.","metadata":{}},{"cell_type":"markdown","source":"### **Predict**\n\nWe can now perform the predictions:","metadata":{}},{"cell_type":"code","source":"display(\"Predictions...\")\n\nLGBM_Forecasts_Families_Submission = {}\n\nfor family in tqdm(family_list):\n\n  sales_family = family_TS_transformed_dict[family]\n  training_data = [ts for ts in sales_family]\n  LGBM_covariates = future_covariates_dict[family]\n  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n\n  forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n                                         series=train_sliced,\n                                         future_covariates=LGBM_covariates,\n                                         past_covariates=transactions_transformed)\n  \n  LGBM_Forecasts_Families_Submission[family] = forecast_LGBM","metadata":{"execution":{"iopub.status.busy":"2023-05-01T13:05:41.220122Z","iopub.execute_input":"2023-05-01T13:05:41.221003Z","iopub.status.idle":"2023-05-01T13:06:24.341041Z","shell.execute_reply.started":"2023-05-01T13:05:41.220954Z","shell.execute_reply":"2023-05-01T13:06:24.339835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: even if the model has an `output_chunk_length` of 1, we can directly instruct it to predict 16 values in the future.\n\nWe now have our predictions. If you follow well, you know the next step.\n\nPreviously, we normalized our data with the `Scaler` function. So the predicted data are also normalized.\n\nTo de-normalize them we use the `inverse_transform` function on each `TimeSeries`:","metadata":{}},{"cell_type":"code","source":"LGBM_Forecasts_Families_back_Submission = {}\n\nfor family in tqdm(family_list):\n\n  LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T13:06:24.342746Z","iopub.execute_input":"2023-05-01T13:06:24.343479Z","iopub.status.idle":"2023-05-01T13:07:16.615357Z","shell.execute_reply.started":"2023-05-01T13:06:24.343429Z","shell.execute_reply":"2023-05-01T13:07:16.613916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, here is the code that allows to go from the predicted time series cluster to the prediction DataFrame:","metadata":{}},{"cell_type":"code","source":"for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n  for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n    if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n        LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n\nlistofseries = []\n\nfor store in tqdm(range(0,54)):\n  for family in family_list:\n      oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n      oneforecast.columns = ['fcast']\n      listofseries.append(oneforecast)\n\ndf_forecasts = pd.concat(listofseries) \ndf_forecasts.reset_index(drop=True, inplace=True)\n\n# No Negative Forecasts\ndf_forecasts[df_forecasts < 0] = 0\nforecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\nforecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\nforecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\nforecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\nforecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n\n# Submission\nsubmission_kaggle = forecasts_kaggle_sorted","metadata":{"execution":{"iopub.status.busy":"2023-05-01T13:07:16.617342Z","iopub.execute_input":"2023-05-01T13:07:16.618179Z","iopub.status.idle":"2023-05-01T13:07:19.54034Z","shell.execute_reply.started":"2023-05-01T13:07:16.618088Z","shell.execute_reply":"2023-05-01T13:07:19.539289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can display the predictions:","metadata":{}},{"cell_type":"code","source":"submission_kaggle.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T13:07:19.541789Z","iopub.execute_input":"2023-05-01T13:07:19.542227Z","iopub.status.idle":"2023-05-01T13:07:19.554341Z","shell.execute_reply.started":"2023-05-01T13:07:19.542181Z","shell.execute_reply":"2023-05-01T13:07:19.552793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**But it’s not over yet!** ☝🏻\n\nNow we need to train several models and apply the Ensemble method.","metadata":{}},{"cell_type":"markdown","source":"## **Multiple models**\n\nAs explained before, the important thing in this code is the hyperparameters. We will train 3 models by taking the following hyperparameters:","metadata":{}},{"cell_type":"code","source":"model_params = [\n    {\"lags\" : 7, \"lags_future_covariates\" : (16,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]},\n    {\"lags\" : 365, \"lags_future_covariates\" : (14,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]},\n    {\"lags\" : 730, \"lags_future_covariates\" : (14,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]}\n]","metadata":{"execution":{"iopub.status.busy":"2023-05-01T13:07:19.555982Z","iopub.execute_input":"2023-05-01T13:07:19.556454Z","iopub.status.idle":"2023-05-01T13:07:19.569316Z","shell.execute_reply.started":"2023-05-01T13:07:19.556419Z","shell.execute_reply":"2023-05-01T13:07:19.568366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For each of these parameters, we will train 33 models, run the predictions and fill the final DataFrame. The 3 DataFrames obtained will be stored in the `submission_kaggle_list` :","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error as msle, mean_squared_error as mse\nfrom lightgbm import early_stopping\n\nsubmission_kaggle_list = []\n\nfor params in model_params:\n\n  LGBM_Models_Submission = {}\n\n  display(\"Training...\")\n\n  for family in tqdm(family_list):\n\n    # Define Data for family\n    sales_family = family_TS_transformed_dict[family]\n    training_data = [ts for ts in sales_family] \n    TCN_covariates = future_covariates_dict[family]\n    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n\n    LGBM_Model_Submission = LightGBMModel(lags = params[\"lags\"],\n                                          lags_future_covariates = params[\"lags_future_covariates\"],\n                                          lags_past_covariates = params[\"lags_past_covariates\"],\n                                          output_chunk_length=1,\n                                          random_state=2022,\n                                          gpu_use_dp= \"false\")\n      \n    LGBM_Model_Submission.fit(series=train_sliced, \n                          future_covariates=TCN_covariates,\n                          past_covariates=transactions_transformed)\n\n    LGBM_Models_Submission[family] = LGBM_Model_Submission\n    \n  display(\"Predictions...\")\n\n\n  LGBM_Forecasts_Families_Submission = {}\n\n  for family in tqdm(family_list):\n\n    sales_family = family_TS_transformed_dict[family]\n    training_data = [ts for ts in sales_family]\n    LGBM_covariates = future_covariates_dict[family]\n    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n\n    forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n                                          series=train_sliced,\n                                          future_covariates=LGBM_covariates,\n                                          past_covariates=transactions_transformed)\n    \n    LGBM_Forecasts_Families_Submission[family] = forecast_LGBM\n\n  # Transform Back\n\n  LGBM_Forecasts_Families_back_Submission = {}\n\n  for family in tqdm(family_list):\n\n    LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)\n\n  # Prepare Submission in Correct Format\n\n  for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n    for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n      if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n          LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n          \n  listofseries = []\n\n  for store in tqdm(range(0,54)):\n    for family in family_list:\n        oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n        oneforecast.columns = ['fcast']\n        listofseries.append(oneforecast)\n\n  df_forecasts = pd.concat(listofseries) \n  df_forecasts.reset_index(drop=True, inplace=True)\n\n  # No Negative Forecasts\n  df_forecasts[df_forecasts < 0] = 0\n  forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n  forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n  forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n  forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n  forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n\n  # Submission\n  submission_kaggle_list.append(forecasts_kaggle_sorted)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T13:07:19.570498Z","iopub.execute_input":"2023-05-01T13:07:19.570873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We end up with four prediction DataFrames that we will sum and average (this is the so-called ensemble method):","metadata":{}},{"cell_type":"code","source":"df_sample_submission['sales'] = (submission_kaggle[['sales']]+submission_kaggle_list[0][['sales']]+submission_kaggle_list[1][['sales']]+submission_kaggle_list[2][['sales']])/4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the result:","metadata":{}},{"cell_type":"code","source":"df_sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can now save the predictions in a CSV file and submit it to Kaggle:","metadata":{}},{"cell_type":"code","source":"df_sample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feel free to tweak the hyperparameters to improve the model.\n\nIf you liked this tutorial, you can put a like on my Kaggle notebook, it will help me a lot !\n\nSee you soon on [Inside Machine Learning!](https://inside-machinelearning.com/en/home/) 😉","metadata":{}}]}