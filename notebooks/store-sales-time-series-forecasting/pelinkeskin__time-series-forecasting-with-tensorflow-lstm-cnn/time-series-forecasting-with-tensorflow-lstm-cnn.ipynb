{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary","metadata":{}},{"cell_type":"markdown","source":"This study focuses on honing time series forecasting skills using TensorFlow's DNN capabilities. The initial stages encompassed thorough Exploratory Data Analysis (EDA), inclusive of visualizations, and meticulous data wrangling. Subsequently, I established a basic XGBoost model without hyperparameter tuning, aiming primarily to extract feature importance and set a performance baseline for comparison. Following this, I adapted the data for univariate time series forecasting, utilizing Tensor to create DNN models incorporating LSTM CNN layers. Excitingly, the DNN model exhibited superior performance compared to the XGBoost model in both validation and on the public leaderboard.\n\nAs I'm relatively new to this domain, I'm open to feedback and suggestions for improvement. Constructive critique is greatly appreciated, as I'm earnestly learning the ropes. And please rate my notebook if you find it helpful. ","metadata":{}},{"cell_type":"markdown","source":"#### Library Importation and Compute Resource Assessment","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:29:33.200996Z","iopub.execute_input":"2023-12-06T12:29:33.201408Z","iopub.status.idle":"2023-12-06T12:29:50.3862Z","shell.execute_reply.started":"2023-12-06T12:29:33.201379Z","shell.execute_reply":"2023-12-06T12:29:50.38515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking GPU to understand how much memory available for training\n!nvidia-smi","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-06T12:21:35.537679Z","iopub.execute_input":"2023-12-06T12:21:35.538074Z","iopub.status.idle":"2023-12-06T12:21:36.5268Z","shell.execute_reply.started":"2023-12-06T12:21:35.538025Z","shell.execute_reply":"2023-12-06T12:21:36.52555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' ","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:21:39.384927Z","iopub.execute_input":"2023-12-06T12:21:39.385312Z","iopub.status.idle":"2023-12-06T12:21:39.392494Z","shell.execute_reply.started":"2023-12-06T12:21:39.385281Z","shell.execute_reply":"2023-12-06T12:21:39.390803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Enabling memory growth for GPU\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\nimport tensorflow as tf\ntf.config.list_physical_devices('GPU') ","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:21:40.656176Z","iopub.execute_input":"2023-12-06T12:21:40.657037Z","iopub.status.idle":"2023-12-06T12:21:54.85783Z","shell.execute_reply.started":"2023-12-06T12:21:40.657003Z","shell.execute_reply":"2023-12-06T12:21:54.856861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython\nimport IPython.display\nimport pickle\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nimport pandas as pd\nimport numpy as np\nimport keras_tuner as kt\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import LeakyReLU\nimport gc\nfrom tensorflow.keras.callbacks import Callback","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:21:56.943751Z","iopub.execute_input":"2023-12-06T12:21:56.944772Z","iopub.status.idle":"2023-12-06T12:21:57.902742Z","shell.execute_reply.started":"2023-12-06T12:21:56.944738Z","shell.execute_reply":"2023-12-06T12:21:57.901926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for reproducibility\nseed0=1337\nnp.random.seed(seed0) \ntf.keras.utils.set_random_seed(seed0)\ntf.config.experimental.enable_op_determinism()\ntf.random.set_seed(seed0)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:22:10.050614Z","iopub.execute_input":"2023-12-06T12:22:10.050994Z","iopub.status.idle":"2023-12-06T12:22:10.056428Z","shell.execute_reply.started":"2023-12-06T12:22:10.050964Z","shell.execute_reply":"2023-12-06T12:22:10.055394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to avoid seeing shortened text data in pandas cell\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:22:13.120222Z","iopub.execute_input":"2023-12-06T12:22:13.120585Z","iopub.status.idle":"2023-12-06T12:22:13.126244Z","shell.execute_reply.started":"2023-12-06T12:22:13.120559Z","shell.execute_reply":"2023-12-06T12:22:13.125176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:22:52.644648Z","iopub.execute_input":"2023-12-06T12:22:52.645549Z","iopub.status.idle":"2023-12-06T12:22:52.651694Z","shell.execute_reply.started":"2023-12-06T12:22:52.645514Z","shell.execute_reply":"2023-12-06T12:22:52.650722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis, Visualization, and Data Wrangling","metadata":{}},{"cell_type":"code","source":"#reading datasets\ntrain=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\nholiday_events=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\")\noil=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\")\nstores=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/stores.csv\")\ntransactions=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T12:23:28.968672Z","iopub.execute_input":"2023-12-06T12:23:28.969566Z","iopub.status.idle":"2023-12-06T12:23:32.67667Z","shell.execute_reply.started":"2023-12-06T12:23:28.96953Z","shell.execute_reply":"2023-12-06T12:23:32.675857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking out dataset content and shape\ndisplay(\"Train\",train.head(2),train.tail(2), train.shape, \"*\"*66)\ndisplay(\"Test\",test.head(2),test.tail(2), test.shape, \"*\"*66)\ndisplay(\"Holiday Events\",holiday_events.head(2),holiday_events.tail(2), holiday_events.shape, \"*\"*66)\ndisplay(\"Oil\",oil.head(2),oil.tail(2), oil.shape, \"*\"*66)\ndisplay(\"Stores\",stores.head(2),stores.tail(2), stores.shape, \"*\"*66)\ndisplay(\"Transactions\",transactions.head(2),transactions.tail(1), transactions.shape, \"*\"*66) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-06T12:23:54.247513Z","iopub.execute_input":"2023-12-06T12:23:54.247883Z","iopub.status.idle":"2023-12-06T12:23:54.352093Z","shell.execute_reply.started":"2023-12-06T12:23:54.247853Z","shell.execute_reply":"2023-12-06T12:23:54.351171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting date feature to datetime data type\ntrain[\"date\"] = pd.to_datetime(train.date)\ntest[\"date\"] = pd.to_datetime(test.date)\nholiday_events[\"date\"] = pd.to_datetime(holiday_events.date)\noil[\"date\"] = pd.to_datetime(oil.date)\ntransactions[\"date\"] = pd.to_datetime(transactions.date)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualising time series data ","metadata":{}},{"cell_type":"code","source":"#Generic helper function to plot time variant data\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    fig, ax = plt.subplots(figsize=(14,5))\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Sales\")\n    plt.grid(True)\n    plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_series(train[\"date\"], train[\"sales\"], format=\"-\", start=0, end=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Zooming into 2017\nplot_series(train[\"date\"], train[\"sales\"], format=\"-\",\n            start=-len(train[\"date\"][train[\"date\"] >=pd.to_datetime(\"2017-01-01\")]), end=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking trend seasonality and residuals\nimport statsmodels.api as sm\nt_series = train.groupby(['date'])['sales'].agg(['mean']).reset_index().rename(columns={'mean': 'msales'})\nt_series = t_series.set_index('date')\nsample= t_series['msales'].resample('MS').mean()\ndecomposition = sm.tsa.seasonal_decompose(sample, model='additive')\nfig = decomposition.plot()\nfig.set_size_inches((14, 6))\nfig.tight_layout()\nplt.show()\nplt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Joining holiday-oil-stores with train and test sets ","metadata":{}},{"cell_type":"markdown","source":"Note: I won't use transactions data because it isn't available for test set and I don't want to dedicate time to extract features utilising it but it can certainly provide useful information and worth engineering in a deeper study.","metadata":{}},{"cell_type":"code","source":"# helper function to do merge datasets \ndef joins(df,holiday_events,oil,stores):\n    print(df.shape)\n    holiday_events=holiday_events.drop_duplicates(subset=['date'], keep='last')\n    df_holiday=pd.merge(df,holiday_events,how=\"left\",on='date', validate=\"many_to_one\")\n    print(f\"Shape after merging with holiday {df_holiday.shape}\")\n    df_holiday_oil=pd.merge(df_holiday,oil,how=\"left\",on='date')\n    print(f\"Shape after merging with holiday + oil {df_holiday_oil.shape}\")\n    df_holiday_oil_stores=pd.merge(df_holiday_oil,stores,how=\"left\",on=\"store_nbr\",suffixes=(\"holiday\",\"stores\"))\n    print(f\"Shape after merging with holiday + oil + stores {df_holiday_oil_stores.shape}\")\n    return df_holiday_oil_stores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging datasets\ntrain_merged=joins(train,holiday_events,oil,stores)\ndisplay(train_merged.head(1),train_merged.tail(1))\ntest_merged=joins(test,holiday_events,oil,stores)\ndisplay(test_merged.head(1),test_merged.tail(1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extracting month and weekday","metadata":{}},{"cell_type":"code","source":"# extracting time features for train dataset\ntrain_merged['day_of_week'] = train_merged['date'].dt.day_of_week\ntrain_merged['day_of_week'] = train_merged['day_of_week']+1\ntrain_merged['month'] = train_merged['date'].dt.month\ntrain_merged['year'] = train_merged['date'].dt.year","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting time features for test dataset\ntest_merged['day_of_week'] = test_merged['date'].dt.day_of_week\ntest_merged['day_of_week'] = test_merged['day_of_week']+1\ntest_merged['month'] = test_merged['date'].dt.month\ntest_merged['year'] = test_merged['date'].dt.year","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing basic properties of features\ndisplay(train_merged.describe(include=\"all\").T,train_merged.isna().sum() )\ndisplay(test_merged.describe(include=\"all\").T,test_merged.isna().sum())","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Changing transfered holidays to normal days","metadata":{}},{"cell_type":"code","source":"#checking value counts of holiday types\ndisplay(train_merged[\"typeholiday\"].value_counts(dropna=False))\n# checking categories of transferred feature of holiday dataset\ndisplay(train_merged[\"transferred\"].unique())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to convert transfered holidays to normal days\ndef transfer_holiday_fix(df):\n    df[\"typeholiday\"]=np.where(df[\"transferred\"]==True,'NDay',df[\"typeholiday\"])\n    df[\"typeholiday\"]=np.where(df[\"typeholiday\"]=='Work Day','NDay',df[\"typeholiday\"])\n    df[\"typeholiday\"]=df[\"typeholiday\"].fillna(\"NDay\")\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merged=transfer_holiday_fix(train_merged)\ntest_merged=transfer_holiday_fix(test_merged)\n# checking new structure of holiday type column\ndisplay(train_merged[\"typeholiday\"].value_counts(dropna=False))\ndisplay(test_merged[\"typeholiday\"].value_counts(dropna=False))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing the key columns","metadata":{}},{"cell_type":"code","source":"#helper function to remove id columns used for joining datasets, \n# note I made decision not to use store_nbr, because I am not using transactions to extract features\n# but it can work as category feature for certain models, \n# my decision was because it is an idendifier and useful information it contains also exist in store type and cluster\ndef select_relevant(df):\n    print(f\"shape before removing columns {df.shape}\")\n    features=[\"date\",\"family\",\"sales\",\"onpromotion\",\"typeholiday\",\"dcoilwtico\",\"city\",\"state\",\n              \"typestores\",\"cluster\",\"day_of_week\",\"month\",\"year\"]\n    if \"sales\" in df.columns:\n        df= df[features]\n    else:\n        features.remove(\"sales\")\n        df= df[features]\n    print(f\"Shape after removing columns {df.shape}\")\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merged=select_relevant(train_merged)\ntest_merged=select_relevant(test_merged)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fixing the issue with missing values in oil prices by interpolating nans and 0s at oil price","metadata":{}},{"cell_type":"code","source":"# plotting current distribution of oil price vs time\nplot_series(train_merged[\"date\"], train_merged[\"dcoilwtico\"], format=\"-\", start=0, end=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to interpolate oil price\ndef interpolate_oil(df):\n    print(f\"Shape before interpolating {df.shape}\")\n    df[\"dcoilwtico\"]=np.where(df[\"dcoilwtico\"] ==0, np.nan, df[\"dcoilwtico\"])\n    df.dcoilwtico.interpolate(limit_direction='both',inplace=True)\n    print(f\"Shape after interpolating {df.shape}\")\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merged=interpolate_oil(train_merged)\ntest_merged=interpolate_oil(test_merged)\ndisplay(f\"Number of nan in train dataset oil price after interpolation {train_merged['dcoilwtico'].isna().sum()}\")\ndisplay(f\"Number of nan in test dataset oil price after interpolation {test_merged['dcoilwtico'].isna().sum()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_series(train_merged[\"date\"], train_merged[\"dcoilwtico\"], format=\"-\", start=0, end=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualising change in signal based on features","metadata":{}},{"cell_type":"code","source":"# globally setting sns to darkgrid theme\nsns.set_theme(style=\"darkgrid\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to plot variation of sales vs time for each class in a feature\ndef plot_by_category(df,category):\n    fig, ax = plt.subplots(figsize=(14,8))\n    \n    df_grouped=df.groupby(['date',category]).agg({'sales': 'mean'}).reset_index()\n    sns.lineplot(data =df_grouped,  x=\"date\", y=\"sales\", hue=category, ax=ax, legend=\"brief\")\n    \n    # Shrink current axis by 20%\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0, box.width * 0.85, box.height])\n\n    # Put a legend to the right of the current axis\n    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoricals=[ 'family', 'typeholiday', 'city', 'state', 'typestores', 'cluster']\nfor cat in categoricals:\n    print(cat)\n    plot_by_category(train_merged,cat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_features=[\"day_of_week\",\"month\",\"year\"]\nfor t_feat in time_features:\n    print(t_feat)\n    plot_by_category(train_merged,t_feat)\n    plot_series(train_merged[t_feat], train_merged[\"sales\"], format=\"-\", start=0, end=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"State and city have very similiar baseline I will use city as it is more specific","metadata":{}},{"cell_type":"code","source":"# helper function to plot numeric features vs time in comparison to sales vs time\ndef plot_by_numeric(df,num_cols):\n    fig, ax = plt.subplots(figsize=(14,8))\n    cols_to_plot=[\"date\",\"sales\",num_cols]\n    df=df[cols_to_plot]\n    df_grouped=df.groupby(['date']).agg({'sales': 'mean', num_cols:'mean'}).reset_index()\n    g1=sns.lineplot(data=df_grouped, x =\"date\", y='sales',legend='full', ax=ax)\n    g2=sns.lineplot(data=df_grouped, x =\"date\", y=num_cols,legend='full', ax=ax)\n    g1.set(yscale='log')\n    g2.set(yscale='log')\n    plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerics=['onpromotion','dcoilwtico']\nfor num_col in numerics:\n    print(num_col)\n    plot_by_numeric(train_merged,num_col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#helper function to plot numeric features vs time in comparison to sales vs time zooming in after 2017.01.01\ndef plot_by_numeric_last_year(df,num_cols):\n    fig, ax = plt.subplots(figsize=(14,8))\n    cols_to_plot=[\"date\",\"sales\",num_cols]\n    df=df[cols_to_plot]\n    df=df[train[\"date\"] >=pd.to_datetime(\"2017-01-01\")]\n    df_grouped=df.groupby(['date']).agg({'sales': 'mean', num_cols:'mean'}).reset_index()\n    g1=sns.lineplot(data=df_grouped, x =\"date\", y='sales', ax=ax)\n    g2=sns.lineplot(data=df_grouped, x =\"date\", y=num_cols, ax=ax)\n    g1.set(yscale='log')\n    g2.set(yscale='log')\n    plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerics=['onpromotion','dcoilwtico']\nfor num_col in numerics:\n    print(num_col)\n    plot_by_numeric_last_year(train_merged,num_col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to extract the features I potentially use for training a model\ndef extract_features(df):\n    print(f\"Shape before extracting {df.shape}\")\n    useful_features=['date', 'family','onpromotion','typeholiday', 'dcoilwtico','city', 'typestores', 'cluster','day_of_week', 'month','year']\n    if \"sales\"in df.columns:\n        df=df[useful_features+[\"sales\"]]\n    else:\n        df=df[useful_features]\n    print(f\"Shape after extracting{df.shape}\")\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merged=extract_features(train_merged)\ntest_merged=extract_features(test_merged)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting category columns to str type\ncategory_columns=['family', 'typeholiday', 'city', 'typestores', 'cluster']\nfor column in category_columns:\n    train_merged[column] = train_merged[column].astype('str')\n    test_merged[column] = test_merged[column].astype('str')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting extracted time features to float\ntime_columns=[\"month\",\"day_of_week\",\"year\"]\nfor column in time_columns:\n    train_merged[column] = train_merged[column].astype('float')\n    test_merged[column] = test_merged[column].astype('float')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking conditions of training and test datasets\ndisplay(train_merged.head(2))\ndisplay(train_merged.tail(2))\ndisplay(train_merged.describe(include=\"all\").T)","metadata":{"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving them in to a csv\ntrain_merged.to_csv('train_merged.csv', index=False)\ntest_merged.to_csv('test_merged.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Common libs & Methods for ML ","metadata":{}},{"cell_type":"code","source":"#main modules for preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer, KBinsDiscretizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.preprocessing import TargetEncoder\n\n#main modules for designing ML pipeliens\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer \nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n\n#main module for evaluation\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error,r2_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to calculate regression metrics\ndef calcMetrics(testActualVal, predictions):\n    #regression evaluation measures\n    data={\"RMSLE\":[mean_squared_log_error(testActualVal, predictions)**0.5],\n         \"MAE\":[mean_absolute_error(testActualVal, predictions)],\n         \"RMSE\":[mean_squared_error(testActualVal, predictions)**0.5],\n         \"R2\":[r2_score(testActualVal, predictions)]}\n    metric_df=pd.DataFrame(data)\n    return metric_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing for XGBoost","metadata":{}},{"cell_type":"code","source":"# helper function to split datset in to train and validation, validation set involves samples after 01.01.2017\ndef split(df):\n    useful_features=['date', 'family','typeholiday','onpromotion', 'dcoilwtico','city', 'typestores',\n                     'cluster','day_of_week', 'month','year']\n    val_start_index=df.shape[0]-len(df[\"date\"][df[\"date\"] >=pd.to_datetime(\"2017-01-01\")])\n    train = df[:val_start_index]\n    val=df[val_start_index:]\n    X_train = train[useful_features]\n    y_train = train[\"sales\"]\n    X_valid = val[useful_features]\n    y_valid = val[\"sales\"]\n    return X_train, y_train, X_valid, y_valid ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train, X_valid, y_valid =split(train_merged)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X_train.head(1),X_train.tail(1),X_train.shape )\ndisplay(X_valid.head(1),X_valid.tail(1), X_valid.shape)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A useful relevant referance: https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html\n#from sklearn.kernel_approximation import Nystroem\n#from sklearn.preprocessing import SplineTransformer\n\n#pipeline for target encoding category features\ncategory_feat=Pipeline(steps=[(\"target_encode\",TargetEncoder(target_type=\"continuous\"))])\n\n# helper functions to be able to get feature names out of functional transformer \ndef f_out_sin(self,input_features):\n    return input_features\ndef f_out_cos(self,input_features):\n    return input_features\n\n# functions to transform time features with sine cosine transformation \ndef sin_transformer(period):\n    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi),feature_names_out=f_out_sin)\n\ndef cos_transformer(period):\n    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi), feature_names_out=f_out_cos)\n\n#adding polynomial transformation on sine_cosine transformed time features to capture linear interactions between time features\ntime_feat=make_pipeline(\n                        ColumnTransformer([\n                            #(\"cyclic_day_of_week\", periodic_spline_transformer(7, n_splines=3), [\"day_of_week\"]),\n                            (\"day_of_week_sin\", sin_transformer(7), [\"day_of_week\"]),\n                            (\"day_of_week_cos\", cos_transformer(7), [\"day_of_week\"]),\n                            #(\"cyclic_month\", periodic_spline_transformer(12, n_splines=6), [\"month\"]),\n                            (\"month_sin\", sin_transformer(12), [\"month\"]),\n                            (\"month_cos\", cos_transformer(12), [\"month\"]),\n                            (\"year_sin\", sin_transformer(365), [\"year\"]),\n                            (\"year_cos\", cos_transformer(365), [\"year\"]),   \n                            ],remainder='drop'),\n    #Nystroem(kernel=\"poly\", degree=2,n_jobs=-1, n_components=85, random_state=0),\n    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False))\n# Note: One of my purpose for building this model is to get to get feature importance rating out of it\n# therefore I don't want to increase number of features and build a complex model\n# If this model be the focus, it might worth trying cyclic spline transformation on time features and kernel approximation to capture\n# non linear interactions between time features, this would increase number of features and extend training time and\n# increase systems memory requirement for training but as demonstrated in referance from documentation I linked above;\n# that approach can be more expressive, the code I commented out is for that approach ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# names of columns to train xgb\ncol_names_classic_ml=['family', 'typeholiday','onpromotion', 'dcoilwtico', 'city', 'typestores',\n           'cluster', 'day_of_week','month','year']\n\n# names of columns after pipeline transformations, \n# note ordering of this list isn't arbitrary.\n# I manually adjusted ordering after getting feature names out of pipeline and verfiying ordering \ncol_names_classic_ml_transformed=['family', 'typeholiday','city', 'typestores','cluster',\n       'day_of_week_sin','day_of_week_cos', 'month_sin', 'month_cos','year_sin', 'year_cos',\n       'day_of_week_sin day_of_week_cos','day_of_week_sin month_sin', 'day_of_week_sin month_cos',\n       'day_of_week_sin year_sin', 'day_of_week_sin year_cos','day_of_week_cos month_sin',\n       'day_of_week_cos month_cos','day_of_week_cos year_sin', 'day_of_week_cos year_cos','month_sin month_cos',\n       'month_sin year_sin','month_sin year_cos','month_cos year_sin','month_cos year_cos', 'year_sin year_cos',\n       'onpromotion', 'dcoilwtico']\n\n# building the pipeline to perform feature engineering\npreprocess_pipe = Pipeline(steps=[\n    ('encoder', ColumnTransformer(\n                    transformers=[\n                        (\"category_trans\",category_feat,category_columns),\n                        (\"time_trans\",time_feat,[\"day_of_week\",\"month\",\"year\"] ),\n                                ],\n                                remainder=\"passthrough\", verbose_feature_names_out=True\n                            )),\n    ('scaler', MinMaxScaler()),\n    (\"pandarizer2\", FunctionTransformer(lambda x: pd.DataFrame(x, columns =  col_names_classic_ml_transformed)))\n                            ],verbose = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_pipe.fit(X_train[col_names_classic_ml],y_train)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking out to see if feature names passed to pandarizer is in correct order, because getting correct output names is important to determine feature importances","metadata":{}},{"cell_type":"code","source":"display(preprocess_pipe['encoder'].feature_names_in_)\ndisplay(preprocess_pipe['encoder'].get_feature_names_out())","metadata":{"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fit transform data\npreprocess_pipe.fit(X_train[col_names_classic_ml],y_train)\nX_train=preprocess_pipe.transform(X_train[col_names_classic_ml])\nX_valid=preprocess_pipe.transform(X_valid[col_names_classic_ml])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X_train.head(2),X_train.tail(2),X_train.shape )\ndisplay(X_valid.head(2),X_valid.tail(2), X_valid.shape)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance, plot_tree","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# call back to avoid overfitting\nearly_stop = xgb.callback.EarlyStopping(rounds=10,\n                                        metric_name='rmse',\n                                        maximize=False,\n                                       save_best= True,\n                                        )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training xgboost model\nxgboost_v00=XGBRegressor(random_state=seed0,verbosity=0, n_jobs = -1, reg_lambda=0.005, \n                         learning_rate=0.01, device='gpu',\n                          n_estimators=5000, objective='reg:squarederror',\n                        callbacks=[early_stop])\nxgboost_v00.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting feature importances\n_ = plot_importance(xgboost_v00,max_num_features = 10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Measuring Performance of XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"# creating to dataframe, I will use to visualise predictions vs actual\nseries_compare=pd.concat([y_train,y_valid], axis=0)\ndf_compare=pd.DataFrame()\ndf_compare[\"actual\"]=series_compare\ndf_compare[\"date\"]=train_merged.date","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting validation set\ny_pred_xgb = xgboost_v00.predict(X_valid)\n# converting negative predictions to 0\ny_pred_xgb=np.where(y_pred_xgb<0,0,y_pred_xgb)\n# calculating metrics\ncalcMetrics(y_valid,y_pred_xgb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# additing predictions to comparison dataframe\ndf_compare[\"xgb_pred\"]=np.nan\ndf_compare.loc[df_compare.index>=df_compare.shape[0]-len(y_pred_xgb),\"xgb_pred\"]=y_pred_xgb\ndf_compare=df_compare.set_index(\"date\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualising predictions\n_ = df_compare[['actual','xgb_pred']].plot(figsize=(15, 5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#zooming to 2017 afterwards that was predicted\n_ = df_compare[['actual','xgb_pred']].tail(len(y_pred_xgb)).plot(figsize=(15, 5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting Test Set with XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"# preprocessing test set to rdy for inputting to model\nX_test=preprocess_pipe.transform(test_merged[col_names_classic_ml])\n#predicting testset\ny_pred_test_xgb = xgboost_v00.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# writing predictions to a csv for submission\noutput = pd.DataFrame({'id': test.id, 'sales': y_pred_test_xgb})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model into a pickle file \n#filename= 'StoreSales_XGB.pkl'\n#with open(filename, 'wb') as handle:\n#    pickle.dump(xgboost_v00, handle, pickle.HIGHEST_PROTOCOL)\n# removing model to free memory, \ndel xgboost_v00","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public score: 0.94056","metadata":{}},{"cell_type":"markdown","source":"# DNN Model : Univariate LSTM - CNN hybrit model","metadata":{}},{"cell_type":"markdown","source":"* To incorporate impact of category features into lstm-cnn hybrid dnn, I will create a feature by merging 3 most important category feature together\n* To train a dnn for each class in this new featrue, first I will discretise target encode new_feature, then discretise it into bins. This will group classes with similar sales figures together and reduce the number of models needed to be trained","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing for DNN","metadata":{}},{"cell_type":"code","source":"# reading processed datasets\n#train_merged=pd.read_csv(\"train_merged.csv\")\n#test_merged=pd.read_csv(\"test_merged.csv\")\n\n# converting extracted time features to float\n#time_columns=[\"month\",\"day_of_week\",\"year\"]\n#for column in time_columns:\n#    train_merged[column] = train_merged[column].astype('float')\n#    test_merged[column] = test_merged[column].astype('float')\n    \n# converting category columns to str type\n#category_columns=['family', 'typeholiday', 'city', 'typestores', 'cluster']\n#for column in category_columns:\n#    train_merged[column] = train_merged[column].astype('str')\n#    test_merged[column] = test_merged[column].astype('str')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making a copy of dataset I prepered before for training\ndnn_df=train_merged.copy()\ndf_dnn_test_check=test_merged.copy()\n# removing the 2nd copy to free memory\ndel train_merged\ndel test_merged\ndnn_df[\"cat_gen\"]=dnn_df[\"family\"]+dnn_df[\"city\"]+dnn_df[\"cluster\"]\ndisplay(f'store_nbr nunique: {train[\"store_nbr\"].nunique()}, family nunique: {dnn_df[\"family\"].nunique()}, city nunique: {dnn_df[\"city\"].nunique()}, cluster nunique: {dnn_df[\"cluster\"].nunique()}, cat_gen nunique: {dnn_df[\"cat_gen\"].nunique()}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#col_names_dnn=['cat_gen', 'family','typeholiday', 'city','typestores', 'cluster','dcoilwtico', 'date','day_of_week', 'month', 'year']\ncol_names_dnn=['cat_gen', 'date']\ndnn_df_x=dnn_df[col_names_dnn]\ndnn_df_y=dnn_df[\"sales\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub pipeline to apply said transformation on derived feature\ncategory_feat_dnn=Pipeline(steps=[(\"target_encode\",TargetEncoder(target_type=\"continuous\")),\n                                 (\"combiner\",KBinsDiscretizer(n_bins=130, strategy='kmeans',\n                                  subsample=None, random_state=seed0,encode='ordinal'))\n                                 ])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# engineering the said feature in pipeline\npreprocess_pipe_dnn = Pipeline(steps=[\n    ('encoder', ColumnTransformer(\n                    transformers=[\n                        (\"category_trans\",category_feat_dnn,[\"cat_gen\"])\n                                ],\n                                remainder=\"passthrough\", verbose_feature_names_out=True\n                            )),\n    (\"pandarizer2\", FunctionTransformer(lambda x: pd.DataFrame(x, columns =  col_names_dnn)))\n                            ],verbose = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting date to string to avoid it getting transformed to float\ndnn_df_x = dnn_df_x.astype({'date':'string'})\npreprocess_pipe_dnn.fit(dnn_df_x,dnn_df_y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking if the ordering of feature names are correct\ndisplay(preprocess_pipe_dnn['encoder'].feature_names_in_)\ndisplay(preprocess_pipe_dnn['encoder'].get_feature_names_out())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_df_x=preprocess_pipe_dnn.transform(dnn_df_x)\ndisplay(dnn_df_x.head(2),dnn_df_x.tail(2),dnn_df_x.shape )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_training_features= ['cat_gen', 'date']\ndnn_df_x=dnn_df_x[dnn_training_features]\n#concatenating X-y datasets\ndnn_df_processed=pd.concat([dnn_df_x,dnn_df_y], axis=1)\n# converting date to datetime dtype\ndnn_df_processed[\"date\"] = pd.to_datetime(dnn_df_processed.date)\ndnn_df_processed[\"sales\"] = dnn_df_processed[\"sales\"].astype(np.float32)\ndnn_df_processed.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing to free memory\ndel dnn_df_x\ndel dnn_df_y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking number of samples in each class of derived feature\ndisplay(dnn_df_processed[\"cat_gen\"].value_counts() , dnn_df_processed[\"cat_gen\"].nunique())","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying the same transformations to test dataset\ndf_dnn_test_check[\"cat_gen\"]=df_dnn_test_check[\"family\"]+df_dnn_test_check[\"city\"]+df_dnn_test_check[\"cluster\"]\ndf_dnn_test_check = df_dnn_test_check.astype({'date':'string'})\ndf_dnn_test_check=preprocess_pipe_dnn.transform(df_dnn_test_check)\ndf_dnn_test_check=df_dnn_test_check[dnn_training_features]\ndf_dnn_test_check[\"date\"] = pd.to_datetime(df_dnn_test_check.date)\n# checking if train have any class that test don't have, if so I will remove it\ndisplay(f\"test: {df_dnn_test_check['cat_gen'].nunique()} number of class in the derived category\")\ndisplay(f\"train: {dnn_df_processed['cat_gen'].nunique()} number of class in the derived category\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no category bin that is in train but not in test","metadata":{}},{"cell_type":"code","source":"# I will use the 95 day from end of train dataset time series for validation \nTest_sample_size=95\nSPLIT_DATE=pd.to_datetime(\"2017-08-15\")-pd.to_timedelta((Test_sample_size), unit='d')\nTrain_sample_size =365*4+1\nTraining_interval=pd.to_timedelta((Train_sample_size), unit='d')\ntrain_data_begin_date=SPLIT_DATE-Training_interval\ndisplay(f\"train_data_begin_date: {train_data_begin_date}\")\ndnn_df_processed=dnn_df_processed.loc[dnn_df_processed[\"date\"]>=train_data_begin_date]\ndisplay(dnn_df_processed.head(2))\n#creating a copy of processed dataset for ease of access to unscaled features for later comparison, evaluation\ndnn_df_processed_unscaled=dnn_df_processed.copy()\ngrouped_sample_lenght=len(dnn_df_processed_unscaled[dnn_df_processed_unscaled[\"cat_gen\"]==0].groupby(\"date\")[['cat_gen','sales']].agg({'cat_gen':'mean', 'sales':'median'}))\ndisplay(f\"lenght of dataframes grouped by cat_gen: {grouped_sample_lenght}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining parameters for training\nSPLIT_TIME =grouped_sample_lenght-Test_sample_size\nWINDOW_SIZE = 16\nBATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 580","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to devide train dataset into sub-datasets for each class in derived category\n# and grouping by date by taking the median of sales\ndef create_datasets(df):\n    dnn_data_sets={}\n    fitted_scalers={}\n    for val in df[\"cat_gen\"].unique():\n        df_segment=df[df[\"cat_gen\"]==val]\n        df_segment_grouped=df_segment.groupby(\"date\")[['cat_gen','sales']].agg({'cat_gen':'mean', 'sales':'median'})\n        df_segment_grouped=df_segment_grouped.drop(columns=['cat_gen'])\n        scaler=MinMaxScaler()\n        scaler.fit(df_segment_grouped[[\"sales\"]])\n        fitted_scalers[val]=scaler\n        df_segment_grouped[\"sales\"]=scaler.transform(df_segment_grouped[[\"sales\"]])\n        df_segment_grouped[\"sales\"] = df_segment_grouped[\"sales\"].astype(np.float32)\n        dnn_data_sets[val]=df_segment_grouped\n    return dnn_data_sets , fitted_scalers\ndnn_data_sets, fitted_scalers=create_datasets(dnn_df_processed) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing a sample of sub-datasets shape info\nSample_of_df_dict=np.random.choice([x for x in dnn_data_sets.keys()], 10 ,replace=False)\nfor key in Sample_of_df_dict:\n    display(f\"{key} : {dnn_data_sets[key].shape}\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verifying number of samples to be used for training and validation\nlen_test=len(dnn_data_sets[0][dnn_data_sets[0].index <=SPLIT_DATE])\ndisplay(f\"train size: {len_test}, test size: {dnn_data_sets[0].shape[0]-len_test}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to parse dataframes to extract time and series\ndef parse_df(df):\n    times =np.array([x for x in range(df.shape[0])])\n    series=df[\"sales\"].to_numpy()\n    return times, series","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to devide time and series to train test splits according to time-step\ndef train_val_split(time, series, time_step):\n    time_train = time[:time_step]\n    series_train = series[:time_step]\n    time_valid = time[time_step:]\n    series_valid = series[time_step:]\n    return time_train, series_train, time_valid, series_valid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to create windowed dataset tensors from series by shifting according to window size \n# and create batches from it for training\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[-1]))\n    ds = ds.batch(batch_size).prefetch(1)\n    return ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n# helper function to create and compile dnn model\ndef create_model(WINDOW_SIZE):\n    drop_out=0\n    activation=LeakyReLU(alpha = 0.01)\n    regularizer=regularizers.l2(1e-4)\n    model = tf.keras.models.Sequential([\n              #tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n              #            input_shape=[WINDOW_SIZE]),\n              tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n                          strides=1,\n                          activation=activation,\n                          padding='causal',\n                          input_shape=[WINDOW_SIZE, 1]),\n              tf.keras.layers.LSTM(64, return_sequences=True, dropout=drop_out),\n              tf.keras.layers.LSTM(64,dropout=drop_out),\n              tf.keras.layers.Dense(30, activation=activation, kernel_regularizer = regularizer),\n              tf.keras.layers.Dense(10, activation=activation, kernel_regularizer = regularizer),\n              tf.keras.layers.Dense(1)\n        ])\n    learning_rate = 1e-3\n    model.compile(loss=tf.keras.losses.Huber(),\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  metrics=[\"mae\"])\n    return model\n# note: conv1d layer return lstm input in appropriate shape for lstm\n# without conv1d layer a lambda layer will be needed to reshape input for lstm, code commented is for this lambda layer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callback function to avoid any memory leak issue and reset states in tf.keras\nclass ClearMemory(Callback):\n    def on_train_end(self, epoch, logs=None):\n        gc.collect()\n        tf.keras.backend.clear_session()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# call back function to avoid overfitting by early stopping and reduce learning rate on plateau to help convergence\ndef call():\n    #early stopping\n    es = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', # metrics to monitor\n    patience=20, # how many epochs before stop\n    verbose=1,\n    mode='min', \n    restore_best_weights=True)\n    \n    # reducing learning rate on plateau\n    rp = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5, # halving learning rate on plateau\n    patience=3,\n    verbose=1,\n    mode='min',\n    min_lr=1e-8)\n    return es,rp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generator function to iterate memory efficently\ndef ited_dict(dict):\n    for key,value in dict.items():\n        yield key, value","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to train model for each class in derived category\n# calls the functions above in correct sequence and saves the models and training history to dictionaries \ndef train_all(dnn_data_sets,WINDOW_SIZE,BATCH_SIZE,SHUFFLE_BUFFER_SIZE, SPLIT_TIME):\n    compiled_models_dict={}\n    history_dict={}\n    total_num_models=len(dnn_data_sets)\n    num_model_to_train=total_num_models\n    for key,df in ited_dict(dnn_data_sets):\n        print(f\"Number of models remained to be trained {num_model_to_train} out of {total_num_models} remained\")\n        print(f\"Training: {key}\")\n        times, sales = parse_df(df)\n        TIME = np.array(times)\n        SERIES = np.array(sales)\n        \n        time_train, series_train, time_valid, series_valid =train_val_split(TIME, SERIES, time_step=SPLIT_TIME)\n        \n        train_set = windowed_dataset(series_train, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE, \n                                     shuffle_buffer=SHUFFLE_BUFFER_SIZE)\n        \n        valid_set = windowed_dataset(series_valid, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE, \n                             shuffle_buffer=SHUFFLE_BUFFER_SIZE)\n        \n        model = create_model(WINDOW_SIZE)\n        callbacks = call()\n        history = model.fit(train_set, epochs=100, verbose=1,validation_data=valid_set, callbacks=[callbacks,ClearMemory()])\n        compiled_models_dict[key]=model\n        del model\n        history_dict[key]=history\n        del history\n        num_model_to_train-=1\n        IPython.display.clear_output()\n        gc.collect()\n        tf.keras.backend.clear_session()\n    print(f\"Trainings finnished: {len(compiled_models_dict)} models trained.\")        \n    return compiled_models_dict, history_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compiled_models_dict, history_dict=train_all(dnn_data_sets,WINDOW_SIZE,BATCH_SIZE,SHUFFLE_BUFFER_SIZE, SPLIT_TIME)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to plot loss graphs \ndef plot_loss_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing 10 randomly selected loss map\nSample_of_loss_graphs=np.random.choice([x for x in history_dict.keys()], 10 ,replace=False,)\nfor key in Sample_of_loss_graphs:\n    print(key)\n    plot_loss_graphs(history_dict[key], \"loss\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing to free memory\ndel history_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function for to create windowed dataset without shuffling for fast forecasting \ndef model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(8).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  helper function that calls required helper methods in correct sequence\n# and return a forecast for series after split_time (validation data)\ndef predict_results(models_dict,dnn_data_sets, WINDOW_SIZE,SPLIT_TIME,fitted_scalers):\n    predictions={}\n    total_num_models=len(dnn_data_sets)\n    num_model_to_train=total_num_models\n    for key,model in ited_dict(models_dict):\n        print(f\"Number of datasets remained to be predicted {num_model_to_train} out of {total_num_models} remained\")\n        print(f\"Predicting: {key}\")\n        times, sales = parse_df(dnn_data_sets[key])\n        TIME = np.array(times)\n        SERIES = np.array(sales)\n        \n        dnn_forecast = model_forecast(model,SERIES, WINDOW_SIZE).squeeze()\n        dnn_forecast = dnn_forecast[SPLIT_TIME - WINDOW_SIZE:-1]\n        \n        predictions_df=pd.DataFrame(index=dnn_data_sets[key][SPLIT_TIME :].index)\n        predictions_df[\"predictions\"]=dnn_forecast\n        del dnn_forecast\n        predictions_df[\"predictions\"]=fitted_scalers[key].inverse_transform(predictions_df[[\"predictions\"]])\n        predictions_df[\"predictions\"]=np.where(predictions_df[\"predictions\"]<0,0,predictions_df[\"predictions\"])\n        predictions[key]=predictions_df\n        num_model_to_train-=1\n        IPython.display.clear_output()\n        gc.collect()\n    print(f\"Predictions finnished: {len(predictions)} dataset predicted.\") \n    return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_dict=predict_results(compiled_models_dict,dnn_data_sets, WINDOW_SIZE,SPLIT_TIME, fitted_scalers)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#helper function to plot time series actual vs time series predicted \ndef plot_series_multi(time1, series1,series2,label1, label2, format1=\"--\",format2=\"-\", start=0, end=None ):\n    fig = plt.figure(figsize=(14,5))\n    ax = fig.add_axes([1, 1, 1, 1])\n    ax.plot(time1[start:end], series1[start:end])\n    ax.plot(time1[start:end], series2[start:end])\n    ax.legend(labels = (label1, label2))\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Sales\")\n    plt.grid(True)\n    plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing out metrics and actual vs predicted time series graphs for a subsample of predictions for sub-grouped validation datasets\nSample_of_actual_predict_graphs=np.random.choice([x for x in predictions_dict.keys()], 10 ,replace=False,)\nfor key in Sample_of_actual_predict_graphs:\n    #printing out metrics\n    print(key)\n    display(calcMetrics(predictions_dict[key].to_numpy(),\n                        fitted_scalers[key].inverse_transform(dnn_data_sets[key][\"sales\"][SPLIT_TIME :].to_numpy().reshape(-1, 1))))\n    \n    time1=dnn_data_sets[key].index.values[SPLIT_TIME :]\n    series1=predictions_dict[key].to_numpy()\n    series2=fitted_scalers[key].inverse_transform(dnn_data_sets[key][\"sales\"][SPLIT_TIME :].to_numpy().reshape(-1, 1))\n    # plotting the graph\n    plot_series_multi(time1, series1,series2,\"predictions\",\"actual\", format1=\"--\",format2=\"-\", start=0, end=None, )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function for mapping predictions from sub-grouped datasets to ungrouped full dataset \ndef aggragate_predictions(df_valid,predictions_dict):\n    df_preds = pd.DataFrame(columns=['predictions'], index=df_valid.index.copy())\n    for key,df in ited_dict(predictions_dict):\n        for date in df.index:\n            df_preds[\"predictions\"]=np.where((df_valid[\"cat_gen\"]==key) & (df_valid[\"date\"]==date),\n                                                   df.iloc[df.index.values==date][\"predictions\"].values[0] ,df_preds[\"predictions\"])\n    return df_preds[\"predictions\"].to_numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function for creating ungrouped full validation dataset and parsing it into X and y\ndef val_dnn_processed(df):\n    val_start_index=df.shape[0]-len(df[\"date\"][df[\"date\"] >SPLIT_DATE])\n    val=df[val_start_index:]\n    y_valid = val.pop(\"sales\")\n    X_valid = val\n    return X_valid, y_valid \nX_valid_dnn, y_valid_dnn = val_dnn_processed(dnn_df_processed_unscaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting full-validation datase and calculating metrics\ny_pred_dnn=aggragate_predictions(X_valid_dnn,predictions_dict)\ncalcMetrics(y_pred_dnn,y_valid_dnn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting actual vs predictions for validation dataset\nplot_series_multi(dnn_df_processed_unscaled.loc[dnn_df_processed_unscaled[\"date\"]>SPLIT_DATE][\"date\"].values,\n                   dnn_df_processed_unscaled.loc[dnn_df_processed_unscaled[\"date\"]>SPLIT_DATE][\"sales\"].values, y_pred_dnn,\n                  \"actual\",\"predictions\",format1='r^',format2=\"b^\", start=0, end=None )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting Test Set with Univariate LSTM - CNN hybrit model","metadata":{}},{"cell_type":"code","source":"# helper function to devide test into sub-datasets for each class in derived category\ndef create_grouped_test_datasets(df):\n    dnn_data_sets={}\n    for val in df[\"cat_gen\"].unique():\n        df_segment=df[df[\"cat_gen\"]==val]\n        df_segment_grouped=df_segment.groupby(\"date\")[['cat_gen']].agg({'cat_gen':'mean'})\n        df_segment_grouped=df_segment_grouped.drop(columns=['cat_gen'])\n        dnn_data_sets[val]=df_segment_grouped\n    return dnn_data_sets \ndnn_data_set_test =create_grouped_test_datasets(df_dnn_test_check) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function that returns predictions for testset for each class in derived category\ndef predict_results_test(models_dict,dnn_data_sets,dnn_data_set_test, WINDOW_SIZE,fitted_scalers):\n    predictions={}\n    for key, model in models_dict.items():\n        times, sales = parse_df(dnn_data_sets[key])\n        SERIES = np.array(sales)\n        dnn_forecast = model_forecast(model,SERIES, WINDOW_SIZE).squeeze()\n        dnn_forecast = dnn_forecast[-WINDOW_SIZE:]\n        predictions_df=pd.DataFrame(index=dnn_data_set_test[key].index)\n        predictions_df[\"predictions\"]=dnn_forecast\n        predictions_df[\"predictions\"]=fitted_scalers[key].inverse_transform(predictions_df[[\"predictions\"]])\n        predictions_df[\"predictions\"]=np.where(predictions_df[\"predictions\"]<0,0,predictions_df[\"predictions\"])\n        predictions[key]=predictions_df \n    return predictions\npredictions_dict_test=predict_results_test(compiled_models_dict,dnn_data_sets,dnn_data_set_test, WINDOW_SIZE,fitted_scalers)","metadata":{"scrolled":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calling aggragate predictions function for mapping predictions from sub-grouped validation datasets to ungrouped full validation dataset\ny_pred_dnn_test=aggragate_predictions(df_dnn_test_check,predictions_dict_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# writing predictions to a csv for submission\noutput = pd.DataFrame({'id': test.id, 'sales': y_pred_dnn_test})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Public score: 0.90424","metadata":{}},{"cell_type":"code","source":"# Saving models into pickle files\n#for key,model in ited_dict(compiled_models_dict): \n#    docs_add=os.getcwd()+'/dnn_models/'\n#    filename= docs_add+key+'.pkl'\n#    with open(filename, 'wb') as handle:\n#        pickle.dump(model, handle, pickle.HIGHEST_PROTOCOL)\n# removing models to free memory \n#del compiled_models_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Concluding Discussion and Future Work:","metadata":{}},{"cell_type":"markdown","source":"Several potential enhancements can elevate the quality and scope of this study. One notable concern pertains to the extensive memory consumption of the DNN models' dictionary in my limited local environment. To manage this, I had to reduce the derived category feature's classes to 1/10 of the original count, consequently training fewer DNN models that could fit within my system's memory constraints. Expanding the memory capacity would facilitate training a greater number of models encompassing more classes within the derived category. This approach could enhance model specificity and potentially bolster accuracy.\n\nFurthermore, an avenue for improvement lies in the utilization of multivariate models. While the current model operates as a univariate setup, incorporating the time-varying nature of oil prices as an input feature could notably enhance accuracy. This integration could offer valuable insights by capturing the contextual dynamics of oil price fluctuations within the forecasting framework.\n\nMoreover, the incorporation of engineered transaction features could be a valuable addition to a multivariate model. This inclusion could potentially introduce significant time-varying aspects that may augment the model's predictive capabilities, especially when combined with other relevant variables.\n\nIn summary, future endeavors could focus on optimizing memory utilization for more expansive model training, transitioning to multivariate models to incorporate additional impactful features such as oil prices, and exploring the incorporation of transaction-based engineered features to enhance predictive accuracy and robustness. These improvements hold the potential to refine the forecasting models and yield more nuanced and accurate predictions.","metadata":{}}]}