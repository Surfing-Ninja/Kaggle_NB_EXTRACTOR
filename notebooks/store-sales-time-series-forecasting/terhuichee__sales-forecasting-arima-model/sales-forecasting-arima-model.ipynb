{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sales forecasting\n\nThe project is mainly to predict the future sales by using the time-series forecasting technique. ","metadata":{}},{"cell_type":"markdown","source":"# Import Dependencies ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport scipy.stats as stats\nfrom scipy.stats import pearsonr\nimport itertools\nfrom statsmodels.tsa.stattools import kpss\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.stattools import adfuller","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\ntest_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\noil_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\ntransaction_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\nstores_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\nholiday_event_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding the Data","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transaction_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stores_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holiday_event_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The sales column is the target variable. \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Merging datasets holiday_event_df, stores_df, oil_df & train_df","metadata":{}},{"cell_type":"code","source":"train_df = train_df.merge(stores_df, on ='store_nbr')\ntrain_df = train_df.merge(oil_df, on ='date', how='left')\nholiday_event_df = holiday_event_df.rename(columns={'type': 'holiday_type'})\ntrain_df = train_df.merge(holiday_event_df, on='date', how='left')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values Detection","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#percentage of missing values in train_df \n\nmissing_percentages = train_df.isnull().sum()/ len(train_df) * 100 \n\nprint(missing_percentages)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove columns that are having more than 30% missing values\n\ncolumns_to_delete = missing_percentages[missing_percentages > 30].index\n\ntrain_df = train_df.drop(columns=columns_to_delete)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Duplicates\n","metadata":{}},{"cell_type":"code","source":"train_df.duplicated().any()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dupes=train_df.duplicated()\n\n#dupes\nsum(dupes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping duplicate values\n\ntrain_df = train_df.drop_duplicates()\ntrain_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.duplicated().any()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.duplicated().any()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check if there still any missing values present in the train_df\n","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate count, mean, std, min, 25%, 50%, 75%, max values for each column. Prepare an analysis of the difference between mean and median for each column and possible reasons for the same.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA ","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Questions\n\n1. Does the type of stores affect the store sales? \n\n2. Which family is having the highest sales? \n\n3. Does promotion able to improve the sales? \n\n4. Which city is having the most number of customers? \n\n5. Which state is having the most number of customers? \n\n6. Which of the stores has the highest sales. \n\n7. Which month is having the most sales, and least sales. \n\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Does the type of stores affect the store sales?","metadata":{}},{"cell_type":"markdown","source":"To answer the first question 'Does the type of stores affect the store sales?' , i will use ANOVA test. \nANOVA (Analysis of Variance) is a statistical test used to determine whether there are significant differences between the means of two or more groups. It compares the variation between the groups (due to the different categories or factors) to the variation within the groups.\n\n\nH0 (>0.05)= The type of stores does not affect store sales. There is no significant difference in store sales between different types of stores.\n\nH1 (<0.05)= The type of stores does affect store sales. There is a significant difference in store sales between different types of stores.\n\n","metadata":{}},{"cell_type":"code","source":"grouped_data = train_df.groupby('type')['sales']\n\n# Perform the ANOVA test\nf_statistic, p_value = stats.f_oneway(*[grouped_data.get_group(type) for type in grouped_data.groups])\n\n# Print the results\nprint(\"F-Statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the F-statistics and p-value above, we reject null hypothesis and accept alternative hypothesis. Hence, the type of stores does affect the store sales. There is a significant difference in store sales between different type. ","metadata":{}},{"cell_type":"code","source":"# Sales Vs Type\n\nplt.scatter(train_df['type'], train_df['sales'])\n\nplt.ylabel('sales')\nplt.xlabel('type')\n\nplt.show()\n     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Which family is having the highest sales?","metadata":{}},{"cell_type":"code","source":"#Pie chart\n\n# Group the data by family and calculate the total sales for each family\nfamily_sales = train_df.groupby('family')['sales'].sum()\n\n# Sort the families based on sales in descending order\nfamily_sales_sorted = family_sales.sort_values(ascending=False)\n\n# Get the top 5 families with the highest sales\ntop_families = family_sales_sorted.head(5)\n\n# Create the pie chart\nplt.pie(top_families, labels=top_families.index, autopct='%1.1f%%', startangle=90)\n\nplt.title('Distribution of Sales by Family')\n\nplt.axis('equal')  \nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the pie chart above, the GROCERY I is having the highest sales, and Baverages comes second highest. ","metadata":{}},{"cell_type":"markdown","source":"### 3. Does promotion able to improve the sales?","metadata":{}},{"cell_type":"markdown","source":"To answer the 3rd question \"Does promotion able to improve the sales?\" I will use Pearson correlation test to determine the relationship between the two variables, as both of the variables are numericals. The Pearson correlation coefficient measures the linear relationship between two continuous variables and ranges from -1 to +1.\n\nH0 (>0.05)= The promotion does not affect store sales. \n\nH1 (<0.05)= The promotion does affect store sales. \n","metadata":{}},{"cell_type":"code","source":"correlation, p_value = pearsonr(train_df['onpromotion'], train_df['sales'])\n\nprint(\"Pearson correlation coefficient:\", correlation)\nprint(\"p-value:\", p_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the Pearson correlation coefficient of 0.4279 and the p-value of 0.0, we can reject the null hypothesis (H0) and conclude that there is a significant relationship between promotion and store sales. Therefore, the promotion does affect store sales.","metadata":{}},{"cell_type":"code","source":"# Scatter plot\nplt.scatter(train_df['onpromotion'], train_df['sales'])\n\nplt.xlabel('Promotion')\nplt.ylabel('Sales')\nplt.title('Promotion vs Sales')\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Which city is having the most most number of customers?","metadata":{}},{"cell_type":"code","source":"#Count Plot \n\n# Create a count plot\nplt.figure(figsize=(10, 6))  # Set the figure size\nsns.countplot(data=train_df, x='city')\n\nplt.xlabel('City')\nplt.ylabel('Count')\nplt.title('Sales Distribution by City')\n\nplt.xticks(rotation=45)\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the count plot above, the Quito city has the most sales. ","metadata":{}},{"cell_type":"markdown","source":"### 5. Which state is having the most number of customers?\n","metadata":{}},{"cell_type":"code","source":"#Count Plot \n\n# Create a count plot\nplt.figure(figsize=(10, 6))  # Set the figure size\nsns.countplot(data=train_df, x='state')\n\nplt.xlabel('state')\nplt.ylabel('Count')\nplt.title('Sales Distribution by City')\n\nplt.xticks(rotation=45)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the count plot above, Pichincha state has the most sales as compared to other states. ","metadata":{}},{"cell_type":"markdown","source":"### 6. Which of the stores has the highest sales. ","metadata":{}},{"cell_type":"code","source":"# Calculate the total sales for each store\nstore_sales = train_df.groupby('store_nbr')['sales'].sum().reset_index()\n\n# Sort the stores based on sales in descending order\nstore_sales = store_sales.sort_values('sales', ascending=False)\n\n# Create a bar plot\nplt.figure(figsize=(12, 6))\nsns.barplot(data=store_sales, x='store_nbr', y='sales')\n\nplt.xlabel('Store Number')\nplt.ylabel('Total Sales')\nplt.title('Total Sales by Store')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Which month is having the most sales, and least sales. ","metadata":{}},{"cell_type":"code","source":"#First convert the 'date' from object to date time \n\ntrain_df['date']= pd.to_datetime(train_df['date'])\n\n\n# create new columns 'month' 'year'\ntrain_df['month'] = train_df['date'].dt.month\ntrain_df['year'] = train_df['date'].dt.year","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group the data by month, year, and calculate the total sales\nmonthly_sales = train_df.groupby(['month', 'year'])['sales'].sum().reset_index()\n\n# Create the line chart\nplt.figure(figsize=(10, 6))  # Set the figure size\n\n# Get unique years and cycle through colors\nyears = monthly_sales['year'].unique()\ncolors = itertools.cycle(['red', 'green', 'blue', 'orange', 'purple'])\n\nfor year in years:\n    year_data = monthly_sales[monthly_sales['year'] == year]\n    plt.plot(year_data['month'], year_data['sales'], marker='o', color=next(colors), label=str(year))\n\nplt.xlabel('Month')\nplt.ylabel('Sales')\nplt.title('Monthly Sales Trend')\n\n# Customize x-axis ticks to show month names\nmonth_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nplt.xticks(range(1, 13), month_names)\n\n\nplt.legend()\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, the orange line which is 2016 has a stable high sales since January to Dec. Between the months in 2016, December had the most sales. In other hand, in comparing to other years, 2013 had an overall lowest sales achieved, especially during February. ","metadata":{}},{"cell_type":"code","source":"train_df = train_df.groupby('date')['sales','onpromotion'].sum().reset_index()\nprint(train_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Autocorrelation \n\nAutocorrelation measures the correlation between a time series and its lagged values. Autocorrelation plots (ACF) and partial autocorrelation plots (PACF) help identify significant lag values and potential autoregressive or moving average components.\n\n- If the autocorrelation value is close to 1 or -1, it indicates a strong positive or negative autocorrelation, respectively.\n\n- If the autocorrelation value is close to 0, it indicates a weak or no autocorrelation.","metadata":{}},{"cell_type":"code","source":"sales_series = train_df['sales']\nautocorr_values = sales_series.autocorr()\nprint(\"Autocorrelation:\", autocorr_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the result above, since the autocorrelation value is close to 1 (0.766), it suggests that there is a positive autocorrelation. A positive autocorrelation indicates that there is a relationship between the current sales values and the previous sales values.","metadata":{}},{"cell_type":"code","source":"plot_acf(train_df['sales'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the PACF\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_pacf(train_df['sales'], ax=ax)\nplt.xlabel('Lag')\nplt.ylabel('Partial Autocorrelation')\nplt.title('Partial Autocorrelation Function (PACF)')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Differencing technique \n\nThis process is meant to transform the time series data to stationary, as ARIMA model only works with stationary time series data. ","metadata":{}},{"cell_type":"code","source":"train_df['diff_sales'] = train_df['sales'].diff()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['diff_sales'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['diff_sales'] = train_df['sales'] - train_df['sales'].shift(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the first row since differencing introduces a NaN value\ntrain_df = train_df.dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['diff_sales'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the autocorrelation\nautocorrelation = sm.tsa.acf(train_df['diff_sales'], nlags=20)\n\n# Plot the autocorrelation chart\nplt.figure(figsize=(10, 6))\nplt.stem(range(len(autocorrelation)), autocorrelation, use_line_collection=True)\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Chart')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stationarity Test \n","metadata":{}},{"cell_type":"markdown","source":"There are various statistical tests to check stationarity, including the Augmented Dickey-Fuller (ADF) test and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test.","metadata":{}},{"cell_type":"markdown","source":"### Augmented Dickey-Fuller (ADF)  test \n\nThe Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series is stationary or non-stationary. Stationarity is an important assumption in many time series analysis models.\n\nThe ADF test evaluates the null hypothesis that the time series has a unit root, indicating non-stationarity. The alternative hypothesis is that the time series is stationary.\n\nWhen performing the ADF test, we obtain the ADF statistic and the p-value. The ADF statistic is a negative number and the more negative it is, the stronger the evidence against the null hypothesis. The p-value represents the probability of observing the ADF statistic or a more extreme value if the null hypothesis were true. A low p-value (below a chosen significance level, typically 0.05) indicates strong evidence against the null hypothesis and suggests that the time series is stationary.","metadata":{}},{"cell_type":"code","source":"ts = train_df['diff_sales']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform the ADF test\nresult = adfuller(ts)\n\n# Extract and print the test statistics and p-value\nadf_statistic = result[0]\np_value = result[1]\nprint(\"ADF Statistic:\", adf_statistic)\nprint(\"p-value:\", p_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ADF statistic is -11.494679187188824. This statistic is a negative value and is more negative than the critical values at common significance levels. This suggests strong evidence against the null hypothesis of a unit root, indicating that the time series is stationary.\n\nThe p-value is 4.645171054101398e-21, which is a very small value close to zero. Typically, if the p-value is below a chosen significance level (e.g., 0.05), it indicates strong evidence to reject the null hypothesis. In your case, the extremely small p-value suggests strong evidence against the presence of a unit root and supports the stationarity of the time series.","metadata":{}},{"cell_type":"markdown","source":"### Kwiatkowski-Phillips-Schmidt-Shin (KPSS)\n\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test is another statistical test used to assess the stationarity of a time series. It is complementary to the Augmented Dickey-Fuller (ADF) test.\n\nThe KPSS test evaluates the null hypothesis that the time series is stationary against the alternative hypothesis of non-stationarity. Unlike the ADF test, which assumes the presence of a unit root, the KPSS test assumes the absence of a unit root.\n\nThe test calculates the KPSS statistic, which measures the cumulative sum of squared deviations from the mean in the series. It also provides a p-value that indicates the probability of observing the KPSS statistic or a more extreme value under the null hypothesis.\n\nInterpreting the results of the KPSS test involves considering the KPSS statistic and the associated p-value. If the KPSS statistic is greater than the critical value at a chosen significance level (e.g., 0.05), it provides evidence against the null hypothesis of stationarity. Conversely, if the KPSS statistic is smaller than the critical value, it suggests that the time series is stationary.","metadata":{}},{"cell_type":"code","source":"result = kpss(ts)\n\n# Extract and print the test statistic and p-value\nkpss_statistic = result[0]\np_value = result[1]\nprint(\"KPSS Statistic:\", kpss_statistic)\nprint(\"p-value:\", p_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The KPSS statistic is 0.02685487746003539. This statistic measures the discrepancy between the observed series and the series' trend. It indicates how far the series deviates from stationarity. A smaller KPSS statistic suggests a closer fit to stationarity.\n\nThe p-value is 0.1, which is equal to the chosen significance level of 0.1. Typically, if the p-value is greater than the significance level, it suggests that there is insufficient evidence to reject the null hypothesis of stationarity. Based on the result, the p-value is equal to the significance level, indicating that the results are inconclusive.","metadata":{}},{"cell_type":"markdown","source":"# Final ACF & PACF ","metadata":{}},{"cell_type":"code","source":"# Plot the Autocorrelation Function (ACF)\nplt.figure(figsize=(10, 4))\nax1 = plt.subplot(121)\nplot_acf(train_df['diff_sales'], ax=ax1)\n\n# Plot the Partial Autocorrelation Function (PACF)\nax2 = plt.subplot(122)\nplot_pacf(train_df['diff_sales'], ax=ax2)\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Autoregressive Integrated Moving Average Model (ARIMA) model","metadata":{}},{"cell_type":"code","source":"p = 2\n\nd = 1 \n\nq = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_np = train_df['diff_sales'].values.astype('float64')\nmodel = sm.tsa.ARIMA(train_np, order=(p, d, q))\n\nresult = model.fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the model summary\nprint(result.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\nstart_idx = len(train_np)\nend_idx = len(train_np) + len(test_df) - 1\npredictions = result.predict(start=start_idx, end=end_idx)\n\n# Print the predictions\nprint(predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_values = train_df['diff_sales']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Truncate or pad the predictions array to match the length of actual_values\npredictions = predictions[:len(actual_values)]\n\n# Calculate evaluation metrics\nmae = np.mean(np.abs(predictions - actual_values))\nmse = np.mean((predictions - actual_values) ** 2)\nrmse = np.sqrt(mse)\n\n# Print the evaluation metrics\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission \n\n","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test_df['id'] \nsubmission['sales'] = np.zeros(len(test_df))\n\n# save the submission file as a CSV file\nsubmission.to_csv('mysubmission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}