{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![image.png](https://img.freepik.com/free-vector/store-buildings-shopping-area-with-parking-scene-illustration_107791-4003.jpg?w=2000&t=st=1679653405~exp=1679654005~hmac=61a5177e5df89555b2947029ab170de95c77980aac7c26d62271ae238af28e25)\n<a href=\"https://www.freepik.com/free-vector/shops-commercial-buildings-exterior-city-street-cartoon-summer-town-with-cafe-library-pharmacy-supermarket-facade-modern-architecture-auto-parts-store-boutique_10798318.htm#page=2&query=store%20illustration&position=13&from_view=search&track=ais\">Image by upklyak</a> on Freepik","metadata":{"_uuid":"c81c9189-3041-4c3b-8cd1-15afd2733a17","_cell_guid":"972ca029-e738-4410-a7c0-b2bee61ef40a","trusted":true}},{"cell_type":"markdown","source":"# Introduction\n\n**Description**\n\nIn this time-series 'getting-started' competition, we are asked to forecast store sales on data from Corporaci√≥n Favorita, a large Ecuadorian-based grocery retailer. We need a model that can predict unit sales for thousands of items sold at different stores.\nFor this competition we have different datasets describing sales, stores, holiday data and more between 2013 and 2017 in Ecuador.\n\n**Goal**\n\nTo predict the sales **per product, per store** **for the next 16 days.**\n\n**Metric**\n\nThe evaluation metric used for this competition is\n**Root-Mean-Squared-Logarithmic-Error** (**RMSLE**). (Taking logs means that errors in predicting big salesnumbers and smaller salesnumbers will affect the result more evenly.)","metadata":{"_uuid":"802924ef-daa7-4574-8b5a-0b43be60e9dc","_cell_guid":"6cb324dc-eb25-4af1-84d8-441f1ae90160","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport datetime as dt\nimport seaborn as sns\nfrom re import search\n\nrandom.seed(333)\npd.options.mode.chained_assignment = None\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"1cc579d3-0e02-4a74-84db-a01ada83460b","_cell_guid":"a7b3beea-ad0c-4712-865e-3eabdb2d62e3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Exploratory Data Analysis:\n\n# 1.1 Dataframes & NAs","metadata":{"_uuid":"19fa2f32-43fd-4d9f-88bd-1d67e7c47446","_cell_guid":"021aa771-ddf7-42a3-ad01-b8a0414b82b2","trusted":true}},{"cell_type":"markdown","source":"As a first step of the EDA we would like to know what our data looks like, and if there are any NAs in the dataset.\n\nWe have different data sets to our disposal:\n\n- **holiday_events**: a list with all ecuadorian holidays and events;\n- **oil**: a list of oilprices meant to serve as an economic indicator of Ecuador;\n- **stores**: a dataset with information about our stores: includes city, state, type and others;\n- **transactions**: a dataset containing the number of aggregated transactions for each store on each day;\n- **test**: general testset of 16 days of sales we will need to predict;\n- **train**: a huge trainset with about 4 years of data to predict our test sales data.\n\nThis notebook makes use of all datasets except for the oilprices as on the surface it didn't seem to give any improvement in the modelling results. This could potentially be added in a future update of this notebook if it does happen to be useful.","metadata":{"_uuid":"cffe6111-2583-4bb0-842e-33e85ebde57e","_cell_guid":"d6851dd8-906f-4695-b9c9-b41a9cfefdb1","trusted":true}},{"cell_type":"code","source":"# Read train/test data and check colnames & NA's:\n\noriginal_train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\noriginal_test = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\n\n# Dataframe info:\nprint(original_train.info())\n\n# Check NAs:\noriginal_train.isna().any()","metadata":{"_uuid":"071cc3ce-b2b0-4956-b976-04297d8e50a3","_cell_guid":"b84a9153-bc4d-4f1b-82fc-060f490ec0ff","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have ID, date, store number, (product) family, saleprice and promotion columns.\nOur train and test datasets have zero missing values.","metadata":{"_uuid":"9cc38d0d-8941-4e89-937c-83ac29d19f7a","_cell_guid":"42cdbbeb-040c-4814-ad90-5584b896cbbc","trusted":true}},{"cell_type":"code","source":"# Find out how many stores, products and dates are in our data:\n\noriginal_train['store_nbr'].unique().__len__() # 54 stores\noriginal_train['family'].unique().__len__() # 33 products\n\nlen(original_train) / 54 / 33 # 1684 days (between 4 and 5 years)\noriginal_train['date'].iloc[0] # 2013-01-01 is start\noriginal_train['date'].iloc[-1] # 2017-08-15 is end\n\nlen(original_test) / 54 / 33 # 16 days\noriginal_test['date'].iloc[0] # 2017-08-16 is test start\noriginal_test['date'].iloc[-1] # 2017-08-31 is test end","metadata":{"_uuid":"c5335547-467f-4fa4-9e82-e307a843eef8","_cell_guid":"7c20c8cb-4e83-44d7-a186-b4bcf698e544","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our main datasets consist of:\n\n- **54 stores**\n\n- **33 productgroups**\n\n\nWe are asked to predict the sales for each productgroup (33), at each store (54) during 16 consecutive days.\nThis means we will have to do 33 * 54 * 16 = 28512 predictions. To make this more manageable, we can create a dataframe and predictions for each individual productgroup. This means we will create 33 dataframes that each do 864 predictions.\n\nFor faster runtime and better oversight we will first create different dataframes that we then merge together through a pipeline into 33 individual product dataframes.","metadata":{"_uuid":"dd32c68a-3491-4a60-9dd6-1b473825ece5","_cell_guid":"08f96d39-3db6-4f66-9400-25193b189d65","trusted":true}},{"cell_type":"markdown","source":"# 1.2 Visualization","metadata":{"_uuid":"75cd9916-e55a-4bdf-90ed-00845d07cfc3","_cell_guid":"6523186a-aab0-4498-bbc0-4aa58d02cded","trusted":true}},{"cell_type":"markdown","source":"Since we are going to create a dataframe per product, it is best to create a graph to see the sales evolution for each family over the years. We want the sum of sales for each month, for each year, for each family, so we aggregate accordingly.","metadata":{"_uuid":"700a637f-cc8b-4bf8-a9d8-58d703919067","_cell_guid":"e53c084a-3375-4c98-9a2b-1d3be81ade7a","trusted":true}},{"cell_type":"code","source":"original_train['date'] = pd.to_datetime(original_train['date'])\noriginal_train['year'] = original_train['date'].dt.year\noriginal_train['month'] = original_train['date'].dt.month\n\nmonthly_sales = original_train.groupby(['family', 'year','month']).agg({\"sales\" : \"sum\"}).reset_index()\n\n# The value of the last month (for each 33 products) we change to nan, as otherwise it will distort\n# the graph since this month's data is incomplete:\nfor x in range(33):\n    z = 55+(x*56)\n    monthly_sales.at[z,'sales'] = np.nan \n\n# We use seaborn's FacetGrid with a col_wrap of 3 to show all the graphs in rows of three.\n# We also need sharey = False so that the y axis of all the graphs is not shared but individual.\nproduct_lineplots = sns.FacetGrid(monthly_sales, col=\"family\", hue='year', sharey=False, height=3.5, col_wrap=3, palette='rocket_r')\nproduct_lineplots.map(sns.lineplot, \"month\", 'sales')\nproduct_lineplots.add_legend()\nproduct_lineplots.set(xlim=(1, 12), ylim=(0, None), xticks=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])","metadata":{"_uuid":"4418d94f-52b7-4069-be11-867e21eb2abe","_cell_guid":"f9b51e3b-008b-484c-a5fc-7f07d337698d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Some observations:*\n\nSome months show weird behaviour for some of our products (for example Pet Supplies, Produce,...):\n- 2014: feb(2), apr(4), may(5), jun(6), aug(8)\n- 2015: jan(1), feb(2), mar(3), apr(4), may(5)\n\n**December:**\n\nSeems to be the best sales month for many products, probably because it's a holiday period with both christmas and new year's eve.\n\n**Books:**\n\nSeems like the 'books' category is at the end of being phased out, it's probably a good idea to set this prediction to zero.\n\n**School And Office Supplies:**\n\nSeems to peak in April, but before this year more in August-September. These dates are particularly interesting for us since our prediction also takes place in the middle of this peak (end of August).","metadata":{"_uuid":"590afa7e-d0a2-456e-9b74-9bcac9dbec84","_cell_guid":"2075fbde-51dd-496f-bb27-4d6df273457c","trusted":true}},{"cell_type":"code","source":"# Create a graph for allsales:\n\ntotal_monthly_sales = original_train.groupby(['year','month']).agg({\"sales\" : \"sum\"}).reset_index()\n\ntotal_monthly_sales.at[55,'sales'] = np.nan\n\ntotal_plot = sns.lineplot(x='month', y='sales', hue='year', palette='rocket_r', data=total_monthly_sales)\ntotal_plot.set(xlim=(1, 12), ylim=(0, None), xticks=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])","metadata":{"_uuid":"e1bb9367-754b-436b-a520-858436d0f620","_cell_guid":"0f7c2fc1-03ac-477c-b812-2bd921b51e51","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.3 Create Variables","metadata":{"_uuid":"e517fa4e-1dbf-4c37-aee3-0f01edf831c9","_cell_guid":"33518d97-e837-454a-b3a1-275d723be380","trusted":true}},{"cell_type":"markdown","source":"We will create:\n\n- An ***independant*** dataframe: where one day = one row, independant of store numbers and product groups, here we create variables that are depending on the date, irrelevant of the store (for example, the day of the week)\n\n- An ***all stores*** dataframe: we create a dataframe where we aggregate all 54 stores, and add variables that are depending on the stores (for example: store_closed)\n\n- A ***group of product*** dataframes: the main dataframe where we aggregate per product and then add the previously created data from the independant df and all stores df.","metadata":{"_uuid":"372c44d3-2565-45c3-a7f0-8e69c7b19dc1","_cell_guid":"80fa58e7-d07f-4b92-bbd8-f470af36d4f0","trusted":true}},{"cell_type":"markdown","source":"# 1.3.1 Independant Dataframe","metadata":{"_uuid":"d0dd45ca-0b93-42e5-ac17-e0d3ac212026","_cell_guid":"0cdc2c91-58ba-45cc-8d93-1ffde3586d9f","trusted":true}},{"cell_type":"markdown","source":"For the **independant** dataframe we create the following functions:\n- **create_date_df**: small function to aggregate the dataframe into a smaller one grouped by date\n\n- **create_paydays**: for the government jobs (the biggest employer in Ecuador), wages are paid on both the first day of the month, and the 15th. This function creates the payday columns: both effective paydays, as a scale that counts up to have a var that shows how long ago people have been paid.\n\n- **onehotencode**: function to turn the chosen columns into different binary columns (we will also use this function for our all stores df).\n\n- **independant pipeline**: pipeline that merges all previous functions to create one dataframe. Here we also add extra variables: regular date variables, a variable that indicates when the earthquake happened, and a variable for when school starts (to increase our school and office supplies score).","metadata":{"_uuid":"18a1b932-276e-44bb-a696-cccc9741ebc7","_cell_guid":"98970677-111f-49b6-9faa-3ec5de19945d","trusted":true}},{"cell_type":"code","source":"def create_date_df(df, store_nr):\n\n    single_store_df = df[df['store_nbr'] == store_nr]\n    single_store_series = single_store_df.groupby([\"date\"]).sum(numeric_only=True)\n\n    return single_store_series","metadata":{"_uuid":"01273f3b-dc78-4ba9-8d5d-07ea462be148","_cell_guid":"e3b4992b-515c-493b-95ac-3decb81fdb9c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_payday_anchors(df):\n\n    df.reset_index(inplace=True)\n    df['Payday'] = 0\n\n    for id, row in df.iterrows():\n\n        if search('-01$', row['date']):\n            df.at[id - 1, 'Payday'] = 1\n\n        if search('-15$', row['date']):\n            df.at[id, 'Payday'] = 1\n\n    df = df[:-1]\n\n    return df","metadata":{"_uuid":"85b6eb86-d584-471e-acaa-3f803cf9a876","_cell_guid":"4ca2abd3-4454-41b8-8c39-8ba0da22a062","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def onehotencode(df, list_of_variables):\n\n    column_name_list = list()\n    my_category_list = list()\n\n    for column in list_of_variables:\n\n        categories = df[column].unique().tolist()\n\n        for i in categories:\n\n            this_list = ((df[column] == i) * 1).tolist()\n\n            column_name_list.append(column + str(i))\n            my_category_list.append(this_list)\n\n            print('Finished ' + str(i))\n\n        print(str(column) + ' is done.')\n\n    onehotencode_df = pd.DataFrame(my_category_list).transpose()\n    onehotencode_df.columns = np.asarray(column_name_list)\n\n    return onehotencode_df","metadata":{"_uuid":"8d895003-4d92-40e8-80b3-e038fab64e7d","_cell_guid":"1e42292b-b82c-4d47-b493-29da583a0328","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def independant_pipeline():\n\n    original_train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\n    original_test = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\n\n    # Get one store one product DF:\n    one_store_df = create_date_df(original_train, 1)\n    one_store_df_test = create_date_df(original_test, 1)\n    one_store_df.drop('sales', axis=1, inplace=True)\n\n    one_store_df = pd.concat([one_store_df, one_store_df_test])\n\n    del original_train\n    del original_test\n\n    ########################\n    # Add Paydays          #\n    ########################\n\n    one_store_df = create_payday_anchors(one_store_df)\n\n    payday_series = one_store_df['Payday']\n    payday_count = 0\n    payday_scale_list = list()\n\n    for x in range(payday_series.__len__()):\n\n        if payday_series[x] == 1:\n            payday_count = 0\n            payday_scale_list.append(payday_count)\n        else:\n            payday_count += 1\n            payday_scale_list.append(payday_count)\n\n    one_store_df['Payday_Scale'] = payday_scale_list\n\n    one_store_df.drop(['id'], axis=1, inplace=True)\n\n    ######################\n    # Add Date Variables #\n    ######################\n\n    dayoftheweek_list = list()\n    dayoftheyear_list = list()\n    monthoftheyear_list = list()\n    year_list = list()\n\n    for x in range(1700): # because 1700 different days\n\n        thisdate = one_store_df['date'][x]\n        thisdayoftheweek = dt.datetime.strptime(thisdate, '%Y-%m-%d').strftime('%A')\n        thisdayoftheyear = dt.datetime.strptime(thisdate, '%Y-%m-%d').strftime('%j')\n        thismonthoftheyear = dt.datetime.strptime(thisdate, '%Y-%m-%d').strftime('%B')\n        thisyear = dt.datetime.strptime(thisdate, '%Y-%m-%d').strftime('%Y')\n\n        dayoftheweek_list.append(thisdayoftheweek)\n        dayoftheyear_list.append(thisdayoftheyear)\n        monthoftheyear_list.append(thismonthoftheyear)\n        year_list.append(thisyear)\n\n    one_store_df['DayOfTheWeek'] = dayoftheweek_list\n    one_store_df['DayOfTheYear'] = dayoftheyear_list\n    one_store_df['MonthOfTheYear'] = monthoftheyear_list\n    one_store_df['Year'] = year_list\n\n    one_store_df['DayOfTheYear'] = pd.to_numeric(one_store_df['DayOfTheYear'])\n\n    # Convert DayOfTheWeek to numeric:\n\n    dayoftheweek_scale_dict = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4,\n                               'Friday': 5, 'Saturday': 6, 'Sunday': 7}\n\n    one_store_df['dayoftheweek_scale'] = one_store_df['DayOfTheWeek'].map(dayoftheweek_scale_dict)\n\n    ########################\n    # OneHotEncode         #\n    ########################\n\n    onehotcolumnlist = ('DayOfTheWeek', 'MonthOfTheYear', 'Year')\n    onehotencode_df = onehotencode(one_store_df, onehotcolumnlist)\n    one_store_df = pd.concat([one_store_df, onehotencode_df], axis=1)\n\n    ########################\n    # Drop Some Cols       #\n    ########################\n\n    one_store_df.drop(['store_nbr', 'DayOfTheWeek', 'onpromotion'], axis=1, inplace=True)\n\n    return one_store_df","metadata":{"_uuid":"62b8ec84-432d-4787-ab02-4497cd4a9e48","_cell_guid":"9fe017bb-79d4-482d-a734-82acbd7c536d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Execute Pipeline --- #\n\nindependant_df = independant_pipeline()","metadata":{"_uuid":"60147193-0ae9-45ee-a2c4-c017b3a75de7","_cell_guid":"02038ad5-8eb4-4184-a8a7-0809709ea7fc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.3.2 All Stores Dataframe","metadata":{"_uuid":"5b91f1a3-eb1c-48f8-833a-06b774485190","_cell_guid":"5e6da83a-e6c3-4831-951a-7b3146862132","trusted":true}},{"cell_type":"markdown","source":"For our **all stores** dataframe we create the following functions:\n- **create_multi_store_one_product_df**: small function to create a df aggregated on stores and filtered on one product.\n\n- **create_holiday_variables**: function that creates all the different holiday variables (depending on the store location). \n\n- **create_location_variables**: function that creates some extra variables based on the store locations. This is external data that we add that was not available in our initial dataframes on kaggle. We specifically look at elevation (Ecuador is a mountainous country) and city density (a store in a big city might have very different effects at certain times than a store in a sparsely populated town).\n\n- **all_stores_pipeline**: pipeline function that merges the previous functions (and also performs onehotencode) to create our all stores df.","metadata":{"_uuid":"8804fb92-b77d-4b76-9167-ba451ecf24b1","_cell_guid":"d5cc48ef-34ca-423c-933e-5cd490bf45e1","trusted":true}},{"cell_type":"code","source":"def create_multi_store_one_product_df(df, product_name):\n\n    multistore_single_product = df[df['family'] == product_name]\n\n    return multistore_single_product","metadata":{"_uuid":"dadfa894-3d04-4d0d-8311-650fbcca2f4a","_cell_guid":"9bd25c8f-61af-4339-b063-7647be7b981c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_holiday_variables(df):\n\n    holidays = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\n\n    holidays = holidays[holidays['transferred'] == False]\n    holidays['holiday_type'] = holidays['type']\n    holidays.drop(['transferred', 'description', 'type'], axis=1, inplace=True)\n\n    national_holidays = holidays[holidays['locale'] == 'National']\n    national_holidays['national_holiday_type'] = national_holidays['holiday_type']\n    national_holidays.drop(['locale', 'locale_name', 'holiday_type'], axis=1, inplace=True)\n    national_holidays.drop_duplicates(subset='date', keep=\"first\", inplace=True)\n    df = pd.merge(df, national_holidays, how='left', on=['date'])\n\n    state_holidays = holidays[holidays['locale'] == 'Regional']\n    state_holidays['state'] = state_holidays['locale_name']\n    state_holidays['state_holiday_type'] = state_holidays['holiday_type']\n    state_holidays.drop(['locale', 'locale_name', 'holiday_type'], axis=1, inplace=True)\n    df = pd.merge(df, state_holidays, how='left', on=['date', 'state'])\n\n    city_holidays = holidays[holidays['locale'] == 'Local']\n    city_holidays['city'] = city_holidays['locale_name']\n    city_holidays['city_holiday_type'] = city_holidays['holiday_type']\n    city_holidays.drop(['locale', 'locale_name', 'holiday_type'], axis=1, inplace=True)\n    city_holidays.drop([265], axis=0, inplace=True)\n    df = pd.merge(df, city_holidays, how='left', on=['date', 'city'])\n\n    df['holiday_type'] = np.nan\n    df['holiday_type'] = df['holiday_type'].fillna(df['national_holiday_type'])\n    df['holiday_type'] = df['holiday_type'].fillna(df['state_holiday_type'])\n    df['holiday_type'] = df['holiday_type'].fillna(df['city_holiday_type'])\n    df.drop(['national_holiday_type', 'state_holiday_type', 'city_holiday_type'], axis=1, inplace=True)\n\n    return df","metadata":{"_uuid":"7013442b-ba31-450a-ad7d-4c29ea74b1a5","_cell_guid":"d75b6757-0e8c-4764-9f93-146d80410d08","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_location_variables(df):\n\n    stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\n\n    # stores['city'].unique()\n    # ['Quito', 'Santo Domingo', 'Cayambe', 'Latacunga', 'Riobamba',\n    #  'Ibarra', 'Guaranda', 'Puyo', 'Ambato', 'Guayaquil', 'Salinas',\n    #  'Daule', 'Babahoyo', 'Quevedo', 'Playas', 'Libertad', 'Cuenca',\n    #  'Loja', 'Machala', 'Esmeraldas', 'Manta', 'El Carmen']\n\n    # Height dict:\n    Height = {'Quito': 2850, 'Santo Domingo': 550, 'Cayambe': 2830, 'Latacunga': 2860,\n              'Riobamba': 2754, 'Ibarra': 2225, 'Guaranda': 2668, 'Puyo': 950,\n              'Ambato': 2577, 'Guayaquil': 0, 'Salinas': 0, 'Daule': 0,\n              'Babahoyo': 0, 'Quevedo': 75, 'Playas': 0, 'Libertad': 36,\n              'Cuenca': 2560, 'Loja': 2060, 'Machala': 0, 'Esmeraldas': 15,\n              'Manta': 0, 'El Carmen': 250}\n\n    # Elevation:\n    # 0 = 0 - 200 (10)\n    # 1 = 200-700 (2)\n    # 2 = 700-1500 (1)\n    # 3 = 1500-2300 (2)\n    # 4 = 2300-3000 (7)\n\n    Population = {'Quito': 2000000, 'Santo Domingo': 460000, 'Cayambe': 40000, 'Latacunga': 100000,\n                  'Riobamba': 157000, 'Ibarra': 150000, 'Guaranda': 35000, 'Puyo': 40000,\n                  'Ambato': 350000, 'Guayaquil': 2750000, 'Salinas': 50000, 'Daule': 130000,\n                  'Babahoyo': 105000, 'Quevedo': 200000, 'Playas': 40000, 'Libertad': 105000,\n                  'Cuenca': 445000, 'Loja': 200000, 'Machala': 260000, 'Esmeraldas': 200000,\n                  'Manta': 240000, 'El Carmen': 120000}\n\n    # Population:\n    # 0 = 0-60000 (5)\n    # 1 = 60000-160000 (12)\n    # 2 = 160000-280000 (3)\n    # 3 = 280000+ (2)\n\n    Size = {'Quito': 372, 'Santo Domingo': 60, 'Cayambe': 378, 'Latacunga': 370,\n            'Riobamba': 59, 'Ibarra': 242, 'Guaranda': 520, 'Puyo': 88,\n            'Ambato': 47, 'Guayaquil': 345, 'Salinas': 27, 'Daule': 475,\n            'Babahoyo': 175, 'Quevedo': 300, 'Playas': 280, 'Libertad': 28,\n            'Cuenca': 71, 'Loja': 44, 'Machala': 67, 'Esmeraldas': 70,\n            'Manta': 60, 'El Carmen': 1250}\n\n    stores[\"City_Population\"] = stores['city'].map(Population)\n    stores[\"City_Elevation\"] = stores['city'].map(Height)\n    stores[\"City_Size\"] = stores['city'].map(Size)\n    stores[\"City_Density\"] = round(stores[\"City_Population\"] / stores[\"City_Size\"],0)\n    stores[\"City_Population_Category\"] = 0\n    stores[\"City_Elevation_Category\"] = 0\n    stores[\"City_Size_Category\"] = 0\n    stores[\"City_Density_Category\"] = 0\n\n    for id, row in stores.iterrows():\n\n        if row['City_Elevation'] < 200:\n            stores.at[id, 'City_Elevation_Category'] = 0\n        elif row['City_Elevation'] < 700:\n            stores.at[id, 'City_Elevation_Category'] = 1\n        elif row['City_Elevation'] < 1500:\n            stores.at[id, 'City_Elevation_Category'] = 2\n        elif row['City_Elevation'] < 2300:\n            stores.at[id, 'City_Elevation_Category'] = 3\n        else:\n            stores.at[id, 'City_Elevation_Category'] = 4\n\n        if row['City_Population'] < 60000:\n            stores.at[id, 'City_Population_Category'] = 0\n        elif row['City_Population'] < 160000:\n            stores.at[id, 'City_Population_Category'] = 1\n        elif row['City_Population'] < 280000:\n            stores.at[id, 'City_Population_Category'] = 2\n        else:\n            stores.at[id, 'City_Population_Category'] = 3\n\n        if row['City_Size'] < 150:\n            stores.at[id, 'City_Size_Category'] = 0\n        elif row['City_Size'] < 325:\n            stores.at[id, 'City_Size_Category'] = 1\n        elif row['City_Size'] < 1000:\n            stores.at[id, 'City_Size_Category'] = 2\n        else:\n            stores.at[id, 'City_Size_Category'] = 3\n\n        if row['City_Density'] < 150:\n            stores.at[id, 'City_Density_Category'] = 0\n        elif row['City_Density'] < 325:\n            stores.at[id, 'City_Density_Category'] = 1\n        elif row['City_Density'] < 1000:\n            stores.at[id, 'City_Density_Category'] = 2\n        elif row['City_Density'] < 3000:\n            stores.at[id, 'City_Density_Category'] = 3\n        elif row['City_Density'] < 7000:\n            stores.at[id, 'City_Density_Category'] = 4\n        else:\n            stores.at[id, 'City_Density_Category'] = 5\n\n    city_variables_df = stores[['store_nbr', 'City_Elevation_Category', 'City_Population_Category', 'City_Size_Category',\n                                'City_Density_Category', 'City_Density']]\n    df = pd.merge(df, city_variables_df, how='left', on='store_nbr')\n    df.drop(['city','state'], axis=1, inplace=True)\n\n    return df","metadata":{"_uuid":"d9ead4c1-3c66-47b6-8205-71f3ca1aa539","_cell_guid":"52eaa4ce-ee1f-4468-aee0-ec8fa7c2978b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def all_stores_pipeline():\n\n    originaltrainFull = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\n    originaltest = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\n\n    stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\n    transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\n\n    all_stores_df = create_multi_store_one_product_df(originaltrainFull, 'AUTOMOTIVE')\n    all_stores_df_test = create_multi_store_one_product_df(originaltest, 'AUTOMOTIVE')\n    all_stores_df.drop('sales', axis=1, inplace=True)\n\n    all_stores_df = pd.concat([all_stores_df, all_stores_df_test])\n    all_stores_df.drop(['id', 'family', 'onpromotion'], axis=1, inplace=True)\n\n    all_stores_df = pd.merge(all_stores_df, stores, how='left', on=['store_nbr'])\n\n    del originaltest\n    del originaltrainFull\n\n    #########################\n    # Add Holiday Variables #\n    #########################\n\n    all_stores_df = create_holiday_variables(all_stores_df)\n\n    ##########################\n    # Add Location Variables #\n    ##########################\n\n    all_stores_df = create_location_variables(all_stores_df)\n\n    ################################\n    # Create Store Closed Variable #\n    ################################\n\n    all_stores_df = pd.merge(all_stores_df, transactions, how='left', on=['date', 'store_nbr'])\n    all_stores_df['transactions'].fillna(0, inplace=True)\n\n    store_closed = [1 if x == 0 else 0 for x in all_stores_df['transactions']]\n\n    all_stores_df['store_closed'] = store_closed\n    all_stores_df['store_closed'].iloc[-864:] = 0\n\n    all_stores_df.drop('transactions', axis=1, inplace=True)\n\n    ###################\n    # OneHotEncode    #\n    ###################\n\n    all_stores_df['isholiday'] = 1\n    thislist = all_stores_df['holiday_type'].isna()\n    all_stores_df.loc[thislist,'isholiday'] = 0\n\n    onehotcolumnlist = ('store_nbr', 'type', 'cluster', 'holiday_type', 'City_Elevation_Category',\n                        'City_Population_Category', 'City_Density_Category', 'City_Size_Category')\n\n    onehotencode_df = onehotencode(all_stores_df, onehotcolumnlist)\n    all_stores_df = pd.concat([all_stores_df, onehotencode_df], axis=1)\n\n    ###################\n    # Drop Some Cols  #\n    ###################\n\n    all_stores_df.drop(['type', 'cluster', 'holiday_type'], axis=1, inplace=True)\n\n    return all_stores_df","metadata":{"_uuid":"0a6a140d-20d8-4ef2-a876-2481837f4754","_cell_guid":"cc87c6d9-be21-4296-858b-3af6b474a4c4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Execute Pipeline --- #\n\nall_stores_df = all_stores_pipeline()","metadata":{"_uuid":"a55e9063-0b0d-4a6d-aed0-a79eb19b4725","_cell_guid":"b65bd706-a06d-4095-9917-9fdb9ec1b3fc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.3.3 Product Dataframes","metadata":{"_uuid":"3e2a8e7c-2bd5-41f0-9d58-3798ec2aaf6e","_cell_guid":"0ac3be10-e986-43ed-aa8b-f0f79179d739","trusted":true}},{"cell_type":"markdown","source":"The last step of our variable creation phase, is to create 33 **product** dataframes. We create a pipeline that integrates both our **independant** and our **all stores** dataframes.","metadata":{"_uuid":"070256fe-b23a-4b03-9bd7-1df6e179176b","_cell_guid":"c93979cd-a6b4-4bc9-82a9-b0ff50a8028a","trusted":true}},{"cell_type":"code","source":"def full_product_pipeline(family, independant_df=independant_df, all_stores_df=all_stores_df):\n\n    originaltrainFull = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\n    originaltest = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\n\n    multistore_product = create_multi_store_one_product_df(originaltrainFull, family)\n\n    # merge with test:\n    multistore_product_test = create_multi_store_one_product_df(originaltest, family)\n    multistore_product_test['sales'] = np.nan\n\n    del originaltrainFull\n    del originaltest\n\n    # take log of sales:\n    multistore_product['sales'] = np.log1p(multistore_product['sales']+1)\n\n    msp_full = pd.concat([multistore_product, multistore_product_test])\n\n    # reset index:\n    msp_full.reset_index(inplace=True, drop=True)\n\n    ######################\n    # Add Independant DF #\n    ######################\n\n    msp_full = pd.merge(msp_full, independant_df, how='left', on=['date'])\n\n    #####################\n    # Add All Stores DF #\n    #####################\n\n    msp_full = pd.merge(msp_full, all_stores_df, how='left', on=['date', 'store_nbr'])\n\n    ############################\n    # Add Earthquake Info      #\n    ############################\n\n    earthquake_day = [1 if x == '2016-04-16' else 0 for x in msp_full['date']]\n    earthquake_impact = [1 if (x > '2016-04-16') & (x < '2016-05-16') else 0 for x in msp_full['date']]\n\n    msp_full['earthquake_day'] = earthquake_day\n    msp_full['earthquake_impact'] = earthquake_impact\n\n    ############################\n    # Add School Info          #\n    ############################\n\n    school_preparation = [1 if (x > '2014-09-15') & (x < '2014-10-15') or (x > '2015-09-15') & (x < '2015-10-15')\n                          or (x > '2016-09-15') & (x < '2016-10-15') or (x > '2017-09-15') & (x < '2017-10-15')\n                          else 0 for x in msp_full['date']]\n\n    msp_full['school_preparation'] = school_preparation\n\n    #############################\n    # Clean DF before modelling #\n    #############################\n\n    msp_full.drop(['family', 'MonthOfTheYear'], axis=1, inplace=True)\n\n    return msp_full","metadata":{"_uuid":"964f0f0d-73be-470f-a363-ae27ac021b2a","_cell_guid":"87495179-73d0-4097-b2e1-5e494e616ef7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Execute Full Product Pipeline for each product --- #\n\n# List all product families:\n\nlist_of_families = ['AUTOMOTIVE', 'BABY CARE', 'BEAUTY', 'BEVERAGES', 'BOOKS',\n                    'BREAD/BAKERY', 'CELEBRATION', 'CLEANING', 'DAIRY', 'DELI', 'EGGS',\n                    'FROZEN FOODS', 'GROCERY I', 'GROCERY II', 'HARDWARE',\n                    'HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES',\n                    'HOME CARE', 'LADIESWEAR', 'LAWN AND GARDEN', 'LINGERIE',\n                    'LIQUOR,WINE,BEER', 'MAGAZINES', 'MEATS', 'PERSONAL CARE',\n                    'PET SUPPLIES', 'PLAYERS AND ELECTRONICS', 'POULTRY',\n                    'PREPARED FOODS', 'PRODUCE', 'SCHOOL AND OFFICE SUPPLIES',\n                    'SEAFOOD']\n\n# Create new .csv for each product family:\n\nfor x in list_of_families:\n\n    this_df = full_product_pipeline(x)\n\n    if x == 'BREAD/BAKERY':\n\n            x = 'BREADBAKERY'\n\n    print('Completed eda for ' + str(x))\n    this_df.to_csv('/kaggle/working/'+str(x)+'.csv', index=False)","metadata":{"_uuid":"165b1372-1ae2-4231-a575-82f9e876b27f","_cell_guid":"903323c8-653e-4294-82e9-57617711802f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Modelling\n\n![image.png](https://img.freepik.com/free-vector/conveyor-belt-with-cardboard-boxes-factory-plant-warehouse-post-office-interior-with-automated-production-line-with-parcels-goods-product-carton-packages-cartoon-illustration_107791-5968.jpg?w=2000&t=st=1679672244~exp=1679672844~hmac=0b9c50cfbb54356678ae4a5b18ab3fecac54d088611f161e4bb1c57ad86e7225)\n<a href=\"https://www.freepik.com/free-vector/conveyor-belt-with-cardboard-boxes-factory-plant-warehouse-post-office-interior-with-automated-production-line-with-parcels-goods-product-carton-packages-cartoon-illustration_13485069.htm#query=product%20binning&position=44&from_view=search&track=ais\">Image by upklyak</a> on Freepik\n\n# 2.1 Validation Testing","metadata":{"_uuid":"4dce4627-f17c-444d-a4b7-85bb9988549b","_cell_guid":"072972aa-7410-4918-abb5-c6ad5d20555c","trusted":true}},{"cell_type":"markdown","source":"For modelling **LGBMR** seem to give the best results. \n\nYou can test the model either on a validation set or on several cross-validation folds. \nMainly for speed, I opted to test on just one validation set 16 days before the actual prediction period.\nThis is not ideal and might to overfit a bit as it is just one testset, but since the validation set is close to the real set it should be somewhat representative of our kaggle test.\n\nWe create some supporting functions to support our modelling journey:\n\n- **scorethis_rmsle**: this function scores a prediction with a set of ground truths in the same way this competition is being scored by kaggle (using rmsle).\n\n- **create_validation**: this creates our validation test and train sets, together with the ground truths. Respectively train, train_y, test and test_y. If validation=False, then this function creates train, train_y and test that we can use for our submission.\n\n- **lgbmr_run**: this function runs our chosen lgbmr model. It has two modes: 'validation' if you want to run on the validation set or 'submission' if you want to run it for a kaggle submission. \n\n- **execute_validation**: executes the chosen model on each of the product dataframes.","metadata":{"_uuid":"b24e3ab2-1a7a-48ac-b886-c2c939d58df8","_cell_guid":"c5c9d4a5-7f29-4f44-9fa8-f00ce7c8092b","trusted":true}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom lightgbm import LGBMRegressor\n\nsample_submission = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv')","metadata":{"_uuid":"9fae144b-9cba-4490-ade7-2d537c0937ad","_cell_guid":"cfa1d3c9-5833-4a21-bedb-acb7f72538ef","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scorethis_rmsle(prediction_list, y_list):\n\n    scorelist = list()\n\n    for x in range(prediction_list.__len__()):\n\n\n        log_score_x = np.abs(np.abs(prediction_list[x]) - np.abs(y_list[x]))\n        \n        try:\n            [scorelist.append(y) for y in log_score_x.values]\n        except:\n            scorelist.append(log_score_x)\n\n    score_array = np.array(scorelist)\n\n    rmsle = np.sqrt(np.mean(score_array**2)) # sqrt of mean of power of difference of the logs\n    rmsle = np.round(rmsle, 3)\n\n    return rmsle","metadata":{"_uuid":"4f166db2-dfc9-4846-bc88-acdf03887a12","_cell_guid":"7f2ef083-7194-4492-82c9-abedb2c566b7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_validation(this_family_df, validation=True):\n    \n    if validation is True:\n    \n        this_family_df = this_family_df[:-864]\n        # Remove the 864 top submission rows if it is for validation\n    \n    this_family_sales = this_family_df['sales']\n\n    this_family_df.drop(['sales', 'date'], axis=1, inplace=True)\n\n    ########################\n    # Scale Data           #\n    ########################\n\n    scaler = MinMaxScaler()\n    this_family_df[this_family_df.columns] = scaler.fit_transform(this_family_df[this_family_df.columns])\n\n    ########################\n    # Split Train and Test #\n    ########################\n\n    test = this_family_df.iloc[-864:]\n    test_y = this_family_sales.iloc[-864:]\n\n    train = this_family_df.iloc[:-864]\n    train_y = this_family_sales.iloc[:-864]\n\n    return train, train_y, test, test_y","metadata":{"_uuid":"2e028388-5742-4739-942b-3453b05c2aba","_cell_guid":"fc5393b2-964a-437c-937e-0f29acbf01e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lgbmr_run(train, train_y, test, test_y,\n           validation=True):\n    \n    #################\n    # Create Model  #\n    #################\n\n    lgbmr_model = LGBMRegressor(\n        colsample_bytree=0.7,\n        learning_rate=0.055,\n        min_child_samples=10,\n        num_leaves=19,\n        objective='regression',\n        n_estimators=1000,\n        n_jobs=4,\n        random_state=337)\n\n    #################\n    # Execute LGBMR #\n    #################\n\n    lgbmr_model.fit(train, train_y)\n    lgbmr_pred = lgbmr_model.predict(test).tolist()\n    lgbmr_pred = [round(x, 2) for x in lgbmr_pred]\n    \n    if validation == True:\n        \n        # validation set also has ground truths:\n        test_y = test_y.to_list()\n\n        return lgbmr_pred, test_y\n\n    else:\n\n        return lgbmr_pred","metadata":{"_uuid":"2669e022-0f5f-4bc6-a13f-0c918ec7b78a","_cell_guid":"d37e337e-01b8-48be-83b1-2df290c5b4ca","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def execute_validation(thisfunc):\n\n    double_list_of_predictions = []\n    double_list_of_ground_truths = []\n\n    for x in list_of_families: # 33\n        \n        if x == 'BREAD/BAKERY':\n\n            x = 'BREADBAKERY'\n            # Otherwise would create an error searching for the BREAD/ directory instead of the file\n\n        print('Evaluating '+str(x)+'...')\n        \n        this_df = pd.read_csv('/kaggle/working/' + str(x) + '.csv')\n\n        train, train_y, test, test_y = create_validation(this_df)\n        pred, y = thisfunc(train, train_y, test, test_y, validation=True)\n        \n        if x == 'BOOKS':\n\n            zero_list = []\n\n            for g in range(864):\n\n                zero_list.append(0.6931471805599453) \n                # this will be exactly 0 when we transform our predictions again\n                # to before we did log(sales +1)\n\n            double_list_of_predictions.append(zero_list)\n            double_list_of_ground_truths.append(y) \n            \n        else:\n            \n            double_list_of_predictions.append(pred) # 33 * [864]\n            double_list_of_ground_truths.append(y) # 33 * [864]\n\n    list_of_predictions = list()\n    list_of_ground_truths = list()\n\n    for x in double_list_of_predictions:\n        for y in x:\n            list_of_predictions.append(y) # unpack 33 * 864\n\n    for x in double_list_of_ground_truths:\n        for z in x:\n            list_of_ground_truths.append(z) # unpack 33 * 864\n\n    return list_of_predictions, list_of_ground_truths","metadata":{"_uuid":"f357e33d-4d39-4c5a-9b84-6884e6a2b141","_cell_guid":"d7149f7f-0625-4a9d-b8d2-7ff6d0f873b3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Execute LGBMR Model On Validation Set --- #\n\n# Run this code if you want to do a validation test + see the score:\n\n# list_of_lgbmr_predictions, list_of_ground_truths = execute_validation(lgbmr_run)\n# scorethis_rmsle(list_of_lgbmr_predictions, list_of_ground_truths)","metadata":{"_uuid":"085a9c0a-ca8a-4b44-ad02-10a286cb8c45","_cell_guid":"ec945e43-98a9-4108-b041-39e7083ff8c7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some observations after more testing:\n\n- This scores 0.337 which is the best score I have gotten on the validation set\n- other hyperparameters on the same model score +0.338\n- other models score 0.358 (XGB) to 0.423 (Lasso) on the validation\n- simple stacking didn't improve the results \n- SVR takes to long without any variable selection method, so this hasn't been tested on this dataset yet","metadata":{"_uuid":"10b2eb0d-8fed-4691-8b81-42924fa766ec","_cell_guid":"f96baeb3-0dad-4670-a5bc-fee34c8a6a21","trusted":true}},{"cell_type":"markdown","source":"# 2.2 Kaggle Submission","metadata":{"_uuid":"d6472822-487a-4eeb-b6a0-b95a79042798","_cell_guid":"43175202-c605-42d8-a4e1-e3af9d6478bb","trusted":true}},{"cell_type":"markdown","source":"Now we are going to execute the same **LGBMR** model we've tested, on our kaggle submission set.","metadata":{"_uuid":"c5802b16-6f78-4ea7-b8a4-ca4244093754","_cell_guid":"3d916b30-d9a1-4cb3-a8bf-eb395b36486f","trusted":true}},{"cell_type":"code","source":"def execute_submission(thisfunc):\n\n    list_of_predictions = []\n\n    for x in list_of_families:\n        \n        if x == 'BREAD/BAKERY':\n\n            x = 'BREADBAKERY'\n            # Otherwise would create an error searching for the BREAD/ directory instead of the file\n\n        print('Evaluating '+str(x)+'...')\n        this_df = pd.read_csv('/kaggle/working/' + str(x) + '.csv')\n        \n        if x == 'BOOKS':\n\n            zero_list = []\n\n            for g in range(864):\n\n                zero_list.append(0.6931471805599453) \n                # this will be exactly 0 when we transform our predictions again\n                # to before we did log(sales +1)\n\n            list_of_predictions.append(zero_list)\n\n        else:\n    \n            train, train_y, test, test_y = create_validation(this_df, validation=False)\n            pred = thisfunc(train, train_y, test, test_y=None, validation=False)\n            list_of_predictions.append(pred)\n    \n    ###############################\n    # Put Back In Submission Form # \n    ###############################\n    \n    restructured_predictions = list()\n\n    for y in range(864):\n\n        for z in range(33):\n            restructured_predictions.append(list_of_predictions[z][y])\n\n    restructured_predictions = np.expm1(restructured_predictions) - 1\n\n    return restructured_predictions","metadata":{"_uuid":"3d648ec8-3f8d-4895-8512-59510fbf3e90","_cell_guid":"5e765710-cbdd-4e1d-8b1c-6a0c646bc0c9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Execute Submission --- #\n\nrestructured_predictions = execute_submission(lgbmr_run)\nsample_submission['sales'] = restructured_predictions\n\n# Convert some (slightly) negative predictions to a zero prediction:\nsample_submission['sales'] = [0 if x < 0 else x for x in sample_submission['sales']]\n\nsample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"_uuid":"c6343343-ea67-42ec-b90d-cdc719afd8df","_cell_guid":"9bf6bc64-93ae-4daa-a0d2-966431831e5a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for reading,\n\n\n- **If you enjoyed this notebook or if you learned something, a simple upvote would be greatly appreciated.**\n\n- **If you find a way to improve on this notebook, let me know in the comments !**\n\n\nArnout","metadata":{"_uuid":"ac0e5720-2116-4e5f-9a6d-9fef04c1c60e","_cell_guid":"ba908d99-8a3a-4617-b017-5cbd3526aa6b","trusted":true}}]}