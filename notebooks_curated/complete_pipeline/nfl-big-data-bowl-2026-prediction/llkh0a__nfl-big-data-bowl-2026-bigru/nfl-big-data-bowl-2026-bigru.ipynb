{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":114239,"databundleVersionId":13825858},{"sourceType":"datasetVersion","sourceId":13384049,"datasetId":8443465,"databundleVersionId":14093911},{"sourceType":"datasetVersion","sourceId":13392261,"datasetId":8431728,"databundleVersionId":14102947}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1926.412062,"end_time":"2025-10-04T05:34:49.10692","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-04T05:02:42.694858","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"split by StratifiedKFold\n\nPer-fold RMSEs: [0.62349, 0.63324, 0.60064, 0.67419, 0.63202]\n\nOOF CV RMSE: 0.63278","metadata":{}},{"cell_type":"code","source":"# ================================================================================\n# NFL BIG DATA BOWL 2026 - COMPLETE WORKING SOLUTION\n# Predicting player movement during pass plays with temporal features\n# ================================================================================\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nimport torch\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport gc\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nimport joblib\nfrom datetime import datetime\nfrom itertools import combinations\n# Machine Learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import OneCycleLR\n# Deep Learning\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport os\nimport math\nfrom torch.optim.lr_scheduler import LinearLR, SequentialLR\nwarnings.filterwarnings('ignore')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{"papermill":{"duration":0.005532,"end_time":"2025-10-04T05:02:52.459331","exception":false,"start_time":"2025-10-04T05:02:52.453799","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    \n    NN_PRETRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-public/bigru-public'\n    SEED = 42\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    N_FOLDS = 5\n    USE_PLAYERS_INTERACTIONS =  True\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    WINDOW_SIZE = 12","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.471428Z","iopub.status.busy":"2025-10-04T05:02:52.471113Z","iopub.status.idle":"2025-10-04T05:02:52.475739Z","shell.execute_reply":"2025-10-04T05:02:52.475061Z"},"papermill":{"duration":0.012016,"end_time":"2025-10-04T05:02:52.476875","exception":false,"start_time":"2025-10-04T05:02:52.464859","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_global_seeds(seed: int = 42):\n    \"\"\"Set seeds for reproducibility.\"\"\"\n    import random, os\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nset_global_seeds(Config.SEED)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.489081Z","iopub.status.busy":"2025-10-04T05:02:52.4887Z","iopub.status.idle":"2025-10-04T05:02:52.496615Z","shell.execute_reply":"2025-10-04T05:02:52.496141Z"},"papermill":{"duration":0.01475,"end_time":"2025-10-04T05:02:52.497525","exception":false,"start_time":"2025-10-04T05:02:52.482775","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(debug_fraction=1.0):\n    \"\"\"Load all training and test data with an option to use a fraction for debugging.\"\"\"\n    print(\"Loading data...\")\n    \n    # Training data\n    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    # Filter existing files\n    train_input_files = [f for f in train_input_files if f.exists()]\n    train_output_files = [f for f in train_output_files if f.exists()]\n    \n    print(f\"Found {len(train_input_files)} weeks of data\")\n    \n    # Load and concatenate with week column\n    train_input = pd.concat(\n        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_input_files, start=1)],\n        ignore_index=True\n    )\n    train_output = pd.concat(\n        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_output_files, start=1)],\n        ignore_index=True\n    )\n    \n    # Test data\n    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n    \n    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n    \n    # Use only a fraction of the games for debugging (select entire games)\n    if debug_fraction < 1.0:\n        unique_game_ids = train_input['game_id'].unique()\n        sampled_game_ids = pd.Series(unique_game_ids).sample(frac=debug_fraction, random_state=42).values\n        train_input = train_input[train_input['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n        train_output = train_output[train_output['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n        print(f\"Using {len(train_input):,} input records from {len(sampled_game_ids)} games for debugging\")\n    \n    return train_input, train_output, test_input, test_template","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.50953Z","iopub.status.busy":"2025-10-04T05:02:52.509325Z","iopub.status.idle":"2025-10-04T05:02:52.515871Z","shell.execute_reply":"2025-10-04T05:02:52.515239Z"},"papermill":{"duration":0.013756,"end_time":"2025-10-04T05:02:52.516857","exception":false,"start_time":"2025-10-04T05:02:52.503101","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metric","metadata":{"papermill":{"duration":0.005294,"end_time":"2025-10-04T05:02:52.528287","exception":false,"start_time":"2025-10-04T05:02:52.522993","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute RMSE for NFL competition.\n    Expected input:\n      - solution and submission as pandas.DataFrame\n      - Column 'id': unique identifier for each (game_id, play_id, nfl_id, frame_id)\n      - Column 'x'\n      - Column 'y'\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> row_id_column_name = 'id'\n    >>> solution = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,3], 'y':[4,2,3]})\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1.1,2,3], 'y':[4,2.2,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    0.0913\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [0,2,3], 'y':[4,2.2,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    0.4163\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,1], 'y':[4,0,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    1.1547\n    \"\"\"\n\n    TARGET = ['x', 'y']\n    if row_id_column_name not in solution.columns:\n        raise ParticipantVisibleError(f\"Solution file missing required column: '{row_id_column_name}'\")\n    if row_id_column_name not in submission.columns:\n        raise ParticipantVisibleError(f\"Submission file missing required column: '{row_id_column_name}'\")\n\n    missing_in_solution = set(TARGET) - set(solution.columns)\n    missing_in_submission = set(TARGET) - set(submission.columns)\n\n    if missing_in_solution:\n        raise ParticipantVisibleError(f'Solution file missing required columns: {missing_in_solution}')\n    if missing_in_submission:\n        raise ParticipantVisibleError(f'Submission file missing required columns: {missing_in_submission}')\n\n    submission = submission[['id'] + TARGET]\n    merged_df = pd.merge(solution, submission, on=row_id_column_name, suffixes=('_true', '_pred'))\n    #log NaN\n    nanx_in_pred = merged_df['x_pred'].isna().sum()\n    nany_in_pred = merged_df['y_pred'].isna().sum()\n    if nanx_in_pred > 0:\n        print(f\"WARNING: Found {nanx_in_pred} NaN predictions in merged results\")\n    if nany_in_pred > 0:\n        print(f\"WARNING: Found {nany_in_pred} NaN predictions in merged results\")\n    nanx_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['x_true'].isna().sum()\n    nany_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['y_true'].isna().sum()\n    if nanx_in_true > 0:\n        print(f\"WARNING: Found {nanx_in_true} NaN true values corresponding to NaN predictions\")\n    if nany_in_true > 0:\n        print(f\"WARNING: Found {nany_in_true} NaN true values corresponding to NaN predictions\")\n    rmse = np.sqrt(\n        0.5 * (mean_squared_error(merged_df['x_true'], merged_df['x_pred']) + mean_squared_error(merged_df['y_true'], merged_df['y_pred']))\n    )\n    return float(rmse)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.54023Z","iopub.status.busy":"2025-10-04T05:02:52.540019Z","iopub.status.idle":"2025-10-04T05:02:52.547324Z","shell.execute_reply":"2025-10-04T05:02:52.546659Z"},"papermill":{"duration":0.014538,"end_time":"2025-10-04T05:02:52.548341","exception":false,"start_time":"2025-10-04T05:02:52.533803","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare features ","metadata":{"papermill":{"duration":0.005315,"end_time":"2025-10-04T05:02:52.559043","exception":false,"start_time":"2025-10-04T05:02:52.553728","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ...existing code...\nfrom collections import defaultdict\n\ndef _compute_interactions_for_play_frames(df_play_frames: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Compute interaction features per (game_id, play_id, frame_id, nfl_id) for selected players:\n      - distance_to_player_mean/min/max_offense/defense\n      - relative_velocity_magnitude_mean/min/max_offense/defense\n      - angle_to_player_mean/min/max_offense/defense  (mean is circular)\n      - nearest_opponent_dist/angle/rel_speed\n\n    Only computes (emits rows) where 'player_to_predict' is True if that column exists.\n    Otherwise computes for all players.\n\n    df_play_frames contains rows for a single (game_id, play_id), multiple frame_ids and all players in those frames.\n    \"\"\"\n    out_rows = []\n    # Per frame to keep matrices tiny\n    for (g, p, f), grp in df_play_frames.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n        n = len(grp)\n        if n == 0:\n            continue\n\n        nfl_ids = grp['nfl_id'].to_numpy()\n        x  = grp['x'].to_numpy(dtype=np.float32)\n        y  = grp['y'].to_numpy(dtype=np.float32)\n        vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n        vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n        is_off = grp['is_offense'].to_numpy().astype(bool)\n\n        compute_mask = grp['player_to_predict'].to_numpy().astype(bool) if 'player_to_predict' in grp.columns else np.ones(n, dtype=bool)\n\n        # Pairwise geometry\n        dx = x[None, :] - x[:, None]\n        dy = y[None, :] - y[:, None]\n        dist = np.sqrt(dx * dx + dy * dy)                  # (n,n)\n        angle_mat = np.arctan2(-dy, -dx)                   # angle i->j (y_j - y_i, x_j - x_i)\n        dvx = vx[:, None] - vx[None, :]\n        dvy = vy[:, None] - vy[None, :]\n        rel_speed = np.sqrt(dvx * dvx + dvy * dvy)         # (n,n)\n\n        # Masks\n        opp_mask = (is_off[:, None] != is_off[None, :])    # opponent pairs\n        np.fill_diagonal(opp_mask, False)\n\n        mask_off = np.broadcast_to(is_off[None, :], (n, n)).copy()\n        mask_def = np.broadcast_to(~is_off[None, :], (n, n)).copy()\n        np.fill_diagonal(mask_off, False)\n        np.fill_diagonal(mask_def, False)\n\n        # Nearest opponent\n        dist_opp = np.where(opp_mask, dist, np.nan)\n        nearest_dist = np.nanmin(dist_opp, axis=1)\n        nearest_idx = np.nanargmin(dist_opp, axis=1)\n        all_nan = ~np.isfinite(nearest_dist)\n        nearest_idx_safe = nearest_idx.copy()\n        nearest_idx_safe[all_nan] = 0\n        nearest_angle = np.take_along_axis(angle_mat, nearest_idx_safe[:, None], axis=1).squeeze(1)\n        nearest_rel   = np.take_along_axis(rel_speed, nearest_idx_safe[:, None], axis=1).squeeze(1)\n        nearest_angle[all_nan] = np.nan\n        nearest_rel[all_nan]   = np.nan\n\n        # Group-wise aggregations\n        # Distances\n        d_off = np.where(mask_off, dist, np.nan)\n        d_def = np.where(mask_def, dist, np.nan)\n        d_mean_o = np.nanmean(d_off, axis=1); d_min_o = np.nanmin(d_off, axis=1); d_max_o = np.nanmax(d_off, axis=1)\n        d_mean_d = np.nanmean(d_def, axis=1); d_min_d = np.nanmin(d_def, axis=1); d_max_d = np.nanmax(d_def, axis=1)\n\n        # Relative speed\n        v_off = np.where(mask_off, rel_speed, np.nan)\n        v_def = np.where(mask_def, rel_speed, np.nan)\n        v_mean_o = np.nanmean(v_off, axis=1); v_min_o = np.nanmin(v_off, axis=1); v_max_o = np.nanmax(v_off, axis=1)\n        v_mean_d = np.nanmean(v_def, axis=1); v_min_d = np.nanmin(v_def, axis=1); v_max_d = np.nanmax(v_def, axis=1)\n\n        # Angles: circular mean for mean, raw min/max for spread reference\n        sinA = np.sin(angle_mat); cosA = np.cos(angle_mat)\n\n        cnt_off = mask_off.sum(axis=1).astype(np.float32)\n        cnt_def = mask_def.sum(axis=1).astype(np.float32)\n\n        # Avoid divide-by-zero; set denom=nan where no neighbors\n        denom_off = np.where(cnt_off > 0, cnt_off, np.nan)\n        denom_def = np.where(cnt_def > 0, cnt_def, np.nan)\n\n        sin_sum_off = (sinA * mask_off).sum(axis=1)\n        cos_sum_off = (cosA * mask_off).sum(axis=1)\n        sin_sum_def = (sinA * mask_def).sum(axis=1)\n        cos_sum_def = (cosA * mask_def).sum(axis=1)\n\n        a_mean_o = np.arctan2(sin_sum_off / denom_off, cos_sum_off / denom_off)\n        a_mean_d = np.arctan2(sin_sum_def / denom_def, cos_sum_def / denom_def)\n\n        a_off = np.where(mask_off, angle_mat, np.nan)\n        a_def = np.where(mask_def, angle_mat, np.nan)\n        a_min_o = np.nanmin(a_off, axis=1); a_max_o = np.nanmax(a_off, axis=1)\n        a_min_d = np.nanmin(a_def, axis=1); a_max_d = np.nanmax(a_def, axis=1)\n\n        # Emit only for players to predict\n        for idx, nid in enumerate(nfl_ids):\n            if not compute_mask[idx]:\n                continue\n            out_rows.append({\n                'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': int(nid),\n\n                'distance_to_player_mean_offense': d_mean_o[idx],\n                'distance_to_player_min_offense': d_min_o[idx],\n                'distance_to_player_max_offense': d_max_o[idx],\n                'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n                'relative_velocity_magnitude_min_offense': v_min_o[idx],\n                'relative_velocity_magnitude_max_offense': v_max_o[idx],\n                'angle_to_player_mean_offense': a_mean_o[idx],\n                'angle_to_player_min_offense': a_min_o[idx],\n                'angle_to_player_max_offense': a_max_o[idx],\n\n                'distance_to_player_mean_defense': d_mean_d[idx],\n                'distance_to_player_min_defense': d_min_d[idx],\n                'distance_to_player_max_defense': d_max_d[idx],\n                'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n                'relative_velocity_magnitude_min_defense': v_min_d[idx],\n                'relative_velocity_magnitude_max_defense': v_max_d[idx],\n                'angle_to_player_mean_defense': a_mean_d[idx],\n                'angle_to_player_min_defense': a_min_d[idx],\n                'angle_to_player_max_defense': a_max_d[idx],\n\n                'nearest_opponent_dist': float(nearest_dist[idx]) if np.isfinite(nearest_dist[idx]) else np.nan,\n                'nearest_opponent_angle': float(nearest_angle[idx]) if np.isfinite(nearest_angle[idx]) else np.nan,\n                'nearest_opponent_rel_speed': float(nearest_rel[idx]) if np.isfinite(nearest_rel[idx]) else np.nan,\n            })\n\n    return pd.DataFrame(out_rows, columns=[\n        'game_id', 'play_id', 'frame_id', 'nfl_id',\n        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed'\n    ])\n# ...existing code...","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def height_to_feet(height_str):\n    \"\"\"Convert height from 'ft-in' format to feet\"\"\"\n    try:\n        ft, inches = map(int, height_str.split('-'))\n        return ft + inches/12\n    except:\n        return None\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.570669Z","iopub.status.busy":"2025-10-04T05:02:52.570449Z","iopub.status.idle":"2025-10-04T05:02:52.573929Z","shell.execute_reply":"2025-10-04T05:02:52.573288Z"},"papermill":{"duration":0.010469,"end_time":"2025-10-04T05:02:52.574938","exception":false,"start_time":"2025-10-04T05:02:52.564469","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_advanced_features(df):\n    \"\"\"\n    STEP 2: Add 30-40 advanced features\n    These are proven to improve performance\n    \"\"\"\n    print(\"Adding advanced features...\")\n    df = df.copy()\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # ==========================================\n    # GROUP 1: Distance Rate Features (3)\n    # ==========================================\n    if 'distance_to_ball' in df.columns:\n        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n        df['time_to_intercept'] = (df['distance_to_ball'] / \n                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n    \n    # ==========================================\n    # GROUP 2: Target Alignment Features (3)\n    # ==========================================\n    if 'ball_direction_x' in df.columns:\n        df['velocity_alignment'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_perpendicular'] = (\n            df['velocity_x'] * (-df['ball_direction_y']) +\n            df['velocity_y'] * df['ball_direction_x']\n        )\n        if 'acceleration_x' in df.columns:\n            df['accel_alignment'] = (\n                df['acceleration_x'] * df['ball_direction_x'] +\n                df['acceleration_y'] * df['ball_direction_y']\n            )\n    \n    # ==========================================\n    # GROUP 3: Multi-Window Rolling (24)\n    # ==========================================\n    for window in [3, 5, 10]:\n        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).mean()\n                )\n                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).std()\n                ).fillna(0)\n    \n    # ==========================================\n    # GROUP 4: Extended Lag Features (8)\n    # ==========================================\n    for lag in [4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n    \n    # ==========================================\n    # GROUP 5: Velocity Change Features (4)\n    # ==========================================\n    if 'velocity_x' in df.columns:\n        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n        df['direction_change'] = df['direction_change'].apply(\n            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        )\n    \n    # ==========================================\n    # GROUP 6: Field Position Features (4)\n    # ==========================================\n    df['dist_from_left'] = df['y']\n    df['dist_from_right'] = 53.3 - df['y']\n    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n    \n    # ==========================================\n    # GROUP 7: Role-Specific Features (3)\n    # ==========================================\n    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n    \n    # ==========================================\n    # GROUP 8: Time Features (2)\n    # ==========================================\n    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n        lambda x: x / (x.max() + 1)\n    )\n    \n    print(f\"Total features after enhancement: {len(df.columns)}\")\n    \n    return df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_sequences(input_df, output_df=None, test_template=None, \n                                            is_training=True, window_size=8,use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n    \"\"\"\n    Prepare sequences with ALL advanced features\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES\")\n    print(f\"{'='*80}\")\n    print(f\"Window size: {window_size}\")\n    \n    input_df = input_df.copy()\n    \n    # ==========================================\n    # BASIC FEATURES\n    # ==========================================\n    print(\"Step 1/4: Adding basic features...\")\n    \n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n    \n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    delta_t = 0.1\n    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n    input_df['o_sin'] = np.sin(np.deg2rad(input_df['o'].fillna(0)))\n    input_df['o_cos'] = np.cos(np.deg2rad(input_df['o'].fillna(0)))\n    input_df['dir_sin'] = np.sin(np.deg2rad(input_df['dir'].fillna(0)))\n    input_df['dir_cos'] = np.cos(np.deg2rad(input_df['dir'].fillna(0)))\n    # Roles\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    \n    # Physics\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n    \n    # Ball features\n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n    \n    # Sort for temporal\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # Original lag features (1-3)\n    for lag in [1, 2, 3]:\n        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n    \n    # EMA features\n    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n    )\n    \n    # ==========================================\n    # ADVANCED FEATURES (NEW!)\n    # ==========================================\n    print(\"Step 2/4: Adding advanced features...\")\n    input_df = add_advanced_features(input_df)\n    # ==========================================\n    # PLAYER INTERACTION FEATURES (NEW!)\n    # ==========================================\n    print(\"Step 3/4: Adding player interaction features...\")\n    if use_players_interactions:\n        agg_rows = []\n        # Group once (avoid overhead of apply per small group)\n        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n            n = len(grp)\n            nfl_ids = grp['nfl_id'].to_numpy()\n            # Only compute/emit for player_to_predict==True (if column exists)\n            compute_mask = grp['player_to_predict'].to_numpy().astype(bool) if 'player_to_predict' in grp.columns else np.ones(n, dtype=bool)\n            if n < 2:\n                # Create empty stats rows (NaNs) only for players to predict\n                for nid in nfl_ids[compute_mask]:\n                    agg_rows.append({\n                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n                        'distance_to_player_mean_offense': np.nan,\n                        'distance_to_player_min_offense': np.nan,\n                        'distance_to_player_max_offense': np.nan,\n                        'relative_velocity_magnitude_mean_offense': np.nan,\n                        'relative_velocity_magnitude_min_offense': np.nan,\n                        'relative_velocity_magnitude_max_offense': np.nan,\n                        'angle_to_player_mean_offense': np.nan,\n                        'angle_to_player_min_offense': np.nan,\n                        'angle_to_player_max_offense': np.nan,\n                        'distance_to_player_mean_defense': np.nan,\n                        'distance_to_player_min_defense': np.nan,\n                        'distance_to_player_max_defense': np.nan,\n                        'relative_velocity_magnitude_mean_defense': np.nan,\n                        'relative_velocity_magnitude_min_defense': np.nan,\n                        'relative_velocity_magnitude_max_defense': np.nan,\n                        'angle_to_player_mean_defense': np.nan,\n                        'angle_to_player_min_defense': np.nan,\n                        'angle_to_player_max_defense': np.nan,\n                        'nearest_opponent_dist': np.nan,\n                        'nearest_opponent_angle': np.nan,\n                        'nearest_opponent_rel_speed': np.nan,\n                    })\n                continue\n\n            x = grp['x'].to_numpy(dtype=np.float32)\n            y = grp['y'].to_numpy(dtype=np.float32)\n            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n            is_offense = grp['is_offense'].to_numpy()\n            is_defense = grp['is_defense'].to_numpy()\n\n            # Pairwise deltas (broadcast)\n            dx = x[None, :] - x[:, None]        # (n,n) x_j - x_i reversed later for angle\n            dy = y[None, :] - y[:, None]\n            # Angle from i -> j (want y_j - y_i, x_j - x_i)\n            angle_mat = np.arctan2(-dy, -dx)    # because dx currently x[None]-x[:,None] => -(x_j - x_i)\n\n            # Distances\n            dist = np.sqrt(dx ** 2 + dy ** 2)\n            # Relative velocity magnitudes\n            dvx = vx[:, None] - vx[None, :]\n            dvy = vy[:, None] - vy[None, :]\n            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n\n            # Offense mask (exclude self)\n            offense_mask = (is_offense[:, None] == is_offense[None, :])\n            offense_mask = (is_offense[:, None] == is_offense[None, :])\n            np.fill_diagonal(offense_mask, False)\n\n            # Defense mask (exclude self)\n            defense_mask = (is_defense[:, None] == is_defense[None, :])\n            np.fill_diagonal(defense_mask, False)\n\n            # Opponent mask (exclude self)\n            opp_mask = (is_offense[:, None] != is_offense[None, :])\n            np.fill_diagonal(opp_mask, False)\n\n            # Mask out self distances\n            dist_diag_nan = dist.copy()\n            np.fill_diagonal(dist_diag_nan, np.nan)\n            rel_diag_nan = rel_speed.copy()\n            np.fill_diagonal(rel_diag_nan, np.nan)\n            angle_diag_nan = angle_mat.copy()\n            np.fill_diagonal(angle_diag_nan, np.nan)\n\n            def masked_stats(mat, mask):\n                # mat, mask shape (n,n)\n                masked = np.where(mask, mat, np.nan)\n                cnt = mask.sum(axis=1)\n                mean = np.nanmean(masked, axis=1)\n                amin = np.nanmin(masked, axis=1)\n                amax = np.nanmax(masked, axis=1)\n                # Rows with zero valid -> set nan\n                zero = cnt == 0\n                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n                return mean, amin, amax\n\n            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n\n            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n\n            # NEW: nearest opponent stats\n            masked_dist_opp = np.where(opp_mask, dist_diag_nan, np.nan)         # (n,n)\n            nearest_dist = np.nanmin(masked_dist_opp, axis=1)                   # (n,)\n            nearest_idx = np.nanargmin(masked_dist_opp, axis=1)                 # (n,)\n            # Guard where all-NaN rows (no opponents)\n            all_nan = ~np.isfinite(nearest_dist)\n            nearest_idx_safe = nearest_idx.copy()\n            nearest_idx_safe[all_nan] = 0\n            nearest_angle = np.take_along_axis(angle_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n            nearest_rel = np.take_along_axis(rel_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n            nearest_angle[all_nan] = np.nan\n            nearest_rel[all_nan] = np.nan\n\n            for idx, nid in enumerate(nfl_ids):\n                if not compute_mask[idx]:\n                    continue  # only for player_to_predict==True\n                agg_rows.append({\n                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n                    'distance_to_player_mean_offense': d_mean_o[idx],\n                    'distance_to_player_min_offense': d_min_o[idx],\n                    'distance_to_player_max_offense': d_max_o[idx],\n                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n                    'relative_velocity_magnitude_min_offense': v_min_o[idx],\n                    'relative_velocity_magnitude_max_offense': v_max_o[idx],\n                    'angle_to_player_mean_offense': a_mean_o[idx],\n                    'angle_to_player_min_offense': a_min_o[idx],\n                    'angle_to_player_max_offense': a_max_o[idx],\n                    'distance_to_player_mean_defense': d_mean_d[idx],\n                    'distance_to_player_min_defense': d_min_d[idx],\n                    'distance_to_player_max_defense': d_max_d[idx],\n                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n                    'relative_velocity_magnitude_min_defense': v_min_d[idx],\n                    'relative_velocity_magnitude_max_defense': v_max_d[idx],\n                    'angle_to_player_mean_defense': a_mean_d[idx],\n                    'angle_to_player_min_defense': a_min_d[idx],\n                    'angle_to_player_max_defense': a_max_d[idx],\n                    'nearest_opponent_dist': nearest_dist[idx],\n                    'nearest_opponent_angle': nearest_angle[idx],\n                    'nearest_opponent_rel_speed': nearest_rel[idx],\n                })\n\n        interaction_agg = pd.DataFrame(agg_rows)\n        input_df = input_df.merge(\n            interaction_agg,\n            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n            how='left'\n        )\n    else:\n        print(\"Skipping fast interaction feature computation (use_fast_interactions=False).\")\n    # ==========================================\n    # FEATURE LIST (ENHANCED)\n    # ==========================================\n    print(\"Step 4/4: Creating sequences...\")\n    \n    feature_cols = [\n        # Core (6)\n        'x', 'y', 's', 'a', 'ball_land_x', 'ball_land_y',\n\n        # Angles encoded (4)\n        'o_sin', 'o_cos', 'dir_sin', 'dir_cos',\n\n        # Player (2)\n        'player_height_feet', 'player_weight',\n        \n        # Motion (6)\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'momentum_x', 'momentum_y', 'kinetic_energy',\n        \n        # Roles (5)\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        \n        # Ball (5)\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n        \n        # Original temporal (15)\n        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n        \n        # NEW: Distance rate (3)\n        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n        \n        # NEW: Target alignment (3)\n        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n        \n        # NEW: Multi-window rolling (24)\n        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n        \n        # NEW: Extended lags (8)\n        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n        \n        # NEW: Velocity changes (4)\n        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n        \n        # NEW: Field position (4)\n        'dist_from_sideline', 'dist_from_endzone',\n        \n        # NEW: Role-specific (3)\n        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n        \n        # NEW: Time (2)\n        'frames_elapsed', 'normalized_time',\n        # New: Player interaction (21)\n        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed',\n    ]\n    \n    # Filter to existing\n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"Using {len(feature_cols)}\")\n    \n    # ==========================================\n    # CREATE SEQUENCES\n    # ==========================================\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    \n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n    \n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n    \n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        \n        input_window = group_df.tail(window_size)\n        \n        if len(input_window) < window_size:\n            # if is_training:\n            #     print(f\"Skipping sequence with insufficient history for {key}\")\n            #     continue\n            print(f\"Padding sequence with insufficient history for {key}\")\n            pad_len = window_size - len(input_window)\n            first = input_window.iloc[0:1].copy()\n            pad_df = pd.concat([first] * pad_len, ignore_index=True)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        \n        # fill within the window only, no future mean usage\n        input_window = input_window.ffill().bfill()\n        input_window = input_window.fillna(0.0)\n\n        seq = input_window[feature_cols].values\n        if np.isnan(seq).any():\n            # print which sequence has NaNs\n            print(f\"NaNs found in sequence for {key}, replacing with 0.0\")\n            # ensure no NaNs go into the model\n            seq = np.nan_to_num(seq, nan=0.0)\n        \n        sequences.append(seq)\n        \n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            \n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n            \n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            \n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n        \n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id']\n        })\n    \n    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n    \n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n    return sequences, sequence_ids, feature_cols","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.586729Z","iopub.status.busy":"2025-10-04T05:02:52.586529Z","iopub.status.idle":"2025-10-04T05:02:52.611132Z","shell.execute_reply":"2025-10-04T05:02:52.610641Z"},"papermill":{"duration":0.031738,"end_time":"2025-10-04T05:02:52.612141","exception":false,"start_time":"2025-10-04T05:02:52.580403","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading and preparing data...\")\ntrain_input, train_output, test_input, test_template = load_data(debug_fraction=1.0)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.623843Z","iopub.status.busy":"2025-10-04T05:02:52.623639Z","iopub.status.idle":"2025-10-04T05:03:10.835804Z","shell.execute_reply":"2025-10-04T05:03:10.835032Z"},"papermill":{"duration":18.219306,"end_time":"2025-10-04T05:03:10.836955","exception":false,"start_time":"2025-10-04T05:02:52.617649","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model class","metadata":{}},{"cell_type":"code","source":"class GaussianNoise(nn.Module):\n    \"\"\"Add Gaussian noise to input tensor\"\"\"\n    def __init__(self, stddev):\n        super().__init__()\n        self.stddev = stddev\n    \n    def forward(self, x):\n        if self.training:\n            noise = torch.randn_like(x) * self.stddev\n            return x + noise\n        return x","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sequence","metadata":{}},{"cell_type":"code","source":"class SeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n        self.pool_ln = nn.LayerNorm(256)\n        self.pool_attn = nn.MultiheadAttention(256, num_heads=4, batch_first=True)\n        self.pool_query = nn.Parameter(torch.randn(1, 1, 256))\n        self.head = nn.Sequential(\n            nn.Linear(256, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, horizon)\n        )\n    \n    def forward(self, x):\n        h, _ = self.gru(x)\n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)\n        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n        out = self.head(ctx.squeeze(1))\n        return torch.cumsum(out, dim=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission maker","metadata":{"papermill":{"duration":0.057921,"end_time":"2025-10-04T05:11:55.279849","exception":false,"start_time":"2025-10-04T05:11:55.221928","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ...existing code...\ndef build_axis_model_from_config(cfg):\n    \"\"\"\n    Instantiate the SeqModel from a saved config.\n    \"\"\"\n    input_dim = cfg['input_dim']\n    horizon = cfg['horizon']\n    return SeqModel(input_dim=input_dim, horizon=horizon)\n\n\ndef _model_tag_from_instance(model):\n    if isinstance(model, SeqModel):\n        return 'seq'\n    return model.__class__.__name__.lower()\n\n\ndef create_model_save_config(model, input_dim, horizon):\n    \"\"\"\n    Build a minimal config for re-instantiation of SeqModel.\n    \"\"\"\n    return {\n        'model': 'seq',\n        'input_dim': int(input_dim),\n        'horizon': int(horizon),\n    }\n\n\ndef save_axis_checkpoint(model, cfg, fold_dir, axis_name='x'):\n    \"\"\"\n    Save checkpoint for SeqModel.\n    \"\"\"\n    cfg = dict(cfg or {})\n    cfg['model'] = 'seq'\n    path = Path(fold_dir) / f'axis_{axis_name}.pt'\n    torch.save({'state_dict': model.state_dict(), 'config': cfg}, str(path))\n\n\ndef load_axis_checkpoint(fold_dir, axis_name='x', device=None):\n    \"\"\"\n    Load SeqModel checkpoint.\n    \"\"\"\n    device = device or Config.DEVICE\n    ckpt_path = Path(fold_dir) / f'axis_{axis_name}.pt'\n    ckpt = torch.load(str(ckpt_path), map_location=device)\n    cfg = ckpt['config']\n    state_dict = ckpt['state_dict']\n\n    try:\n        model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n        model.load_state_dict(state_dict, strict=True)\n        model.eval()\n        return model, cfg\n    except Exception as e:\n        print(f\"[{axis_name}] SeqModel load failed ({e}); retrying with strict=False.\")\n\n    # Last resort: best-effort non-strict load\n    model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n    if missing or unexpected:\n        print(f\"[{axis_name}] Warning: missing={len(missing)}, unexpected={len(unexpected)} when loading non-strict.\")\n    model.eval()\n    return model, cfg\n\n\ndef load_folds_xy(num_folds, models_dir=None, device=None):\n    \"\"\"\n    Load SeqModel checkpoints for all folds.\n    \"\"\"\n    device = device or Config.DEVICE\n    base = Path(models_dir) if models_dir else Path('.')\n    models_x, models_y, scalers, cfgs = [], [], [], []\n    for fold in range(1, num_folds + 1):\n        fold_dir = base / f'fold_{fold}'\n        try:\n            mx, cfgx = load_axis_checkpoint(fold_dir, 'x', device=device)\n            my, cfgy = load_axis_checkpoint(fold_dir, 'y', device=device)\n            scaler = joblib.load(str(fold_dir / 'lstm_feature_scaler_fold.joblib'))\n            models_x.append(mx)\n            models_y.append(my)\n            scalers.append(scaler)\n            cfgs.append(cfgx)\n            print(f'Loaded fold {fold} OK')\n        except Exception as e:\n            print(f'Fold {fold} load failed: {e}')\n    return models_x, models_y, scalers, cfgs\n# ...existing code...","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_ensemble_predictions_xy(\n    models_x, models_y, scalers, X_test_unscaled, test_seq_ids, test_template, batch_size=1024\n):\n    \"\"\"\n    Ensemble test-time predictions using separate axis models (dx and dy) across folds.\n    - models_x, models_y: lists of FlexibleSeqModel (same length, one per fold)\n    - scalers: list of StandardScaler, aligned with models (or None entries)\n    - X_test_unscaled: list/array of (T,F) sequences (unscaled)\n    - test_seq_ids: list of dicts with keys [game_id, play_id, nfl_id, frame_id(last)]\n    - test_template: DataFrame with required submission rows\n\n    Returns: DataFrame with columns [id, x, y]\n    \"\"\"\n    if len(models_x) == 0 or len(models_x) != len(models_y):\n        print(\"No axis models or mismatched model counts.\")\n        return None\n    if scalers is not None and len(scalers) != len(models_x):\n        raise ValueError(\"Length of scalers must match number of folds (or be None).\")\n\n    # Convert sequences to array of objects for robust handling\n    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n    N = len(X_test_unscaled)\n\n    # Last observed absolute positions from the sequences (assumes feat[0]=x, feat[1]=y)\n    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n\n    # Per-fold cumulative displacement predictions\n    per_fold_dx = []\n    per_fold_dy = []\n\n    for i in range(len(models_x)):\n        model_x = models_x[i]\n        model_y = models_y[i]\n        scaler = scalers[i] if scalers is not None else None\n\n        # Scale per sequence for this fold\n        if scaler is not None:\n            scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n        else:\n            scaled = X_test_unscaled\n\n        # Stack to (N,T,F)\n        X = np.stack(scaled.astype(np.float32))\n        device = next(model_x.parameters()).device\n        ds = TensorDataset(torch.from_numpy(X))\n        dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n\n        dx_list, dy_list = [], []\n        model_x.eval(); model_y.eval()\n        with torch.no_grad():\n            for (batch,) in dl:\n                batch = batch.to(device)    # (B,T,F)\n                dx = model_x(batch)         # (B,H)\n                dy = model_y(batch)         # (B,H)\n                dx_list.append(dx.cpu().numpy())\n                dy_list.append(dy.cpu().numpy())\n        dx_cum = np.vstack(dx_list)  # (N,H)\n        dy_cum = np.vstack(dy_list)  # (N,H)\n\n        per_fold_dx.append(dx_cum)\n        per_fold_dy.append(dy_cum)\n\n    # Ensemble by mean across folds\n    ens_dx = np.mean(np.stack(per_fold_dx, axis=0), axis=0)  # (N,H)\n    ens_dy = np.mean(np.stack(per_fold_dy, axis=0), axis=0)  # (N,H)\n\n    # Create submission rows by mapping to test_template frame order per (game,play,nfl)\n    test_meta = pd.DataFrame(test_seq_ids)\n    out_rows = []\n    H = ens_dx.shape[1]\n    for i, seq_info in test_meta.iterrows():\n        game_id = int(seq_info['game_id'])\n        play_id = int(seq_info['play_id'])\n        nfl_id = int(seq_info['nfl_id'])\n\n        frame_ids = (\n            test_template[\n                (test_template['game_id'] == game_id) &\n                (test_template['play_id'] == play_id) &\n                (test_template['nfl_id'] == nfl_id)\n            ]['frame_id'].sort_values().tolist()\n        )\n        for t, frame_id in enumerate(frame_ids):\n            tt = t if t < H else H - 1\n            px = np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n            py = np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            out_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n                'x': px,\n                'y': py\n            })\n    submission = pd.DataFrame(out_rows)\n    return submission\n# ...existing code...","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:55.525945Z","iopub.status.busy":"2025-10-04T05:11:55.525632Z","iopub.status.idle":"2025-10-04T05:11:55.537866Z","shell.execute_reply":"2025-10-04T05:11:55.537077Z"},"papermill":{"duration":0.077056,"end_time":"2025-10-04T05:11:55.539154","exception":false,"start_time":"2025-10-04T05:11:55.462098","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_ensemble_val_predictions(models, scalers, X_val_unscaled, val_ids, y_val_dx_fold, y_val_dy_fold, val_data, exclude_fold=None):\n    \"\"\"\n    Generate ensemble predictions for validation data and prepare for scoring.\n    Excludes the model from the same fold to prevent potential overfitting/leakage.\n    \n    Args:\n        models: List of trained models\n        scalers: List of scalers (one per model)\n        X_val_unscaled: Validation sequences (unscaled)\n        val_ids: List of dicts with sequence metadata\n        y_val_dx_fold, y_val_dy_fold: Ground truth displacements\n        val_data: DataFrame with x_last, y_last\n        exclude_fold: Index of the fold to exclude (0-based)\n    \n    Returns:\n        ensemble_pred_df, ensemble_true_df: DataFrames for scoring\n    \"\"\"\n    pred_rows = []\n    true_rows = []\n    \n    for i, seq_info in enumerate(val_ids):\n        game_id = seq_info['game_id']\n        play_id = seq_info['play_id']\n        nfl_id = seq_info['nfl_id']\n        x_last = val_data.iloc[i]['x_last']\n        y_last = val_data.iloc[i]['y_last']\n        \n        # Ground truth\n        dx_true = y_val_dx_fold[i]\n        dy_true = y_val_dy_fold[i]\n        for t in range(len(dx_true)):\n            frame_rel = t + 1\n            true_x = x_last + dx_true[t]\n            true_y = y_last + dy_true[t]\n            true_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n                'x': true_x,\n                'y': true_y\n            })\n        \n        # Ensemble predictions (exclude the model from the same fold)\n        per_model_dx = []\n        per_model_dy = []\n        for j, model in enumerate(models):\n            if exclude_fold is not None and j == exclude_fold:\n                continue  # Skip the model trained on this fold\n            scaler = scalers[j]\n            scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n            scaled_seq = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n            model.eval()\n            with torch.no_grad():\n                output = model(scaled_seq).cpu().numpy()[0]  # (max_frames_output, 2)\n            per_model_dx.append(output[:, 0])\n            per_model_dy.append(output[:, 1])\n        \n        # Average across remaining models\n        if per_model_dx:  # Ensure there are models to average\n            ens_dx = np.mean(per_model_dx, axis=0)\n            ens_dy = np.mean(per_model_dy, axis=0)\n        else:\n            # Fallback: use the last known position (though this shouldn't happen with n_folds > 1)\n            ens_dx = np.zeros(len(dx_true))\n            ens_dy = np.zeros(len(dy_true))\n        \n        # Generate predictions for each frame\n        for t in range(len(dx_true)):\n            pred_x = x_last + ens_dx[t]\n            pred_y = y_last + ens_dy[t]\n            pred_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{t+1}\",\n                'x': np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n                'y': np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            })\n    \n    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:55.669945Z","iopub.status.busy":"2025-10-04T05:11:55.669676Z","iopub.status.idle":"2025-10-04T05:11:55.678495Z","shell.execute_reply":"2025-10-04T05:11:55.677798Z"},"papermill":{"duration":0.073178,"end_time":"2025-10-04T05:11:55.679652","exception":false,"start_time":"2025-10-04T05:11:55.606474","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Infer","metadata":{"papermill":{"duration":0.113745,"end_time":"2025-10-04T05:34:40.640903","exception":false,"start_time":"2025-10-04T05:34:40.527158","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nprint(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\nmodels_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build test sequences\ntest_sequences, test_seq_ids,feature_cols = prepare_sequences(\n    test_input, test_template=test_template, is_training=False, window_size=Config.WINDOW_SIZE\n)\nprint(f\"Prepared {len(test_sequences)} test sequences with shape: {test_sequences[0].shape}.\")\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:34:46.212777Z","iopub.status.busy":"2025-10-04T05:34:46.212514Z","iopub.status.idle":"2025-10-04T05:34:46.819393Z","shell.execute_reply":"2025-10-04T05:34:46.818626Z"},"papermill":{"duration":0.721145,"end_time":"2025-10-04T05:34:46.82054","exception":false,"start_time":"2025-10-04T05:34:46.099395","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the trained per-fold axis models\nsubmission_xy = create_ensemble_predictions_xy(\n    models_x=models_x_nn,\n    models_y=models_y_nn,\n    scalers=scalers,\n    X_test_unscaled=test_sequences,\n    test_seq_ids=test_seq_ids,\n    test_template=test_template,\n    batch_size=1024\n)\nsubmission_xy.to_csv('submission.csv', index=False)\nprint(\"Saved submission.csv\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_xy","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:34:47.054131Z","iopub.status.busy":"2025-10-04T05:34:47.053782Z","iopub.status.idle":"2025-10-04T05:34:47.070518Z","shell.execute_reply":"2025-10-04T05:34:47.069976Z"},"papermill":{"duration":0.134959,"end_time":"2025-10-04T05:34:47.071503","exception":false,"start_time":"2025-10-04T05:34:46.936544","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}