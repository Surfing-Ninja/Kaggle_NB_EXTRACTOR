{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13302452,"sourceType":"datasetVersion","datasetId":8431728}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1926.412062,"end_time":"2025-10-04T05:34:49.10692","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-04T05:02:42.694858","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I am back with another neural network, applying what Iâ€™ve learned from the last attempt:\nhttps://www.kaggle.com/code/llkh0a/nfl-big-data-bowl-2026-lstm\n\n- Huber loss is better than RMSE.\n- Predicting x and y separately is better than predicting the tuple (x, y) from the same model.\n- LSTM is good, and so is GRU.\n- Adding player interactions improves performance.\n- window_size > 8 might create some issues during submission, but handling it well can significantly help the models.","metadata":{}},{"cell_type":"markdown","source":"The lastest version include Catboost model idea from https://www.kaggle.com/code/hiwe0305/nfl-big-data-baseline/","metadata":{}},{"cell_type":"code","source":"# ================================================================================\n# NFL BIG DATA BOWL 2026 - COMPLETE WORKING SOLUTION\n# Predicting player movement during pass plays with temporal features\n# ================================================================================\nimport torch\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport gc\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nimport joblib\nfrom datetime import datetime\nfrom itertools import combinations\n# Machine Learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom tqdm import tqdm\n# Deep Learning\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport os\nwarnings.filterwarnings('ignore')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{"papermill":{"duration":0.005532,"end_time":"2025-10-04T05:02:52.459331","exception":false,"start_time":"2025-10-04T05:02:52.453799","status":"completed"},"tags":[]}},{"cell_type":"code","source":"BASED_SPECS_ATTEMPT_1 = [\n    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 2, \"dropout\": 0.1, \"repeat\": 1},  # more GRU depth\n    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n]\n\nBASED_SPECS_ATTEMPT_2 = [\n    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 2},          # more Transformer depth\n    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n]\n\nBASED_SPECS_ATTEMPT_3 = [\n    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 2},                # more TCN depth\n]\n\nBASED_SPECS_ATTEMPT_4 = [\n    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n    {\"type\": \"transformer\", \"nhead\": 8, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},          # more attn heads (128 % 8 == 0)\n    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n]\n\nBASED_SPECS_ATTEMPT_5 = [\n    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 2},  # more GRU blocks\n    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    NN_PRETRAIN_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-public/results\"\n    PREPROCESSED_DATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-public/results\"\n    CATBOOST_PRETRAIN_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-public/results/catboost\"\n    BLEND_WEIGHT = 0.45\n    SEED = 42\n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    MAX_SPEED = 12.0\n    N_FOLDS = 5\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # LSTM_DATA_DIR = '/kaggle/input/prepare-lstm'\n    \n    HIDDEN_DIM = 128\n    NUM_LAYERS = 2\n    DROPOUT = 0.3\n    MAX_FUTURE_HORIZON = 94 #unchangable\n\n    PATIENCE = 30\n    EPOCHS = 200\n    DEBUG_FRACTION = 1.0\n    BATCH_SIZE = 256\n    LEARNING_RATE = 1e-3\n    # important parameters\n    #basic\n    BASED_SPECS = [\n        {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \n         \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n        {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \n         \"dropout\": 0.1, \"repeat\": 1},\n        {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \n         \"dropout\": 0.1, \"repeat\": 1},\n    ]\n    # BASED_SPECS = BASED_SPECS_ATTEMPT_1\n    # BASED_SPECS = BASED_SPECS_ATTEMPT_2\n    # BASED_SPECS = BASED_SPECS_ATTEMPT_3\n    # BASED_SPECS = BASED_SPECS_ATTEMPT_4\n    BASED_SPECS = BASED_SPECS_ATTEMPT_5\n    USE_PLAYERS_INTERACTIONS = True\n    WINDOW_SIZE = 8\n\n    # Set to low value if need to debug\n    # EPOCHS = 1\n    # DEBUG_FRACTION = 0.05","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.471428Z","iopub.status.busy":"2025-10-04T05:02:52.471113Z","iopub.status.idle":"2025-10-04T05:02:52.475739Z","shell.execute_reply":"2025-10-04T05:02:52.475061Z"},"papermill":{"duration":0.012016,"end_time":"2025-10-04T05:02:52.476875","exception":false,"start_time":"2025-10-04T05:02:52.464859","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_global_seeds(seed: int = 42):\n    \"\"\"Set seeds for reproducibility.\"\"\"\n    import random, os\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nset_global_seeds(Config.SEED)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.489081Z","iopub.status.busy":"2025-10-04T05:02:52.4887Z","iopub.status.idle":"2025-10-04T05:02:52.496615Z","shell.execute_reply":"2025-10-04T05:02:52.496141Z"},"papermill":{"duration":0.01475,"end_time":"2025-10-04T05:02:52.497525","exception":false,"start_time":"2025-10-04T05:02:52.482775","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ================================================================================\n# DATA LOADING\n# ================================================================================\n\ndef load_data(debug_fraction=1.0):\n    \"\"\"Load all training and test data with an option to use a fraction for debugging.\"\"\"\n    print(\"Loading data...\")\n    \n    # Training data\n    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    # Filter existing files\n    train_input_files = [f for f in train_input_files if f.exists()]\n    train_output_files = [f for f in train_output_files if f.exists()]\n    \n    print(f\"Found {len(train_input_files)} weeks of data\")\n    \n    # Load and concatenate\n    train_input = pd.concat([pd.read_csv(f) for f in tqdm(train_input_files, desc=\"Input\")], ignore_index=True)\n    train_output = pd.concat([pd.read_csv(f) for f in tqdm(train_output_files, desc=\"Output\")], ignore_index=True)\n    \n    # Test data\n    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n    \n    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n    \n    # Use only a fraction of the games for debugging (select entire games)\n    if debug_fraction < 1.0:\n        unique_game_ids = train_input['game_id'].unique()\n        sampled_game_ids = pd.Series(unique_game_ids).sample(frac=debug_fraction, random_state=42).values\n        train_input = train_input[train_input['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n        train_output = train_output[train_output['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n        print(f\"Using {len(train_input):,} input records from {len(sampled_game_ids)} games for debugging\")\n    \n    return train_input, train_output, test_input, test_template\n# ================================================================================","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.50953Z","iopub.status.busy":"2025-10-04T05:02:52.509325Z","iopub.status.idle":"2025-10-04T05:02:52.515871Z","shell.execute_reply":"2025-10-04T05:02:52.515239Z"},"papermill":{"duration":0.013756,"end_time":"2025-10-04T05:02:52.516857","exception":false,"start_time":"2025-10-04T05:02:52.503101","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metric","metadata":{"papermill":{"duration":0.005294,"end_time":"2025-10-04T05:02:52.528287","exception":false,"start_time":"2025-10-04T05:02:52.522993","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute RMSE for NFL competition.\n    Expected input:\n      - solution and submission as pandas.DataFrame\n      - Column 'id': unique identifier for each (game_id, play_id, nfl_id, frame_id)\n      - Column 'x'\n      - Column 'y'\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> row_id_column_name = 'id'\n    >>> solution = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,3], 'y':[4,2,3]})\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1.1,2,3], 'y':[4,2.2,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    0.0913\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [0,2,3], 'y':[4,2.2,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    0.4163\n    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,1], 'y':[4,0,3]})\n    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n    1.1547\n    \"\"\"\n\n    TARGET = ['x', 'y']\n    if row_id_column_name not in solution.columns:\n        raise ParticipantVisibleError(f\"Solution file missing required column: '{row_id_column_name}'\")\n    if row_id_column_name not in submission.columns:\n        raise ParticipantVisibleError(f\"Submission file missing required column: '{row_id_column_name}'\")\n\n    missing_in_solution = set(TARGET) - set(solution.columns)\n    missing_in_submission = set(TARGET) - set(submission.columns)\n\n    if missing_in_solution:\n        raise ParticipantVisibleError(f'Solution file missing required columns: {missing_in_solution}')\n    if missing_in_submission:\n        raise ParticipantVisibleError(f'Submission file missing required columns: {missing_in_submission}')\n\n    submission = submission[['id'] + TARGET]\n    merged_df = pd.merge(solution, submission, on=row_id_column_name, suffixes=('_true', '_pred'))\n    #log NaN\n    nanx_in_pred = merged_df['x_pred'].isna().sum()\n    nany_in_pred = merged_df['y_pred'].isna().sum()\n    if nanx_in_pred > 0:\n        print(f\"WARNING: Found {nanx_in_pred} NaN predictions in merged results\")\n    if nany_in_pred > 0:\n        print(f\"WARNING: Found {nany_in_pred} NaN predictions in merged results\")\n    nanx_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['x_true'].isna().sum()\n    nany_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['y_true'].isna().sum()\n    if nanx_in_true > 0:\n        print(f\"WARNING: Found {nanx_in_true} NaN true values corresponding to NaN predictions\")\n    if nany_in_true > 0:\n        print(f\"WARNING: Found {nany_in_true} NaN true values corresponding to NaN predictions\")\n    rmse = np.sqrt(\n        0.5 * (mean_squared_error(merged_df['x_true'], merged_df['x_pred']) + mean_squared_error(merged_df['y_true'], merged_df['y_pred']))\n    )\n    return float(rmse)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.54023Z","iopub.status.busy":"2025-10-04T05:02:52.540019Z","iopub.status.idle":"2025-10-04T05:02:52.547324Z","shell.execute_reply":"2025-10-04T05:02:52.546659Z"},"papermill":{"duration":0.014538,"end_time":"2025-10-04T05:02:52.548341","exception":false,"start_time":"2025-10-04T05:02:52.533803","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare features for LSTM","metadata":{"papermill":{"duration":0.005315,"end_time":"2025-10-04T05:02:52.559043","exception":false,"start_time":"2025-10-04T05:02:52.553728","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def height_to_feet(height_str):\n    \"\"\"Convert height from 'ft-in' format to feet\"\"\"\n    try:\n        ft, inches = map(int, height_str.split('-'))\n        return ft + inches/12\n    except:\n        return None\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.570669Z","iopub.status.busy":"2025-10-04T05:02:52.570449Z","iopub.status.idle":"2025-10-04T05:02:52.573929Z","shell.execute_reply":"2025-10-04T05:02:52.573288Z"},"papermill":{"duration":0.010469,"end_time":"2025-10-04T05:02:52.574938","exception":false,"start_time":"2025-10-04T05:02:52.564469","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_sequences(input_df, output_df=None, test_template=None, is_training=True,\n                               window_size=Config.WINDOW_SIZE, cache_dir=\"cache\", save_to_disk=True,\n                               use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n    \"\"\"Prepare sequences (FAST interaction features using vectorized per-frame computation).\"\"\"\n    print(\"Preparing sequences for LSTM...\")\n    print('Using window size = ', window_size)\n    input_df = input_df.copy()\n\n    input_df['player_height_feet'] = input_df['player_height'].map(height_to_feet)\n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    delta_t = 0.1\n    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n\n    current_date = datetime.now()\n    input_df['age'] = input_df['player_birth_date'].apply(\n        lambda x: (current_date - datetime.strptime(x, '%Y-%m-%d')).days // 365 if pd.notnull(x) else None\n    )\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n    input_df['force'] = mass_kg * input_df['a']\n\n    input_df['rolling_mean_velocity_x'] = (\n        input_df.groupby(['game_id', 'play_id', 'nfl_id'])['velocity_x']\n        .transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n    )\n    input_df['rolling_std_acceleration'] = (\n        input_df.groupby(['game_id', 'play_id', 'nfl_id'])['a']\n        .transform(lambda x: x.rolling(window=window_size, min_periods=1).std())\n    )\n    # New features\n    input_df[\"heading_x\"] = np.sin(dir_rad)\n    input_df[\"heading_y\"] = np.cos(dir_rad)\n    input_df[\"acceleration_x\"] = input_df[\"a\"] * input_df[\"heading_x\"]\n    input_df[\"acceleration_y\"] = input_df[\"a\"] * input_df[\"heading_y\"]\n    input_df[\"accel_magnitude\"] = np.sqrt(input_df[\"acceleration_x\"]**2 + input_df[\"acceleration_y\"]**2)\n    if all(col in input_df.columns for col in ['ball_land_x', 'ball_land_y']):\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx ** 2 + ball_dy ** 2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n        input_df['estimated_time_to_ball'] = input_df['distance_to_ball'] / 20.0\n        input_df['projected_time_to_ball'] = input_df['distance_to_ball'] / (np.abs(input_df['closing_speed']) + 0.1)\n\n    input_df['is_right'] = (input_df['play_direction'] == 'right').astype(int)\n    input_df['is_left'] = (input_df['play_direction'] == 'left').astype(int)\n    print(\"Calculating interaction features...\")\n    # -------- PLAYER INTERACTION FEATURES --------\n    if use_players_interactions:\n        agg_rows = []\n        # Group once (avoid overhead of apply per small group)\n        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n            n = len(grp)\n            nfl_ids = grp['nfl_id'].to_numpy()\n            if n < 2:\n                # Create empty stats rows (NaNs) so merge still works\n                for nid in nfl_ids:\n                    agg_rows.append({\n                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n                        'distance_to_player_mean_offense': np.nan,\n                        'distance_to_player_min_offense': np.nan,\n                        'distance_to_player_max_offense': np.nan,\n                        'relative_velocity_magnitude_mean_offense': np.nan,\n                        'relative_velocity_magnitude_min_offense': np.nan,\n                        'relative_velocity_magnitude_max_offense': np.nan,\n                        'angle_to_player_mean_offense': np.nan,\n                        'angle_to_player_min_offense': np.nan,\n                        'angle_to_player_max_offense': np.nan,\n                        'distance_to_player_mean_defense': np.nan,\n                        'distance_to_player_min_defense': np.nan,\n                        'distance_to_player_max_defense': np.nan,\n                        'relative_velocity_magnitude_mean_defense': np.nan,\n                        'relative_velocity_magnitude_min_defense': np.nan,\n                        'relative_velocity_magnitude_max_defense': np.nan,\n                        'angle_to_player_mean_defense': np.nan,\n                        'angle_to_player_min_defense': np.nan,\n                        'angle_to_player_max_defense': np.nan,\n                    })\n                continue\n\n            x = grp['x'].to_numpy(dtype=np.float32)\n            y = grp['y'].to_numpy(dtype=np.float32)\n            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n            is_offense = grp['is_offense'].to_numpy()\n            is_defense = grp['is_defense'].to_numpy()\n\n            # Pairwise deltas (broadcast)\n            dx = x[None, :] - x[:, None]        # (n,n) x_j - x_i reversed later for angle\n            dy = y[None, :] - y[:, None]\n            # Angle from i -> j (want y_j - y_i, x_j - x_i)\n            angle_mat = np.arctan2(-dy, -dx)    # because dx currently x[None]-x[:,None] => -(x_j - x_i)\n\n            # Distances\n            dist = np.sqrt(dx ** 2 + dy ** 2)\n            # Relative velocity magnitudes\n            dvx = vx[:, None] - vx[None, :]\n            dvy = vy[:, None] - vy[None, :]\n            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n\n            # Offense mask (exclude self)\n            offense_mask = (is_offense[:, None] == is_offense[None, :])\n            np.fill_diagonal(offense_mask, False)\n\n            # Defense mask (exclude self)\n            defense_mask = (is_defense[:, None] == is_defense[None, :])\n            np.fill_diagonal(defense_mask, False)\n\n            # Mask out self distances\n            dist_diag_nan = dist.copy()\n            np.fill_diagonal(dist_diag_nan, np.nan)\n            rel_diag_nan = rel_speed.copy()\n            np.fill_diagonal(rel_diag_nan, np.nan)\n            angle_diag_nan = angle_mat.copy()\n            np.fill_diagonal(angle_diag_nan, np.nan)\n\n            def masked_stats(mat, mask):\n                # mat, mask shape (n,n)\n                masked = np.where(mask, mat, np.nan)\n                cnt = mask.sum(axis=1)\n                mean = np.nanmean(masked, axis=1)\n                amin = np.nanmin(masked, axis=1)\n                amax = np.nanmax(masked, axis=1)\n                # Rows with zero valid -> set nan\n                zero = cnt == 0\n                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n                return mean, amin, amax\n\n            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n\n            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n\n            for idx, nid in enumerate(nfl_ids):\n                agg_rows.append({\n                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n                    'distance_to_player_mean_offense': d_mean_o[idx],\n                    'distance_to_player_min_offense': d_min_o[idx],\n                    'distance_to_player_max_offense': d_max_o[idx],\n                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n                    'relative_velocity_magnitude_min_offense': v_min_o[idx],\n                    'relative_velocity_magnitude_max_offense': v_max_o[idx],\n                    'angle_to_player_mean_offense': a_mean_o[idx],\n                    'angle_to_player_min_offense': a_min_o[idx],\n                    'angle_to_player_max_offense': a_max_o[idx],\n                    'distance_to_player_mean_defense': d_mean_d[idx],\n                    'distance_to_player_min_defense': d_min_d[idx],\n                    'distance_to_player_max_defense': d_max_d[idx],\n                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n                    'relative_velocity_magnitude_min_defense': v_min_d[idx],\n                    'relative_velocity_magnitude_max_defense': v_max_d[idx],\n                    'angle_to_player_mean_defense': a_mean_d[idx],\n                    'angle_to_player_min_defense': a_min_d[idx],\n                    'angle_to_player_max_defense': a_max_d[idx],\n                })\n\n        interaction_agg = pd.DataFrame(agg_rows)\n        input_df = input_df.merge(\n            interaction_agg,\n            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n            how='left'\n        )\n    else:\n        print(\"Skipping fast interaction feature computation (use_fast_interactions=False).\")\n\n    # -------- (rest of original sequence creation unchanged) --------\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n\n    target_rows = output_df if is_training else test_template\n    grouped_input = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n\n    feature_cols = [\n        # Basic player features\n        'x', 'y', 's', 'a', 'o', 'dir','frame_id','ball_land_x','ball_land_y',\n        'absolute_yardline_number',\n        'player_height_feet', 'player_weight',\n        'velocity_x', 'velocity_y',\n        'momentum_x', 'momentum_y',\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n        'age', 'kinetic_energy', 'force',\n        'rolling_mean_velocity_x', 'rolling_std_acceleration',\n        # New features\n        'heading_x', 'heading_y', 'acceleration_x', 'acceleration_y', 'accel_magnitude',\n\n        \n        # Ball-related features (if available)\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y',\n        'closing_speed', 'estimated_time_to_ball', 'projected_time_to_ball'\n    ]\n    # Interaction features\n    players_interaction_features = [        \n        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',]\n    if 'distance_to_ball' in input_df.columns:\n        feature_cols += [\n            'distance_to_ball','angle_to_ball','ball_direction_x','ball_direction_y',\n            'closing_speed','estimated_time_to_ball','projected_time_to_ball'\n        ]\n    if use_players_interactions:\n        feature_cols += players_interaction_features\n    # # remove features with too many NaNs\n    # valid_frac = input_df[feature_cols].notna().mean()\n    # #print removed features\n    # removed_features = valid_frac[valid_frac < 0.7].index.tolist()\n    # if removed_features:\n    #     print(f\"Removing {len(removed_features)} features with >30% NaNs: {removed_features}\")\n    # feature_cols = valid_frac[valid_frac >= 0.7].index.tolist()\n\n    print(f\"Using {len(feature_cols)} features for LSTM input\")\n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups)):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        try:\n            group_df = grouped_input.get_group(key)\n        except KeyError:\n            continue\n        input_window = group_df.tail(window_size)\n        if len(input_window) < window_size:\n            print(f\"Warning: sequence too short for {key}, got {len(input_window)} frames, needed {window_size}\")\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        input_window = input_window.fillna(group_df.mean(numeric_only=True)) #\n        seq = input_window[feature_cols].values\n        if np.isnan(seq.astype(np.float32)).any():\n            if is_training:\n                continue\n            else:\n                seq = np.nan_to_num(seq, nan=0.0)\n        sequences.append(seq)\n        last_frame_id = input_window['frame_id'].iloc[-1]\n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            last_x = input_window.iloc[-1]['x']\n            last_y = input_window.iloc[-1]['y']\n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': last_frame_id\n        })\n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n    return sequences, sequence_ids\n# ...existing code...","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.586729Z","iopub.status.busy":"2025-10-04T05:02:52.586529Z","iopub.status.idle":"2025-10-04T05:02:52.611132Z","shell.execute_reply":"2025-10-04T05:02:52.610641Z"},"papermill":{"duration":0.031738,"end_time":"2025-10-04T05:02:52.612141","exception":false,"start_time":"2025-10-04T05:02:52.580403","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading and preparing data...\")\ntrain_input, train_output, test_input, test_template = load_data(debug_fraction=Config.DEBUG_FRACTION)\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:02:52.623843Z","iopub.status.busy":"2025-10-04T05:02:52.623639Z","iopub.status.idle":"2025-10-04T05:03:10.835804Z","shell.execute_reply":"2025-10-04T05:03:10.835032Z"},"papermill":{"duration":18.219306,"end_time":"2025-10-04T05:03:10.836955","exception":false,"start_time":"2025-10-04T05:02:52.617649","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if Config.PREPROCESSED_DATA_DIR is not None:\n    print(\"Loading preprocessed LSTM data from disk...\")\n    lstm_data = joblib.load(os.path.join(Config.PREPROCESSED_DATA_DIR, 'lstm_sequences_targets_ids.joblib'))\n    sequences = lstm_data['sequences']\n    targets_dx = lstm_data['targets_dx']\n    targets_dy = lstm_data['targets_dy']\n    targets_frame_ids = lstm_data['targets_frame_ids']\n    sequence_ids = lstm_data['ids']\n    print(f\"Loaded {len(sequences)} sequences from {Config.PREPROCESSED_DATA_DIR}\")\nelse:\n    sequences, targets_dx, targets_dy,targets_frame_ids,ids = prepare_sequences(\n        input_df=train_input,\n        output_df=train_output,\n        is_training=True,\n        window_size=Config.WINDOW_SIZE,\n    )\n    # save to /kaggle/working\n    joblib.dump({\n        'sequences': sequences,\n        'targets_dx': targets_dx,\n        'targets_dy': targets_dy,\n        'targets_frame_ids': targets_frame_ids,\n        'ids': ids\n    }, 'lstm_sequences_targets_ids.joblib')\n\n    print(\"Saved sequences, targets_dx, targets_dy, targets_frame_ids, ids to lstm_sequences_targets_ids.joblib\")\n\n# Prepare 3D sequences for LSTM\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:03:10.85213Z","iopub.status.busy":"2025-10-04T05:03:10.851892Z","iopub.status.idle":"2025-10-04T05:11:53.185152Z","shell.execute_reply":"2025-10-04T05:11:53.184394Z"},"papermill":{"duration":522.400546,"end_time":"2025-10-04T05:11:53.24486","exception":false,"start_time":"2025-10-04T05:03:10.844314","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(sequences),sequences[0].shape,len(targets_dx),targets_dx[0].shape,targets_dy[0].shape","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:53.363546Z","iopub.status.busy":"2025-10-04T05:11:53.362867Z","iopub.status.idle":"2025-10-04T05:11:53.368532Z","shell.execute_reply":"2025-10-04T05:11:53.368025Z"},"papermill":{"duration":0.066615,"end_time":"2025-10-04T05:11:53.369499","exception":false,"start_time":"2025-10-04T05:11:53.302884","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_oof_predictions(model, scaler, X_val_unscaled, val_ids, y_val_dx, y_val_dy, y_val_frame_ids, val_data):\n    \"\"\"\n    Build per-frame OOF predictions using ALL models (no exclusion).\n    Returns pred_df, true_df with real frame_ids.\n    \"\"\"\n    pred_rows, true_rows = [], []\n    for i, seq_info in enumerate(val_ids):\n        game_id = seq_info['game_id']\n        play_id = seq_info['play_id']\n        nfl_id = seq_info['nfl_id']\n        x_last = val_data.iloc[i]['x_last']\n        y_last = val_data.iloc[i]['y_last']\n        dx_true = y_val_dx[i]\n        dy_true = y_val_dy[i]\n        frame_ids_future = y_val_frame_ids[i]  # real future frame_ids\n        # True rows\n        for t in range(len(dx_true)):\n            true_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_ids_future[t]}\",\n                'x': x_last + dx_true[t],\n                'y': y_last + dy_true[t]\n            })\n        # Ensemble predictions\n        per_model_dx, per_model_dy = [], []\n        \n            \n        scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n        inp = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n        model.eval()\n        with torch.no_grad():\n            out = model(inp).cpu().numpy()[0]  # (H,2) cumulative dx,dy\n        per_model_dx.append(out[:,0])\n        per_model_dy.append(out[:,1])\n        ens_dx = np.mean(per_model_dx, axis=0)\n        ens_dy = np.mean(per_model_dy, axis=0)\n        # Use only required length\n        for t in range(len(dx_true)):\n            pred_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_ids_future[t]}\",\n                'x': np.clip(x_last + ens_dx[t], Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n                'y': np.clip(y_last + ens_dy[t], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX),\n            })\n    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:53.486827Z","iopub.status.busy":"2025-10-04T05:11:53.486617Z","iopub.status.idle":"2025-10-04T05:11:53.494866Z","shell.execute_reply":"2025-10-04T05:11:53.494211Z"},"papermill":{"duration":0.068478,"end_time":"2025-10-04T05:11:53.495953","exception":false,"start_time":"2025-10-04T05:11:53.427475","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# PREDICTION UTILITIES\n# ================================================================================\n\ndef displacement_to_position(displacement_dx, displacement_dy, x_last, y_last):\n    \"\"\"\n    Convert displacement predictions to absolute positions.\n    \n    Args:\n        displacement_dx: Predicted displacement in x direction\n        displacement_dy: Predicted displacement in y direction  \n        x_last: Last known x position\n        y_last: Last known y position\n        \n    Returns:\n        pred_x, pred_y: Absolute predicted positions\n    \"\"\"\n    pred_x = x_last + displacement_dx\n    pred_y = y_last + displacement_dy\n    \n    # Apply field constraints\n    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    return pred_x, pred_y\n\n\ndef predict_with_lstm(model, X_test, test_data):\n    \"\"\"\n    Make predictions with trained LSTM model.\n    \n    Args:\n        model: Trained LSTM model\n        X_test: Test sequences (batch, sequence_length, features)\n        test_data: Test dataframe for position conversion\n        \n    Returns:\n        pred_x, pred_y: Absolute predicted positions\n    \"\"\"\n    device = next(model.parameters()).device\n    model.eval()\n    \n    predictions_dx = []\n    predictions_dy = []\n    \n    # Predict in batches\n    batch_size = 1024\n    test_dataset = TensorDataset(torch.FloatTensor(X_test))\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    with torch.no_grad():\n        for batch_X, in test_loader:\n            batch_X = batch_X.to(device)\n            outputs = model(batch_X)\n            \n            predictions_dx.extend(outputs[:, 0].cpu().numpy())\n            predictions_dy.extend(outputs[:, 1].cpu().numpy())\n    \n    # Convert to absolute positions\n    pred_x, pred_y = displacement_to_position(\n        np.array(predictions_dx), \n        np.array(predictions_dy),\n        test_data['x_last'].values,\n        test_data['y_last'].values\n    )\n    \n    return pred_x, pred_y","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:53.611503Z","iopub.status.busy":"2025-10-04T05:11:53.611304Z","iopub.status.idle":"2025-10-04T05:11:53.617576Z","shell.execute_reply":"2025-10-04T05:11:53.616799Z"},"papermill":{"duration":0.065335,"end_time":"2025-10-04T05:11:53.618601","exception":false,"start_time":"2025-10-04T05:11:53.553266","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict function","metadata":{"papermill":{"duration":0.057055,"end_time":"2025-10-04T05:11:53.733072","exception":false,"start_time":"2025-10-04T05:11:53.676017","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def make_test_predictions_lstm(models, X_test, test_seq_ids, test_input):\n    \"\"\"\n    Make predictions on test data using ensemble of trained LSTM models.\n    \n    Args:\n        models: List of trained LSTM models\n        X_test: Test sequences (batch, sequence_length, features)\n        test_seq_ids: Mapping info for test sequences\n        test_input: Original test input dataframe\n        \n    Returns:\n        submission: DataFrame with id, x, y columns\n    \"\"\"\n    print(\"Making test predictions...\")\n    \n    if len(X_test) == 0:\n        print(\"WARNING: No test sequences provided. Using fallback predictions.\")\n        # Fallback: use last known positions\n        submission = pd.DataFrame({\n            'id': (test_input['game_id'].astype(str) + '_' + \n                  test_input['play_id'].astype(str) + '_' + \n                  test_input['nfl_id'].astype(str) + '_' + \n                  test_input['frame_id'].astype(str)),\n            'x': test_input['x'].values,\n            'y': test_input['y'].values\n        })\n        return submission\n    \n    print(f\"Test sequences shape: {X_test.shape}\")\n    \n    # Get ensemble predictions\n    all_predictions_dx = []\n    all_predictions_dy = []\n    \n    for i, model in enumerate(models):\n        print(f\"Predicting with model {i+1}/{len(models)}...\")\n        \n        device = next(model.parameters()).device\n        model.eval()\n        \n        predictions_dx = []\n        predictions_dy = []\n        \n        # Predict in batches\n        batch_size = 512\n        test_dataset = TensorDataset(torch.FloatTensor(X_test))\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n        \n        with torch.no_grad():\n            for batch_X, in test_loader:\n                batch_X = batch_X.to(device)\n                outputs = model(batch_X)\n                \n                predictions_dx.extend(outputs[:, 0].cpu().numpy())\n                predictions_dy.extend(outputs[:, 1].cpu().numpy())\n        \n        all_predictions_dx.append(np.array(predictions_dx))\n        all_predictions_dy.append(np.array(predictions_dy))\n    \n    # Ensemble average\n    ensemble_dx = np.mean(all_predictions_dx, axis=0)\n    ensemble_dy = np.mean(all_predictions_dy, axis=0)\n    \n    # Initialize output arrays with NaN\n    final_pred_x = np.full(len(test_input), np.nan)\n    final_pred_y = np.full(len(test_input), np.nan)\n    \n    # Map predictions back to original test rows\n    for i, seq_info in enumerate(test_seq_ids):\n        # Find corresponding row in test_input\n        mask = ((test_input['game_id'] == seq_info['game_id']) &\n               (test_input['play_id'] == seq_info['play_id']) &\n               (test_input['nfl_id'] == seq_info['nfl_id']) &\n               (test_input['frame_id'] == seq_info['frame_id']))\n        \n        if mask.any():\n            # Get reference position\n            ref_x = test_input.loc[mask, 'x'].iloc[0]\n            ref_y = test_input.loc[mask, 'y'].iloc[0]\n            \n            # Convert displacement to absolute position\n            pred_x = ref_x + ensemble_dx[i]\n            pred_y = ref_y + ensemble_dy[i]\n            \n            # Store predictions\n            final_pred_x[mask] = pred_x\n            final_pred_y[mask] = pred_y\n    \n    # Fill any remaining NaN with original positions\n    nan_mask = np.isnan(final_pred_x)\n    final_pred_x[nan_mask] = test_input.loc[nan_mask, 'x'].values\n    final_pred_y[nan_mask] = test_input.loc[nan_mask, 'y'].values\n    \n    # Create submission DataFrame\n    submission = pd.DataFrame({\n        'id': (test_input['game_id'].astype(str) + '_' + \n              test_input['play_id'].astype(str) + '_' + \n              test_input['nfl_id'].astype(str) + '_' + \n              test_input['frame_id'].astype(str)),\n        'x': final_pred_x,\n        'y': final_pred_y\n    })\n    \n    # Final validation\n    submission['x'] = np.clip(submission['x'], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    submission['y'] = np.clip(submission['y'], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    \n    print(f\"Created submission with {len(submission)} predictions\")\n    print(f\"X range: [{submission['x'].min():.2f}, {submission['x'].max():.2f}]\")\n    print(f\"Y range: [{submission['y'].min():.2f}, {submission['y'].max():.2f}]\")\n    \n    return submission","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:53.850238Z","iopub.status.busy":"2025-10-04T05:11:53.849886Z","iopub.status.idle":"2025-10-04T05:11:53.860599Z","shell.execute_reply":"2025-10-04T05:11:53.859929Z"},"papermill":{"duration":0.070533,"end_time":"2025-10-04T05:11:53.861603","exception":false,"start_time":"2025-10-04T05:11:53.79107","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TemporalHuber1D","metadata":{"papermill":{"duration":0.057917,"end_time":"2025-10-04T05:11:53.977354","exception":false,"start_time":"2025-10-04T05:11:53.919437","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ...existing code...\nimport math\n\nclass TemporalHuber1D(nn.Module):\n    \"\"\"\n    1D Temporal Huber with optional exponential time-decay.\n    pred/target: (B, L); mask: (B, L) with 1 for valid steps.\n    \"\"\"\n    def __init__(self, delta=0.5, time_decay=0.03):\n        super().__init__()\n        self.delta = float(delta)\n        self.time_decay = float(time_decay)\n\n    def forward(self, pred, target, mask):\n        # pred, target, mask -> (B, L)\n        err = pred - target\n        abs_e = torch.abs(err)\n        per_elem = torch.where(\n            abs_e <= self.delta,\n            0.5 * err * err,\n            self.delta * (abs_e - 0.5 * self.delta)\n        )\n        if self.time_decay > 0:\n            L = pred.size(1)\n            t = torch.arange(L, device=pred.device).float()\n            w = torch.exp(-self.time_decay * t).view(1, L)\n            per_elem = per_elem * w\n            mask = mask * w\n        per_elem = per_elem * mask\n        denom = mask.sum() + 1e-8\n        return per_elem.sum() / denom","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:54.095619Z","iopub.status.busy":"2025-10-04T05:11:54.095288Z","iopub.status.idle":"2025-10-04T05:11:54.100987Z","shell.execute_reply":"2025-10-04T05:11:54.100461Z"},"papermill":{"duration":0.066217,"end_time":"2025-10-04T05:11:54.102018","exception":false,"start_time":"2025-10-04T05:11:54.035801","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model class","metadata":{}},{"cell_type":"code","source":"\n# --------------------- Building blocks ---------------------\n\nclass Residual(nn.Module):\n    def __init__(self, mod, dim_in, dim_out, drop_prob=0.0):\n        super().__init__()\n        self.mod = mod\n        self.proj = nn.Identity() if dim_in == dim_out else nn.Linear(dim_in, dim_out)\n        self.dropout = nn.Dropout(drop_prob)\n\n    def forward(self, x):\n        # x: (B, T, D_in)\n        y = self.mod(x)                               # (B, T, D_out)\n        x_proj = self.proj(x)                         # (B, T, D_out)\n        return self.dropout(y) + x_proj\n\n\nclass RNNBlock(nn.Module):\n    def __init__(self, input_dim, hidden_dim, rnn=\"gru\", num_layers=1, dropout=0.1, bidirectional=False):\n        super().__init__()\n        rnn_cls = nn.GRU if rnn.lower() == \"gru\" else nn.LSTM\n        self.rnn = rnn_cls(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=bidirectional\n        )\n        self.out_dim = hidden_dim * (2 if bidirectional else 1)\n\n    def forward(self, x):\n        # x: (B, T, F)\n        y, _ = self.rnn(x)\n        return y  # (B, T, out_dim)\n\n\nclass Conv1DBlock(nn.Module):\n    \"\"\"\n    Temporal Conv (TCN-style): depthwise separable convs with dilation, LayerNorm (on feature dim), GELU, residual.\n    \"\"\"\n    def __init__(self, dim, kernel_size=3, dilation=1, dropout=0.1):\n        super().__init__()\n        self.dim = dim\n        pad = (kernel_size - 1) * dilation // 2\n        self.pre_ln = nn.LayerNorm(dim)                 # apply on (B,T,D) before transpose\n        self.dw = nn.Conv1d(dim, dim, kernel_size, padding=pad, dilation=dilation, groups=dim)\n        self.pw = nn.Conv1d(dim, dim, 1)\n        self.act = nn.GELU()\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # x: (B, T, D)\n        y = self.pre_ln(x)              # (B,T,D)\n        y = y.transpose(1, 2)           # (B,D,T)\n        y = self.dw(y)                  # (B,D,T)\n        y = self.act(y)\n        y = self.pw(y)                  # (B,D,T)\n        y = self.drop(y)\n        return y.transpose(1, 2)        # (B,T,D)\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\n    PreNorm Transformer encoder block with MultiheadAttention + FFN + residuals.\n    \"\"\"\n    def __init__(self, dim, nhead=4, ff_mult=4, dropout=0.1):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads=nhead, dropout=dropout, batch_first=True)\n        self.ln2 = nn.LayerNorm(dim)\n        self.ff = nn.Sequential(\n            nn.Linear(dim, ff_mult * dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_mult * dim, dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x, attn_mask=None, key_padding_mask=None):\n        # x: (B, T, D)\n        h = self.ln1(x)\n        y, _ = self.attn(h, h, h, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n        x = x + y\n        h = self.ln2(x)\n        h = x + self.ff(h)\n        return h  # (B, T, D)\n\n\nclass SEBlock(nn.Module):\n    \"\"\"Squeeze-and-Excitation over features per time step.\"\"\"\n    def __init__(self, dim, r=4):\n        super().__init__()\n        hidden = max(1, dim // r)\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # x: (B, T, D)\n        s = x.mean(dim=1)          # (B, D)\n        g = self.net(s).unsqueeze(1)  # (B,1,D)\n        return x * g\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerBlockWrapper(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.block = block\n    def forward(self, x):\n        return self.block(x)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------- Flexible model ---------------------\n\nclass FlexibleSeqModel(nn.Module):\n    \"\"\"\n    Flexible sequence model that stacks customizable blocks and predicts a single-axis cumulative displacement\n    over H horizons. Use two instances for dx and dy (separate models), as per your better LB finding.\n    Supported blocks in block_specs:\n      {\"type\": \"rnn\", \"rnn\": \"gru\"|\"lstm\", \"hidden\": 128, \"layers\": 1, \"bidirectional\": False}\n      {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 1}\n      {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4}\n      {\"type\": \"se\"}  # squeeze-excitation\n    pooling: \"last\" | \"mean\" | \"attn\"\n    predict_mode: \"steps\" (per-step increments, then cumsum) | \"cumulative\"\n    \"\"\"\n    def __init__(\n        self,\n        input_dim: int,\n        horizon: int,\n        block_specs: list,\n        dropout: float = 0.2,\n        pooling: str = \"attn\",\n        predict_mode: str = \"steps\",\n        attn_pool_heads: int = 4,\n    ):\n        super().__init__()\n        self.horizon = horizon\n        self.predict_mode = predict_mode\n        self.pooling = pooling\n\n        dim = input_dim\n        blocks = []\n        for spec in block_specs:\n            t = spec[\"type\"].lower()\n            if t == \"rnn\":\n                blk = RNNBlock(\n                    input_dim=dim,\n                    hidden_dim=spec.get(\"hidden\", 128),\n                    rnn=spec.get(\"rnn\", \"gru\"),\n                    num_layers=spec.get(\"layers\", 1),\n                    dropout=spec.get(\"dropout\", 0.1),\n                    bidirectional=spec.get(\"bidirectional\", False),\n                )\n                blocks.append(Residual(blk, dim, blk.out_dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n                dim = blk.out_dim\n            elif t == \"tcn\":\n                blk = Conv1DBlock(dim, kernel_size=spec.get(\"kernel\", 3), dilation=spec.get(\"dilation\", 1), dropout=spec.get(\"dropout\", 0.1))\n                blocks.append(Residual(blk, dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n            elif t == \"transformer\":\n                blk = TransformerBlock(dim, nhead=spec.get(\"nhead\", 4), ff_mult=spec.get(\"ff_mult\", 4), dropout=spec.get(\"dropout\", 0.1))\n                blocks.append(Residual(TransformerBlockWrapper(blk), dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n            elif t == \"se\":\n                blk = SEBlock(dim, r=spec.get(\"r\", 4))\n                blocks.append(Residual(blk, dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n            else:\n                raise ValueError(f\"Unknown block type: {t}\")\n        self.blocks = nn.ModuleList(blocks)\n\n        # Attention pooling (if selected)\n        if pooling == \"attn\":\n            self.pool_ln = nn.LayerNorm(dim)\n            self.pool_attn = nn.MultiheadAttention(dim, num_heads=attn_pool_heads, batch_first=True)\n            self.pool_vec = nn.Parameter(torch.randn(1, 1, dim))  # learned query token\n        elif pooling == \"mean\":\n            self.pool_ln = nn.LayerNorm(dim)\n        else:\n            self.pool_ln = nn.LayerNorm(dim)\n\n        # Head predicts either steps or cumulative for a single axis\n        self.head = nn.Sequential(\n            nn.Linear(dim, 128),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, horizon)   # output: (B, H) for one axis\n        )\n\n    def forward(self, x):\n        # x: (B, T, F)\n        h = x\n        for blk in self.blocks:\n            h = blk(h)  # (B, T, D)\n\n        # pooling to (B, D)\n        if self.pooling == \"attn\":\n            B, T, D = h.shape\n            q = self.pool_vec.expand(B, -1, -1)       # (B, 1, D)\n            k = v = self.pool_ln(h)\n            ctx, _ = self.pool_attn(q, k, v)          # (B, 1, D)\n            ctx = ctx.squeeze(1)\n        elif self.pooling == \"mean\":\n            ctx = self.pool_ln(h).mean(dim=1)\n        else:\n            ctx = self.pool_ln(h[:, -1, :])  # last step\n\n        out = self.head(ctx)                   # (B, H)\n        if self.predict_mode == \"steps\":\n            out = torch.cumsum(out, dim=1)     # convert steps -> cumulative\n        return out  # (B, H) single axis cumulative\n# ...existing code...","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train function","metadata":{"papermill":{"duration":0.057494,"end_time":"2025-10-04T05:11:54.218485","exception":false,"start_time":"2025-10-04T05:11:54.160991","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"# ...existing code...\n\ndef _prepare_targets_axis(batch_axis, max_h):\n    \"\"\"\n    Pad 1D axis targets to (B, L) and produce masks (B, L).\n    \"\"\"\n    tensors, masks, lengths = [], [], []\n    for arr in batch_axis:\n        L = len(arr)\n        pad_len = max_h - L\n        padded = np.pad(arr, (0, pad_len), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors.append(torch.tensor(padded, dtype=torch.float32))\n        masks.append(torch.tensor(mask, dtype=torch.float32))\n        lengths.append(L)\n    return torch.stack(tensors), torch.stack(masks), lengths\n\n\ndef train_axis_model(\n    X_train, y_train_axis, X_val, y_val_axis, input_dim, horizon,\n    block_specs, pooling=\"attn\", predict_mode=\"steps\",\n    batch_size=256, epochs=100, lr=1e-3, patience=15,\n    delta=0.5, time_decay=0.03, verbose_every=5\n):\n    \"\"\"\n    Train a single-axis model (dx or dy) predicting cumulative displacement over horizon.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = FlexibleSeqModel(\n        input_dim=input_dim, horizon=horizon, block_specs=block_specs,\n        pooling=pooling, predict_mode=predict_mode, dropout=0.2\n    ).to(device)\n\n    crit = TemporalHuber1D(delta=delta, time_decay=time_decay)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5, verbose=False)\n\n    # Pre-batch to keep parity with your pipeline\n    max_h = horizon\n    train_batches = []\n    for i in range(0, len(X_train), batch_size):\n        end = min(i + batch_size, len(X_train))\n        batch_X = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))  # (B,T,F)\n        batch_y, batch_m, lengths = _prepare_targets_axis([y_train_axis[j] for j in range(i, end)], max_h)\n        train_batches.append((batch_X, batch_y, batch_m, lengths))\n\n    val_batches = []\n    for i in range(0, len(X_val), batch_size):\n        end = min(i + batch_size, len(X_val))\n        batch_X = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        batch_y, batch_m, lengths = _prepare_targets_axis([y_val_axis[j] for j in range(i, end)], max_h)\n        val_batches.append((batch_X, batch_y, batch_m, lengths))\n\n    best_loss, best_state, bad = float('inf'), None, 0\n    for ep in range(1, epochs + 1):\n        model.train()\n        tl = []\n        for bx, by, bm, _ in train_batches:\n            bx = bx.to(device); by = by.to(device); bm = bm.to(device)\n            pred = model(bx)              # (B, H)\n            loss = crit(pred, by, bm)\n            opt.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n            tl.append(loss.item())\n\n        model.eval()\n        vl = []\n        with torch.no_grad():\n            for bx, by, bm, _ in val_batches:\n                bx = bx.to(device); by = by.to(device); bm = bm.to(device)\n                pred = model(bx)\n                loss = crit(pred, by, bm)\n                vl.append(loss.item())\n        v = float(np.mean(vl)) if vl else float('inf')\n        sch.step(v)\n\n        if ep % max(1, verbose_every) == 0:\n            print(f\"Axis train epoch {ep}: train {np.mean(tl):.4f} val {v:.4f}\")\n\n        if v + 1e-6 < best_loss:\n            best_loss = v\n            best_state = {k: v_.detach().cpu().clone() for k, v_ in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                print(f\"Early stop axis at epoch {ep}\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    return model, best_loss\n\n\ndef create_oof_predictions_xy(\n    model_x, model_y, scaler, X_val_unscaled, val_ids, y_val_dx, y_val_dy, y_val_frame_ids, val_data, horizon\n):\n    \"\"\"\n    Build OOF predictions combining separate x and y axis models.\n    Uses real future frame_ids for scoring alignment.\n    \"\"\"\n    device = next(model_x.parameters()).device\n    pred_rows, true_rows = [], []\n    for i, seq_info in enumerate(val_ids):\n        game_id = seq_info['game_id']; play_id = seq_info['play_id']; nfl_id = seq_info['nfl_id']\n        x_last = val_data.iloc[i]['x_last']; y_last = val_data.iloc[i]['y_last']\n        dx_true = y_val_dx[i]; dy_true = y_val_dy[i]\n        future_ids = y_val_frame_ids[i]\n\n        # Truth rows\n        for t in range(len(dx_true)):\n            true_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{future_ids[t]}\",\n                'x': x_last + dx_true[t],\n                'y': y_last + dy_true[t],\n            })\n\n        # Predict cumulative dx, dy\n        seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n        inp = torch.tensor(seq).unsqueeze(0).to(device)\n        with torch.no_grad():\n            pred_dx = model_x(inp).cpu().numpy()[0]  # (H,)\n            pred_dy = model_y(inp).cpu().numpy()[0]\n        for t in range(len(dx_true)):\n            px = np.clip(x_last + pred_dx[t], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n            py = np.clip(y_last + pred_dy[t], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            pred_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{future_ids[t]}\",\n                'x': px, 'y': py\n            })\n    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)\n\n\ndef run_multi_fold_training_xy(\n    sequences, targets_dx, targets_dy, targets_frame_ids, ids,\n    block_specs,\n    pooling=\"attn\", predict_mode=\"steps\",\n    lr=1e-3, n_folds=5, epochs=100, patience=15\n):\n    # Ensure object arrays\n    if not isinstance(sequences, np.ndarray): sequences = np.array(sequences, dtype=object)\n    if not isinstance(targets_dx, np.ndarray): targets_dx = np.array(targets_dx, dtype=object)\n    if not isinstance(targets_dy, np.ndarray): targets_dy = np.array(targets_dy, dtype=object)\n    if not isinstance(targets_frame_ids, np.ndarray): targets_frame_ids = np.array(targets_frame_ids, dtype=object)\n\n    groups = np.array([d['game_id'] for d in ids])\n    gkf = GroupKFold(n_splits=n_folds)\n    input_dim = sequences[0].shape[-1]\n    H = Config.MAX_FUTURE_HORIZON\n\n    models_x, models_y, scalers = [], [], []\n    fold_metrics = []\n    oof_pred_parts, oof_true_parts = [], []\n\n    for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), start=1):\n        print(f\"\\n--- Fold {fold}/{n_folds} ---\")\n        X_tr_u = sequences[tr]; X_va_u = sequences[va]\n        dx_tr = targets_dx[tr]; dy_tr = targets_dy[tr]\n        dx_va = targets_dx[va]; dy_va = targets_dy[va]\n        fid_va = targets_frame_ids[va]\n        val_ids = [ids[i] for i in va]\n\n        # Scaler on train frames only\n        scaler = StandardScaler()\n        scaler.fit(np.vstack([s for s in X_tr_u]))\n        def apply_scaler(arr): return np.array([scaler.transform(s) for s in arr], dtype=object)\n        X_tr = np.stack(apply_scaler(X_tr_u).astype(np.float32))\n        X_va = np.stack(apply_scaler(X_va_u).astype(np.float32))\n\n        # Train axis models\n        model_x, _ = train_axis_model(\n            X_tr, dx_tr, X_va, dx_va, input_dim=input_dim, horizon=H,\n            block_specs=block_specs, pooling=pooling, predict_mode=predict_mode,\n            batch_size=Config.BATCH_SIZE, epochs=epochs, lr=lr, patience=patience,\n            delta=0.5, time_decay=0.03, verbose_every=5\n        )\n        model_y, _ = train_axis_model(\n            X_tr, dy_tr, X_va, dy_va, input_dim=input_dim, horizon=H,\n            block_specs=block_specs, pooling=pooling, predict_mode=predict_mode,\n            batch_size=Config.BATCH_SIZE, epochs=epochs, lr=lr, patience=patience,\n            delta=0.5, time_decay=0.03, verbose_every=5\n        )\n\n        # Save fold models/scaler\n        models_x.append(model_x); models_y.append(model_y); scalers.append(scaler)\n        os.makedirs(f'fold_{fold}', exist_ok=True)\n        torch.save({'state_dict': model_x.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, f'fold_{fold}/axis_x.pt')\n        torch.save({'state_dict': model_y.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, f'fold_{fold}/axis_y.pt')\n        joblib.dump(scaler, f'fold_{fold}/lstm_feature_scaler_fold.joblib')\n\n        # OOF for this fold\n        val_df = pd.DataFrame(val_ids)\n        val_df['x_last'] = np.array([s[-1,0] for s in X_va_u])\n        val_df['y_last'] = np.array([s[-1,1] for s in X_va_u])\n        oof_pred, oof_true = create_oof_predictions_xy(\n            model_x, model_y, scaler, X_va_u, val_ids, dx_va, dy_va, fid_va, val_df, horizon=H\n        )\n        oof_pred_parts.append(oof_pred); oof_true_parts.append(oof_true)\n\n        # Fold score\n        fold_rmse = score(oof_true, oof_pred, 'id')\n        fold_metrics.append(fold_rmse)\n        print(f\"Fold {fold} RMSE: {fold_rmse:.5f}\")\n\n    oof_pred_df = pd.concat(oof_pred_parts, ignore_index=True).drop_duplicates('id')\n    oof_true_df = pd.concat(oof_true_parts, ignore_index=True).drop_duplicates('id')\n    cv = score(oof_true_df, oof_pred_df, 'id')\n    print(\"\\n--- Multi-Fold Summary ---\")\n    for i, m in enumerate(fold_metrics, 1):\n        print(f\"Fold {i}: {m:.5f}\")\n    print(f\"OOF CV Score: {cv:.5f}\")\n    return models_x, models_y, scalers, fold_metrics, cv, oof_pred_df\n# ...existing code...","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train 1 fold","metadata":{"papermill":{"duration":0.057782,"end_time":"2025-10-04T05:11:54.471302","exception":false,"start_time":"2025-10-04T05:11:54.41352","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n# Train 1 fold using GroupKFold\n\n\nprint(f\"Sequences shape: {len(sequences)}\")  # Already an object array\nprint(f\"First sequence shape: {sequences[0].shape if len(sequences) > 0 else 'N/A'}\")\nprint(f\"Targets_dx: {len(targets_dx)} sequences, lengths: {[len(dx) for dx in targets_dx[:5]]}...\")  # Show first 5 lengths\nprint(f\"Targets_dy: {len(targets_dy)} sequences, lengths: {[len(dy) for dy in targets_dy[:5]]}...\")\n\n\n# Get number of output frames from the targets\nnum_frames_output = [targets_dx[i].shape for i in range(len(targets_dx))]\n# print(f\"Number of output frames to predict: {num_frames_output}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:54.591572Z","iopub.status.busy":"2025-10-04T05:11:54.591161Z","iopub.status.idle":"2025-10-04T05:11:55.036772Z","shell.execute_reply":"2025-10-04T05:11:55.036004Z"},"papermill":{"duration":0.509221,"end_time":"2025-10-04T05:11:55.03813","exception":false,"start_time":"2025-10-04T05:11:54.528909","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ...existing code...\n\n# Train only the first fold grouped by game_id\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\n\n# Ensure arrays are object arrays for flexible slicing\nsequences = np.array(sequences, dtype=object)\ntargets_dx = np.array(targets_dx, dtype=object)\ntargets_dy = np.array(targets_dy, dtype=object)\ntargets_frame_ids = np.array(targets_frame_ids, dtype=object)\n\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# # Group by game_id and take the first split\n# groups = np.array([d['game_id'] for d in ids])\n# gkf = GroupKFold(n_splits=Config.N_FOLDS)\n# train_idx, val_idx = next(gkf.split(sequences, groups=groups))\n\n# X_train_unscaled = sequences[train_idx]\n# X_val_unscaled = sequences[val_idx]\n# y_train_dx_fold = targets_dx[train_idx]\n# y_train_dy_fold = targets_dy[train_idx]\n# y_val_dx_fold = targets_dx[val_idx]\n# y_val_dy_fold = targets_dy[val_idx]\n# y_val_frame_ids_fold = targets_frame_ids[val_idx]\n\n# # Validation metadata (use unscaled last positions)\n# val_ids = [ids[i] for i in val_idx]\n# val_data = pd.DataFrame(val_ids)\n# val_data['x_last'] = np.array([s[-1, 0] for s in X_val_unscaled])\n# val_data['y_last'] = np.array([s[-1, 1] for s in X_val_unscaled])\n\n# # Fit scaler on training-fold frames only (no leakage)\n# scaler = StandardScaler()\n# scaler.fit(np.vstack([s for s in X_train_unscaled]))\n\n# def apply_scaler_to_sequences(seq_array, scaler):\n#     return np.array([scaler.transform(s) for s in seq_array], dtype=object)\n\n# X_train_fold = apply_scaler_to_sequences(X_train_unscaled, scaler)\n# X_val_fold = apply_scaler_to_sequences(X_val_unscaled, scaler)\n\n# input_dim = X_train_unscaled[0].shape[-1]\n# H = Config.MAX_FUTURE_HORIZON\n# # # Default block specs if not defined earlier\n# # if 'block_specs' not in globals():\n# def expand_block_specs(specs):\n#     out = []\n#     for spec in specs:\n#         k = int(spec.get(\"repeat\", 1))\n#         spec = {k_: v for k_, v in spec.items() if k_ != \"repeat\"}\n#         out.extend([dict(spec) for _ in range(k)])\n#     return out\n# base_specs = [\n#     {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n#     {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n#     {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n# ]\n# block_specs = expand_block_specs(base_specs)\n# # Train separate axis models\n\n# print(\"\\nTraining axis X model (first fold)...\")\n# model_x, best_loss_x = train_axis_model(\n#     X_train_fold, y_train_dx_fold, X_val_fold, y_val_dx_fold,\n#     input_dim=input_dim, horizon=H, block_specs=block_specs,\n#     pooling=\"mean\", predict_mode=\"steps\",\n#     batch_size=Config.BATCH_SIZE, epochs=200, lr=Config.LEARNING_RATE,\n#     patience=Config.PATIENCE, delta=0.5, time_decay=0.03, verbose_every=5\n# )\n\n# print(\"\\nTraining axis Y model (first fold)...\")\n# model_y, best_loss_y = train_axis_model(\n#     X_train_fold, y_train_dy_fold, X_val_fold, y_val_dy_fold,\n#     input_dim=input_dim, horizon=H, block_specs=block_specs,\n#     pooling=\"mean\", predict_mode=\"steps\",\n#     batch_size=Config.BATCH_SIZE, epochs=200, lr=Config.LEARNING_RATE,\n#     patience=Config.PATIENCE, delta=0.5, time_decay=0.03, verbose_every=5\n# )\n\n# # Save fold_1 artifacts\n# os.makedirs('fold_1', exist_ok=True)\n# torch.save({'state_dict': model_x.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, 'fold_1/axis_x.pt')\n# torch.save({'state_dict': model_y.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, 'fold_1/axis_y.pt')\n# joblib.dump(scaler, 'fold_1/lstm_feature_scaler_fold.joblib')\n\n# # OOF predictions and score for this fold\n# oof_pred_1, oof_true_1 = create_oof_predictions_xy(\n#     model_x, model_y, scaler,\n#     X_val_unscaled, val_ids,\n#     y_val_dx_fold, y_val_dy_fold, y_val_frame_ids_fold,\n#     val_data, horizon=H\n# )\n# fold1_rmse = score(oof_true_1, oof_pred_1, 'id')\n# print(f\"\\n[Fold 1] RMSE: {fold1_rmse:.5f}\")\n\n# # Expose as lists for downstream inference utilities\n# models_x = [model_x]\n# models_y = [model_y]\n# scalers = [scaler]\n# # ...existing code...","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# oof_true_1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission maker","metadata":{"papermill":{"duration":0.057921,"end_time":"2025-10-04T05:11:55.279849","exception":false,"start_time":"2025-10-04T05:11:55.221928","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def predict_with_improved_lstm(model, X_test, test_data,test_template=None, return_all=True):\n    \"\"\"\n    Predict cumulative displacements for each horizon.\n    Returns:\n      pred_first_x, pred_first_y, dx_cum, dy_cum, (optional) abs_all_x, abs_all_y\n    \"\"\"\n    device = next(model.parameters()).device\n    model.eval()\n    X = np.array(X_test, dtype=np.float32)\n    test_dataset = TensorDataset(torch.from_numpy(X))\n    loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    dx_list, dy_list = [], []\n    with torch.no_grad():\n        for (batch,) in loader:\n            batch = batch.to(device)\n            out = model(batch)  # (B, H, 2) cumulative displacements\n            # print(f\"Predicted batch shape: {out.shape}\")\n            dx_list.append(out[:, :, 0].cpu().numpy())\n            dy_list.append(out[:, :, 1].cpu().numpy())\n    # print(f\"Predicted {len(dx_list)} batches\")\n    if not dx_list:\n        print(\"WARNING: No predictions made. Using fallback.\")\n        empty = np.zeros((0, getattr(model, \"max_frames_output\", 1)))\n        return empty, empty, empty, empty, empty, empty\n    dx_cum = np.vstack(dx_list)\n    dy_cum = np.vstack(dy_list)\n    x_last = test_data['x_last'].values\n    y_last = test_data['y_last'].values\n    abs_all_x = x_last[:, None] + dx_cum\n    abs_all_y = y_last[:, None] + dy_cum\n    abs_all_x = np.clip(abs_all_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n    abs_all_y = np.clip(abs_all_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n    pred_first_x = abs_all_x[:, 0]\n    pred_first_y = abs_all_y[:, 0]\n    # print(pred_first_x.shape, pred_first_y.shape, dx_cum.shape, dy_cum.shape, abs_all_x.shape, abs_all_y.shape)\n    # print(abs_all_x[0])\n    if return_all:\n        return pred_first_x, pred_first_y, dx_cum, dy_cum, abs_all_x, abs_all_y\n    # print(pred_first_x.shape, pred_first_y.shape, dx_cum.shape, dy_cum.shape)\n    return pred_first_x, pred_first_y, dx_cum, dy_cum","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:55.396375Z","iopub.status.busy":"2025-10-04T05:11:55.395917Z","iopub.status.idle":"2025-10-04T05:11:55.402755Z","shell.execute_reply":"2025-10-04T05:11:55.402091Z"},"papermill":{"duration":0.06616,"end_time":"2025-10-04T05:11:55.4038","exception":false,"start_time":"2025-10-04T05:11:55.33764","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_ensemble_predictions_xy(\n    models_x, models_y, scalers, X_test_unscaled, test_seq_ids, test_template, batch_size=1024\n):\n    \"\"\"\n    Ensemble test-time predictions using separate axis models (dx and dy) across folds.\n    - models_x, models_y: lists of FlexibleSeqModel (same length, one per fold)\n    - scalers: list of StandardScaler, aligned with models (or None entries)\n    - X_test_unscaled: list/array of (T,F) sequences (unscaled)\n    - test_seq_ids: list of dicts with keys [game_id, play_id, nfl_id, frame_id(last)]\n    - test_template: DataFrame with required submission rows\n\n    Returns: DataFrame with columns [id, x, y]\n    \"\"\"\n    if len(models_x) == 0 or len(models_x) != len(models_y):\n        print(\"No axis models or mismatched model counts.\")\n        return None\n    if scalers is not None and len(scalers) != len(models_x):\n        raise ValueError(\"Length of scalers must match number of folds (or be None).\")\n\n    # Convert sequences to array of objects for robust handling\n    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n    N = len(X_test_unscaled)\n\n    # Last observed absolute positions from the sequences (assumes feat[0]=x, feat[1]=y)\n    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n\n    # Per-fold cumulative displacement predictions\n    per_fold_dx = []\n    per_fold_dy = []\n\n    for i in range(len(models_x)):\n        model_x = models_x[i]\n        model_y = models_y[i]\n        scaler = scalers[i] if scalers is not None else None\n\n        # Scale per sequence for this fold\n        if scaler is not None:\n            scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n        else:\n            scaled = X_test_unscaled\n\n        # Stack to (N,T,F)\n        X = np.stack(scaled.astype(np.float32))\n        device = next(model_x.parameters()).device\n        ds = TensorDataset(torch.from_numpy(X))\n        dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n\n        dx_list, dy_list = [], []\n        model_x.eval(); model_y.eval()\n        with torch.no_grad():\n            for (batch,) in dl:\n                batch = batch.to(device)    # (B,T,F)\n                dx = model_x(batch)         # (B,H)\n                dy = model_y(batch)         # (B,H)\n                dx_list.append(dx.cpu().numpy())\n                dy_list.append(dy.cpu().numpy())\n        dx_cum = np.vstack(dx_list)  # (N,H)\n        dy_cum = np.vstack(dy_list)  # (N,H)\n\n        per_fold_dx.append(dx_cum)\n        per_fold_dy.append(dy_cum)\n\n    # Ensemble by mean across folds\n    ens_dx = np.mean(np.stack(per_fold_dx, axis=0), axis=0)  # (N,H)\n    ens_dy = np.mean(np.stack(per_fold_dy, axis=0), axis=0)  # (N,H)\n\n    # Create submission rows by mapping to test_template frame order per (game,play,nfl)\n    test_meta = pd.DataFrame(test_seq_ids)\n    out_rows = []\n    H = ens_dx.shape[1]\n    for i, seq_info in test_meta.iterrows():\n        game_id = int(seq_info['game_id'])\n        play_id = int(seq_info['play_id'])\n        nfl_id = int(seq_info['nfl_id'])\n\n        frame_ids = (\n            test_template[\n                (test_template['game_id'] == game_id) &\n                (test_template['play_id'] == play_id) &\n                (test_template['nfl_id'] == nfl_id)\n            ]['frame_id'].sort_values().tolist()\n        )\n        for t, frame_id in enumerate(frame_ids):\n            tt = t if t < H else H - 1\n            px = np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n            py = np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            out_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n                'x': px,\n                'y': py\n            })\n    submission = pd.DataFrame(out_rows)\n    return submission\n# ...existing code...","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:55.525945Z","iopub.status.busy":"2025-10-04T05:11:55.525632Z","iopub.status.idle":"2025-10-04T05:11:55.537866Z","shell.execute_reply":"2025-10-04T05:11:55.537077Z"},"papermill":{"duration":0.077056,"end_time":"2025-10-04T05:11:55.539154","exception":false,"start_time":"2025-10-04T05:11:55.462098","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_ensemble_val_predictions(models, scalers, X_val_unscaled, val_ids, y_val_dx_fold, y_val_dy_fold, val_data, exclude_fold=None):\n    \"\"\"\n    Generate ensemble predictions for validation data and prepare for scoring.\n    Excludes the model from the same fold to prevent potential overfitting/leakage.\n    \n    Args:\n        models: List of trained models\n        scalers: List of scalers (one per model)\n        X_val_unscaled: Validation sequences (unscaled)\n        val_ids: List of dicts with sequence metadata\n        y_val_dx_fold, y_val_dy_fold: Ground truth displacements\n        val_data: DataFrame with x_last, y_last\n        exclude_fold: Index of the fold to exclude (0-based)\n    \n    Returns:\n        ensemble_pred_df, ensemble_true_df: DataFrames for scoring\n    \"\"\"\n    pred_rows = []\n    true_rows = []\n    \n    for i, seq_info in enumerate(val_ids):\n        game_id = seq_info['game_id']\n        play_id = seq_info['play_id']\n        nfl_id = seq_info['nfl_id']\n        x_last = val_data.iloc[i]['x_last']\n        y_last = val_data.iloc[i]['y_last']\n        \n        # Ground truth\n        dx_true = y_val_dx_fold[i]\n        dy_true = y_val_dy_fold[i]\n        for t in range(len(dx_true)):\n            frame_rel = t + 1\n            true_x = x_last + dx_true[t]\n            true_y = y_last + dy_true[t]\n            true_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n                'x': true_x,\n                'y': true_y\n            })\n        \n        # Ensemble predictions (exclude the model from the same fold)\n        per_model_dx = []\n        per_model_dy = []\n        for j, model in enumerate(models):\n            if exclude_fold is not None and j == exclude_fold:\n                continue  # Skip the model trained on this fold\n            scaler = scalers[j]\n            scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n            scaled_seq = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n            model.eval()\n            with torch.no_grad():\n                output = model(scaled_seq).cpu().numpy()[0]  # (max_frames_output, 2)\n            per_model_dx.append(output[:, 0])\n            per_model_dy.append(output[:, 1])\n        \n        # Average across remaining models\n        if per_model_dx:  # Ensure there are models to average\n            ens_dx = np.mean(per_model_dx, axis=0)\n            ens_dy = np.mean(per_model_dy, axis=0)\n        else:\n            # Fallback: use the last known position (though this shouldn't happen with n_folds > 1)\n            ens_dx = np.zeros(len(dx_true))\n            ens_dy = np.zeros(len(dy_true))\n        \n        # Generate predictions for each frame\n        for t in range(len(dx_true)):\n            pred_x = x_last + ens_dx[t]\n            pred_y = y_last + ens_dy[t]\n            pred_rows.append({\n                'id': f\"{game_id}_{play_id}_{nfl_id}_{t+1}\",\n                'x': np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n                'y': np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            })\n    \n    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:55.669945Z","iopub.status.busy":"2025-10-04T05:11:55.669676Z","iopub.status.idle":"2025-10-04T05:11:55.678495Z","shell.execute_reply":"2025-10-04T05:11:55.677798Z"},"papermill":{"duration":0.073178,"end_time":"2025-10-04T05:11:55.679652","exception":false,"start_time":"2025-10-04T05:11:55.606474","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5folds training","metadata":{"papermill":{"duration":0.05882,"end_time":"2025-10-04T05:11:55.797121","exception":false,"start_time":"2025-10-04T05:11:55.738301","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Train","metadata":{"papermill":{"duration":0.059111,"end_time":"2025-10-04T05:11:56.171205","exception":false,"start_time":"2025-10-04T05:11:56.112094","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Check NaN in sequences robustly\nnan_count = 0\nfor i, seq in enumerate(sequences):\n    try:\n        arr = np.array(seq, dtype=np.float32)\n        if np.isnan(arr).any():\n            nan_mask = np.isnan(arr)\n            nan_features = np.where(nan_mask.any(axis=0))[0]\n            print(f\"WARNING: NaN values found in sequence index {i}, feature columns: {nan_features}\")\n            nan_count += 1\n    except Exception as e:\n        print(f\"Could not check sequence {i}: {e}\")\nprint(f\"Total sequences with NaN: {nan_count}\")","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:56.287715Z","iopub.status.busy":"2025-10-04T05:11:56.287218Z","iopub.status.idle":"2025-10-04T05:11:56.767764Z","shell.execute_reply":"2025-10-04T05:11:56.766903Z"},"papermill":{"duration":0.540465,"end_time":"2025-10-04T05:11:56.769085","exception":false,"start_time":"2025-10-04T05:11:56.22862","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def expand_block_specs(specs):\n    \"\"\"\n    Supports {\"type\": \"...\", ..., \"repeat\": k} to replicate blocks.\n    Returns a flat list of block specs (without 'repeat').\n    \"\"\"\n    out = []\n    for spec in specs:\n        k = int(spec.get(\"repeat\", 1))\n        spec = {k_: v for k_, v in spec.items() if k_ != \"repeat\"}\n        out.extend([dict(spec) for _ in range(k)])\n    return out","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example block configuration\n\nblock_specs = expand_block_specs(Config.BASED_SPECS)\nif Config.NN_PRETRAIN_DIR is None:\n    models_x, models_y, scalers, fold_metrics, cv, oof_pred_df = run_multi_fold_training_xy(\n        sequences, targets_dx, targets_dy, targets_frame_ids, ids,\n        block_specs=block_specs,\n        pooling=\"mean\",            # simpler, robust\n        predict_mode=\"steps\",\n        lr=Config.LEARNING_RATE,\n        # n_folds = 5,\n        n_folds=Config.N_FOLDS,\n        epochs=Config.EPOCHS,\n        # epochs=10,\n        patience=Config.PATIENCE\n    )\n    print(\"Final OOF CV:\", cv)","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:11:56.886715Z","iopub.status.busy":"2025-10-04T05:11:56.88634Z","iopub.status.idle":"2025-10-04T05:34:40.412508Z","shell.execute_reply":"2025-10-04T05:34:40.411704Z"},"papermill":{"duration":1363.585884,"end_time":"2025-10-04T05:34:40.413726","exception":false,"start_time":"2025-10-04T05:11:56.827842","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Infer","metadata":{"papermill":{"duration":0.113745,"end_time":"2025-10-04T05:34:40.640903","exception":false,"start_time":"2025-10-04T05:34:40.527158","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def build_axis_model_from_config(cfg):\n    return FlexibleSeqModel(\n        input_dim=cfg['input_dim'],\n        horizon=cfg['horizon'],\n        block_specs=cfg['block_specs'],\n        dropout=cfg.get('dropout', 0.2),\n        pooling=cfg.get('pooling', 'attn'),\n        predict_mode=cfg.get('predict_mode', 'steps'),\n        attn_pool_heads=cfg.get('attn_pool_heads', 4),\n    )\n\ndef save_axis_checkpoint(model, cfg, fold_dir, axis_name='x'):\n    path = Path(fold_dir) / f'axis_{axis_name}.pt'\n    torch.save({'state_dict': model.state_dict(), 'config': cfg}, str(path))\n\ndef load_axis_checkpoint(fold_dir, axis_name='x', device=None):\n    device = device or Config.DEVICE\n    ckpt = torch.load(str(Path(fold_dir) / f'axis_{axis_name}.pt'), map_location=device)\n    cfg = ckpt['config']\n    model = build_axis_model_from_config(cfg).to(device)\n    model.load_state_dict(ckpt['state_dict'])\n    model.eval()\n    return model, cfg\n\ndef load_folds_xy(num_folds, models_dir=None, device=None):\n    device = device or Config.DEVICE\n    base = Path(models_dir) if models_dir else Path('.')\n    models_x, models_y, scalers, cfgs = [], [], [], []\n    for fold in range(1, num_folds + 1):\n        fold_dir = base / f'fold_{fold}'\n        try:\n            mx, cfg = load_axis_checkpoint(fold_dir, 'x', device=device)\n            my, _   = load_axis_checkpoint(fold_dir, 'y', device=device)\n            scaler = joblib.load(str(fold_dir / 'lstm_feature_scaler_fold.joblib'))\n            models_x.append(mx); models_y.append(my); scalers.append(scaler); cfgs.append(cfg)\n            print(f'Loaded fold {fold} OK')\n        except Exception as e:\n            print(f'Fold {fold} load failed: {e}')\n    return models_x, models_y, scalers, cfgs","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if Config.NN_PRETRAIN_DIR is not None:\n    print(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\n    models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\nelse:\n    models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=None, device=Config.DEVICE)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build test sequences\ntest_sequences, test_seq_ids = prepare_sequences(\n    test_input, test_template=test_template, is_training=False, window_size=Config.WINDOW_SIZE\n)\nprint(f\"Prepared {len(test_sequences)} test sequences with shape: {test_sequences[0].shape}.\")\n# Use the trained per-fold axis models\nsubmission_xy = create_ensemble_predictions_xy(\n    models_x=models_x_nn,\n    models_y=models_y_nn,\n    scalers=scalers,\n    X_test_unscaled=test_sequences,\n    test_seq_ids=test_seq_ids,\n    test_template=test_template,\n    batch_size=1024\n)\nsubmission_xy.to_csv('submission_xy.csv', index=False)\nprint(\"Saved submission_xy.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:34:46.212777Z","iopub.status.busy":"2025-10-04T05:34:46.212514Z","iopub.status.idle":"2025-10-04T05:34:46.819393Z","shell.execute_reply":"2025-10-04T05:34:46.818626Z"},"papermill":{"duration":0.721145,"end_time":"2025-10-04T05:34:46.82054","exception":false,"start_time":"2025-10-04T05:34:46.099395","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_xy","metadata":{"execution":{"iopub.execute_input":"2025-10-04T05:34:47.054131Z","iopub.status.busy":"2025-10-04T05:34:47.053782Z","iopub.status.idle":"2025-10-04T05:34:47.070518Z","shell.execute_reply":"2025-10-04T05:34:47.069976Z"},"papermill":{"duration":0.134959,"end_time":"2025-10-04T05:34:47.071503","exception":false,"start_time":"2025-10-04T05:34:46.936544","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Catboost","metadata":{}},{"cell_type":"code","source":"# === CatBoost inference: load pretrained models, build test features, predict ===\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostRegressor\n\n# --------------------------- GNN-lite + FE (define if missing) --------------------------- #\ndef _to_inches(h):\n    try:\n        a, b = str(h).split(\"-\")\n        return float(a) * 12.0 + float(b)\n    except Exception:\n        return np.nan\n\nif 'engineer_advanced_features' not in globals():\n    def engineer_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n        df = df.copy()\n        df[\"height_inches\"] = df[\"player_height\"].map(_to_inches)\n        df[\"bmi\"] = (df[\"player_weight\"] / (df[\"height_inches\"]**2)) * 703.0\n\n        dir_rad = np.radians(df[\"dir\"].fillna(0.0))\n        df[\"heading_x\"] = np.sin(dir_rad)\n        df[\"heading_y\"] = np.cos(dir_rad)\n\n        s = df[\"s\"].fillna(0.0)\n        a = df[\"a\"].fillna(0.0)\n        df[\"velocity_x\"] = s * df[\"heading_x\"]\n        df[\"velocity_y\"] = s * df[\"heading_y\"]\n        df[\"acceleration_x\"] = a * df[\"heading_x\"]\n        df[\"acceleration_y\"] = a * df[\"heading_y\"]\n\n        dx = df[\"ball_land_x\"] - df[\"x\"]\n        dy = df[\"ball_land_y\"] - df[\"y\"]\n        dist = np.sqrt(dx**2 + dy**2)\n        df[\"dist_to_ball\"] = dist\n        df[\"angle_to_ball\"] = np.arctan2(dy, dx)\n        bux = dx / (dist + 1e-6)\n        buy = dy / (dist + 1e-6)\n\n        df[\"velocity_toward_ball\"] = df[\"velocity_x\"]*bux + df[\"velocity_y\"]*buy\n        df[\"velocity_alignment\"]   = df[\"heading_x\"]*bux + df[\"heading_y\"]*buy\n\n        df[\"speed_squared\"]   = s**2\n        df[\"accel_magnitude\"] = np.sqrt(df[\"acceleration_x\"]**2 + df[\"acceleration_y\"]**2)\n        w = df[\"player_weight\"].fillna(0.0)\n        df[\"momentum_x\"] = w * df[\"velocity_x\"]\n        df[\"momentum_y\"] = w * df[\"velocity_y\"]\n        df[\"kinetic_energy\"] = 0.5 * w * df[\"speed_squared\"]\n\n        df[\"role_targeted_receiver\"] = (df[\"player_role\"] == \"Targeted Receiver\").astype(int)\n        df[\"role_defensive_coverage\"] = (df[\"player_role\"] == \"Defensive Coverage\").astype(int)\n        df[\"role_passer\"] = (df[\"player_role\"] == \"Passer\").astype(int)\n        df[\"side_offense\"] = (df[\"player_side\"] == \"Offense\").astype(int)\n        return df\n\nif 'add_sequence_features' not in globals():\n    def add_sequence_features(df: pd.DataFrame) -> pd.DataFrame:\n        df = df.sort_values([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"])\n        gcols = [\"game_id\",\"play_id\",\"nfl_id\"]\n        for lag in [1,2,3,4,5]:\n            for c in [\"x\",\"y\",\"velocity_x\",\"velocity_y\",\"s\",\"a\"]:\n                if c in df.columns:\n                    df[f\"{c}_lag{lag}\"] = df.groupby(gcols)[c].shift(lag)\n        for win in [3,5]:\n            for c in [\"x\",\"y\",\"velocity_x\",\"velocity_y\",\"s\"]:\n                if c in df.columns:\n                    df[f\"{c}_rolling_mean_{win}\"] = (\n                        df.groupby(gcols)[c].rolling(win, min_periods=1).mean()\n                          .reset_index(level=[0,1,2], drop=True)\n                    )\n                    df[f\"{c}_rolling_std_{win}\"] = (\n                        df.groupby(gcols)[c].rolling(win, min_periods=1).std()\n                          .reset_index(level=[0,1,2], drop=True)\n                    )\n        for c in [\"velocity_x\",\"velocity_y\"]:\n            if c in df.columns:\n                df[f\"{c}_delta\"] = df.groupby(gcols)[c].diff()\n        return df\n\n# Default GNN-lite knobs if not already defined\nK_NEIGH = globals().get(\"K_NEIGH\", 6)\nRADIUS  = globals().get(\"RADIUS\", 30.0)\nTAU     = globals().get(\"TAU\", 8.0)\n\nif 'compute_neighbor_embeddings' not in globals():\n    def compute_neighbor_embeddings(input_df: pd.DataFrame,\n                                    k_neigh: int = K_NEIGH,\n                                    radius: float = RADIUS,\n                                    tau: float = TAU) -> pd.DataFrame:\n        cols_needed = [\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\",\"x\",\"y\",\"velocity_x\",\"velocity_y\",\"player_side\"]\n        src = input_df[cols_needed].copy()\n\n        last = (src.sort_values([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"])\n                   .groupby([\"game_id\",\"play_id\",\"nfl_id\"], as_index=False)\n                   .tail(1)\n                   .rename(columns={\"frame_id\":\"last_frame_id\"})\n                   .reset_index(drop=True))\n\n        tmp = last.merge(\n            src.rename(columns={\n                \"frame_id\":\"nb_frame_id\",\"nfl_id\":\"nfl_id_nb\",\n                \"x\":\"x_nb\",\"y\":\"y_nb\",\"velocity_x\":\"vx_nb\",\"velocity_y\":\"vy_nb\",\"player_side\":\"player_side_nb\"\n            }),\n            left_on=[\"game_id\",\"play_id\",\"last_frame_id\"],\n            right_on=[\"game_id\",\"play_id\",\"nb_frame_id\"],\n            how=\"left\",\n        )\n\n        tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n\n        tmp[\"dx\"]  = tmp[\"x_nb\"] - tmp[\"x\"]\n        tmp[\"dy\"]  = tmp[\"y_nb\"] - tmp[\"y\"]\n        tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n        tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n        tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n\n        tmp = tmp[np.isfinite(tmp[\"dist\"])]\n        tmp = tmp[tmp[\"dist\"] > 1e-6]\n        if radius is not None:\n            tmp = tmp[tmp[\"dist\"] <= radius]\n\n        tmp[\"is_ally\"] = (tmp[\"player_side_nb\"].fillna(\"\") == tmp[\"player_side\"].fillna(\"\")).astype(np.float32)\n\n        keys = [\"game_id\",\"play_id\",\"nfl_id\"]\n        tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n        if k_neigh is not None:\n            tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n\n        tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n        sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n        tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"]/sum_w, 0.0)\n\n        tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n        tmp[\"wn_opp\"]  = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n\n        for col in [\"dx\",\"dy\",\"dvx\",\"dvy\"]:\n            tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n            tmp[f\"{col}_opp_w\"]  = tmp[col] * tmp[\"wn_opp\"]\n\n        tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n        tmp[\"dist_opp\"]  = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n\n        ag = tmp.groupby(keys).agg(\n            gnn_ally_dx_mean = (\"dx_ally_w\",\"sum\"),\n            gnn_ally_dy_mean = (\"dy_ally_w\",\"sum\"),\n            gnn_ally_dvx_mean= (\"dvx_ally_w\",\"sum\"),\n            gnn_ally_dvy_mean= (\"dvy_ally_w\",\"sum\"),\n            gnn_opp_dx_mean  = (\"dx_opp_w\",\"sum\"),\n            gnn_opp_dy_mean  = (\"dy_opp_w\",\"sum\"),\n            gnn_opp_dvx_mean = (\"dvx_opp_w\",\"sum\"),\n            gnn_opp_dvy_mean = (\"dvy_opp_w\",\"sum\"),\n            gnn_ally_cnt     = (\"is_ally\",\"sum\"),\n            gnn_opp_cnt      = (\"is_ally\", lambda s: float(len(s) - s.sum())),\n            gnn_ally_dmin    = (\"dist_ally\",\"min\"),\n            gnn_ally_dmean   = (\"dist_ally\",\"mean\"),\n            gnn_opp_dmin     = (\"dist_opp\",\"min\"),\n            gnn_opp_dmean    = (\"dist_opp\",\"mean\"),\n        ).reset_index()\n\n        near = tmp.loc[tmp[\"rnk\"]<=3, keys+[\"rnk\",\"dist\"]].copy()\n        near[\"rnk\"] = near[\"rnk\"].astype(int)\n        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n        dwide = dwide.rename(columns={1:\"gnn_d1\",2:\"gnn_d2\",3:\"gnn_d3\"}).reset_index()\n        ag = ag.merge(dwide, on=keys, how=\"left\")\n\n        for c in [\"gnn_ally_dx_mean\",\"gnn_ally_dy_mean\",\"gnn_ally_dvx_mean\",\"gnn_ally_dvy_mean\",\n                  \"gnn_opp_dx_mean\",\"gnn_opp_dy_mean\",\"gnn_opp_dvx_mean\",\"gnn_opp_dvy_mean\"]:\n            ag[c] = ag[c].fillna(0.0)\n        for c in [\"gnn_ally_cnt\",\"gnn_opp_cnt\"]:\n            ag[c] = ag[c].fillna(0.0)\n        for c in [\"gnn_ally_dmin\",\"gnn_opp_dmin\",\"gnn_ally_dmean\",\"gnn_opp_dmean\",\"gnn_d1\",\"gnn_d2\",\"gnn_d3\"]:\n            ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n        return ag\n\nif 'physics_baseline' not in globals():\n    def physics_baseline(x_last, y_last, vx_last, vy_last, dt):\n        px = x_last + vx_last * dt\n        py = y_last + vy_last * dt\n        px = np.clip(px, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n        py = np.clip(py, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n        return px, py\n\n# --------------------------- Load pretrained artifacts --------------------------- #\nckpt_name = \"catboost_models_5fold_gnnlite.pkl\"\ncat_ckpt_path = os.path.join(Config.CATBOOST_PRETRAIN_DIR, ckpt_name)\nif not os.path.exists(cat_ckpt_path):\n    raise FileNotFoundError(f\"CatBoost models not found at {cat_ckpt_path}\")\n\nwith open(cat_ckpt_path, \"rb\") as f:\n    cat_art = pickle.load(f)\n\n# Baseline artifact structure: lists of models per fold, and a single feature list\nmodels_x_cb = cat_art[\"models_x\"]   # list[CatBoostRegressor]\nmodels_y_cb = cat_art[\"models_y\"]   # list[CatBoostRegressor]\nfeat_cols_cat = cat_art[\"features\"]\ncv_rmse = cat_art.get(\"cv_rmse\", None)\nprint(f\"Loaded CatBoost fold models: {len(models_x_cb)} X, {len(models_y_cb)} Y\")\nprint(f\"Feature count: {len(feat_cols_cat)}\")\nif cv_rmse is not None:\n    print(f\"CV RMSE per fold (abs with baseline): {cv_rmse}\")\n\n# --------------------------- Build test features --------------------------- #\nte_in = test_input.copy()\nte_tpl = test_template.copy()\n\nprint(\"Engineering test features (geometry + lags)â€¦\")\nte_in = engineer_advanced_features(te_in)\nte_in = add_sequence_features(te_in)\n\nprint(\"Computing neighbor embeddings (last-frame, GNN-lite)â€¦\")\ngnn_te = compute_neighbor_embeddings(te_in, k_neigh=K_NEIGH, radius=RADIUS, tau=TAU)\n\n# Last observed frame per (game,play,nfl)\nagg_te = (\n    te_in.sort_values([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"])\n         .groupby([\"game_id\",\"play_id\",\"nfl_id\"], as_index=False)\n         .tail(1)\n         .rename(columns={\"frame_id\":\"last_frame_id\"})\n)\n\n# Merge last observed stats + GNN features into template rows\nte = te_tpl.merge(agg_te, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\nte = te.merge(gnn_te, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n\n# Time deltas (10 Hz)\nte[\"delta_frames\"] = (te[\"frame_id\"] - te[\"last_frame_id\"]).clip(lower=0).astype(float)\nte[\"delta_t\"] = te[\"delta_frames\"] / 10.0\n\n# Ensure all features exist\nfor c in feat_cols_cat:\n    if c not in te.columns:\n        te[c] = 0.0\n\n# Clean feature matrix\nte.loc[:, feat_cols_cat] = (\n    te[feat_cols_cat].replace([np.inf, -np.inf], np.nan).fillna(0.0).to_numpy()\n)\nXtest = te[feat_cols_cat].values.astype(np.float32)\n\n# Physics baseline at test\ntbx, tby = physics_baseline(\n    te[\"x\"].values, te[\"y\"].values,\n    te[\"velocity_x\"].values, te[\"velocity_y\"].values,\n    te[\"delta_t\"].values\n)\n\n# --------------------------- Predict residuals and add baseline --------------------------- #\nprint(\"Predicting with CatBoost fold models (residual -> absolute)â€¦\")\npred_rx = np.mean([m.predict(Xtest) for m in models_x_cb], axis=0)\npred_ry = np.mean([m.predict(Xtest) for m in models_y_cb], axis=0)\npred_x_cat = np.clip(pred_rx + tbx, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\npred_y_cat = np.clip(pred_ry + tby, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n\n# ---- Save submission ----\nsubmission_catboost = pd.DataFrame({\n    \"id\": (te[\"game_id\"].astype(str) + \"_\" +\n           te[\"play_id\"].astype(str) + \"_\" +\n           te[\"nfl_id\"].astype(str) + \"_\" +\n           te[\"frame_id\"].astype(str)),\n    \"x\": pred_x_cat, \"y\": pred_y_cat\n})\nsubmission_catboost.to_csv(\"submission_catboost.csv\", index=False)\nprint(\"Saved submission_catboost.csv\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_catboost","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count rows of each submission\nlen(submission_xy), len(submission_catboost)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ---- Blend CatBoost with loaded NN submission (submission_xy) ----\nif 'submission_xy' in globals():\n    sub_nn = submission_xy.copy()\n    sub_cat = submission_catboost.copy()\n    ens = sub_nn.merge(sub_cat, on=\"id\", suffixes=(\"_nn\", \"_cat\"), how=\"inner\")\n    if len(ens) == 0:\n        print(\"WARNING: No common ids to blend. Skipping ensemble.\")\n    else:\n        W = Config.BLEND_WEIGHT  # blend weight NN\n        ens['x'] = np.clip(W*ens['x_nn'] + (1.0-W)*ens['x_cat'], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n        ens['y'] = np.clip(W*ens['y_nn'] + (1.0-W)*ens['y_cat'], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n        submission_ensemble = ens[['id','x','y']].copy()\n        submission_ensemble.to_csv(\"submission.csv\", index=False)\n        print(\"Saved submission.csv (NN-CB 50/50)\")\nelse:\n    print(\"NN submission (submission_xy) not found in scope. Ensemble skipped.\")\n# ...existing code...","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_ensemble.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_ensemble.head()","metadata":{},"outputs":[],"execution_count":null}]}