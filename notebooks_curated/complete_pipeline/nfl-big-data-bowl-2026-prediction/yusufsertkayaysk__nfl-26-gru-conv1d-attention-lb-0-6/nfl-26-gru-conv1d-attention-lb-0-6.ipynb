{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3791.752927,"end_time":"2025-10-14T08:52:24.298814","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-14T07:49:12.545887","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0fcfa418fdd842e6a5405ada527f0964":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ffd7f83d1ab4b7d85b5ac4f03cb9c45":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e6ab106777784501b8745cd6e9cea977","placeholder":"‚Äã","style":"IPY_MODEL_434f27ef257a4145a5fe271e3a4f14bb","tabbable":null,"tooltip":null,"value":"‚Äá472/472‚Äá[00:12&lt;00:00,‚Äá39.88it/s]"}},"2a1f7fbc59e44e8c8e596e89ff6bf80b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"300430baf04e463bb1b66d8372abc035":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a673107f55d6451bbdf438fbad5ef39e","IPY_MODEL_8824faa5d7f34e82ae78552c3456aaf3","IPY_MODEL_9fe6bb8ebe40400ca111339ed8a61ed5"],"layout":"IPY_MODEL_9568c68d25f44106af5a36af681e53c6","tabbable":null,"tooltip":null}},"39e73ed488eb498c916c331c462723f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3fb7fe4fa7da48b89fc0d06336673205":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9880efc787694f7bba53f50ee5b745f0","IPY_MODEL_c89ab6be122240898294002153e1c3fb","IPY_MODEL_0ffd7f83d1ab4b7d85b5ac4f03cb9c45"],"layout":"IPY_MODEL_5509051491864e8ea590943b64832bca","tabbable":null,"tooltip":null}},"434f27ef257a4145a5fe271e3a4f14bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"437b420b8c11410380bb8a7571406b6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5509051491864e8ea590943b64832bca":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87638117dda64486bb6c6377acae671f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8824faa5d7f34e82ae78552c3456aaf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_2a1f7fbc59e44e8c8e596e89ff6bf80b","max":46045,"min":0,"orientation":"horizontal","style":"IPY_MODEL_437b420b8c11410380bb8a7571406b6b","tabbable":null,"tooltip":null,"value":46045}},"9568c68d25f44106af5a36af681e53c6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9880efc787694f7bba53f50ee5b745f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0fcfa418fdd842e6a5405ada527f0964","placeholder":"‚Äã","style":"IPY_MODEL_87638117dda64486bb6c6377acae671f","tabbable":null,"tooltip":null,"value":"100%"}},"9fe6bb8ebe40400ca111339ed8a61ed5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e95caaeaf3eb4a4baef77a758bb29fe9","placeholder":"‚Äã","style":"IPY_MODEL_cb3e862f00f040d8aa1bc557e235d120","tabbable":null,"tooltip":null,"value":"‚Äá46045/46045‚Äá[21:52&lt;00:00,‚Äá35.60it/s]"}},"a673107f55d6451bbdf438fbad5ef39e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e4df5566fbf941d6b9db2da03a221432","placeholder":"‚Äã","style":"IPY_MODEL_39e73ed488eb498c916c331c462723f4","tabbable":null,"tooltip":null,"value":"100%"}},"a7953fbb35c04e9c922e191fd7a7e22a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c89ab6be122240898294002153e1c3fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_a7953fbb35c04e9c922e191fd7a7e22a","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dbd34ca02901438ea57b4c904313ca2c","tabbable":null,"tooltip":null,"value":472}},"cb3e862f00f040d8aa1bc557e235d120":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"dbd34ca02901438ea57b4c904313ca2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4df5566fbf941d6b9db2da03a221432":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6ab106777784501b8745cd6e9cea977":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e95caaeaf3eb4a4baef77a758bb29fe9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üèà Hats Off to the Community üôå\n\nBig thanks to the amazing Kaggle community ‚Äî especially these brilliant notebooks that inspired and helped shape my approach:\n\n- [NFL BDB 26 Feature-Rich GRU Predictor (LB 0.61)](https://www.kaggle.com/code/tasmim/nfl-bdb-26-feature-rich-gru-predictor-lb-0-61) by [Tasmim](https://www.kaggle.com/tasmim)\n- [NFL Big Data Bowl 2026 Prediction](https://www.kaggle.com/code/shaistashahid/nfl-big-data-bowl-2026-prediction) by [Shaista Shahid](https://www.kaggle.com/shaistashahid)\n\nI integrated ideas from both into my architecture and reached **~0.609 LB** ‚Äî huge respect to the authors for their excellent work and detailed insights!  \nThis is what makes Kaggle awesome ‚Äî collaboration, learning, and pushing the boundaries together. \n\n","metadata":{}},{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom torch.utils.data import TensorDataset, DataLoader\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:15:23.036102Z","iopub.execute_input":"2025-10-21T12:15:23.036347Z","iopub.status.idle":"2025-10-21T12:15:28.612983Z","shell.execute_reply.started":"2025-10-21T12:15:23.036305Z","shell.execute_reply":"2025-10-21T12:15:28.612042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n    \n    SEED = 42\n    N_FOLDS = 5\n    BATCH_SIZE = 512\n    EPOCHS = 120\n    PATIENCE = 20\n    LEARNING_RATE = 5e-4\n    \n    WINDOW_SIZE = 9\n    HIDDEN_DIM = 192\n    MAX_FUTURE_HORIZON = 94\n    \n    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=13):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(Config.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:15:28.614781Z","iopub.execute_input":"2025-10-21T12:15:28.615095Z","iopub.status.idle":"2025-10-21T12:15:28.682024Z","shell.execute_reply.started":"2025-10-21T12:15:28.615077Z","shell.execute_reply":"2025-10-21T12:15:28.681238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# COMBINED FEATURE ENGINEERING ","metadata":{}},{"cell_type":"code","source":"def height_to_feet(height_str):\n    try:\n        ft, inches = map(int, str(height_str).split('-'))\n        return ft + inches/12\n    except:\n        return 6.0\n\ndef add_advanced_features(df):\n    \"\"\"Enhanced feature engineering\"\"\"\n    print(\"Adding advanced features...\")\n    df = df.copy()\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # Distance Rate Features\n    if 'distance_to_ball' in df.columns:\n        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n        df['time_to_intercept'] = (df['distance_to_ball'] / \n                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n    \n    # Target Alignment Features\n    if 'ball_direction_x' in df.columns:\n        df['velocity_alignment'] = (\n            df['velocity_x'] * df['ball_direction_x'] +\n            df['velocity_y'] * df['ball_direction_y']\n        )\n        df['velocity_perpendicular'] = (\n            df['velocity_x'] * (-df['ball_direction_y']) +\n            df['velocity_y'] * df['ball_direction_x']\n        )\n        if 'acceleration_x' in df.columns:\n            df['accel_alignment'] = (\n                df['acceleration_x'] * df['ball_direction_x'] +\n                df['acceleration_y'] * df['ball_direction_y']\n            )\n    \n    # Multi-Window Rolling\n    for window in [3, 5, 10]:\n        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n            if col in df.columns:\n                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).mean()\n                )\n                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n                    lambda x: x.rolling(window, min_periods=1).std()\n                ).fillna(0)\n    \n    # Extended Lag Features\n    for lag in [4, 5]:\n        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n            if col in df.columns:\n                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n    \n    # Velocity Change Features\n    if 'velocity_x' in df.columns:\n        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n        df['direction_change'] = df['direction_change'].apply(\n            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n        )\n    \n    # Field Position Features\n    df['dist_from_left'] = df['y']\n    df['dist_from_right'] = 53.3 - df['y']\n    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n    \n    # Role-Specific Features\n    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n    \n    # Time Features\n    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n        lambda x: x / (x.max() + 1)\n    )\n    \n    return df\n\ndef prepare_combined_features(input_df, output_df=None, test_template=None, is_training=True, window_size=10):\n    \"\"\"COMBINED: Advanced features + enhanced preprocessing WITH LAST POSITIONS\"\"\"\n    print(f\"Preparing COMBINED sequences (window_size={window_size})...\")\n    \n    input_df = input_df.copy()\n    \n    # BASIC FEATURES\n    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n    \n    # Enhanced motion features\n    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n    o_rad = np.deg2rad(input_df['o'].fillna(0))\n    \n    input_df['velocity_x'] = input_df['s'] * np.sin(dir_rad)\n    input_df['velocity_y'] = input_df['s'] * np.cos(dir_rad)\n    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n    input_df['orientation_x'] = np.sin(o_rad)\n    input_df['orientation_y'] = np.cos(o_rad)\n    \n    # Enhanced roles\n    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n    input_df['is_rusher'] = (input_df['player_role'] == 'Pass Rusher').astype(int)\n    \n    # Field position (enhanced)\n    input_df['field_x_norm'] = (input_df['x'] - Config.FIELD_X_MIN) / (Config.FIELD_X_MAX - Config.FIELD_X_MIN)\n    input_df['field_y_norm'] = (input_df['y'] - Config.FIELD_Y_MIN) / (Config.FIELD_Y_MAX - Config.FIELD_Y_MIN)\n    input_df['distance_to_sideline'] = np.minimum(input_df['y'], 53.3 - input_df['y'])\n    input_df['distance_to_endzone'] = np.minimum(input_df['x'], 120 - input_df['x'])\n    \n    # Physics features\n    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n    \n    # Ball features\n    if 'ball_land_x' in input_df.columns:\n        ball_dx = input_df['ball_land_x'] - input_df['x']\n        ball_dy = input_df['ball_land_y'] - input_df['y']\n        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n        input_df['closing_speed'] = (\n            input_df['velocity_x'] * input_df['ball_direction_x'] +\n            input_df['velocity_y'] * input_df['ball_direction_y']\n        )\n    \n    # Sort for temporal features\n    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    gcols = ['game_id', 'play_id', 'nfl_id']\n    \n    # Enhanced temporal features\n    for lag in [1, 2, 3, 5]:\n        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n        input_df[f's_lag{lag}'] = input_df.groupby(gcols)['s'].shift(lag)\n    \n    # Multiple EMA smoothing\n    for alpha in [0.1, 0.3, 0.5]:\n        input_df[f'velocity_x_ema_{alpha}'] = input_df.groupby(gcols)['velocity_x'].transform(\n            lambda x: x.ewm(alpha=alpha, adjust=False).mean()\n        )\n        input_df[f'velocity_y_ema_{alpha}'] = input_df.groupby(gcols)['velocity_y'].transform(\n            lambda x: x.ewm(alpha=alpha, adjust=False).mean()\n        )\n    \n    # ADVANCED FEATURES\n    input_df = add_advanced_features(input_df)\n    \n    # COMBINED FEATURE LIST\n    feature_cols = [\n        # Core tracking (8)\n        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id',\n        'ball_land_x', 'ball_land_y',\n        \n        # Player attributes (2)\n        'player_height_feet', 'player_weight',\n        \n        # Enhanced motion (7)\n        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n        'orientation_x', 'orientation_y',\n        'kinetic_energy',\n        \n        # Roles (6)\n        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer', 'is_rusher',\n        \n        # Field position (6)\n        'field_x_norm', 'field_y_norm', \n        'dist_from_sideline', 'dist_from_endzone',\n        'distance_to_sideline', 'distance_to_endzone',\n        \n        # Ball interaction (5)\n        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n        \n        # Enhanced temporal (20)\n        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1', 's_lag1',\n        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2', 's_lag2',\n        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3', 's_lag3',\n        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5', 's_lag5',\n        \n        # Multiple EMAs (6)\n        'velocity_x_ema_0.1', 'velocity_y_ema_0.1',\n        'velocity_x_ema_0.3', 'velocity_y_ema_0.3', \n        'velocity_x_ema_0.5', 'velocity_y_ema_0.5',\n        \n        # Advanced features\n        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n        'frames_elapsed', 'normalized_time',\n        \n        # Rolling features (selective)\n        'velocity_x_roll5', 'velocity_y_roll5', 's_roll5', 'a_roll5',\n        'velocity_x_std5', 'velocity_y_std5', 's_std5', 'a_std5',\n    ]\n    \n    # Filter to existing columns\n    feature_cols = [c for c in feature_cols if c in input_df.columns]\n    print(f\"Using {len(feature_cols)} COMBINED features\")\n    \n    # CREATE SEQUENCES\n    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n    \n    target_rows = output_df if is_training else test_template\n    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n    \n    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, last_positions = [], [], [], [], [], []\n    \n    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups)):\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        try:\n            group_df = grouped.get_group(key)\n        except KeyError:\n            continue\n        \n        input_window = group_df.tail(window_size)\n        \n        if len(input_window) < window_size:\n            if is_training:\n                continue\n            pad_len = window_size - len(input_window)\n            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n        \n        # Enhanced imputation\n        input_window = input_window.fillna(method='ffill').fillna(method='bfill')\n        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n        \n        seq = input_window[feature_cols].values\n        \n        if np.isnan(seq).any():\n            if is_training:\n                continue\n            seq = np.nan_to_num(seq, nan=0.0)\n        \n        sequences.append(seq)\n        \n        # Store last positions for metric calculation\n        last_x = input_window.iloc[-1]['x']\n        last_y = input_window.iloc[-1]['y']\n        last_positions.append((last_x, last_y))\n        \n        if is_training:\n            out_grp = output_df[\n                (output_df['game_id']==row['game_id']) &\n                (output_df['play_id']==row['play_id']) &\n                (output_df['nfl_id']==row['nfl_id'])\n            ].sort_values('frame_id')\n            \n            dx = out_grp['x'].values - last_x\n            dy = out_grp['y'].values - last_y\n            \n            targets_dx.append(dx)\n            targets_dy.append(dy)\n            targets_frame_ids.append(out_grp['frame_id'].values)\n        \n        sequence_ids.append({\n            'game_id': key[0],\n            'play_id': key[1],\n            'nfl_id': key[2],\n            'frame_id': input_window.iloc[-1]['frame_id']\n        })\n    \n    print(f\"Created {len(sequences)} sequences\")\n    \n    if is_training:\n        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols, last_positions\n    return sequences, sequence_ids, feature_cols, last_positions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:15:28.682974Z","iopub.execute_input":"2025-10-21T12:15:28.683214Z","iopub.status.idle":"2025-10-21T12:15:28.715591Z","shell.execute_reply.started":"2025-10-21T12:15:28.683197Z","shell.execute_reply":"2025-10-21T12:15:28.714806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ENHANCED MODEL ARCHITECTURE ","metadata":{}},{"cell_type":"code","source":"class EnhancedSeqModel(nn.Module):\n    def __init__(self, input_dim, horizon):\n        super().__init__()\n        self.horizon = horizon\n        \n        self.gru = nn.GRU(input_dim, 192, num_layers=3, batch_first=True, dropout=0.2, bidirectional=False)\n        \n        self.conv1d = nn.Sequential(\n            nn.Conv1d(192, 128, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv1d(128, 128, kernel_size=5, padding=2),\n            nn.GELU(),\n        )\n        \n        self.pool_ln = nn.LayerNorm(192)\n        self.pool_attn = nn.MultiheadAttention(192, num_heads=8, batch_first=True, dropout=0.1)\n        self.pool_query = nn.Parameter(torch.randn(1, 1, 192))\n        \n        self.head = nn.Sequential(\n            nn.Linear(192 + 128, 256),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, horizon * 2)\n        )\n        \n        self.initialize_weights()\n    \n    def initialize_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n            elif isinstance(module, nn.GRU):\n                for name, param in module.named_parameters():\n                    if 'weight' in name:\n                        nn.init.orthogonal_(param)\n                    elif 'bias' in name:\n                        nn.init.constant_(param, 0)\n    \n    def forward(self, x):\n        h, _ = self.gru(x)\n        \n        h_conv = self.conv1d(h.transpose(1, 2)).transpose(1, 2)\n        h_conv_pool = h_conv.mean(dim=1)\n        \n        B = h.size(0)\n        q = self.pool_query.expand(B, -1, -1)\n        h_norm = self.pool_ln(h)\n        ctx, _ = self.pool_attn(q, h_norm, h_norm)\n        ctx = ctx.squeeze(1)\n        \n        combined = torch.cat([ctx, h_conv_pool], dim=1)\n        \n        out = self.head(combined)\n        out = out.view(B, 2, self.horizon)\n        \n        out = torch.cumsum(out, dim=2)\n        \n        return out[:, 0, :], out[:, 1, :]\n\n# CORRECTED METRIC FUNCTIONS\ndef compute_rmse(pred_dx, pred_dy, target_dx, target_dy, mask):\n    \"\"\"Calculate RMSE - CORRECTED VERSION\n    Official formula: sqrt(0.5 * (MSE_x + MSE_y))\n    \"\"\"\n    squared_errors_x = ((pred_dx - target_dx)**2) * mask\n    squared_errors_y = ((pred_dy - target_dy)**2) * mask\n    \n    # Sum of squared errors divided by sum of mask (number of valid predictions)\n    mse_x = squared_errors_x.sum() / (mask.sum() + 1e-8)\n    mse_y = squared_errors_y.sum() / (mask.sum() + 1e-8)\n    \n    # Competition formula: sqrt(0.5 * (MSE_x + MSE_y))\n    combined_mse = 0.5 * (mse_x + mse_y)\n    return torch.sqrt(combined_mse).item()\n\ndef calculate_oof_rmse(sequences, targets_dx, targets_dy, oof_predictions, last_positions):\n    \"\"\"Calculate overall OOF RMSE - CORRECTED VERSION\n    Official formula: sqrt(0.5 * (MSE_x + MSE_y))\n    \"\"\"\n    all_squared_errors_x = []\n    all_squared_errors_y = []\n    total_samples = 0\n    \n    for i in range(len(sequences)):\n        target_dx = targets_dx[i]\n        target_dy = targets_dy[i]\n        pred_dx = oof_predictions[i, :len(target_dx), 0]\n        pred_dy = oof_predictions[i, :len(target_dy), 1]\n        last_x, last_y = last_positions[i]\n        \n        # Convert displacements to absolute positions\n        pred_x = last_x + pred_dx\n        pred_y = last_y + pred_dy\n        target_x = last_x + target_dx\n        target_y = last_y + target_dy\n        \n        # Calculate squared errors\n        squared_errors_x = (pred_x - target_x) ** 2\n        squared_errors_y = (pred_y - target_y) ** 2\n        \n        all_squared_errors_x.extend(squared_errors_x)\n        all_squared_errors_y.extend(squared_errors_y)\n        total_samples += len(target_dx)\n    \n    # Compute MSE separately for x and y\n    mse_x = np.sum(all_squared_errors_x) / total_samples\n    mse_y = np.sum(all_squared_errors_y) / total_samples\n    \n    # Apply competition formula\n    oof_rmse = np.sqrt(0.5 * (mse_x + mse_y))\n    \n    return oof_rmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:15:28.71656Z","iopub.execute_input":"2025-10-21T12:15:28.71682Z","iopub.status.idle":"2025-10-21T12:15:28.741569Z","shell.execute_reply.started":"2025-10-21T12:15:28.716797Z","shell.execute_reply":"2025-10-21T12:15:28.740897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n# ENHANCED TRAINING \n","metadata":{}},{"cell_type":"code","source":"class EnhancedTemporalLoss(nn.Module):\n    def __init__(self, delta=0.5, time_decay=0.05, velocity_weight=0.1):\n        super().__init__()\n        self.delta = delta\n        self.time_decay = time_decay\n        self.velocity_weight = velocity_weight\n        self.huber = nn.SmoothL1Loss(reduction='none')\n    \n    def forward(self, pred_dx, pred_dy, target_dx, target_dy, mask):\n        L = pred_dx.size(1)\n        t = torch.arange(L, device=pred_dx.device).float()\n        time_weights = torch.exp(-self.time_decay * t).view(1, L)\n        \n        # Position loss with time decay\n        loss_dx = self.huber(pred_dx, target_dx) * time_weights\n        loss_dy = self.huber(pred_dy, target_dy) * time_weights\n        \n        masked_loss_dx = (loss_dx * mask).sum() / (mask.sum() + 1e-8)\n        masked_loss_dy = (loss_dy * mask).sum() / (mask.sum() + 1e-8)\n        \n        # Competition-style position loss: 0.5 * (loss_x + loss_y)\n        position_loss = 0.5 * (masked_loss_dx + masked_loss_dy)\n        \n        # Optional velocity consistency\n        if self.velocity_weight > 0:\n            pred_velocity_x = torch.diff(pred_dx, dim=1, prepend=torch.zeros_like(pred_dx[:, :1]))\n            pred_velocity_y = torch.diff(pred_dy, dim=1, prepend=torch.zeros_like(pred_dy[:, :1]))\n            target_velocity_x = torch.diff(target_dx, dim=1, prepend=torch.zeros_like(target_dx[:, :1]))\n            target_velocity_y = torch.diff(target_dy, dim=1, prepend=torch.zeros_like(target_dy[:, :1]))\n            \n            velocity_loss = (\n                self.huber(pred_velocity_x, target_velocity_x).mean() +\n                self.huber(pred_velocity_y, target_velocity_y).mean()\n            ) * self.velocity_weight\n            \n            total_loss = position_loss + velocity_loss\n        else:\n            total_loss = position_loss\n        \n        return total_loss\n\ndef prepare_targets_enhanced(batch_dx, batch_dy, max_h):\n    \"\"\"Prepare targets with proper masking\"\"\"\n    tensors_dx, tensors_dy, masks = [], [], []\n    for dx_arr, dy_arr in zip(batch_dx, batch_dy):\n        L = len(dx_arr)\n        padded_dx = np.pad(dx_arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        padded_dy = np.pad(dy_arr, (0, max_h - L), constant_values=0).astype(np.float32)\n        mask = np.zeros(max_h, dtype=np.float32)\n        mask[:L] = 1.0\n        tensors_dx.append(torch.tensor(padded_dx))\n        tensors_dy.append(torch.tensor(padded_dy))\n        masks.append(torch.tensor(mask))\n    return torch.stack(tensors_dx), torch.stack(tensors_dy), torch.stack(masks)\n\ndef train_model_combined(X_train, y_dx_train, y_dy_train, X_val, y_dx_val, y_dy_val, input_dim, horizon, config):\n    device = config.DEVICE\n    model = EnhancedSeqModel(input_dim, horizon).to(device)\n    \n    criterion = EnhancedTemporalLoss(delta=0.5, time_decay=0.05, velocity_weight=0.05)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=config.LEARNING_RATE, \n        epochs=config.EPOCHS, steps_per_epoch=len(X_train)//config.BATCH_SIZE+1\n    )\n    \n    train_batches = []\n    for i in range(0, len(X_train), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_train))\n        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n        by_dx, by_dy, bm = prepare_targets_enhanced(\n            [y_dx_train[j] for j in range(i, end)],\n            [y_dy_train[j] for j in range(i, end)], \n            horizon\n        )\n        train_batches.append((bx, by_dx, by_dy, bm))\n    \n    val_batches = []\n    for i in range(0, len(X_val), config.BATCH_SIZE):\n        end = min(i + config.BATCH_SIZE, len(X_val))\n        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n        by_dx, by_dy, bm = prepare_targets_enhanced(\n            [y_dx_val[j] for j in range(i, end)],\n            [y_dy_val[j] for j in range(i, end)],\n            horizon\n        )\n        val_batches.append((bx, by_dx, by_dy, bm))\n    \n    best_rmse, best_state, bad = float('inf'), None, 0\n    \n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        train_losses = []\n        \n        for bx, by_dx, by_dy, bm in train_batches:\n            bx, by_dx, by_dy, bm = bx.to(device), by_dx.to(device), by_dy.to(device), bm.to(device)\n            pred_dx, pred_dy = model(bx)\n            loss = criterion(pred_dx, pred_dy, by_dx, by_dy, bm)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            train_losses.append(loss.item())\n        \n        model.eval()\n        val_losses, val_rmses = [], []\n        with torch.no_grad():\n            for bx, by_dx, by_dy, bm in val_batches:\n                bx, by_dx, by_dy, bm = bx.to(device), by_dx.to(device), by_dy.to(device), bm.to(device)\n                pred_dx, pred_dy = model(bx)\n                loss = criterion(pred_dx, pred_dy, by_dx, by_dy, bm)\n                rmse = compute_rmse(pred_dx, pred_dy, by_dx, by_dy, bm)\n                val_losses.append(loss.item())\n                val_rmses.append(rmse)\n        \n        train_loss = np.mean(train_losses)\n        val_loss = np.mean(val_losses)\n        val_rmse = np.mean(val_rmses)\n        \n        if epoch % 10 == 0:\n            lr = scheduler.get_last_lr()[0]\n            print(f\"  Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_rmse={val_rmse:.4f}, lr={lr:.2e}\")\n        \n        if val_rmse < best_rmse:\n            best_rmse = val_rmse\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            bad = 0\n        else:\n            bad += 1\n            if bad >= config.PATIENCE:\n                print(f\"  Early stop at epoch {epoch}\")\n                break\n    \n    if best_state:\n        model.load_state_dict(best_state)\n    \n    return model, best_rmse\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:15:28.742324Z","iopub.execute_input":"2025-10-21T12:15:28.742591Z","iopub.status.idle":"2025-10-21T12:15:28.769862Z","shell.execute_reply.started":"2025-10-21T12:15:28.742567Z","shell.execute_reply":"2025-10-21T12:15:28.769093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MAIN PIPELINE","metadata":{}},{"cell_type":"code","source":"def main():\n    config = Config()\n    \n    print(\"=\" * 90)\n    print(\"üöÄ MAIN PIPELINE WITH CORRECTED METRIC üöÄ\".center(90))\n    print(\"=\" * 90)\n    print(\"\\nüîó Key Corrections:\\n\")\n    print(\"  ‚úÖ Competition Metric: sqrt(0.5 * (MSE_x + MSE_y))\")\n    print(\"  ‚úÖ Proper OOF Calculation with absolute positions\")\n    print(\"  ‚úÖ Enhanced loss function aligned with competition\")\n    print(\"  ‚úÖ Last position tracking for accurate metric calculation\\n\")\n    print(\"=\" * 90)\n    \n    # Load data\n    print(\"\\n[1/4] Loading data...\")\n    train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n    \n    train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n    train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n    \n    test_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\n    test_template = pd.read_csv(config.DATA_DIR / \"test.csv\")\n    \n    # Prepare combined sequences WITH LAST POSITIONS\n    print(\"\\n[2/4] Preparing COMBINED sequences with position tracking...\")\n    (sequences, targets_dx, targets_dy, targets_frame_ids, \n     sequence_ids, feature_cols, last_positions) = prepare_combined_features(\n        train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n    )\n    \n    sequences = np.array(sequences, dtype=object)\n    targets_dx = np.array(targets_dx, dtype=object)\n    targets_dy = np.array(targets_dy, dtype=object)\n    last_positions = np.array(last_positions)\n    \n    print(f\"Feature dimension: {sequences[0].shape[-1]}\")\n    \n    # Train with combined approach\n    print(\"\\n[3/4] Training COMBINED model...\")\n    groups = np.array([d['game_id'] for d in sequence_ids])\n    gkf = GroupKFold(n_splits=config.N_FOLDS)\n    \n    models, scalers, fold_rmses = [], [], []\n    oof_predictions = np.zeros((len(sequences), config.MAX_FUTURE_HORIZON, 2))\n    \n    for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n        print(f\"\\nFold {fold}/{config.N_FOLDS}\")\n        \n        X_tr = sequences[tr]\n        X_va = sequences[va]\n        \n        scaler = StandardScaler()\n        scaler.fit(np.vstack([s for s in X_tr]))\n        \n        X_tr_scaled = np.stack([scaler.transform(s) for s in X_tr])\n        X_va_scaled = np.stack([scaler.transform(s) for s in X_va])\n        \n        model, val_rmse = train_model_combined(\n            X_tr_scaled, targets_dx[tr], targets_dy[tr], \n            X_va_scaled, targets_dx[va], targets_dy[va],\n            X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n        )\n        \n        # Store OOF predictions for validation set\n        model.eval()\n        with torch.no_grad():\n            X_va_tensor = torch.tensor(X_va_scaled.astype(np.float32)).to(config.DEVICE)\n            pred_dx, pred_dy = model(X_va_tensor)\n            oof_predictions[va, :, 0] = pred_dx.cpu().numpy()\n            oof_predictions[va, :, 1] = pred_dy.cpu().numpy()\n        \n        models.append(model)\n        scalers.append(scaler)\n        fold_rmses.append(val_rmse)\n        \n        print(f\"Fold {fold} completed with val_RMSE: {val_rmse:.4f}\")\n    \n    # Calculate overall OOF RMSE using CORRECTED metric\n    print(\"\\n\" + \"=\"*80)\n    print(\"CROSS-VALIDATION RESULTS\")\n    print(\"=\"*80)\n    for fold, rmse in enumerate(fold_rmses, 1):\n        print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n    \n    mean_rmse = np.mean(fold_rmses)\n    std_rmse = np.std(fold_rmses)\n    print(f\"\\nMean CV RMSE: {mean_rmse:.4f} ¬± {std_rmse:.4f}\")\n    \n    # Calculate full OOF RMSE using CORRECTED metric with absolute positions\n    oof_rmse = calculate_oof_rmse(sequences, targets_dx, targets_dy, oof_predictions, last_positions)\n    print(f\"Overall OOF RMSE: {oof_rmse:.4f}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Predict\n    print(\"\\n[4/4] Generating final predictions...\")\n    test_sequences, test_ids, _, test_last_positions = prepare_combined_features(\n        test_input, test_template=test_template, is_training=False, window_size=config.WINDOW_SIZE\n    )\n    \n    X_test = np.array(test_sequences, dtype=object)\n    test_last_x = np.array([pos[0] for pos in test_last_positions])\n    test_last_y = np.array([pos[1] for pos in test_last_positions])\n    \n    # Ensemble predictions\n    all_dx, all_dy = [], []\n    \n    for model, sc in zip(models, scalers):\n        X_scaled = np.stack([sc.transform(s) for s in X_test])\n        X_tensor = torch.tensor(X_scaled.astype(np.float32)).to(config.DEVICE)\n        \n        model.eval()\n        with torch.no_grad():\n            dx, dy = model(X_tensor)\n            all_dx.append(dx.cpu().numpy())\n            all_dy.append(dy.cpu().numpy())\n    \n    ens_dx = np.mean(all_dx, axis=0)\n    ens_dy = np.mean(all_dy, axis=0)\n    \n    # Create submission\n    rows = []\n    H = ens_dx.shape[1]\n    \n    for i, sid in enumerate(test_ids):\n        fids = test_template[\n            (test_template['game_id'] == sid['game_id']) &\n            (test_template['play_id'] == sid['play_id']) &\n            (test_template['nfl_id'] == sid['nfl_id'])\n        ]['frame_id'].sort_values().tolist()\n        \n        for t, fid in enumerate(fids):\n            tt = min(t, H - 1)\n            px = np.clip(test_last_x[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n            py = np.clip(test_last_y[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n            \n            rows.append({\n                'id': f\"{sid['game_id']}_{sid['play_id']}_{sid['nfl_id']}_{fid}\",\n                'x': float(px),\n                'y': float(py)\n            })\n    \n    submission = pd.DataFrame(rows)\n    submission.to_csv(\"submission.csv\", index=False)\n    \n    print(\"üèÅ FINAL SUBMISSION SUMMARY\".center(70))\n    print(\"=\" * 70)\n    \n    print(f\"\\n ‚úì Submission saved with CORRECTED METRIC!\")\n    print(f\"   ‚îú‚îÄ Rows: {len(submission)}\")\n    print(f\"   ‚îú‚îÄ Features used: {len(feature_cols)}\")\n    print(f\"   ‚îú‚îÄ OOF RMSE: {oof_rmse:.4f}\")\n    print(f\"   ‚îî‚îÄ Expected LB RMSE: {oof_rmse:.4f} (¬±0.01)\")\n    \n    print(\"\\n Metric Corrections Applied:\")\n    print(f\"   ‚Ä¢ Competition formula: sqrt(0.5 * (MSE_x + MSE_y))\")\n    print(f\"   ‚Ä¢ Proper absolute position calculation\")\n    print(f\"   ‚Ä¢ Correct OOF RMSE computation\")\n    print(f\"   ‚Ä¢ Enhanced loss alignment\")\n    \n    print(\"=\" * 70)\n    \n    return submission\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:15:28.770692Z","iopub.execute_input":"2025-10-21T12:15:28.771396Z","iopub.status.idle":"2025-10-21T12:16:49.025089Z","shell.execute_reply.started":"2025-10-21T12:15:28.771371Z","shell.execute_reply":"2025-10-21T12:16:49.023853Z"}},"outputs":[],"execution_count":null}]}