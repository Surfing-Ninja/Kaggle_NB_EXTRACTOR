{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Predicting Loan Payback**\nThis notebook tackles Playground Series S5E11 – [https://www.kaggle.com/competitions/playground-series-s5e11](http://)\n\n* EDA\n* Preprocessing\n* Feature engineering\n* Modeling\n* Ensemble ...\n* Stacking ...\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom scipy.stats import uniform, randint\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:25.849117Z","iopub.execute_input":"2025-11-09T04:22:25.85036Z","iopub.status.idle":"2025-11-09T04:22:25.860388Z","shell.execute_reply.started":"2025-11-09T04:22:25.850308Z","shell.execute_reply":"2025-11-09T04:22:25.859222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Import Train, Test, Submission_Sample Files","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\nsub = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:25.862838Z","iopub.execute_input":"2025-11-09T04:22:25.863507Z","iopub.status.idle":"2025-11-09T04:22:27.339276Z","shell.execute_reply.started":"2025-11-09T04:22:25.863479Z","shell.execute_reply":"2025-11-09T04:22:27.338011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Droping \"id\" column\ntrain = train.drop(\"id\", axis=1)\ntest = test.drop(\"id\", axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:27.340258Z","iopub.execute_input":"2025-11-09T04:22:27.340537Z","iopub.status.idle":"2025-11-09T04:22:27.397854Z","shell.execute_reply.started":"2025-11-09T04:22:27.340517Z","shell.execute_reply":"2025-11-09T04:22:27.396613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. EDA","metadata":{}},{"cell_type":"code","source":"print(f\"df Train shape {train.shape}\")\nprint(f\"df Test shape {test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:27.399821Z","iopub.execute_input":"2025-11-09T04:22:27.400166Z","iopub.status.idle":"2025-11-09T04:22:27.405672Z","shell.execute_reply.started":"2025-11-09T04:22:27.400132Z","shell.execute_reply":"2025-11-09T04:22:27.404557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:27.406892Z","iopub.execute_input":"2025-11-09T04:22:27.407277Z","iopub.status.idle":"2025-11-09T04:22:27.438782Z","shell.execute_reply.started":"2025-11-09T04:22:27.407247Z","shell.execute_reply":"2025-11-09T04:22:27.437624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## Check Null and Duplicate Values\n","metadata":{}},{"cell_type":"code","source":"print(\"Number of null value in Train DF : \",train.isna().sum().sum())\nprint(\"Number of null value in Test DF : \",test.isna().sum().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:27.440105Z","iopub.execute_input":"2025-11-09T04:22:27.44053Z","iopub.status.idle":"2025-11-09T04:22:27.73548Z","shell.execute_reply.started":"2025-11-09T04:22:27.440506Z","shell.execute_reply":"2025-11-09T04:22:27.734247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Check Duplicate Rows\nprint(\"Number of Duplicated Row in Train DF : \", train.duplicated().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:27.736697Z","iopub.execute_input":"2025-11-09T04:22:27.737052Z","iopub.status.idle":"2025-11-09T04:22:28.151673Z","shell.execute_reply.started":"2025-11-09T04:22:27.73703Z","shell.execute_reply":"2025-11-09T04:22:28.150686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Divide Numerical and Categorical Columns","metadata":{}},{"cell_type":"code","source":"num_cols = train.select_dtypes(exclude= 'object').columns\n\ncat_cols = train.select_dtypes(include= 'object').columns\n\n\nprint(f\"\\n Numerical features + Target ({len(num_cols)}):\")\nfor i, col in enumerate(num_cols, 1):\n    print(f\"   {i}. {col}\")\nprint(\"=\"*50)\nprint(f\"\\n Categorical features ({len(cat_cols)}):\")\nfor i, col in enumerate(cat_cols, 1):\n    print(f\"   {i}. {col}\")\n\nprint(f\"\\n Total  features: {len(num_cols) + len(cat_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:28.152727Z","iopub.execute_input":"2025-11-09T04:22:28.153214Z","iopub.status.idle":"2025-11-09T04:22:28.193857Z","shell.execute_reply.started":"2025-11-09T04:22:28.153144Z","shell.execute_reply":"2025-11-09T04:22:28.192842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerical Features Analyse","metadata":{}},{"cell_type":"code","source":"train[num_cols].describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:28.197622Z","iopub.execute_input":"2025-11-09T04:22:28.198433Z","iopub.status.idle":"2025-11-09T04:22:28.409646Z","shell.execute_reply.started":"2025-11-09T04:22:28.198395Z","shell.execute_reply":"2025-11-09T04:22:28.40892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(num_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:28.410551Z","iopub.execute_input":"2025-11-09T04:22:28.411081Z","iopub.status.idle":"2025-11-09T04:22:28.418107Z","shell.execute_reply.started":"2025-11-09T04:22:28.411052Z","shell.execute_reply":"2025-11-09T04:22:28.417029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Correlation","metadata":{}},{"cell_type":"code","source":"# numerical features correlation\nplt.figure(figsize=(8, 6))\ncorrelation_matrix = train[num_cols ].corr()\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', \n            linewidths=1, cmap=\"Greens\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:28.419002Z","iopub.execute_input":"2025-11-09T04:22:28.419256Z","iopub.status.idle":"2025-11-09T04:22:28.903046Z","shell.execute_reply.started":"2025-11-09T04:22:28.419238Z","shell.execute_reply":"2025-11-09T04:22:28.901747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Droping Target column\nnum_cols=num_cols.drop('loan_paid_back')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:28.904439Z","iopub.execute_input":"2025-11-09T04:22:28.904841Z","iopub.status.idle":"2025-11-09T04:22:28.910707Z","shell.execute_reply.started":"2025-11-09T04:22:28.904812Z","shell.execute_reply":"2025-11-09T04:22:28.909281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Distribution and Outlier","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nfor i, col in enumerate(num_cols, 1):\n    plt.subplot(len(num_cols), 2, 2*i - 1)\n    sns.histplot(train[col], kde=True, bins=40, color=\"#8da0cb\")\n    plt.title(f'Distribution: {col}')\n\n    plt.subplot(len(num_cols), 2, 2*i)\n    sns.boxplot(x=train[col], color=\"#fc8d62\")\n    plt.title(f'Boxplot: {col}')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:28.91181Z","iopub.execute_input":"2025-11-09T04:22:28.912156Z","iopub.status.idle":"2025-11-09T04:22:44.77852Z","shell.execute_reply.started":"2025-11-09T04:22:28.912127Z","shell.execute_reply":"2025-11-09T04:22:44.777278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train vs Test (distribution drift)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nfor i, col in enumerate(num_cols, 1):\n    plt.subplot(2, 3, i)\n    sns.kdeplot(train[col], label='Train', fill=True, alpha=0.5)\n    sns.kdeplot(test[col], label='Test', fill=True, alpha=0.3)\n    plt.title(f'Distribution comparison: {col}')\n    plt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:22:44.779677Z","iopub.execute_input":"2025-11-09T04:22:44.780034Z","iopub.status.idle":"2025-11-09T04:23:05.994518Z","shell.execute_reply.started":"2025-11-09T04:22:44.780006Z","shell.execute_reply":"2025-11-09T04:23:05.99348Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explore the Target Variable","metadata":{}},{"cell_type":"code","source":"# 1. Basic counts\ntarget_counts = train['loan_paid_back'].value_counts()\n\n# 2. Percentages\ntarget_percent = train['loan_paid_back'].value_counts(normalize=True) * 100\n\ntarget_counts = train['loan_paid_back'].value_counts()\ntarget_percent = train['loan_paid_back'].value_counts(normalize=True) * 100\n\n# 2. Plot\nplt.figure(figsize=(5,4))\nbars = plt.bar(target_counts.index.astype(str),\n               target_counts.values,\n               color=['#66c2a5','#fc8d62'])\n\n# Add percentage labels on each bar\nfor bar in bars:\n    height = bar.get_height()\n    percent = (height / target_counts.sum()) * 100\n    plt.text(bar.get_x() + bar.get_width()/2, height + 1000,  # adjust '1000' if scale differs\n             f'{percent:.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.title('Distribution of Loan Paid Back')\nplt.xlabel('Loan Paid Back (1 = Yes, 0 = No)')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:05.995575Z","iopub.execute_input":"2025-11-09T04:23:05.995991Z","iopub.status.idle":"2025-11-09T04:23:06.195055Z","shell.execute_reply.started":"2025-11-09T04:23:05.995951Z","shell.execute_reply":"2025-11-09T04:23:06.193895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Class Imbalance\n\n**This is a moderately imbalanced dataset — about 80/20 split.**\n\n**That means:**\n\nThere are much fewer “default” cases (0.0).\n\nA naive model that always predicts “1.0” (paid back) would already get ~80% accuracy — but terrible ROC-AUC because it can’t rank risk properly.\n\n*So roughly 4 out of 5 loans are successfully repaid, and 1 out of 5 defaults.*","metadata":{}},{"cell_type":"markdown","source":"## Categorical Feature Analyse","metadata":{}},{"cell_type":"code","source":"for col in cat_cols:\n    print(f\"\\n=== {col.upper()} ===\")\n    \n    # Frequency table\n    freq = train[col].value_counts(dropna=False)\n  \n    \n    # Repayment rate (mean of target per category)\n    repayment_rate = train.groupby(col)['loan_paid_back'].mean().sort_values(ascending=False)\n\n    \n    # Combine both \n    summary = pd.concat([freq, repayment_rate], axis=1)\n    summary.columns = ['Count', 'Repayment_Rate']\n    print(\"\\nSummary:\")\n    print(summary)\n    \n    # --- Visualization ---\n    plt.figure(figsize=(8,4))\n    \n    # Bar for repayment rate (target mean)\n    sns.barplot(\n        x=repayment_rate.index,\n        y=repayment_rate.values,\n        palette=\"viridis\"\n    )\n    plt.title(f'Repayment Rate by {col}')\n    plt.ylabel('Mean loan_paid_back (repayment rate)')\n    plt.xlabel(col)\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:06.196116Z","iopub.execute_input":"2025-11-09T04:23:06.196455Z","iopub.status.idle":"2025-11-09T04:23:08.08215Z","shell.execute_reply.started":"2025-11-09T04:23:06.196434Z","shell.execute_reply":"2025-11-09T04:23:08.080994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:08.082961Z","iopub.execute_input":"2025-11-09T04:23:08.083281Z","iopub.status.idle":"2025-11-09T04:23:08.103881Z","shell.execute_reply.started":"2025-11-09T04:23:08.083253Z","shell.execute_reply":"2025-11-09T04:23:08.102866Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Preprocessing\n","metadata":{}},{"cell_type":"markdown","source":"###  Skewness\n","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew\n\nskew_values = train[num_cols].apply(lambda x: skew(x.dropna()))\nprint(skew_values.sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:08.105281Z","iopub.execute_input":"2025-11-09T04:23:08.105647Z","iopub.status.idle":"2025-11-09T04:23:08.206208Z","shell.execute_reply.started":"2025-11-09T04:23:08.105624Z","shell.execute_reply":"2025-11-09T04:23:08.204806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"skewed_cols = skew_values[abs(skew_values) > 1].index.tolist()\nprint(\"Highly skewed columns:\", skewed_cols)\n\nfor col in skewed_cols:\n    train[col] = np.log1p(train[col])\n    test[col]  = np.log1p(test[col])\n\nfrom sklearn.preprocessing import PowerTransformer\n\n# Initialize Yeo-Johnson transformer\npt = PowerTransformer(method='yeo-johnson')\n\n# Apply transformation to skewed columns\n# train[skewed_cols] = pt.fit_transform(train[skewed_cols])\n# test[skewed_cols] = pt.transform(test[skewed_cols])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:08.207458Z","iopub.execute_input":"2025-11-09T04:23:08.207749Z","iopub.status.idle":"2025-11-09T04:23:08.224792Z","shell.execute_reply.started":"2025-11-09T04:23:08.207728Z","shell.execute_reply":"2025-11-09T04:23:08.223707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### outliers (IQR)","metadata":{}},{"cell_type":"code","source":"for col in num_cols:\n    Q1 = train[col].quantile(0.25)\n    Q3 = train[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    train[col] = train[col].clip(lower=lower_bound, upper=upper_bound)\n    test[col] = test[col].clip(lower=lower_bound, upper=upper_bound)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:08.22584Z","iopub.execute_input":"2025-11-09T04:23:08.226291Z","iopub.status.idle":"2025-11-09T04:23:08.477034Z","shell.execute_reply.started":"2025-11-09T04:23:08.22626Z","shell.execute_reply":"2025-11-09T04:23:08.475856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target = 'loan_paid_back'\ncols = train.columns\ncols = cols.drop('loan_paid_back')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:08.478101Z","iopub.execute_input":"2025-11-09T04:23:08.478475Z","iopub.status.idle":"2025-11-09T04:23:08.484309Z","shell.execute_reply.started":"2025-11-09T04:23:08.478435Z","shell.execute_reply":"2025-11-09T04:23:08.48295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Target Encoding\n\nthanks to yousef Elshahat\n\n[https://www.kaggle.com/code/yousefelshahat2/simple-xgboost-only-competition-data-s5e11/notebook](http://)","metadata":{}},{"cell_type":"code","source":"def target_encoding(train, predict, n_splits=5):\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    mean_features_train = pd.DataFrame(index=train.index)\n    mean_features_test = pd.DataFrame(index=predict.index)\n\n    for col in cols:\n        # --- K-Fold Target Mean Encoding ---\n        mean_encoded = np.zeros(len(train))\n        for tr_idx, val_idx in kf.split(train):\n            tr_fold = train.iloc[tr_idx]\n            val_fold = train.iloc[val_idx]\n            mean_map = tr_fold.groupby(col)[target].mean()\n            mean_encoded[val_idx] = val_fold[col].map(mean_map)\n\n        mean_features_train[f'mean_{col}'] = mean_encoded\n\n        # --- Apply global mean mapping to prediction/test data ---\n        global_mean = train.groupby(col)[target].mean()\n        mean_features_test[f'mean_{col}'] = predict[col].map(global_mean)\n\n    # --- Concatenate new features at once to avoid fragmentation ---\n    train = pd.concat([train, mean_features_train], axis=1)\n    predict = pd.concat([predict, mean_features_test], axis=1)\n\n    # Defragment\n    train = train.copy()\n    predict = predict.copy()\n    return train, predict\n\ntrain , test = target_encoding(train, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:08.485637Z","iopub.execute_input":"2025-11-09T04:23:08.486288Z","iopub.status.idle":"2025-11-09T04:23:15.284611Z","shell.execute_reply.started":"2025-11-09T04:23:08.486257Z","shell.execute_reply":"2025-11-09T04:23:15.283202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Add Frequency Feature\n\nthanks to yousef Elshahat\n\n[https://www.kaggle.com/code/yousefelshahat2/simple-xgboost-only-competition-data-s5e11/notebook](http://)","metadata":{}},{"cell_type":"code","source":"def create_frequency_features(df, df_test):\n\n    # Pre-allocate DataFrames for new features to avoid fragmentation\n    freq_features_train = pd.DataFrame(index=df.index)\n    freq_features_test = pd.DataFrame(index=df_test.index)\n    bin_features_train = pd.DataFrame(index=df.index)\n    bin_features_test = pd.DataFrame(index=df_test.index)\n\n    for col in cols:\n        # --- Frequency encoding ---\n        freq = df[col].value_counts()\n        df[f\"{col}_freq\"] = df[col].map(freq)\n        freq_features_test[f\"{col}_freq\"] = df_test[col].map(freq).fillna(freq.mean())\n\n        # --- Quantile binning for numeric columns ---\n        if col in num_cols:\n            for q in [5, 10, 15]:\n                try:\n                    train_bins, bins = pd.qcut(df[col], q=q, labels=False, retbins=True, duplicates=\"drop\")\n                    bin_features_train[f\"{col}_bin{q}\"] = train_bins\n                    bin_features_test[f\"{col}_bin{q}\"] = pd.cut(df_test[col], bins=bins, labels=False, include_lowest=True)\n                except Exception:\n                    bin_features_train[f\"{col}_bin{q}\"] = 0\n                    bin_features_test[f\"{col}_bin{q}\"] = 0\n\n    # Concatenate all new features at once\n    df = pd.concat([df, freq_features_train, bin_features_train], axis=1)\n    df_test = pd.concat([df_test, freq_features_test, bin_features_test], axis=1)\n\n    return df, df_test\n\ntrain, test = create_frequency_features(train, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:15.285911Z","iopub.execute_input":"2025-11-09T04:23:15.286274Z","iopub.status.idle":"2025-11-09T04:23:17.386601Z","shell.execute_reply.started":"2025-11-09T04:23:15.286237Z","shell.execute_reply":"2025-11-09T04:23:17.385456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:17.391004Z","iopub.execute_input":"2025-11-09T04:23:17.391403Z","iopub.status.idle":"2025-11-09T04:23:17.425257Z","shell.execute_reply.started":"2025-11-09T04:23:17.391382Z","shell.execute_reply":"2025-11-09T04:23:17.424249Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"**[https://www.kaggle.com/code/analyticaobscura/s5e11-loan-payback-xgb-lgbm-ann](http://)**","metadata":{}},{"cell_type":"code","source":"\n# train['loan_to_income'] = train['loan_amount'] / (train['annual_income'] + 1)\n# test['loan_to_income'] = test['loan_amount'] / (test['annual_income'] + 1)\n\n# train['total_debt'] = train['debt_to_income_ratio'] * train['annual_income']\n# test['total_debt'] = test['debt_to_income_ratio'] * test['annual_income']\n\n# train['available_income'] = train['annual_income'] * (1 - train['debt_to_income_ratio'])\n# test['available_income'] = test['annual_income'] * (1 - test['debt_to_income_ratio'])\n\n# train['affordability'] = train['available_income'] / (train['loan_amount'] + 1)\n# test['affordability'] = test['available_income'] / (test['loan_amount'] + 1)\n\n# train['monthly_payment'] = train['loan_amount'] * (1 + train['interest_rate']/100) / 12\n# test['monthly_payment'] = test['loan_amount'] * (1 + test['interest_rate']/100) / 12\n\n# train['payment_to_income'] = train['monthly_payment'] / (train['annual_income']/12 + 1)\n# test['payment_to_income'] = test['monthly_payment'] / (test['annual_income']/12 + 1)\n\n# train['risk_score'] = (train['debt_to_income_ratio'] * 40 + \n#                        (1 - train['credit_score']/850) * 30 + train['interest_rate'] * 2)\n# test['risk_score'] = (test['debt_to_income_ratio'] * 40 + \n#                       (1 - test['credit_score']/850) * 30 + test['interest_rate'] * 2)\n\n\n# train['credit_interest'] = train['credit_score'] * train['interest_rate'] / 100\n# test['credit_interest'] = test['credit_score'] * test['interest_rate'] / 100\n\n# train['income_credit'] = np.log1p(train['annual_income']) * train['credit_score'] / 1000\n# test['income_credit'] = np.log1p(test['annual_income']) * test['credit_score'] / 1000\n\n# train['debt_loan'] = train['debt_to_income_ratio'] * np.log1p(train['loan_amount'])\n# test['debt_loan'] = test['debt_to_income_ratio'] * np.log1p(test['loan_amount'])\n\n# train['log_income'] = np.log1p(train['annual_income'])\n# test['log_income'] = np.log1p(test['annual_income'])\n\n# train['log_loan'] = np.log1p(train['loan_amount'])\n# test['log_loan'] = np.log1p(test['loan_amount'])\n\n# for df in [train, test]:\n#     df['income_to_dti'] = df['annual_income'] / (1 + df['debt_to_income_ratio'])\n#     df['interest_to_score'] = df['interest_rate'] / df['credit_score']\n#     df['loan_per_score'] = df['loan_amount'] / df['credit_score']\n#     df['loan_to_dti'] = df['loan_amount'] / (1 + df['debt_to_income_ratio'])\n\n\n# # credit score categories\n\ndef map_fico_tier(score):\n    \"\"\"Maps a credit score to its corresponding FICO tier.\"\"\"\n    if score >= 800:\n        return 'Exceptional'\n    elif score >= 740:\n        return 'Very Good'\n    elif score >= 670:\n        return 'Good'\n    elif score >= 580:\n        return 'Fair'\n    else: # Below 580\n        return 'Poor'\n\ndef map_vantage_tier(score):\n    \"\"\"Maps a credit score to its corresponding VantageScore tier.\"\"\"\n    if score >= 781:\n        return 'Excellent'\n    elif score >= 661:\n        return 'Good'\n    elif score >= 601:\n        return 'Fair'\n    elif score >= 500:\n        return 'Poor'\n    else: # Below 500\n        return 'Very Poor'\n\n# Creates two new categorical features based on FICO and VantageScore ranges \n# using the existing 'credit_score' column in both train and test DataFrames.\n\n# for data in [train, test]:\n#     data['credit_score_FICO_tier'] = data['credit_score'].apply(map_fico_tier)\n#     data['credit_score_Vantage_tier'] = data['credit_score'].apply(map_vantage_tier)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:17.426617Z","iopub.execute_input":"2025-11-09T04:23:17.427109Z","iopub.status.idle":"2025-11-09T04:23:17.891464Z","shell.execute_reply.started":"2025-11-09T04:23:17.427078Z","shell.execute_reply":"2025-11-09T04:23:17.890423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encode Categortical Columns\n\n**1. grade_subgrade Split** : A1 is the best credit, F5 is the riskiest\n\n\n* A1\t       -----Excellent borrower, safest\n* A5\t       -----Slightly lower credit quality\n* B1\t       -----Moderate risk\n* D4\t       -----Risky\n* F5\t       -----Very high risk\n\n**2. Onehot other categorical column**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n\ntrain['grade'] = train['grade_subgrade'].str[0]\ntrain['subgrade'] = train['grade_subgrade'].str[1:].astype(int)\n\ntest['grade'] = test['grade_subgrade'].str[0]\ntest['subgrade'] = test['grade_subgrade'].str[1:].astype(int)\n\ngrade_order = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6}\ntrain['grade'] = train['grade'].map(grade_order)\ntest['grade'] = test['grade'].map(grade_order)\n\ntrain = train.drop('grade_subgrade', axis =1)\ntest = test.drop('grade_subgrade', axis =1)\n\n\ncat_cols = train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\nonehot_cols = cat_cols\n\n# onehot_cols = ['gender', 'marital_status', 'education_level', \n#                'employment_status', 'loan_purpose']\n\n\n\n# One-hot encode\nohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nencoded_train = ohe.fit_transform(train[onehot_cols])\nencoded_test = ohe.transform(test[onehot_cols])\n\n# Convert to DataFrame\nencoded_train_df = pd.DataFrame(encoded_train, \n                                columns=ohe.get_feature_names_out(onehot_cols),\n                                index=train.index)\nencoded_test_df = pd.DataFrame(encoded_test, \n                               columns=ohe.get_feature_names_out(onehot_cols),\n                               index=test.index)\n\n# Concatenate back\ntrain = pd.concat([train.drop(columns=onehot_cols), encoded_train_df], axis=1)\ntest = pd.concat([test.drop(columns=onehot_cols), encoded_test_df], axis=1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:17.892501Z","iopub.execute_input":"2025-11-09T04:23:17.892819Z","iopub.status.idle":"2025-11-09T04:23:22.575877Z","shell.execute_reply.started":"2025-11-09T04:23:17.892795Z","shell.execute_reply":"2025-11-09T04:23:22.574862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['credit_rank'] = train['grade'] * 10 + train['subgrade']\ntest['credit_rank']  = test['grade'] * 10 + test['subgrade']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:22.577034Z","iopub.execute_input":"2025-11-09T04:23:22.577471Z","iopub.status.idle":"2025-11-09T04:23:22.588664Z","shell.execute_reply.started":"2025-11-09T04:23:22.57744Z","shell.execute_reply":"2025-11-09T04:23:22.587461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train.drop(columns='loan_paid_back',axis=1)\ny = train['loan_paid_back']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:22.589855Z","iopub.execute_input":"2025-11-09T04:23:22.591213Z","iopub.status.idle":"2025-11-09T04:23:22.734003Z","shell.execute_reply.started":"2025-11-09T04:23:22.591184Z","shell.execute_reply":"2025-11-09T04:23:22.732791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:22.735031Z","iopub.execute_input":"2025-11-09T04:23:22.735304Z","iopub.status.idle":"2025-11-09T04:23:22.791924Z","shell.execute_reply.started":"2025-11-09T04:23:22.735278Z","shell.execute_reply":"2025-11-09T04:23:22.790782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Modeling","metadata":{}},{"cell_type":"markdown","source":"## 1. LightGBM ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom lightgbm import LGBMClassifier\n\nparams = dict(\n    n_estimators=1320,\n    learning_rate=0.05,\n    num_leaves=93,\n    max_depth=5,\n    colsample_bytree=0.975,\n    subsample=0.743,\n    reg_alpha=2.95,\n    reg_lambda=0.0022,\n    random_state=42,\n    n_jobs=-1,\n    metric='auc',\n    objective='binary',\n    boosting_type='gbdt',\n    verbosity=-1,\n)\n\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(test))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nroc_curves, fold_scores = [], []\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n    print(f\"--- Fold {fold}/{skf.n_splits} ---\")\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_val, y_val)],\n        eval_metric='auc',\n    )\n\n    val_pred = model.predict_proba(X_val)[:, 1]\n    oof_preds[val_idx] = val_pred\n\n    test_preds += model.predict_proba(test)[:, 1] / skf.n_splits\n\n    auc = roc_auc_score(y_val, val_pred)\n    fold_scores.append(auc)\n    print(f\"Fold {fold} AUC: {auc:.4f}\")\n\n    fpr, tpr, _ = roc_curve(y_val, val_pred)\n    roc_curves.append((fpr, tpr, auc))\n\noverall_auc = roc_auc_score(y, oof_preds)\nprint(\"Fold AUCs:\", [round(s, 4) for s in fold_scores])\nprint(f\"Overall OOF AUC: {overall_auc:.5f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T04:23:22.792953Z","iopub.execute_input":"2025-11-09T04:23:22.793364Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model = LGBMClassifier(**params)\nfinal_model.fit(X, y)","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lgb_params = dict(\n#     n_estimators=1320,\n#     learning_rate=0.05,\n#     num_leaves=93,\n#     max_depth=5,\n#     colsample_bytree=0.975,\n#     subsample=0.743,\n#     reg_alpha=2.95,\n#     reg_lambda=0.0022,\n#     random_state=42,\n#     n_jobs=-1,\n#     metric='auc',\n#     objective='binary',\n#     boosting_type='gbdt',\n#     verbosity=-1,\n# )\n\n# xgb_params = dict(\n#     objective=\"binary:logistic\",\n#     eval_metric=\"auc\",\n#     tree_method=\"hist\",           \n#     max_depth=6,\n#     learning_rate=0.0669438421783529,\n#     n_estimators=732,\n#     min_child_weight=8.368496274182363,\n#     subsample=0.8638990746572127,\n#     colsample_bytree=0.9262609574627299,\n#     gamma=1.9880100566380507,\n#     reg_alpha=0.010470012214699875,\n#     reg_lambda=0.010061409517576274,\n#     max_bin=504,                  \n#     random_state=42,\n#     n_jobs=-1,\n#     verbosity=0   \n# )\n\n# lgb_model = LGBMClassifier(**lgb_params)\n\n# xgb_model = xgb.XGBClassifier(**xgb_params)\n\n\n# # oof_preds = np.zeros(len(X))\n# # test_preds = np.zeros(len(test))\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# roc_curves, fold_scores = [], []\n\n# for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n#     print(f\"--- Fold {fold}/{skf.n_splits} ---\")\n#     X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n#     y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n\n#     lgb_model.fit(\n#         X_tr, y_tr,\n#         eval_set=[(X_val, y_val)],\n#         eval_metric='auc',\n#     )\n#     lgb_pred = lgb_model.predict_proba(X_val)[:, 1]\n\n#     xgb_model.fit(\n#         X_tr, y_tr,\n#         eval_set=[(X_val, y_val)],\n#         verbose=False\n#     )\n#     xgb_pred = xgb_model.predict_proba(X_val)[:, 1]\n\n\n#     val_pred =  xgb_pred \n#     val_pred = 0.6 * xgb_pred + 0.4 * lgb_pred\n    \n#     auc = roc_auc_score(y_val, val_pred)\n#     fold_scores.append(auc)\n#     print(f\"Fold {fold} AUC: {auc:.4f}\")\n\n#     fpr, tpr, _ = roc_curve(y_val, val_pred)\n#     roc_curves.append((fpr, tpr, auc))\n\n# print(\"Fold AUCs:\", [round(s, 4) for s in fold_scores])\n# simple_avg_score = np.mean(fold_scores)\n# print(f\"\\nSimple Average CV Score: {simple_avg_score:.5f} (+/- {np.std(fold_scores):.5f})\")","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_pred = final_model.predict_proba(test)[:, 1]\n# xgb_pred = xgb_model.predict_proba(test)[:, 1]\n\n# ensemble_pred = 0.6 * xgb_pred + 0.4 * lgb_pred\nensemble_pred = lgb_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nsub['loan_paid_back'] = ensemble_pred\n\nsub.to_csv('submission.csv', index=False)\n\nsub.head()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}