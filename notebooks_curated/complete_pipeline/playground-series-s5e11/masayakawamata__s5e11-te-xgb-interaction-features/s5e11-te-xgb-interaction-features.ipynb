{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"},{"sourceId":13059803,"sourceType":"datasetVersion","datasetId":8264672}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %load_ext cudf.pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:03.010257Z","iopub.execute_input":"2025-11-06T05:34:03.010564Z","iopub.status.idle":"2025-11-06T05:34:03.015712Z","shell.execute_reply.started":"2025-11-06T05:34:03.010536Z","shell.execute_reply":"2025-11-06T05:34:03.014844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:03.017402Z","iopub.execute_input":"2025-11-06T05:34:03.017708Z","iopub.status.idle":"2025-11-06T05:34:03.036513Z","shell.execute_reply.started":"2025-11-06T05:34:03.017688Z","shell.execute_reply":"2025-11-06T05:34:03.035419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np\n\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\norig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)\nprint('Orig Shape:', orig.shape)\n\ntrain.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:03.037655Z","iopub.execute_input":"2025-11-06T05:34:03.037977Z","iopub.status.idle":"2025-11-06T05:34:07.49575Z","shell.execute_reply.started":"2025-11-06T05:34:03.037948Z","shell.execute_reply":"2025-11-06T05:34:07.494785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET = 'loan_paid_back'\nCATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nBASE = [col for col in train.columns if col not in ['id', TARGET]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:07.496957Z","iopub.execute_input":"2025-11-06T05:34:07.497322Z","iopub.status.idle":"2025-11-06T05:34:07.502731Z","shell.execute_reply.started":"2025-11-06T05:34:07.497296Z","shell.execute_reply":"2025-11-06T05:34:07.501631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import combinations\n\nINTER = []\n\nfor col1, col2 in combinations(CATS+['debt_to_income_ratio', 'credit_score', 'interest_rate'], 2):\n    new_col_name = f'{col1}_{col2}'\n    INTER.append(new_col_name)\n    for df in [train, test, orig]:\n        df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n        \nprint(f'{len(INTER)} Features.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:07.505349Z","iopub.execute_input":"2025-11-06T05:34:07.506224Z","iopub.status.idle":"2025-11-06T05:34:10.883509Z","shell.execute_reply.started":"2025-11-06T05:34:07.50619Z","shell.execute_reply":"2025-11-06T05:34:10.882383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ORIG = []\n\nfor col in BASE:\n    # MEAN\n    mean_map = orig.groupby(col)[TARGET].mean()\n    new_mean_col_name = f\"orig_mean_{col}\"\n    mean_map.name = new_mean_col_name\n    \n    train = train.merge(mean_map, on=col, how='left')\n    test = test.merge(mean_map, on=col, how='left')\n    ORIG.append(new_mean_col_name)\n\n    # COUNT\n    new_count_col_name = f\"orig_count_{col}\"\n    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n    \n    train = train.merge(count_map, on=col, how='left')\n    test = test.merge(count_map, on=col, how='left')\n    ORIG.append(new_count_col_name)\n\nprint(len(ORIG), 'Orig Features Created!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:10.884533Z","iopub.execute_input":"2025-11-06T05:34:10.884878Z","iopub.status.idle":"2025-11-06T05:34:22.028295Z","shell.execute_reply.started":"2025-11-06T05:34:10.884851Z","shell.execute_reply":"2025-11-06T05:34:22.02758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = BASE + ORIG + INTER\nprint(len(FEATURES), 'Features.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:22.029088Z","iopub.execute_input":"2025-11-06T05:34:22.029432Z","iopub.status.idle":"2025-11-06T05:34:22.03372Z","shell.execute_reply.started":"2025-11-06T05:34:22.02941Z","shell.execute_reply":"2025-11-06T05:34:22.032984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train[FEATURES]\ny = train[TARGET]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:22.034555Z","iopub.execute_input":"2025-11-06T05:34:22.034802Z","iopub.status.idle":"2025-11-06T05:34:22.243115Z","shell.execute_reply.started":"2025-11-06T05:34:22.034775Z","shell.execute_reply":"2025-11-06T05:34:22.242489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\n\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:22.243945Z","iopub.execute_input":"2025-11-06T05:34:22.244218Z","iopub.status.idle":"2025-11-06T05:34:22.822733Z","shell.execute_reply.started":"2025-11-06T05:34:22.244194Z","shell.execute_reply":"2025-11-06T05:34:22.82211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:22.823374Z","iopub.execute_input":"2025-11-06T05:34:22.823724Z","iopub.status.idle":"2025-11-06T05:34:23.055631Z","shell.execute_reply.started":"2025-11-06T05:34:22.823697Z","shell.execute_reply":"2025-11-06T05:34:23.054996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 5,\n    'colsample_bytree': 0.5,\n    'subsample': 0.6,\n    'n_estimators': 10000,\n    'learning_rate': 0.01,\n    'early_stopping_rounds': 100,\n    'random_state': 42,\n    'n_jobs': -1,\n    'enable_categorical': True,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:23.056584Z","iopub.execute_input":"2025-11-06T05:34:23.056839Z","iopub.status.idle":"2025-11-06T05:34:23.061188Z","shell.execute_reply.started":"2025-11-06T05:34:23.056822Z","shell.execute_reply":"2025-11-06T05:34:23.060301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Target Encoder that supports multiple aggregation functions,\n    internal cross-validation for leakage prevention, and smoothing.\n\n    Parameters\n    ----------\n    cols_to_encode : list of str\n        List of column names to be target encoded.\n\n    aggs : list of str, default=['mean']\n        List of aggregation functions to apply. Any function accepted by\n        pandas' `.agg()` method is supported, such as:\n        'mean', 'std', 'var', 'min', 'max', 'skew', 'nunique', \n        'count', 'sum', 'median'.\n        Smoothing is applied only to the 'mean' aggregation.\n\n    cv : int, default=5\n        Number of folds for cross-validation in fit_transform.\n\n    smooth : float or 'auto', default='auto'\n        The smoothing parameter `m`. A larger value puts more weight on the \n        global mean. If 'auto', an empirical Bayes estimate is used.\n        \n    drop_original : bool, default=False\n        If True, the original columns to be encoded are dropped.\n    \"\"\"\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n        self.cols_to_encode = cols_to_encode\n        self.aggs = aggs\n        self.cv = cv\n        self.smooth = smooth\n        self.drop_original = drop_original\n        self.mappings_ = {}\n        self.global_stats_ = {}\n\n    def fit(self, X, y):\n        \"\"\"\n        Learn mappings from the entire dataset.\n        These mappings are used for the transform method on validation/test data.\n        \"\"\"\n        temp_df = X.copy()\n        temp_df['target'] = y\n\n        # Learn global statistics for each aggregation\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n\n        # Learn category-specific mappings\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n        \n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Apply learned mappings to the data.\n        Unseen categories are filled with global statistics.\n        \"\"\"\n        X_transformed = X.copy()\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n                new_col_name = f'TE_{col}_{agg_func}'\n                map_series = self.mappings_[col][agg_func]\n                X_transformed[new_col_name] = X[col].map(map_series)\n                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n        \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        Fit and transform the data using internal cross-validation to prevent leakage.\n        \"\"\"\n        # First, fit on the entire dataset to get global mappings for transform method\n        self.fit(X, y)\n\n        # Initialize an empty DataFrame to store encoded features\n        encoded_features = pd.DataFrame(index=X.index)\n        \n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n\n        for train_idx, val_idx in kf.split(X, y):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx]\n            \n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n\n            for col in self.cols_to_encode:\n                # --- Calculate mappings only on the training part of the fold ---\n                for agg_func in self.aggs:\n                    new_col_name = f'TE_{col}_{agg_func}'\n                    \n                    # Calculate global stat for this fold\n                    fold_global_stat = y_train.agg(agg_func)\n                    \n                    # Calculate category stats for this fold\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n\n                    # --- Apply smoothing only for 'mean' aggregation ---\n                    if agg_func == 'mean':\n                        counts = temp_df_train.groupby(col)['target'].count()\n                        \n                        m = self.smooth\n                        if self.smooth == 'auto':\n                            # Empirical Bayes smoothing\n                            variance_between = mapping.var()\n                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n                            if variance_between > 0:\n                                m = avg_variance_within / variance_between\n                            else:\n                                m = 0  # No smoothing if no variance between groups\n                        \n                        # Apply smoothing formula\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n                        encoded_values = X_val[col].map(smoothed_mapping)\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n                    \n                    # Store encoded values for the validation fold\n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n\n        # Merge with original DataFrame\n        X_transformed = X.copy()\n        for col in encoded_features.columns:\n            X_transformed[col] = encoded_features[col]\n            \n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n            \n        return X_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:23.062142Z","iopub.execute_input":"2025-11-06T05:34:23.062424Z","iopub.status.idle":"2025-11-06T05:34:23.08575Z","shell.execute_reply.started":"2025-11-06T05:34:23.0624Z","shell.execute_reply":"2025-11-06T05:34:23.084834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(test))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f'--- Fold {fold}/{N_SPLITS} ---')\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    X_test = test[FEATURES].copy()\n\n    TE = TargetEncoder(cols_to_encode=INTER, cv=5, smooth='auto', aggs=['mean'], drop_original=True)\n    X_train = TE.fit_transform(X_train, y_train)\n    X_val = TE.transform(X_val)\n    X_test = TE.transform(X_test)\n\n    X_train[BASE] = X_train[BASE].astype('category')\n    X_val[BASE] = X_val[BASE].astype('category')\n    X_test[BASE] = X_test[BASE].astype('category')\n\n    model = XGBClassifier(**params)\n    \n    model.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              verbose=1000)\n\n    val_preds = model.predict_proba(X_val)[:, 1]\n    oof_preds[val_idx] = val_preds\n    \n    fold_score = roc_auc_score(y_val, val_preds)\n    print(f'Fold {fold} AUC: {fold_score:.4f}')\n    test_preds += model.predict_proba(X_test)[:, 1] / N_SPLITS\n\noverall_auc = roc_auc_score(y, oof_preds)\nprint(f'====================')\nprint(f'Overall OOF AUC: {overall_auc:.4f}')\nprint(f'====================')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:34:23.086658Z","iopub.execute_input":"2025-11-06T05:34:23.08697Z","execution_failed":"2025-11-06T05:34:47.37Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfeature_importances = model.feature_importances_\n\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns, \n    'importance': feature_importances\n})\n\nimportance_df = importance_df.sort_values('importance', ascending=False)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12, 20))\nsns.barplot(x='importance', \n            y='feature', \n            data=importance_df.head(50)) \nplt.title('Feature Importance (Fold5 model)')\nplt.xlabel('Importance Score')\nplt.ylabel('Features')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-06T05:34:47.37Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame({'id': train.id, TARGET: oof_preds}).to_csv(f'oof_xgb_cv_{overall_auc}.csv', index=False)\npd.DataFrame({'id': test.id, TARGET: test_preds}).to_csv(f'test_xgb_cv_{overall_auc}.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-06T05:34:47.37Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}