{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"},{"sourceId":109140027,"sourceType":"kernelVersion"},{"sourceId":120028489,"sourceType":"kernelVersion"},{"sourceId":124388418,"sourceType":"kernelVersion"}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n<center>\n<img src=\"https://cdn.pixabay.com/photo/2020/09/14/10/45/spaceship-5570682__340.jpg\" width=1000 height=1000 />\n</center>","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Hello Kagglers, we have a challenge here, to use our data science skills to solve the cosmic mystery and retrieve the lost passengers. This is an extensive manual on binary classification utilizing the dataset from the Spaceship Titanic.</font>\n\n<font size=\"3\">Table of Contents:</font>\n* <font size=\"3\">1. INTRODUCTION</font>\n* <font size=\"3\"> 2. IMPORTS</font>\n* <font size=\"3\"> 3. EXPLORATORY DATA ANALYSIS</font>\n    - <font size=\"3\">3.1 Target Analysis</font>\n    - <font size=\"3\">3.2 Numerical Feature Analysis</font>\n        - <font size=\"3\">3.2.1 Train Test Distributions</font>\n        - <font size=\"3\">3.2.2 Feature Target Distributions</font>\n        - <font size=\"3\">3.2.3 Bivariate Analysis</font>\n            - <font size=\"3\">3.2.3.1 Pair Plots</font>\n            - <font size=\"3\">3.2.3.2 Violion Plots</font>\n            - <font size=\"3\">3.2.3.3 t-test</font>\n            - <font size=\"3\">3.2.3.4 ANOVA</font>\n            - <font size=\"3\">3.2.3.5 Alternate Method to Pair Plots</font>\n    - <font size=\"3\">3.3 Categorical/Discrete Feature Analysis</font>\n    - <font size=\"3\">3.4 Correlation Plot</font>\n* <font size=\"3\"> 4. DATA CLEANING</font>\n    - <font size=\"3\">4.1 Passenger Group & Cabin</font>\n    - <font size=\"3\">4.2 Name</font>\n    - <font size=\"3\">4.3 Handling Missing Values</font>\n* <font size=\"3\"> 5. FEATURE ENGINEERING</font>\n    - <font size=\"3\">5.1 Numerical Transformations: Transformation Selection Explained</font> \n    - <font size=\"3\">5.2 Encoding Techniques: Multiple Encoding Techniques Implemented</font>\n    - <font size=\"3\">5.3 TFIDF-PCA (Text Transformation)</font>\n    - <font size=\"3\">5.4 Encoding Techniques</font>\n    - <font size=\"3\">5.5 Group Clustered-One Hot Transformation: Something Different!</font>\n    - <font size=\"3\">5.6 Multiplicaive Features</font>\n    - <font size=\"3\">5.7 Less important features</font>\n    - <font size=\"3\">5.8 Feature Elimination/Selection</font>\n* <font size=\"3\"> 6. Scaling Data</font>\n* <font size=\"3\"> 7. Model Development</font>\n   - <font size=\"3\">7.1 Define & Tune Models : 16 Models with Hyperparameter Tuning</font>\n   - <font size=\"3\">7.2 Model Selection</font>\n   - <font size=\"3\">7.3 Ensembling Optimizer </font>\n   - <font size=\"3\">7.4 Model Training</font>\n   - <font size=\"3\">7.5 Feature Importance</font>\n   - <font size=\"3\">7.6 Results</font> \n* <font size=\"3\"> 8. Experimentation</font>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# 2. Import Libraries and Data","metadata":{}},{"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport os\nimport datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom prettytable import PrettyTable\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_notebook\ntqdm_notebook.get_lock().locks = []\n# !pip install sweetviz\n# import sweetviz as sv\nimport concurrent.futures\nfrom copy import deepcopy       \nfrom functools import partial\nfrom itertools import combinations\nimport random\nfrom random import randint, uniform\nimport gc\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom itertools import combinations\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xg\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\nfrom sklearn.cluster import KMeans\n!pip install yellowbrick\nfrom yellowbrick.cluster import KElbowVisualizer\n!pip install gap-stat\nfrom gap_statistic.optimalK import OptimalK\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import boxcox\nimport math\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.base import BaseEstimator, TransformerMixin\n!pip install optuna\nimport optuna\n!pip install cmaes\nimport cmaes\nimport xgboost as xgb\n!pip install catboost\n!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\nimport lightgbm as lgb\n!pip install category_encoders\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import Pool\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.pandas.set_option('display.max_columns',None)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-30T10:50:21.923291Z","iopub.execute_input":"2024-03-30T10:50:21.924284Z","iopub.status.idle":"2024-03-30T10:52:57.438065Z","shell.execute_reply.started":"2024-03-30T10:50:21.924239Z","shell.execute_reply":"2024-03-30T10:52:57.43662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/spaceship-titanic/train.csv')\ntest=pd.read_csv('../input/spaceship-titanic/test.csv')\n\nsub = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\n\ntrain.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:52:57.441794Z","iopub.execute_input":"2024-03-30T10:52:57.442307Z","iopub.status.idle":"2024-03-30T10:52:57.571644Z","shell.execute_reply.started":"2024-03-30T10:52:57.442247Z","shell.execute_reply":"2024-03-30T10:52:57.570254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n## 2.1 Data Description\n1. **PassengerId** - <font size=\"3\">A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</font>\n\n2. **HomePlanet** - <font size=\"3\">The planet the passenger departed from, typically their planet of permanent residence.</font>\n3. **CryoSleep** - <font size=\"3\">Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</font>\n4. **Cabin** - <font size=\"3\">The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</font>\n5. **Destination** - <font size=\"3\">The planet the passenger will be debarking to.</font>\n6. **Age** - <font size=\"3\">The age of the passenger.</font>\n7. **VIP** - Whether the passenger has paid for special VIP service during the voyage.</font>\n8. **RoomService, FoodCourt, ShoppingMall, Spa, VRDeck** - <font size=\"3\">Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.</font>\n9. **Name** - <font size=\"3\">The first and last names of the passenger.</font>\n10. <span style=\"color:blue\">**Transported** - <font size=\"3\">Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict</font>.</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n## 2.2 Check Missing Values","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\n\ntable.field_names = ['Column Name', 'Data Type', 'Non-Null Count']\nfor column in train.columns:\n    data_type = str(train[column].dtype)\n    non_null_count = train[column].count()\n    table.add_row([column, data_type, non_null_count])\nprint(table)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:52:57.573486Z","iopub.execute_input":"2024-03-30T10:52:57.57385Z","iopub.status.idle":"2024-03-30T10:52:57.599312Z","shell.execute_reply.started":"2024-03-30T10:52:57.573815Z","shell.execute_reply":"2024-03-30T10:52:57.598158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.matrix(train)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:52:57.603608Z","iopub.execute_input":"2024-03-30T10:52:57.603954Z","iopub.status.idle":"2024-03-30T10:52:58.468166Z","shell.execute_reply.started":"2024-03-30T10:52:57.603919Z","shell.execute_reply":"2024-03-30T10:52:58.466891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.matrix(test)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:52:58.470522Z","iopub.execute_input":"2024-03-30T10:52:58.471477Z","iopub.status.idle":"2024-03-30T10:52:59.25937Z","shell.execute_reply.started":"2024-03-30T10:52:58.471425Z","shell.execute_reply":"2024-03-30T10:52:59.258409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Looks like we have a many missing values, let's try to deal with them with the help of EDA","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# 3. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n## 3.1 Target Analysis","metadata":{}},{"cell_type":"code","source":"# Calculate the proportion of each class\nclass_counts = train['Transported'].value_counts()\nclass_proportions = class_counts / train.shape[0]\nclass_proportions = class_proportions.values.tolist()\nclass_proportions_str = [f'{prop:.2%}' for prop in class_proportions]\n\n# Set the color palette\ncolors = sns.color_palette('pastel')[0:len(class_counts)]\n\n# Plot the distribution of the target variable\nplt.figure(figsize=(8, 4))\nsns.countplot(x='Transported', data=train, palette=colors)\nplt.title('Distribution of Target Variable', fontsize=16)\nplt.xlabel('Transported', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.ylim([0, len(train)])\nfor i, count in enumerate(class_counts):\n    plt.text(i, count + 50, class_proportions_str[i], ha='center', fontsize=14, color='black')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nsns.despine()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:52:59.261126Z","iopub.execute_input":"2024-03-30T10:52:59.261875Z","iopub.status.idle":"2024-03-30T10:52:59.535715Z","shell.execute_reply.started":"2024-03-30T10:52:59.261831Z","shell.execute_reply":"2024-03-30T10:52:59.534401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">We have about same % of people who got transported and not. So, the data is indeed balanced and accuary is a good metric to choose while we build the models","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n## 3.2 Numerical Features Analysis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.1\"></a>\n### 3.2.1 Train & Test Data Distributions","metadata":{}},{"cell_type":"code","source":"cont_cols=[f for f in train.columns if train[f].dtype in [float,int] and train[f].nunique()>3]\n\n# Create subplots for each continuous column\nfig, axs = plt.subplots(len(cont_cols), 2, figsize=(10,20))\n\n# Loop through each continuous column and plot the histograms\nfor i, col in enumerate(cont_cols):\n    # Determine the range of values to plot\n    max_val = max(train[col].max(), test[col].max())\n    min_val = min(train[col].min(), test[col].min())\n    range_val = max_val - min_val\n    \n    # Determine the bin size and number of bins\n    bin_size = range_val / 20\n    num_bins_train = round(range_val / bin_size)\n    num_bins_test = round(range_val / bin_size)\n    \n    # Plot the histograms\n    sns.histplot(train[col], ax=axs[i][0], color='blue', kde=True, label='Train', bins=num_bins_train)\n    sns.histplot(test[col], ax=axs[i][1], color='red', kde=True, label='Test', bins=num_bins_test)\n    axs[i][0].set_title(f'Train - {col}')\n    axs[i][0].set_xlabel('Value')\n    axs[i][0].set_ylabel('Frequency')\n    axs[i][1].set_title(f'Test - {col}')\n    axs[i][1].set_xlabel('Value')\n    axs[i][1].set_ylabel('Frequency')\n    axs[i][0].legend()\n    axs[i][1].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:52:59.537114Z","iopub.execute_input":"2024-03-30T10:52:59.537463Z","iopub.status.idle":"2024-03-30T10:53:04.344603Z","shell.execute_reply.started":"2024-03-30T10:52:59.537428Z","shell.execute_reply":"2024-03-30T10:53:04.34306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">**Inferences**:\n\n<font size=\"3\">From the distributions of the continuous features, one thing we can clearly understand is that they are skewed and have outliers. So, we can consider options like log transformations","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.2\"></a>\n### 3.2.2 Train Data Distributions across Classes","metadata":{}},{"cell_type":"code","source":"# Create subplots for each continuous feature\nfig, axs = plt.subplots(nrows=len(cont_cols), figsize=(8, 4 * len(cont_cols)))\nfor i, col in enumerate(cont_cols):\n    sns.boxplot(x='Transported', y=col, data=train, ax=axs[i], palette='pastel')\n    axs[i].set_title(f'{col.title()} vs Target', fontsize=16)\n    axs[i].set_xlabel('Transported', fontsize=14)\n    axs[i].set_ylabel(col.title(), fontsize=14)\n    axs[i].tick_params(axis='both', labelsize=14)\n    sns.despine()\n\n# Adjust spacing between subplots\nfig.tight_layout()\n\n# Display the plot\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:53:04.34692Z","iopub.execute_input":"2024-03-30T10:53:04.347386Z","iopub.status.idle":"2024-03-30T10:53:05.706208Z","shell.execute_reply.started":"2024-03-30T10:53:04.347336Z","shell.execute_reply":"2024-03-30T10:53:05.70495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distributions between both the classes tell us that using these features directly into model would hinder the performance.\nThese are the things that we could try:\n1. Create Bins\n2. Use algorithms that are unaffected by outliers","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.3\"></a>\n### 3.2.3 Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.3.1\"></a>\n#### 3.2.3.1 Pair Plots","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data=train, vars=cont_cols, hue='Transported')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:53:05.70796Z","iopub.execute_input":"2024-03-30T10:53:05.708786Z","iopub.status.idle":"2024-03-30T10:53:33.864489Z","shell.execute_reply.started":"2024-03-30T10:53:05.708743Z","shell.execute_reply":"2024-03-30T10:53:33.86271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">**Inferences:**\n1. <font size=\"3\">The plot of Spa vs VRDeck has a good seperation between the classes. It's very clear that people who had spent less money on these were mostly Transported. \n2. <font size=\"3\">The above statement holds true for Spa vs RoomService.\n3. <font size=\"3\">VRDeck, Spa, RoomService have a good differentiation between classes. \n4. <font size=\"3\">We can create a new feature that tells the total expenditure in the above three features. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.3.2\"></a>\n#### 3.2.3.2 Violin Plots","metadata":{}},{"cell_type":"code","source":"# Define the numerical features to plot\nfeatures=[\"Spa\",\"VRDeck\",\"RoomService\"]\n\n# Create a figure with multiple subplots\nfig, axs = plt.subplots(1, len(features), figsize=(16, 5))\n\n# Loop through each feature and plot a violin plot on a separate subplot\nfor i, col in enumerate(features):\n    sns.violinplot(x='Transported', y=col, data=train, ax=axs[i])\n    axs[i].set_title(f'{col.title()} Distribution by Target', fontsize=14)\n    axs[i].set_xlabel('Transported', fontsize=12)\n    axs[i].set_ylabel(col.title(), fontsize=12)\n    sns.despine()\n\n# Adjust spacing between subplots\nfig.tight_layout()\n\n# Display the plot\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:53:33.870874Z","iopub.execute_input":"2024-03-30T10:53:33.871619Z","iopub.status.idle":"2024-03-30T10:53:34.749577Z","shell.execute_reply.started":"2024-03-30T10:53:33.871566Z","shell.execute_reply":"2024-03-30T10:53:34.748288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clear Confirmation from the above Violin plots that the distribution between classes are very different**","metadata":{}},{"cell_type":"markdown","source":"#### 3.2.3.3 t-test","metadata":{}},{"cell_type":"markdown","source":"1. <font size=\"3\">The t-test is a statistical test used to determine whether the means of two groups are significantly different from each other.</font>\n2. <font size=\"3\">The t-test produces a t-value which is used to calculate a p-value. The p-value represents the probability of observing a t-value as extreme or more extreme than the one observed if the null hypothesis (no difference between means) is true.</font>\n3. <font size=\"3\">If the p-value is less than the chosen significance level (usually 0.05), then we reject the null hypothesis and conclude that there is a significant difference between the means. If the p-value is greater than the significance level, then we fail to reject the null hypothesis and conclude that there is not enough evidence to say that there is a significant difference between the means.</font>","metadata":{}},{"cell_type":"code","source":"\ndef perform_ttest(train, feature_list, target):\n    \"\"\"\n    Performs t-test on a list of independent features for a binary classification problem\n    \n    :param train: pandas dataframe containing the training data\n    :param feature_list: list of feature names to perform t-test on\n    :param target: name of the target variable (binary)\n    :return: dictionary containing t-test results\n    \"\"\"\n    ttest_results = {}\n    table = PrettyTable()\n\n    table.field_names = ['Feature', 't_stat', 'p_val']\n    \n    for feature in feature_list:\n        group_0 = train[train[target] == 0][feature]\n        group_1 = train[train[target] == 1][feature]\n        \n        t_stat, p_val = ttest_ind(group_0, group_1, nan_policy='omit')\n        table.add_row([feature,t_stat, p_val ])\n        \n    return print(table)\nperform_ttest(train, cont_cols, 'Transported')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T10:53:34.751198Z","iopub.execute_input":"2024-03-30T10:53:34.751699Z","iopub.status.idle":"2024-03-30T10:53:34.795029Z","shell.execute_reply.started":"2024-03-30T10:53:34.751646Z","shell.execute_reply":"2024-03-30T10:53:34.793569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inferences**\n1. <font size=\"3\">All features except ShoppingMall have p-value less than 0.05, that indicates less than the significance levels and there is difference between the classes for variables.</font>","metadata":{}},{"cell_type":"markdown","source":"#### 3.2.3.4 ANOVA","metadata":{}},{"cell_type":"code","source":"from scipy.stats import f_oneway\n\ndef perform_anova(train, feature_list, target):\n    \"\"\"\n    Performs ANOVA on a list of independent features for a binary classification problem\n    \n    :param train: pandas dataframe containing the training data\n    :param feature_list: list of feature names to perform ANOVA on\n    :param target: name of the target variable (binary)\n    :return: dictionary containing ANOVA results\n    \"\"\"\n    anova_results = {}\n    table = PrettyTable()\n    \n    table.field_names = ['Feature', 'F-statistic', 'p-value']\n    \n    for feature in feature_list:\n        groups = []\n        for group_value in train[target].unique():\n            group = train[train[target] == group_value][feature].dropna()\n            groups.append(group)\n        \n        f_stat, p_val = f_oneway(*groups)\n        table.add_row([feature, f_stat, p_val])\n        \n    return print(table)\n\nperform_anova(train, cont_cols, 'Transported')","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:53:34.798693Z","iopub.execute_input":"2024-03-30T10:53:34.799054Z","iopub.status.idle":"2024-03-30T10:53:34.833994Z","shell.execute_reply.started":"2024-03-30T10:53:34.799018Z","shell.execute_reply":"2024-03-30T10:53:34.832421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.3.5 Alternate Method to Pair Plots","metadata":{}},{"cell_type":"markdown","source":"1. <font size=\"3\">If we have many features, it would be difficult to visually plot and understand all the features. So here is a method that can tell us which pair of features together are really important in the classification task</font>\n2. <font size=\"3\">Let's apply SVM just using a pair of two features and the target feature, see if it's able to do the the job better</font>\n\nThanks @[louiesage](http://www.kaggle.com/louiesage)for the suggestion","metadata":{}},{"cell_type":"code","source":"feature_pairs = list(combinations(cont_cols, 2))\ntable = PrettyTable()\ntable.field_names = ['Feature Pair', 'Accuracy']\n\n# Fill missing values with the mean of the column\nimputer = SimpleImputer(strategy='mean')\ntrain_imputed = imputer.fit_transform(train[cont_cols])\n\nfor pair in feature_pairs:\n    # Using the entire train data to fit, not a CV because it is time consuming\n    x_temp = train_imputed[:, [cont_cols.index(pair[0]), cont_cols.index(pair[1])]]\n    y_temp = train['Transported']\n    model = SVC(gamma='auto')\n    model.fit(x_temp, y_temp)\n    y_pred = model.predict(x_temp)\n    acc = accuracy_score(y_temp, y_pred)\n    table.add_row([pair, acc])\ntable.sortby = 'Accuracy'\ntable.reversesort = True\nprint(table)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:53:34.838077Z","iopub.execute_input":"2024-03-30T10:53:34.838501Z","iopub.status.idle":"2024-03-30T10:55:22.628974Z","shell.execute_reply.started":"2024-03-30T10:53:34.83846Z","shell.execute_reply":"2024-03-30T10:55:22.627538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inferences:**\n1. <font size=\"3\">Earlier from the pair plots, we have established that Spa, VRDeck,& RoomService are really important in the classification. Now, the above method is moore reliable than visualization because we have numbers.</font>\n2. <font size=\"3\">FoodCourt & RoomService together has really good classification ability. Perhaps, we can try to combine all the expenditure features than just combining Spa,VRDeck, & RoomService</font>\n3. <font size=\"3\">A note here is that it is our understanding of the data and the feature enable us to decide what combination makes a better feature. Since, all the top features in the table are expenditure features, it is easy to understand that creating a combined total expenditure would be a better option</font>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n## 3.3 Categorical/Discrete Analysis","metadata":{}},{"cell_type":"code","source":"cat_features=[f for f in train.columns if f not in cont_cols+[\"PassengerId\",\"Name\",\"Transported\"] and train[f].nunique()<50]\ncat_features","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:22.631Z","iopub.execute_input":"2024-03-30T10:55:22.631424Z","iopub.status.idle":"2024-03-30T10:55:22.645373Z","shell.execute_reply.started":"2024-03-30T10:55:22.631383Z","shell.execute_reply":"2024-03-30T10:55:22.644387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.1 Target Distributions","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntarget = 'Transported'\n\n# Create subplots for each categorical feature\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\n\n# Loop through each categorical feature and plot the contingency table in a subplot\nfor i, col in enumerate(cat_features):\n    contingency_table = pd.crosstab(train[col], train[target], normalize='index')\n    contingency_table.plot(kind='bar', stacked=True, ax=axs[i//2, i%2])\n    axs[i//2, i%2].set_title(f\"{col.title()} Distribution by Target\")\n    axs[i//2, i%2].set_xlabel(col.title())\n    axs[i//2, i%2].set_ylabel(\"Proportion\")\n    \n# Adjust spacing between subplots\nfig.tight_layout()\n\n# Show the plot\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:22.646592Z","iopub.execute_input":"2024-03-30T10:55:22.647636Z","iopub.status.idle":"2024-03-30T10:55:23.813965Z","shell.execute_reply.started":"2024-03-30T10:55:22.647594Z","shell.execute_reply":"2024-03-30T10:55:23.812583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inferences**\n1. Less number of people from Earth are saved :(\n2. Cryosleep has good difference in proportions, poeple who are in Cryosleep are  more likely to be Transported\n3. All the categories have differences in distributions in the classes\n","metadata":{}},{"cell_type":"markdown","source":"## 3.4 Correlation Plot","metadata":{}},{"cell_type":"code","source":"features=[f for f in train.columns if train[f].astype!='O' and f not in ['Transported']]\ncorr = train[features].corr()\nplt.figure(figsize = (10, 10), dpi = 300)\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, cmap = sns.diverging_palette(500, 10, as_cmap=True), annot = True, annot_kws = {'size' : 7})\nplt.title('Train Feature Correlation Matrix\\n', fontsize = 25, weight = 'bold')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:23.815884Z","iopub.execute_input":"2024-03-30T10:55:23.816271Z","iopub.status.idle":"2024-03-30T10:55:24.788954Z","shell.execute_reply.started":"2024-03-30T10:55:23.816233Z","shell.execute_reply":"2024-03-30T10:55:24.787655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\"> We can see that they are correlated only to a litlle extent however, our feature engineering techniques might create highly correlated features</font>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# 4. Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n## 4.1 Passenger Group & Cabin","metadata":{}},{"cell_type":"markdown","source":"1. **<font size=\"3\">Passenger Group</font>**: <font size=\"3\">Since it is mentioned in the problem statement that Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</font>\n2. **<font size=\"3\">Cabin</font>**: <font size=\"3\">Cabin has the format Deck/Number/Side.</font>","metadata":{}},{"cell_type":"code","source":"# Extract passenger Groups\ntrain[\"group\"]=train[\"PassengerId\"].str[:-3].astype(int)\ntest[\"group\"]=test[\"PassengerId\"].str[:-3].astype(int)\n\n# Extract Deck \ndef deck(x):\n    x=str(x)\n    if x=='nan':\n        return 'Missing_Deck'\n    else:\n        x=x.split('/')\n        return x[0]\ntrain['cabin_deck']=train[\"Cabin\"].apply(deck)\ntest['cabin_deck']=test[\"Cabin\"].apply(deck)\n\n# Extract the cabin number\ndef num(x):\n    x=str(x)\n    if x=='nan':\n        return None\n    else:\n        x=x.split('/')\n        return int(x[1])\ntrain['cabin_num']=train[\"Cabin\"].apply(num)\ntest['cabin_num']=test[\"Cabin\"].apply(num)\n\n# Extract the Cabin Side\ndef side(x):\n    x=str(x)\n    if x=='nan':\n        return \"Missing_Side\"\n    else:\n        x=x.split('/')\n        return x[2]\n\ntrain['cabin_side']=train[\"Cabin\"].apply(side)\ntest['cabin_side']=test[\"Cabin\"].apply(side)\n\ntrain.drop(columns=[\"Cabin\"],inplace=True)\ntest.drop(columns=[\"Cabin\"],inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:24.790269Z","iopub.execute_input":"2024-03-30T10:55:24.790652Z","iopub.status.idle":"2024-03-30T10:55:24.871199Z","shell.execute_reply.started":"2024-03-30T10:55:24.790616Z","shell.execute_reply":"2024-03-30T10:55:24.869864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n## 4.2 Name ","metadata":{}},{"cell_type":"code","source":"# Extract the last names \ndef name(x):\n    x=str(x)\n    x=x.lower()\n    x=x.strip()\n    x=x.split(\" \")\n    if len(x)>1:\n        return x[-1]\n    else:\n        return (x[0])\n# We first fill the missing names and then extract the last names\ntrain['Name']=train['Name'].fillna(\"No_Name\")\ntest['Name']=test['Name'].fillna(\"No_Name\")\ntrain['Last_Name']=train['Name'].apply(name)\ntest['Last_Name']=test['Name'].apply(name)\nprint(train['Last_Name'].isna().sum(),test['Last_Name'].isna().sum())","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:24.873364Z","iopub.execute_input":"2024-03-30T10:55:24.874161Z","iopub.status.idle":"2024-03-30T10:55:24.910129Z","shell.execute_reply.started":"2024-03-30T10:55:24.874105Z","shell.execute_reply":"2024-03-30T10:55:24.908877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bool to Num\ndef bool_c(x):\n    if x==True:\n        return 1\n    elif x==False:\n        return 0\n    else:\n        return np.nan\ntrain[\"VIP\"]=train[\"VIP\"].apply(bool_c)\ntest[\"VIP\"]=test[\"VIP\"].apply(bool_c)\ntrain[\"CryoSleep\"]=train[\"CryoSleep\"].apply(bool_c)\ntest[\"CryoSleep\"]=test[\"CryoSleep\"].apply(bool_c)\ntrain[\"Transported\"]=train[\"Transported\"].astype(int)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:24.911849Z","iopub.execute_input":"2024-03-30T10:55:24.912249Z","iopub.status.idle":"2024-03-30T10:55:24.947105Z","shell.execute_reply.started":"2024-03-30T10:55:24.912212Z","shell.execute_reply":"2024-03-30T10:55:24.945771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n## 4.3 Handling Missing Values","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.3.1\"></a>\n### 4.3.1 Missing Categorical features","metadata":{}},{"cell_type":"code","source":"# Categorical features\n\nmiss_cat=[feature for feature in train.columns if train[feature].isnull().sum()>0 and train[feature].dtype=='O']\nmiss_cat","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:24.948875Z","iopub.execute_input":"2024-03-30T10:55:24.949278Z","iopub.status.idle":"2024-03-30T10:55:24.97282Z","shell.execute_reply.started":"2024-03-30T10:55:24.94924Z","shell.execute_reply":"2024-03-30T10:55:24.971876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the missing percentages for both train and test data\ntrain_missing_pct = train[miss_cat].isnull().mean() * 100\ntest_missing_pct = test[miss_cat].isnull().mean() * 100\n\n# Combine the missing percentages for train and test data into a single dataframe\nmissing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train %', 'Test%'])\n\n# Print the missing percentage dataframe\nprint(missing_pct_df)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T10:55:24.974924Z","iopub.execute_input":"2024-03-30T10:55:24.975264Z","iopub.status.idle":"2024-03-30T10:55:24.997989Z","shell.execute_reply.started":"2024-03-30T10:55:24.975231Z","shell.execute_reply":"2024-03-30T10:55:24.996758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Both train and test datasets have around the same missing values. For now, we can use some analysis and fill them.\nThere is also an option to drop the rows with all four features missing</font>","metadata":{}},{"cell_type":"markdown","source":"**Iterative CatBoost Imputer**: <font size=\"3\"> Please refer to my notebook [here](https://www.kaggle.com/code/arunklenin/ps3e15-iterative-catboost-imputer-ensemble?scriptVersionId=130271409) in which I implemented this method for the first time</font>","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:55:24.999573Z","iopub.execute_input":"2024-03-30T10:55:24.999965Z","iopub.status.idle":"2024-03-30T10:55:25.03007Z","shell.execute_reply.started":"2024-03-30T10:55:24.999928Z","shell.execute_reply":"2024-03-30T10:55:25.028444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in miss_cat:\n    train[feature].fillna(\"missing_\"+feature,inplace=True)\n    test[feature].fillna(\"missing_\"+feature,inplace=True)\n    \n# Calculate the missing percentages for both train and test data\ntrain_missing_pct = train[miss_cat].isnull().mean() * 100\ntest_missing_pct = test[miss_cat].isnull().mean() * 100\n\n# Combine the missing percentages for train and test data into a single dataframe\nmissing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train %', 'Test%'])\n\n# Print the missing percent\nprint(missing_pct_df)\n\n# cat_params={\n#             'depth': 7,\n#             'learning_rate': 0.1,\n#             'l2_leaf_reg': 0.7,\n#             'random_strength': 0.2,\n#             'max_bin': 200,\n#             'od_wait': 65,\n#             'one_hot_max_size': 70,\n#             'grow_policy': 'Depthwise',\n#             'bootstrap_type': 'Bayesian',\n#             'od_type': 'Iter',\n#             'eval_metric': 'MultiClass',\n#             'loss_function': 'MultiClass',\n# }\n# def store_missing_rows(df, features):\n#     missing_rows = {}\n    \n#     for feature in features:\n#         missing_rows[feature] = df[df[feature].isnull()]\n    \n#     return missing_rows\n\n\n# def fill_missing_categorical(train,test,target, features, max_iterations=10):\n    \n#     df=pd.concat([train.drop(columns=[target,\"PassengerId\"]),test.drop(columns=['PassengerId'])],axis=\"rows\")\n#     df=df.reset_index(drop=True)\n    \n#     # Step 1: Store the instances with missing values in each feature\n#     missing_rows = store_missing_rows(df, features)\n    \n#     # Step 2: Initially fill all missing values with \"Missing\"\n#     for f in features:\n#         df[f]=df[f].fillna(\"Missing_\"+f)\n# #     df[features] = df[features].fillna(\"Missing\")\n    \n#     for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n#         for feature in features:\n#             # Skip features with no missing values\n#             rows_miss = missing_rows[feature].index\n            \n#             missing_temp = df.loc[rows_miss].copy()\n#             non_missing_temp = df.drop(index=rows_miss).copy()\n#             missing_temp = missing_temp.drop(columns=[feature])\n            \n#             cat_features = [x for x in df.columns if df[x].dtype==\"O\" and x != feature]\n            \n#             # Step 3: Use the remaining features to predict missing values using Random Forests\n#             X_train = non_missing_temp.drop(columns=[feature])\n#             y_train = non_missing_temp[[feature]]\n            \n#             catboost_classifier = CatBoostClassifier(**cat_params)\n#             catboost_classifier.fit(X_train, y_train, cat_features=cat_features, verbose=False)\n            \n#             # Step 4: Predict missing values for the feature and update all N features\n#             y_pred = catboost_classifier.predict(missing_temp)\n#             df.loc[rows_miss, feature] = y_pred\n#     train[features] = np.array(df.iloc[:train.shape[0]][features])\n#     test[features] = np.array(df.iloc[train.shape[0]:][features])\n    \n#     return train,test\n\n\n# train ,test= fill_missing_categorical(train,test,\"Transported\",miss_cat,10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:25.032272Z","iopub.execute_input":"2024-03-30T10:55:25.032759Z","iopub.status.idle":"2024-03-30T10:55:25.062967Z","shell.execute_reply.started":"2024-03-30T10:55:25.032698Z","shell.execute_reply":"2024-03-30T10:55:25.061374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3.2\"></a>\n### 4.3.2 Missing Numerical features","metadata":{}},{"cell_type":"code","source":"miss_cont=[feature for feature in train.columns if train[feature].isnull().sum()>0 and train[feature].dtype!='O' and feature not in ['Transported']]\nmiss_cont","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:25.064344Z","iopub.execute_input":"2024-03-30T10:55:25.064684Z","iopub.status.idle":"2024-03-30T10:55:25.09258Z","shell.execute_reply.started":"2024-03-30T10:55:25.064651Z","shell.execute_reply":"2024-03-30T10:55:25.091216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the missing percentages for both train and test data\ntrain_missing_pct = train[miss_cont].isnull().mean() * 100\ntest_missing_pct = test[miss_cont].isnull().mean() * 100\n\n# Combine the missing percentages for train and test data into a single dataframe\nmissing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train %', 'Test%'])\n\n# Print the missing percentage dataframe\nprint(missing_pct_df)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:25.094224Z","iopub.execute_input":"2024-03-30T10:55:25.094639Z","iopub.status.idle":"2024-03-30T10:55:25.110186Z","shell.execute_reply.started":"2024-03-30T10:55:25.0946Z","shell.execute_reply":"2024-03-30T10:55:25.108866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Most of the features are expenditure features and my hypothesis is that if someone is in CryoSleep, it is not possible to spend money in these activities.</font>","metadata":{}},{"cell_type":"code","source":"# First lets fill CryoSleep, based on totdal expenditure\nexp_features=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\ntrain[\"Expenditure\"]=train[exp_features].sum(axis=\"columns\")\ntest[\"Expenditure\"]=test[exp_features].sum(axis=\"columns\")\n\n# Zero expenditure indicate that they are in CryoSleep\ntrain['CryoSleep']=np.where(train['Expenditure']==0,1,0)\ntest['CryoSleep']=np.where(test['Expenditure']==0,1,0)\n\n# Also, if they are VIPs, they probably would not choose to be in CryoSleep\ntrain['VIP']=np.where(train['CryoSleep']==0,1,0)\ntest['VIP']=np.where(test['CryoSleep']==0,1,0)\n\ntrain.drop(columns=[\"Expenditure\"],inplace=True)\ntest.drop(columns=[\"Expenditure\"],inplace=True)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T10:55:25.112512Z","iopub.execute_input":"2024-03-30T10:55:25.11339Z","iopub.status.idle":"2024-03-30T10:55:25.135004Z","shell.execute_reply.started":"2024-03-30T10:55:25.113279Z","shell.execute_reply":"2024-03-30T10:55:25.133897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:55:25.136712Z","iopub.execute_input":"2024-03-30T10:55:25.137438Z","iopub.status.idle":"2024-03-30T10:55:25.170871Z","shell.execute_reply.started":"2024-03-30T10:55:25.137383Z","shell.execute_reply":"2024-03-30T10:55:25.169866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in exp_features:\n    train[col]=np.where(train[\"CryoSleep\"]==1,0,train[col])\n    test[col]=np.where(test[\"CryoSleep\"]==1,0,test[col])    \n    \n# Calculate the missing percentages for both train and test data\ntrain_missing_pct = train[miss_cont].isnull().mean() * 100\ntest_missing_pct = test[miss_cont].isnull().mean() * 100\n\n# Combine the missing percentages for train and test data into a single dataframe\nmissing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train %', 'Test%'])\n\n# Print the missing percentage dataframe\nprint(missing_pct_df)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T10:55:25.184213Z","iopub.execute_input":"2024-03-30T10:55:25.184643Z","iopub.status.idle":"2024-03-30T10:55:25.211138Z","shell.execute_reply.started":"2024-03-30T10:55:25.184601Z","shell.execute_reply":"2024-03-30T10:55:25.209683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">Now we have filled almost many of them, the rest can be filled with KNN Imputer</font>","metadata":{}},{"cell_type":"code","source":"miss_cont=[feature for feature in train.columns if train[feature].isnull().sum()>0 and train[feature].dtype!='O' and feature not in ['Transported']]\nmiss_cont\nimputer=KNNImputer(n_neighbors=5)\ntrain[miss_cont]=imputer.fit_transform(train[miss_cont])\ntest[miss_cont]=imputer.transform(test[miss_cont])\n\n# Calculate the missing percentages for both train and test data\ntrain_missing_pct = train[miss_cont].isnull().mean() * 100\ntest_missing_pct = test[miss_cont].isnull().mean() * 100\n\n# Combine the missing percentages for train and test data into a single dataframe\nmissing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train %', 'Test%'])\n\n# Print the missing percentage dataframe\nprint(missing_pct_df)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T10:55:25.212479Z","iopub.execute_input":"2024-03-30T10:55:25.212835Z","iopub.status.idle":"2024-03-30T10:55:26.075804Z","shell.execute_reply.started":"2024-03-30T10:55:25.212799Z","shell.execute_reply":"2024-03-30T10:55:26.07442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cb_params = {\n#             'iterations': 500,\n#             'depth': 6,\n#             'learning_rate': 0.02,\n#             'l2_leaf_reg': 0.5,\n#             'random_strength': 0.2,\n#             'max_bin': 150,\n#             'od_wait': 80,\n#             'one_hot_max_size': 70,\n#             'grow_policy': 'Depthwise',\n#             'bootstrap_type': 'Bayesian',\n#             'od_type': 'IncToDec',\n#             'eval_metric': 'RMSE',\n#             'loss_function': 'RMSE',\n#             'random_state': 42,\n#         }\n# def rmse(y1,y2):\n#     return(np.sqrt(mean_squared_error(y1,y2)))\n\n# def fill_missing_numerical(train,test,target, features, max_iterations=10):\n    \n#     df=pd.concat([train.drop(columns=[target,\"PassengerId\"]),test.drop(columns=\"PassengerId\")],axis=\"rows\")\n#     df=df.reset_index(drop=True)\n    \n#     # Step 1: Store the instances with missing values in each feature\n#     missing_rows = store_missing_rows(df, features)\n    \n#     # Step 2: Initially fill all missing values with \"Missing\"\n#     for f in features:\n#         df[f]=df[f].fillna(df[f].mean())\n    \n#     cat_features=[f for f in df.columns if df[f].dtype==\"O\"]\n#     dictionary = {feature: [] for feature in features}\n    \n#     for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n#         for feature in features:\n#             # Skip features with no missing values\n#             rows_miss = missing_rows[feature].index\n            \n#             missing_temp = df.loc[rows_miss].copy()\n#             non_missing_temp = df.drop(index=rows_miss).copy()\n#             y_pred_prev=missing_temp[feature]\n#             missing_temp = missing_temp.drop(columns=[feature])\n            \n            \n#             # Step 3: Use the remaining features to predict missing values using Random Forests\n#             X_train = non_missing_temp.drop(columns=[feature])\n#             y_train = non_missing_temp[[feature]]\n            \n#             catboost_classifier = CatBoostRegressor(**cb_params)\n#             catboost_classifier.fit(X_train, y_train,cat_features=cat_features, verbose=False)\n            \n#             # Step 4: Predict missing values for the feature and update all N features\n#             y_pred = catboost_classifier.predict(missing_temp)\n#             df.loc[rows_miss, feature] = y_pred\n#             error_minimize=rmse(y_pred,y_pred_prev)\n#             dictionary[feature].append(error_minimize)  # Append the error_minimize value\n\n#     for feature, values in dictionary.items():\n#         iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n#         plt.plot(iterations, values, label=feature)  # plot the values\n#         plt.xlabel('Iterations')\n#         plt.ylabel('RMSE')\n#         plt.title('Minimization of RMSE with iterations')\n#         plt.legend()\n#         plt.show()\n#     train[features] = np.array(df.iloc[:train.shape[0]][features])\n#     test[features] = np.array(df.iloc[train.shape[0]:][features])\n\n#     return train,test\n\n\n# train,test = fill_missing_numerical(train,test,\"Transported\",miss_cont,20)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:55:26.077725Z","iopub.execute_input":"2024-03-30T10:55:26.078536Z","iopub.status.idle":"2024-03-30T10:55:26.088694Z","shell.execute_reply.started":"2024-03-30T10:55:26.078486Z","shell.execute_reply":"2024-03-30T10:55:26.087233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# 5. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">We have already seen in EDA that VRDeck, Spa, RoomService have good distisgunshing capability. Let us combine the features to create an expenditure feature in these categories and also the total expenditure</font>","metadata":{}},{"cell_type":"code","source":"train[\"expenditure\"]=train[\"VRDeck\"]+train[\"Spa\"]+train[\"RoomService\"]\ntest[\"expenditure\"]=test[\"VRDeck\"]+test[\"Spa\"]+test[\"RoomService\"]\n\n# train[\"total_exp\"]=train[\"VRDeck\"]+train[\"Spa\"]+train[\"RoomService\"]+train['FoodCourt']+train['ShoppingMall']\n# test[\"total_exp\"]=test[\"VRDeck\"]+test[\"Spa\"]+test[\"RoomService\"]+train['FoodCourt']+train['ShoppingMall']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:26.090547Z","iopub.execute_input":"2024-03-30T10:55:26.090939Z","iopub.status.idle":"2024-03-30T10:55:26.104961Z","shell.execute_reply.started":"2024-03-30T10:55:26.090902Z","shell.execute_reply":"2024-03-30T10:55:26.10376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_feat=[f for f in train.columns if train[f].dtype!=\"O\" and train[f].nunique()>10] # The rest are discrete/categorical\nnum_feat","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:26.106668Z","iopub.execute_input":"2024-03-30T10:55:26.107132Z","iopub.status.idle":"2024-03-30T10:55:26.126986Z","shell.execute_reply.started":"2024-03-30T10:55:26.107055Z","shell.execute_reply":"2024-03-30T10:55:26.125998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.1.1\"></a>\n# 5.1 Numerical Feature Transformations","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">We're going to see what transformation works better for each feature and select them, the idea is to compress the data. There could be situations where you will have to stretch the data. These are the methods applied:</font>\n<font size=\"3\">\n1. <font size=\"3\"> **Log Transformation**</font>: <font size=\"3\">This transformation involves taking the logarithm of each data point. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                            y = log(x)\n\n2. <font size=\"3\">**Square Root Transformation**</font>: <font size=\"3\">This transformation involves taking the square root of each data point. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                            y = sqrt(x)\n\n3. <font size=\"3\">**Box-Cox Transformation**</font>: <font size=\"3\">This transformation is a family of power transformations that includes the log and square root transformations as special cases. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                            y = [(x^lambda) - 1] / lambda if lambda != 0\n                            y = log(x) if lambda = 0\n\n4. <font size=\"3\">**Yeo-Johnson Transformation**</font>: <font size=\"3\">This transformation is similar to the Box-Cox transformation, but it can be applied to both positive and negative values. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                            y = [(|x|^lambda) - 1] / lambda if x >= 0, lambda != 0\n                            y = log(|x|) if x >= 0, lambda = 0\n                            y = -[(|x|^lambda) - 1] / lambda if x < 0, lambda != 2\n                            y = -log(|x|) if x < 0, lambda = 2\n\n5. <font size=\"3\">**Power Transformation**</font>: <font size=\"3\">This transformation involves raising each data point to a power. It is useful when the data is highly skewed and the variance increases with the mean. The power can be any value, and is often determined using statistical methods such as the Box-Cox or Yeo-Johnson transformations.</font>\n                            y = [(x^lambda) - 1] / lambda if method = \"box-cox\" and lambda != 0\n                            y = log(x) if method = \"box-cox\" and lambda = 0\n                            y = [(x + 1)^lambda - 1] / lambda if method = \"yeo-johnson\" and x >= 0, lambda != 0\n                            y = log(x + 1) if method = \"yeo-johnson\" and x >= 0, lambda = 0\n                            y = [-(|x| + 1)^lambda - 1] / lambda if method = \"yeo-johnson\" and x < 0, lambda != 2\n                            y = -log(|x| + 1) if method = \"yeo-johnson\" and x < 0, lambda = 2","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Let's also do a grouped clustering follwed by a WOE encoding on these numerical features</font>","metadata":{}},{"cell_type":"code","source":"# Below are the functions to decide the decision boundaries in order to maximize Accuracy/ f1-score\ndef f1_cutoff(precisions, recalls, thresholds):\n    a=precisions*recalls/(recalls+precisions)\n    b=sorted(zip(a,thresholds))\n    return b[-1][1]\ndef acc_cutoff(y_valid, y_pred_valid):\n    y_valid=np.array(y_valid)\n    y_pred_valid=np.array(y_pred_valid)\n    fpr, tpr, threshold = metrics.roc_curve(y_valid, y_pred_valid)\n    pred_valid = pd.DataFrame({'label': y_pred_valid})\n    thresholds = np.array(threshold)\n    pred_labels = (pred_valid['label'].values > thresholds[:, None]).astype(int)\n    acc_scores = (pred_labels == y_valid).mean(axis=1)\n    acc_df = pd.DataFrame({'threshold': threshold, 'test_acc': acc_scores})\n    acc_df.sort_values(by='test_acc', ascending=False, inplace=True)\n    cutoff = acc_df.iloc[0, 0]\n    return cutoff\n    \nsc=MinMaxScaler()\nunimportant_features=[]\ntable = PrettyTable()\ndt_params= {'min_samples_split': 80, 'min_samples_leaf': 30, 'max_depth': 8, 'criterion': 'absolute_error'}\n\ntable.field_names = ['Original Feature', 'Original Accuracy(CV-TRAIN)', 'Transformed Feature', 'Tranformed Accuracy(CV-TRAIN)']\nfor col in num_feat:\n    \n    # Log Transformation after MinMax Scaling(keeps data between 0 and 1)\n    train[\"log_\"+col]=np.log1p(sc.fit_transform(train[[col]]))\n    test[\"log_\"+col]=np.log1p(sc.transform(test[[col]]))\n    \n    # Square Root Transformation\n    train[\"sqrt_\"+col]=np.sqrt(sc.fit_transform(train[[col]]))\n    test[\"sqrt_\"+col]=np.sqrt(sc.transform(test[[col]]))\n    \n    # Box-Cox transformation\n    transformer = PowerTransformer(method='box-cox')\n    train[\"bx_cx_\"+col] = transformer.fit_transform(sc.fit_transform(train[[col]])+1) # adjusted to make it +ve\n    test[\"bx_cx_\"+col] = transformer.transform(sc.transform(test[[col]])+1)\n    \n    # Yeo-Johnson transformation\n    transformer = PowerTransformer(method='yeo-johnson')\n    train[\"y_J_\"+col] = transformer.fit_transform(train[[col]])\n    test[\"y_J_\"+col] = transformer.transform(test[[col]])\n    \n    # Power transformation, 0.25\n    power_transform = lambda x: np.power(x, 0.25) \n    transformer = FunctionTransformer(power_transform)\n    train[\"pow_\"+col] = transformer.fit_transform(sc.fit_transform(train[[col]]))\n    test[\"pow_\"+col] = transformer.transform(sc.transform(test[[col]]))\n    \n    # Power transformation, 0.1\n    power_transform = lambda x: np.power(x, 0.1) \n    transformer = FunctionTransformer(power_transform)\n    train[\"pow2_\"+col] = transformer.fit_transform(sc.fit_transform(train[[col]]))\n    test[\"pow2_\"+col] = transformer.transform(sc.transform(test[[col]]))\n    \n    # log to power transformation\n    train[\"log_pow2\"+col]=np.log1p(train[\"pow2_\"+col])\n    test[\"log_pow2\"+col]=np.log1p(test[\"pow2_\"+col])\n    \n    temp_cols=[col,\"log_\"+col,\"sqrt_\"+col, \"bx_cx_\"+col,\"y_J_\"+col ,\"pow_\"+col,\"pow2_\"+col,\"log_pow2\"+col ]\n    \n    # Fill na becaue, it would be Nan if the vaues are negative and a transformation applied on it\n    train[temp_cols]=train[temp_cols].fillna(0)\n    test[temp_cols]=test[temp_cols].fillna(0)\n    \n    #Apply PCA on  the features and compute an additional column\n    pca=TruncatedSVD(n_components=1)\n    x_pca_train=pca.fit_transform(train[temp_cols])\n    x_pca_test=pca.transform(test[temp_cols])\n    x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n    x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n    temp_cols.append(col+\"_pca_comb\")\n    #print(temp_cols)\n    \n    train=pd.concat([train,x_pca_train],axis='columns')\n    test=pd.concat([test,x_pca_test],axis='columns')\n    \n    # See which transformation along with the original is giving you the best univariate fit with target\n    kf=KFold(n_splits=10, shuffle=True, random_state=42)\n    \n    ACC=[]\n    \n    for f in temp_cols:\n        X=train[[f]].values\n        y=train[\"Transported\"].values\n        \n        acc=[]\n        for train_idx, val_idx in kf.split(X,y):\n            X_train,y_train=X[train_idx],y[train_idx]\n            x_val,y_val=X[val_idx],y[val_idx]\n            \n            model=LogisticRegression()\n#             model=DecisionTreeRegressor(**dt_params)\n            model.fit(X_train,y_train)\n            y_pred=model.predict_proba(x_val)[:,1]\n            precisions,recalls, thresholds=precision_recall_curve(y_val,y_pred)\n            \n#             cutoff=f1_cutoff(precisions,recalls, thresholds)\n            cutoff=acc_cutoff(y_val,y_pred)\n#             print(cutoff)\n            predicted =pd.DataFrame()\n            predicted[\"Transported\"] = y_pred\n            y_pred=np.where(predicted[\"Transported\"]>float(cutoff),1,0)\n            acc.append(accuracy_score(y_val,y_pred))\n        ACC.append((f,np.mean(acc)))\n        if f==col:\n            orig_acc=np.mean(acc)\n    best_col, best_acc=sorted(ACC, key=lambda x:x[1], reverse=True)[0]\n    \n    cols_to_drop = [f for f in temp_cols if  f!= best_col]\n#     print(cols_to_drop)\n    final_selection=[f for f in temp_cols if f not in cols_to_drop]\n    if cols_to_drop:\n        unimportant_features=unimportant_features+cols_to_drop\n    table.add_row([col,orig_acc,best_col ,best_acc])\nprint(table)    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:26.128761Z","iopub.execute_input":"2024-03-30T10:55:26.129413Z","iopub.status.idle":"2024-03-30T10:55:39.676663Z","shell.execute_reply.started":"2024-03-30T10:55:26.12937Z","shell.execute_reply":"2024-03-30T10:55:39.675316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n# 5.2 Categorical Features","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">For each categorical variable, perform the following encoding techniques:</font>\n1. <font size=\"3\">**Count/Frequency Encoding**</font>: <font size=\"3\">Count the number of occurrences of each category and replace the category with its log count.</font>\n2. <font size=\"3\">**Count Labeling**</font>: <font size=\"3\">Assign a label to each category based on its count, with higher counts receiving higher labels.</font>\n3. <font size=\"3\"> **WOE Binning**</font>: <font size=\"3\">Calculate the Weight of Evidence (WOE) for each category based on the target variable, where higher WOE values indicate a higher likelihood of the target variable being 1</font>\n4. <font size=\"3\"> **Target-Guided Mean Encoding**</font>: <font size=\"3\">Rank the categories based on the mean of target column across each category</font>\n5. <font size=\"3\"> **Group Clustering**</font>: <font size=\"3\">All the features created from the above mentioned encdoing techniques will be grouped and clustered followed by a Log transformation of Target-mean across clusters</font>\n6. <font size=\"3\"> **One-Hot Encoding**</font>: <font size=\"3\">Instead of applying OHE on individual features, OHE will be applied on the clusters created from all encoded features</font>\n\n<font size=\"3\"> Finally, the encoding technique will be selected based on their Accuracy CV performance on single feature model</font>","metadata":{}},{"cell_type":"code","source":"cat_features=[*set([feature for feature in train.columns if train[feature].nunique()<=10 or train[feature].dtype=='O'])-set([\"PassengerId\",\"Transported\",\"VIP\",\"CryoSleep\"])]\ntrain[cat_features].nunique()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T10:55:39.67874Z","iopub.execute_input":"2024-03-30T10:55:39.679228Z","iopub.status.idle":"2024-03-30T10:55:39.740756Z","shell.execute_reply.started":"2024-03-30T10:55:39.679175Z","shell.execute_reply":"2024-03-30T10:55:39.739292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.3 TFIDF-PCA (Text Transformation)","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">*Name* and *Last_Name* has a lot of categies, so let me first handle them sing some text transformation techniques. If this doesn't work, I will drop this column<font>\n\n<font size=\"3\">Applied **TFIDF** Text transformation creating 1000 vectors and then applied **PCA** to reduce it to 10 columns</font>","metadata":{}},{"cell_type":"code","source":"def tf_idf(train, test, column,n,p):\n    vectorizer=TfidfVectorizer(max_features=n)\n    vectors_train=vectorizer.fit_transform(train[column])\n    vectors_test=vectorizer.transform(test[column])\n    \n    svd=TruncatedSVD(p)\n    x_pca_train=svd.fit_transform(vectors_train)\n    x_pca_test=svd.transform(vectors_test)\n    tfidf_df_train=pd.DataFrame(x_pca_train)\n    tfidf_df_test=pd.DataFrame(x_pca_test)\n\n    \n    cols=[(column+\"_tfidf_\"+str(f)) for f in tfidf_df_train.columns]\n    tfidf_df_train.columns=cols\n    tfidf_df_test.columns=cols\n    train=pd.concat([train,tfidf_df_train], axis=\"columns\")\n    test=pd.concat([test,tfidf_df_test], axis=\"columns\")\n    \n    return (train, test)\n\n(train,test)=tf_idf(train,test,\"Last_Name\",1000,5)\ntrain.drop(columns=[\"Name\",\"Last_Name\"], inplace=True)\ntest.drop(columns=[\"Name\",\"Last_Name\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:55:39.742729Z","iopub.execute_input":"2024-03-30T10:55:39.743484Z","iopub.status.idle":"2024-03-30T10:55:39.912391Z","shell.execute_reply.started":"2024-03-30T10:55:39.743429Z","shell.execute_reply":"2024-03-30T10:55:39.91047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.4 Encoding Techniques","metadata":{}},{"cell_type":"code","source":"cat_features=['HomePlanet', 'cabin_deck', 'Destination', 'cabin_side']\ntable = PrettyTable()\ntable.field_names = ['Feature', 'Encoded Feature', \"Accuracy (CV)- Logistic regression\"]\n\ndef OHE(train,test,cols,target):\n    combined = pd.concat([train, test], axis=0)\n    for col in cols:\n        one_hot = pd.get_dummies(combined[col])\n        counts = combined[col].value_counts()\n        min_count_category = counts.idxmin()\n        one_hot = one_hot.drop(min_count_category, axis=1)\n        combined = pd.concat([combined, one_hot], axis=\"columns\")\n        combined = combined.drop(col, axis=1)\n        combined = combined.loc[:, ~combined.columns.duplicated()]\n    \n    # split back to train and test dataframes\n    train_ohe = combined[:len(train)]\n    test_ohe = combined[len(train):]\n    test_ohe.reset_index(inplace=True,drop=True)\n    test_ohe.drop(columns=[target],inplace=True)\n    \n    return train_ohe, test_ohe\n\nfor feature in cat_features:\n    ## Target Guided Mean --Data Leakage Possible\n    \n    cat_labels=train.groupby([feature])['Transported'].mean().sort_values().index\n    cat_labels2={k:i for i,k in enumerate(cat_labels,0)}\n    train[feature+\"_target\"]=train[feature].map(cat_labels2)\n    test[feature+\"_target\"]=test[feature].map(cat_labels2)\n    \n    ## Count Encoding\n    \n    dic=train[feature].value_counts().to_dict()\n    train[feature+\"_count\"]=np.log1p(train[feature].map(dic))\n    test[feature+\"_count\"]=np.log1p(test[feature].map(dic))\n\n    \n    ## Count Labeling\n    \n    dic2=train[feature].value_counts().to_dict()\n    list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n    # list1=np.arange(len(dic2.values())) # Higher rank for low count\n    dic3=dict(zip(list(dic2.keys()),list1))\n    train[feature+\"_count_label\"]=train[feature].replace(dic3)\n    test[feature+\"_count_label\"]=test[feature].replace(dic3)\n\n    \n    ## WOE Binning\n    cat_labels=np.log1p(train.groupby([feature])['Transported'].sum()/(train.groupby([feature])['Transported'].count()-train.groupby([feature])['Transported'].sum()))#.sort_values().index\n    cat_labels2=cat_labels.to_dict()\n    train[feature+\"_WOE\"]=train[feature].map(cat_labels2)\n    test[feature+\"_WOE\"]=test[feature].map(cat_labels2)\n    \n    \n    temp_cols=[feature+\"_target\", feature+\"_count\", feature+\"_count_label\",feature+\"_WOE\"]\n    \n    \n    # It is possible to have NaN values in the test data when new categories are seen\n    imputer=KNNImputer(n_neighbors=5)\n    train[temp_cols]=imputer.fit_transform(train[temp_cols])\n    test[temp_cols]=imputer.transform(test[temp_cols])\n    \n    \n    if train[feature].dtype!=\"O\":\n        temp_cols.append(feature)\n    else:\n        train.drop(columns=[feature],inplace=True)\n        test.drop(columns=[feature],inplace=True)\n    # Also, doing a group clustering on all encoding types and an additional one-hot on the clusters\n    \n    temp_train=train[temp_cols]\n    temp_test=test[temp_cols]\n    \n    sc=StandardScaler()\n    temp_train=sc.fit_transform(temp_train)\n    temp_test=sc.transform(temp_test)\n    model = KMeans()\n\n\n    # Initialize the KElbowVisualizer with the KMeans model and desired range of clusters\n    visualizer = KElbowVisualizer(model, k=(3, 15), metric='calinski_harabasz', timings=False)\n\n    # Fit the visualizer to the data\n    visualizer.fit(np.array(temp_train))\n\n    ideal_clusters = visualizer.elbow_value_\n    plt.xlabel('Number of clusters (k)')\n    plt.ylabel('Calinski-Harabasz Index')\n    plt.title(\"Clustering on encoded featured from \"+feature)\n    plt.show()\n    print(ideal_clusters)\n    if ideal_clusters is not None:\n        \n        kmeans = KMeans(n_clusters=ideal_clusters)\n        kmeans.fit(np.array(temp_train))\n        labels_train = kmeans.labels_\n\n        train[feature+'_cat_cluster_WOE'] = labels_train\n        test[feature+'_cat_cluster_WOE'] = kmeans.predict(np.array(temp_test))\n\n        train[feature+'_cat_OHE_cluster']=feature+\"_OHE_\"+train[feature+'_cat_cluster_WOE'].astype(str)\n        test[feature+'_cat_OHE_cluster']=feature+\"_OHE_\"+test[feature+'_cat_cluster_WOE'].astype(str)\n\n        train, test=OHE(train,test, [feature+'_cat_OHE_cluster'],\"Transported\")\n\n        cat_labels=cat_labels=np.log1p(train.groupby([feature+'_cat_cluster_WOE'])['Transported'].mean())\n        cat_labels2=cat_labels.to_dict()\n        train[feature+'_cat_cluster_WOE']=train[feature+'_cat_cluster_WOE'].map(cat_labels2)\n        test[feature+'_cat_cluster_WOE']=test[feature+'_cat_cluster_WOE'].map(cat_labels2)\n        \n        temp_cols=temp_cols+[feature+'_cat_cluster_WOE']\n    else:\n        print(\"No good clusters were found, skipped without clustering and OHE\")\n        \n\n    \n    \n    \n    # See which transformation along with the original is giving you the best univariate fit with target\n    skf=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n    \n    accuaries=[]\n    \n    for f in temp_cols:\n        X=train[[f]].values\n        y=train[\"Transported\"].values\n        \n        acc=[]\n        for train_idx, val_idx in skf.split(X,y):\n            X_train,y_train=X[train_idx],y[train_idx]\n            x_val,y_val=X[val_idx],y[val_idx]\n            \n            model=LogisticRegression()\n            model.fit(X_train,y_train)\n            y_pred=model.predict_proba(x_val)[:,1]\n            precisions,recalls, thresholds=precision_recall_curve(y_val,y_pred)\n#             cutoff=f1_cutoff(precisions,recalls, thresholds)\n            cutoff=acc_cutoff(y_val,y_pred)\n#             print(cutoff)\n            predicted =pd.DataFrame()\n            predicted[\"Transported\"] = y_pred\n            y_pred=np.where(predicted[\"Transported\"]>float(cutoff),1,0)\n            acc.append(accuracy_score(y_val,y_pred))\n        accuaries.append((f,np.mean(acc)))\n    best_col, best_acc=sorted(accuaries, key=lambda x:x[1], reverse=True)[0]\n    \n    # check correlation between best_col and other columns and drop if correlation >0.75\n    corr = train[temp_cols].corr(method='pearson')\n    corr_with_best_col = corr[best_col]\n    cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.75 and f != best_col]\n    final_selection=[f for f in temp_cols if f not in cols_to_drop]\n    if cols_to_drop:\n        train = train.drop(columns=cols_to_drop)\n        test = test.drop(columns=cols_to_drop)\n    table.add_row([feature,best_col ,best_acc])\nprint(table)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:55:39.915644Z","iopub.execute_input":"2024-03-30T10:55:39.916581Z","iopub.status.idle":"2024-03-30T10:56:51.084924Z","shell.execute_reply.started":"2024-03-30T10:55:39.916507Z","shell.execute_reply":"2024-03-30T10:56:51.083433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\"> No column with WOE Encoding has been selected</font>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.3\"></a>\n## 5.5 Clustering-One Hot Transformation","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Let's take the unimportant feartures we created using transformations and use them to create clusters followed by a one hot encoding on them. We wil apply this on each subset of original features</font>","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\ntable.field_names = ['Cluster WOE Feature', 'MAE(CV-TRAIN)']\nfor col in num_feat:\n    sub_set=[f for f in unimportant_features if col in f]\n    print(sub_set)\n    temp_train=train[sub_set]\n    temp_test=test[sub_set]\n    sc=StandardScaler()\n    temp_train=sc.fit_transform(temp_train)\n    temp_test=sc.transform(temp_test)\n    model = KMeans()\n\n\n    # Initialize the KElbowVisualizer with the KMeans model and desired range of clusters\n    visualizer = KElbowVisualizer(model, k=(3, 25), metric='calinski_harabasz', timings=False)\n\n    # Fit the visualizer to the data\n    visualizer.fit(np.array(temp_train))\n    plt.xlabel('Number of clusters (k)')\n    plt.ylabel('Calinski-Harabasz Index')\n    plt.show()\n\n    ideal_clusters = visualizer.elbow_value_\n    if ideal_clusters is None:\n        ideal_clusters=25\n\n    # print(ideal_clusters)\n    kmeans = KMeans(n_clusters=ideal_clusters)\n    kmeans.fit(np.array(temp_train))\n    labels_train = kmeans.labels_\n\n    train[col+'_OHE_cluster'] = labels_train\n    test[col+'_OHE_cluster'] = kmeans.predict(np.array(temp_test))\n    # Also, making a copy to do mean encoding followed by a log transformation\n    \n    train[col+\"_unimp_cluster_WOE\"]=train[col+'_OHE_cluster']\n    test[col+\"_unimp_cluster_WOE\"]=test[col+'_OHE_cluster'] \n    cat_labels=cat_labels=np.log1p(train.groupby([col+\"_unimp_cluster_WOE\"])['Transported'].mean())\n    cat_labels2=cat_labels.to_dict()\n    train[col+\"_unimp_cluster_WOE\"]=train[col+\"_unimp_cluster_WOE\"].map(cat_labels2)\n    test[col+\"_unimp_cluster_WOE\"]=test[col+\"_unimp_cluster_WOE\"].map(cat_labels2)\n\n    X=train[[col+\"_unimp_cluster_WOE\"]].values\n    y=train[\"Transported\"].values\n\n    ACC=[]\n    for train_idx, val_idx in kf.split(X,y):\n        X_train,y_train=X[train_idx],y[train_idx]\n        x_val,y_val=X[val_idx],y[val_idx]\n\n        model=LogisticRegression()\n        model.fit(X_train,y_train)\n        y_pred=model.predict_proba(x_val)[:,1]\n        precisions,recalls, thresholds=precision_recall_curve(y_val,y_pred)\n#             cutoff=f1_cutoff(precisions,recalls, thresholds)\n        cutoff=acc_cutoff(y_val,y_pred)\n#             print(cutoff)\n        predicted =pd.DataFrame()\n        predicted[\"Transported\"] = y_pred\n        y_pred=np.where(predicted[\"Transported\"]>float(cutoff),1,0)\n        ACC.append(accuracy_score(y_val,y_pred))\n    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(ACC)])\n    \n    train[col+'_OHE_cluster']=col+\"_OHE_\"+train[col+'_OHE_cluster'].astype(str)\n    test[col+'_OHE_cluster']=col+\"_OHE_\"+test[col+'_OHE_cluster'].astype(str)\n    train, test=OHE(train,test,[col+'_OHE_cluster'],\"Transported\")\nprint(table)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:56:51.089597Z","iopub.execute_input":"2024-03-30T10:56:51.090002Z","iopub.status.idle":"2024-03-30T11:01:13.325664Z","shell.execute_reply.started":"2024-03-30T10:56:51.089957Z","shell.execute_reply":"2024-03-30T11:01:13.324511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.6 Multiplicative Features","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">In this section, a new product feature if created on by multiplying all continuous original features. The final selection of features depend on the Accuracy values with a cutoff</font>","metadata":{}},{"cell_type":"code","source":"# from itertools import combinations\n# # num_features=[f for f in train.columns if train[f].nunique()>100 and f not in ['Transported',\"PassengerId\"]]\n# feature_pairs = list(combinations(num_feat, 2))\n\n# table = PrettyTable()\n# table.field_names = ['Pair Features', 'Accuracy(CV-TRAIN)', \"Selected\"]\n\n\n# selected_features=[]\n# max_product=float('-inf')\n# for pair in feature_pairs:\n#     col1, col2 = pair\n# #     print(pair)\n#     product_col_train = train[col1] * train[col2]\n#     product_col_test= test[col1] * test[col2]\n#     name=f'{col1}_{col2}_product'\n#     train[name] = product_col_train\n#     test[name] = product_col_test\n#     max_product = max(max_product, product_col_train.max())\n\n#     kf=KFold(n_splits=5, shuffle=True, random_state=42)\n#     MAE=[]\n#     X=train[[name]].values\n#     y=train[\"Transported\"].values\n\n#     ACC=[]\n#     for train_idx, val_idx in kf.split(X,y):\n#         X_train,y_train=X[train_idx],y[train_idx]\n#         x_val,y_val=X[val_idx],y[val_idx]\n\n#         model=LogisticRegression()\n#         model.fit(X_train,y_train)\n#         y_pred=model.predict_proba(x_val)[:,1]\n#         precisions,recalls, thresholds=precision_recall_curve(y_val,y_pred)\n# #             cutoff=f1_cutoff(precisions,recalls, thresholds)\n#         cutoff=acc_cutoff(y_val,y_pred)\n# #             print(cutoff)\n#         predicted =pd.DataFrame()\n#         predicted[\"Transported\"] = y_pred\n#         y_pred=np.where(predicted[\"Transported\"]>float(cutoff),1,0)\n#         ACC.append(accuracy_score(y_val,y_pred))\n#     if np.mean(ACC)<0.7:\n#         unimportant_features.append(name)\n#         selected=\"No\"\n#     else:\n#         selected_features.append(pair)\n#         selected=\"Yes\"\n#     table.add_row([pair,np.mean(ACC),selected ])\n# table.sortby = 'Accuracy(CV-TRAIN)'\n# table.reversesort = True\n# print(table)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T11:01:13.327194Z","iopub.execute_input":"2024-03-30T11:01:13.328366Z","iopub.status.idle":"2024-03-30T11:01:13.33459Z","shell.execute_reply.started":"2024-03-30T11:01:13.328295Z","shell.execute_reply":"2024-03-30T11:01:13.333615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.7 Less Important Features","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">There are a lot of features created and many of them are not important/highly correlated, the first level of reduction is to create subsets based on the original features, apply PCA to select PC1 and drop the subset</font>","metadata":{}},{"cell_type":"code","source":"print(\"Number of Unimportant Features are \",len(unimportant_features))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:01:13.336214Z","iopub.execute_input":"2024-03-30T11:01:13.337415Z","iopub.status.idle":"2024-03-30T11:01:13.353236Z","shell.execute_reply.started":"2024-03-30T11:01:13.337361Z","shell.execute_reply":"2024-03-30T11:01:13.351854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.reset_index(inplace=True,drop=True)\nfor col in cont_cols:\n    sub_set=[f for f in unimportant_features if col in f]\n    \n    existing=[f for f in train.columns if f in sub_set]\n    temp_train=train[existing]\n    temp_test=test[existing]\n    sc=StandardScaler()\n    temp_train=sc.fit_transform(temp_train)\n    temp_test=sc.transform(temp_test)\n    \n    pca=TruncatedSVD(n_components=1)\n    x_pca_train=pca.fit_transform(temp_train)\n    x_pca_test=pca.transform(temp_test)\n    x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_unimp\"])\n    x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_unimp\"])\n    \n    train=pd.concat([train,x_pca_train],axis='columns')\n    test=pd.concat([test,x_pca_test],axis='columns')\n    for f in sub_set:\n        if f in train.columns and f not in cont_cols:\n            train=train.drop(columns=[f])\n            test=test.drop(columns=[f])","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:01:13.355037Z","iopub.execute_input":"2024-03-30T11:01:13.355517Z","iopub.status.idle":"2024-03-30T11:01:14.600191Z","shell.execute_reply.started":"2024-03-30T11:01:13.355468Z","shell.execute_reply":"2024-03-30T11:01:14.599095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.8 Feature Selection","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\"> We have create a lot of columns from transformations, clustering, encoding, PCA. Let's look at the correlation between all the features derived fro the initial numerical features</font>","metadata":{}},{"cell_type":"code","source":"num_derived_list=[]\nfor f1 in train.columns:\n    for f2 in num_feat:\n        if f2 in f1:\n            num_derived_list.append(f1)\nnum_derived_list=[*set(num_derived_list)]      \n                       \ncorr = train[num_derived_list].corr()\nplt.figure(figsize = (40, 40), dpi = 300)\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, cmap = sns.diverging_palette(500, 10, as_cmap=True), annot = True, annot_kws = {'size' : 8})\nplt.title('Post-Feature Engineering Correlation Matrix\\n', fontsize = 10, weight = 'bold')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:01:14.602253Z","iopub.execute_input":"2024-03-30T11:01:14.602966Z","iopub.status.idle":"2024-03-30T11:01:39.34119Z","shell.execute_reply.started":"2024-03-30T11:01:14.602913Z","shell.execute_reply":"2024-03-30T11:01:39.339509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not so much correlation but there are red/green spots, so let's reduce them\n**Steps to Eliminate Correlated Fruit Features**:\n1. <font size=\"3\">Group features based on their parent feature. For example, all features derived from Age come under one set</font>\n2. <font size=\"3\">Apply PCA on the set, Cluster-Target Encoding on the set</font>\n3. <font size=\"3\">See the performance of each feature on a cross-validated single feature-target model</font>\n4. <font size=\"3\">Select the feature with highest CV-MAE</font>","metadata":{}},{"cell_type":"code","source":"final_drop_list=[]\n\ntable = PrettyTable()\ntable.field_names = ['Original', 'Final Transformed feature', \"Accuray(CV)- Logistic Regression\"]\n\nthreshold=0.8\n# It is possible that multiple parent features share same child features, so storing selected features to avoid selecting the same feature again\nbest_cols=[]\n\nfor col in num_feat:\n    sub_set=[f for f in num_derived_list if col in f]\n    # print(sub_set)\n    \n    correlated_features = []\n\n    # Loop through each feature\n    for i, feature in enumerate(sub_set):\n        # Check correlation with all remaining features\n        for j in range(i+1, len(sub_set)):\n            correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n            # If correlation is greater than threshold, add to list of highly correlated features\n            if correlation > threshold:\n                correlated_features.append(sub_set[j])\n\n    # Remove duplicate features from the list\n    correlated_features = list(set(correlated_features))\n    if len(correlated_features)>1:\n\n        temp_train=train[correlated_features]\n        temp_test=test[correlated_features]\n        #Scale before applying PCA\n        sc=StandardScaler()\n        temp_train=sc.fit_transform(temp_train)\n        temp_test=sc.transform(temp_test)\n\n        # Initiate PCA\n        pca=TruncatedSVD(n_components=1)\n        x_pca_train=pca.fit_transform(temp_train)\n        x_pca_test=pca.transform(temp_test)\n        x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n        x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n        train=pd.concat([train,x_pca_train],axis='columns')\n        test=pd.concat([test,x_pca_test],axis='columns')\n\n        # Clustering\n        model = KMeans()\n\n\n        # Initialize the KElbowVisualizer with the KMeans model and desired range of clusters\n        visualizer = KElbowVisualizer(model, k=(10, 25), metric='calinski_harabasz', timings=False)\n\n        # Fit the visualizer to the data\n        visualizer.fit(np.array(temp_train))\n        plt.xlabel('Number of clusters (k)')\n        plt.ylabel('Calinski-Harabasz Index')\n        plt.title(\"Clustering on features from \"+col)\n        plt.show()\n\n        ideal_clusters = visualizer.elbow_value_\n        \n        if ideal_clusters is None:\n            ideal_clusters=10\n\n        # print(ideal_clusters)\n        kmeans = KMeans(n_clusters=ideal_clusters)\n        kmeans.fit(np.array(temp_train))\n        labels_train = kmeans.labels_\n\n        train[col+'_final_cluster'] = labels_train\n        test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n\n        cat_labels=cat_labels=np.log1p(train.groupby([col+\"_final_cluster\"])['Transported'].mean())\n        cat_labels2=cat_labels.to_dict()\n        train[col+\"_final_cluster\"]=train[col+\"_final_cluster\"].map(cat_labels2)\n        test[col+\"_final_cluster\"]=test[col+\"_final_cluster\"].map(cat_labels2)\n\n        correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n        # See which transformation along with the original is giving you the best univariate fit with target\n        kf=KFold(n_splits=5, shuffle=True, random_state=42)\n\n        ACC=[]\n\n        for f in correlated_features:\n            X=train[[f]].values\n            y=train[\"Transported\"].values\n\n            acc=[]\n            for train_idx, val_idx in kf.split(X,y):\n                X_train,y_train=X[train_idx],y[train_idx]\n                x_val,y_val=X[val_idx],y[val_idx]\n\n                model=LogisticRegression()\n                model.fit(X_train,y_train)\n                y_pred=model.predict_proba(x_val)[:,1]\n                precisions,recalls, thresholds=precision_recall_curve(y_val,y_pred)\n                cutoff=acc_cutoff(y_val,y_pred)\n                predicted =pd.DataFrame()\n                predicted[\"Transported\"] = y_pred\n                y_pred=np.where(predicted[\"Transported\"]>float(cutoff),1,0)\n                acc.append(accuracy_score(y_val,y_pred))\n\n            if f not in best_cols:\n                ACC.append((f,np.mean(acc)))\n        best_col, best_acc=sorted(ACC, key=lambda x:x[1], reverse=True)[0]\n        best_cols.append(best_col)\n\n        cols_to_drop = [f for f in correlated_features if  f not in  best_cols]\n        if cols_to_drop:\n            final_drop_list=final_drop_list+cols_to_drop\n        table.add_row([col,best_col ,best_acc])\n    else:\n        print(f\"All features for {col} have correlation less than threshold\")\n        table.add_row([col,\"All features selected\" ,\"--\"])\nprint(table)      ","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:01:39.343312Z","iopub.execute_input":"2024-03-30T11:01:39.343728Z","iopub.status.idle":"2024-03-30T11:05:11.141632Z","shell.execute_reply.started":"2024-03-30T11:01:39.343688Z","shell.execute_reply":"2024-03-30T11:05:11.139569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_drop_list=[f for f in final_drop_list if f not in cont_cols]\ntrain.drop(columns=[*set(final_drop_list)],inplace=True)\ntest.drop(columns=[*set(final_drop_list)],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:11.143702Z","iopub.execute_input":"2024-03-30T11:05:11.144871Z","iopub.status.idle":"2024-03-30T11:05:11.160845Z","shell.execute_reply.started":"2024-03-30T11:05:11.144752Z","shell.execute_reply":"2024-03-30T11:05:11.159434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# 6. Scaling the Data","metadata":{}},{"cell_type":"code","source":"feature_scale=[feature for feature in train.columns if feature not in ['PassengerId','Transported']]\nscaler=StandardScaler()\n\ntrain[feature_scale]=scaler.fit_transform(train[feature_scale])\ntest[feature_scale]=scaler.transform(test[feature_scale])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:11.163228Z","iopub.execute_input":"2024-03-30T11:05:11.164104Z","iopub.status.idle":"2024-03-30T11:05:11.20821Z","shell.execute_reply.started":"2024-03-30T11:05:11.164016Z","shell.execute_reply":"2024-03-30T11:05:11.206893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ID=test[['PassengerId']]\ntrain.drop(columns=['PassengerId'],inplace=True)\ntest.drop(columns=['PassengerId'],inplace=True)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T11:05:11.210036Z","iopub.execute_input":"2024-03-30T11:05:11.210544Z","iopub.status.idle":"2024-03-30T11:05:11.235737Z","shell.execute_reply.started":"2024-03-30T11:05:11.210488Z","shell.execute_reply":"2024-03-30T11:05:11.232016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=train.drop(['Transported'],axis=1)\ny_train=train['Transported']\n\nX_test=test.copy()\nprint(X_train.shape,X_test.shape)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-03-30T11:05:11.23708Z","iopub.execute_input":"2024-03-30T11:05:11.2379Z","iopub.status.idle":"2024-03-30T11:05:11.249726Z","shell.execute_reply.started":"2024-03-30T11:05:11.237839Z","shell.execute_reply":"2024-03-30T11:05:11.247965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# 7. Model Development","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7.1\"></a>\n## 7.1 Define and Tune Models","metadata":{}},{"cell_type":"markdown","source":"### 7.1.1 ANNs","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\"> If you want to apply ANNs, you can uncomment the below section. But since we had many outliers and in consideration of the training time, I would prefer tree based/other ML models.</font>","metadata":{}},{"cell_type":"code","source":"# !pip install tensorflow\nimport tensorflow\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.layers import Dropout","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:11.251989Z","iopub.execute_input":"2024-03-30T11:05:11.252531Z","iopub.status.idle":"2024-03-30T11:05:19.312501Z","shell.execute_reply.started":"2024-03-30T11:05:11.252475Z","shell.execute_reply":"2024-03-30T11:05:19.311123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd=tensorflow.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True)\nrms = tensorflow.keras.optimizers.RMSprop()\nnadam=tensorflow.keras.optimizers.Nadam(\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n)\nlrelu = lambda x: tensorflow.keras.activations.relu(x, alpha=0.1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.314387Z","iopub.execute_input":"2024-03-30T11:05:19.315561Z","iopub.status.idle":"2024-03-30T11:05:19.421854Z","shell.execute_reply.started":"2024-03-30T11:05:19.315514Z","shell.execute_reply":"2024-03-30T11:05:19.420643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann = Sequential()\nann.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='he_uniform', activation=lrelu))\nann.add(Dropout(0.1))\nann.add(Dense(16,  kernel_initializer='he_uniform', activation=lrelu))\nann.add(Dropout(0.1))\n# model.add(Dense(32,  kernel_initializer='he_uniform', activation='relu'))\n# model.add(Dropout(0.1))\n\nann.add(Dense(1,  kernel_initializer='he_uniform', activation='sigmoid'))\nann.compile(loss=\"binary_crossentropy\", optimizer=nadam,metrics=['accuracy'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.423786Z","iopub.execute_input":"2024-03-30T11:05:19.424456Z","iopub.status.idle":"2024-03-30T11:05:19.551744Z","shell.execute_reply.started":"2024-03-30T11:05:19.424398Z","shell.execute_reply":"2024-03-30T11:05:19.550759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.2 XGBoost Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# A=time.time()\n# # Set up the XGBoost classifier with default hyperparameters\n# xgb_params = {\n#     'n_estimators': 500,\n#     'learning_rate': 0.05,\n#     'max_depth': 7,\n#     'subsample': 1.0,\n#     'colsample_bytree': 1.0,\n#     'n_jobs': -1,\n#     'eval_metric': 'logloss',\n#     'objective': 'binary:logistic',\n#     'verbosity': 0,\n#     'random_state': 1,\n# }\n# model = xgb.XGBClassifier(**xgb_params)\n\n# # Define the hyperparameters to tune and their search ranges\n# param_dist = {\n#     'n_estimators': np.arange(50, 1000,50),\n#     'max_depth': np.arange(3, 15,2),\n#     'learning_rate': np.arange(0.001, 0.05,0.004),\n#     'subsample': [0.1,0.3,0.5,0.7,0.9],\n#     'colsample_bytree': [0.1,0.3,0.5,0.7,0.9],\n# }\n\n# # Set up the RandomizedSearchCV object with cross-validation\n# random_search = RandomizedSearchCV(model, param_distributions=param_dist, cv=3, n_iter=50, random_state=1, n_jobs=-1)\n\n# # Fit the RandomizedSearchCV object to the training data\n# random_search.fit(X_train, y_train)\n\n# # Print the best hyperparameters and corresponding mean cross-validation score\n# print(\"Best hyperparameters: \", random_search.best_params_)\n# print(\"Best mean cross-validation score: {:.3f}\".format(random_search.best_score_))\n\n# # Evaluate the best model on the test data\n# best_model = random_search.best_estimator_\n# print(best_model)\n\n# xgb_params=random_search.best_params_\n\n# B=time.time()\n# print((B-A)/60) ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.553234Z","iopub.execute_input":"2024-03-30T11:05:19.55413Z","iopub.status.idle":"2024-03-30T11:05:19.561053Z","shell.execute_reply.started":"2024-03-30T11:05:19.554078Z","shell.execute_reply":"2024-03-30T11:05:19.559778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params={'colsample_bytree': 0.8498791800104656, 'learning_rate': 0.020233442882782587, 'max_depth': 4, 'n_estimators': 469, 'subsample': 0.746529796772373}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.563224Z","iopub.execute_input":"2024-03-30T11:05:19.564016Z","iopub.status.idle":"2024-03-30T11:05:19.574049Z","shell.execute_reply.started":"2024-03-30T11:05:19.563966Z","shell.execute_reply":"2024-03-30T11:05:19.573152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.3 LightGBM Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# # Set up the LightGBM classifier with default hyperparameters\n# lgb_params = {\n#     'n_estimators': 100,\n#     'max_depth': 7,\n#     'learning_rate': 0.05,\n#     'subsample': 0.2,\n#     'colsample_bytree': 0.56,\n#     'reg_alpha': 0.25,\n#     'reg_lambda': 5e-08,\n#     'objective': 'binary',\n#     'metric': 'accuracy',\n#     'boosting_type': 'gbdt',\n#     'device': 'cpu',\n#     'random_state': 1,\n# }\n# model = lgb.LGBMClassifier(**lgb_params)\n\n# # Define the hyperparameters to tune and their search ranges\n# param_dist = {\n#     'n_estimators': np.arange(50, 1000,50),\n#     'max_depth': np.arange(3, 15,2),\n#     'learning_rate': np.arange(0.001, 0.02,0.002),\n#     'subsample': [0.1,0.3,0.5,0.7,0.9],\n#     'colsample_bytree': [0.1,0.3,0.5,0.7,0.9],\n#     'reg_alpha': [uniform(0, 1),uniform(0, 1),uniform(0, 1),uniform(0, 1)],\n#     'reg_lambda': [uniform(0, 1),uniform(0, 1),uniform(0, 1),uniform(0, 1)],\n# }\n\n# # Set up the RandomizedSearchCV object with cross-validation\n# random_search = RandomizedSearchCV(model, param_distributions=param_dist, cv=3, n_iter=20, random_state=1, n_jobs=-1)\n\n# # Fit the RandomizedSearchCV object to the training data\n# random_search.fit(X_train, y_train)\n\n# # Print the best hyperparameters and corresponding mean cross-validation score\n# print(\"Best hyperparameters: \", random_search.best_params_)\n# print(\"Best mean cross-validation score: {:.3f}\".format(random_search.best_score_))\n\n# # Evaluate the best model on the test data\n# best_model = random_search.best_estimator_\n\n# lgb_params=random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.575739Z","iopub.execute_input":"2024-03-30T11:05:19.576535Z","iopub.status.idle":"2024-03-30T11:05:19.585033Z","shell.execute_reply.started":"2024-03-30T11:05:19.576485Z","shell.execute_reply":"2024-03-30T11:05:19.583826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params={'colsample_bytree': 0.7774799983649324, 'learning_rate': 0.007653648135411494, 'max_depth': 5, 'n_estimators': 350, 'reg_alpha': 0.14326300616140863, 'reg_lambda': 0.9310129332502252, 'subsample': 0.6189257947519665}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.586842Z","iopub.execute_input":"2024-03-30T11:05:19.587224Z","iopub.status.idle":"2024-03-30T11:05:19.599701Z","shell.execute_reply.started":"2024-03-30T11:05:19.587186Z","shell.execute_reply":"2024-03-30T11:05:19.598289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.4 CatBoost Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# # define the hyperparameter search space\n# param_distributions = {\n#     'depth':  np.arange(3, 15,2),\n#     'learning_rate': np.arange(0.001, 0.02,0.002),\n#     'l2_leaf_reg': [0.1, 0.5, 0.7],\n#     'random_strength': [0.1, 0.2, 0.3],\n#     'max_bin': [100, 150, 200],\n#     'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide'],\n#     'bootstrap_type': ['Bayesian', 'Bernoulli'],\n#     'one_hot_max_size': [10, 50, 70],\n# }\n\n# # create a CatBoostClassifier model with default parameters\n# model = CatBoostClassifier(iterations=200, eval_metric='Accuracy', loss_function='Logloss', task_type='CPU')\n\n# # perform random search with cross-validation\n# random_search = RandomizedSearchCV(\n#     estimator=model,\n#     param_distributions=param_distributions,\n#     n_iter=50,  # number of parameter settings that are sampled\n#     scoring='neg_log_loss',  # use negative log-loss as the evaluation metric\n#     cv=3,  # 5-fold cross-validation\n#     verbose=1,\n#     random_state=42\n# )\n\n# # fit the random search object to the training data\n# random_search.fit(X_train, y_train)\n\n# # print the best parameters and best score\n# print('Best score:', -1 * random_search.best_score_)\n# print('Best parameters:', random_search.best_params_)\n\n# cat_params=random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.601852Z","iopub.execute_input":"2024-03-30T11:05:19.602398Z","iopub.status.idle":"2024-03-30T11:05:19.611196Z","shell.execute_reply.started":"2024-03-30T11:05:19.60234Z","shell.execute_reply":"2024-03-30T11:05:19.609966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_params={'random_strength': 0.1, 'one_hot_max_size': 10, 'max_bin': 100, 'learning_rate': 0.01, 'l2_leaf_reg': 0.5, 'grow_policy': 'Lossguide', 'depth': 5, 'bootstrap_type': 'Bernoulli'}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.615096Z","iopub.execute_input":"2024-03-30T11:05:19.615602Z","iopub.status.idle":"2024-03-30T11:05:19.62475Z","shell.execute_reply.started":"2024-03-30T11:05:19.615561Z","shell.execute_reply":"2024-03-30T11:05:19.623491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.5 Logistic Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# # define the hyperparameter search space\n# param_grid = {\n#     'penalty': ['l1', 'l2', 'elasticnet'],\n#     'C': [0.001,0.01, 0.1, 1, 10, 100],\n#     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n# }\n\n# # create a LogisticRegression model with default parameters\n# model = LogisticRegression(max_iter=500, random_state=2023)\n\n# # perform grid search with cross-validation\n# grid_search = GridSearchCV(\n#     estimator=model,\n#     param_grid=param_grid,\n#     scoring='roc_auc',  # use accuracy as the evaluation metric\n#     cv=5,  # 5-fold cross-validation\n#     verbose=1,\n#     n_jobs=-1\n# )\n\n# # fit the grid search object to the training data\n# grid_search.fit(X_train, y_train)\n\n# # print the best parameters and best score\n# print('Best score:', grid_search.best_score_)\n# print('Best parameters:', grid_search.best_params_)\n# lg_params=grid_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.628883Z","iopub.execute_input":"2024-03-30T11:05:19.629521Z","iopub.status.idle":"2024-03-30T11:05:19.638514Z","shell.execute_reply.started":"2024-03-30T11:05:19.629481Z","shell.execute_reply":"2024-03-30T11:05:19.637314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_params={'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.640251Z","iopub.execute_input":"2024-03-30T11:05:19.640805Z","iopub.status.idle":"2024-03-30T11:05:19.65125Z","shell.execute_reply.started":"2024-03-30T11:05:19.640762Z","shell.execute_reply":"2024-03-30T11:05:19.650223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.6 Random Forests Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# # define the hyperparameter search space\n# param_distributions = {\n#     'n_estimators': [100, 200, 300, 400, 500],\n#     'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, None],\n#     'max_features': ['sqrt', 'log2', None],\n#     'min_samples_split': [2, 5, 10],\n#     'min_samples_leaf': [1, 2, 4],\n#     'bootstrap': [True, False]\n# }\n\n# # create a RandomForestClassifier model with default parameters\n# model = RandomForestClassifier(bootstrap=False, max_depth=4, max_features='sqrt',\n#                        min_samples_leaf=2, min_samples_split=5,\n#                        n_estimators=341, random_state=42)\n\n# # perform random search with cross-validation\n# random_search = RandomizedSearchCV(\n#     estimator=model,\n#     param_distributions=param_distributions,\n#     n_iter=15,  # number of parameter settings that are sampled\n#     scoring='accuracy',  # use accuracy as the evaluation metric\n#     cv=5,  # 5-fold cross-validation\n#     verbose=1,\n#     n_jobs=-1,\n#     random_state=42\n# )\n\n# # fit the random search object to the training data\n# random_search.fit(X_train, y_train)\n# best_model = random_search.best_estimator_\n# # print the best parameters and best score\n# print('Best score:', random_search.best_score_)\n# print('Best parameters:', random_search.best_params_)\n# rf_params=random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.652349Z","iopub.execute_input":"2024-03-30T11:05:19.652712Z","iopub.status.idle":"2024-03-30T11:05:19.663132Z","shell.execute_reply.started":"2024-03-30T11:05:19.652677Z","shell.execute_reply":"2024-03-30T11:05:19.662048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_params={'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 6, 'bootstrap': True}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.664572Z","iopub.execute_input":"2024-03-30T11:05:19.664902Z","iopub.status.idle":"2024-03-30T11:05:19.676594Z","shell.execute_reply.started":"2024-03-30T11:05:19.664869Z","shell.execute_reply":"2024-03-30T11:05:19.67555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.7 HistGBM Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"\n# # Define the hyperparameter grid to search\n# param_grid = {\n#     'learning_rate': [0.01, 0.05, 0.1, 0.2],\n#     'max_depth': [3, 5, 7, 9],\n#     'max_leaf_nodes': [15, 31, 63, 127],\n#     'min_samples_leaf': [1, 3, 5, 7],\n#     'l2_regularization': np.logspace(-4, 1, 6),\n#     'max_bins': [32, 64, 128, 256],\n#     'random_state': [42]\n# }\n\n# # Create a HistGradientBoostingClassifier object\n# clf = HistGradientBoostingClassifier(max_iter=2000)\n\n# # Create a RandomizedSearchCV object\n# random_search = RandomizedSearchCV(\n#     clf, \n#     param_distributions=param_grid,\n#     n_iter=30, # number of parameter settings that are sampled\n#     cv=3, # cross-validation generator\n#     scoring='accuracy',\n#     n_jobs=-1,\n#     random_state=42\n# )\n\n# # Fit the RandomizedSearchCV object on the training data\n# random_search.fit(X_train, y_train)\n\n# # Print the best hyperparameters\n# print(random_search.best_params_)\n\n# histGBM_params=random_search.best_params_\n# histGBM_params.pop('random_state')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.677955Z","iopub.execute_input":"2024-03-30T11:05:19.678308Z","iopub.status.idle":"2024-03-30T11:05:19.68653Z","shell.execute_reply.started":"2024-03-30T11:05:19.678271Z","shell.execute_reply":"2024-03-30T11:05:19.685188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.8 GBM Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# # Define the hyperparameter search space\n# param_dist = {\n#     'n_estimators': np.arange(100, 1000, 50),\n#     'learning_rate': np.logspace(-4, 0, num=100),\n#     'max_depth': [2, 3, 4, 5, 6],\n#     'min_samples_split': [2, 3, 4, 5, 6],\n#     'min_samples_leaf': [1, 2, 3, 4, 5],\n#     'max_features': ['sqrt', 'log2', None]\n# }\n\n# # Create the GradientBoostingClassifier model\n# model = GradientBoostingClassifier(max_depth=4, max_features='sqrt',\n#                                    min_samples_leaf=2, min_samples_split=5,\n#                                    n_estimators=341, random_state=42)\n\n# # Create the random search object\n# random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=100,\n#                                    cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n\n# # Fit the random search object to the data\n# random_search.fit(X_train, y_train)\n# best_model = random_search.best_estimator_\n\n# # Print the best parameters and best score\n# print(\"Best parameters: \", random_search.best_params_)\n# print(\"Best score: \", random_search.best_score_)\n\n# gbm_params=random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.688161Z","iopub.execute_input":"2024-03-30T11:05:19.688555Z","iopub.status.idle":"2024-03-30T11:05:19.700062Z","shell.execute_reply.started":"2024-03-30T11:05:19.688517Z","shell.execute_reply":"2024-03-30T11:05:19.69883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm_params={'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 5, 'learning_rate': 0.0004430621457583882}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.701275Z","iopub.execute_input":"2024-03-30T11:05:19.701608Z","iopub.status.idle":"2024-03-30T11:05:19.713148Z","shell.execute_reply.started":"2024-03-30T11:05:19.701567Z","shell.execute_reply":"2024-03-30T11:05:19.712045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.9 SVM Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# # Define the hyperparameter space\n# param_distributions = {\n#     'C': uniform(0.1, 10),\n#     'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n#     'degree': randint(1, 10),\n#     'gamma': ['scale', 'auto'] + list(uniform(0.01, 1).rvs(10)),\n# }\n\n# # Define the model\n# model = SVC(probability=True)\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(model, param_distributions, n_iter=5, cv=3, n_jobs=-1, random_state=1)\n\n# # Fit the random search to the data\n# random_search.fit(X_train[:1000], y_train[:1000])\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# svm_params=random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.714946Z","iopub.execute_input":"2024-03-30T11:05:19.715294Z","iopub.status.idle":"2024-03-30T11:05:19.723055Z","shell.execute_reply.started":"2024-03-30T11:05:19.715257Z","shell.execute_reply":"2024-03-30T11:05:19.721964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.10 KNN Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"\n# # Define the hyperparameter space\n# param_distributions = {\n#     'n_neighbors': np.arange(2, 20,2),\n#     'weights': ['uniform', 'distance'],\n#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n#     'leaf_size': randint(1, 100),\n#     'p': [1, 2],\n# }\n\n# # Define the model\n# model = KNeighborsClassifier()\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(model, param_distributions, n_iter=20, cv=5, n_jobs=-1, random_state=1)\n\n# # Fit the random search to the data\n# random_search.fit(X_train, y_train)\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# knn_params = random_search.best_params_\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.724413Z","iopub.execute_input":"2024-03-30T11:05:19.724748Z","iopub.status.idle":"2024-03-30T11:05:19.732802Z","shell.execute_reply.started":"2024-03-30T11:05:19.724716Z","shell.execute_reply":"2024-03-30T11:05:19.73185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.11 MLP Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# # Define the hyperparameter space\n# param_distributions = {\n#     'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n#     'activation': ['logistic', 'tanh', 'relu'],\n#     'solver': ['lbfgs', 'adam'],\n#     'alpha': uniform(0.0001, 0.1),\n#     'learning_rate': ['constant', 'invscaling', 'adaptive'],\n#     'learning_rate_init': uniform(0.0001, 0.1),\n# }\n\n# # Define the model\n# model = MLPClassifier(random_state=42, max_iter=1000)\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(model, param_distributions, n_iter=20, cv=5, n_jobs=-1, random_state=1)\n\n# # Fit the random search to the data\n# random_search.fit(X_train[:1000], y_train[:1000])\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# mlp_params = random_search.best_params_\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.734378Z","iopub.execute_input":"2024-03-30T11:05:19.734963Z","iopub.status.idle":"2024-03-30T11:05:19.745697Z","shell.execute_reply.started":"2024-03-30T11:05:19.734916Z","shell.execute_reply":"2024-03-30T11:05:19.74453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.12 GPC Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# from sklearn.gaussian_process import GaussianProcessClassifier\n# from sklearn.gaussian_process.kernels import RBF\n# from scipy.stats import uniform, randint\n\n# # Define the hyperparameter space\n# param_distributions = {\n#     \"kernel\": [1.0 * RBF(l) for l in uniform(0.01, 10).rvs(10)],\n#     \"optimizer\": [\"fmin_l_bfgs_b\", \"fmin_tnc\", \"fmin_powell\"],\n#     \"max_iter_predict\": randint(10, 500),\n# }\n\n# # Define the model\n# model = GaussianProcessClassifier(random_state=1)\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(\n#     model, param_distributions, n_iter=20, cv=5, n_jobs=-1, random_state=1\n# )\n\n# # Fit the random search to the data\n# random_search.fit(X_train[:1000], y_train[:1000])\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# gpc_params = random_search.best_params_\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.747158Z","iopub.execute_input":"2024-03-30T11:05:19.747789Z","iopub.status.idle":"2024-03-30T11:05:19.755764Z","shell.execute_reply.started":"2024-03-30T11:05:19.747749Z","shell.execute_reply":"2024-03-30T11:05:19.75466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.13 ExtraTrees Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"\n# # Define the hyperparameter space\n# param_distributions = {\n#     'n_estimators': np.arange(100, 1000,100),\n#     'max_depth': [None, 5, 10, 15],\n#     'max_features': ['sqrt', 'log2'],\n#     'min_samples_split': np.arange(2, 10,2),\n#     'min_samples_leaf': np.arange(1, 5,1),\n#     'bootstrap': [True, False]\n# }\n\n# # Define the model\n# model = ExtraTreesClassifier(random_state=1)\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(model, param_distributions, n_iter=20, cv=5, n_jobs=-1, random_state=1)\n\n# # Fit the random search to the data\n# random_search.fit(X_train, y_train)\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# et_params = random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.757448Z","iopub.execute_input":"2024-03-30T11:05:19.758087Z","iopub.status.idle":"2024-03-30T11:05:19.770599Z","shell.execute_reply.started":"2024-03-30T11:05:19.75805Z","shell.execute_reply":"2024-03-30T11:05:19.769391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.14 DecisionTrees Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# # Define the hyperparameter space\n# param_distributions = {\n#     'max_depth': np.arange(2, 50,1),\n#     'min_samples_split': np.arange(2, 20,2),\n#     'min_samples_leaf': np.arange(1, 10,1),\n#     'criterion': ['gini', 'entropy'],\n# }\n\n# # Define the model\n# model = DecisionTreeClassifier(random_state=42)\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(model, param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=1)\n\n# # Fit the random search to the data\n# random_search.fit(X_train, y_train)\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# dtc_params = random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.774968Z","iopub.execute_input":"2024-03-30T11:05:19.775416Z","iopub.status.idle":"2024-03-30T11:05:19.783813Z","shell.execute_reply.started":"2024-03-30T11:05:19.77537Z","shell.execute_reply":"2024-03-30T11:05:19.782811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc_params= {'min_samples_split': 4, 'min_samples_leaf': 9, 'max_depth': 4, 'criterion': 'gini'}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.788272Z","iopub.execute_input":"2024-03-30T11:05:19.788695Z","iopub.status.idle":"2024-03-30T11:05:19.798603Z","shell.execute_reply.started":"2024-03-30T11:05:19.788657Z","shell.execute_reply":"2024-03-30T11:05:19.797369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.15 AdaBoost Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# from sklearn.ensemble import AdaBoostClassifier\n\n# # Define the hyperparameter space\n# param_distributions = {\n#     'n_estimators': np.range(50, 500,50),\n#     'learning_rate': [0.01, 0.05, 0.1, 0.5, 1],\n#     'algorithm': ['SAMME', 'SAMME.R']\n# }\n\n# # Define the model\n# model = AdaBoostClassifier(random_state=42)\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(model, param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=42)\n\n# # Fit the random search to the data\n# random_search.fit(X_train, y_train)\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# # Set the best parameters to the model\n# ada_params = random_search.best_params_","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.800116Z","iopub.execute_input":"2024-03-30T11:05:19.800722Z","iopub.status.idle":"2024-03-30T11:05:19.809818Z","shell.execute_reply.started":"2024-03-30T11:05:19.800683Z","shell.execute_reply":"2024-03-30T11:05:19.808775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.16 Naive Bayes Tuning","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"\n# # Define the hyperparameter space\n# param_distributions = {\n#     'var_smoothing': np.arange(1e-10, 1e-8,1e-9)\n# }\n\n# # Define the model\n# model = GaussianNB()\n\n# # Define the random search with cross validation\n# random_search = RandomizedSearchCV(model, param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=1)\n\n# # Fit the random search to the data\n# random_search.fit(X_train, y_train)\n\n# # Print the best parameters and score\n# print(f\"Best parameters: {random_search.best_params_}\")\n# print(f\"Best score: {random_search.best_score_}\")\n\n# nb_params=random_search.best_params_\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-30T11:05:19.811245Z","iopub.execute_input":"2024-03-30T11:05:19.81188Z","iopub.status.idle":"2024-03-30T11:05:19.819921Z","shell.execute_reply.started":"2024-03-30T11:05:19.811841Z","shell.execute_reply":"2024-03-30T11:05:19.818836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.2\"></a>\n## 7.2 Model Selection","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Kudos to tetsu2131( [http://www.kaggle.com/tetsutani]()) for this framework, the below parts of the code has been taken and modified from. Please support the account if you this work.</font>","metadata":{}},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, kfold=True, n_splits=5):\n        self.n_splits = n_splits\n        self.kfold = kfold\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n        else:\n            X_train, X_val = X.iloc[:int(X_train.shape[0]/10)], X.iloc[int(X_train.shape[0]/10):]\n            y_train, y_val = y.iloc[:int(X_train.shape[0]/10)], y.iloc[int(X_train.shape[0]/10):]\n            yield X_train, X_val, y_train, y_val\n\nclass Classifier:\n    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_model()\n        self.len_models = len(self.models)\n        \n    def _define_model(self):\n        xgb_params.update({\n            'n_estimators': self.n_estimators,\n            'objective': 'binary:logistic',\n            'n_jobs': -1,\n            'random_state': self.random_state,\n        })\n        if self.device == 'gpu':\n            xgb_params.update({\n            'tree_method' :'gpu_hist',\n            'predictor': 'gpu_predictor',\n          })\n\n        lgb_params.update({\n            'n_estimators': self.n_estimators,\n            'objective': 'binary',\n            'random_state': self.random_state,\n        })\n\n        cat_params.update({\n            'n_estimators': self.n_estimators,\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n        })\n        \n        cat_sym_params = cat_params.copy()\n        cat_sym_params['grow_policy'] = 'SymmetricTree'\n        cat_dep_params = cat_params.copy()\n        cat_dep_params['grow_policy'] = 'Depthwise'\n        dt_params= {'min_samples_split': 80, 'min_samples_leaf': 30, 'max_depth': 8, 'criterion': 'gini'}\n#         rf_params.update({\n#             'n_estimators': self.n_estimators,\n#         })\n        models = {\n            'xgb': xgb.XGBClassifier(**xgb_params),\n            'lgb': lgb.LGBMClassifier(**lgb_params),\n            'cat': CatBoostClassifier(**cat_params),\n            \"cat_sym\": CatBoostClassifier(**cat_sym_params),\n            \"cat_dep\": CatBoostClassifier(**cat_dep_params),\n            'lr': LogisticRegression(),\n            'rf': RandomForestClassifier(max_depth= 9,max_features= 'auto',min_samples_split= 10,\n                                                           min_samples_leaf= 4,  n_estimators=500,random_state=self.random_state),\n            'hgb': HistGradientBoostingClassifier(max_iter=self.n_estimators,learning_rate=0.01, loss=\"binary_crossentropy\", \n                                                  n_iter_no_change=300,random_state=self.random_state),\n            'gbdt': GradientBoostingClassifier(**gbm_params,random_state=self.random_state),\n            'svc': SVC(gamma=\"auto\", probability=True),\n            'knn': KNeighborsClassifier(n_neighbors=5),\n            'mlp': MLPClassifier(random_state=self.random_state, max_iter=1000),\n#             'gpc': GaussianProcessClassifier(**gpc_params, random_state=self.random_state),\n            'etr':ExtraTreesClassifier(min_samples_split=55, min_samples_leaf= 15, max_depth=10,\n                                       n_estimators=200,random_state=self.random_state),\n            'dt' :DecisionTreeClassifier(**dt_params,random_state=self.random_state),\n            'ada': AdaBoostClassifier(random_state=self.random_state),\n#             'GNB': GaussianNB(**nb_params),\n#             'ann':ann,\n        }\n        \n        return models","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:19.821362Z","iopub.execute_input":"2024-03-30T11:05:19.822029Z","iopub.status.idle":"2024-03-30T11:05:19.849698Z","shell.execute_reply.started":"2024-03-30T11:05:19.82199Z","shell.execute_reply":"2024-03-30T11:05:19.848554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.3\"></a>\n## 7.3 Optimizer","metadata":{}},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", -1, 1) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n\n        # Calculate the Recall score for the weighted prediction\n        precisions,recalls, thresholds=precision_recall_curve(y_true,weighted_pred)\n#         cutoff=f1_cutoff(precisions,recalls, thresholds)\n        cutoff=acc_cutoff(y_true,weighted_pred)\n        y_weight_pred=np.where(weighted_pred>=float(cutoff),1,0)        \n        score = metrics.accuracy_score(y_true, y_weight_pred)\n        log_loss_score=log_loss(y_true, weighted_pred)\n        return score#/log_loss_score\n\n    def fit(self, y_true, y_preds, n_trials=10000):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        self.study = optuna.create_study(sampler=sampler, study_name=\"OptunaWeights\", direction='maximize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds, n_trials=10000):\n        self.fit(y_true, y_preds, n_trials=n_trials)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights\n    \ndef acc_cutoff_class(y_valid, y_pred_valid):\n    y_valid=np.array(y_valid)\n    y_pred_valid=np.array(y_pred_valid)\n    fpr, tpr, threshold = metrics.roc_curve(y_valid, y_pred_valid)\n    pred_valid = pd.DataFrame({'label': y_pred_valid})\n    thresholds = np.array(threshold)\n    pred_labels = (pred_valid['label'].values > thresholds[:, None]).astype(int)\n    acc_scores = (pred_labels == y_valid).mean(axis=1)\n    acc_df = pd.DataFrame({'threshold': threshold, 'test_acc': acc_scores})\n    acc_df.sort_values(by='test_acc', ascending=False, inplace=True)\n    cutoff = acc_df.iloc[0, 0]\n    y_pred_valid=np.where(y_pred_valid<float(cutoff),0,1)\n    return y_pred_valid","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:19.851371Z","iopub.execute_input":"2024-03-30T11:05:19.852071Z","iopub.status.idle":"2024-03-30T11:05:19.874116Z","shell.execute_reply.started":"2024-03-30T11:05:19.85203Z","shell.execute_reply":"2024-03-30T11:05:19.872788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.4\"></a>\n## 7.4 Model Training","metadata":{}},{"cell_type":"code","source":"kfold = True\nn_splits = 1 if not kfold else 10\nrandom_state = 2023\nrandom_state_list = [2140] # used by split_data [71]\nn_estimators = 9999 # 9999\nearly_stopping_rounds = 200\nverbose = False\ndevice = 'cpu'\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)\n\n# Initialize an array for storing test predictions\ntest_predss = np.zeros(X_test.shape[0])\ny_train_pred=y_train.copy()\ntest_class=sub.copy()\nensemble_score = []\nweights = []\ntrained_models = {'xgb':[], 'lgb':[], 'cat':[]}\n\n    \nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n            \n    # Get a set of Regressor models\n    classifier = Classifier(n_estimators, device, random_state)\n    models = classifier.models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if ('cat' in name) or (\"lgb\" in name) or (\"xgb\" in name):\n            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        elif name in 'ann':\n            model.fit(X_train_, y_train_, validation_data=(X_val, y_val),batch_size=5, epochs=50,verbose=verbose)\n        else:\n            model.fit(X_train_, y_train_)\n        \n        if name in 'ann':\n            test_pred = np.array(model.predict(X_test))[:, 0]\n            y_val_pred = np.array(model.predict(X_val))[:, 0]\n        else:\n            test_pred = model.predict_proba(X_test)[:, 1]\n            y_val_pred = model.predict_proba(X_val)[:, 1]\n\n#         score = roc_auc_score(y_val, y_val_pred)\n        score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] Accuracy score: {score:.5f}')\n        \n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n        \n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n    # Use Optuna to find the best ensemble weights\n    optweights = OptunaWeights(random_state=random_state)\n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n    \n    cutoff=acc_cutoff(y_val,y_val_pred)\n#     score = roc_auc_score(y_val, y_val_pred)\n    score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] --------Accuracy score {score:.5f}')\n    ensemble_score.append(score)\n    weights.append(optweights.weights)\n    \n    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n    y_train_pred.loc[y_val.index]=np.array(y_val_pred)\n    \n    test_class[n]=np.where(optweights.predict(test_preds)>float(cutoff),1,0) \n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:19.878795Z","iopub.execute_input":"2024-03-30T11:05:19.879195Z","iopub.status.idle":"2024-03-30T11:05:20.496282Z","shell.execute_reply.started":"2024-03-30T11:05:19.879155Z","shell.execute_reply":"2024-03-30T11:05:20.493616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean Accuracy score of the ensemble\nmean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Ensemble Accuracy score {mean_score:.5f}  {std_score:.5f}')\n\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f}  {std_weight:.5f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.497386Z","iopub.status.idle":"2024-03-30T11:05:20.498238Z","shell.execute_reply.started":"2024-03-30T11:05:20.49794Z","shell.execute_reply":"2024-03-30T11:05:20.497967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\"> Based on the validation set, we can decide the decision boundary that maximizes Accuracy","metadata":{}},{"cell_type":"code","source":"precisions,recalls, thresholds=precision_recall_curve(y_train,y_train_pred)\n# cutoff=f1_cutoff(precisions,recalls, thresholds)\ncutoff=acc_cutoff(y_train,y_train_pred)\ncutoff","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.50048Z","iopub.status.idle":"2024-03-30T11:05:20.501097Z","shell.execute_reply.started":"2024-03-30T11:05:20.500779Z","shell.execute_reply":"2024-03-30T11:05:20.500814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred=np.where(test_predss>float(cutoff),1,0)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.503105Z","iopub.status.idle":"2024-03-30T11:05:20.503731Z","shell.execute_reply.started":"2024-03-30T11:05:20.503398Z","shell.execute_reply":"2024-03-30T11:05:20.503432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.5\"></a>\n## 7.5 Feature importance Visualization (XGBoost, LightGBM, Catboost)","metadata":{}},{"cell_type":"code","source":"def visualize_importance(models, feature_cols, title, top=20):\n    importances = []\n    feature_importance = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df[\"importance\"] = model.feature_importances_\n        _df[\"feature\"] = pd.Series(feature_cols)\n        _df[\"fold\"] = i\n        _df = _df.sort_values('importance', ascending=False)\n        _df = _df.head(top)\n        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n        \n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n    plt.figure(figsize=(12, 10))\n    sns.barplot(x='importance', y='feature', data=feature_importance, color='skyblue', errorbar='sd')\n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.title(f'{title} Feature Importance [Top {top}]', fontsize=18)\n    plt.grid(True, axis='x')\n    plt.show()\n    \nfor name, models in trained_models.items():\n    visualize_importance(models, list(X_train.columns), name)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.505751Z","iopub.status.idle":"2024-03-30T11:05:20.50637Z","shell.execute_reply.started":"2024-03-30T11:05:20.506037Z","shell.execute_reply":"2024-03-30T11:05:20.50607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\"> We can see that the transformation techniques had worked well and the column Expenditure created using bivariate analysis has been helpful.\n<font size=\"3\"> We can also see TFIDF based columns, cluster encoded columns worked well","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7.6\"></a>\n## 7.6 Results","metadata":{}},{"cell_type":"code","source":"test_class=test_class.drop(columns=['PassengerId','Transported'])\ntest_class['Transported']=test_class.mode(axis=1)[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.508742Z","iopub.status.idle":"2024-03-30T11:05:20.509387Z","shell.execute_reply.started":"2024-03-30T11:05:20.509042Z","shell.execute_reply":"2024-03-30T11:05:20.509077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub['Transported'] = np.where(test_predss>cutoff,1,0).astype(bool)\nsub['Transported']=test_class['Transported']\nsub.to_csv('submission_model.csv',index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.511036Z","iopub.status.idle":"2024-03-30T11:05:20.512178Z","shell.execute_reply.started":"2024-03-30T11:05:20.511847Z","shell.execute_reply":"2024-03-30T11:05:20.511884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[\"Transported\"].value_counts()/sub[\"Transported\"].shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.514048Z","iopub.status.idle":"2024-03-30T11:05:20.514661Z","shell.execute_reply.started":"2024-03-30T11:05:20.514313Z","shell.execute_reply":"2024-03-30T11:05:20.514364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Experiment","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">We can use the results from different models and use AND/OR gates to identify data points that are difficult to predict. This would work because these are coming from possibly different feature engineering methods and ensemble models</font>\n","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\"> In this case I'm selecting the publicly available top scored submissions</font>\n\n* Notebook by [@viktortaran](https://www.kaggle.com/code/viktortaran/space-titanic)\n* Notebook by [@danutstinga](https://www.kaggle.com/code/danutstinga/solution)\n* Notebook by [@jimliu](https://www.kaggle.com/code/jimliu/0-81669-misaelcribeiro-solution-modularity-fe)","metadata":{}},{"cell_type":"code","source":"sub1=pd.read_csv(\"/kaggle/input/space-titanic/XGB_best.csv\")\nsub2=pd.read_csv(\"/kaggle/input/solution/submission.csv\")\nsub3=pd.read_csv(\"/kaggle/input/0-81669-misaelcribeiro-solution-modularity-fe/submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.516319Z","iopub.status.idle":"2024-03-30T11:05:20.517257Z","shell.execute_reply.started":"2024-03-30T11:05:20.516925Z","shell.execute_reply":"2024-03-30T11:05:20.516962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply an OR Gate to increase bads","metadata":{}},{"cell_type":"code","source":"sub_combined=sub1.copy()\nsub1.Transported.value_counts()/sub1.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.519251Z","iopub.status.idle":"2024-03-30T11:05:20.519864Z","shell.execute_reply.started":"2024-03-30T11:05:20.51954Z","shell.execute_reply":"2024-03-30T11:05:20.519572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_combined['Transported']=sub1['Transported'] | sub2['Transported'] |sub3['Transported'] |sub[\"Transported\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.520991Z","iopub.status.idle":"2024-03-30T11:05:20.521604Z","shell.execute_reply.started":"2024-03-30T11:05:20.521269Z","shell.execute_reply":"2024-03-30T11:05:20.521303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_combined['Transported'].value_counts()/sub1.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.52318Z","iopub.status.idle":"2024-03-30T11:05:20.523803Z","shell.execute_reply.started":"2024-03-30T11:05:20.523471Z","shell.execute_reply":"2024-03-30T11:05:20.523504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_combined.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T11:05:20.52625Z","iopub.status.idle":"2024-03-30T11:05:20.526871Z","shell.execute_reply.started":"2024-03-30T11:05:20.526543Z","shell.execute_reply":"2024-03-30T11:05:20.526578Z"},"trusted":true},"execution_count":null,"outputs":[]}]}